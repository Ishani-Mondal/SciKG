{"title": [{"text": "Syntactic Category Learning as Iterative Prototype-Driven Clustering", "labels": [], "entities": [{"text": "Syntactic Category Learning", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9031961758931478}]}], "abstractContent": [{"text": "We layout a model for minimally supervised syntactic category acquisition which combines psychologically plausible concepts from standard NLP part-of-speech tagging applications with simple cognitively motivated dis-tributional statistics.", "labels": [], "entities": [{"text": "syntactic category acquisition", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.6966068347295126}, {"text": "NLP part-of-speech tagging", "start_pos": 138, "end_pos": 164, "type": "TASK", "confidence": 0.5833161075909933}]}, {"text": "The model assumes a small set of seed words (Haghighi and Klein, 2006), an approach with motivation in (Pinker, 1984)'s semantic bootstrapping hypothesis, and repeatedly constructs hierarchical ag-glomerative clusterings over a growing lexicon.", "labels": [], "entities": []}, {"text": "Clustering is performed on the basis of word-adjacent syntactic frames alone (Mintz, 2003) with no reference to word-internal features , which has been shown to yield qualitatively coherent POS clusters (Redington et al., 1998).", "labels": [], "entities": []}, {"text": "A prototype-driven labeling process based on tree-distance yields results comparable to unsupervised algorithms based on complex statistical optimization while maintaining its cognitive underpinnings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised part-of-speech (POS) tagging is one of statistical NLP's classic problems (, but its reliance on large POS-annotated corpora makes it impractical to extend to new domains and unsuitable for low-resource languages without pre-existing annotation.", "labels": [], "entities": [{"text": "Supervised part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5721179942289988}]}, {"text": "Unsupervised tagging presents an interesting alternative because it does not require labelled training data, but it is a more difficult problem, and typical performance is substantially lower.", "labels": [], "entities": [{"text": "Unsupervised tagging", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6090448349714279}]}, {"text": "The fundamental problem of training without examples aside, unsupervised tagging algorithms induce some number of clusters which must be mapped onto a desired tag set.", "labels": [], "entities": []}, {"text": "This mapping need not be one-to-one, so error maybe introduced going from algorithm output to final results.", "labels": [], "entities": []}, {"text": "Within NLP, unsupervised POS tagging is typically approached as a statistical optimization problem, though implementations vary widely. and induce clusters via class-based n-gram models, the latter including morphological information.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.7988690435886383}]}, {"text": "While these perform reasonably well, more recent approaches have instead been centered around HMMs. and Johnson (2007) both place Dirichlet priors over the multinomial parameters of roughly typical HMM POS taggers.", "labels": [], "entities": []}, {"text": "define a feature-based HMM instead which allows the inclusion of discriminative orthographic information.", "labels": [], "entities": []}, {"text": "1) implement a model based on Markov random fields (MRFs), an undirected generalization of HMMs.", "labels": [], "entities": []}, {"text": "Haghighi & Klein (H&K) also differs from the above models in that it is minimally supervised with prototypes.", "labels": [], "entities": []}, {"text": "In their implementation, three words per tag are defined as prototypes or seeds and labelled with their correct tags, and the MRF sorts out which prototypes unknown words are most similar to.", "labels": [], "entities": []}, {"text": "They choose the most frequent words per tag imposing the requirement that each seed must only support a single tag, so given the 45-tag Penn Treebank tag set, this yields 135 seeds, a tiny fraction of the tens of thousands of types in the Wall Street Journal corpus.", "labels": [], "entities": [{"text": "Penn Treebank tag set", "start_pos": 136, "end_pos": 157, "type": "DATASET", "confidence": 0.9914091676473618}, {"text": "Wall Street Journal corpus", "start_pos": 239, "end_pos": 265, "type": "DATASET", "confidence": 0.9535123258829117}]}, {"text": "Their MRF is trained on both distributional features and orthographic features follow-ing.", "labels": [], "entities": [{"text": "MRF", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9066408276557922}]}, {"text": "This achieves impressive results on English, but their reliance on orthographic features curtails the model's performance on Chinese.", "labels": [], "entities": []}, {"text": "H&K's and the other computational models above are engineered primarily with performance in mind rather than cognitive plausibility.", "labels": [], "entities": []}, {"text": "The complex optimization models in particular are slow, taking tens of hours to complete, while the simpler n-gram based models run in tens of minutes.", "labels": [], "entities": []}, {"text": "But fundamentally, they are all tools which take advantage of statistical, especially distributional information.", "labels": [], "entities": []}, {"text": "For well over half a century now, it has been understood that children make use of distributional cues in the process of syntactic category (roughly POS) assignment as well.", "labels": [], "entities": [{"text": "syntactic category (roughly POS) assignment", "start_pos": 121, "end_pos": 164, "type": "TASK", "confidence": 0.5938886020864759}]}, {"text": "For example,'s classic study found that children recognize a nonce word \"sib\" as a noun when if it is introduced to them in a sentence like \"This is a sib,\" but prefer to label it as a verb if it is introduced in \"It is sibbing.\"", "labels": [], "entities": []}, {"text": "Along similar lines, present nonce words to year-old children and watch whether they then tend to focus on an image depicting an objector an action.", "labels": [], "entities": []}, {"text": "They find that children presented with \"the mige\" prefer to look at the object image over the action image, consistent with them understanding that the word should be a noun.", "labels": [], "entities": []}, {"text": "These experiments demonstrate that learners make use of distributional information (e.g., \"a/the \") even with limited exposure.", "labels": [], "entities": []}, {"text": "Studies of childdirected corpora suggest the presence of distributional cues as well in a more naturalistic setting).", "labels": [], "entities": []}, {"text": "Children's sensitivity to such distributional information has been operationalized through the notion of frequent frames, single-word contexts on either side of an item.", "labels": [], "entities": []}, {"text": "Experimental evidence demonstrates that children who are exposed to items within the same frame, (e.g, \"the is\") treat those items as members of the same class.", "labels": [], "entities": []}, {"text": "However, the large number classes induced from frequent frames do not provide a clean one-to-one mapping to syntactic categories (.", "labels": [], "entities": []}, {"text": "Syntactic frames can be seen as a purely structural cue, but semantic information play a role as well.", "labels": [], "entities": []}, {"text": "As described by the semantic bootstrapping hypothesis, children have innate rules for mapping real-world semantic onto syntactic categories.", "labels": [], "entities": []}, {"text": "For example, actions should be verbs, objects should be nouns, and soon.", "labels": [], "entities": []}, {"text": "These then serve as anchors in the input to provide early distributional context.", "labels": [], "entities": []}, {"text": "The validity of semantic bootstrapping's claims of innateness and the exact nature and number of syntactic classes are not critical for the present work, rather it is sufficient that bootstrapping guides children to some kind of categorization.", "labels": [], "entities": []}, {"text": "Experimental work has long confirmed that semantic bootstrapping basic prediction holds and that children really do associate actions and concrete objects with verbs and nouns.", "labels": [], "entities": [{"text": "semantic bootstrapping basic prediction", "start_pos": 42, "end_pos": 81, "type": "TASK", "confidence": 0.5948309972882271}]}, {"text": "We develop a computational implementation for semantic bootstrapping in syntactic frames which draws inspiration from hierarchical clustering and from's prototype-driven model for POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 180, "end_pos": 191, "type": "TASK", "confidence": 0.7838831543922424}]}, {"text": "Since we know that children do not wait patiently to buildup large vocabularies replete with distributional information before attempting to assign syntactic categories, our algorithm runs iteratively on the lexicon as it grows, revising category assignments as more evidence comes in.", "labels": [], "entities": []}, {"text": "Additionally, we discard all word-internal morphological and phonological information to test the distributional cues on their own.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: section 2 introduces the iterative prototype-driven clustering model for tagging along with the basic insights behind it.", "labels": [], "entities": []}, {"text": "Section 3 discusses the problem of evaluation for unsupervised POS tagging, provides results for childdirected English under various conditions, comparative results across nine other languages, and a comparison with H&K on English and Chinese.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.847731351852417}]}, {"text": "Section 4 reviews the model's cognitive plausibility and discusses possible extensions to the algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, the iterative prototype-driven labeling model is evaluated on corpora from a wide range of languages.", "labels": [], "entities": []}, {"text": "First, we discuss results on the English Brown (Harvard) corpus within the CHILDES corpus of child-directed speech, since this serves as an in-domain application.", "labels": [], "entities": [{"text": "English Brown (Harvard) corpus", "start_pos": 33, "end_pos": 63, "type": "DATASET", "confidence": 0.8568075150251389}, {"text": "CHILDES corpus of child-directed speech", "start_pos": 75, "end_pos": 114, "type": "DATASET", "confidence": 0.9036401629447937}]}, {"text": "Performance is compared on a range of tag sets, seed set sizes, and seed selection procedures.", "labels": [], "entities": []}, {"text": "Next, we test on a range of languages from the Universal Dependency Treebank () in order to gain insight into how language specific factors influence results while keeping extra-linguistic factors constant to the extent possible.", "labels": [], "entities": [{"text": "Universal Dependency Treebank", "start_pos": 47, "end_pos": 76, "type": "DATASET", "confidence": 0.5628348092238108}]}, {"text": "Finally, we run on the WSJ portion of the Penn Treebank () and on the Penn Chinese Treebank () in order to compare against H&K's prototype-driven model and ground this work in the wider field.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.9049149751663208}, {"text": "Penn Treebank", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8607034385204315}, {"text": "Penn Chinese Treebank", "start_pos": 70, "end_pos": 91, "type": "DATASET", "confidence": 0.9706092476844788}]}, {"text": "Evaluation for supervised POS tagging is straightforward.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.8830616176128387}]}, {"text": "Sentences from a test corpus are labelled, then for each token, the proposed label is compared to the human annotated label.", "labels": [], "entities": []}, {"text": "The simplest of such metrics is one-to-one token accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9747306704521179}]}, {"text": "On the other hand, the problem of evaluation is more complicated for unsupervised algorithms.", "labels": [], "entities": []}, {"text": "Results are broadly not comparable across experiments because a wide of evaluation metrics are employed.", "labels": [], "entities": []}, {"text": "Token-based metrics are an excellent option when the task is to automatically annotate running text with part-of-speech tags, but they have undesirable traits when applied to syntactic category learning.", "labels": [], "entities": [{"text": "syntactic category learning", "start_pos": 175, "end_pos": 202, "type": "TASK", "confidence": 0.6644579768180847}]}, {"text": "If syntactic category learning is analogous to labeling types in a dictionary or lexicon, then labeling sequences of text just obfuscates results.", "labels": [], "entities": [{"text": "syntactic category learning", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.641645590464274}]}, {"text": "Type frequencies are not uniform across a corpus, so tokenbased metrics weight assignments to frequent types higher than assignments to infrequent types.", "labels": [], "entities": []}, {"text": "For example, an algorithm which labels the pronoun \"you\" incorrectly will be punished more severely than one which labels the pronoun \"whomever\" incorrectly simply because \"you\" is more common than \"whomever\" inmost corpora.", "labels": [], "entities": []}, {"text": "This is particularly problematic because word frequencies follow a distribution where just a few types are hundreds of times more frequent than most others.", "labels": [], "entities": []}, {"text": "Mislabeling any pronoun is hundreds of times more damaging than labeling almost any noun from our daily lives (like \"table\" or \"bug\") incorrectly.", "labels": [], "entities": []}, {"text": "Mislabeling any common noun is much more damaging than mislabeling a rare noun, and noun frequencies can be highly corpus specific.", "labels": [], "entities": []}, {"text": "This makes it difficult to gauge the relative performance of different models.", "labels": [], "entities": []}, {"text": "A one-to-many type accuracy is a better choice for scoring syntactic category learning because each item is weighted the same for scoring.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8621044158935547}, {"text": "scoring syntactic category learning", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.6912619695067406}]}, {"text": "In natural language, types can potentially support multiple labels, so a one-to-many metric is needed to account for this.", "labels": [], "entities": []}, {"text": "For example, \"bat\" could be a noun or a verb, so an algorithm which classifies it as either should be correct.", "labels": [], "entities": []}, {"text": "The many-to-one accuracy used in some unsupervised models does not make sense here because the model does not output clusters which need to be mapped.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9785112738609314}]}, {"text": "The algorithm never demarcates clusters and instead assigns labels to individual items.", "labels": [], "entities": []}, {"text": "The difference between token and type-based metrics becomes clear when calculating the baselines for our model.", "labels": [], "entities": []}, {"text": "Scoring by type accuracy, the baseline is arrived at by scoring the initial seeds as correct and marking everything else wrong.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9517350792884827}]}, {"text": "This typically yields a score of under 10%, often under 1%, and corresponds directly to the proportion of types which are selected as seeds.", "labels": [], "entities": []}, {"text": "However, this tends to correspond to a > 50% token accuracy because seeds are drawn from the most frequent types.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9633892774581909}]}, {"text": "A final type score of, say, 70% is more meaningful than a final token score of 70% because the improvement over the baseline is greater.", "labels": [], "entities": [{"text": "final type score", "start_pos": 2, "end_pos": 18, "type": "METRIC", "confidence": 0.834284782409668}]}, {"text": "The CHILDES corpus contains transcriptions of naturalistic child-directed speech.", "labels": [], "entities": [{"text": "CHILDES corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8787714540958405}]}, {"text": "The English subset studied here consists of all caregiver text extracted from Adam, Eve, and Sarah of the Harvard (Brown) corpus, yielding 8,307 types and 588,888 tokens.", "labels": [], "entities": [{"text": "Harvard (Brown) corpus", "start_pos": 106, "end_pos": 128, "type": "DATASET", "confidence": 0.946484899520874}]}, {"text": "The tag set used in the Brown corpus consists of 55 idiosyncratic tags, and this is tested along with a mapping that reduces these to an 8-tag (+SKIP 5 ) set (DT, IN, JJ, NN, PRP, RB, VB, SKIP).", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9648354351520538}]}, {"text": "the impact of k on performance and indicates the best achieved type accuracy results training on the full Brown tag set then mapping to the reduced set (Brown-to-Reduced) for scoring for final k = 100, k = 1000, and k = 8307.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9787574410438538}]}, {"text": "6 As expected, percent correct decreases for higher k because infrequent words which occur only once or twice in the corpus provide weaker distributional information than frequent words do.", "labels": [], "entities": [{"text": "correct", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9708716869354248}]}, {"text": "The number of seeds used never reaches 55*3 because some tags do not appear in the top k words.", "labels": [], "entities": []}, {"text": "This is the same as the WSJ corpus FW-problem described earlier.", "labels": [], "entities": [{"text": "WSJ corpus FW-problem", "start_pos": 24, "end_pos": 45, "type": "DATASET", "confidence": 0.8344829281171163}]}, {"text": "To determine what impact the choice of tag set and number of seeds have on the results, the experiment was run and evaluated directly on the Brown tag set as well as the reduced tag set.", "labels": [], "entities": []}, {"text": "Reduced tag set experiments were run with 3 seeds per tag and 11 per tag.", "labels": [], "entities": []}, {"text": "The Brown-to-reduced tests smooth out the difference between related tags when computing accuracy scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9938955903053284}]}, {"text": "The difference between B-to-R and straight Brown in implies that the model struggles to differentiate some of the Brown tags.", "labels": [], "entities": []}, {"text": "This could be because some of the more eccentric Brown tags do not actually distinguish distributionally coherent classes.", "labels": [], "entities": []}, {"text": "The difference between the 3-seed and 11-seed results indicates that performance largely depends on the number of seeds.", "labels": [], "entities": []}, {"text": "Note, however, that the baseline is still quite low even with 11 seeds per tag.", "labels": [], "entities": []}, {"text": "The point of syntactic frames is that they are available as primary evidence early on, and nobody who works on them would argue that they are the only source of evidence, so it is unsurprising that performance declines using frames alone for large k.", "labels": [], "entities": []}, {"text": "If syntactic frames are indeed useful, then we would expect their application on early vocabulary to carry benefits downstream, and this this is what   shows.", "labels": [], "entities": []}, {"text": "It compares the results of iterative application from k = 100 to k = 8307 used for the previous experiments to a single non-iterative application k = 8307.", "labels": [], "entities": []}, {"text": "The iterative results are 9 to 18 points higher, demonstrating that syntactic frames applied to early vocabulary setup better performance later.", "labels": [], "entities": []}, {"text": "The iterative application of the algorithm which expands the seed set as the lexicon grows is critical to model performance.", "labels": [], "entities": []}, {"text": "Up to this point, seeds have been selected post hoc by corpus frequency, which cannot possibly be how children use semantic bootstrapping.", "labels": [], "entities": []}, {"text": "To correct for this, a set of 82 lower frequency but high saliency seed words was selected for comparison based on studies of salience in child-directed speech.", "labels": [], "entities": []}, {"text": "Tables 5 lays out the result achieved with salient seeds on the reduced tag set, which can be compared to the 11 frequent seed results from Table 2.", "labels": [], "entities": []}, {"text": "The type accuracy is lower and roughly what is to be expected given the size of the seed set even though the seeds themsleves are of lower frequency on average.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9276267886161804}]}, {"text": "The CHILDES results suggest that information from syntactic frames is useful for assigning syntactic categories from English child-directed speech.", "labels": [], "entities": [{"text": "CHILDES", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.8622875213623047}, {"text": "assigning syntactic categories from English child-directed speech", "start_pos": 81, "end_pos": 146, "type": "TASK", "confidence": 0.7564234222684588}]}, {"text": "In order to compare performance across other languages as well, we apply the algorithm to nine languages from the Universal Dependency Treebank: German, Spanish, French, Indonesian, Italian, Japanese, Korean, Brazilian Portuguese, and Swedish.", "labels": [], "entities": [{"text": "Universal Dependency Treebank", "start_pos": 114, "end_pos": 143, "type": "DATASET", "confidence": 0.7407530943552653}]}, {"text": "These corpora share a 10-tag tag set (excluding punctuation and non-word tags) and were compiled for the same task, which means that different performances reflect differences in the languages themselves to the extent possible.", "labels": [], "entities": []}, {"text": "In order to more closely align these experiments to syntactic category learning, no seeds were provided for the punctuation ( , MAD, MID, PAD) or non-word (X) tags, and punctuation and non-words were not scored.", "labels": [], "entities": []}, {"text": "Performance varies substantially across languages.", "labels": [], "entities": []}, {"text": "At k = 1000, type accuracy ranges from the low 30s (Korean), to the high 70s (German, French, Indonesian), and at k = 10000 from the mid 20s (German) to the 60s (French, Indonesian).", "labels": [], "entities": [{"text": "type", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.7349017858505249}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9238600730895996}]}, {"text": "Much of this variation can be explained linguistically with the important caveat that extra-linguistic factors in the corpora must still beat play.", "labels": [], "entities": []}, {"text": "Languages with complicated inflection are at a disadvantage because the distributional context information of their roots is spread across inflectional forms, and without reference to word-internal features, it is impossible to group these forms by root to pool that information.", "labels": [], "entities": []}, {"text": "For example, while all the distributional information for English BLUE is collected in the contexts for the word \"blue,\" German spreads its distributional information across the contexts for its six inflected forms, \"blau,\" \"blauer,\" \"blauen,\" \"blauem,\" \"blaue,\" and \"blaues,\" and our algorithm has noway to combine them.", "labels": [], "entities": []}, {"text": "Another way to think about this is to compare token/type ratios between languages, which are equivalent to how many syntactic frames on average contribute to the context vectors for each type.", "labels": [], "entities": []}, {"text": "A low ratio indicates that the typical context vector is sparser because it accounts for information from fewer frames.", "labels": [], "entities": []}, {"text": "Languages with complex inflection naturally have more types and a lower token/type ratio.", "labels": [], "entities": []}, {"text": "The particularly poor performance for Korean and Japanese is at least partially due to the UTB corpus tokenization which attaches phrase final syntactic clitics to the preceding word as opposed to standalone tokens.", "labels": [], "entities": [{"text": "UTB corpus", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.9083099067211151}]}, {"text": "This is inline with the traditional conception of \"word\" boundaries in these languages (called bunsetsu in Japanese and eojeol in Korean), but is not an obviously correct choice from a cognitive standpoint, and would be equivalent to not segmenting the possessive 's clitic in English.", "labels": [], "entities": []}, {"text": "As shown in, bunsetsu tokenization creates multiple words for APPLE and PEAR and two example clitics, each with disjoint right contexts, while standalone tokenization would yield one word each for APPLE and PEAR with both clitics as right contexts.", "labels": [], "entities": []}, {"text": "This technical choice effectively renders the clitics as inflections like in German rather than as useful syntactic context information.", "labels": [], "entities": []}, {"text": "Also, it is unclear why Korean and Japanese perform better at high k than low k at the reported confidences.", "labels": [], "entities": []}, {"text": "Languages with freer word order are also at a disadvantage because the entire premise of syntactic frames assumes that the syntax forces categories to appear adjacent to certain other categories.", "labels": [], "entities": []}, {"text": "A freer word order means more violations of this assump-8 This only happens for Japanese and Korean, and only at c = 0.7, 0.9 for both.", "labels": [], "entities": []}, {"text": "tion relative to the number of frames attested, which manifests as more uniform and less discriminable context vectors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: CHILDES type accuracy by tree size. Baseline indi-", "labels": [], "entities": [{"text": "CHILDES", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8943042755126953}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.806075930595398}, {"text": "Baseline indi-", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.7189080119132996}]}, {"text": " Table 2: CHILDES type accuracy by tag set and seed set size", "labels": [], "entities": [{"text": "CHILDES", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8159974217414856}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.6048375964164734}]}, {"text": " Table 3: Comparison between CHILDES basic and iterative", "labels": [], "entities": [{"text": "CHILDES", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.7595369219779968}]}, {"text": " Table 4: CHILDES one-to-one token accuracy performance", "labels": [], "entities": [{"text": "CHILDES", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8847646117210388}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.8768974542617798}]}, {"text": " Table 5: CHILDES type accuracy with salient seeds", "labels": [], "entities": [{"text": "CHILDES", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8763405680656433}, {"text": "type", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.7042706608772278}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.8859016299247742}]}, {"text": " Table 6: UTB type accuracy by language", "labels": [], "entities": [{"text": "UTB type accuracy", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.5605798363685608}]}, {"text": " Table 8: Wall Street Journal and Chinese Treebank Type Accu-", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.9575080871582031}, {"text": "Chinese Treebank Type Accu-", "start_pos": 34, "end_pos": 61, "type": "DATASET", "confidence": 0.8652503252029419}]}, {"text": " Table 9: Wall Street Journal token accuracy and comparison", "labels": [], "entities": [{"text": "Wall Street Journal token", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.9201201349496841}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.970933735370636}, {"text": "comparison", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.6364986896514893}]}, {"text": " Table 10: Chinese Treebank token accuracy and comparison", "labels": [], "entities": [{"text": "Chinese Treebank token", "start_pos": 11, "end_pos": 33, "type": "DATASET", "confidence": 0.9295476277669271}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9698132276535034}]}]}