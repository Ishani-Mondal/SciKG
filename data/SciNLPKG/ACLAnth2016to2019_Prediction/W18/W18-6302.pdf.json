{"title": [{"text": "Character-level Chinese-English Translation through ASCII Encoding", "labels": [], "entities": [{"text": "Character-level Chinese-English Translation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5816934108734131}]}], "abstractContent": [{"text": "Character-level Neural Machine Translation (NMT) models have recently achieved impressive results on many language pairs.", "labels": [], "entities": [{"text": "Character-level Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7220273188182286}]}, {"text": "They mainly do well for Indo-European language pairs, where the languages share the same writing system.", "labels": [], "entities": []}, {"text": "However, for translating between Chinese and English, the gap between the two different writing systems poses a major challenge because of alack of systematic correspondence between the individual linguistic units.", "labels": [], "entities": [{"text": "translating between Chinese and English", "start_pos": 13, "end_pos": 52, "type": "TASK", "confidence": 0.8370759963989258}]}, {"text": "In this paper, we enable character-level NMT for Chinese, by breaking down Chinese characters into linguistic units similar to that of Indo-European languages.", "labels": [], "entities": [{"text": "NMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.8010278344154358}]}, {"text": "We use the Wubi encoding scheme 1 , which preserves the original shape and semantic information of the characters, while also being reversible.", "labels": [], "entities": []}, {"text": "We show promising results from training Wubi-based models on the character-and subword-level with recurrent as well as convolutional models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Character-level sequence-to-sequence (Seq2Seq) models for machine translation can perform comparably to subword-to-subword or subwordto-character models, when dealing with IndoEuropean language pairs, such as German-English or Czech-English (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7329305112361908}]}, {"text": "Such language pairs benefit from having a common Latin character representation, which facilitates suitable character-to-character mappings to be learned.", "labels": [], "entities": []}, {"text": "This method, however, is more difficult for nonLatin language pairs, such as Chinese-English.", "labels": [], "entities": []}, {"text": "Chinese characters differ from English characters, in the sense that they carry more meaning and resemble subword units in English.", "labels": [], "entities": []}, {"text": "For example, the Chinese character '\u4eba' corresponds to the Overview of the wubi2en approach to Chinese-to-English translation.", "labels": [], "entities": []}, {"text": "A raw Chinese word ('\u627f\u8bfa') is encoded into ASCII characters ('bd|yad'), using the Wubi encoding method, before passing it to a Seq2Seq network.", "labels": [], "entities": []}, {"text": "The network generates the English translation 'commitment', processing one ASCII character at a time.", "labels": [], "entities": []}, {"text": "word 'human' in English.", "labels": [], "entities": []}, {"text": "This lack of correspondence makes the problem more demanding fora Chinese-English character-to-character model, as it would be forced to map higher-level linguistic units in Chinese to individual Latin characters in English.", "labels": [], "entities": []}, {"text": "Good performance on this task may, therefore, require specific architectural decisions.", "labels": [], "entities": []}, {"text": "In this paper, we propose a simple solution to this challenge: encode Chinese into a meaningful string of ASCII characters, using the Wubi method (Lunde, 2009) (Section 3).", "labels": [], "entities": [{"text": "Lunde, 2009)", "start_pos": 147, "end_pos": 159, "type": "DATASET", "confidence": 0.8563854247331619}]}, {"text": "This encoding enables efficient and accurate character-level prediction applications in Chinese, with no changes required to the model architecture (see).", "labels": [], "entities": [{"text": "character-level prediction", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.7135104537010193}]}, {"text": "Our approach significantly reduces the character vocabulary size of a Chinese text, while preserving the shape and semantic information encoded in the Chinese characters.", "labels": [], "entities": []}, {"text": "We demonstrate the utility of the Wubi encoding on subword-and character-level Chinese NMT, comparing the performance of systems trained on Wubi vs. raw Chinese characters (Section 4).", "labels": [], "entities": []}, {"text": "We test three types of Seq2Seq models: recurrent () convolutional) as well as hybrid ().", "labels": [], "entities": []}, {"text": "Our results demonstrate the utility of Wubi as a preprocessing step for Chinese translation tasks, showing promising performance.", "labels": [], "entities": [{"text": "Chinese translation tasks", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.7742528716723124}]}], "datasetContent": [{"text": "In this work, we use a subset of the English and Chinese parts of the United Nations Parallel Corpus ().", "labels": [], "entities": [{"text": "United Nations Parallel Corpus", "start_pos": 70, "end_pos": 100, "type": "DATASET", "confidence": 0.8930999785661697}]}, {"text": "We choose the UN corpus because of its high-quality, man-made translations.", "labels": [], "entities": [{"text": "UN corpus", "start_pos": 14, "end_pos": 23, "type": "DATASET", "confidence": 0.9300176799297333}]}, {"text": "The dataset is sufficient for our purpose: our aim here is not to reach state-of-the-art performance on Chinese-English translation, but to demonstrate the potential of the Wubi encoding on the character level.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.597040593624115}]}, {"text": "We preprocess the UN dataset with the MOSES tokenizer 4 , and use Jieba 5 to segment the Chinese sentence into words, following which we encode the texts into Wubi.", "labels": [], "entities": [{"text": "UN dataset", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.874570369720459}]}, {"text": "We use the '|' character as a subword separator for Wubi, in order to ensure that the mapping from Chinese to Wubi is unique.", "labels": [], "entities": []}, {"text": "We also convert all Chinese punctuation marks (e.g. '\u3002\u3001\u300a\u300b') from UTF-8 to ASCII (e.g. '.,<>') because they share similar linguistic roles to English punctuations.", "labels": [], "entities": []}, {"text": "This conversion additionally decreases the size of the Wubi character vocabulary.", "labels": [], "entities": []}, {"text": "Our final dataset contains 2.1M sentence pairs for training, and 55k pairs for validation and testing respectively contains additional statistics).", "labels": [], "entities": []}, {"text": "Note that our procedures are entirely reversible.", "labels": [], "entities": []}, {"text": "To investigate the utility of the Wubi encoding, we compare the performance of NMT models on four training pairs: raw Chinese-to-English (cn2en) versus Wubi-to-English (wubi2en); English-to-raw Chinese (en2cn) versus Englishto-Wubi (en2wubi).", "labels": [], "entities": []}, {"text": "For each pair, we investigate three levels of sequence granularity: wordlevel, subword-level, and character-level.", "labels": [], "entities": []}, {"text": "The word-level operates on individual English words (e.g. walk) and either raw-Chinese words (e.g. \u7f16 \u8bbe) or Wubi words (e.g. sh|wy).", "labels": [], "entities": []}, {"text": "We limit all wordlevel vocabularies to the 50k most frequent words for each language.", "labels": [], "entities": []}, {"text": "The subword-level is produced using the byte pair encoding (BPE) scheme, capping the vocabulary size at 10k for each language.", "labels": [], "entities": []}, {"text": "The character-level operates on individual raw-Chinese characters (e.g. '\u91cd'), or individual ASCII characters.", "labels": [], "entities": []}, {"text": "In, we present the BLEU scores for all the previously described experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9992378950119019}]}, {"text": "Before computing BLEU, we convert all Chinese outputs to Wubi to ensure a consistent comparison.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9963383674621582}]}, {"text": "This conversion has a one-to-one mapping between Chinese and Wubi, whereas, in the reverse direction, ill-formed Wubi output on the character-level might not be reversible to Chinese.", "labels": [], "entities": []}, {"text": "On the word-level, the Wubi-based models achieve comparable results to their counterparts in Chinese, in both translation directions.", "labels": [], "entities": []}, {"text": "LSTM significantly outperforms FConv across all experiments here, most likely due to its much larger size (see).", "labels": [], "entities": [{"text": "FConv", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.860164999961853}]}, {"text": "On the subword-level, we observe a slight increase of about 0.5 BLEU when translating from English to Wubi instead of raw Chinese.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9995629191398621}]}, {"text": "This increase is most likely due to the difference in the BPE vocabularies: while the English and Wubi BPE rules that were learned cover 100% of the dataset, for Chinese this is 98.7% -the remaining 1.3% had to be replaced by the unk symbol under our vocabulary constraints.", "labels": [], "entities": []}, {"text": "While the models were capable of compensating for this gap when translating to English, in the reverse direction it resulted in a loss of performance.", "labels": [], "entities": []}, {"text": "This highlights one benefit of Wubi on the subword-level: the Latin encoding seems to give a greater flexibility for extracting suitable BPE rules.", "labels": [], "entities": []}, {"text": "It would be interesting to repeat this comparison using much larger datasets and larger BPE vocabularies.", "labels": [], "entities": []}, {"text": "Character-level translation is more difficult than word-level, since the models are expected to not only predict sentence-level semantics, but also to generate the correct spelling of each word.", "labels": [], "entities": [{"text": "Character-level translation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7135679423809052}]}, {"text": "Our char2char Wubi models outperformed the raw Chinese models with 0.95 BLEU points when translating to English, and 0.65 BLEU when translating from English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9989237189292908}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9984885454177856}]}, {"text": "The differences are statistically significant (p = 0.001 and p = 0.034 respectively) according to bootstrap resampling) with 1500 samples.", "labels": [], "entities": []}, {"text": "The results demonstrate the advantage of Wubi on the characterlevel, which outperforms raw Chinese even though it has fewer parameters dedicated for character embeddings and that it has to deal with substantially longer input or output sequences (see).", "labels": [], "entities": []}, {"text": "In, we plot the sentence-level BLEU scores obtained by the char2char models on our test set, with respect to the length of the input sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9801040887832642}]}, {"text": "When translating from Chinese to En-  glish () the Wubi-based model consistently outperforms the raw Chinese model, for all input lengths.", "labels": [], "entities": []}, {"text": "Interestingly, the gap between the two systems increases for longer Chinese inputs of over 20 words, indicating that Wubi is more robust for such examples.", "labels": [], "entities": []}, {"text": "This result could be explained by the fact that the encoder of the char2char model is more suitable for modeling languages with a higher level of granularity such as English and German.", "labels": [], "entities": []}, {"text": "When translating from English to Chinese) Wubi still has a small edge, however in this case we seethe reverse trend: it performs much better on shorter sentences up to 12 English words.", "labels": [], "entities": []}, {"text": "Perhaps, the increased granularity of the output sequence led to an advantage during decoding using beam search.", "labels": [], "entities": []}, {"text": "Interestingly, all the char2char models use only a tiny fraction of their parameters as embeddings, due to the much smaller size of their vocabularies.", "labels": [], "entities": []}, {"text": "The best-performing LSTM word-level model has the majority of its parameters, 61% or over 50M, dedicated to word embeddings.", "labels": [], "entities": []}, {"text": "For the Wubibased character-level models, the number is only 0.3% or 0.21M.", "labels": [], "entities": [{"text": "Wubibased", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.9304782152175903}]}, {"text": "There is even a significant difference between Wubi and Chinese on the characterlevel, for example, en2wb has 12 times fewer embedding parameters than en2cn.", "labels": [], "entities": []}, {"text": "Thus, although char2char performed worse than LSTM in our experiments, these results highlight the potential of character-level prediction for developing compact yet performant translation systems, for Latin as well as non-Latin languages.", "labels": [], "entities": [{"text": "character-level prediction", "start_pos": 112, "end_pos": 138, "type": "TASK", "confidence": 0.7103385776281357}]}, {"text": "In, we present four examples from our test dataset that cover short as well as long sentences.", "labels": [], "entities": []}, {"text": "We also include the translations produced by the character-level char2char systems, which is the main focus of this paper.", "labels": [], "entities": []}, {"text": "Full examples from the additional systems are available in the supplementary material.", "labels": [], "entities": []}, {"text": "In the first example, which is a short sentence resembling the headline of a document, both the wubi2en and cn2en models produced correct translations.", "labels": [], "entities": []}, {"text": "When translating from English to Chinese, however, the en2wubi produced the word '\u4e0e' (highlighted in red) which more correctly matches the ground truth text.", "labels": [], "entities": []}, {"text": "In contrast, the en2cn model produced the synonym '\u548c'.", "labels": [], "entities": []}, {"text": "In the second example, the en2wubi output completely matches the ground truth and is superior to the en2cn output.", "labels": [], "entities": []}, {"text": "The latter failed to correctly translate 'the' to '\u8fd9\u6b21' (marked in green).", "labels": [], "entities": []}, {"text": "The wubi2en translation in the third example accurately translated the word 'believe' (marked in blue) and the full form of the abbreviation 'ldcs' -'the least developed countries' (highlighted in green), whereas the cn2en chooses 'are convinced' and ignores 'ldcs' in its output sentence.", "labels": [], "entities": []}, {"text": "Interestingly, although the ground truth text maps the word 'essential' (marked in red) to three Chinese words '\u81f3 \u4e3a \u91cd \u8981', both en2wubi and en2cn use only a single word to interpret it.", "labels": [], "entities": []}, {"text": "Arguably, en2wubi's translation '\u81f3\u5173\u91cd\u8981' is closer to the ground truth than en2cn's translation '\u5fc5\u4e0d\u53ef\u5c11'.", "labels": [], "entities": []}, {"text": "The fourth example is more challenging.", "labels": [], "entities": []}, {"text": "There, the English ground truth 'requested' (highlighted in blue) maps to two different parts of the Chinese ground truth '\u63d0\u51fa' (in blue) and.", "labels": [], "entities": [{"text": "English ground truth", "start_pos": 11, "end_pos": 31, "type": "DATASET", "confidence": 0.795261561870575}]}, {"text": "This one-to-many mapping confuses both translation models.", "labels": [], "entities": []}, {"text": "While the wubi2en output is closer to the ground truth, the two have little overlap.", "labels": [], "entities": []}, {"text": "For the English-to-Chinese task, the en2cn translation is better than the one produced by en2wubi: while en2cn successfully translated 'without explanation' (in red), the en2wubi model ignored this part of the sentence.", "labels": [], "entities": []}, {"text": "The Wubi-based models tend to produce slightly shorter translations for both directions (see).", "labels": [], "entities": []}, {"text": "In overall, the Wubi-based outputs appear to be visibly better than the raw Chinesebased outputs, in both directions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of our dataset (mean and stan- dard deviation).", "labels": [], "entities": [{"text": "mean and stan- dard deviation", "start_pos": 37, "end_pos": 66, "type": "METRIC", "confidence": 0.7583075364430746}]}, {"text": " Table 4: BLEU test scores on the UN dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9969276785850525}, {"text": "UN dataset", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.9649387001991272}]}]}