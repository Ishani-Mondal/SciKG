{"title": [{"text": "Neural Models for Key Phrase Extraction and Question Generation", "labels": [], "entities": [{"text": "Key Phrase Extraction", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.6865862905979156}, {"text": "Question Generation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8128454983234406}]}], "abstractContent": [{"text": "We propose a two-stage neural model to tackle question generation from documents.", "labels": [], "entities": [{"text": "question generation from documents", "start_pos": 46, "end_pos": 80, "type": "TASK", "confidence": 0.8235862106084824}]}, {"text": "First, our model estimates the probability that word sequences in a document are ones that a human would pick when selecting candidate answers by training a neural key-phrase extractor on the answers in a question-answering corpus.", "labels": [], "entities": []}, {"text": "Predicted key phrases then act as target answers and condition a sequence-to-sequence question-generation model with a copy mechanism.", "labels": [], "entities": []}, {"text": "Empirically, our key-phrase extraction model significantly out-performs an entity-tagging baseline and existing rule-based approaches.", "labels": [], "entities": [{"text": "key-phrase extraction", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7701252698898315}]}, {"text": "We further demonstrate that our question generation system formulates fluent, answerable questions from key phrases.", "labels": [], "entities": [{"text": "question generation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7549983561038971}]}, {"text": "This two-stage system could be used to augment or generate reading comprehension datasets, which maybe leveraged to improve machine reading systems or in educational settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering and machine comprehension has gained increased interest in the past few years.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9136278629302979}, {"text": "machine comprehension", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.8039537966251373}]}, {"text": "An important contributing factor is the emergence of several large-scale QA datasets.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.8240667879581451}]}, {"text": "However, the creation of these datasets is a labour-intensive and expensive process that usually comes at significant financial cost.", "labels": [], "entities": []}, {"text": "Meanwhile, given the complexity of the problem space, even the largest QA dataset can still exhibit strong biases in many aspects including question and answer types, domain coverage, linguistic style, etc.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.7118658870458603}]}, {"text": "To address this limitation, we propose and evaluate neural models for automatic question-answer pair generation that involves two inter-related components: first, a system to identify candidate answer entities or events (key phrases) within a passage or document; second, a question generation module to construct questions about a given key phrases.", "labels": [], "entities": [{"text": "question-answer pair generation", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.6562491456667582}, {"text": "question generation", "start_pos": 274, "end_pos": 293, "type": "TASK", "confidence": 0.7195360064506531}]}, {"text": "As a financially more efficient and scalable alternative to the human curation of QA datasets, the resulting system can potentially accelerate further progress in the field.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.7003828585147858}]}, {"text": "Specifically, We formulate the key phrase extraction component as modeling the probability of potential answers conditioned on a given document, i.e., P (a|d).", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7458094954490662}]}, {"text": "Inspired by successful work in question answering, we propose a sequence-tosequence model that generates a set of key-phrase boundaries.", "labels": [], "entities": [{"text": "question answering", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.8835318088531494}]}, {"text": "This model can flexibly select an arbitrary number of key phrases from a document.", "labels": [], "entities": []}, {"text": "To teach it to assign high probability to humanselected answers, we train the model on largescale, crowd-sourced question-answering datasets.", "labels": [], "entities": []}, {"text": "We thus take a purely data-driven approach to understand the priors that humans have when selecting answer candidates, working from the premise that crowdworkers tend to select entities or events that interest them when formulating their own comprehension questions.", "labels": [], "entities": []}, {"text": "If this premise is correct, then the growing collection of crowd-sourced question-answering datasets can be harnessed to learn models for key phrases of interest to human readers.", "labels": [], "entities": []}, {"text": "Given a set of extracted key phrases, we then approach the question generation component by modeling the conditional probability of a question given a document-answer pair, i.e., P (q|a, d).", "labels": [], "entities": [{"text": "question generation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7876836359500885}]}, {"text": "To this end, we use a sequence-to-sequence model with attention () and the pointer-softmax mechanism.", "labels": [], "entities": []}, {"text": "This component is also trained to maximize the likelihood of questions estimated on a QA dataset.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.8104709684848785}]}, {"text": "When training this component, the model sees the ground truth answers from the dataset.", "labels": [], "entities": []}, {"text": "Empirically, our proposed model for key phrase extraction outperforms two baseline systems by a significant margin.", "labels": [], "entities": [{"text": "key phrase extraction", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.6588910818099976}]}, {"text": "We support these quantitative findings with qualitative examples of generated question-answer pairs given documents.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our experiments on the SQuAD (Rajpurkar et al., 2016) and NewsQA () datasets.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.7347612977027893}, {"text": "NewsQA () datasets", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.8795799016952515}]}, {"text": "These are machine comprehension corpora consisting of over 100k crowd-sourced question-answer pairs.", "labels": [], "entities": []}, {"text": "SQuAD contains 536 paragraphs from Wikipedia while NewsQA was created on 12,744 news articles.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8575042486190796}, {"text": "NewsQA", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9410971999168396}]}, {"text": "Simple preprocessing is performed, including lower-casing and word tokenization using NLTK.", "labels": [], "entities": [{"text": "NLTK", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.9169093370437622}]}, {"text": "Since the test split of SQuAD is hidden from the public, we use 5,158 question-answer pairs (self-contained in 23 Wikipedia articles) from the training set for development, and use the official development data to report test results.", "labels": [], "entities": []}, {"text": "Since each key phrase is itself a multi-word unit, we believe that a na\u00a8\u0131vena\u00a8\u0131ve, word-level F1 that considers an entire key phrase as a single unit is not well suited to this evaluation.", "labels": [], "entities": []}, {"text": "We therefore propose an extension of the SQuAD F1 metric (for a single answer span) to multiple spans within a document, which we call the multi-span F1 score.", "labels": [], "entities": [{"text": "SQuAD F1 metric", "start_pos": 41, "end_pos": 56, "type": "METRIC", "confidence": 0.6927282214164734}, {"text": "F1 score", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9165834784507751}]}, {"text": "This metric is calculated as follows.", "labels": [], "entities": []}, {"text": "Given the predicted phras\u00ea e i and a gold phrase e j , we first construct a pairwise, token-level F 1 score matrix of elements f i,j between the two phrases\u00eaphrases\u02c6phrases\u00ea i and e j . Max-pooling along the gold-label axis essentially assesses the precision of each prediction, with partial matches accounted for by the pairwise F1 (identical to evaluation of a single answer in SQuAD) in the cells: pi = max j (f i,j ).", "labels": [], "entities": [{"text": "token-level F 1 score matrix", "start_pos": 86, "end_pos": 114, "type": "METRIC", "confidence": 0.7085421562194825}, {"text": "precision", "start_pos": 249, "end_pos": 258, "type": "METRIC", "confidence": 0.9987820982933044}, {"text": "F1", "start_pos": 330, "end_pos": 332, "type": "METRIC", "confidence": 0.8907568454742432}]}, {"text": "Analogously, the recall for label e j can be computed by max-pooling along the prediction axis: r j = maxi (f i,j ).", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9992621541023254}]}, {"text": "We define the multi-span F1 score using the mean precision \u00af p = avg(p i ) and recall \u00af r = avg(r j ): Doc.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.972292423248291}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.6543248891830444}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9974114298820496}]}, {"text": "inflammation is one of the first responses of the immune system to infection . the symptoms of inflammation are redness , swelling , heat , and pain , which are caused by increased blood flow into tissue . inflammation is produced by eicosanoids and cytokines , which are released by injured or infected cells . eicosanoids include prostaglandins that produce fever and the dilation of blood vessels associated with inflammation , and leukotrienes that attract certain white blood cells ( leukocytes ) . .", "labels": [], "entities": []}, {"text": "Qualitatively, we observe that the entity-based models have a strong bias for numeric types, which often fail to capture interesting information in a document.", "labels": [], "entities": []}, {"text": "We also notice that entitybased systems tend to select the central topical entity as answer, which does not match the distribution of answers typically selected by humans.", "labels": [], "entities": []}, {"text": "For example, given a Wikipedia article on Kenya stating that agriculture is the second largest contributor to kenya 's gross domestic product (gdp), entity-based systems propose kenya as an answer phrase.", "labels": [], "entities": []}, {"text": "This leads to the (low-quality) question what country is nigeria's second largest contributor to?", "labels": [], "entities": []}, {"text": "4 Given the same document, the pointer model picked agriculture as the answer and asked what is the second largest contributor to kenya 's gross domestic product ?  We can quantitatively evaluate our question generation module by conditioning it on gold answers from the SQuAD development set.", "labels": [], "entities": [{"text": "SQuAD development set", "start_pos": 271, "end_pos": 292, "type": "DATASET", "confidence": 0.8924388488133749}]}, {"text": "We can then use standard automatic evaluation metrics for generative models of text such as BLEU.", "labels": [], "entities": [{"text": "generative models of text", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.8916380405426025}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9742610454559326}]}, {"text": "Our question generation model evaluated in such a manner yields 10.4 BLEU 4 . However, there can exist a many possible ways to formulate a question given the same answer.", "labels": [], "entities": [{"text": "question generation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6927888095378876}, {"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9949491024017334}]}, {"text": "BLEU thus becomes a less desirable metric by penalizing any generation that does not closely match (lexically) the reference question.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9797747731208801}]}, {"text": "To address this issue, we propose to evaluate a generated question by employing a pre-trained QA model.", "labels": [], "entities": []}, {"text": "Specifically, suppose question\u02c6qquestion\u02c6 question\u02c6q is generated from document d and answer a, and the pre-trained QA model outputs answer\u00e2answer\u02c6answer\u00e2 given the input d and\u02c6qand\u02c6 and\u02c6q.", "labels": [], "entities": []}, {"text": "If the QA model is assumed to be able to answer the gold question q with the gold answer a, then the F1 score between a and\u00e2and\u02c6and\u00e2 may serve as a proxy to the semantic equivalence between q and\u02c6qand\u02c6 and\u02c6q -regardless of the amount of word/n-gram overlap between q and\u02c6qand\u02c6 and\u02c6q.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9864810705184937}]}, {"text": "Quantitatively, a match-LSTM model ( pre-trained on gold squad question/answer pairs achieves an F1 score of 72.4% on our generated questions in comparison to 73.8% on the SQuAD dev set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9901719689369202}, {"text": "SQuAD dev set", "start_pos": 172, "end_pos": 185, "type": "DATASET", "confidence": 0.8559154073397318}]}, {"text": "In addition to the automatic evaluation metrics, we also undertook a human evaluation of generated questions and answers.", "labels": [], "entities": []}, {"text": "We present several answer-extraction and question-generation examples in.", "labels": [], "entities": []}, {"text": "Each example contains a document and three corresponding QA pairs, generated respectively by H&S, by our two-stage framework, and by the original SQuAD crowdworkers.", "labels": [], "entities": []}, {"text": "We now discuss the relative qualities of QA pairs from each synthetic method.", "labels": [], "entities": []}, {"text": "H&S Key phrases selected by the H&S model are structurally distinct from the PtrNet and human-generated answers.", "labels": [], "entities": [{"text": "PtrNet", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.8958671689033508}]}, {"text": "For example, they may start with prepositions, such as of, by, and to, or consist of very long phrases like that student motivation and attitudes towards school are closely linked to student-teacher relationships.", "labels": [], "entities": []}, {"text": "As seen in, these key phrases may also contain vague phrases such as \"this theory\", \"some studies\", \"a person\", etc., which renders them less natural for question generation.", "labels": [], "entities": [{"text": "question generation", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.7539187073707581}]}, {"text": "The H&S question generator appears to produce a few ungrammatical sentences, e.g., the first time -what was the yuan dynasty that non-native chinese people ruled all of china ? Our system Since our key phrase extractor was trained on SQuAD, the selected key phrases more closely resemble gold SQuAD answers.", "labels": [], "entities": []}, {"text": "However, sometimes the generated questions do not target the extracted answers, eg, eicosanoids and cytokines -what are bacteria produced by ? (first document in).", "labels": [], "entities": []}, {"text": "Interestingly, our model is sometimes able to resolve coreferent entities.", "labels": [], "entities": []}, {"text": "For instance, to generate the mongol empire --the yuan dynasty is considered to be the continuation of what ? the model must resolve the pronoun it to yuan dynasty in it is generally considered to be the continuation of the mongol empire (third document in).", "labels": [], "entities": []}, {"text": "We carried out human evaluations on the question generation module in isolation as well as in conjunction with the key phrase extraction module.", "labels": [], "entities": [{"text": "question generation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7652112245559692}, {"text": "key phrase extraction", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.6864553391933441}]}, {"text": "Evaluating the ability of the Question Generation Module to transfer to new settings We asked crowdworkers part of an internal evaluation system to evaluate two different aspects of questions generated by our module -fluency and correctness.", "labels": [], "entities": []}, {"text": "Our system was provided Internet articles and candidate answers selected from an internal search engine thereby evaluating the model's ability to generalize from simple RC datasets to the real world.", "labels": [], "entities": []}, {"text": "For fluency evaluations, they were asked whether the generated questions sounded natural (ignoring semantics) with scores of 0/1/2 corresponding to \"No\", \"Somewhat\" and \"Yes\".", "labels": [], "entities": []}, {"text": "17.5% were labeled 0, 22.7% were labeled 1 and 59.8% were labeled 2.", "labels": [], "entities": []}, {"text": "For correctness evaluations, annotators were asked if the given answer was the correct answer for the given question.", "labels": [], "entities": []}, {"text": "64.4% of questions were labeled incorrect, leaving 35.6% labeled as correct.", "labels": [], "entities": []}, {"text": "This particular evaluation differs slightly from others with regard to the module used (it was trained a combination of SQuAD + NewsQA + TriviaQA ().", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 128, "end_pos": 134, "type": "DATASET", "confidence": 0.7288771271705627}]}, {"text": "Also the documents and answers used provided via an internal tool.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Model evaluation on key phrase extrac- tion", "labels": [], "entities": []}]}