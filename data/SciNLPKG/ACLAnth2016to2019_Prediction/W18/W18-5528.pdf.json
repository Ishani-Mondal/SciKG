{"title": [{"text": "A Mostly Unlexicalized Model For Recognizing Textual Entailment", "labels": [], "entities": [{"text": "Recognizing Textual Entailment", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8764845331509908}]}], "abstractContent": [{"text": "Many approaches to automatically recognizing entailment relations have employed classi-fiers overhand engineered lexicalized features, or deep learning models that implicitly capture lexicalization through word embeddings.", "labels": [], "entities": [{"text": "automatically recognizing entailment relations", "start_pos": 19, "end_pos": 65, "type": "TASK", "confidence": 0.6902022510766983}]}, {"text": "This reliance on lexicalization may complicate the adaptation of these tools between domains.", "labels": [], "entities": []}, {"text": "For example, such a system trained in the news domain may learn that a sentence like \"Pales-tinians recognize Texas as part of Mexico\" tends to be unsupported, but this fact (and its corresponding lexicalized cues) have no value in, say, a scientific domain.", "labels": [], "entities": []}, {"text": "To mitigate this dependence on lexicalized information, in this paper we propose a model that reads two sentences , from any given domain, to determine entailment without using lexicalized features.", "labels": [], "entities": []}, {"text": "Instead our model relies on features that are either unlexicalized or are domain independent such as proportion of negated verbs, antonyms, or noun overlap.", "labels": [], "entities": []}, {"text": "In its current implementation , this model does not perform well on the FEVER dataset, due to two reasons.", "labels": [], "entities": [{"text": "FEVER dataset", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.8499931991100311}]}, {"text": "First, for the information retrieval portion of the task we used the baseline system provided, since this was not the aim of our project.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.848694771528244}]}, {"text": "Second, this is work in progress and we still are in the process of identifying more features and gradually increasing the accuracy of our model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9994850158691406}]}, {"text": "In the end, we hope to build a generic end-to-end classi-fier, which can be used in a domain outside the one in which it was trained, with no or minimal retraining .", "labels": [], "entities": []}], "introductionContent": [{"text": "The rampant spread of fake data recently (be it in news or scientific facts) and its impact in our day today life has renewed interest in the topic of disambiguating between fake, or unsupported, information and real, or supported, information.", "labels": [], "entities": []}, {"text": "Last year, the fake news challenge () was organized as a valuable first step towards creating systems to detect inaccurate claims.", "labels": [], "entities": []}, {"text": "This year the fact verification challenge (FEVER) () was organized to further this.", "labels": [], "entities": [{"text": "fact verification challenge", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.8248611887296041}, {"text": "FEVER", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9340572953224182}]}, {"text": "Specifically it was organized to foster the development of systems that combine information retrieval (IR) and textual entailment recognition (RTE) together to address the fake claim problem.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 80, "end_pos": 106, "type": "TASK", "confidence": 0.7415942907333374}, {"text": "textual entailment recognition (RTE)", "start_pos": 111, "end_pos": 147, "type": "TASK", "confidence": 0.8231898794571558}]}, {"text": "However, developing a system that is trained to tackle the issue only in one area (in this case for fake news detection) does not solve the problem in other domains.", "labels": [], "entities": [{"text": "fake news detection", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.653310110171636}]}, {"text": "For example, models developed to specifically detect fake news might notwork well to detect fake science articles.", "labels": [], "entities": []}, {"text": "An alternative will be to create systems that can be trained in broader domains and can then be used to test on specific domains with minimal modification and/or parameter tuning.", "labels": [], "entities": []}, {"text": "Such a system should also be able to capture the underlying idiosyncratic characteristics of the human author who originally created such fake data.", "labels": [], "entities": []}, {"text": "For example some common techniques used by writers of such fake articles include using hedging words (e.g., possibly), circumventing facts, avoiding mentioning direct evidence, hyperbole etc.", "labels": [], "entities": []}, {"text": "To this end, we propose a largely unlexicalized approach that, when coupled with an information retrieval (IR) system that assembles relevant articles fora given claim, would serve as a crossdomain fake-data detection tool which could either stand-alone or potentially supplement other domain-specific lexicalized systems.", "labels": [], "entities": [{"text": "crossdomain fake-data detection", "start_pos": 186, "end_pos": 217, "type": "TASK", "confidence": 0.6487321158250173}]}, {"text": "The goal of this paper is to present a description of such a preliminary system developed for the FEVER challenge, its performance, and our intended future work.", "labels": [], "entities": [{"text": "FEVER challenge", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.43900351226329803}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of our submitted model on the  test data.", "labels": [], "entities": []}, {"text": " Table 2: Oracle classification on claims in the develop- ment set using gold sentences as evidence", "labels": [], "entities": [{"text": "Oracle classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9391363859176636}, {"text": "develop- ment set", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.6260963976383209}]}, {"text": " Table 4: Top five features with the highest weight in each class, where the positive class is REFUTES and the  negative class is SUPPORTS.", "labels": [], "entities": [{"text": "REFUTES", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9941313862800598}, {"text": "SUPPORTS", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.6056839227676392}]}]}