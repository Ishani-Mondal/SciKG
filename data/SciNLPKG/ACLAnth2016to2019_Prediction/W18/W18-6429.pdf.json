{"title": [{"text": "Tencent Neural Machine Translation Systems for WMT18", "labels": [], "entities": [{"text": "Tencent Neural Machine Translation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5856242552399635}, {"text": "WMT18", "start_pos": 47, "end_pos": 52, "type": "TASK", "confidence": 0.7086104154586792}]}], "abstractContent": [{"text": "We participated in the WMT 2018 shared news translation task on English\u2194Chinese language pair.", "labels": [], "entities": [{"text": "WMT 2018 shared news translation task", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.8055875798066457}]}, {"text": "Our systems are based on attentional sequence-to-sequence models with some form of recursion and self-attention.", "labels": [], "entities": []}, {"text": "Some data augmentation methods are also introduced to improve the translation performance.", "labels": [], "entities": []}, {"text": "The best translation result is obtained with ensemble and reranking techniques.", "labels": [], "entities": [{"text": "translation", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9602252840995789}]}, {"text": "Our Chinese\u2192English system achieved the highest cased BLEU score among all 16 submitted systems, and our English\u2192Chinese system ranked the third out of 18 submitted systems.", "labels": [], "entities": [{"text": "cased", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9839602112770081}, {"text": "BLEU score", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9238637685775757}]}], "introductionContent": [{"text": "In recent years, the emergence of seq2seq models has revolutionized the field of MT by replacing traditional phrase-based approaches with neural machine translation (NMT) systems based on the encoder-decoder paradigm.", "labels": [], "entities": [{"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.99394291639328}]}, {"text": "A successful extension of encoder-decoder models is the attention mechanism which conducts a soft search over source tokens and yields an attentive vector to represent the most relevant segments of the source sentence for the current decoding state ().", "labels": [], "entities": []}, {"text": "Most recently, the Transformer model, which is based solely on a self-attention mechanism and feedforward connections, has further advanced the field of NMT, both in terms of translation quality and speed of convergence(.", "labels": [], "entities": []}, {"text": "In this paper, we describe the Tencent NMT (TNMT) systems submissions for the WMT 2018 Chinese\u2192English and English\u2192Chinese translation task.", "labels": [], "entities": [{"text": "WMT 2018 Chinese\u2192English and English\u2192Chinese translation task", "start_pos": 78, "end_pos": 139, "type": "TASK", "confidence": 0.6044419489123605}]}, {"text": "We propose two different architectures as our end to end approaches namely RNMT and Transformer.", "labels": [], "entities": []}, {"text": "For RNMT, we implemented a hybrid multi-layer attention-based encoder-decoder model.", "labels": [], "entities": []}, {"text": "The decoder was implemented as Recurrent Neural Networks (RNNs) and the encoder was represented with self-attention layers.", "labels": [], "entities": []}, {"text": "We also integrated with some recent promising techniques in RNMT including the methods which made significantly contribution to the success of Transformer.", "labels": [], "entities": [{"text": "RNMT", "start_pos": 60, "end_pos": 64, "type": "TASK", "confidence": 0.8581127524375916}, {"text": "Transformer", "start_pos": 143, "end_pos": 154, "type": "TASK", "confidence": 0.8101809024810791}]}, {"text": "In doing so, we come up with an enhanced version of RNMT that achieves comparable performance with Transformer.", "labels": [], "entities": [{"text": "RNMT", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.8311375379562378}, {"text": "Transformer", "start_pos": 99, "end_pos": 110, "type": "DATASET", "confidence": 0.8430757522583008}]}, {"text": "For Transformer, we follow the latest version of the Transformer model in the public Tensor2Tensor 1 codebase.", "labels": [], "entities": [{"text": "Tensor2Tensor 1 codebase", "start_pos": 85, "end_pos": 109, "type": "DATASET", "confidence": 0.6727005243301392}]}, {"text": "The Transformer model replaces the recurrent connections with self-attention which can betaken as a complement with the RNMT model.", "labels": [], "entities": []}, {"text": "For data augmentation, we used automatic back-translation of a sub-selected monolingual News corpus as additional training data(.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7624502182006836}]}, {"text": "To achieve strong machine translation performance, we further leverage the joint training method described in () to optimize both the target-to-source (T2S) and source-to-target (S2T) model by extending the back-translation method.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7739221751689911}]}, {"text": "The joint training method uses both the monolingual and bilingual data and updates NMT models through several iterations.", "labels": [], "entities": []}, {"text": "We also apply several knowledge distillation methods to leverage the information gain of different architectures.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7347478270530701}]}, {"text": "To alleviate the exposure bias problem of the left-to-right (L2R) model, Agreement Regularization was introduced as a teacher network (.", "labels": [], "entities": [{"text": "Agreement Regularization", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.7926077246665955}]}, {"text": "Ensemble teacher networks and architecture teacher networks are also introduced to boost the performance of a single model.", "labels": [], "entities": []}, {"text": "In addition, we consider the system combination and improve the performance by reranking () the n-best translation outputs of the ensemble models with some effective features, including the target-to-source (T2S) score, left-to-right (L2R) score, right-toleft (R2L) score, Transformer score and RNMT score.", "labels": [], "entities": [{"text": "RNMT score", "start_pos": 295, "end_pos": 305, "type": "METRIC", "confidence": 0.8145568072795868}]}, {"text": "The ensemble models are trained with different architectures or parameter settings to increase the diversity of the system.", "labels": [], "entities": []}, {"text": "As a result, our Chinese\u2192English system achieved the highest cased BLEU score among all 16 submitted systems, and our English\u2192Chinese system ranked the third out of 18 submitted systems.", "labels": [], "entities": [{"text": "cased", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9829943180084229}, {"text": "BLEU score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9228484034538269}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: English\u2192Chinese Systems BLEU results on de-", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.993621289730072}]}]}