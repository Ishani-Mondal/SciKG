{"title": [{"text": "The Benefit of Pseudo-Reference Translations in Quality Estimation of MT Output", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, a novel approach to Quality Estimation is introduced, which extends the method in (Duma and Menzel, 2017) by also considering pseudo-reference translations as data sources to the tree and sequence kernels used before.", "labels": [], "entities": [{"text": "Quality Estimation", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.6920384764671326}]}, {"text": "Two variants of the system were submitted to the sentence level WMT18 Quality Estimation Task for the English-German language pair.", "labels": [], "entities": [{"text": "WMT18 Quality Estimation", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.3977891206741333}]}, {"text": "They have been ranked 4th and 6th out of 13 systems in the SMT track, while in the NMT track ranks 4 and 5 out of 11 submissions have been reached.", "labels": [], "entities": [{"text": "SMT track", "start_pos": 59, "end_pos": 68, "type": "DATASET", "confidence": 0.8468556106090546}, {"text": "NMT track", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.9704969227313995}]}], "introductionContent": [{"text": "The purpose of Quality Estimation (QE), as a subfield of Machine Translation (MT), is to allow the evaluation of MT output without the necessity of providing a reference translation.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7184857904911042}, {"text": "Machine Translation (MT)", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.8415231704711914}]}, {"text": "This would be extremely beneficial in the development cycle of a MT system, as it would permit fast and cost efficient evaluation phases.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9902979731559753}]}, {"text": "In the case of the previous Quality Estimation Shared Task () together with the current campaign), the purpose for the sentence level track was to predict the effort required in order to post-edit a candidate translation as measured by the Human-mediated Translation Edit Rate (HTER)) score.", "labels": [], "entities": [{"text": "Human-mediated Translation Edit Rate (HTER)) score", "start_pos": 240, "end_pos": 290, "type": "METRIC", "confidence": 0.7123578004539013}]}, {"text": "In this paper an extension of the QE method introduced in) is presented.", "labels": [], "entities": []}, {"text": "Our earlier version of the metric was based on learning HTER scores using tree and sequence kernels.", "labels": [], "entities": []}, {"text": "The kernel functions were applied not only on the source segments and the candidate translations, but also on the back-translations of the MT output into the source language.", "labels": [], "entities": []}, {"text": "The back-translations were obtained using an online MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9649613499641418}]}, {"text": "The extension proposed in this paper uses the same input data.", "labels": [], "entities": []}, {"text": "In addition, however, the kernel functions are defined to also consider pseudoreferences as an additional source of evidence.", "labels": [], "entities": []}, {"text": "The pseudo-references represent translations of the source segments into the target language and were obtained using the same online MT system as for the back-translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 133, "end_pos": 135, "type": "TASK", "confidence": 0.9106224775314331}]}, {"text": "By applying both the sequence and the tree kernels on the pseudoreferences, we wanted to determine if an additional data source, even if artificially generated, would have a positive impact on our previous QE method.", "labels": [], "entities": []}, {"text": "Throughout the rest of the paper we will refer to both the newly developed QE method as well as to its earlier version as Tree and Sequence Kernel Quality Estimation (TSKQE), but the variant under consideration will be marked through the use of subscripts together with superscripts.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 related work is presented, focusing on kernel based QE methods.", "labels": [], "entities": []}, {"text": "In the next section the implementation details for TSKQE are presented.", "labels": [], "entities": [{"text": "TSKQE", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.4908693730831146}]}, {"text": "This is followed by the evaluation setup and a discussion of the results.", "labels": [], "entities": []}, {"text": "The paper concludes with future work ideas and final remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation was performed measuring the correlation between the TSKQE scores and the HTER gold standards.", "labels": [], "entities": [{"text": "TSKQE scores", "start_pos": 67, "end_pos": 79, "type": "METRIC", "confidence": 0.7219026386737823}, {"text": "HTER gold standards", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.7998513579368591}]}, {"text": "This was achieved by computing the Pearson correlation coefficient, which results in a number between -1 and 1.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 35, "end_pos": 66, "type": "METRIC", "confidence": 0.8538960615793864}]}, {"text": "A score of 1 indicates that there is a perfect agreement between the two sets of scores, while a score of -1 would suggest a negative agreement.", "labels": [], "entities": []}, {"text": "In addition to the Pearson coefficient, the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE) were also calculated.", "labels": [], "entities": [{"text": "Pearson coefficient", "start_pos": 19, "end_pos": 38, "type": "METRIC", "confidence": 0.9619998931884766}, {"text": "Mean Absolute Error (MAE)", "start_pos": 44, "end_pos": 69, "type": "METRIC", "confidence": 0.9533369143803915}, {"text": "Root Mean Squared Error (RMSE)", "start_pos": 78, "end_pos": 108, "type": "METRIC", "confidence": 0.882475027016231}]}, {"text": "For both these evaluation methods, the closer their score is to 0, the better the QE system should be considered.", "labels": [], "entities": [{"text": "QE", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9659833312034607}]}, {"text": "The significance testing of the results was performed using the methodology presented in, which is based on pairwise testing using the Williams test In terms of the data sets, TSKQE was evaluated on the English-German datasets () provided by the WMT18 Quality Estimation sentence level task.", "labels": [], "entities": [{"text": "TSKQE", "start_pos": 176, "end_pos": 181, "type": "METRIC", "confidence": 0.8361465930938721}, {"text": "WMT18 Quality Estimation sentence level task", "start_pos": 246, "end_pos": 290, "type": "DATASET", "confidence": 0.7507862051328024}]}, {"text": "In contrast to the years before, the campaign offered two tracks for this language pair: in addition to the traditional one focused on SMT systems, another one considered the evaluation of an NMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.9892988204956055}]}, {"text": "Both tracks used translations from the IT domain, with the data consisting of tuples made up of the source segment, the candidate translation, the reference translation and the HTER score associated to that candidate translation.", "labels": [], "entities": [{"text": "HTER score", "start_pos": 177, "end_pos": 187, "type": "METRIC", "confidence": 0.9428705275058746}]}, {"text": "For the NMT system, 13,442 tuples were made available for the training, with an additional 1,000 tuples provided for development purposes.", "labels": [], "entities": []}, {"text": "In the case of the SMT system, the training set was larger, consisting of 26,273 instances, with the same number of 1000 tuples made available for evaluation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9919718503952026}]}, {"text": "We compared the performance of TSKQE with a weak but also with a strong baseline.", "labels": [], "entities": [{"text": "TSKQE", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.5708431601524353}]}, {"text": "The former is represented by the QE system trained only on the 17 baseline features offered by the WMT18 QE campaign organizers.", "labels": [], "entities": [{"text": "WMT18 QE campaign organizers", "start_pos": 99, "end_pos": 127, "type": "DATASET", "confidence": 0.8900831043720245}]}, {"text": "The features have been regularly used over the past campaigns and include, for example, the number of tokens in the source sentence or the LM probability of the target sentence.", "labels": [], "entities": [{"text": "LM probability", "start_pos": 139, "end_pos": 153, "type": "METRIC", "confidence": 0.8154221475124359}]}, {"text": "We used these baseline features not only to build the baseline system, but also integrated them into TSKQE by means of a Radial Basis Function (RBF) kernel.", "labels": [], "entities": [{"text": "TSKQE", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.7291252017021179}]}, {"text": "For this purpose, we applied a Z-score standardization to rescale the feature values.", "labels": [], "entities": []}, {"text": "For the strong baseline, we considered a variant of one of the QE systems introduced by), based on Partial Tree Kernels applied to the source segments and candidate translations.", "labels": [], "entities": []}, {"text": "In our notation, this would correspond to the TSKQE ptk basic notation.", "labels": [], "entities": [{"text": "TSKQE ptk basic notation", "start_pos": 46, "end_pos": 70, "type": "DATASET", "confidence": 0.634785495698452}]}, {"text": "The results of the evaluation for both the NMT and the SMT tracks are presented in.", "labels": [], "entities": [{"text": "NMT", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.8636016249656677}, {"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9789916276931763}]}, {"text": "We highlighted in bold the highest Pearson values.", "labels": [], "entities": [{"text": "Pearson values", "start_pos": 35, "end_pos": 49, "type": "METRIC", "confidence": 0.9619568884372711}]}, {"text": "Furthermore, we marked using an asterisk the two variants which we have chosen as our submissions   0.001  to the WMT18 QE sentence level task.", "labels": [], "entities": [{"text": "WMT18 QE sentence level task", "start_pos": 114, "end_pos": 142, "type": "TASK", "confidence": 0.6647156834602356}]}, {"text": "The results of the significance tests for two sets of TSKQE models are displayed in.", "labels": [], "entities": []}, {"text": "Here, each table can be read as a matrix, where both the rows and columns correspond to the different TSKQE systems.", "labels": [], "entities": []}, {"text": "The significance testing was performed only for the pairs of systems where the column model achieved a higher Pearson correlation than the row model.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 110, "end_pos": 129, "type": "METRIC", "confidence": 0.9755382835865021}]}, {"text": "Otherwise, the cell was marked with a hyphen sign.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The results of the evaluation for the different TSKQE models.", "labels": [], "entities": []}, {"text": " Table 2: Significance Williams test results.", "labels": [], "entities": [{"text": "Significance Williams test", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.6570005416870117}]}]}