{"title": [{"text": "A Methodology for Evaluating Interaction Strategies of Task-Oriented Conversational Agents", "labels": [], "entities": [{"text": "Evaluating Interaction Strategies of Task-Oriented Conversational Agents", "start_pos": 18, "end_pos": 90, "type": "TASK", "confidence": 0.7494029232433864}]}], "abstractContent": [{"text": "In task-oriented conversational agents, more attention has been usually devoted to assessing task effectiveness, rather than to how the task is achieved.", "labels": [], "entities": []}, {"text": "However, conversational agents are moving towards more complex and human-like interaction capabilities (e.g. the ability to use a formal/informal register, to show an empathetic behavior), for which standard evaluation methodologies may not suffice.", "labels": [], "entities": []}, {"text": "In this paper, we provide a novel methodology to assess-in a completely controlled way-the impact on the quality of experience of agent's interaction strategies.", "labels": [], "entities": []}, {"text": "The methodology is based on a within subject design , where two slightly different transcripts of the same interaction with a conversational agent are presented to the user.", "labels": [], "entities": []}, {"text": "Through a series of pilot experiments we prove that this methodology allows fast and cheap experi-mentation/evaluation, focusing on aspects that are overlooked by current methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "The evaluation of task-oriented conversational agents is usually focused on measuring their effectiveness, either at the single turn level -see for example) -or at the level of the whole interaction -e.g success rate).", "labels": [], "entities": []}, {"text": "Still, as conversational agents are becoming more complex and human-like), these evaluation methodologies may not suffice.", "labels": [], "entities": []}, {"text": "In this paper, we present a framework for evaluating interaction strategies of conversational agents during their development phase.", "labels": [], "entities": []}, {"text": "Our approach combines in a novel way methodologies already tested and validated, and is based on Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, 978-1-948087-75-9 a pairwise comparison of manually curated transcripts of possible interactions.", "labels": [], "entities": [{"text": "EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI", "start_pos": 121, "end_pos": 209, "type": "TASK", "confidence": 0.8568247854709625}]}, {"text": "On the one hand, our methodology is inspired by the Human-Computer-Interaction (HCI) literature by dividing the evaluation of a system in the Quality of Service (QoS) and Quality of Experience (QoE) dimensions).", "labels": [], "entities": []}, {"text": "The former corresponds to the efficiency of the system, while the latter refers to the way in which the system accomplishes the task.", "labels": [], "entities": []}, {"text": "In dialogue systems evaluation the traditional focus is on the QoS, while in this work we deal also with the QoE.", "labels": [], "entities": [{"text": "dialogue systems evaluation", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7214158773422241}]}, {"text": "On the other hand, we take advantage of crowdsourcing methodologies, a fast and cheap way we use to evaluate interactions while maintaining complete control over experimental conditions -by using a design similar to A/B testing, but in a 'within subject' condition.", "labels": [], "entities": []}, {"text": "In this setting two slightly different versions of the same interaction with a conversational agent are presented to the user fora pairwise comparison (e.g. the same interaction using a formal/informal register).", "labels": [], "entities": []}, {"text": "Unlike standard Wizard of Oz (WoZ) or lab experiments, the user does not directly interact with the system, rather s/he reads the manually curated transcript, so to eliminate confounding variables and make data collection much faster.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: in Section 2 we discuss some of the main approaches used in the evaluation of conversational agents.", "labels": [], "entities": []}, {"text": "In Section 3 and 4 we present our framework and provide some pilot experiments respectively.", "labels": [], "entities": []}, {"text": "Finally in Section 5 we discuss the advantages of the approach in light of the results of the experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe a showcase experiment for our methodology, where we evaluated 5 possible variants of CH1, a conversational agent that we implemented in order to calculate the carbohydrates of user's meals.", "labels": [], "entities": []}, {"text": "We setup a two variants testing for each independent variable, where we provided to the subjects of the experiment the transcripts of some conversations between a human user and CH1.", "labels": [], "entities": []}, {"text": "Before starting the experiment, the user received a short text describing the task.", "labels": [], "entities": []}, {"text": "In this section we describe the main characteristics of our evaluation experiment.", "labels": [], "entities": []}, {"text": "Subjects: 143 subjects from the US were recruited using the CrowdFlower platform: 93 male and 50 female.", "labels": [], "entities": []}, {"text": "36 were between 18-24 years old, 58 were between 25-34 years old, 31 were between 35-49 years old, 18 were 50 or more aged.", "labels": [], "entities": []}, {"text": "Design: The design was completely withinsubject, i.e. each subject was presented with one of the control and experimental transcripts for the 5 variables.", "labels": [], "entities": []}, {"text": "Transcripts order among variables and between control/experimental conditions was randomized in order to avoid any framing effect or stimulus order effect (.", "labels": [], "entities": []}, {"text": "Quality control: all subjects were level 3 contributors (maximum expertise/reliability) and a minimum of 3 minutes was set to accept the responses to the questionnaire.", "labels": [], "entities": []}, {"text": "No \"gold-standard\" item was used to evaluate rater reliability, as the two former controls proved to be enough for our case, as found in post hoc analysis.: Ratio of subjects that preferred the experimental over the control condition.", "labels": [], "entities": [{"text": "rater reliability", "start_pos": 45, "end_pos": 62, "type": "METRIC", "confidence": 0.7477191984653473}]}, {"text": "Judgments collected: the total number of judgments collected is 2860: 143 subjects that answered four questions for each of the 5 independent variables.", "labels": [], "entities": []}, {"text": "Cost: Overall, the experiment cost was 51.48$ resulting in a cost of roughly 10$ for evaluating each variable.", "labels": [], "entities": []}, {"text": "The duration of the experiment was about 12 hours.", "labels": [], "entities": []}, {"text": "As aside note, the experiment got a high feedback in terms of contributor satisfaction (an overall evaluation of 4.8/5).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Control (on the left) and experimental (on the right) transcript for the empathy independent variable.  Portions of CH1 utterances that were changed in order to realize the variable are in bold.", "labels": [], "entities": []}, {"text": " Table 2: Ratio of subjects that preferred the experimen- tal over the control condition.", "labels": [], "entities": [{"text": "Ratio", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9672608971595764}]}]}