{"title": [{"text": "CLUF: a Neural Model for Second Language Acquisition Modeling", "labels": [], "entities": [{"text": "Second Language Acquisition Modeling", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.6859124228358269}]}], "abstractContent": [{"text": "Second Language Acquisition Modeling is the task to predict whether a second language learner would respond correctly in future exercises based on their learning history.", "labels": [], "entities": [{"text": "Second Language Acquisition Modeling", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7318774163722992}]}, {"text": "In this paper, we propose a neural network based system to utilize rich contextual, linguistic and user information.", "labels": [], "entities": []}, {"text": "Our neural model consists of a Context encoder, a Linguistic feature encoder, a User information encoder and a Format information encoder (CLUF).", "labels": [], "entities": []}, {"text": "Furthermore , a decoder is introduced to combine such encoded features and make final predictions.", "labels": [], "entities": []}, {"text": "Our system ranked in first place in the English track and second place in the Span-ish and French track with an AUROC score of 0.861, 0.835 and 0.854 respectively.", "labels": [], "entities": [{"text": "English track", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.9285882711410522}, {"text": "Span-ish", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9170953631401062}, {"text": "French track", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.8581695854663849}, {"text": "AUROC score", "start_pos": 112, "end_pos": 123, "type": "METRIC", "confidence": 0.7881990671157837}]}], "introductionContent": [{"text": "Education systems that can adapt to the presenting of educational materials according to students' personal learning needs have great potential.", "labels": [], "entities": []}, {"text": "Specifically, in the area of second language learning, we try to predict whether the learning materials are too easy or too hard for language learners.", "labels": [], "entities": [{"text": "second language learning", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.64201620221138}]}, {"text": "Therefore, we study the Second Language Acquisition Modeling (SLAM) task to build a model of the language learning process.", "labels": [], "entities": [{"text": "Second Language Acquisition Modeling (SLAM) task", "start_pos": 24, "end_pos": 72, "type": "TASK", "confidence": 0.7980524897575378}]}, {"text": "Bayesian Knowledge Tracing (BKT)) that models students' knowledge overtime is a well-established problem.", "labels": [], "entities": [{"text": "Bayesian Knowledge Tracing (BKT))", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7100652158260345}]}, {"text": "It takes a Hidden Markov Model (HMM) with binary hidden states to represent knowledge acquisition for each concept separately.", "labels": [], "entities": []}, {"text": "BKT had been successfully applied to subjects like mathematics and programming, where a limited number of concepts can be predefined.", "labels": [], "entities": [{"text": "BKT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8367381691932678}]}, {"text": "However, in language learning, it's difficult to define a small number of concepts, especially when the vocabulary size increases overtime.", "labels": [], "entities": []}, {"text": "Deep Knowledge Tracing (DKT)) is a recent implementation of knowledge tracing which uses Recurrent Neural Networks (RNNs) to model student's learning trace.", "labels": [], "entities": [{"text": "Deep Knowledge Tracing (DKT))", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7341337005297343}, {"text": "knowledge tracing", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7188697010278702}]}, {"text": "Although RNNs and its commonly used variants, such as Gated Recurrent Units ( and Long Short-Term Memory (LSTM), are capable of exploring dynamic temporal behavior fora time sequence, it's hard to model extremely long learning history that can range over months even years.) is a novel approach for the SLAM task, which combines a psycholinguistic model of human memory with modern machine learning techniques.", "labels": [], "entities": [{"text": "SLAM task", "start_pos": 303, "end_pos": 312, "type": "TASK", "confidence": 0.9425204694271088}]}, {"text": "It had demonstrated state-ofart performance for predicting student recall rates.", "labels": [], "entities": [{"text": "predicting student", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8324095010757446}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.8797156810760498}]}, {"text": "Mapping symbols, such as characters or words, into a continuous space is a popular method in natural language processing).", "labels": [], "entities": []}, {"text": "It achieved remarkable success in many tasks, for example, neural language modeling (, machine translation (), text classification (, sentiment analysis (dos) and machine reading comprehension (.", "labels": [], "entities": [{"text": "neural language modeling", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.6472273071606954}, {"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7745518088340759}, {"text": "text classification", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.840558648109436}, {"text": "sentiment analysis", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.8316621780395508}]}, {"text": "In this work, we introduce a similar neural approach for the SLAM task, where we use neural encoders to extract features from each exercise as well as metadata about student and session.", "labels": [], "entities": [{"text": "SLAM task", "start_pos": 61, "end_pos": 70, "type": "TASK", "confidence": 0.940551370382309}]}, {"text": "To be specific, we build a Context encoder, a Linguistic feature encoder, a User information encoder and a Format information encoder (CLUF) to calculate high-level representations from characters, words, part-of-speech (POS) labels, syntactic dependency labels, user id and country, exercise type, client, etc.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Duolingo SLAM dataset () is organized into three language tracks: \u2022 en es: English learners (who already speak Spanish) \u2022 es en: Spanish learners (who already speak English) \u2022 fr en: French learners (who already speak English) According to, most tokens (more than 80%) are perfect matches and are given the label 0 for \"OK\".", "labels": [], "entities": [{"text": "Duolingo SLAM dataset", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.807589570681254}]}, {"text": "Tokens that are missing or spelled incorrectly (ignoring capitalization, punctuation, and accents) are given the label 1 denoting a mistake.", "labels": [], "entities": []}, {"text": "Across the three language tracks, en es has the lowest positive ratio, while es en has the highest out-of-vocabulary (OOV) ratio.", "labels": [], "entities": [{"text": "out-of-vocabulary (OOV) ratio", "start_pos": 99, "end_pos": 128, "type": "METRIC", "confidence": 0.8490129470825195}]}, {"text": "shows the features provided with the SLAM dataset.", "labels": [], "entities": [{"text": "SLAM dataset", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.7887360751628876}]}, {"text": "In our system, we used all features except the morphology features and syntactic dependency edges, as we did not get any improvement during experiments.", "labels": [], "entities": []}, {"text": "Perhaps it is because that the neural networks already encoded similar information from characters, words and their syntactic dependency labels.", "labels": [], "entities": []}, {"text": "We considered the words that appear less than five times in the training data as unknown token.", "labels": [], "entities": []}, {"text": "For students with more than one nationality, only the first one was used.", "labels": [], "entities": []}, {"text": "The embedding size was set to 100, and the Dropout () regularization was applied, where the dropout rate was set to 0.5.", "labels": [], "entities": [{"text": "Dropout () regularization", "start_pos": 43, "end_pos": 68, "type": "METRIC", "confidence": 0.6633307536443075}]}, {"text": "We used the Adam optimization algorithm) with a learning rate of 0.001.", "labels": [], "entities": []}, {"text": "The word level context encoder was a twolayer Bidirectional LSTM.", "labels": [], "entities": []}, {"text": "The character level context encoder had one LSTM layer for encoding each word and three Bidirectional LSTM layers above the MoT layer.", "labels": [], "entities": []}, {"text": "Furthermore, the linguistic   The relative improvement over the baseline encoder was a two-layer LSTM.", "labels": [], "entities": []}, {"text": "Both of the user encoder and format encoder were one-layer fullyconnected feedforward networks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The SLAM dataset statistics", "labels": [], "entities": [{"text": "SLAM dataset", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.7456570118665695}]}, {"text": " Table 3: Results of the en es track.", "labels": [], "entities": []}, {"text": " Table 4: Results of the es en track.", "labels": [], "entities": []}, {"text": " Table 5: Results of the fr en track.", "labels": [], "entities": [{"text": "fr en track", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.7148552735646566}]}, {"text": " Table 7: Encoder analysis. LUF has no context  encoder; CUF has no linguistic encoder; CLF has  no user encoder; CLU is the model without format  encoder.", "labels": [], "entities": [{"text": "Encoder analysis", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7925178110599518}, {"text": "CUF", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.9356504678726196}]}]}