{"title": [{"text": "GKR: the Graphical Knowledge Representation for semantic parsing", "labels": [], "entities": [{"text": "GKR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8013201355934143}, {"text": "Graphical Knowledge Representation", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.6292577385902405}, {"text": "semantic parsing", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7758752107620239}]}], "abstractContent": [{"text": "This paper describes the first version of an open-source semantic parser that creates graphical representations of sentences to be used for further semantic processing, e.g. for natural language inference, reasoning and semantic similarity.", "labels": [], "entities": []}, {"text": "The Graphical Knowledge Representation which is output by the parser is inspired by the Abstract Knowledge Representation , which separates out conceptual and contextual levels of representation that deal respectively with the subject matter of a sentence and its existential commitments.", "labels": [], "entities": []}, {"text": "Our representation is a layered graph with each sub-graph holding different kinds of information, including one sub-graph for concepts and one for contexts.", "labels": [], "entities": []}, {"text": "Our first evaluation of the system shows an F-score of 85% in accurately representing sentences as semantic graphs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9994404911994934}]}], "introductionContent": [{"text": "Semantic parsing to construct graphical meaning representations is an active topic at the moment ().", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8200817108154297}]}, {"text": "It is not without its critics, however.", "labels": [], "entities": []}, {"text": "object to the conflation of sentence meaning with speaker meaning, inherent in trying to use annotations to learn a direct mapping from sentences onto highly domain specific meaning representations. and have also questioned the expressive power of Abstract Meaning Representation (AMR) (), one of the most popular graphical meaning representations.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 248, "end_pos": 285, "type": "TASK", "confidence": 0.7587413241465887}]}, {"text": "We believe that both lines of criticism are wellfounded, but that there is still value in parsing to produce graphical representations.", "labels": [], "entities": []}, {"text": "This paper describes the first version of an open source semantic parser that creates graphical representations that are inspired by those produced by the proprietary system described in.", "labels": [], "entities": []}, {"text": "Salient features of the system are: \u2022 It uses the enhanced dependencies of the Stanford Neural Universal Dependency parser) to create dependency graphs, on top of which fuller semantic graphs are constructed.", "labels": [], "entities": []}, {"text": "\u2022 Interaction between different sub-graphs is used to account for phenomena like Booleans (negation, disjunction), modals and irrealis contexts, distributivity and quantifier scope, co-reference, and sense selection.", "labels": [], "entities": []}, {"text": "\u2022 Though oriented to using formal ontologies to support a Natural Logic) style of Natural Language Inference (NLI), it also supports the somewhat different task of measuring semantic similarity.", "labels": [], "entities": []}, {"text": "\u2022 More philosophically, we view our graphs as first-class semantic objects that should be directly manipulated in reasoning and other forms of semantic processing.", "labels": [], "entities": []}, {"text": "We do not see them as just a prettier way of writing down formulas in first-or higher-order logic.", "labels": [], "entities": []}, {"text": "In the next section we briefly describe the precursors and motivations behind our approach.", "labels": [], "entities": []}, {"text": "In section 3 we present the Graphical Knowledge Representation (GKR) and how it is constructed.", "labels": [], "entities": [{"text": "Graphical Knowledge Representation (GKR)", "start_pos": 28, "end_pos": 68, "type": "TASK", "confidence": 0.7248167594273885}]}, {"text": "Section 4 evaluates the current parsing into GKR, while section 5 discusses our future additions to the system.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9687449932098389}, {"text": "GKR", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8922995328903198}]}, {"text": "In section 6 we compare GKR to other similar representations and parsers.", "labels": [], "entities": []}, {"text": "In the last section we offer our conclusions and point to a companion paper discussing named graphs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We would like to evaluate our semantic parser to see how many phenomena can already be accurately represented and what should still be improved or implemented.", "labels": [], "entities": []}, {"text": "To this end, we use the HP test suite by, an extensive test suite with various kinds of syntactic and semantic phenomena, originally created for the evaluation of parsers and other NLP systems.", "labels": [], "entities": [{"text": "HP test suite", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9306378960609436}]}, {"text": "The test suite features 1250 sentences dealing with some 290 distinct syntactic and semantic phenomena and sub-phenomena.", "labels": [], "entities": []}, {"text": "Some of the contained sentences are ungrammatical on purpose (and marked as such).", "labels": [], "entities": []}, {"text": "For our testing we chose to use a subset of the test suite consisting of 781 sentences (and 180 phenomena, an average of 4.3 sentences pro phenomenon).", "labels": [], "entities": []}, {"text": "We decided to exclude ungrammatical sentences (314) and sentences with typos (20) since our testing is aiming at testing the coverage of the semantic graphs and not the accuracy of the parser -which we inevitably and indirectly do as will be shown shortly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9988860487937927}]}, {"text": "We also excluded all sentences (135) with conditionals, anaphora and ellipsis phenomena because such cases are still under implementation and thus yet not part of our system.", "labels": [], "entities": []}, {"text": "The test set does not include challenging lexical semantics phenomena, e.g. polysemous words, as it aims at the coverage of syntactic and deeper semantic phenomena.", "labels": [], "entities": []}, {"text": "We run the test set of 781 sentences through our semantic parser and got human-readable representations of the semantic graphs which 2 annotators manually evaluated for their correctness.", "labels": [], "entities": []}, {"text": "A representation was judged correct when the concepts, contexts and properties sub-graphs exactly capture the information they should.", "labels": [], "entities": []}, {"text": "If the dependency graph is wrong, then the whole representation is labelled as parser error.", "labels": [], "entities": []}, {"text": "Erroneous syntactic parsing will always produce erroneous conceptual and contextual graphs, which we do not deal with at the moment.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7478112578392029}]}, {"text": "The lexical sub-graph was also not judged for the correctness of the selected senses as this would result in evaluating the disambiguation algorithm and the coverage of the lexical resources themselves, which is not the goal of this work.", "labels": [], "entities": []}, {"text": "However, any failures in the lexical resources and thus in the lexical sub-graph do not have an impact on the rest of the graphs, which again confirms the flexibility of the layered graph approach.", "labels": [], "entities": []}, {"text": "The results of the manual evaluation are shown in shows that 185 cases could not be correctly parsed by the Stanford Parser and thus the output semantic representation is inevitably wrong as well.", "labels": [], "entities": []}, {"text": "From the remaining 596 sentences for which a correct parse was given, 591 were rewritten to correct semantic graphs and 5 had semantic graphs with missing or wrong information.", "labels": [], "entities": []}, {"text": "The overall performance of the system can be seen in.", "labels": [], "entities": []}, {"text": "The initial version of our semantic parser achieves an F-score of 85% when tested on this subset of the HP test suite.", "labels": [], "entities": [{"text": "F-score", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9994240999221802}, {"text": "HP test suite", "start_pos": 104, "end_pos": 117, "type": "DATASET", "confidence": 0.960706889629364}]}, {"text": "Although this test suite and evaluation are not exhaustive, the performance of the system delivers promising results.", "labels": [], "entities": []}, {"text": "Note that the relative quality of the integrated tools, e.g. the syntactic parser, the implicatives-factives lexicon,", "labels": [], "entities": []}], "tableCaptions": []}