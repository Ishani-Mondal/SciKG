{"title": [{"text": "Team UMBC-FEVER : Claim verification using Semantic Lexical Resources", "labels": [], "entities": [{"text": "UMBC-FEVER", "start_pos": 5, "end_pos": 15, "type": "DATASET", "confidence": 0.9095317721366882}]}], "abstractContent": [{"text": "We describe our system used in the 2018 FEVER shared task.", "labels": [], "entities": [{"text": "FEVER shared task", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.44593791166941327}]}, {"text": "The system employed a frame-based information retrieval approach to select Wikipedia sentences providing evidence and used a two-layer multilayer perceptron to classify a claim as corrector not.", "labels": [], "entities": []}, {"text": "Our submission achieved a score of 0.3966 on the Evidence F1 metric with accuracy of 44.79%, and FEVER score of 0.2628 F1 points.", "labels": [], "entities": [{"text": "F1 metric", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.836873322725296}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9991022348403931}, {"text": "FEVER score", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9878557622432709}, {"text": "F1", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.9374776482582092}]}], "introductionContent": [{"text": "We describe our system and its use in the FEVER shared task (.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9617416858673096}]}, {"text": "We focused on two parts of the problem: (i) information retrieval and (ii) classification.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.868166983127594}]}, {"text": "For the first we opted fora linguistically-inspired approach: we automatically annotated claim sentences and Wikipedia page sentences with syntactic features and semantic frames from FrameNet ( and used the result to retrieve sentences relevant to the claims that provide evidence of their veracity.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 183, "end_pos": 191, "type": "DATASET", "confidence": 0.9363184571266174}]}, {"text": "For classification, we used a simple two-layer perceptron and experimented with several configurations to determine the optimal settings.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9710091352462769}]}, {"text": "Though the overall classification of our best version was lower than the best approach from, which used a more sophisticated classification approach, we scored 10 th out of 24 for the information retrieval task (measured by F 1 ).", "labels": [], "entities": [{"text": "information retrieval task", "start_pos": 184, "end_pos": 210, "type": "TASK", "confidence": 0.8352870941162109}, {"text": "F 1", "start_pos": 224, "end_pos": 227, "type": "METRIC", "confidence": 0.9850744307041168}]}, {"text": "The improvement in our system worked well on the IR task, obtaining a relative improvement of 131% on retrieving evidence over the baseline F1 measure.", "labels": [], "entities": [{"text": "IR task", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.9175504446029663}, {"text": "F1", "start_pos": 140, "end_pos": 142, "type": "METRIC", "confidence": 0.9677383899688721}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance on development dataset of the system on different settings. We achieve comparable  classification performance with simple classifier model thanks to better evidence retrieval.", "labels": [], "entities": []}, {"text": " Table 2: MLP classifier parameter values", "labels": [], "entities": [{"text": "MLP classifier parameter", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7599585851033529}]}, {"text": " Table 3: Classification-without-provenance ac- curacy confusion matrices on the development  dataset for the three classes under.", "labels": [], "entities": []}]}