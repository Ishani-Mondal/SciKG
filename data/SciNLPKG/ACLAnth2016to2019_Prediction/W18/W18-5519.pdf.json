{"title": [{"text": "SIRIUS-LTG: An Entity Linking Approach to Fact Extraction and Verification", "labels": [], "entities": [{"text": "Fact Extraction and Verification", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.8355554342269897}]}], "abstractContent": [{"text": "This article presents the SIRIUS-LTG system for the Fact Extraction and VERification (FEVER) Shared Task.", "labels": [], "entities": [{"text": "Fact Extraction and VERification (FEVER) Shared Task", "start_pos": 52, "end_pos": 104, "type": "TASK", "confidence": 0.6607596211963229}]}, {"text": "It consists of three components: 1) Wikipedia Page Retrieval: First we extract the entities in the claim, then we find potential Wikipedia URI candidates for each of the entities using a SPARQL query over DBpedia 2) Sentence selection: We investigate various techniques i.e. Smooth Inverse Frequency (SIF), Word Mover's Distance (WMD), Soft-Cosine Similarity, Cosine similarity with unigram Term Frequency Inverse Document Frequency (TF-IDF) to rank sentences by their similarity to the claim.", "labels": [], "entities": [{"text": "Wikipedia Page Retrieval", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7490264773368835}, {"text": "Sentence selection", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.8624169528484344}]}, {"text": "3) Textual Entailment: We compare three models for the task of claim classification.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7495548129081726}, {"text": "claim classification", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7188896983861923}]}, {"text": "We apply a Decomposable Attention (DA) model (Parikh et al., 2016), a Decomposed Graph Entail-ment (DGE) model (Khot et al., 2018) and a Gradient-Boosted Decision Trees (TalosTree) model (Sean et al., 2017) for this task.", "labels": [], "entities": []}, {"text": "The experiments show that the pipeline with simple Cosine Similarity using TFIDF in sentence selection along with DA model as labelling model achieves the best results on the development set (F1 evidence: 32.17, label accuracy: 59.61 and FEVER score: 0.3778).", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.701449379324913}, {"text": "F1 evidence", "start_pos": 192, "end_pos": 203, "type": "METRIC", "confidence": 0.9799035489559174}, {"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.544614315032959}, {"text": "FEVER score", "start_pos": 238, "end_pos": 249, "type": "METRIC", "confidence": 0.9881456792354584}]}, {"text": "Furthermore , it obtains 30.19, 48.87 and 36.55 in terms of F1 evidence, label accuracy and FEVER score, respectively, on the test set.", "labels": [], "entities": [{"text": "F1 evidence", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.9838027656078339}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.7420448660850525}, {"text": "FEVER score", "start_pos": 92, "end_pos": 103, "type": "METRIC", "confidence": 0.9829941987991333}]}, {"text": "Our system ranks 15th among 23 participants in the shared task prior to any human-evaluation of the evidence.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Web contains vast amounts of data from many heterogeneous sources, and the harvesting of information from these sources can be extremely valuable for several domains and applications such as, for instance, business intelligence.", "labels": [], "entities": []}, {"text": "The volume and variety of data on the Web are increasing at a very rapid pace, making their use and processing increasingly difficult.", "labels": [], "entities": []}, {"text": "A large volume of information on the Web consists of unstructured text which contains facts about named entities (NE) such as people, places and organizations.", "labels": [], "entities": []}, {"text": "At the same time, the recent evolution of publishing and connecting data over the Web dubbed \"Linked Data\" provides a machine-readable and enriched representation of many of the world's entities, together with their semantic characteristics.", "labels": [], "entities": []}, {"text": "These structured data sources area result of the creation of large knowledge bases (KB) by different communities, which are often interlinked, as is the case of DBpedia ( 1 , Yago 2 ( and FreeBase.", "labels": [], "entities": [{"text": "FreeBase", "start_pos": 188, "end_pos": 196, "type": "DATASET", "confidence": 0.9411048889160156}]}, {"text": "This characteristic of the Web of data empowers both humans and computer agents to discover more concepts by easily navigating among the datasets, and can profitably be exploited in complex tasks such as information retrieval, question answering, knowledge extraction and reasoning.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 204, "end_pos": 225, "type": "TASK", "confidence": 0.7723091244697571}, {"text": "question answering", "start_pos": 227, "end_pos": 245, "type": "TASK", "confidence": 0.883878767490387}, {"text": "knowledge extraction", "start_pos": 247, "end_pos": 267, "type": "TASK", "confidence": 0.8101511299610138}]}, {"text": "Fact extraction from unstructured text is a task central to knowledge base construction.", "labels": [], "entities": [{"text": "Fact extraction from unstructured text", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.9264646172523499}, {"text": "knowledge base construction", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.6874697208404541}]}, {"text": "While this process is vital for many NLP applications, misinformation (false information) or disinformation (deliberately false information) from unreliable sources, can provide false output and mislead the readers.", "labels": [], "entities": []}, {"text": "Such risks could be properly managed by applying NLP techniques aimed at solving the task of fact verification, i.e., to detect and discriminate misinformation and prevent its propagation.", "labels": [], "entities": [{"text": "fact verification", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7003170549869537}]}, {"text": "The Fact Extraction and VERification (FEVER) shared task 4 () addresses both problems.", "labels": [], "entities": [{"text": "Fact Extraction", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6304730027914047}, {"text": "VERification (FEVER", "start_pos": 24, "end_pos": 43, "type": "METRIC", "confidence": 0.5851978858311971}]}, {"text": "In this work, we introduce a pipeline system for each phase of the FEVER shared task.", "labels": [], "entities": [{"text": "FEVER shared task", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.519235094388326}]}, {"text": "In our pipeline, we first identify entities in a given claim, then we extract candidate Wikipedia pages for each of the entities and the most similar sen-tences are obtained using a textual similarity measure.", "labels": [], "entities": []}, {"text": "Finally, we label the claim with regard to evidence sentences using a textual entailment technique.", "labels": [], "entities": []}], "datasetContent": [{"text": "The shared-task () provides an annotated dataset of 185,445 claims along with their evidence sets.", "labels": [], "entities": []}, {"text": "The shared-task dataset is divided into 145,459 , 19,998 and 19,998 train, development and test instances, respectively.", "labels": [], "entities": []}, {"text": "The claims are generated from information extracted from Wikipedia.", "labels": [], "entities": []}, {"text": "The Wikipedia dump (version June 2017) was processed with Stanford CoreNLP, and the claims sampled from the introductory sections of approximately 50,000 popular pages.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.9404363334178925}]}, {"text": "In this section we evaluate our system in the two main subtasks of the shared task: I) evidence extraction (wiki-page retrieval and sentence selection) and II) Entailment.", "labels": [], "entities": [{"text": "evidence extraction", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8502472341060638}, {"text": "sentence selection", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.6777950376272202}]}, {"text": "Since, the scoring formula in the shared-task considers only the first 5 predicted sentence evidences, we choose 5-most similar sentences in the sentence selection phase (Section 2.2).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pipeline performance on the dev set with the sentence selection module. (*) In the FEVER  baseline the label accuracy uses the annotated evidence instead of evidence from the evidence extraction  module.", "labels": [], "entities": [{"text": "FEVER  baseline", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.9043347835540771}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9053466320037842}]}, {"text": " Table 3: Final system pipeline results over test set.", "labels": [], "entities": []}]}