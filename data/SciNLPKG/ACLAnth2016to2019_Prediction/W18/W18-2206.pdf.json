{"title": [{"text": "System description of Supervised and Unsupervised Neural Machine Translation approaches from \"NL Processing\" team at DeepHack.Babel task", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.6370594203472137}]}], "abstractContent": [{"text": "A comparison 1 of supervised and unsupervised Neural Machine Translation (NMT) models was done for the corpora provided by the DeepHack.Babel competition.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.8277476827303568}]}, {"text": "It is shown that for even small parallel corpus, fully supervised NMT gives better results than fully unsupervised for the case of constrained domain of the corpus.", "labels": [], "entities": []}, {"text": "We have also implemented a fully unsuper-vised and a semi-supervised NMT models which have not given positive results compared to fully supervised models.", "labels": [], "entities": []}, {"text": "A blind setup is described where participants know at no point what language pair is used for translation, so no extra data could be integrated in pre-submission phase or during training.", "labels": [], "entities": []}, {"text": "Finally, future competition organizers should find ways to protect their competition setups against various attacks in order to prevent from revealing of language pairs.", "labels": [], "entities": []}, {"text": "We have reported two possible types of attacks on the blind setup.", "labels": [], "entities": []}, {"text": "1 Competition Setup The work presented here is motivated by the following observation: an industrial Neural Machine Translation (NMT) system is usually built on a huge parallel corpora and trained for days or even weeks.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 101, "end_pos": 133, "type": "TASK", "confidence": 0.8322175542513529}]}, {"text": "A \"raw\" NMT model is then tuned by additional training on client-specific data and by augmentation with some domain-specific information.", "labels": [], "entities": []}, {"text": "What if it is not as important to have such a heavy and difficult-to-train model?", "labels": [], "entities": []}, {"text": "Instead, why not just use a simple bootstart model based only on the client's data with a subsequent augmentation done using unsupervised learning, which would use any available non-parallel corpora?", "labels": [], "entities": []}, {"text": "If such approach would produce results comparable to models trained on large parallel corpora, one could significantly reduce costs of preparing parallel corpora and instead focus on better unsupervised models which work with non-parallel corpora (which are much easier and cheaper to produce).", "labels": [], "entities": []}, {"text": "It might also help for the case of low resource languages when no large parallel corpora exist.", "labels": [], "entities": []}, {"text": "This paper attempts to answer these questions.", "labels": [], "entities": []}, {"text": "We present the results obtained by the \"NL Processing\" team in the DeepHack.Babel hackathon 2 on semi-supervised machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.6886630058288574}]}, {"text": "Organizers of the competition created a blind setup a casein which the source and target languages are not known at any stage of 1 Images used to train models are publicly available at https://github.com/aoboturov/ loresmt-nlprocessing 2 The leaderboard:", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Descriptive statistics for the corpora.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results for models in the blind set-up, measured in BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9985643029212952}]}]}