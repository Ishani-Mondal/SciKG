{"title": [{"text": "A neural parser as a direct classifier for head-final languages", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper demonstrates a neural parser implementation suitable for consistently head-final languages such as Japanese.", "labels": [], "entities": []}, {"text": "Unlike the transition-and graph-based algorithms inmost state-of-the-art parsers, our parser directly selects the headword of a dependent from a limited number of candidates.", "labels": [], "entities": []}, {"text": "This method drastically simplifies the model so that we can easily interpret the output of the neural model.", "labels": [], "entities": []}, {"text": "Moreover, by exploiting grammatical knowledge to restrict possible modification types, we can control the output of the parser to reduce specific errors without adding annotated corpora.", "labels": [], "entities": []}, {"text": "The neu-ral parser performed well both on conventional Japanese corpora and the Japanese version of Universal Dependency corpus, and the advantages of distributed representations were observed in the comparison with the non-neural conventional model.", "labels": [], "entities": [{"text": "Universal Dependency corpus", "start_pos": 100, "end_pos": 127, "type": "DATASET", "confidence": 0.611247718334198}]}], "introductionContent": [{"text": "Dependency parsing helps a lotto give intuitive relationships between words such as nounverb and adjective-noun combinations.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7507233023643494}]}, {"text": "Those outputs are consumed in text mining systems) and rule-based approaches such as in fine-grained sentiment extractors (), though some of recent end-to-end systems do not require intermediate parsing structures.", "labels": [], "entities": [{"text": "text mining", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.7663166224956512}, {"text": "fine-grained sentiment extractors", "start_pos": 88, "end_pos": 121, "type": "TASK", "confidence": 0.6441250840822855}]}, {"text": "Many recent dependency parsers have been implemented with neural net (NN) methods with (typically bidirectional) LSTM and distributed word vectors), as we can see in the 2017 shared task on dependency parsing from raw text for 49 languages () based on the multilingual corpora of Universal Dependencies (UD) ().", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8047654926776886}, {"text": "dependency parsing from raw text", "start_pos": 190, "end_pos": 222, "type": "TASK", "confidence": 0.8491579294204712}]}, {"text": "Most of such dependency parsers exploit a transition-based algorithm (), a graph-based algorithm) or a combination of both.", "labels": [], "entities": []}, {"text": "Those algorithms addressed several problems in multilingual dependency analysis such as bidirectional dependency relationships and nonprojective sentences.", "labels": [], "entities": [{"text": "multilingual dependency analysis", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.6336452960968018}]}, {"text": "However, it is hard to intuitively interpret the actions to be trained on the transition-based parser.", "labels": [], "entities": []}, {"text": "Though it can handle the history of past parsing actions, the output may violate syntactic constraints due to the limitation of visible histories.", "labels": [], "entities": []}, {"text": "The graph-based approach captures global information in a sentence, but the difficulty in reflecting the interaction of attachment decisions causes contradictory labels in a tree.", "labels": [], "entities": []}, {"text": "The parsing results from the participants in the 2017 shared task show low scores on Japanese (67 to 82% in the UAS scores, excluding the team that provided the data) in particular, which shows that the language-universal approaches do notwork effectively for Japanese.", "labels": [], "entities": [{"text": "UAS scores", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.7349454462528229}]}, {"text": "The syntactic structures in the Japanese version of Universal Dependencies () have dependencies in both directions, as well as in other languages, since it is based on the word level annotations and the content-head dependency schema.", "labels": [], "entities": []}, {"text": "However, when the syntactic structures are expressed with the dependencies between phrasal units (so-called bunsetsus in Japanese; 'PU' hereafter in this paper), the head element always comes in the right position, since Japanese is a consistently headfinal language.", "labels": [], "entities": []}, {"text": "This allows us to apply a method for such languages to simplify the model.", "labels": [], "entities": []}, {"text": "We con-: Japanese word-level dependencies in the UD-style content-head schema for an example sentence \"\" ('A boy bought a red ball and is playing at the school') . structed a neural parsing model to directly select the headword among the limited candidates.", "labels": [], "entities": []}, {"text": "The model works as a classifier that outputs intuitive and consistent results while exploiting grammatical knowledge.", "labels": [], "entities": []}, {"text": "Section 2 reviews the head-final property of Japanese and the Triplet/Quadruplet Model () to exploit syntactic knowledge in a machine-learning parser.", "labels": [], "entities": []}, {"text": "Section 3 designs our neural model relying on the grammatical knowledge, and its experimental results are reported in Section 4.", "labels": [], "entities": []}, {"text": "Other head-final languages are discussed in Section 5 and some related approaches are discussed in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used EDR Japanese Corpus (EDR, 1996) for the initial training and evaluation.", "labels": [], "entities": [{"text": "EDR Japanese Corpus (EDR, 1996)", "start_pos": 8, "end_pos": 39, "type": "DATASET", "confidence": 0.9337799847126007}]}, {"text": "After remov-: The accuracy of PU dependencies tested on EDR corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9994520545005798}, {"text": "EDR corpus", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.9374552369117737}]}, {"text": "The 'nearest baseline' denotes the ratio of the case where the head is the right next PU.", "labels": [], "entities": []}, {"text": "ing inconsistent PUs due to tokenization mismatch between the corpus and the runtime process, the evaluation was conducted on 2,941 test sentences.", "labels": [], "entities": []}, {"text": "160,080 sentences were used for training and 8,829 sentences were kept for validation.", "labels": [], "entities": []}, {"text": "The models were implemented with TensorFlow ().", "labels": [], "entities": []}, {"text": "The loss function was calculated by cross entropy.", "labels": [], "entities": []}, {"text": "The L2 normalization factor multiplied by 10 \u22128 was added, and output was optimized with AdamOptimizer ().", "labels": [], "entities": [{"text": "AdamOptimizer", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.9198853373527527}]}, {"text": "Words are expressed by two vectors.", "labels": [], "entities": []}, {"text": "One was 100 dimensional embeddings of the surface form -the other was 50 dimensional embeddings of 148 types of values of the combination of 74 types of fine-grained PoS and a binary feature to find the existence of a comma in the PU.", "labels": [], "entities": []}, {"text": "The three features between PUs were converted into 10 dimensional vectors.", "labels": [], "entities": []}, {"text": "All of these vectors were randomly initialized and updated during the training.", "labels": [], "entities": []}, {"text": "The input layer formed 990 in the triplet model and 1,320 dimensions in the quadruplet model.", "labels": [], "entities": []}, {"text": "The dimension of the hidden layer was set to 200 and conducted abeam search with the size 5.", "labels": [], "entities": []}, {"text": "shows the accuracies of dependency parsing by the conventional model trained with logistic regression and our proposed neural net model.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.868105560541153}]}, {"text": "Both models used the very similar grammar rules and the features are used.", "labels": [], "entities": []}, {"text": "While the logistic regression method required manual selection of combination of features to get optimal accuracy, the neural net model outperformed the others when the training corpus was more than 120k sentences, by 0.4 points when the training corpus was 160k sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9990870952606201}]}, {"text": "The maximum number of the training data in neural net model (160k) is less than that used in the logistic regression method (190k) because the development set needed to be: Ablation studies to remove content word vocabularies and commas.", "labels": [], "entities": []}, {"text": "kept for the neural model, and some sentences were dropped as described in Section 4.1.", "labels": [], "entities": []}, {"text": "Only words in the modifier PU and the candidate PUs were used in these methods, and other surrounding context and other dependencies were not considered.", "labels": [], "entities": []}, {"text": "By capturing appropriate contexts of candidate PUs selected by the grammar rules and heuristics, our method successfully predicted the dependencies with relatively small pieces of information compared to the initial transitionbased neural parser) that used a maximum of 18 words in the buffer, stack and modifiers.", "labels": [], "entities": []}, {"text": "There was a huge difference in the training speed.", "labels": [], "entities": []}, {"text": "The logistic regression method took 4 to 20 hours on a CPU, but the neural net model converged in 5 to 15 seconds on a GPU.", "labels": [], "entities": []}, {"text": "We conducted an ablation study to seethe contribution of attributes.", "labels": [], "entities": []}, {"text": "We focused on the vocabulary of content words that can be better captured using distributed representation rather than the conventional method, and commas that played an important role in suggesting long-distance attachments.", "labels": [], "entities": []}, {"text": "According to the results in, the contribution of the content words (the vocabulary size was 11,362) was not very big; even if the content words were ignored, the loss of accuracy was only 0.36 points.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9991187453269958}]}, {"text": "On the other hand, the model ignoring commas (where all of the features regarding commas was removed) downgraded the  accuracy by nearly 2 points, which suggests that commas are important in parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9995489716529846}, {"text": "parsing", "start_pos": 191, "end_pos": 198, "type": "TASK", "confidence": 0.9671448469161987}]}, {"text": "The example dependency in ('president' \u2190 'introduced') was correctly solved by our neural model.", "labels": [], "entities": []}, {"text": "Though many PUs followed the modifier PU in question, the model selected the headword from only three candidates restricted by the grammar rules, and the known dependency relationship between two PUs is guaranteed to be associated with the parsing result.", "labels": [], "entities": []}, {"text": "The conventional model without neural net wrongly selected the first candidate ('acquired') as the head.", "labels": [], "entities": []}, {"text": "The ablation of the content words also made the prediction wrong, that clarified that it was because the conventional model did not capture the content words and the attributes in the distance were stronger.", "labels": [], "entities": []}, {"text": "On the other hand, the neural model with the content word embeddings appropriately captured the relationship between the functional word in the modifier PU (accusative case) and the content word of the correct candidate PU ('introduced').", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The excerpt of Japanese grammar rules. The left side is the condition of the modifier PU  specified with the rightmost morpheme (except for punctuation) with optional preceding morphemes,  and the right side is the list of the condition for the modifiable PUs specified with the head word and  optional functional words.", "labels": [], "entities": []}, {"text": " Table 2: Percentages of the position of the correct  modified PU among the candidate PUs selected  by the initial grammar rules. The column 'Sum'  shows the coverage of the 1st, 2nd and last (the far- thest) PUs in the distance from the modifier PUs.  The EDR Japanese corpus was used in this analy- sis.", "labels": [], "entities": [{"text": "Sum", "start_pos": 142, "end_pos": 145, "type": "METRIC", "confidence": 0.860664963722229}, {"text": "EDR Japanese corpus", "start_pos": 257, "end_pos": 276, "type": "DATASET", "confidence": 0.9620403051376343}]}, {"text": " Table 5: F1 scores of tokenization and UAS on the UD Japanese-GSD test set. The top section shows the  systems which used their own tokenizers. The second section is a comparison with the systems relying  on the default settings of UDPipe, and the bottom section is the situation to ru parsers using the gold PoS  as input.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9994799494743347}, {"text": "tokenization", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.9710279107093811}, {"text": "UAS", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9738315343856812}, {"text": "UD Japanese-GSD test set", "start_pos": 51, "end_pos": 75, "type": "DATASET", "confidence": 0.9550211429595947}]}, {"text": " Table 6: The ratio of head-final dependencies by  languages. \"All\" denotes the ratio of all nodes ex- cept for the root. \"Content\" is the head-final ra- tio for content words, i.e. functional PoSs (ADP,  AUX, CCONJ, DET, SCONJ, SYM, PART, and  PUNCT) are excluded. \"selected\" means the  more selective ones, excluding the labels conj,  fixed, flat, aux and mark.", "labels": [], "entities": []}]}