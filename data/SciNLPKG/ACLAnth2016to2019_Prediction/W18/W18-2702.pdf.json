{"title": [{"text": "A Shared Attention Mechanism for Interpretation of Neural Automatic Post-Editing Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic post-editing (APE) systems aim to correct the systematic errors made by machine translators.", "labels": [], "entities": [{"text": "Automatic post-editing (APE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.623128741979599}]}, {"text": "In this paper, we propose a neural APE system that encodes the source (src) and machine translated (mt) sentences with two separate encoders, but leverages a shared attention mechanism to better understand how the two inputs contribute to the generation of the post-edited (pe) sentences.", "labels": [], "entities": []}, {"text": "Our empirical observations have showed that when the mt is incorrect, the attention shifts weight toward tokens in the src sentence to properly edit the incorrect translation.", "labels": [], "entities": []}, {"text": "The model has been trained and evaluated on the official data from the WMT16 and WMT17 APE IT domain English-German shared tasks.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.9637651443481445}, {"text": "WMT17 APE IT domain English-German shared tasks", "start_pos": 81, "end_pos": 128, "type": "DATASET", "confidence": 0.8917898791176933}]}, {"text": "Additionally, we have used the extra 500K artificial data provided by the shared task.", "labels": [], "entities": []}, {"text": "Our system has been able to reproduce the accuracies of systems trained with the same data, while at the same time providing better interpretability.", "labels": [], "entities": []}], "introductionContent": [{"text": "In current professional practice, translators tend to follow a two-step approach: first, they run a machine translator (MT) to obtain a first-cut translation; then, they manually correct the MT output to produce a result of adequate quality.", "labels": [], "entities": []}, {"text": "The latter step is commonly known as post-editing (PE).", "labels": [], "entities": []}, {"text": "Stemming from this two-step approach and the recent success of deep networks in MT (, the MT research community has devoted increasing attention to the task of automatic postediting (APE) ( . The rationale of an APE system is to be able to automatically correct the systematic errors made by the MT and thus dispense with or reduce the work of the human post-editors.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.897875189781189}, {"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.8732602000236511}]}, {"text": "The data for training and evaluating these systems usually consist of triplets (src, mt, pe), where src is the sentence in the source language, mt is the output of the MT, and pe is the human post-edited sentence.", "labels": [], "entities": []}, {"text": "Note that the pe is obtained by correcting the mt, and therefore these two sentences are closely related.", "labels": [], "entities": []}, {"text": "An APE system is \"monolingual\" if it only uses the mt to predict the post-edits, or \"contextual\" if it uses both the src and the mt as inputs).", "labels": [], "entities": []}, {"text": "Despite their remarkable progress in recent years, neural APE systems are still elusive when it comes to interpretability.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.9594311714172363}]}, {"text": "In deep learning, highly interpretable models can help researchers to overcome outstanding issues such as learning from fewer annotations, learning with human-computer interactions and debugging network representations (.", "labels": [], "entities": []}, {"text": "More specifically in APE, a system that provides insights on its decisions can help the human post-editor to understand the system's errors and consequently provide better corrections.", "labels": [], "entities": [{"text": "APE", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8079085946083069}]}, {"text": "As our main contribution, in this paper we propose a contextual APE system based on the seq2seq model with attention which allows for inspecting the role of the src and the mt in the editing.", "labels": [], "entities": []}, {"text": "We modify the basic model with two separate encoders for the src and the mt, but with a single attention mechanism shared by the hidden vectors of both encoders.", "labels": [], "entities": []}, {"text": "At each decoding step, the shared attention has to decide whether to place more weight on the tokens from the src or the mt.", "labels": [], "entities": []}, {"text": "In our experiments, we clearly observe that when the mt translation contains mistakes (word order, incorrect words), the model learns to shift the attention toward tokens in the source language, aiming to get extra \"context\" or information that will help to correctly edit the translation.", "labels": [], "entities": []}, {"text": "Instead, if the mt sentence is correct, the model simply learns to pass it on word byword.", "labels": [], "entities": []}, {"text": "In Section 4.4, we have plotted the attention weight matrices of several predictions to visualize this finding.", "labels": [], "entities": []}, {"text": "The model has been trained and evaluated with the official datasets from the WMT16 and WMT17 Information Technology (IT) domain APE English-German (en-de) shared tasks (.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.979917585849762}, {"text": "WMT17 Information Technology (IT) domain APE English-German (en-de) shared tasks", "start_pos": 87, "end_pos": 167, "type": "DATASET", "confidence": 0.8832424581050873}]}, {"text": "We have also used the 500K artificial data provided in the shared task for extra training.", "labels": [], "entities": []}, {"text": "For some of the predictions in the test set, we have analysed the plots of attention weight matrices to shed light on whether the model relies more on the src or the mt at each time step.", "labels": [], "entities": []}, {"text": "Moreover, our model has achieved higher accuracy than previous systems that used the same training setting (official datasets + 500K extra artificial data).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9993591904640198}]}], "datasetContent": [{"text": "For training and evaluation we have used the WMT17 APE 1 IT domain English-German dataset.", "labels": [], "entities": [{"text": "WMT17 APE 1 IT domain English-German dataset", "start_pos": 45, "end_pos": 89, "type": "DATASET", "confidence": 0.8914955088070461}]}, {"text": "This dataset consists of 11,000 triplets for training, 1,000 for validation and 2,000 for testing.", "labels": [], "entities": []}, {"text": "The hyper-parameters have been selected using only the validation set and used unchanged on the test set.", "labels": [], "entities": []}, {"text": "We have also trained the model with the 12,000 sentences from the previous year (WMT16), fora total of 23,000 training triplets.", "labels": [], "entities": [{"text": "WMT16)", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.7840666472911835}]}], "tableCaptions": [{"text": " Table 1: The model and its hyper-parameters.", "labels": [], "entities": []}]}