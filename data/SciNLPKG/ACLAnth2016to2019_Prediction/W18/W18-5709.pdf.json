{"title": [{"text": "A Knowledge-Grounded Multimodal Search-Based Conversational Agent", "labels": [], "entities": []}], "abstractContent": [{"text": "Multimodal search-based dialogue is a challenging new task: It extends visually grounded question answering systems into multi-turn conversations with access to an external database.", "labels": [], "entities": [{"text": "Multimodal search-based dialogue", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.663578192392985}, {"text": "question answering", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7423841655254364}]}, {"text": "We address this new challenge by learning a neural response generation system from the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017).", "labels": [], "entities": [{"text": "neural response generation", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.6882132987181345}]}, {"text": "We introduce a knowledge-grounded multi-modal conversational model where an encoded knowledge base (KB) representation is appended to the decoder input.", "labels": [], "entities": []}, {"text": "Our model substantially outperforms strong baselines in terms of text-based similarity measures (over 9 BLEU points, 3 of which are solely due to the use of additional information from the KB).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9985080361366272}]}], "introductionContent": [{"text": "Conversational agents have become ubiquitous, with variants ranging from open-domain conversational chit-chat bots) to domainspecific task-based dialogue systems (.", "labels": [], "entities": []}, {"text": "Our work builds upon the recently released Multimodal Dialogue (MMD) dataset (, which contains dialogue sessions in the ecommerce (fashion) domain.", "labels": [], "entities": []}, {"text": "illustrates an example chat session with multimodal interaction between the user and the system.", "labels": [], "entities": []}, {"text": "We focus on the task of generating textual responses conditioned on the previous conversational history.", "labels": [], "entities": []}, {"text": "Traditional goal-oriented dialogue systems relied on slot-filling approach to this task, i.e. explicit modelling of all attributes in the domain (;).", "labels": [], "entities": []}, {"text": "On the other hand, previous work on MMD data used direct learning from raw texts with implicit semantic representation only.", "labels": [], "entities": [{"text": "MMD", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9731611013412476}]}, {"text": "This paper attempts to combine both approaches by learning to generate replies from raw user input, while also incorporating Knowledge Base (KB) inputs (i.e. explicit semantics) into the generation process.", "labels": [], "entities": []}, {"text": "We discuss how our model is able to handle various user intents (request types) and the impact of incorporating the additional explicit semantic information from the KB into particular targeted intents.", "labels": [], "entities": []}, {"text": "We use user intent annotation and KB queries provided with the dataset for the purpose of this work.", "labels": [], "entities": []}, {"text": "Our main contribution is the resulting fully data-driven model for the task of conversational multimodal dialogue generation, grounded in conversational text history, vision and KB inputs.", "labels": [], "entities": [{"text": "conversational multimodal dialogue generation", "start_pos": 79, "end_pos": 124, "type": "TASK", "confidence": 0.6882383301854134}]}, {"text": "We also illustrate a method to improve context modelling over multiple images and show great improvements over the baseline.", "labels": [], "entities": [{"text": "context modelling", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.696932777762413}]}, {"text": "Finally, we present a detailed analysis of the outputs generated by our system corresponding to different user intents.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our work is based on the Multimodal Dialogue (MMD) dataset (), which consists of 150k chat sessions.", "labels": [], "entities": []}, {"text": "User queries can be complex from the perspective of multimodal taskspecific dialogue, such as \"Show me more images of the 3rd product in some different directions\".", "labels": [], "entities": []}, {"text": "However, it also heavily relies on the external KB to answer product attributes related to user queries, such as \"What is the brand/material of the suit in 3rd image?\" or \"Show something similar to 1st result but in a different material\".", "labels": [], "entities": []}, {"text": "This dataset contains raw chat logs as well as metadata information of the corresponding products.", "labels": [], "entities": []}, {"text": "Around 400 anonymised celebrity profiles have been introduced in the system to emulate endorsement in recommendation, such as \"What kind of slippers are endorsed by cel 145?\".", "labels": [], "entities": []}, {"text": "For each dialogue turn, there are manual annotations of the user intent available.", "labels": [], "entities": []}, {"text": "We use the intents to construct celebrity encodings.", "labels": [], "entities": []}, {"text": "On average, each session contains 40 dialogue turns.", "labels": [], "entities": []}, {"text": "The system response depends on the intent state of the user query and on average contains 8 words and 4 images per utterance.", "labels": [], "entities": []}, {"text": "We created our own version of the dataset from the raw chat logs of the dialogue session and metadata information.", "labels": [], "entities": []}, {"text": "As discussed in Section 3.1, this was necessary to model the visual context over multiple images.", "labels": [], "entities": []}, {"text": "We created the KB input to our model as described in Section 3.2 from the raw chat logs and the metadata information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automated evaluation based on BLEU- 4, METEOR and ROUGE-L metrics. Here, 'M'  represents multimodality while 'T' stands for text- only model. 'attn' denotes use of attention and  'kb' signifies incorporating Knowledge Base in- put. 'Cxt' represents context size for the dialogue  history.  *Saha et al. was trained on a different version of  the dataset, as discussed in Section 3.", "labels": [], "entities": [{"text": "BLEU- 4", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9660429159800211}, {"text": "METEOR", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.8595183491706848}, {"text": "ROUGE-L", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9155455231666565}]}]}