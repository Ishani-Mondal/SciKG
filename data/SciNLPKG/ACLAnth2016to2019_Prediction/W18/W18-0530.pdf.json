{"title": [{"text": "A Semantic Role-based Approach to Open-Domain Automatic Question Generation", "labels": [], "entities": [{"text": "Open-Domain Automatic Question Generation", "start_pos": 34, "end_pos": 75, "type": "TASK", "confidence": 0.5943810343742371}]}], "abstractContent": [{"text": "We present a novel rule-based system for automatic generation of factual questions from sentences, using semantic role labeling (SRL) as the main form of text analysis.", "labels": [], "entities": [{"text": "automatic generation of factual questions from sentences", "start_pos": 41, "end_pos": 97, "type": "TASK", "confidence": 0.8328097122056144}, {"text": "semantic role labeling (SRL)", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.8046092689037323}]}, {"text": "The system is capable of generating both wh-questions and yes/no questions from the same semantic analysis.", "labels": [], "entities": []}, {"text": "We present an extensive evaluation of the system and compare it to a recent neural network architecture for question generation.", "labels": [], "entities": [{"text": "question generation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.798352986574173}]}, {"text": "The SRL-based system outperforms the neural system in both average quality and variety of generated questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic generation of questions (AQG) is an important and challenging research area in natural language processing.", "labels": [], "entities": [{"text": "Automatic generation of questions (AQG)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8728743493556976}, {"text": "natural language processing", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.645101398229599}]}, {"text": "AQG systems can be useful for educational applications such as assessment of reading comprehension, intelligent tutoring, dialogue agents, and instructional games.", "labels": [], "entities": []}, {"text": "Most of the research on AQG focuses on factoid questionsquestions that are generated from reading passages and ask about information that is expressed in the text itself (as opposed to, e.g., readers' opinions of the text or external knowledge related to the text).", "labels": [], "entities": [{"text": "factoid questionsquestions that are generated from reading passages and ask about information that is expressed in the text itself (as opposed to, e.g., readers' opinions of the text or external knowledge related to the text)", "start_pos": 39, "end_pos": 264, "type": "Description", "confidence": 0.7726295376435305}]}, {"text": "Traditional architectures for AQG involve syntactic and semantic analysis of text, with rulebased and template-based modules for converting linguistic analyses into questions.", "labels": [], "entities": []}, {"text": "Many of these systems employ semantic role labeling (SRL) as an important analytic component.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.7776295393705368}]}, {"text": "Recently, neural network architectures have also been proposed for the AQG task (.", "labels": [], "entities": [{"text": "AQG task", "start_pos": 71, "end_pos": 79, "type": "TASK", "confidence": 0.5276674628257751}]}, {"text": "In this paper we present an automatic question generation system based on semantic role labeling.", "labels": [], "entities": [{"text": "question generation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7153208404779434}, {"text": "semantic role labeling", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.6461842556794485}]}, {"text": "The system generates questions directly from semantic analysis, without templates.", "labels": [], "entities": []}, {"text": "Our system includes two innovations.", "labels": [], "entities": []}, {"text": "While previous SRLbased AQG systems generated only wh-questions, ours is the first reported system that also generates yes/no questions from SRL analysis.", "labels": [], "entities": [{"text": "SRLbased AQG", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.5750774890184402}]}, {"text": "It is also the first system that generates questions for copular sentences from their SRL analysis (both yes/no and wh-questions).", "labels": [], "entities": []}, {"text": "To evaluate the performance of our system, we compare the quality of its output with that of a state-of-the-art neural network AQG system, over the same set of texts.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, ours is the first direct comparison of SRLbased and neural AQG systems.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents related work on AQG.", "labels": [], "entities": [{"text": "AQG", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.521211564540863}]}, {"text": "Section 3 describes our SRL-based system and section 4 outlines the neural network AQG system.", "labels": [], "entities": [{"text": "SRL-based", "start_pos": 24, "end_pos": 33, "type": "TASK", "confidence": 0.9289863109588623}]}, {"text": "Section 5 describes the annotation study.", "labels": [], "entities": []}, {"text": "Results are presented in section 6 and error analysis in section 7.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 39, "end_pos": 53, "type": "METRIC", "confidence": 0.9436869621276855}]}], "datasetContent": [{"text": "We evaluate the SRL and neural network systems' capacity to produce generally good questions, focusing only on question-generation capabilities.", "labels": [], "entities": [{"text": "SRL", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9049271941184998}]}, {"text": "In this respect, our evaluation study differs from some previous studies in which the AQG system was tasked with performing both content selection (picking the 'important' sentences for which questions would be generated) and the question generation process itself.", "labels": [], "entities": [{"text": "content selection", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.6909113973379135}, {"text": "question generation", "start_pos": 230, "end_pos": 249, "type": "TASK", "confidence": 0.6590655446052551}]}, {"text": "We believe that content selection depends very much on the goals of the educational task for which questions are generated, and should be seen as a separate task.", "labels": [], "entities": [{"text": "content selection", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7419098913669586}]}, {"text": "For a similar perspective, see.", "labels": [], "entities": []}, {"text": "Since our focus is on question generation for educational applications, we selected five expository texts.", "labels": [], "entities": [{"text": "question generation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7805274128913879}]}, {"text": "Three of the texts consist of several initial paragraphs from Wikipedia articles and two are complete short articles from an educational website . From those texts, we retained only declarative sentences that have at least five words and do not include conditional (if.", "labels": [], "entities": []}, {"text": "Our corpus consists of 171 sentences, with a maximum sentence length of 50 tokens and an average of 17.", "labels": [], "entities": []}, {"text": "Both the neural and the SRL-based systems were tasked with generating questions for each of the 171 sentences.", "labels": [], "entities": []}, {"text": "The SRL-based system generated at least one question for 165 sentences and failed to provide an output for 6 sentences.", "labels": [], "entities": []}, {"text": "Overall, the SRL-based system generated 890 questions, with an average of 5.4 questions per sentence.", "labels": [], "entities": [{"text": "SRL-based", "start_pos": 13, "end_pos": 22, "type": "TASK", "confidence": 0.8952139019966125}]}, {"text": "There are two reasons for this abundance.", "labels": [], "entities": []}, {"text": "First, the system attempts to generate a yes/no question for each predicate in each sentence.", "labels": [], "entities": []}, {"text": "As a result, it generated 236 yes/no questions.", "labels": [], "entities": []}, {"text": "Next, the system attempts to generate a constituent question for almost every argument  of every predicate.", "labels": [], "entities": []}, {"text": "If a sentence contains multiple predicates, even more questions are generated.", "labels": [], "entities": []}, {"text": "The system generated 654 constituent questions.", "labels": [], "entities": []}, {"text": "The neural system generated one question for each of 169 sentences (and failed for two sentences).", "labels": [], "entities": []}, {"text": "All questions generated by the system resemble constituent questions because the SQuAD dataset does not contain yes/no questions.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9104970097541809}]}, {"text": "We investigated whether it was possible to generate more than one question per sentence by retrieving hypotheses from the beam search, but the hypotheses are not fully formed and are small variants of the best question for each sentence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of sentences, generated questions and evaluation ratings (average of two raters).", "labels": [], "entities": []}, {"text": " Table 3: Average ratings for SRL system yes/no ques- tions (SRL-YNQ), constituent questions (SRL-CQ),  and neural network questions (NN). Total is the sum  of grammar, semantics, and relevance.", "labels": [], "entities": [{"text": "SRL system yes/no ques- tions", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.8559798672795296}]}]}