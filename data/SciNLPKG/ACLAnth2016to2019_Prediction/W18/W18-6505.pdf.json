{"title": [{"text": "End-to-End Content and Plan Selection for Data-to-Text Generation", "labels": [], "entities": [{"text": "Data-to-Text Generation", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7121558338403702}]}], "abstractContent": [{"text": "Learning to generate fluent natural language from structured data with neural networks has become an common approach for NLG.", "labels": [], "entities": []}, {"text": "This problem can be challenging when the form of the struc-tured data varies between examples.", "labels": [], "entities": []}, {"text": "This paper presents a survey of several extensions to sequence-to-sequence models to account for the latent content selection process, particularly variants of copy attention and coverage decoding.", "labels": [], "entities": []}, {"text": "We further propose a training method based on diverse ensembling to encourage models to learn distinct sentence templates during training.", "labels": [], "entities": []}, {"text": "An empirical evaluation of these techniques shows an increase in the quality of generated text across five automated metrics, as well as human evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent developments in end-to-end learning with neural networks have enabled methods to generate textual output from complex structured inputs such as images and tables.", "labels": [], "entities": []}, {"text": "These methods may also enable the creation of text-generation models that are conditioned on multiple key-value attribute pairs.", "labels": [], "entities": []}, {"text": "The conditional generation of fluent text poses multiple challenges since a model has to select content appropriate for an utterance, develop a sentence layout that fits all selected information, and finally generate fluent language that incorporates the content.", "labels": [], "entities": []}, {"text": "End-to-end methods have already been applied to increasingly complex data to simultaneously learn sentence planning and surface realization but were often restricted by the limited data availability.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.6737622320652008}, {"text": "surface realization", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7881976664066315}]}, {"text": "The re-  cent creation of datasets such as the E2E NLG dataset () provides an opportunity to further advance methods for text generation.", "labels": [], "entities": [{"text": "E2E NLG dataset", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9704797863960266}, {"text": "text generation", "start_pos": 121, "end_pos": 136, "type": "TASK", "confidence": 0.804536908864975}]}, {"text": "In this work, we focus on the generation of language from meaning representations (MR), as shown in.", "labels": [], "entities": [{"text": "generation of language from meaning representations (MR)", "start_pos": 30, "end_pos": 86, "type": "TASK", "confidence": 0.7536615000830756}]}, {"text": "This task requires learning a semantic alignment from MR to utterance, wherein the MR can comprise a variable number of attributes.", "labels": [], "entities": []}, {"text": "Recently, end-to-end generation has been handled primarily by Sequence-to-sequence (S2S) models) that encode some information and decode it into a desired format.", "labels": [], "entities": [{"text": "end-to-end generation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6741121411323547}]}, {"text": "Extensions for summarization and other tasks have developed a mechanism to copy words from the input into a generated text (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9897151589393616}]}, {"text": "We begin with a strong S2S model with copymechanism for the E2E NLG task and include methods that can help to control the length of a generated text and how many inputs a model uses (.", "labels": [], "entities": [{"text": "E2E NLG task", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.9130420883496603}]}, {"text": "Finally, we also present results of the Transformer architecture ( as an alternative S2S variant.", "labels": [], "entities": []}, {"text": "We show that these extensions lead to improved text generation and content selection.", "labels": [], "entities": [{"text": "text generation", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7937626838684082}, {"text": "content selection", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.6882465183734894}]}, {"text": "We further propose a training approach based on the diverse ensembling technique.", "labels": [], "entities": []}, {"text": "In this technique, multiple models are trained to partition the training data during the process of training the model itself, thus leading to models that follow distinct sentence templates.", "labels": [], "entities": []}, {"text": "We show that this approach improves the quality of generated text, but also the robustness of the training process to outliers in the training data.", "labels": [], "entities": []}, {"text": "Experiments are run on the E2E NLG challenge . We show that the application of this technique increases the quality of generated text across five different automated metrics (BLEU, NIST, METEOR, ROUGE, and CIDEr) over the multiple strong S2S baseline models.", "labels": [], "entities": [{"text": "E2E NLG challenge", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.8997023304303488}, {"text": "BLEU", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9981939196586609}, {"text": "METEOR", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.9412434101104736}, {"text": "ROUGE", "start_pos": 195, "end_pos": 200, "type": "METRIC", "confidence": 0.9723209738731384}]}, {"text": "Among 60 submissions to the challenge, our approach ranked first in METEOR, ROUGE, and CIDEr scores, third in BLEU, and sixth in NIST.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9886435866355896}, {"text": "ROUGE", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9932748079299927}, {"text": "CIDEr", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9394762516021729}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9988170862197876}, {"text": "NIST", "start_pos": 129, "end_pos": 133, "type": "DATASET", "confidence": 0.9323160648345947}]}], "datasetContent": [{"text": "We apply our method to the crowd-sourced E2E NLG dataset of   For all LSTM-based S2S models, we use a twolayer bidirectional LSTM encoder, and hidden and embedding sizes of 750.", "labels": [], "entities": [{"text": "E2E NLG dataset", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.9117840528488159}]}, {"text": "During training, we apply dropout with probability 0.2 and train models with Adam () and an initial learning rate of 0.002.", "labels": [], "entities": []}, {"text": "We evaluate both mlp and dot attention types.", "labels": [], "entities": []}, {"text": "The Transformer model has 4 layers with hidden and embedding sizes 512.", "labels": [], "entities": []}, {"text": "We use the training rate schedule described by, using Adam and a maximum learning rate of 0.1 after 2,000 warmup steps.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.929957389831543}]}, {"text": "The diverse ensembling technique is applied to all approaches, pre-training all models for 4 epochs and then activating the sMCL loss.", "labels": [], "entities": []}, {"text": "All models are implemented in OpenNMTpy ( 2 . The parameters were found by grid search starting from the parameters used in the TGEN model by.", "labels": [], "entities": []}, {"text": "Unless stated otherwise, models do not block repeat sentence beginnings, since it results in worse performance in automated met-rics.", "labels": [], "entities": [{"text": "repeat sentence beginnings", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.7094833254814148}]}, {"text": "We show results on the multi-reference validation and the blind test sets for the five metrics BLEU (), NIST), METEOR), ROUGE, and CIDEr ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9964761137962341}, {"text": "NIST", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.4895448088645935}, {"text": "METEOR", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9849956631660461}, {"text": "ROUGE", "start_pos": 120, "end_pos": 125, "type": "METRIC", "confidence": 0.9915717840194702}]}, {"text": "shows the results of different models on the validation set.", "labels": [], "entities": []}, {"text": "During inference, we set the length penalty parameter \u03b1 to 0.4, the coverage penalty parameter \u03b2 to 0.1, and use beam search with abeam size of 10.", "labels": [], "entities": [{"text": "length penalty parameter \u03b1", "start_pos": 29, "end_pos": 55, "type": "METRIC", "confidence": 0.9689026474952698}, {"text": "coverage penalty parameter \u03b2", "start_pos": 68, "end_pos": 96, "type": "METRIC", "confidence": 0.9740720838308334}]}, {"text": "Our models outperform all shown baselines, which represent all published results on this dataset to date.", "labels": [], "entities": []}, {"text": "Except for the copyonly condition, the data-efficient dot outperforms mlp.", "labels": [], "entities": []}, {"text": "Both copy-attention and diverse ensembling increase performance, and combining the two methods yields the highest BLEU and NIST scores across all conditions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9985889792442322}, {"text": "NIST", "start_pos": 123, "end_pos": 127, "type": "DATASET", "confidence": 0.4880993366241455}]}, {"text": "The Transformer performs similarly to the vanilla S2S models, with a lower BLEU but higher ROUGE score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9996378421783447}, {"text": "ROUGE score", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.9832884073257446}]}, {"text": "Diverse ensembling also increases the performance with the Transformer model, leading to the highest ROUGE score across all model configurations.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 101, "end_pos": 112, "type": "METRIC", "confidence": 0.9840365946292877}]}, {"text": "shows generated text from different models.", "labels": [], "entities": []}, {"text": "We can observe that the model without copy attention omits the rating, and without ensem-bling, the sentence structure repeats and thus looks unnatural.", "labels": [], "entities": []}, {"text": "With ensembling, both models produce sensible output with different sentence layouts.", "labels": [], "entities": []}, {"text": "We note that often, only the better of the two models in the ensemble produces output better than the baselines.", "labels": [], "entities": []}, {"text": "We further analyze how many attributes are omitted by the systems in Section 7.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of different S2S approaches and published baseline models on the E2E NLG validation  set. The second section shows models without diverse ensembling, the third section with it. The fourth  section shows results of the Transformer model. / indicates that numbers were not reported.", "labels": [], "entities": [{"text": "E2E NLG validation  set", "start_pos": 83, "end_pos": 106, "type": "DATASET", "confidence": 0.9746879786252975}]}, {"text": " Table 4: Variants of diverse ensembling. The top section shows results of varying the number of models  in a diverse ensemble on the validation set. The bottom section shows results with different numbers of  shared parameters between two models in a diverse ensemble. All results are generated with setup (8)  from Table 2.", "labels": [], "entities": []}, {"text": " Table 5: The results of our model on the blind E2E NLG test set. Notable rankings within the 60  submitted systems are shown in parentheses. Systems by Freitag and Roy (2018) and Su et al. (2018)", "labels": [], "entities": [{"text": "E2E NLG test set", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.8966454416513443}]}]}