{"title": [{"text": "Self-training improves Recurrent Neural Networks performance for Temporal Relation Extraction", "labels": [], "entities": [{"text": "Recurrent Neural Networks", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.8802536129951477}, {"text": "Temporal Relation Extraction", "start_pos": 65, "end_pos": 93, "type": "TASK", "confidence": 0.8925421833992004}]}], "abstractContent": [{"text": "Neural network models are oftentimes restricted by limited labeled instances and resort to advanced architectures and features for cutting edge performance.", "labels": [], "entities": []}, {"text": "We propose to build a recurrent neural network with multiple semantically heterogeneous embeddings within a self-training framework.", "labels": [], "entities": []}, {"text": "Our framework makes use of labeled, unlabeled, and social media data, operates on basic features, and is scalable and generalizable.", "labels": [], "entities": []}, {"text": "With this method, we establish the state-of-the-art result for both in-and cross-domain fora clinical temporal relation extraction task.", "labels": [], "entities": [{"text": "clinical temporal relation extraction task", "start_pos": 93, "end_pos": 135, "type": "TASK", "confidence": 0.6561256766319274}]}], "introductionContent": [{"text": "Neural network methods have obtained spectacular successes in the fields of computer vision, speech recognition (, and machine translation (, where large datasets are available for training.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.830249696969986}, {"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.8473722636699677}]}, {"text": "For extracting information from text, however, performance gains have been minimal or non-existent, with published work emphasizing that such performance parity is not obtainable without extensive feature engineering.", "labels": [], "entities": []}, {"text": "Unlike other settings that have seen performance gains, information extraction tasks related to text typically have much smaller supervised training sets, and the neural network algorithms presumably do not see enough instances to optimally tune the large parameter space.", "labels": [], "entities": [{"text": "information extraction tasks related to text", "start_pos": 56, "end_pos": 100, "type": "TASK", "confidence": 0.8446031908194224}]}, {"text": "In this paper, we examine the important information extraction task of temporal relation extraction from clinical text.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7462620437145233}, {"text": "temporal relation extraction from clinical text", "start_pos": 71, "end_pos": 118, "type": "TASK", "confidence": 0.7710949927568436}]}, {"text": "The state-of-the-art for this task is a machine learner with a heavily-engineered set of features ().", "labels": [], "entities": []}, {"text": "The identification of temporal relations from the clinical text in the electronic medical records has been drawing growing attention because of its potential to provide accurate fine-grained analyses of many medical phenomena (e.g., disease progression, longitudinal effects of medications), with many clinical applications such as question answering, clinical outcomes prediction (), and recognition of temporal patterns and timelines (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 332, "end_pos": 350, "type": "TASK", "confidence": 0.9106045365333557}, {"text": "clinical outcomes prediction", "start_pos": 352, "end_pos": 380, "type": "TASK", "confidence": 0.6830083330472311}, {"text": "recognition of temporal patterns and timelines", "start_pos": 389, "end_pos": 435, "type": "TASK", "confidence": 0.8221609592437744}]}, {"text": "Obtaining large supervised datasets for clinical tasks is expensive and difficult, so it has been challenging to show meaningful improvements from the recent explosion of sophisticated neural network methods.", "labels": [], "entities": []}, {"text": "Our hypothesis is that the range of interesting phenomena found in clinical data is much broader than what is covered by available gold standard datasets for temporal information extraction.", "labels": [], "entities": [{"text": "temporal information extraction", "start_pos": 158, "end_pos": 189, "type": "TASK", "confidence": 0.6578921576340994}]}, {"text": "The results of Clinical TempEval 2017 (  strongly support this latter point, as the performance of submitted systems drops severely when trained on gold instances in one domain and tested on anew domain.", "labels": [], "entities": [{"text": "Clinical TempEval 2017", "start_pos": 15, "end_pos": 37, "type": "DATASET", "confidence": 0.6589832305908203}]}, {"text": "We are thus inspired to make use of unlabeled data in addition to gold standard data with a simple semisupervised learning method-self-training and combine it with varieties of pre-trained word embeddings to overcome gaps in training data coverage.", "labels": [], "entities": []}, {"text": "In self-training), a classifier is first trained on existing labeled data, and then applied to unlabeled data (typically a much larger amount).", "labels": [], "entities": []}, {"text": "The predicted instances above a confidence threshold are added to the training set and the classifier is re-trained.", "labels": [], "entities": []}, {"text": "Self-training is especially attractive in a neural network setting because the primitive feature types used by these networks (i.e., tokens) are computationally more efficient to obtain than the sophisticated features typically used by feature engineering methods.", "labels": [], "entities": []}, {"text": "For pre-training, we investigate the use of multiple external data sources to train word embeddings that form the input layer of the model.", "labels": [], "entities": []}, {"text": "Since our Besides showing that neural network approaches to information extraction can outperform featureengineering approaches, we find that self-training works better in the neural network setting than with existing state-of-the-art feature-engineering approaches.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8122438490390778}]}, {"text": "Finally, we show that these methods generalize to new clinical domains better than the feature-engineering approaches we compare them to, obtaining state-of-the-art performance in an unsupervised domain adaptation setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with several combinations of clinical and cancer-related social media and Google news embeddings.", "labels": [], "entities": []}, {"text": "We tested three modes of merging silver instances with gold annotations, lower right): 1) Posi-Merge: merging the positive predictions (i.e. CONTAINS and CONTAINED-BY relations) with the gold relations; 2) sub-Merge: merging a subset of the silver data (a random sample of 45K silver samples including CONTAINS, CONTAINED-BY, and NONE relations) with the gold relations; and 3) all-Merge: merging all silver data with the gold relations.", "labels": [], "entities": []}, {"text": "After merging, we shuffled gold and silver instances together to balance the batch-wise computation.", "labels": [], "entities": []}, {"text": "Models utilizing self-training were trained on the gold colon cancer training set of the THYME corpus and silver instances predicted from the unlabeled colon cancer data.", "labels": [], "entities": [{"text": "THYME corpus", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.9558591246604919}]}, {"text": "Models were tested on the gold colon cancer and gold brain cancer development sets of the THYME corpus, comparing in-domain and cross-domain performance to select the best models for testing.", "labels": [], "entities": [{"text": "THYME corpus", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.9048239588737488}]}, {"text": "The best models were tested on the gold colon cancer and brain cancer test sets (Clinical TempEval 2017 test sets).", "labels": [], "entities": [{"text": "gold colon cancer and brain cancer test sets (Clinical TempEval 2017 test sets", "start_pos": 35, "end_pos": 113, "type": "DATASET", "confidence": 0.7098555096558162}]}, {"text": "All models were evaluated with the metrics precision (P), recall (R) and F1-score (F), using the standard Clinical TempEval evaluation script, where the P and R definitions are enhanced through temporal closure (UzZaman and Allen, 2011; UzZaman et al., 2012): when calculating precision, we run temporal closure on the gold relations but not on the system-generated ones; when calculating recall, we run temporal closure on the systemgenerated relations but not on the gold ones.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9372712969779968}, {"text": "recall (R)", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9457390606403351}, {"text": "F1-score (F)", "start_pos": 73, "end_pos": 85, "type": "METRIC", "confidence": 0.9616202861070633}, {"text": "precision", "start_pos": 277, "end_pos": 286, "type": "METRIC", "confidence": 0.9803732633590698}, {"text": "recall", "start_pos": 389, "end_pos": 395, "type": "METRIC", "confidence": 0.9748077988624573}]}, {"text": "shows performance of the THYME system and various bi-directional RNN methods on the colon cancer and brain cancer development sets.", "labels": [], "entities": []}, {"text": "For RNNs, we evaluated both LSTM and GRU models.", "labels": [], "entities": []}, {"text": "For embedding combinations, we tested using the clinical embedding alone (C), using both clinical and cancer-related social media embeddings (CS), using both clinical and Google News embeddings (CG), and using Google News Embeddings alone (G).", "labels": [], "entities": []}, {"text": "For ways to merge silver samples with gold instances we tested no-selftraining in which no silver instances were used, all-Merge in which all silver instances were used, sub-Merge in which a subset of silver samples were used, and Posi-Merge in which only the positive silver instances were used.", "labels": [], "entities": []}, {"text": "Among all settings, bi-LSTM CG Posi-Merge and bi-LSTM CS PosiMerge achieved the best F1-score (F1b) on the brain development set; bi-LSTM CS Posi-Merge had the best F1-score (F1c) on the colon development set.", "labels": [], "entities": [{"text": "F1-score (F1b)", "start_pos": 85, "end_pos": 99, "type": "METRIC", "confidence": 0.9342749863862991}, {"text": "F1-score (F1c)", "start_pos": 165, "end_pos": 179, "type": "METRIC", "confidence": 0.866114541888237}]}, {"text": "These two best performing neural models along with the THYME no-self-training system were tested on the Clinical TempEval test splits.", "labels": [], "entities": [{"text": "THYME", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.7274021506309509}, {"text": "Clinical TempEval test splits", "start_pos": 104, "end_pos": 133, "type": "DATASET", "confidence": 0.8622800409793854}]}, {"text": "shows that the bi-LSTM models outperform the SVM-based THYME system and the Clinical TempEval 2017 top system, especially on the cross-domain experiments.", "labels": [], "entities": [{"text": "Clinical TempEval 2017 top", "start_pos": 76, "end_pos": 102, "type": "DATASET", "confidence": 0.7837169468402863}]}, {"text": "The THYME system performance on the colon test set is 0.621 F1 which is an improvement over previously reported results ().", "labels": [], "entities": [{"text": "THYME", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9815892577171326}, {"text": "colon test set", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.7240655024846395}, {"text": "F1", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9996640682220459}]}, {"text": "The THYME system result on the brain cancer testis reported here for the first time.", "labels": [], "entities": [{"text": "THYME", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9776887893676758}]}, {"text": "Note that the THYME system was trained on all gold colon cancer annotations (training, development and test), while the bi-LSTM models were trained on gold training colon cancer data and positive silver colon cancer samples.", "labels": [], "entities": [{"text": "THYME", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.6183274388313293}]}, {"text": "The best Clinical TempEval result on the gold colon cancer test set -0.613 F1-score -is reported by the LIMSI-COT system which makes use of cTAKESgenerated features.", "labels": [], "entities": [{"text": "Clinical TempEval", "start_pos": 9, "end_pos": 26, "type": "METRIC", "confidence": 0.6767904460430145}, {"text": "gold colon cancer test set", "start_pos": 41, "end_pos": 67, "type": "DATASET", "confidence": 0.6828234791755676}, {"text": "F1-score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.8972138166427612}]}, {"text": "The best Clinical TempEval result on the gold brain cancer test set -0.34 F1-score -is achieved by the GUIR system (, while LIMSI-COT obtains 0.33 cross-domain F1-score.", "labels": [], "entities": [{"text": "gold brain cancer test set", "start_pos": 41, "end_pos": 67, "type": "DATASET", "confidence": 0.6596827566623688}, {"text": "F1-score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9479367733001709}, {"text": "GUIR", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.5574445128440857}, {"text": "F1-score", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9566301107406616}]}], "tableCaptions": [{"text": " Table 4: CONTAINS relations on colon cancer and brain cancer test set", "labels": [], "entities": [{"text": "CONTAINS", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.5661405920982361}]}, {"text": " Table 4. We experimented with concatenating all  three embeddings (clinical, cancer-related social", "labels": [], "entities": []}]}