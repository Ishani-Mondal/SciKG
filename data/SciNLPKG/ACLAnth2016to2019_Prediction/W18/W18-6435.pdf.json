{"title": [], "abstractContent": [{"text": "We evaluate the output of 16 English-to-German MT systems with respect to the translation of pronouns in the context of the WMT 2018 competition.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9135584831237793}, {"text": "WMT 2018 competition", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.6410638689994812}]}, {"text": "We work with a test suite specifically designed to assess system quality in various fine-grained categories known to be problematic.", "labels": [], "entities": []}, {"text": "The main evaluation scores come from a semi-automatic process, combining automatic reference matching with extensive manual annotation of uncertain cases.", "labels": [], "entities": []}, {"text": "We find that current NMT systems are good at translating pronouns with intra-sentential reference , but the inter-sentential cases remain difficult.", "labels": [], "entities": []}, {"text": "NMT systems are also good at the translation of event pronouns, unlike systems from the phrase-based SMT paradigm.", "labels": [], "entities": [{"text": "translation of event pronouns", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.8616700321435928}, {"text": "SMT paradigm", "start_pos": 101, "end_pos": 113, "type": "TASK", "confidence": 0.8399937152862549}]}, {"text": "No single system performs best at translating all types of anaphoric pronouns, suggesting unexplained random effects influencing the translation of pronouns with NMT.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data-driven machine translation (MT) systems are very good at making translation choices based on the words in the immediate neighbourhood of the word currently being generated, but aspects of translation that require keeping track of long-distance dependencies continue to pose problems.", "labels": [], "entities": [{"text": "Data-driven machine translation (MT)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7787748376528422}]}, {"text": "Linguistically, long-distance dependencies often arise from discourse-level phenomena such as pronominal reference, lexical cohesion, text structure, etc.", "labels": [], "entities": []}, {"text": "Initially largely ignored, such problems have attracted increasing attention in the statistical MT (SMT) community in recent years.", "labels": [], "entities": [{"text": "MT (SMT)", "start_pos": 96, "end_pos": 104, "type": "TASK", "confidence": 0.8579048067331314}]}, {"text": "One important problem that has proved to be surprisingly difficult despite extensive research is the translation of pronouns (.", "labels": [], "entities": [{"text": "translation of pronouns", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.8707529505093893}]}, {"text": "* All authors contributed equally.", "labels": [], "entities": []}, {"text": "Since the invention of the BLEU score (), the MT community has measured progress to a large extent with the help of summary scores that are easy to compute, but strongly affected by the corpus-level frequency of certain phenomena, and that tend to neglect specific linguistic relations and problems that occur infrequently.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9754920601844788}, {"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9730292558670044}]}, {"text": "The advent of neural MT (NMT) with its improved capacity for modeling more complex relationships between linguistic elements has brought an increased interest in linguistic problems perceived as difficult, which are often not captured well by metrics like BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 256, "end_pos": 260, "type": "METRIC", "confidence": 0.8477681875228882}]}, {"text": "It has been suggested that test suites composed of difficult cases could provide more relevant insights into the performance of MT systems than corpus-level summary scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.9933633804321289}]}, {"text": "In this paper, we present a semi-automatic evaluation of the systems participating in the EnglishGerman news translation track of the MT shared task at the WMT 2018 conference.", "labels": [], "entities": [{"text": "EnglishGerman news translation track", "start_pos": 90, "end_pos": 126, "type": "TASK", "confidence": 0.7055559903383255}, {"text": "MT shared task", "start_pos": 134, "end_pos": 148, "type": "TASK", "confidence": 0.8039347728093466}, {"text": "WMT 2018 conference", "start_pos": 156, "end_pos": 175, "type": "DATASET", "confidence": 0.7597726186116537}]}, {"text": "The analysis was carried outwith the help of an English-German adaptation of the PROTEST test suite for pronoun translation).", "labels": [], "entities": [{"text": "PROTEST test suite", "start_pos": 81, "end_pos": 99, "type": "DATASET", "confidence": 0.813880463441213}, {"text": "pronoun translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.6573221981525421}]}, {"text": "The test suite allows us to perform a fine-grained evaluation for different types of pronouns.", "labels": [], "entities": []}, {"text": "Whilst the translation of event pronouns, which caused serious problems in earlier evaluations of SMT systems , seems to be handled fairly well by modern NMT systems, we find that translating anaphoric pronouns is still difficult, especially (but not only) if the pronoun has an antecedent in a different sentence.", "labels": [], "entities": [{"text": "translation of event pronouns", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.8234829008579254}, {"text": "SMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9854357838630676}]}, {"text": "Our results also confirm earlier findings that suggested the need fora careful evaluation that is sensitive to specific linguistic problems.", "labels": [], "entities": []}, {"text": "Whilst BLEU scores as a measure of general translation quality are strongly correlated with pronoun correctness, there are significant outliers that would be missed by an evaluation focusing on BLEU only.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9972347617149353}, {"text": "pronoun correctness", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7184312343597412}, {"text": "BLEU", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.9866256713867188}]}, {"text": "Moreover, evaluating pro-noun translations by comparison with a reference translation is not reliable for all types of pronouns ( . This fact limits the usefulness of automatic pronoun evaluation metrics such as APT) and affects the semi-automatic evaluation of our test suite as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation included 10 systems submitted to the English-German sub-task of the WMT 2018 competition and 6 anonymized online translation systems.", "labels": [], "entities": [{"text": "WMT 2018 competition", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.6281027396519979}]}, {"text": "Among the WMT submissions, all of the systems are neural models, with the Transformer (Vaswani et al., 2017) being a popular architecture choice.", "labels": [], "entities": [{"text": "WMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8061199188232422}]}, {"text": "Implementation details can be found in the system description papers published at WMT 2018.", "labels": [], "entities": [{"text": "WMT 2018", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.8676526248455048}]}, {"text": "We provide scores from two different automatic evaluation metrics for all systems in our dataset (see and).", "labels": [], "entities": []}, {"text": "To give a general impression of the translation quality achieved by the various systems, we include the BLEU scores on the TED talks from which the test suite is derived.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9993444085121155}, {"text": "TED talks", "start_pos": 123, "end_pos": 132, "type": "DATASET", "confidence": 0.8817223310470581}]}, {"text": "These scores differ from the BLEU scores of the official WMT evaluation because they are computed on a different test set, containing texts from, we did not define any \"equivalent\" pronouns in the APT metric, but counted exact matches only.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9990231990814209}, {"text": "WMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.7601253986358643}, {"text": "APT metric", "start_pos": 197, "end_pos": 207, "type": "DATASET", "confidence": 0.7764367759227753}]}, {"text": "A regression fit between the BLEU scores obtained and the number of examples annotated as correct by each system indicates a strong correlation between the two; r = 0.912, N = 16, p < 0.001), as does a similar analysis for the APT score (r = 0.887, N = 15, p < 0.001).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9954639077186584}, {"text": "APT score", "start_pos": 227, "end_pos": 236, "type": "DATASET", "confidence": 0.7316738665103912}]}, {"text": "These results, however, should betaken with a grain of salt, as we argue further in Section 5.", "labels": [], "entities": []}, {"text": "The semi-automatic evaluation method is a twopass procedure.", "labels": [], "entities": []}, {"text": "It is motivated by the observation that automatic reference-based methods can identify correct examples with relatively high precision, but low recall ( . The evaluation procedure relies on word alignments, which were generated automatically by running Giza++ () in both directions with grow-diag-final symmetrization ().", "labels": [], "entities": [{"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9814295172691345}, {"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9977719187736511}, {"text": "word alignments", "start_pos": 190, "end_pos": 205, "type": "TASK", "confidence": 0.6766879707574844}]}, {"text": "The word alignments for the examples in the reference translation were corrected manually.", "labels": [], "entities": []}, {"text": "In the first step, the candidate translations are matched against the reference translation to ap- prove examples that we can assume to be correct with reasonable confidence.", "labels": [], "entities": []}, {"text": "Examples in the event and pleonastic categories can be approved based on a pronoun match alone; for the anaphoric categories, we also require matching antecedent translations.", "labels": [], "entities": []}, {"text": "Two pronoun translations are considered to match if the sets of words aligned to the pronouns have at least one element in common after lowercasing.", "labels": [], "entities": []}, {"text": "For antecedent translations, the word sequences aligned to the source antecedent must be completely equal for an automatic match.", "labels": [], "entities": []}, {"text": "As a special exception, no automatic matches are generated for pronoun translations containing the word sie alone, so that the ambiguity between third-person plural sie and the pronoun of polite address Sie can be manually resolved.", "labels": [], "entities": [{"text": "pronoun translations containing the word sie", "start_pos": 63, "end_pos": 107, "type": "TASK", "confidence": 0.8093265295028687}]}, {"text": "In the second step, all examples not automatically approved are loaded into a graphical analysis tool specifically designed for the PROTEST test suite . The tool presents the annotator with the source pronoun, its translation by a given system, and the previous sentence for context.", "labels": [], "entities": [{"text": "PROTEST test suite", "start_pos": 132, "end_pos": 150, "type": "DATASET", "confidence": 0.8421000242233276}]}, {"text": "In the case of anaphoric pronouns, the context includes the sentence with the antecedent and one additional sentence.", "labels": [], "entities": []}, {"text": "The examples were split randomly over four annotators.", "labels": [], "entities": []}, {"text": "The annotators, who are translator trainees at Saar-  land University, are all native speakers of German with a good knowledge of English.", "labels": [], "entities": [{"text": "Saar-  land University", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.6954695284366608}]}, {"text": "To improve the quality of the annotations, the annotators had been trained beforehand on the output of a baseline NMT system.", "labels": [], "entities": []}, {"text": "In total, 3,200 pronoun examples from 16 systems were evaluated.", "labels": [], "entities": []}, {"text": "1,150 examples were approved automatically and 2,050 examples were referred for manual annotation.", "labels": [], "entities": []}, {"text": "To verify the validity of the semi-automatic method, we also solicited manual annotations fora random sample of 350 examples that had been approved automatically.", "labels": [], "entities": []}, {"text": "The first step of our two-step procedure can only approve examples, it never rejects them automatically.", "labels": [], "entities": []}, {"text": "As a consequence, our semi-automatic evaluation is biased towards correctness with respect to a fully manual evaluation.", "labels": [], "entities": []}, {"text": "The scores presented in will therefore tend to overestimate the actual system performance.", "labels": [], "entities": []}, {"text": "The results of the human annotation of the random sample of 320 examples automatically matched as correct are presented in.", "labels": [], "entities": []}, {"text": "Consistently with similar results for French (Hardmeier and Guillou, 2018), 86.6% of the automatically approved examples were accepted as correct by the evaluators.", "labels": [], "entities": []}, {"text": "However, we must highlight that the accuracy of the automatic evaluation varies substantially across categories.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9995142221450806}]}, {"text": "Whilst pronouns known to be pleonastic can be checked automatically with very good confidence, the automatic evaluation of anaphoric pronouns is much more difficult, with an evaluation accuracy as low as 55.2% in the intersentential subject it case.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9365905523300171}]}, {"text": "This reflects the general difficulty of automatic pronoun evaluation (  and reinforces the positive bias discussed in the previous paragraph for these categories in particular.", "labels": [], "entities": [{"text": "automatic pronoun evaluation", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6138606270154318}]}, {"text": "The results of the semi-automatic evaluation are displayed in.", "labels": [], "entities": []}, {"text": "For the counts in this table, we used manual annotations wherever possible.", "labels": [], "entities": []}, {"text": "Automatic annotations were used only for those examples that had not been annotated manually.", "labels": [], "entities": []}, {"text": "The best result was obtained by the MicrosoftMarian system, which translated 157 out of 200 pronouns correctly.", "labels": [], "entities": [{"text": "MicrosoftMarian", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9272223114967346}]}, {"text": "It is followed by a group of 5 shared task submissions that achieved scores between 145 and 148.", "labels": [], "entities": []}, {"text": "Three of the online systems also reached scores over 140.", "labels": [], "entities": []}, {"text": "The remaining shared task submissions are JHU with a score of 132 and LMU-nmt with a score of 127.", "labels": [], "entities": [{"text": "JHU", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.7259684205055237}]}, {"text": "Unsurprisingly, the unsupervised submissions are ranked last.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic evaluation results.", "labels": [], "entities": []}, {"text": " Table 2: Human evaluation of automatically approved  examples", "labels": [], "entities": []}, {"text": " Table 3: Pronoun and antecedent translations marked as correct, per system", "labels": [], "entities": []}]}