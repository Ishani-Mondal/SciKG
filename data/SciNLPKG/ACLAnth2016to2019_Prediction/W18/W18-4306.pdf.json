{"title": [{"text": "Crowdsourcing StoryLines: Harnessing the Crowd for Causal Relation Annotation", "labels": [], "entities": [{"text": "Causal Relation Annotation", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.7875029643376669}]}], "abstractContent": [{"text": "This paper describes a crowdsourcing experiment on the annotation of plot-like structures in En-glish news articles.", "labels": [], "entities": [{"text": "annotation of plot-like structures in En-glish news articles", "start_pos": 55, "end_pos": 115, "type": "TASK", "confidence": 0.68158158659935}]}, {"text": "The CrowdTruth methodology and metrics have been applied to select valid annotations from the crowd.", "labels": [], "entities": []}, {"text": "We further run an in-depth analysis of the annotated data by comparing it with available expert data.", "labels": [], "entities": []}, {"text": "Our results show a valuable use of crowdsourcing annotations for such complex semantic tasks, and promote anew annotation approach that combines crowd and experts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Causal relations area pervasive phenomenon inhuman activities, including narrative production.", "labels": [], "entities": []}, {"text": "Causality is actually the main component of narratives, regardless of the mediums (novels, news articles, comments, micro-blogs, pictures, among others) and their fictional status (fictional vs. non-fictional narratives).", "labels": [], "entities": []}, {"text": "Ina narrative text, causal connections between events allow the story to progress, the actors to participate, and eventually reach a conclusion.", "labels": [], "entities": []}, {"text": "Causality is responsible for logically connecting the events together in a meaningful way.", "labels": [], "entities": []}, {"text": "If we shift perspective, and look at narratives from the point of view of the producers rather than their structural properties, it is easy to observe how humans impose causal, or explanatory, relations among events that they perceive or are involved into.", "labels": [], "entities": []}, {"text": "Humans have a great appetite for information and are inconstant need to find explanations for the things they observe.", "labels": [], "entities": []}, {"text": "We search the present for cues and evidence, merge and resolve information with what we already known (i.e., the past), and use this information to (try to) predict the future and make decisions.", "labels": [], "entities": []}, {"text": "Explanatory relations and narrative strategies are one of the major cognitive tools we use to observe the world and, most importantly, to interpret it.", "labels": [], "entities": []}, {"text": "When reporting on an event in the world, or telling someone a personal experience, we do not merely describe what happens, i.e., we do not just list events in the order of occurrence 1 , but we connect them in a set of coherent patterns, or, in other words, we give rise to plot structures (.", "labels": [], "entities": []}, {"text": "Plot structures express a form of reasoning about causal relations between events and states composing the narrative.", "labels": [], "entities": []}, {"text": "The current stream of data and information is growing everyday and its size and complexity is such that humans may suffer from \"information overload\".", "labels": [], "entities": []}, {"text": "To minimise such a problem, intelligent content management systems have been developed and they became more and more popular and used.", "labels": [], "entities": []}, {"text": "Different methods and approaches have been developed to provide users with personalised and relevant information.", "labels": [], "entities": []}, {"text": "However, most of this information is given in the form of full text documents that require the users to read them to identify (i.e., extract) the information.", "labels": [], "entities": []}, {"text": "Automatic processing would be beneficial, especially if the results are presented as structured data based on narrative strategies.", "labels": [], "entities": []}, {"text": "We follow, in this This work is licenced under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": []}, {"text": "Licence details: http:// creativecommons.org/licenses/by/4.0/ respect, the proposal of automatically generating storylines of events (.", "labels": [], "entities": []}, {"text": "This paper reports on a crowdsourcing experiment on the annotation of causal relations between pairs of events in news data.", "labels": [], "entities": []}, {"text": "The main contributions of this work are: \u2022 an analysis of the crowdsourced data, in terms of parameters that may affect the annotation quality, time, and evaluation of the data using the CrowdTruth methodology (; \u2022 a comparison between experts and crowdsourced annotated data with respect to a publicly available reference benchmark corpus for storyline evaluation, the Event StoryLine Corpus (ESC)); \u2022 the release of an enhanced version of the Event Storyline Corpus (ESC v1.2).", "labels": [], "entities": [{"text": "Event Storyline Corpus (ESC v1.2)", "start_pos": 445, "end_pos": 478, "type": "DATASET", "confidence": 0.8801936677524022}]}, {"text": "The remainder of the paper is organised as follows: Section 2 provides an overview of related work on the annotation of causal relations in different datasets, highlighting differences and commonalities with our contribution.", "labels": [], "entities": []}, {"text": "Section 3 describes the dataset and the crowdsourcing experiment settings based on the CrowdTruth metrics.", "labels": [], "entities": []}, {"text": "Section 4 reports on an in-depth analysis of the crowd data and its comparison with existing expert annotated data.", "labels": [], "entities": []}, {"text": "Finally, Section 5 summarises our findings and suggests directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experimental dataset covers 22 topics from the Event StoryLine Corpus v1.0 (ESC v1.0).", "labels": [], "entities": [{"text": "Event StoryLine Corpus v1.0 (ESC v1.0)", "start_pos": 51, "end_pos": 89, "type": "DATASET", "confidence": 0.8142350018024445}]}, {"text": "The ESC corpus contains expert annotations that cover a high range of entities and relations such as: actors, locations, temporal expressions, events, temporal relations, event coreference relations, and plot-like relations between pairs of events.", "labels": [], "entities": [{"text": "ESC corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8575347363948822}]}, {"text": "The plot relations in the ESC data are marked with a <PLOT LINK> tag, and broadly correspond to contingent relations between pairs of events.", "labels": [], "entities": [{"text": "ESC data", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.875244140625}, {"text": "PLOT LINK> tag", "start_pos": 54, "end_pos": 68, "type": "METRIC", "confidence": 0.9244901835918427}]}, {"text": "The annotation of these links is based on relatively simple annotation guidelines, instructing the annotators in the identification of the eligible pairs of events and associated relation (i.e., relation directionality).", "labels": [], "entities": []}, {"text": "The inter-annotator agreement for <PLOT LINK> has been calculated using the Dice coefficient and equals 0.638.", "labels": [], "entities": [{"text": "PLOT LINK>", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.8436268369356791}, {"text": "Dice coefficient", "start_pos": 76, "end_pos": 92, "type": "METRIC", "confidence": 0.6928553283214569}]}, {"text": "ESC consists of 22 topics, fora total of 281 news articles.", "labels": [], "entities": [{"text": "ESC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9461557865142822}]}, {"text": "We extracted 1,204 annotated sentences containing at least two expert annotated events.", "labels": [], "entities": []}, {"text": "Following the approach of the ESC corpus, we have excluded events belonging to the following classes from the event pairs: ASPECTUAL, REPORTING, CAUSATIVE, and GENERIC.", "labels": [], "entities": [{"text": "ESC corpus", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.8752712607383728}, {"text": "ASPECTUAL", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9384817481040955}, {"text": "REPORTING", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9737955331802368}, {"text": "GENERIC", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.7410707473754883}]}, {"text": "These classes actually represent sets of event mentions which cannot give rise to a plot-like structure, or a contingent relation.", "labels": [], "entities": []}, {"text": "For instance, on the one hand, in the case of a REPORTING event (e.g. say, report), a plot-like relation holds with respect to the actual content of what is \"reported\" rather than between the marker of the presence of a reporting event.", "labels": [], "entities": []}, {"text": "On the other hand, CAUSATIVE events (e.g. cause, sparkle, trigger) have been excluded as they are interpreted as explicit markers of a causal relation.", "labels": [], "entities": [{"text": "CAUSATIVE events (e.g. cause, sparkle, trigger)", "start_pos": 19, "end_pos": 66, "type": "TASK", "confidence": 0.6140864193439484}]}, {"text": "The actual plot relation holds between their arguments.", "labels": [], "entities": []}, {"text": "An overview of the dataset is shown in.", "labels": [], "entities": []}, {"text": "The ESC dataset contains 2,290 manually annotated <PLOT LINK> relations between event pairs.", "labels": [], "entities": [{"text": "ESC dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9289932548999786}, {"text": "PLOT LINK>", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9004594286282858}]}, {"text": "This set of relations is then expanded to 5,684 pairs when using coreference relations.", "labels": [], "entities": []}, {"text": "As for the manually annotated pairs, only 1,571 out of 2,290 (68.6%) occur in the same sentence.", "labels": [], "entities": []}, {"text": "In the 1,204 sentences that we selected in our experiments, there are only 1,540 expert annotated event pairs, that are further used in our analysis (Section 4).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The ESC dataset contains 2,290 manually annotated <PLOT LINK> rela- tions between event pairs. This set of relations is then expanded to 5,684 pairs when using coreference  relations. As for the manually annotated pairs, only 1,571 out of 2,290 (68.6%) occur in the same sen- tence. In the 1,204 sentences that we selected in our experiments, there are only 1,540 expert annotated  event pairs, that are further used in our analysis (Section 4).", "labels": [], "entities": [{"text": "ESC dataset", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.9294524192810059}, {"text": "PLOT LINK> rela-", "start_pos": 61, "end_pos": 77, "type": "METRIC", "confidence": 0.9052248358726501}]}, {"text": " Table 2: Overview of crowdsourcing experiments to derive optimal annotation settings and template", "labels": [], "entities": []}, {"text": " Table 3: Comparing Experts and Crowd: Causal Relation Identification", "labels": [], "entities": [{"text": "Causal Relation Identification", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.788307269414266}]}, {"text": " Table 4: Comparing Experts and Crowd: Event Pairs Detection (only)", "labels": [], "entities": [{"text": "Event Pairs Detection", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.6459329128265381}]}]}