{"title": [{"text": "Embedding Individual Table Columns for Resilient SQL Chatbots", "labels": [], "entities": []}], "abstractContent": [{"text": "Most of the world's data is stored in relational databases.", "labels": [], "entities": []}, {"text": "Accessing these requires specialized knowledge of the Structured Query Language (SQL), putting them out of the reach of many people.", "labels": [], "entities": []}, {"text": "A recent research thread in Natural Language Processing (NLP) aims to alleviate this problem by automatically translating natural language questions into SQL queries.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.7116231024265289}]}, {"text": "While the proposed solutions area great start, they lack robustness and do not easily generalize: the methods require high quality descriptions of the database table columns, and the most widely used training dataset, WikiSQL, is heavily biased towards using those descriptions as part of the questions.", "labels": [], "entities": []}, {"text": "In this work, we propose solutions to both problems: we entirely eliminate the need for column descriptions, by relying solely on their contents, and we augment the WikiSQL dataset by paraphrasing column names to reduce bias.", "labels": [], "entities": [{"text": "WikiSQL dataset", "start_pos": 165, "end_pos": 180, "type": "DATASET", "confidence": 0.9011797606945038}]}, {"text": "We show that the accuracy of existing methods drops when trained on our augmented , column-agnostic dataset, and that our own method reaches state of the art accuracy, while relying on column contents only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9992560744285583}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9974473714828491}]}], "introductionContent": [{"text": "Recent developments in Natural Language Understanding (NLU) have led to a big proliferation of text-and speech-based bot interfaces.", "labels": [], "entities": [{"text": "Natural Language Understanding (NLU)", "start_pos": 23, "end_pos": 59, "type": "TASK", "confidence": 0.8100539445877075}]}, {"text": "Home appliances, such as smart speakers and chatbots, rely mostly on a well-structured knowledge base or an external Application Programming Interface (API) to provide the desired response.", "labels": [], "entities": []}, {"text": "This limits the usability of such systems in a context where the data is stored in a (local) relational database.", "labels": [], "entities": []}, {"text": "This constraint led to the development of text to Structured Query Language (SQL) systems, also known as SQL bots.", "labels": [], "entities": []}, {"text": "Given a question, in natural 1 equal contribution language, pertaining to a certain database table, these bots will automatically generate the corresponding SQL query and return the requested data.", "labels": [], "entities": []}, {"text": "Considering the vast usage of relational databases on the internet and in private companies, SQL bots area simple new interface that enables nontechnical people to access data.", "labels": [], "entities": []}, {"text": "The first approaches in the field relied on parsers and pattern-matching rules to understand the question and produce appropriate answers (.", "labels": [], "entities": []}, {"text": "Later developments introduced semantic grammar systems and intermediate language systems.", "labels": [], "entities": []}, {"text": "More recently, new NLU methods, such as pointer-networks, pushed the state-ofthe-art results in several domains, including parsing (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 123, "end_pos": 130, "type": "TASK", "confidence": 0.9680572152137756}]}, {"text": "Current state-of-the-art models are based on sketches and have primarily two inputs: the question and the descriptions of the table columns (i.e., the column names).", "labels": [], "entities": []}, {"text": "Relying on the column names is limiting, since the whole model is based on several strong premises: (a) the names are high quality and descriptive enough; (b) the names do not change; (c) the names are known to the user of the bot.", "labels": [], "entities": []}, {"text": "These are very strong assumptions: often, column names do not even exist (i.e., the generic col1, col2, etc. are used instead).", "labels": [], "entities": []}, {"text": "Moreover, if as we observe in, a column contains the names of colleges, just changing the column name form \"College\" to \"School\" does not make the content any less informative.", "labels": [], "entities": []}, {"text": "The expectation from a bot is that their quality is not sensitive to cosmetic changes to the underlying table.", "labels": [], "entities": []}, {"text": "Finally, users do not necessarily know the structure of the table, let alone the column names.", "labels": [], "entities": []}, {"text": "In this paper, we build and present ICE (Individual Column Embeddings) -a novel approach of representing the database table columns, by using their contents instead of their names.", "labels": [], "entities": []}, {"text": "To do so, we construct a column embedding vector space, where we embed the columns.", "labels": [], "entities": []}, {"text": "This embedding is then used as a substitute for the encoding of the column descriptions (headers) in a state of the art sketch-based model.", "labels": [], "entities": []}, {"text": "In addition, to empirically show the value of using ICE, we generate anew, column-agnostic dataset based on the widely used WikiSQL dataset (.", "labels": [], "entities": [{"text": "WikiSQL dataset", "start_pos": 124, "end_pos": 139, "type": "DATASET", "confidence": 0.8949257433414459}]}, {"text": "In WikiSQL, a substantial bias towards the inclusion in the question of the column name is built-in.", "labels": [], "entities": []}, {"text": "For instance more than 79% of questions contain the name of the column that needs to be selected.", "labels": [], "entities": []}, {"text": "Additionally around 59% contain the names of all columns form the SQL where clause.", "labels": [], "entities": []}, {"text": "With ICE, we are eliminating the strong assumption that the users have access to the table structure.", "labels": [], "entities": []}, {"text": "Hence, we also need a less biased dataset to show the value of our method.", "labels": [], "entities": []}, {"text": "We thus create an open source data augmentation tool to paraphrase part of the questions in WikiSQL: where the column names are present, we replace them with similar expressions (e.g., synonyms), removing some of the built-in bias.", "labels": [], "entities": []}, {"text": "We train and test our ICE-based model on both the original WikiSQL dataset and our columnagnostic version of the dataset.", "labels": [], "entities": [{"text": "WikiSQL dataset", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.8967184722423553}]}, {"text": "We show that we maintain the same accuracy on both datasets with all three tasks: aggregation, column-selection and where clause generation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9993411898612976}, {"text": "where clause generation", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.7171466946601868}]}, {"text": "We also train the original SQLNet () model on the column-agnostic dataset and find a 7% accuracy drop in the where clause generation task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9991266131401062}, {"text": "where clause generation", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.7238813042640686}]}, {"text": "Ina nutshell, the most important contribution of this work is that we improve the model resilience by limiting its reliance on arbitrary descriptions of the data within the tables.", "labels": [], "entities": []}, {"text": "In addition, we expand the applicability of SQL bots to users who do not know the internal structure of the databases they are trying to access.", "labels": [], "entities": []}, {"text": "By eliminating the need to encode the column headers, we also reduce the overall complexity of the model.", "labels": [], "entities": []}, {"text": "This is achieved by removing the LSTM networks used to generate unique column header encodings for the aggregation prediction, selection prediction and where clause generation.", "labels": [], "entities": [{"text": "selection prediction", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.8187078535556793}, {"text": "clause generation", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.7269036024808884}]}, {"text": "The paper is organized as follows: Section 2 presents the related work for translating sentences to SQL and for vector space embeddings.", "labels": [], "entities": []}, {"text": "In Section 3, we describe ICE -our method for column content embeddings.", "labels": [], "entities": [{"text": "ICE", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.5936620235443115}]}, {"text": "In the next section, we introduce our column-agnostic model for translating sentences to SQL.", "labels": [], "entities": []}, {"text": "We present the evaluation results in Section 5 and finally conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation on the full original WikiSQL dataset determines whether the individual column embeddings are suitable replacements for headers when the column name appears in the question.", "labels": [], "entities": [{"text": "WikiSQL dataset", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.8755390644073486}]}, {"text": "summarizes the results of our model SQLNet+ICE and compares them with the results of two baselines: SQLNet and Seq2SQL.", "labels": [], "entities": []}, {"text": "We portray the accuracy values on the development and test sets for the three slots we fill in the sketch: Aggregation function, Column Selection and Where clause generation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9992161989212036}, {"text": "Aggregation", "start_pos": 107, "end_pos": 118, "type": "METRIC", "confidence": 0.9102294445037842}, {"text": "Where clause generation", "start_pos": 150, "end_pos": 173, "type": "TASK", "confidence": 0.5904269516468048}]}, {"text": "We observe that SQLNet+ICE performs similarly to the original SQLNet model in both cases and superior to Seq2SQL.", "labels": [], "entities": []}, {"text": "This result shows that we can build an equally performing model that is resilient to changes to the DB schema or complete absence of knowledge about it.", "labels": [], "entities": []}, {"text": "We note that the accuracy of the aggregation function also changes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9996832609176636}]}, {"text": "This happens because the aggregation classifier has either the column or header embeddings as inputs, as shown in 2.", "labels": [], "entities": []}, {"text": "There is a small decrease of accuracy for the Aggregation and Where clauses, while the accuracy on the Column Selection performs slightly better.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9994502663612366}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9996129870414734}]}, {"text": "These results are expected, as the queries strongly rely on the direct column names mentions.", "labels": [], "entities": []}, {"text": "The second experiment shows the more realistic results, obtained on the column-agnostic WikiSQL Dataset.", "labels": [], "entities": [{"text": "WikiSQL Dataset", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.9436286687850952}]}, {"text": "The results in show that SQLNet struggles to predict correctly the column related    parts of the query, especially in the case of the where clause generation.", "labels": [], "entities": [{"text": "where clause generation", "start_pos": 135, "end_pos": 158, "type": "TASK", "confidence": 0.7595622340838114}]}, {"text": "This drop in the accuracy is expected, since the where clause predictor is the most complex part of the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9996594190597534}]}, {"text": "Without the original dataset bias where the column names were present in the questions, the column names are not descriptive enough.This leads to a drop of 10.7% on the validation and 8.8% on test dataset.", "labels": [], "entities": []}, {"text": "On the other hand, our model is capable of overcoming this situation and find the queries with a much smaller drop of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.997365415096283}]}, {"text": "Although the performance is also worse than with the original dataset, the accuracy obtained using SQLNet with individual column embeddings in the where clauses is only 2.1% lower in validation and 1.7% in test.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9994416832923889}]}, {"text": "Using individual column embeddings makes the SQLNet model more versatile, as it can address the scenario where the user is not aware of the table structure.", "labels": [], "entities": []}, {"text": "To better understand our results on the Column-agnostic WikiSQL dataset we run the evaluation just with questions that have been rephrased, which represent around 20% of the whole data set, as shown in. summarizes these results, with SQLNet is the original model described in ().", "labels": [], "entities": [{"text": "Column-agnostic WikiSQL dataset", "start_pos": 40, "end_pos": 71, "type": "DATASET", "confidence": 0.7368254065513611}]}, {"text": "The previously seen drop in SQLNet accuracy on the column selection and whereclause predictions is exacerbated -showing that indeed the paraphrasing is indeed the root cause.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9892123937606812}]}, {"text": "This effect is comparatively mild in SQLNet + ICE.", "labels": [], "entities": [{"text": "SQLNet + ICE", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.5720860759417216}]}], "tableCaptions": [{"text": " Table 2: Model accuracies on the Original WikiSQL Dataset", "labels": [], "entities": [{"text": "Original WikiSQL Dataset", "start_pos": 34, "end_pos": 58, "type": "DATASET", "confidence": 0.6384037931760153}]}, {"text": " Table 3: Model accuracies on the Column-agnostic WikiSQL Dataset", "labels": [], "entities": [{"text": "Column-agnostic WikiSQL Dataset", "start_pos": 34, "end_pos": 65, "type": "DATASET", "confidence": 0.6299876073996226}]}, {"text": " Table 4: Model accuracies on the paraphrased ques- tions only on Aggreation, Selection and Where-clause  tasks.", "labels": [], "entities": []}]}