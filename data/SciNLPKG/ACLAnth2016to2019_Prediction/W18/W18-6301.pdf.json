{"title": [], "abstractContent": [{"text": "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine.", "labels": [], "entities": []}, {"text": "This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.999269425868988}]}, {"text": "1 On WMT'14 English-German translation, we match the accuracy of Vaswani et al.", "labels": [], "entities": [{"text": "WMT'14 English-German translation", "start_pos": 5, "end_pos": 38, "type": "DATASET", "confidence": 0.9385824799537659}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9994385838508606}]}, {"text": "(2017) in under 5 hours when training on 8 GPUs and we obtain anew state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9867942929267883}]}, {"text": "We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9950985312461853}, {"text": "Paracrawl dataset", "start_pos": 77, "end_pos": 94, "type": "DATASET", "confidence": 0.9835849702358246}]}, {"text": "On the WMT'14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.", "labels": [], "entities": [{"text": "WMT'14 English-French task", "start_pos": 7, "end_pos": 33, "type": "DATASET", "confidence": 0.7807265122731527}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9995978474617004}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has seen impressive progress in the recent years with the introduction of evermore efficient architectures.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8441080451011658}]}, {"text": "Similar sequence-to-sequence models are also applied to other natural language processing tasks, such as abstractive summarization () and dialog (.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.6634010374546051}]}, {"text": "Currently, training state-of-the-art models on large datasets is computationally intensive and can require several days on a machine with 8 highend graphics processing units (GPUs).", "labels": [], "entities": []}, {"text": "Scaling training to multiple machines enables faster experimental turn-around but also introduces new challenges: How do we maintain efficiency in a distributed setup when some batches process faster *Work done while at Facebook AI Research.", "labels": [], "entities": []}, {"text": "Our implementation is available at: https://www.github.com/pytorch/fairseq than others (i.e., in the presence of stragglers)?", "labels": [], "entities": []}, {"text": "How do larger batch sizes affect optimization and generalization performance?", "labels": [], "entities": []}, {"text": "While stragglers primarily affect multi-machine training, questions about the effectiveness of large batch training are relevant even for users of commodity hardware on a single machine, especially as such hardware continues to improve, enabling bigger models and batch sizes.", "labels": [], "entities": []}, {"text": "In this paper, we first explore approaches to improve training efficiency on a single machine.", "labels": [], "entities": []}, {"text": "By training with reduced floating point precision we decrease training time by 65% with no effect on accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9375311732292175}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9994183778762817}]}, {"text": "Next, we assess the effect of dramatically increasing the batch size from 25k to over 400k tokens, a necessary condition for large scale parallelization with synchronous training.", "labels": [], "entities": []}, {"text": "We implement this on a single machine by accumulating gradients from several batches before each update.", "labels": [], "entities": []}, {"text": "We find that by training with large batches and by increasing the learning rate we can further reduce training time by 40% on a single machine.", "labels": [], "entities": []}, {"text": "Finally, we parallelize training across 16 machines and find that we can reduce training time by an additional 90% compared to a single machine.", "labels": [], "entities": []}, {"text": "Our improvements enable training a Transformer model on the WMT'16 En-De dataset to the same accuracy as   Training with large batches is less data-efficient, but can be parallelized.", "labels": [], "entities": [{"text": "WMT'16 En-De dataset", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.9441919922828674}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9989007711410522}]}, {"text": "Batch sizes given in number of target tokens excluding padding.", "labels": [], "entities": []}], "datasetContent": [{"text": "We run experiments on two language pairs, English to German (En-De) and English to French (En-Fr).", "labels": [], "entities": []}, {"text": "For En-De we replicate the setup of which relies on the WMT'16 training data with 4.5M sentence pairs; we validate on newstest13 and test on newstest14.", "labels": [], "entities": [{"text": "WMT'16 training data", "start_pos": 56, "end_pos": 76, "type": "DATASET", "confidence": 0.9310500820477804}, {"text": "newstest13", "start_pos": 118, "end_pos": 128, "type": "DATASET", "confidence": 0.9617588520050049}, {"text": "newstest14", "start_pos": 141, "end_pos": 151, "type": "DATASET", "confidence": 0.9700220227241516}]}, {"text": "We use a vocabulary of 32K symbols based on a joint source and target byte pair encoding (BPE;).", "labels": [], "entities": []}, {"text": "For En-Fr, we train on WMT'14 and borrow the setup of with 36M training sentence pairs.", "labels": [], "entities": [{"text": "WMT'14", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.9717231392860413}]}, {"text": "We use newstest12+13 for validation and newstest14 for test.", "labels": [], "entities": [{"text": "newstest14", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8773802518844604}]}, {"text": "The 40K vocabulary is based on a joint source and target BPE factorization.", "labels": [], "entities": [{"text": "BPE", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.6546881794929504}]}, {"text": "We also experiment with scaling training beyond 36M sentence pairs by using data from the Paracrawl corpus.", "labels": [], "entities": [{"text": "Paracrawl corpus", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.9831803441047668}]}, {"text": "This dataset is extremely large with more than 4.5B pairs for En-De and more than 4.2B pairs for En-Fr.", "labels": [], "entities": []}, {"text": "We rely on the BPE vocabulary built on WMT data for each language pair and explore filtering this noisy dataset in Section 4.5.", "labels": [], "entities": [{"text": "WMT data", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.8636611998081207}]}, {"text": "We measure case-sensitive tokenized BLEU with multi-bleu.pl 2 and de-tokenized BLEU with SacreBLEU 3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.961707592010498}, {"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.985077440738678}]}, {"text": "All results use beam search with abeam width of 4 and length penalty of 0.6, following.", "labels": [], "entities": [{"text": "length penalty", "start_pos": 54, "end_pos": 68, "type": "METRIC", "confidence": 0.9848303198814392}]}, {"text": "Checkpoint averaging is not used, except where specified otherwise.", "labels": [], "entities": [{"text": "Checkpoint averaging", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6434946209192276}]}, {"text": "In this section we present results for improving training efficiency via reduced precision floating point (Section 4.1), training with larger batches (Section 4.2), and training with multiple nodes in a distributed setting (Section 4.3).", "labels": [], "entities": [{"text": "precision floating point", "start_pos": 81, "end_pos": 105, "type": "METRIC", "confidence": 0.92423548301061}]}], "tableCaptions": [{"text": " Table 3: Test BLEU (newstest14) when training  with WMT+Paracrawl data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.969983696937561}, {"text": "WMT+Paracrawl data", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.8498181998729706}]}]}