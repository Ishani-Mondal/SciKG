{"title": [{"text": "Contextual Encoding for Translation Quality Estimation", "labels": [], "entities": [{"text": "Contextual Encoding", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8045690953731537}, {"text": "Translation Quality Estimation", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.8980591297149658}]}], "abstractContent": [{"text": "The task of word-level quality estimation (QE) consists of taking a source sentence and machine-generated translation, and predicting which words in the output are correct and which are wrong.", "labels": [], "entities": [{"text": "word-level quality estimation (QE)", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.7853880822658539}]}, {"text": "In this paper, propose a method to effectively encode the local and global contextual information for each target word using a three-part neural network approach.", "labels": [], "entities": []}, {"text": "The first part uses an embedding layer to represent words and their part-of-speech tags in both languages.", "labels": [], "entities": []}, {"text": "The second part leverages a one-dimensional convolution layer to integrate local context information for each target word.", "labels": [], "entities": []}, {"text": "The third part applies a stack of feed-forward and recurrent neural networks to further encode the global context in the sentence before making the predictions.", "labels": [], "entities": []}, {"text": "This model was submitted as the CMU entry to the WMT2018 shared task on QE, and achieves strong results, ranking first in three of the six tracks.", "labels": [], "entities": [{"text": "WMT2018 shared task on QE", "start_pos": 49, "end_pos": 74, "type": "DATASET", "confidence": 0.7388198852539063}]}], "introductionContent": [{"text": "Quality estimation (QE) refers to the task of measuring the quality of machine translation (MT) system outputs without reference to the gold translations (.", "labels": [], "entities": [{"text": "Quality estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7868357837200165}, {"text": "machine translation (MT) system outputs", "start_pos": 71, "end_pos": 110, "type": "TASK", "confidence": 0.7345441324370248}]}, {"text": "QE research has grown increasingly popular due to the improved quality of MT systems, and potential for reductions in post-editing time and the corresponding savings in labor costs).", "labels": [], "entities": [{"text": "MT", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.9821911454200745}]}, {"text": "QE can be performed on multiple granularities, including at word level, sentence level, or document level.", "labels": [], "entities": [{"text": "QE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7630127668380737}]}, {"text": "In this paper, we focus on quality estimation at word level, which is framed as the task of performing binary classification of translated tokens, assigning \"OK\" or \"BAD\" labels.", "labels": [], "entities": [{"text": "BAD", "start_pos": 166, "end_pos": 169, "type": "METRIC", "confidence": 0.8592311143875122}]}, {"text": "Our software is available at https://github.com/ junjiehu/CEQE.", "labels": [], "entities": []}, {"text": "Early work on this problem mainly focused on hand-crafted features with simple regression/classification models.", "labels": [], "entities": []}, {"text": "Recent papers have demonstrated that utilizing recurrent neural networks (RNN) can result in large gains in QE performance.", "labels": [], "entities": [{"text": "QE", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.8915082216262817}]}, {"text": "However, these approaches encode the context of the target word by merely concatenating its left and right context words, giving them limited ability to control the interaction between the local context and the target word.", "labels": [], "entities": []}, {"text": "In this paper, we propose a neural architecture, Context Encoding Quality Estimation (CEQE), for better encoding of context in word-level QE.", "labels": [], "entities": []}, {"text": "Specifically, we leverage the power of both (1) convolution modules that automatically learn local patterns of surrounding words, and (2) handcrafted features that allow the model to make more robust predictions in the face of a paucity of labeled data.", "labels": [], "entities": []}, {"text": "Moreover, we further utilize stacked recurrent neural networks to capture the long-term dependencies and global context information from the whole sentence.", "labels": [], "entities": []}, {"text": "We tested our model on the official benchmark of the WMT18 word-level QE task.", "labels": [], "entities": [{"text": "WMT18 word-level QE task", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.5099496617913246}]}, {"text": "On this task, it achieved highly competitive results, with the best performance over other competitors on English-Czech, English-Latvian (NMT) and English-Latvian (SMT) word-level QE task, and ranking second place on English-German (NMT) and German-English word-level QE task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our CEQE model on the WMT2018 Quality Estimation Shared Task 3 for wordlevel English-German, German-English, EnglishCzech, and English-Latvian QE.", "labels": [], "entities": [{"text": "WMT2018 Quality Estimation Shared Task 3", "start_pos": 34, "end_pos": 74, "type": "DATASET", "confidence": 0.7300977607568105}]}, {"text": "Words in all languages are lowercased.", "labels": [], "entities": []}, {"text": "The evaluation metric is the multiplication of F1-scores for the \"OK\" and \"BAD\" classes against the true labels.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9952385425567627}, {"text": "BAD", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.8955205678939819}]}, {"text": "F1-score is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9842601418495178}, {"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9995085000991821}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9963105320930481}]}, {"text": "In Table 3, our model achieves the best performance on three out of six test sets in the WMT 2018 wordlevel QE shared task.", "labels": [], "entities": [{"text": "WMT 2018 wordlevel QE shared task", "start_pos": 89, "end_pos": 122, "type": "DATASET", "confidence": 0.7762702107429504}]}], "tableCaptions": [{"text": " Table 1: Statistics of POS tags over all language pairs", "labels": [], "entities": []}, {"text": " Table 3: Best performance of our model on six datasets  in the WMT2018 word-level QE shared task on the  leader board (updated on July 27th 2018)", "labels": [], "entities": [{"text": "WMT2018 word-level QE shared task", "start_pos": 64, "end_pos": 97, "type": "TASK", "confidence": 0.6527991771697998}]}, {"text": " Table 4: Ablation study on the WMT18 Test Set", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9944062232971191}, {"text": "WMT18 Test Set", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.943827211856842}]}]}