{"title": [{"text": "A Review of Standard Text Classification Practices for Multi-label Toxicity Identification of Online Content", "labels": [], "entities": [{"text": "Standard Text Classification", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6637503306070963}, {"text": "Multi-label Toxicity Identification of Online Content", "start_pos": 55, "end_pos": 108, "type": "TASK", "confidence": 0.812122235695521}]}], "abstractContent": [{"text": "Language toxicity identification presents a gray area in the ethical debate surrounding freedom of speech and censorship.", "labels": [], "entities": [{"text": "Language toxicity identification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.664861818154653}]}, {"text": "To-day's social media landscape is littered with unfiltered content that can be anywhere from slightly abusive to hate inducing.", "labels": [], "entities": []}, {"text": "In response, we focused on training a multi-label classifier to detect both the type and level of toxicity in online content.", "labels": [], "entities": []}, {"text": "This content is typically colloquial and conversational in style.", "labels": [], "entities": []}, {"text": "Its classification therefore requires huge amounts of annotated data due to its variability and inconsistency.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9607361555099487}]}, {"text": "We compare standard methods of text classification in this task.", "labels": [], "entities": [{"text": "text classification", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7881907820701599}]}, {"text": "A conventional one-vs-rest SVM classifier with character and word level frequency-based representation of text reaches 0.9763 ROC AUC score.", "labels": [], "entities": [{"text": "ROC AUC score", "start_pos": 126, "end_pos": 139, "type": "METRIC", "confidence": 0.8996634682019552}]}, {"text": "We demonstrated that lever-aging more advanced technologies such as word embeddings, recurrent neural networks , attention mechanism, stacking of classifiers and semi-supervised training can improve the ROC AUC score of classification to 0.9862.", "labels": [], "entities": [{"text": "ROC AUC score", "start_pos": 203, "end_pos": 216, "type": "METRIC", "confidence": 0.5034653842449188}]}, {"text": "We suggest that in order to choose the right model one has to consider the accuracy of models as well as inference complexity based on the application .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9989591836929321}]}], "introductionContent": [{"text": "While the sheer volume of online content presents a major challenge in information management, we are equally plagued by our current inability to effectively monitor its contents.", "labels": [], "entities": [{"text": "information management", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.7911317944526672}]}, {"text": "In particular, social media platforms are ridden with verbal abuse, giving way to an increasingly unsafe and highly offensive online environment.", "labels": [], "entities": []}, {"text": "With the threat of sanctions and user turnover, governments and social media platforms currently have huge incentives to create systems that accurately detect and remove abusive content.", "labels": [], "entities": []}, {"text": "When considering possible solutions, the binary classification of online data, as simply toxic and non-toxic content, can be very problematic.", "labels": [], "entities": []}, {"text": "Even with very low error rates of misclassification, the removal of said flagged conversations can impact a user's reputation or freedom of speech.", "labels": [], "entities": []}, {"text": "Developing classifiers that can flag the type and likelihood of toxic content is afar better approach.", "labels": [], "entities": []}, {"text": "It empowers users and online platforms to control their content based on provided metrics and calculated thresholds.", "labels": [], "entities": []}, {"text": "While a multi-label classifier would yield a more powerful application, it's also a considerably more challenging natural language processing problem.", "labels": [], "entities": []}, {"text": "Online conversational text contains shortenings, abbreviations, spelling mistakes, and ever evolving slang.", "labels": [], "entities": []}, {"text": "Huge annotated datasets are needed so that the models can learn all this variability across communities and online platforms.", "labels": [], "entities": []}, {"text": "Furthermore, building a representative and high volume annotated dataset of social media contents for multiple types of toxicity can be exhaustive.", "labels": [], "entities": []}, {"text": "It is a subjective, disturbing and time consuming task.", "labels": [], "entities": []}, {"text": "Critical consideration of the relationships between different subtasks is needed to label this data (.", "labels": [], "entities": []}, {"text": "Additionally, the annotated datasets will always be unbalanced since some types of toxic content are much more prevalent than others.", "labels": [], "entities": []}, {"text": "Some of the back-end approaches to this task have been well researched.", "labels": [], "entities": []}, {"text": "Hand-authoring syntactic rules can be leveraged to detect offensive content and identify potential offensive users on social media ().", "labels": [], "entities": []}, {"text": "Also, morphological, syntactic and user behavior level features have been shown to be useful in learning abusive behavior.", "labels": [], "entities": []}, {"text": "Conventional machine learning classifiers such as SVM classifiers) and linear regressions models () have also been used to effectively detect abusive online language.", "labels": [], "entities": []}, {"text": "Deep learning models with word embeddings as text representations are state-of-theart text classification solutions that show effectiveness in many tasks such as sentiment analysis and the detection of hate speech.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.9651764035224915}, {"text": "detection of hate speech", "start_pos": 189, "end_pos": 213, "type": "TASK", "confidence": 0.8636120706796646}]}, {"text": "Although all these methods are well studied and established, it is not always clear what the best choice fora specific task is due to the trade-off between acquired success rate of the classification model and the complexities of its deployment and inference.", "labels": [], "entities": []}, {"text": "In our work, we used the Wikimedia Toxicity dataset to investigate how various methods of designing a standard text classifier can impact the classification success rate as well as its inference cost.", "labels": [], "entities": [{"text": "Wikimedia Toxicity dataset", "start_pos": 25, "end_pos": 51, "type": "DATASET", "confidence": 0.8940227031707764}]}, {"text": "This dataset was published and used fora Kaggle competition.", "labels": [], "entities": [{"text": "Kaggle competition", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.5946919023990631}]}, {"text": "In the context of the competition, it is a common practice to train multiple large size models and ensemble them to get the highest results, tailored for the competition test set.", "labels": [], "entities": []}, {"text": "Here, however, we only looked at standard classification models that are suitable to be deployed and used for inference in real-time.", "labels": [], "entities": []}, {"text": "For text representations, we looked at frequency-based methods and multiple word embeddings.", "labels": [], "entities": []}, {"text": "For classification models, we considered neural network models that can learn sentence representation using recurrent neural networks and attention layers.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.7097575068473816}]}, {"text": "We also investigated stacking classifiers and used them to automatically label the unannotated part of the dataset to be added to the training set.", "labels": [], "entities": [{"text": "stacking classifiers", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.8768497407436371}]}, {"text": "This paper highlights how we compared various standard methods to help identify what the best practices for this application are.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the dataset, annotation, cleaning and augmentation steps that we applied.", "labels": [], "entities": []}, {"text": "In Section 3, we review some of the commonly used text representation methods and look at how representation of text can impact the classification results.", "labels": [], "entities": [{"text": "text representation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7388838827610016}]}, {"text": "In Section 4, we compare neural network models that are effective in learning long sequences.", "labels": [], "entities": []}, {"text": "In Section 5, we investigate how stacking two classifiers can improve results.", "labels": [], "entities": []}, {"text": "In Section 6, we investigate the impact of using automatically labeled datasets to", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we used Wikimedias Toxicity Data Set (.", "labels": [], "entities": [{"text": "Wikimedias Toxicity Data Set", "start_pos": 22, "end_pos": 50, "type": "DATASET", "confidence": 0.8926877081394196}]}, {"text": "This dataset is also available on Figshare https://figshare.com/articles/ Wikipedia_Detox_Data/4054689 as the Wikipedia Human Annotations of Toxicity on Talk Pages and contains about 215K annotated examples from Wikipedia Talk pages.", "labels": [], "entities": [{"text": "Figshare", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9597185254096985}, {"text": "Wikipedia_Detox_Data/4054689", "start_pos": 74, "end_pos": 102, "type": "DATASET", "confidence": 0.7222733753068107}]}, {"text": "The dataset has been annotated by Kaggle based on asking 5000 crowd-workers to rate Wikipedia comments according to their toxicity (which they evaluated based on how likely they were to make others leave the conversation).", "labels": [], "entities": [{"text": "Kaggle", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.903504490852356}]}, {"text": "The labels include seven types: neutral, toxic, severe toxic, obscene, threat, insult and identity hate.", "labels": [], "entities": []}, {"text": "This dataset was published in two parts namely train and test set.", "labels": [], "entities": []}, {"text": "The train set has 159571 annotated comments while the test set includes about 160k entries.", "labels": [], "entities": []}, {"text": "However, only 63978 of test comments are identified as valid and annotated, which are used here as test set.", "labels": [], "entities": []}, {"text": "There are more than 24 million words in this dataset yet the vocabulary size is only 495147.", "labels": [], "entities": []}, {"text": "This is a very unbalanced dataset and a sample can get more than one label.", "labels": [], "entities": []}, {"text": "shows the count of multiple labels in the train set as well as the training labels' overlaps.", "labels": [], "entities": []}, {"text": "For all the experiments the AUC score is calculated which is the area under the curve (true positive rate vs the false positive rate) is calculated for the test set as the evaluation metric.", "labels": [], "entities": [{"text": "AUC score", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9784707725048065}]}, {"text": "All classes except for the non-toxic examples are augmented through translation to French, Dutch and Spanish before translating back to English.", "labels": [], "entities": []}, {"text": "Using this method, we get slightly different sentences and the label is preserved.", "labels": [], "entities": []}, {"text": "Punctuation was removed and a set of very common word variations (including abbreviations) on social media were found and replaced by the original word.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9495067000389099}]}, {"text": "This cleaning reduced the vocabulary from 495147 to 434161.", "labels": [], "entities": [{"text": "vocabulary", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.8531988859176636}]}], "tableCaptions": [{"text": " Table 1: Comparison of different text  representation methods in training one-vs-rest  SVM classifiers", "labels": [], "entities": []}, {"text": " Table 2: comparison of different classification  models", "labels": [], "entities": []}]}