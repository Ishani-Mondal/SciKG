{"title": [{"text": "Toward Data-Driven Tutorial Question Answering with Deep Learning Conversational Models", "labels": [], "entities": [{"text": "Data-Driven Tutorial Question Answering", "start_pos": 7, "end_pos": 46, "type": "TASK", "confidence": 0.6793491691350937}]}], "abstractContent": [{"text": "There has been an increase in popularity of data-driven question answering systems given their recent success.", "labels": [], "entities": [{"text": "question answering", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7780364751815796}]}, {"text": "This paper explores the possibility of building a tutorial question answering system for Java programming from data sampled from a community-based question answering forum.", "labels": [], "entities": [{"text": "question answering", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.6890919655561447}, {"text": "question answering forum", "start_pos": 147, "end_pos": 171, "type": "TASK", "confidence": 0.7744748791058859}]}, {"text": "This paper reports on the creation of a dataset that could support building such a tutorial question answering system and discusses the methodology to create the 106,386 question strong dataset.", "labels": [], "entities": [{"text": "question answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7150799185037613}]}, {"text": "We investigate how retrieval-based and generative models perform on the given dataset.", "labels": [], "entities": []}, {"text": "The work also investigates the usefulness of using hybrid approaches such as combining retrieval-based and generative models.", "labels": [], "entities": []}, {"text": "The results indicate that building data-driven tutorial systems using community-based question answering forums holds significant promise.", "labels": [], "entities": [{"text": "question answering forums", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.7745424509048462}]}], "introductionContent": [{"text": "Question answering in dialogue is a central concern for designing the next generation of dialogue systems.", "labels": [], "entities": [{"text": "Question answering in dialogue", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8804084807634354}]}, {"text": "Recent work has made great strides in generating dialogue, for example, with neural conversation models (, persona-based conversation models () and adversarial models (.", "labels": [], "entities": []}, {"text": "Specifically, for responding to questions, information-retrieval techniques have long been explored (.", "labels": [], "entities": []}, {"text": "A critical open question is how to build data-driven systems for specific domains.", "labels": [], "entities": []}, {"text": "A challenge that is faced by the community for such systems is the availability of data for those domains.", "labels": [], "entities": []}, {"text": "Given that transfer learning has not yet been shown to yield good results (, there has been investigation in the area of partially data-driven and hand-crafted systems.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.9596693813800812}]}, {"text": "However, handcrafted systems face tremendous limitations in authoring.", "labels": [], "entities": []}, {"text": "Data-driven dialogue systems, which derive their functionality from corpora, have the potential to eliminate this bottleneck.", "labels": [], "entities": []}, {"text": "This work explores the possibility of building a data-driven question-answering system for Java programming.", "labels": [], "entities": []}, {"text": "We leverage a promising source of data by drawing from community-based question answering forums of Stack Exchange.", "labels": [], "entities": [{"text": "Stack Exchange", "start_pos": 100, "end_pos": 114, "type": "DATASET", "confidence": 0.810735821723938}]}, {"text": "Forums typically also have sub-forums, such as Stack Overflow for programming questions and Ask Ubuntu for Ubuntu operating system related questions.", "labels": [], "entities": []}, {"text": "Such community-based forums serve as excellent datasets for specific domains, such as programming or IT support, that are otherwise not easily available to the general public.", "labels": [], "entities": []}, {"text": "The promise of this data is further demonstrated by other work done using the Stack Exchange data: explore how to use semantic parsing to convert an English sentence or query into a code snippet, while investigate returning relevant question answer pairs for Swing, Boost and LINQ by using indexing techniques and building feature-based classifiers.", "labels": [], "entities": [{"text": "Stack Exchange data", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.7214354276657104}, {"text": "semantic parsing", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.7737880349159241}]}, {"text": "With technology becoming ubiquitous, having programming skills are highly sought after.", "labels": [], "entities": []}, {"text": "Ina University or MOOC setting, 'Introduction to Programming' courses typically have a large class size, and with a limited number of Teaching Assistants, providing individual help becomes a difficult task.", "labels": [], "entities": []}, {"text": "The work in this paper focuses on attempting to assist in helping students learn Java programming with a data-driven tutorial question answering system.", "labels": [], "entities": []}, {"text": "This work attempts to build the tutorial question-answering system as both a retrieval-based question answering system () via the Dual Encoder architecture and as a generative question answering system () via the Sequence-to-Sequence architecture).", "labels": [], "entities": [{"text": "question answering", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7139501571655273}, {"text": "generative question answering", "start_pos": 165, "end_pos": 194, "type": "TASK", "confidence": 0.864672064781189}]}, {"text": "The retrieval-based model answers the user's question by predicting the most relevant answer from a set of predefined answers.", "labels": [], "entities": []}, {"text": "In contrast to the retrievalbased model, the generative model answers the user's question by generating new answers based on the data on which the model was trained.", "labels": [], "entities": []}, {"text": "Both of these approaches rely on building good semantic representations of the input in the vector space using word embeddings ( . This work also explores the usefulness of a hybrid approach involving the combination of the retrieval-based and generative models.", "labels": [], "entities": []}, {"text": "This paper thus represents the first work to explore deep learning techniques for data-driven tutorial dialogue for Java programming.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected all corresponding answers from our set of filtered questions to create an initial corpus.", "labels": [], "entities": []}, {"text": "This corpus contained 107,961 questiondescription-answer triplets, of which 47,220 questions did not have a 'user accepted best answer'.", "labels": [], "entities": []}, {"text": "A statistical analysis based on a naive word split showed that there were outliers in the corpus, with very large maximum lengths of up to 10,000 words in an answer.", "labels": [], "entities": []}, {"text": "We identified and removed the outliers in the corpus by removing the current largest sample and monitoring the average length of the corpus.", "labels": [], "entities": []}, {"text": "We continued to remove the largest sample till we obtained a rela- tively stable average value.", "labels": [], "entities": []}, {"text": "This outlier determination was performed for each sample type of question, description and answer separately.", "labels": [], "entities": []}, {"text": "Ultimately, we removed questions longer than 19 words, or whose descriptions were longer than 125 words, or with answers longer than 175 words.", "labels": [], "entities": []}, {"text": "The questions, descriptions and answers in the dataset were then converted into a sequence of numbers using word indexing techniques, in order to be usable by a machine learning model.", "labels": [], "entities": []}, {"text": "The word indexing techniques involved first tokenizing the sentences into word tokens by using an open-source tokenizer (Python NLTK).", "labels": [], "entities": [{"text": "word indexing", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.7682358026504517}, {"text": "tokenizing the sentences into word tokens", "start_pos": 44, "end_pos": 85, "type": "TASK", "confidence": 0.7849157750606537}]}, {"text": "Each word was labelled with a unique index and stored as a key-value pair in a data structure.", "labels": [], "entities": []}, {"text": "Secondly, the words in each sentence were replaced by the corresponding indexes using the data structure created above to obtain a sequence of numbers which corresponded to the original sentence.", "labels": [], "entities": []}, {"text": "A total of 284, 827 words were obtained through tokenization and subsequently indexed in the data structure.", "labels": [], "entities": []}, {"text": "To maintain the uniformity of sentence length, we 'pre-pad' the sequence with 0 before the original sequence.", "labels": [], "entities": []}, {"text": "Adding zeros at the start of the original sequence (if required) allows the network to accept a fixed sequence length and the nature of the number zero also allows us to denote that the element in the sequence is an empty space.", "labels": [], "entities": []}, {"text": "We 'pre-pad' and thus structure the sequence with actual content towards the end of the sequence because a time-based neural network is more likely to 'remember' time steps towards the end of the sequence, as those would be stored in the more recent memory which is captured by the network.", "labels": [], "entities": []}, {"text": "The filtering of the sentences with length thresholds is important, as it is difficult to capture semantic representations for lengthy text using word embeddings.", "labels": [], "entities": []}, {"text": "Setting these thresholds resulted in a reduced dataset of 106,386 questions.", "labels": [], "entities": []}, {"text": "The statistics for the final dataset are shown in.", "labels": [], "entities": []}, {"text": "We also make this dataset available for public use as a contribution of this paper.", "labels": [], "entities": []}, {"text": "All the experiments performed as apart of this work were done on a desktop with the following specification i7 8-core CPU, 32GB RAM, and NVIDIA GTX 1070 8GB VRAM.", "labels": [], "entities": [{"text": "RAM", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.8655275106430054}]}, {"text": "The dataset of 106,386 was split into separate training, testing and validation sets.", "labels": [], "entities": []}, {"text": "Given the large network sizes used in the experiments, there were a correspondingly large number of parameters to be trained for each network, which in turn required a sufficiently large dataset to train on.", "labels": [], "entities": []}, {"text": "Taking this into consideration we chose to not follow the traditional 80-20 train-test split but rather maintain a large enough training set and use the rest of the data for testing and validation.", "labels": [], "entities": []}, {"text": "The training set thus contained 100,000 questions and corresponding description and answers triplets, the test dataset contained 5,000 triplets and the validation set contained 1,386 triplets.", "labels": [], "entities": []}, {"text": "For the dual encoder experiments, the training set size was 200,000 as we had to use both positive and negative samples while training.", "labels": [], "entities": []}, {"text": "Whereas for the sequence-to-sequence model training we used only 100,000 description and answer pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Testing recall@k for group size 10", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.976631224155426}]}]}