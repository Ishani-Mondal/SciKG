{"title": [{"text": "Input Combination Strategies for Multi-Source Transformer Decoder", "labels": [], "entities": [{"text": "Multi-Source Transformer Decoder", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.6536549131075541}]}], "abstractContent": [{"text": "In multi-source sequence-to-sequence tasks, the attention mechanism can be modeled in several ways.", "labels": [], "entities": []}, {"text": "This topic has been thoroughly studied on recurrent architectures.", "labels": [], "entities": []}, {"text": "In this paper, we extend the previous work to the encoder-decoder attention in the Transformer architecture.", "labels": [], "entities": []}, {"text": "We propose four different input combination strategies for the encoder-decoder attention: serial, parallel, flat, and hierarchical.", "labels": [], "entities": []}, {"text": "We evaluate our methods on tasks of multimodal translation and translation with multiple source languages.", "labels": [], "entities": [{"text": "multimodal translation", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6371897608041763}]}, {"text": "The experiments show that the models are able to use multiple sources and improve over single source base-lines.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Transformer model ( ) recently demonstrated superior performance in neural machine translation (NMT) and other sequence generation tasks such as text summarization or image captioning . However, all of these setups consider only a single input to the decoder part of the model.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 72, "end_pos": 104, "type": "TASK", "confidence": 0.8412204086780548}, {"text": "text summarization or image captioning", "start_pos": 149, "end_pos": 187, "type": "TASK", "confidence": 0.6689017295837403}]}, {"text": "In the Transformer architecture, the representation of the source sequence is supplied to the decoder through the encoder-decoder attention.", "labels": [], "entities": []}, {"text": "This attention sub-layer is applied between the self-attention and feed-forward sub-layers in each Transformer layer.", "labels": [], "entities": []}, {"text": "Such arrangement leaves many options for the incorporation of multiple encoders.", "labels": [], "entities": []}, {"text": "So far, attention in sequence-to-sequence learning with multiple source sequences was mostly studied in the context of recurrent neural networks (RNNs).", "labels": [], "entities": []}, {"text": "Libovick\u00b4yLibovick\u00b4y and Helcl (2017) explicitly capture the distribution over multiple inputs by projecting the input representations to a shared vector space and either computing the attention overall hidden states at once, or hierarchically, using another level of attention applied on the context vectors.", "labels": [], "entities": []}, {"text": "employ a gating mechanism for combining the context vectors.", "labels": [], "entities": []}, {"text": "adapted the gating mechanism for use within the Transformer model for context-aware MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.8725685477256775}]}, {"text": "The other aproaches are however not directly usable in the Transformer model.", "labels": [], "entities": []}, {"text": "We propose a number of strategies of combining the different sources in the Transformer model.", "labels": [], "entities": []}, {"text": "Some of the strategies described in this work are an adaptation of the strategies previously used with recurrent neural networks , whereas the rest of them is a novel contribution devised for the Transformer architecture.", "labels": [], "entities": []}, {"text": "We test these strategies on multimodal machine translation (MMT) and multi-source machine translation (MSMT) tasks.", "labels": [], "entities": [{"text": "multimodal machine translation (MMT)", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.7976024548212687}, {"text": "multi-source machine translation (MSMT)", "start_pos": 69, "end_pos": 108, "type": "TASK", "confidence": 0.7757310668627421}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly describe the decoder part of the Transformer model.", "labels": [], "entities": []}, {"text": "We propose a number of input combination strategies for the multi-source Transformer model in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 describes the experiments we performed, and Section 5 shows the results of quantitative evaluation.", "labels": [], "entities": []}, {"text": "An overview of the related work is given in Section 6.", "labels": [], "entities": []}, {"text": "We discuss the results and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our experiments on two different tasks: multimodal translation and multi-source machine translation.", "labels": [], "entities": [{"text": "multimodal translation", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7362382709980011}, {"text": "multi-source machine translation", "start_pos": 78, "end_pos": 110, "type": "TASK", "confidence": 0.6403190791606903}]}, {"text": "We use Neural Monkey (Helcl and Libovick\u00b4yLibovick\u00b4y, 2017) 1 for design, training, and evaluation of the experiments.", "labels": [], "entities": []}, {"text": "In all experiments, the encoder part of the network follows the Transformer architecture as described by . We optimize the model parameters using Adam optimizer () with initial learning rate 0.2, and Noam learning rate decay ( ) with \u03b2 1 = 0.9, \u03b2 2 = 0.98, = 10 \u22129 , and 4,000 warm-up steps.", "labels": [], "entities": []}, {"text": "The size of a mini-batch size of 32 for MMT, and 24 for multisource MT experiments.", "labels": [], "entities": [{"text": "MMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9466003179550171}, {"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.8879308700561523}]}, {"text": "During decoding, we use beam search of width 10 and length normalization of 1.0 ().", "labels": [], "entities": [{"text": "length normalization", "start_pos": 52, "end_pos": 72, "type": "METRIC", "confidence": 0.9569054245948792}]}], "tableCaptions": [{"text": " Table 1: Quantitative results of the MMT experiments on the 2016 test set. Column 'adv. BLEU' is an  adversarial evaluation with randomized image input.", "labels": [], "entities": [{"text": "MMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9548794031143188}, {"text": "2016 test set", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.8200817505518595}, {"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9770476818084717}]}, {"text": " Table 2: Quantitative results of the MMT experiment. The adversarial evaluation shows the BLEU score  when one input language was changed randomly.", "labels": [], "entities": [{"text": "MMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9528962969779968}, {"text": "BLEU score", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9819245636463165}]}]}