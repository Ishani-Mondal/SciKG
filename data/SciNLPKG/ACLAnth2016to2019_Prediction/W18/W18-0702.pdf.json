{"title": [{"text": "Anaphora Resolution with the ARRAU Corpus", "labels": [], "entities": [{"text": "ARRAU", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.854884684085846}]}], "abstractContent": [{"text": "The ARRAU corpus is an anaphorically annotated corpus of English providing rich linguistic information about anaphora resolution.", "labels": [], "entities": [{"text": "ARRAU corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9359873533248901}, {"text": "anaphora resolution", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.72691610455513}]}, {"text": "The most distinctive feature of the corpus is the annotation of a wide range of anaphoric relations , including bridging references and discourse deixis in addition to identity (coref-erence).", "labels": [], "entities": []}, {"text": "Other distinctive features include treating all NPs as markables, including non-referring NPs; and the annotation of a variety of morphosyntactic and semantic mention and entity attributes, including the genericity status of the entities referred to by markables.", "labels": [], "entities": []}, {"text": "The corpus however has not been extensively used for anaphora resolution research so far.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8388064503669739}]}, {"text": "In this paper, we discuss three datasets extracted from the ARRAU corpus to support the three subtasks of the CRAC 2018 Shared Task-identity anaphora resolution over ARRAU-style markables, bridging references resolution, and discourse deixis; the evaluation scripts assessing system performance on those datasets; and preliminary results on these three tasks that may serve as baseline for subsequent research in these phenomena.", "labels": [], "entities": [{"text": "ARRAU corpus", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.9716927409172058}, {"text": "CRAC 2018 Shared Task-identity anaphora resolution", "start_pos": 110, "end_pos": 160, "type": "TASK", "confidence": 0.8521467546621958}, {"text": "bridging references resolution", "start_pos": 189, "end_pos": 219, "type": "TASK", "confidence": 0.5988731880982717}]}], "introductionContent": [{"text": "The release of the ONTONOTES coreference corpus () and the organization of two CONLL shared tasks based on the dataset () have resulted in a substantial increase in coreference research, both in terms of quantity and in terms of quality.", "labels": [], "entities": [{"text": "ONTONOTES coreference corpus", "start_pos": 19, "end_pos": 47, "type": "DATASET", "confidence": 0.7504312992095947}]}, {"text": "We expect ONTONOTES to remain a key resource for the field for many years.", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.7073891162872314}]}, {"text": "However, ONTONOTES also has a number of frequently mentioned limitations, including: \u2022 Not all NPs of relevance to anaphora resolution are treated as markables.", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 9, "end_pos": 18, "type": "DATASET", "confidence": 0.4450133740901947}, {"text": "anaphora resolution", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7402676045894623}]}, {"text": "For instance, expletives are not annotated.", "labels": [], "entities": []}, {"text": "\u2022 And even among referring markables, singletons are not annotated, nor are references to abstract objects or many types of generic objects ().", "labels": [], "entities": []}, {"text": "Furthermore, anaphora resolution involves a number of phenomena besides 'coreference', such as bridging reference and discourse deixis.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.9018486440181732}]}, {"text": "Only a simple form of discourse deixis, event anaphora, is annotated in ONTONOTES; bridging reference was not annotated, although a subset of the corpus has been annotated with this information by.", "labels": [], "entities": [{"text": "ONTONOTES", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.6419021487236023}]}, {"text": "A number of these limitations are overcome in the ARRAU corpus ().", "labels": [], "entities": [{"text": "ARRAU corpus", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9523636698722839}]}, {"text": "In ARRAU, all NPs are considered markables, including expletives and singletons.", "labels": [], "entities": [{"text": "ARRAU", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8596521615982056}]}, {"text": "Both discourse deixis and bridging reference have been annotated.", "labels": [], "entities": []}, {"text": "The corpus however, hasn't been widely used for anaphora resolution research yet, with a few exceptions.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.867170125246048}]}, {"text": "There area number of reasons for this, ranging from the fact that research in both bridging reference and discourse deixis is still limited, to the unusual markup format.", "labels": [], "entities": []}, {"text": "The objective of this paper is to introduce the community to the three datasets extracted from the ARRAU corpus to support this year's CRAC18 Shared task, the first evaluation campaign based on ARRAU.", "labels": [], "entities": [{"text": "ARRAU corpus", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.9695753753185272}, {"text": "ARRAU", "start_pos": 194, "end_pos": 199, "type": "DATASET", "confidence": 0.9636465907096863}]}, {"text": "Our hope is that making such datasets available may, on the one hand, facilitate the use of ARRAU; on the other, increase the community of researchers working on these aspects of anaphora resolution.", "labels": [], "entities": [{"text": "ARRAU", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.8611614108085632}, {"text": "anaphora resolution", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.8748210072517395}]}], "datasetContent": [{"text": "The coreference evaluation script developed by Moosavi and Strube was modified to produce the scorer for Task 1.", "labels": [], "entities": []}, {"text": "We will refer to this script as 'the extended coreference scorer' below.", "labels": [], "entities": []}, {"text": "The extended scorer, when run excluding non-referring expressions and singletons and ignoring MIN information, evaluates a system's response using the same metrics (indeed, a reimplementation of the same code) as the standard CONLL evaluation script, v8.", "labels": [], "entities": []}, {"text": "When required to use MIN information, the extended scorer follows the MUC convention, and considers a mention boundary correct if it contains the MIN and doesn't go beyond the annotated maximum boundary.", "labels": [], "entities": [{"text": "MUC", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8773285150527954}, {"text": "MIN", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.7823265194892883}]}, {"text": "When singletons are to be considered, singletons are also included in the scores (all metrics apart from MUC can deal with singletons).", "labels": [], "entities": [{"text": "MUC", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.8791080117225647}]}, {"text": "Finally, when run in allmarkables mode, the script scores referring and non-referring expressions separately.", "labels": [], "entities": []}, {"text": "Referring expressions are scored using the CONLL metrics; for non-referring expressions, the script evaluates P, Rand F1 at non-referring expression identification.", "labels": [], "entities": [{"text": "CONLL", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.6784276366233826}, {"text": "Rand F1", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.7423636317253113}, {"text": "non-referring expression identification", "start_pos": 124, "end_pos": 163, "type": "TASK", "confidence": 0.759129265944163}]}, {"text": "The extended coreference scorer is available from Moosavi's github at https://github.", "labels": [], "entities": []}, {"text": "com/ns-moosavi/coval.", "labels": [], "entities": []}, {"text": "No system participated in Task 1 and Task 3 of the shared task.", "labels": [], "entities": []}, {"text": "In this Section we discuss the results obtained with Task 2, as well as the baseline results for markable extraction and Task 1.", "labels": [], "entities": [{"text": "markable extraction", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7842676639556885}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics for the four ARRAU sub-corpora.", "labels": [], "entities": [{"text": "ARRAU sub-corpora", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.8973059058189392}]}, {"text": " Table 2: Distribution of bridging references in ARRAU.", "labels": [], "entities": [{"text": "ARRAU", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.7800418734550476}]}, {"text": " Table 3: Distribution of discourse deixis in the subdomains of ARRAU.", "labels": [], "entities": [{"text": "ARRAU", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.8765699863433838}]}, {"text": " Table 4: (Uryupina and Poesio, 2012): Running  BART on different ARRAU genres and on different  ONTONOTES genres. MUC score.", "labels": [], "entities": [{"text": "BART", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9644913077354431}, {"text": "ARRAU", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.6474405527114868}, {"text": "ONTONOTES", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9049296379089355}, {"text": "MUC score", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.7527979910373688}]}, {"text": " Table 5:  Markable extraction in ARRAU and  ONTONOTES.", "labels": [], "entities": [{"text": "Markable extraction", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.6992226243019104}, {"text": "ARRAU", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9591622948646545}, {"text": "ONTONOTES", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.6647197008132935}]}, {"text": " Table 6: Baseline results on Task 1. Gold markables.", "labels": [], "entities": [{"text": "Gold", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9337648153305054}]}, {"text": " Table 7: Baseline results on Task 1 with predicted men- tions, without MIN information.", "labels": [], "entities": []}, {"text": " Table 8: Baseline results on Task 1 with predicted men- tions, using MIN information.", "labels": [], "entities": []}, {"text": " Table 9: Roesiger's results on Task 2 for all domains.", "labels": [], "entities": []}]}