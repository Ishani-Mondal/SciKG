{"title": [{"text": "Statistical learning theory and linguistic typology: a learnability perspective on OT's strict domination\u00c9mile", "labels": [], "entities": [{"text": "Statistical learning theory", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8202602863311768}, {"text": "OT's strict domination\u00c9mile", "start_pos": 83, "end_pos": 110, "type": "DATASET", "confidence": 0.8229831755161285}]}], "abstractContent": [{"text": "This paper develops a learnability argument for strict domination by looking at the generalization error of learners trained on OT and HG target grammars.", "labels": [], "entities": [{"text": "strict domination", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7018065452575684}]}, {"text": "The argument is based on both a review of error bounds in the recent statistical learning literature and simulation results on realistic phonological test cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "According to Optimality Theory (OT;), constraint interaction in natural language phonology is severely constrained by the hypothesis of strict domination.", "labels": [], "entities": []}, {"text": "According to this hypothesis, \"the constraints arranged in a hierarchy\" and \"each constraint is strictly more important than -takes absolute priority over -all the constraints lower-ranked in the hierarchy.", "labels": [], "entities": []}, {"text": "Strict domination thus limits drastically the range of possible strength-interactions between constraints to those representable with the algebra of total order\".", "labels": [], "entities": []}, {"text": "This hypothesis of strict domination has been challenged in the recent phonological literature, which has therefore started to explore an implementation of constraint-based phonology which does away with strict domination, known as Harmonic Grammar (HG;).", "labels": [], "entities": []}, {"text": "Section 2 re-assesses the OT versus HG debate, concluding that HG over-generates for many natural constraint sets and that natural language phonology thus supports OT's hypothesis of strict domination.", "labels": [], "entities": []}, {"text": "Why should constraint interaction in natural language phonology display strict domination?", "labels": [], "entities": []}, {"text": "conjecture that \"demands of learnability provide a pressure for strict domination among constraints\" although they admit that \"it remains an open problem to formally characterize exactly what is essential about strict domination to guarantee efficient learning.\"  take a closer look at this alleged connection between strict domination and learnability.", "labels": [], "entities": []}, {"text": "They look at error bounds in terms of a classical measure of the learning complexity of a hypothesis class, namely its Vapnik-Chervonenkis (VC) dimension).", "labels": [], "entities": []}, {"text": "But they find that the VC dimension is the same for OT and HG, despite OT typologies being smaller than HG typologies because of strict domination.", "labels": [], "entities": []}, {"text": "They conclude that, \"though there maybe factors that favor one model over the other, the complexity of learning is not one of them.\"", "labels": [], "entities": []}, {"text": "Yet, VC dimension is an old measure of learning complexity (it dates back to the seventies) which is inevitably coarse as it applies to completely arbitrary classifiers.", "labels": [], "entities": []}, {"text": "Since, statistical learning theory has instead focused on a special class of classifiers, namely voting classifiers which aggregate the \"votes\" of more basic classifiers scaled through corresponding weights.", "labels": [], "entities": [{"text": "statistical learning theory", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.8837447563807169}]}, {"text": "For this special class of classifiers, better error bounds have been developed, which take into account the margin of \"confidence\" with which a classifier succeeds on the data.", "labels": [], "entities": []}, {"text": "More recently,) have further refined margin theory through error bounds which depend not only on the margin but also on the rate of decay of the weights of the basic classifiers: the bounds get better (that is provide guarantees fora smaller generalization error) as the rate of decay increases.", "labels": [], "entities": [{"text": "margin theory", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.8391967415809631}]}, {"text": "Crucially, HG and OT grammars can be construed as voting classifiers with the phonological constraints playing the role of the basic classifiers.", "labels": [], "entities": []}, {"text": "Section 3 thus brings Koltchinskii and Panchenko's result to bear on the debate between HG and OT, through the well known characterization of OT as a special case of HG with weights decreasing fast, specifically exponentially.", "labels": [], "entities": [{"text": "OT", "start_pos": 95, "end_pos": 97, "type": "DATASET", "confidence": 0.6644577980041504}]}, {"text": "Section 4 complements these theoretical results with simulation-based estimates of the generalization error (codes and data are provided as online supplements).", "labels": [], "entities": [{"text": "generalization", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.9569568037986755}]}, {"text": "We look at two test cases related to vowel harmony and syllable types.", "labels": [], "entities": []}, {"text": "We compute the corresponding typologies of OT grammars and HG-non-OT grammars (namely HG grammars with no OT correspondent).", "labels": [], "entities": []}, {"text": "For both types of target grammars, we compute the generalization error of the hypothesis that performs better (that is, has the largest margin) on a training set of cardinality n.", "labels": [], "entities": []}, {"text": "We show that on average the generalization error decreases faster as a function of n for the OT targets than for the HGnon-OT ones.", "labels": [], "entities": [{"text": "generalization error", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7863971889019012}, {"text": "HGnon-OT", "start_pos": 117, "end_pos": 125, "type": "DATASET", "confidence": 0.9414352774620056}]}, {"text": "Section 5 concludes the paper and discusses various issues to explore in future research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}