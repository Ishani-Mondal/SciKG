{"title": [{"text": "Anaphora Resolution for Twitter Conversations: An Exploratory Study", "labels": [], "entities": [{"text": "Anaphora Resolution for Twitter Conversations", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.5949827253818512}]}], "abstractContent": [{"text": "We present a corpus study of pronominal anaphora on Twitter conversations.", "labels": [], "entities": []}, {"text": "After outlining the specific features of this genre, with respect to reference resolution, we explain the construction of our corpus and the annotation steps.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.7376625835895538}]}, {"text": "From this we derive a list of phenomena that need to be considered when performing anaphora resolution on this type of data.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7104301899671555}]}, {"text": "Finally, we test the performance of an off-the-shelf resolution system, and provide some qualitative error analysis.", "labels": [], "entities": []}], "introductionContent": [{"text": "We are interested in the task of pronominal anaphora resolution for conversations in Twitter, which to our knowledge has not been addressed so far.", "labels": [], "entities": [{"text": "pronominal anaphora resolution", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.608783076206843}]}, {"text": "By 'conversation', we mean tree structures originating from the reply-to relation; when using replies, people often (though not always) interact with each other across several turns.", "labels": [], "entities": []}, {"text": "Hence, anaphora resolution needs to attend both to the general and well-known problems of handling Twitter language, and potentially to aspects of conversation structure.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.940060168504715}]}, {"text": "In order to study the properties of coreference relations in these conversations, we built a corpus that is designed to represent a number of different relevant phenomena, which we selected carefully.", "labels": [], "entities": []}, {"text": "We annotated pronouns and their antecedents, so that the data can be used for systematically testing anaphora resolvers, and we conducted experiments with the Stanford system.", "labels": [], "entities": [{"text": "testing anaphora resolvers", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.6237282752990723}, {"text": "Stanford system", "start_pos": 159, "end_pos": 174, "type": "DATASET", "confidence": 0.9670923948287964}]}, {"text": "The paper is structured as follows: Section 2 introduces general phenomena found in Twitter conversations and describes earlier research.", "labels": [], "entities": []}, {"text": "Section 3 discusses our approach to corpus construction", "labels": [], "entities": [{"text": "corpus construction", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8669106662273407}]}], "datasetContent": [{"text": "In our experiments, the resolver's algorithm option is set to the value of \"clustering\".", "labels": [], "entities": [{"text": "resolver", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.9665560126304626}]}, {"text": "There is also an option for activating the \"conll\" settings in the Stanford resolver.", "labels": [], "entities": [{"text": "Stanford resolver", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.8926510214805603}]}, {"text": "When this setting is on, the resolver does not mark the predicative nominals and appositives, because in the CoNLL 11/12 shared tasks, these were not treated as markables . We We also conducted experiments with the raw input text (i.e., with no speaker or turn info provided), but it is ongoing work to interpret the difference in the results we found.", "labels": [], "entities": [{"text": "CoNLL 11/12 shared tasks", "start_pos": 109, "end_pos": 133, "type": "DATASET", "confidence": 0.943374345699946}]}, {"text": "preferred to keep that setting off, as our dataset is annotated for those mentions.", "labels": [], "entities": []}, {"text": "The metric scores are calculated using the reference implementation of the CoNLL scorer).", "labels": [], "entities": [{"text": "CoNLL scorer", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.9021316766738892}]}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "It is difficult to compare them to results published on standard monologue text datasets, due to our selecting only a subset of the output chains.", "labels": [], "entities": []}, {"text": "The scorer considers a mention to be correct only if it matches the exact same span in the manual gold annotations.", "labels": [], "entities": [{"text": "mention", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9691409468650818}]}, {"text": "The partial match scoring (e.g. checking the matching of heads for each phrase) might be more insightful for our data as the impact of differences in annotation schemes will be reduced by this way.", "labels": [], "entities": []}, {"text": "The comparison of the metric results with each other may create more understanding on the strong and the weak aspects of the resolver on Twitter conversations.", "labels": [], "entities": []}, {"text": "We leave the partial matching scoring and analysis of the differences in metric results as future work.", "labels": [], "entities": []}, {"text": "However, for the present study, we made a qualitative analysis of the errors existing in the automated results and present them in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Descriptive statistics of annotations in the cor- pus", "labels": [], "entities": []}, {"text": " Table 4: Evaluation results with speaker and turn info  included in the input data (CONLL 2012 scorer)", "labels": [], "entities": [{"text": "CONLL 2012 scorer", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.9469249844551086}]}, {"text": " Table 5: Statistical information on error categories", "labels": [], "entities": []}]}