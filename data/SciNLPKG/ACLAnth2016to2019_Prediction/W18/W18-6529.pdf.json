{"title": [{"text": "Sequence-to-Sequence Models for Data-to-Text Natural Language Generation: Word-vs. Character-based Processing and Output Diversity", "labels": [], "entities": [{"text": "Data-to-Text Natural Language Generation", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.6883754283189774}]}], "abstractContent": [{"text": "We present a comparison of word-based and character-based sequence-to-sequence models for data-to-text natural language generation, which generate natural language descriptions for structured inputs.", "labels": [], "entities": [{"text": "data-to-text natural language generation", "start_pos": 90, "end_pos": 130, "type": "TASK", "confidence": 0.7250937074422836}]}, {"text": "On the datasets of two recent generation challenges, our models achieve comparable or better automatic evaluation results than the best challenge submissions.", "labels": [], "entities": []}, {"text": "Subsequent detailed statistical and human analyses shed light on the differences between the two input representations and the diversity of the generated texts.", "labels": [], "entities": []}, {"text": "Ina controlled experiment with synthetic training data generated from templates, we demonstrate the ability of neural models to learn novel combinations of the templates and thereby generalize beyond the linguistic structures they were trained on.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language generation (NLG) is an actively researched task, which according to can be divided into text-to-text generation, such as machine translation), text summarization (, or open-domain conversation response generation () on the one hand, and data-to-text generation on the other hand.", "labels": [], "entities": [{"text": "Natural language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7760619421799978}, {"text": "text-to-text generation", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.7493320405483246}, {"text": "machine translation", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.7177121490240097}, {"text": "text summarization", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.7909879386425018}, {"text": "open-domain conversation response generation", "start_pos": 185, "end_pos": 229, "type": "TASK", "confidence": 0.6330539658665657}, {"text": "data-to-text generation", "start_pos": 254, "end_pos": 277, "type": "TASK", "confidence": 0.7532400190830231}]}, {"text": "Here, we focus on the latter, the task of generating textual descriptions for structured data.", "labels": [], "entities": []}, {"text": "Data-to-text generation comprises the generation of system responses based on dialog acts in task-oriented dialog systems), sport games reports and weather forecasts (, and database entry descriptions (.", "labels": [], "entities": [{"text": "Data-to-text generation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7208715379238129}]}, {"text": "In this paper, we focus on sentence planning and surface realization.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7438951730728149}, {"text": "surface realization", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7885835766792297}]}, {"text": "We build on data-totext datasets of two recent shared tasks for end-toend NLG, namely the E2E challenge () and WebNLG challenge ().", "labels": [], "entities": []}, {"text": "Example input-text pairs for both datasets are shown in.", "labels": [], "entities": []}, {"text": "Neural sequence to sequence (Seq2Seq) models) have shown promising results for this task, especially in combination with an attention mechanism.", "labels": [], "entities": []}, {"text": "Several recent NLG approaches, as well as most systems in the E2E and WebNLG challenge are based on this architecture.", "labels": [], "entities": [{"text": "WebNLG challenge", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.8915753364562988}]}, {"text": "While most NLG models generate text word byword, promising results were also obtained by encoding the input and generating the output text character-by-character (.", "labels": [], "entities": []}, {"text": "Five out of 62 E2E challenge submissions operate on the character-level.", "labels": [], "entities": []}, {"text": "However, it is difficult to draw conclusions from the challenge results with respect to this difference, since the submitted systems also differ in other aspects and were evaluated on a single dataset only.", "labels": [], "entities": []}, {"text": "Besides adequacy and fluency, variation is an important aspect in NLG ().", "labels": [], "entities": []}, {"text": "In addition to comparing the linguistic and contentwise correctness of word-and character-based Seq2Seq models through automatic and human evaluation, we investigate the variety of their outputs.", "labels": [], "entities": []}, {"text": "While template-based systems can assure perfect content and linguistic quality, they often suffer from low diversity.", "labels": [], "entities": []}, {"text": "Conversely, neural models might generalize beyond a limited amount of training texts or templates, thereby producing more diverse outputs.", "labels": [], "entities": []}, {"text": "To test this hypothesis, we train Seq2Seq models on template-generated texts with a controlled amount of variation and show that they not only reproduce the templates, but also delexicalized reference 1: Customers gave NAME, near NEAR, a 3 out of 5 rating.", "labels": [], "entities": [{"text": "NAME", "start_pos": 219, "end_pos": 223, "type": "DATASET", "confidence": 0.8059616684913635}, {"text": "NEAR", "start_pos": 230, "end_pos": 234, "type": "DATASET", "confidence": 0.7604898810386658}]}, {"text": "WebNLG input: cityServed(Abilene Regional Airport), isPartOf(Abilene) reference 1: Abilene is in Texas and is served by the Abilene regional airport.", "labels": [], "entities": [{"text": "WebNLG", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.95782870054245}]}, {"text": "reference 2: Abilene, part of Texas, is served by the Abilene regional airport.", "labels": [], "entities": []}, {"text": "delexicalized input: city served(AGENT-1), is part of(BRIDGE-1) delexicalized reference 1: BRIDGE-1 is in PATIENT-1 and is served by the AGENT-1.: Example input-reference pairs from the E2E and WebNLG development set.", "labels": [], "entities": [{"text": "BRIDGE-1", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.924774169921875}, {"text": "BRIDGE-1", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.7411088347434998}, {"text": "WebNLG development set", "start_pos": 194, "end_pos": 216, "type": "DATASET", "confidence": 0.916232188542684}]}, {"text": "generate novel structures resulting from template combinations.", "labels": [], "entities": []}, {"text": "In sum, we make the following contribution: \u2022 We compare word-and character-based Seq2Seq models for NLG on two datasets.", "labels": [], "entities": []}, {"text": "\u2022 We conduct an extensive automatic and manual analysis of the generated texts and compare them to human performance.", "labels": [], "entities": []}, {"text": "\u2022 In an experiment with synthetic training data generated from templates, we demonstrate the ability of neural NLG models to learn template combinations and thereby generalize beyond the linguistic structures they were trained on.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our experiments with the OpenNMT toolkit (, which we extend to also perform character-based processing.", "labels": [], "entities": []}, {"text": "We: E2E and WebNLG training split statistics for word-based processing after delexicalization and character-based processing.", "labels": [], "entities": []}, {"text": "tuned the hyperparameters for each dataset and processing method to optimize the BLEU score on the development sets.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9778404235839844}]}, {"text": "The word-based model for the E2E dataset is trained by stochastic gradient descent (SGD) and an initial learning rate of 1.0.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.9474250972270966}]}, {"text": "For all other models, we achieved better performance with the Adam optimizer () with an initial learning rate of 0.001.", "labels": [], "entities": []}, {"text": "If there is no improvement in the development perplexity, or in any case after the eighth epoch, we halve the learning rate.", "labels": [], "entities": []}, {"text": "Also, we clip all gradients to a maximum of five.", "labels": [], "entities": []}, {"text": "We use a batch size of 64.", "labels": [], "entities": []}, {"text": "To prevent overfitting, we dropout units in the context vectors with a probability of 0.3.", "labels": [], "entities": []}, {"text": "We keep the model with the lowest development perplexity in 13 training epochs.", "labels": [], "entities": []}, {"text": "The word-based E2E model has 64-dimensional word embeddings and a single encoder and decoder layer with 64 units each.", "labels": [], "entities": []}, {"text": "All other models use 500-dimensional word-or character embeddings and two layers in the encoder and decoder with 500 dimensions each.", "labels": [], "entities": []}, {"text": "While a unidirectional encoder was sufficient for the word-based models, bidirectional encoders were beneficial for the character-based models on both datasets.", "labels": [], "entities": []}, {"text": "We use abeam size of 15 for decoding with the word-based models, and found a smaller beam of five to yield better results for the character-based models.", "labels": [], "entities": []}, {"text": "This is probably due to the much smaller vocabulary size of the character-based models.", "labels": [], "entities": []}, {"text": "For automatic evaluation, we report BLEU (Papineni et al.,), which measures the precision of the generated n-grams compared to the references, and recall-oriented ROUGE-L, which measures the longest common subsequence between the generated texts and the references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9994169473648071}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9984502792358398}, {"text": "recall-oriented", "start_pos": 147, "end_pos": 162, "type": "METRIC", "confidence": 0.9817681312561035}, {"text": "ROUGE-L", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.5600957870483398}]}, {"text": "We compute these scores with the E2E challenge evaluation script 2 . and 3 display the results on the E2E and WebNLG test sets for models of the respective challenges and our own models . Since the performance of neural models can vary considerably due to random parameter initialization and randomized training procedures, we train ten models with different random seeds for each setting and report the average (avg) and standard deviation (SD).", "labels": [], "entities": [{"text": "E2E", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9555712938308716}, {"text": "WebNLG test sets", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.9115239779154459}, {"text": "standard deviation (SD)", "start_pos": 422, "end_pos": 445, "type": "METRIC", "confidence": 0.9767062306404114}]}, {"text": "To gain an impression of the expressiveness of the automatic evaluation scores for NLG, we computed the average scores that the human references would obtain.", "labels": [], "entities": []}, {"text": "character-based models . Strikingly, on the E2E development set, both model variants significantly outperform human texts by far with respect to both automatic evaluation measures.", "labels": [], "entities": [{"text": "E2E development set", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.9692885875701904}]}, {"text": "While the human BLEU score is significantly higher than those of both systems on the WebNLG development set, there is no statistical difference between human and system ROUGE-L scores.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9762251377105713}, {"text": "WebNLG development set", "start_pos": 85, "end_pos": 107, "type": "DATASET", "confidence": 0.9652820229530334}, {"text": "ROUGE-L", "start_pos": 169, "end_pos": 176, "type": "METRIC", "confidence": 0.9134026765823364}]}, {"text": "This further demonstrates the limited utility of BLEU and ROUGLE-L scores to evaluate NLG outputs, which was previously suggested by weak correlations of such scores with human judgments ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.998747706413269}, {"text": "ROUGLE-L", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9854448437690735}]}, {"text": "Furthermore, the high scores on the E2E dataset imply that the models succeed in picking up patterns from the training data that transfer well to the similar development set, whereas human variation and creativity are punished by lexical overlap-based automatic evaluation scores.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.9642954170703888}]}, {"text": "While correctness is a necessity in NLG, in many settings it is not sufficient.", "labels": [], "entities": []}, {"text": "Often, variation of the generated texts is crucial to avoid repetitive and unnatural outputs.", "labels": [], "entities": []}, {"text": "shows automatically computed statistics on the diversity of the generated texts of both models and human texts and on the overlap of the (generated) texts with the training set.", "labels": [], "entities": []}, {"text": "We measure diversity by the number of unique sentences and words in all development set references and generated texts, as done e.g. by.", "labels": [], "entities": []}, {"text": "Additionally, we report the Shannon text entropy as measure of the amount of variation in the texts following).", "labels": [], "entities": []}, {"text": "We compute the text entropy E for words (unigrams) and uni-, bi-, and trigrams as follows: where V is the set of all word types or uni-, biand trigrams, f denotes frequency and total is the token count or total number of uni-, bi-and trigrams in the texts, respectively.", "labels": [], "entities": []}, {"text": "To measure the extent by which the models generalize beyond plugging in restaurant or other entity names into templates extracted from the training data, we compute the results on the delexicalized outputs of the word-based models and delexicalize the character-based models' outputs.", "labels": [], "entities": []}, {"text": "For the human scores, we generate n artificial prediction files, treating each n-th reference (42 for E2E, 8 for WebNLG) as reference, apply delexicalization, and average the scores for then files.", "labels": [], "entities": []}, {"text": "On both datasets, our systems produce significantly less varied outputs and reproduce more Example: NAME is a family-friendly coffee shop which serves Chinese food in the low price range.", "labels": [], "entities": []}, {"text": "It has a high customer rating and is located in the city centre area, near NEAR.", "labels": [], "entities": [{"text": "NEAR", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.9738094806671143}]}, {"text": "texts and sentences from the training data than the human texts.", "labels": [], "entities": []}, {"text": "Interestingly, however, the characterbased models generate significantly more unique sentences and copy significantly less from the training data than the word-based models, which copy about 40% of their generated sentences from the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: E2E and WebNLG training split statis- tics for word-based processing after delexicaliza- tion and character-based processing.", "labels": [], "entities": []}, {"text": " Table 2: E2E test set results. Own results corre- spond to avg\u00b1SD of ten runs and single result of  best models on the development set.", "labels": [], "entities": [{"text": "E2E test set", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.9339360992113749}, {"text": "avg\u00b1SD", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9144043525060018}]}, {"text": " Table 3: WebNLG test set results. Own results  correspond to single best model on development  set and avg\u00b1SD of ten runs.", "labels": [], "entities": [{"text": "WebNLG test set", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.9515683054924011}, {"text": "avg\u00b1SD", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9307923316955566}]}, {"text": " Table 4: E2E and WebNLG development set re- sults in the format avg\u00b1SD. Human results are av- eraged over using each human reference as predic- tion once.", "labels": [], "entities": [{"text": "E2E", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.9379550218582153}, {"text": "WebNLG", "start_pos": 18, "end_pos": 24, "type": "DATASET", "confidence": 0.9261146783828735}]}, {"text": " Table 5: Percentage of affected instances in man- ual error analysis of 15 randomly selected devel- opment set instances for each input length.", "labels": [], "entities": []}, {"text": " Table 6: Linguistic diversity of development set references and generated texts as avg\u00b1SD. '% new' de- notes the share of generated texts or sentences that do not appear in training references. Higher indicates  more diversity for all measures.", "labels": [], "entities": []}, {"text": " Table 7: Manual evaluation of generated texts for  10 random test instances of a word-based model  trained with synthetic training data from two tem- plates. c@n: avg. number of correct texts (with  respect to content and language) among the top n  hypotheses.", "labels": [], "entities": []}, {"text": " Table 8: Hyperparameters for the models trained  on synthetic training data generated from Tem- plate 1 (T 1), Template 2 (T 2) and both (T 1+2).", "labels": [], "entities": []}]}