{"title": [{"text": "SMT versus NMT: Preliminary comparisons for Irish", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.973633348941803}]}], "abstractContent": [{"text": "In this paper, we provide a preliminary comparison of statistical machine translation (SMT) and neural machine translation (NMT) for English\u2192Irish in the fixed domain of public administration.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.7571124831835429}, {"text": "neural machine translation", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.7515930930773417}]}, {"text": "We discuss the challenges for SMT and NMT of a less-resourced language such as Irish, and show that while an out-of-the-box NMT system may not fare quite as well as our tailor-made domain-specific SMT system, the future may still be promising for EN\u2192GA NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9933746457099915}]}], "introductionContent": [{"text": "In recent times, NMT has been widely hailed as a significant development in the improvement in quality of machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.844731068611145}]}, {"text": "However, as a technique that is data-hungry, there is a concern that languages with fewer resources may not benefit to the same degree that wellresourced major languages do.", "labels": [], "entities": []}, {"text": "In order to prevent a low-resource language such as Irish being left behind in the context of these advancements, we take the first steps towards applying NMT methods to English\u2192Irish (EN\u2192GA) translation.", "labels": [], "entities": [{"text": "English\u2192Irish (EN\u2192GA) translation", "start_pos": 170, "end_pos": 203, "type": "TASK", "confidence": 0.5142048001289368}]}, {"text": "Irish is the national and official language of the Republic of Ireland, and an official EU language.", "labels": [], "entities": []}, {"text": "While EN\u2192GA MT is rarely used for comprehension purposes, 1 MT is invaluable in meeting the language rights needs of native Irish speakers.", "labels": [], "entities": [{"text": "EN\u2192GA MT", "start_pos": 6, "end_pos": 14, "type": "TASK", "confidence": 0.5407261997461319}]}, {"text": "MT has already been proven useful in the post-editing environment of an official Irish government department, where the translation of EN\u2192GA documents has been facilitated by a Moses-based statistical machine translation (SMT) system (.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8964760899543762}, {"text": "translation of EN\u2192GA documents", "start_pos": 120, "end_pos": 150, "type": "TASK", "confidence": 0.801159530878067}, {"text": "statistical machine translation (SMT)", "start_pos": 189, "end_pos": 226, "type": "TASK", "confidence": 0.8015368282794952}]}, {"text": "The success of this domain-specific SMT system is due in part to the availability of high quality parallel data in this particular domain (see).", "labels": [], "entities": [{"text": "SMT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9189232587814331}]}, {"text": "The quality of MT is currently unreliable for official translation in an EU setting, however.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9328128695487976}, {"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7405762672424316}]}, {"text": "This is partly due to a derogation imposed on the production of official Irish language texts in the EU.", "labels": [], "entities": []}, {"text": "While the European Commission is moving towards using NMT engines in the new eTranslation platform, Irish is not yet sufficiently supported.", "labels": [], "entities": [{"text": "Irish", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.966435968875885}]}, {"text": "Despite a relatively low availability of resources -in terms of both bilingual and monolingual digital content -we have previously shown that a domain-tailored SMT system can achieve promising translation quality (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 160, "end_pos": 163, "type": "TASK", "confidence": 0.8991013169288635}]}, {"text": "The question remains whether NMT can achieve a similar level of usability for Irish in this setting.", "labels": [], "entities": [{"text": "NMT", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.8137741088867188}]}, {"text": "While the introduction of deep learning methods to the field of MT has witnessed a breakthrough in recent years, the positive impact of NMT is not felt across the board.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9853066205978394}]}, {"text": "As highlight, current NMT systems can face a number of challenges when dealing with specific tasks.", "labels": [], "entities": []}, {"text": "These challenges include low-resourced languages, low-frequency words arising from inflection, long sentences, and out-of-domain texts.", "labels": [], "entities": []}, {"text": "The latter may not apply to our test case, as the success of our earlier SMT system lies in the closed domain nature of the use case (public administration data), yet the other factors are very real for the Irish language in general.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9917241930961609}]}, {"text": "In this study, we report on recent scores from the training of an updated Irish SMT engine, based on our latest data sets.", "labels": [], "entities": [{"text": "Irish SMT engine", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.7901518742243449}]}, {"text": "We then present a preliminary NMT baseline, based on the same training and test data as previous SMT experiments, in order to investigate its strengths and weaknesses with respect to Irish.", "labels": [], "entities": [{"text": "NMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8094088435173035}, {"text": "SMT", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9895932674407959}, {"text": "Irish", "start_pos": 183, "end_pos": 188, "type": "DATASET", "confidence": 0.9490784406661987}]}, {"text": "The paper is divided as follows: Section 2 provides the context within which our work is relevant, both in terms of low-resourced MT and the use of MT in professional translation environments.", "labels": [], "entities": [{"text": "MT", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.9629707932472229}, {"text": "MT in professional translation", "start_pos": 148, "end_pos": 178, "type": "TASK", "confidence": 0.6779651567339897}]}, {"text": "In Section 3 we outline the datasets we use in training and testing, and give some background on the types and sources of this data.", "labels": [], "entities": []}, {"text": "Section 4 details how the SMT and NMT experiments were implemented.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9881072640419006}]}, {"text": "Section 5 provides updated results for EN-GA SMT in this domain and establishes preliminary results for EN-GA NMT.", "labels": [], "entities": [{"text": "EN-GA SMT", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.676986813545227}, {"text": "EN-GA NMT", "start_pos": 104, "end_pos": 113, "type": "DATASET", "confidence": 0.7001027762889862}]}, {"text": "Finally, in Section 6 we provide some conclusions and indicate possible options for future work in this area.", "labels": [], "entities": []}], "datasetContent": [{"text": "To add to this baseline system, we also perform a few preliminary experiments to investigate the affect that altering parameters or using other methods would have on an EN-GA NMT system.", "labels": [], "entities": []}, {"text": "\u2022 NMT-250 One such experiment involves experimenting with the number of hidden layers in our NMT system.", "labels": [], "entities": [{"text": "NMT-250", "start_pos": 2, "end_pos": 9, "type": "DATASET", "confidence": 0.9352072477340698}]}, {"text": "We implement a smaller model i.e. reduced the number of hidden states from 500 to 250.", "labels": [], "entities": []}, {"text": "The results for this system are presented in wherein this system is referred to as 'NMT-250'.", "labels": [], "entities": [{"text": "NMT-250", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.863659679889679}]}, {"text": "\u2022 NMT+ADAM We also experiment with implementing the stochastic gradient descent with 'Adam', a method for stochastic optimisation (.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9646701812744141}]}, {"text": "This method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.", "labels": [], "entities": []}, {"text": "We implement this method using the recommended learning rate for Adam (0.001) and denote this system in as NMT+ADAM.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.7162948250770569}]}, {"text": "\u2022 NMT+BPE In order to address the inflectional nature of the Irish language, we experiment with the use of byte-pair encoding (BPE).", "labels": [], "entities": [{"text": "BPE", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9729835391044617}]}, {"text": "BPE is a technique presented by and adapted for NMT by.", "labels": [], "entities": [{"text": "BPE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5396455526351929}, {"text": "NMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8602089285850525}]}, {"text": "In terms of MT, it aims to increase vocabulary coverage by encoding rare and unknown words as sequences of subword units.", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9925823211669922}]}, {"text": "As data sparsity is an issue especially relevant to a low-resourced inflectional language such as Irish, reducing out of vocabulary (OOV) words is a promising technique.", "labels": [], "entities": []}, {"text": "This system is referred to as NMT+BPE in and.", "labels": [], "entities": [{"text": "BPE", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.8481330275535583}]}, {"text": "Both the SMT and NMT systems were tested on the same test set that were used in earlier experiments (, consisting of 1,500 in-domain sentences randomly selected and set aside from the bilingual corpus.: BLEU scores for SMT and NMT EN-GA systems before and after applying the automated post-editing module.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9466912150382996}, {"text": "BLEU", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.9995710253715515}, {"text": "SMT", "start_pos": 219, "end_pos": 222, "type": "TASK", "confidence": 0.9582395553588867}]}, {"text": "The highest BLEU score and lowest TER score are highlighted in bold.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9847557544708252}, {"text": "TER score", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9848221838474274}]}], "tableCaptions": [{"text": " Table 1: Size and distribution of translation model training data.", "labels": [], "entities": [{"text": "translation model training", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.8928815722465515}]}, {"text": " Table 2: Additional monolingual (GA) text used for training the SMT language model", "labels": [], "entities": [{"text": "SMT language", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.9226028919219971}]}, {"text": " Table 3: BLEU scores for SMT and NMT EN-GA systems before and after applying the auto- mated post-editing module. The highest BLEU score and lowest TER score are highlighted in  bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990278482437134}, {"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9262332916259766}, {"text": "BLEU score", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9851639568805695}, {"text": "TER score", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9563910663127899}]}]}