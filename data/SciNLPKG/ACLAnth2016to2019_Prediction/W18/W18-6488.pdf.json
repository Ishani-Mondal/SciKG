{"title": [{"text": "Prompsit's submission to WMT 2018 Parallel Corpus Filtering shared task", "labels": [], "entities": [{"text": "Prompsit's submission to WMT 2018 Parallel Corpus Filtering shared task", "start_pos": 0, "end_pos": 71, "type": "DATASET", "confidence": 0.7594739171591672}]}], "abstractContent": [{"text": "This paper describes Prompsit Language Engi-neering's submissions to the WMT 2018 parallel corpus filtering shared task.", "labels": [], "entities": [{"text": "WMT 2018 parallel corpus filtering shared task", "start_pos": 73, "end_pos": 119, "type": "TASK", "confidence": 0.6939326652458736}]}, {"text": "Our four submissions were based on an automatic clas-sifier for identifying pairs of sentences that are mutual translations.", "labels": [], "entities": []}, {"text": "A set of hand-crafted hard rules for discarding sentences with evident flaws were applied before the classifier.", "labels": [], "entities": []}, {"text": "We explored different strategies for achieving a training corpus with diverse vocabulary and fluent sentences: language model scoring , an active-learning-inspired data selection algorithm and n-gram saturation.", "labels": [], "entities": []}, {"text": "Our submissions were very competitive in comparison with other participants on the 100 million word training corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the systems submitted by Prompsit Language Engineering 1 to the parallel corpus filtering shared task ( featured in the Third Conference on Machine Translation.", "labels": [], "entities": [{"text": "parallel corpus filtering shared task", "start_pos": 85, "end_pos": 122, "type": "TASK", "confidence": 0.7170949637889862}, {"text": "Third Conference on Machine Translation", "start_pos": 141, "end_pos": 180, "type": "TASK", "confidence": 0.47339776158332825}]}, {"text": "Given a very noisy 1 billion-word GermanEnglish parallel corpus crawled from the web, 2 participants have to subselect sentence pairs that amount to (a) 10 million words (10M dataset), and (b) 100 million words (100M dataset).", "labels": [], "entities": [{"text": "GermanEnglish parallel corpus", "start_pos": 34, "end_pos": 63, "type": "DATASET", "confidence": 0.9132610758145651}]}, {"text": "In this shared task, performance of the sentence filtering is estimated as the translation quality (as measured by BLEU) of phrase-based statistical machine translation (SMT) and neural machine translation (NMT) systems built from the subselected data.", "labels": [], "entities": [{"text": "sentence filtering", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7238287627696991}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9990984201431274}, {"text": "phrase-based statistical machine translation (SMT) and neural machine translation (NMT)", "start_pos": 124, "end_pos": 211, "type": "TASK", "confidence": 0.7589856045586723}]}, {"text": "Evaluation sets belong to different domains, which discourages strategies based on domain relatedness.", "labels": [], "entities": []}, {"text": "Our submission is built upon the assumption that a training set that maximizes the quality of machine translation (MT) must meet the following requirements: \u2022 Parallel sentences must be mutual translations.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.8419559240341187}]}, {"text": "\u2022 Sentences must be fluent in the corresponding language in order to build a reliable language model/NMT decoder.", "labels": [], "entities": []}, {"text": "We work under the hypothesis that the sentence D0006 Tooth brush A NOELL / F945J 0,21 is less useful fora language model than I brush my teeth and look in the mirror, despite containing a similar amount of tokens.", "labels": [], "entities": [{"text": "D0006 Tooth brush A NOELL / F945J 0,21", "start_pos": 47, "end_pos": 85, "type": "METRIC", "confidence": 0.687821002677083}]}, {"text": "\u2022 Vocabulary must be diverse, since the MT systems are evaluated with test sets from different domains.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9696448445320129}]}, {"text": "We built a training corpus that meets the aforementioned requirements in a sequential process that comprises the following steps: 1.", "labels": [], "entities": []}, {"text": "As a preprocessing step, deletion of parallel sentences by means of a set of handcrafted hard rules implemented in the translation memory cleaning tool Bicleaner.", "labels": [], "entities": [{"text": "Bicleaner", "start_pos": 152, "end_pos": 161, "type": "DATASET", "confidence": 0.7423281073570251}]}, {"text": "3 These rules are addressed at detecting evident flaws such as languages different from English and German, encoding errors, very different lengths in parallel sentences, etc. and speeding up the subsequent steps.", "labels": [], "entities": []}, {"text": "2. Detection of misaligned parallel sentences by means of an automatic classifier.", "labels": [], "entities": [{"text": "Detection of misaligned parallel sentences", "start_pos": 3, "end_pos": 45, "type": "TASK", "confidence": 0.9035917758941651}]}, {"text": "3. Scoring of sentences based on fluency and diversity: four different approaches were tested and submitted.", "labels": [], "entities": [{"text": "Scoring of sentences", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.897863487402598}]}, {"text": "The remainder of the paper is organized as follows: Section 2 outlines related approaches, Sections 3 and 4 respectively describe the steps 2 and 3 of our processing pipeline.", "labels": [], "entities": []}, {"text": "Section 5 confirms the positive impact of our processing pipeline on translation quality by comparing it with other baseline approaches.", "labels": [], "entities": [{"text": "translation", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.9762702584266663}]}, {"text": "Finally, the paper ends with some concluding remarks and the suggestion of potential future research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We built MT systems from the four scoring alternatives presented and compared them with two baseline systems: one in which the sentences were randomly chosen from the noisy, crawled data Note that, in the prompsit-lm submission, two different placeholders replacement strategies were applied.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9895542860031128}]}, {"text": "Firstly, that described in Section 4.3 was applied in order to obtain language model perplexities.", "labels": [], "entities": []}, {"text": "Afterwards, the one described in Section 4.1 was applied in order to discard similar sentences.", "labels": [], "entities": []}, {"text": "In the prompsit-lm-nota submission, only the first one was applied.", "labels": [], "entities": []}, {"text": "Concerning truecasing, preliminary experiments showed that it has a limited impact for language model scoring, hence the main difference between the submissions is the strength of n-gram saturation: fewer sentences are discarded if placeholders are disabled.", "labels": [], "entities": [{"text": "language model scoring", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.674709270397822}]}, {"text": "(random) and another one in which the hard-rule filtering was applied and each sentence was simply scored by the classifier (only-classifier; 10M and 100M datasets were built by selecting sentences in descending classifier score order).", "labels": [], "entities": []}, {"text": "Systems were trained following the official instructions from the shared task.", "labels": [], "entities": []}, {"text": "10 SMT systems were built with Moses and tuned with Batch MIRA.", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.98929762840271}, {"text": "Batch", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.7784249186515808}, {"text": "MIRA", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9403674602508545}]}, {"text": "A 5-gram language model was estimated from the TL side of the training corpus.", "labels": [], "entities": []}, {"text": "NMT systems followed the Transformer architecture ( and were built with Marian (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8343250751495361}]}, {"text": "49 500 byte pair encoding merge operations ( were applied to segment the words in the NMT training corpus.", "labels": [], "entities": [{"text": "NMT training corpus", "start_pos": 86, "end_pos": 105, "type": "DATASET", "confidence": 0.7593598365783691}]}, {"text": "The development set (used for tuning the parameters of the log-linear model in SMT and for early stopping in NMT) was newstest2016, while the test set was newstest2017.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.968448281288147}, {"text": "NMT", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.7628475427627563}, {"text": "newstest2016", "start_pos": 118, "end_pos": 130, "type": "DATASET", "confidence": 0.9011515378952026}, {"text": "newstest2017", "start_pos": 155, "end_pos": 167, "type": "DATASET", "confidence": 0.9542916417121887}]}, {"text": "presents the (cased) BLEU scores obtained by the MT systems built.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9968739748001099}, {"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.8488510847091675}]}, {"text": "It can be observed that the scores of NMT systems trained on random subsamples (random baseline) are very low if we compare them with SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.7451362609863281}]}, {"text": "This confirms that NMT is very sensitive to noisy training data).", "labels": [], "entities": []}, {"text": "An important increase in BLEU for all systems can be observed when filtering with hard rules and classifier (only-classifier system).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9992004036903381}]}, {"text": "After this filtering, NMT outperforms SMT for both training set sizes.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9910723567008972}]}, {"text": "Concerning our submissions, results show that adding n-gram saturation (prompsit-sat) slightly improves the results in the four datasets, which confirms that vocabulary diversity is relevant for this task.", "labels": [], "entities": []}, {"text": "We can also observe in Table 3 that the number of unknown words in the test set was slightly reduced.", "labels": [], "entities": []}, {"text": "Our active learning strategy for achieving vocabulary diversity (prompsit-al), however, brought a degradation in the 10M dataset and alight improvement in the 100M one.", "labels": [], "entities": []}, {"text": "If we analyze vocabulary sizes (displayed in), it was reduced (in comparison with prompsit-sat) only for the 10M dataset, and the number of unknown words in the test set increased.", "labels": [], "entities": [{"text": "10M dataset", "start_pos": 109, "end_pos": 120, "type": "DATASET", "confidence": 0.7451682388782501}]}, {"text": "A potential solution for this issue could be reducing the block size for the first iterations of the active learning algorithm, so that more itera- tions are executed before obtaining the 10M training set.", "labels": [], "entities": []}, {"text": "The submissions that aimed at increasing the fluency of the training corpus brought alight improvement in translation quality for the NMT system trained on the 100M dataset.", "labels": [], "entities": [{"text": "translation", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.948664128780365}, {"text": "100M dataset", "start_pos": 160, "end_pos": 172, "type": "DATASET", "confidence": 0.6963724344968796}]}, {"text": "On the contrary, they further reduced the vocabulary sizes and increased the unknown rate for the 10M dataset.", "labels": [], "entities": [{"text": "unknown rate", "start_pos": 77, "end_pos": 89, "type": "METRIC", "confidence": 0.9806664884090424}, {"text": "10M dataset", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.7893864512443542}]}, {"text": "We believe this is due to the fact that, with this approach, fluency had a stronger influence than vocabulary diversity in the criterion for selecting sentences for the small dataset.", "labels": [], "entities": []}, {"text": "Only the top 836 520 sentences with smallest perplexity were explored for building the final 10M training corpus obtained with prompsit-lm, which contained 551 098 sentences.", "labels": [], "entities": []}, {"text": "11 A manual inspection of the sentences included in the 100M dataset but not in the 10M one showed that they were perfectly fluent.", "labels": [], "entities": [{"text": "100M dataset", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.6809922754764557}]}, {"text": "This means fluent sentences which are more interesting (from a vocabulary point of view) have been ignored when building the 10M dataset, since the process is mainly guided by perplexity.", "labels": [], "entities": [{"text": "10M dataset", "start_pos": 125, "end_pos": 136, "type": "DATASET", "confidence": 0.8390995860099792}]}, {"text": "This problem disappears in the large data set, that is large enough to contain diverse vocabulary.", "labels": [], "entities": []}, {"text": "The BLEU scores reported in this section do not exactly match those published in the official results () because, unlike the scores reported in this paper, the official scores were averaged over multiple training runs and multiple evaluation corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9989573955535889}]}, {"text": "Nevertheless, the relative performance of our four submissions remains the same.", "labels": [], "entities": []}, {"text": "Our active learning and language model scoring strategies were very competitive for the 100M dataset and were ranked very close to the top performing systems, while our best performing submissions for the 10M dataset were in the middle of the ranking.", "labels": [], "entities": [{"text": "100M dataset", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.6762357205152512}, {"text": "10M dataset", "start_pos": 205, "end_pos": 216, "type": "DATASET", "confidence": 0.7083930820226669}]}, {"text": "The difference between these two numbers is the amount of sentences removed by the n-gram saturation algorithm.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores obtained by our 4 submissions and two baseline approaches.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988343119621277}]}]}