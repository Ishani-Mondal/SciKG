{"title": [{"text": "BioAMA: Towards an End to End BioMedical Question Answering System", "labels": [], "entities": [{"text": "BioMedical Question Answering", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.6225785116354624}]}], "abstractContent": [{"text": "In this paper, we present a novel Biomedi-cal Question Answering system, BioAMA: \"Biomedical Ask Me Anything\" on task 5b of the annual BioASQ challenge (Ba-likas et al., 2015).", "labels": [], "entities": [{"text": "Biomedi-cal Question Answering", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.5770516097545624}]}, {"text": "We focus on a wide variety of question types including factoid, list based, summary and yes/no type questions that generate both exact and well-formed 'ideal' answers.", "labels": [], "entities": []}, {"text": "For summary-type questions, we combine effective IR-based techniques for retrieval and diversification of relevant snippets fora question to create an end-to-end system which achieves a ROUGE-2 score of 0.72 and a ROUGE-SU4 score of 0.71 on ideal answer questions (7% improvement over the previous best model).", "labels": [], "entities": [{"text": "ROUGE-2 score", "start_pos": 186, "end_pos": 199, "type": "METRIC", "confidence": 0.9803125858306885}, {"text": "ROUGE-SU4 score", "start_pos": 214, "end_pos": 229, "type": "METRIC", "confidence": 0.9794409275054932}]}, {"text": "Additionally, we propose a novel Natural Language Inference (NLI) based framework to answer the yes/no questions.", "labels": [], "entities": []}, {"text": "To train the NLI model, we also devise a transfer-learning technique by cross-domain projection of word embeddings.", "labels": [], "entities": []}, {"text": "Finally, we present a two-stage approach to address the factoid and list type questions by first generating a candidate set using NER taggers and ranking them using both supervised and unsu-pervised techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the era of ever advancing medical sciences and the age of the internet, a remarkable amount of medical literature is constantly being posted online.", "labels": [], "entities": []}, {"text": "This has led to a need for an effective retrieval and indexing system which can allow us to extract meaningful information from these vast knowledge sources.", "labels": [], "entities": []}, {"text": "One of the most effective and natural ways to leverage this huge amount of data in real life is to build a Question Answering (QA) system which will allow us to directly query this data and extract meaningful and structured information in a human readable form.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.8187491714954376}]}, {"text": "Our key novel contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We achieve state of the art results in automatic evaluation measures for the ideal answer questions in Task 5b of the BioASQ dataset, yielding a 7% improvement over the previous state of the art system ().", "labels": [], "entities": [{"text": "BioASQ dataset", "start_pos": 118, "end_pos": 132, "type": "DATASET", "confidence": 0.9438180327415466}]}, {"text": "2. We introduce a novel NLI-based approach for answering the yes/no style questions in the BioASQ dataset.", "labels": [], "entities": [{"text": "BioASQ dataset", "start_pos": 91, "end_pos": 105, "type": "DATASET", "confidence": 0.9538109600543976}]}, {"text": "We model this as a Textual Entailment (TE) problem and use Hierarchical Convolutional Neural Network based Infersent models () to answer the question.", "labels": [], "entities": [{"text": "Textual Entailment (TE) problem", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.7805357972780863}]}, {"text": "To address the challenge of inadequate training data, we also introduce a novel embedding projection technique which allows for effective transfer learning from models trained on larger datasets with a different vocabulary to work well on the much smaller BioASQ dataset.", "labels": [], "entities": [{"text": "BioASQ dataset", "start_pos": 256, "end_pos": 270, "type": "DATASET", "confidence": 0.9598631858825684}]}, {"text": "3. We present two-stage approach to answer factoid and list type questions.", "labels": [], "entities": []}, {"text": "By using an ensemble of biomedical NER taggers to generate a candidate answer set, we devise unsupervised and supervised ranking algorithms to generate the final predictions.", "labels": [], "entities": [{"text": "NER taggers", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.8264626860618591}]}, {"text": "4. We improve upon the MMR framework for relevant sentence selection from the chosen snippets that was introduced in the work of.", "labels": [], "entities": [{"text": "MMR", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9308323264122009}, {"text": "relevant sentence selection", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6139076550801595}]}, {"text": "We experiment with a number of more informative similarity metrics to replace and improve upon the baseline Jaccard similarity metric.", "labels": [], "entities": []}], "datasetContent": [{"text": "The pipeline described above is primarily designed to improve the ROUGE evaluation metric.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.7041438817977905}]}, {"text": "Although a higher ROUGE score does not necessarily reflect improved human readability, MMR can improve readability by reducing redundancy in generated answers.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.974495142698288}, {"text": "MMR", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9177922010421753}]}, {"text": "Results for ideal answers for Task 5 phase bare shown in Table 1.", "labels": [], "entities": []}, {"text": "We also compare our results with other state of the art approaches in.", "labels": [], "entities": []}, {"text": "For parsing the questions, we used BLLIP reranking parser) (Charniak-Johnson parser) and used the model GENIA+PubMed for biomedical text.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.966245174407959}, {"text": "BLLIP", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9651231169700623}, {"text": "GENIA+PubMed", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.8538566827774048}]}, {"text": "For training the textual entailment classifier using InferSent's sentence embeddings, we used Stanford's SNLI dataset () to achieve a test-set accuracy of 84.7%.", "labels": [], "entities": [{"text": "textual entailment classifier", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.6417697370052338}, {"text": "SNLI dataset", "start_pos": 105, "end_pos": 117, "type": "DATASET", "confidence": 0.8223195374011993}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9890860319137573}]}], "tableCaptions": [{"text": " Table 1: ROUGE scores for different experiments  on similarity metrics for extractive summarization", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9852428436279297}, {"text": "summarization", "start_pos": 87, "end_pos": 100, "type": "TASK", "confidence": 0.8396279811859131}]}, {"text": " Table 3: Baseline recall of different NER Taggers  measured by the fraction of questions that can be  answered by an ideal classifier if the candidates  are chosen using the tagger. We also measure pre- cision as the fraction of total unique tokens from  the documents that are tagged.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9311979413032532}, {"text": "NER Taggers", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.880003809928894}]}, {"text": " Table 4: Comparison of our model with other state of the art approaches", "labels": [], "entities": []}]}