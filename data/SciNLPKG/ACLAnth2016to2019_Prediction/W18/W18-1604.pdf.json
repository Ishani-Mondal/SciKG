{"title": [{"text": "Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting", "labels": [], "entities": [{"text": "Evaluating Creative Language Generation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7918651252985001}]}], "abstractContent": [{"text": "Language generation tasks that seek to mimic human ability to use language creatively are difficult to evaluate, since one must consider creativity, style, and other non-trivial aspects of the generated text.", "labels": [], "entities": [{"text": "Language generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6802859902381897}]}, {"text": "The goal of this paper is to develop evaluations methods for one such task, ghostwriting of rap lyrics, and to provide an explicit, quantifiable foundation for the goals and future directions for this task.", "labels": [], "entities": []}, {"text": "Ghostwriting must produce text that is similar in style to the emulated artist, yet distinct in content.", "labels": [], "entities": []}, {"text": "We develop a novel evaluation methodology that addresses several complementary aspects of this task, and illustrate how such evaluation can be used to meaningfully analyze system performance.", "labels": [], "entities": []}, {"text": "We provide a corpus of lyrics for 13 rap artists, annotated for stylistic similarity, which allows us to assess the feasibility of manual evaluation for generated verse.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language generation tasks are often among the most difficult to evaluate.", "labels": [], "entities": [{"text": "Language generation tasks", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7723390857378641}]}, {"text": "Evaluating machine translation, image captioning, summarization, and other similar tasks is typically done via comparison with existing human-generated \"references\".", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7288602888584137}, {"text": "image captioning", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7364928424358368}, {"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9727590680122375}]}, {"text": "However, human beings also use language creatively, and for the language generation tasks that seek to mimic this ability, determining how accurately the generated text represents its target is insufficient, as one also needs to evaluate creativity and style.", "labels": [], "entities": [{"text": "language generation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7595809996128082}]}, {"text": "We believe that one of the reasons such tasks receive little attention is the lack of sound evaluation methodology, without which no task is well-defined, and no progress can be made.", "labels": [], "entities": []}, {"text": "The goal of this paper is to develop an evaluation methodology for one such task, ghostwriting, or more specifically, ghostwriting of rap lyrics.", "labels": [], "entities": []}, {"text": "Ghostwriting is ubiquitous in politics, literature, and music; as such, it introduces a distinction between the performer/presenter of text, lyrics, etc, and the creator of text/lyrics.", "labels": [], "entities": []}, {"text": "The goal of ghostwriting is to present something in a style that is believable enough to be credited to the performer.", "labels": [], "entities": []}, {"text": "In the domain of rap specifically, rappers sometimes function as ghostwriters early on before embarking on their own public careers, and there are even businesses that provide written lyrics as a service . The goal of automatic ghostwriting is therefore to create a system that can take as input a given artist's work and generate similar yet unique lyrics.", "labels": [], "entities": []}, {"text": "Our objective in this work is to provide a quantifiable direction and foundation for the task of rap lyric generation and similar tasks through (1) developing an evaluation methodology for such models and (2) illustrating how such evaluation can be used to analyze system performance, including advantages and limitations of a specific language model developed for this task.", "labels": [], "entities": [{"text": "rap lyric generation", "start_pos": 97, "end_pos": 117, "type": "TASK", "confidence": 0.7444990475972494}]}, {"text": "As an illustration case, we use the ghostwriter model previously proposed in exploratory work by, which uses a recurrent neural network (RNN) with Long Short-Term Memory (LSTM) for rap lyric generation.", "labels": [], "entities": [{"text": "Long Short-Term Memory (LSTM)", "start_pos": 147, "end_pos": 176, "type": "METRIC", "confidence": 0.809666727979978}, {"text": "rap lyric generation", "start_pos": 181, "end_pos": 201, "type": "TASK", "confidence": 0.7031228542327881}]}, {"text": "The following are the main contributions of this paper.", "labels": [], "entities": []}, {"text": "We present a comprehensive manual evaluation methodology of the generated verses along three key aspects: fluency, coherence, and style matching.", "labels": [], "entities": [{"text": "style matching", "start_pos": 130, "end_pos": 144, "type": "TASK", "confidence": 0.6366000175476074}]}, {"text": "We introduce an improvement to the semi-automatic methodology used by that automatically penalizes repetitive text, removing the need for manual intervention.", "labels": [], "entities": []}, {"text": "Finally, we build a corpus of lyrics for 13 rap artists, each with his own unique style, and conduct a comprehensive evaluation of the LSTM model performance using the new evaluation methodol-ogy.", "labels": [], "entities": []}, {"text": "The corpus includes style matching annotation for select verses in dataset, which can form a gold standard for future work on automatic representation of similarity between artists' styles.", "labels": [], "entities": []}, {"text": "The resulting rap lyric dataset is publicly available from the authors' website 2 . Additionally, we believe that the annotation method we propose for manual style evaluation can be used for other similar generation tasks.", "labels": [], "entities": [{"text": "manual style evaluation", "start_pos": 151, "end_pos": 174, "type": "TASK", "confidence": 0.6268248558044434}]}, {"text": "One example is 'Deep Art' work in the computer vision community that seeks to apply the style of a particular painting to other images (.", "labels": [], "entities": []}, {"text": "Although manual inspection of results of such models suggests reasonable success, a systemic evaluation methodology has yet to be proposed.", "labels": [], "entities": []}, {"text": "With this in mind, we make the interface used for style evaluation in this work available for public use.", "labels": [], "entities": [{"text": "style evaluation", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.7673753499984741}]}, {"text": "Our evaluation results highlight the truly multifaceted nature of the ghostwriting task.", "labels": [], "entities": []}, {"text": "While having a single measure of success is clearly desirable, our analysis shows the need for complementary metrics that evaluate different components of the overall task.", "labels": [], "entities": []}, {"text": "Indeed, despite the fact that our test-case LSTM model outperforms a baseline model across numerous artists based on automated evaluation, the full set of evaluation metrics is able to showcase the LSTM model's strengths and weakness.", "labels": [], "entities": []}, {"text": "The coherence evaluation demonstrates the difficulty of incorporating large amounts of training data into the LSTM model, which intuitively would be desirable to create a flexible ghostwriting model.", "labels": [], "entities": []}, {"text": "The style matching experiments suggest that the LSTM is effective at capturing an artist's general style, however, this may indicate that it tends to form 'average' verses, which are then more likely to be matched with existing verses from an artist rather than another random verse from the same artist.", "labels": [], "entities": []}, {"text": "Overall, the evaluation methodology we present provides an explicit, quantifiable foundation for the ghostwriting task, allowing fora deeper understanding of the task's goals and future research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "6 http://www.ohhla.com/ using the NLTK library ().", "labels": [], "entities": [{"text": "NLTK library", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.96885946393013}]}, {"text": "Since the preprocessing was done heuristically, the resulting dataset may still contain some text that is not actual verse, but rather dialogue or chorus lines.", "labels": [], "entities": []}, {"text": "We therefore filter out all verses that are shorter than 20 tokens.", "labels": [], "entities": []}, {"text": "Statistics of our dataset are shown in.", "labels": [], "entities": []}, {"text": "We believe that adequate evaluation for the ghostwriting task requires both manual and automatic approaches.", "labels": [], "entities": []}, {"text": "The automated evaluation methodology enables large-scale analysis of the generated verse.", "labels": [], "entities": []}, {"text": "However, given the nature of the task, the automated evaluation is notable to assess certain critical aspects of fluency and style, such as the vocabulary, the tone, and the themes preferred by a particular artist.", "labels": [], "entities": []}, {"text": "In this section, we present a manual methodology we propose for evaluating these aspects of the generated verse, as well as an improvement to the automatic methodology proposed by.", "labels": [], "entities": []}, {"text": "We have designed two annotation tasks for manual evaluation.", "labels": [], "entities": []}, {"text": "The first task is to determine how fluent and coherent the generated verses are.", "labels": [], "entities": []}, {"text": "The second task is to evaluate manually how well the generated verses match the style of the target artist.", "labels": [], "entities": []}, {"text": "Fluency/Coherence Evaluation Given a generated verse, we ask annotators to determine the fluency and coherence of the lyrics.", "labels": [], "entities": []}, {"text": "Even though our evaluation is for systems that produce entire verses, we follow the work of Wu and annotate fluency, as well as coherence, at the line level.", "labels": [], "entities": []}, {"text": "To assess fluency, we ask to what extent a given line can be considered a valid English utterance.", "labels": [], "entities": []}, {"text": "Since a language model may produce highly disjointed verses as it progresses through the training process, we offer the annotator three options for grading fluency: strongly fluent, weakly fluent, and not fluent.", "labels": [], "entities": []}, {"text": "If a line is disjointed, i.e., it is only fluent in specific segments of the line, the annotators are instructed to mark it as weakly fluent.", "labels": [], "entities": []}, {"text": "The grade of not fluent is reserved for highly incoherent text.", "labels": [], "entities": []}, {"text": "To assess coherence, we ask the annotator how well a given line matches the preceding line.", "labels": [], "entities": []}, {"text": "That is, how believable is it that these two lines would follow each other in a rap verse.", "labels": [], "entities": []}, {"text": "We offer the annotators the same choices as in the fluency evaluation: strongly coherent, weakly coherent, and not coherent.", "labels": [], "entities": []}, {"text": "During the training process, a language model may output the same line repeatedly.", "labels": [], "entities": []}, {"text": "We account for this in our coherence evaluation by defining the consecutive repetition of a line as not coherent.", "labels": [], "entities": []}, {"text": "This is important to define because the line on its own maybe strongly fluent, however, it is not correct to consider averse that consists of a single fluent line repeated indefinitely to be coherent.", "labels": [], "entities": []}, {"text": "Style Matching The goal of the style matching annotation is to determine how well a given verse captures the style of the target artist.", "labels": [], "entities": [{"text": "Style Matching", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7166673541069031}]}, {"text": "In this annotation task, a user is presented with averse and asked to compare it against four other verses.", "labels": [], "entities": []}, {"text": "The goal is to pick the verse that is written in a similar style.", "labels": [], "entities": []}, {"text": "One of the four choices is always averse from the same artist that was used to generate the verse being evaluated.", "labels": [], "entities": []}, {"text": "The other three verses are chosen from the remaining artists in our dataset.", "labels": [], "entities": []}, {"text": "Each verse is evaluated in this manner four times, each time against different verses, so that it has the chance to get matched with averse from each of the remaining twelve artists.", "labels": [], "entities": []}, {"text": "The generated verse is considered stylistically consistent if the annotators tend to select the verse that belongs to the target artist.", "labels": [], "entities": []}, {"text": "To evaluate the difficulty of this task, we also perform style matching annotation for authentic verse, in which the evaluated verse is not generated, but rather is an actual existing verse from the target artist.", "labels": [], "entities": []}, {"text": "The results of our automated evaluation are shown in.", "labels": [], "entities": []}, {"text": "For each artist, we calculate their average rhyme density across all verses.", "labels": [], "entities": [{"text": "rhyme density", "start_pos": 44, "end_pos": 57, "type": "METRIC", "confidence": 0.9455529749393463}]}, {"text": "We then use this value to determine at which iteration this rhyme density is achieved during generation (using the regression line for rhyme density).", "labels": [], "entities": []}, {"text": "Next, we use the maximum similarity regression to determine the maximum similarity score at that iteration.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 72, "end_pos": 88, "type": "METRIC", "confidence": 0.8993556201457977}]}, {"text": "Low maximum similarity score indicates that we have maintained stylistic similarity while producing new, previously unseen lyrics.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.8668093383312225}]}, {"text": "Note that the reason for negative numbers in is that in the beginning of training (in case of LSTM) and at a low n-gram length (for the baseline model), the models actually achieved a rhyme density that exceeded the artist's average rhyme density.", "labels": [], "entities": []}, {"text": "As a result, the rhyme density regression line hits the average rhyme density on a negative iteration.", "labels": [], "entities": []}, {"text": "The main generative model we use in our evaluation experiments is an LSTM.", "labels": [], "entities": []}, {"text": "Similar to, we use an n-gram model as a baseline system for automated evaluation.", "labels": [], "entities": []}, {"text": "We refer the reader to the original work fora detailed description.", "labels": [], "entities": []}, {"text": "After every 100 iterations of training 10 the LSTM model generates averse.", "labels": [], "entities": []}, {"text": "For the baseline model, we generate five verses at values 1-9 for n.", "labels": [], "entities": []}, {"text": "We see a correspondence between higher n and higher iteration: as both increase, the models become more 'fit' to the training data.", "labels": [], "entities": []}, {"text": "For the baseline model, we use the verses generated at different n-gram lengths (n = 1...9) to obtain the values for regression.", "labels": [], "entities": []}, {"text": "At every value of n, we take the average rhyme density and maximum similarity score of the five verses that we generate to create a single data point for rhyme density and maximum similarity score, respectively.", "labels": [], "entities": [{"text": "maximum similarity score", "start_pos": 59, "end_pos": 83, "type": "METRIC", "confidence": 0.697291781504949}, {"text": "maximum similarity score", "start_pos": 172, "end_pos": 196, "type": "METRIC", "confidence": 0.6657369136810303}]}, {"text": "To enable comparison, we also create nine data points from the verses generated by the LSTM, which is done as follows: a separate model for each artist is trained fora minimum of 16,400 iterations.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.7712289094924927}]}, {"text": "We take the verses generated every 2,000 iterations, from 0 to 16,000 iterations, giving us nine points.", "labels": [], "entities": []}, {"text": "The averages for each point are obtained by using the verses generated in iterations \u00b1x, x \u2208 {100, 200, 300, 400} for each interval of 2,000.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Rap lyrics dataset statistics. Vocabulary richness measures how varied an artist's vocabulary is,  computed as the total number of words divided by vocabulary size.", "labels": [], "entities": []}, {"text": " Table 3: The results of the automated evaluation. The bold indicates the system with a lower similarity at  the target rhyme density.", "labels": [], "entities": []}, {"text": " Table 4: The correlation between the four metrics  we have developed: Coherence, Fluency, similar- ity score based on automated evaluation (Similar- ity), and Style Matching (Matching).", "labels": [], "entities": [{"text": "Fluency", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9969070553779602}, {"text": "Style Matching", "start_pos": 160, "end_pos": 174, "type": "TASK", "confidence": 0.7262328565120697}]}]}