{"title": [], "abstractContent": [{"text": "Recent neural models for response generation show good results in terms of general responses.", "labels": [], "entities": [{"text": "response generation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8744438588619232}]}, {"text": "In real conversations, however, depending on the speaker/responder, similar utterances should require different responses.", "labels": [], "entities": []}, {"text": "In this study, we attempt to consider individual user's information in adjusting the notable sequence-to-sequence (seq2seq) model for more diverse, user-specific responses.", "labels": [], "entities": []}, {"text": "We assume that we need user-specific features to adjust the response and we argue that some selected representative words from the users are suitable for this task.", "labels": [], "entities": []}, {"text": "Furthermore, we prove that even for unseen or unknown users, our model can provide more diverse and interesting responses, while maintaining correlation with input utterances.", "labels": [], "entities": []}, {"text": "Experimental results with human evaluation show that our model can generate more interesting responses than the popular seq2seqmodel and achieve higher relevance with input utterances than our base-line.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human-computer conversation is a challenging task in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.7311010261376699}]}, {"text": "The aim of conversation models is to generate fluent and relevant responses given an input in a free format, i.e., not just in the form of a question.", "labels": [], "entities": []}, {"text": "A large amount of available data on the Internet has sparked the shift in conversation models.", "labels": [], "entities": []}, {"text": "Starting with, completely datadriven models are now commonly used to generate responses.", "labels": [], "entities": []}, {"text": "Furthermore, the sequence-tosequence (seq2seq) model initiated by  notably to machine translation (MT) and response generation.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.8357924103736878}, {"text": "response generation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7002512514591217}]}, {"text": "Actual conversations involving humans would be more engaging and the responses are not always general and monotonic.", "labels": [], "entities": []}, {"text": "However, neural conversation models tend to generate safe, general, and uninteresting responses, e.g., I don't know or I'm OK ().", "labels": [], "entities": []}, {"text": "We argue that, aside from adding or understanding the context of a conversation, speaking style and response diversity also play an important role in delivering a more interesting conversation.", "labels": [], "entities": []}, {"text": "Recent studies addressed the response diversity and engagement issues and have attempted to generate responses better than the common and general ones.", "labels": [], "entities": []}, {"text": "Some tackled this issue by defining and emphasizing context; previous utterances are commonly used as context in a conversation).", "labels": [], "entities": []}, {"text": "Other studies have attempted to diversify or manipulate responses using specific attributes such as user identification (), profile information sets (), topics (, and speci-fied mechanisms ( . In this study, we focus on the issue of \"response style.\"", "labels": [], "entities": []}, {"text": "We intend to let the model learn to generate responses that resemble those of areal person.", "labels": [], "entities": []}, {"text": "Given an input utterance and user-specific information, the model will generate a response relevant to the input utterance based on the given userspecific information.", "labels": [], "entities": []}, {"text": "The existing methods that exhibit the use of user-specific information (, usually require that the users appear in the training data.", "labels": [], "entities": []}, {"text": "Therefore, these existing methods cannot handle the unseen users, i.e., users that do not exist in the training data.", "labels": [], "entities": []}, {"text": "This is a limitation that we want to address in this study.", "labels": [], "entities": []}, {"text": "As we intend to make our model versatile, we want to cover also the users that are not present in the training data.", "labels": [], "entities": []}, {"text": "Hence, in this study, we propose a model that also works with unseen users.", "labels": [], "entities": []}, {"text": "Since we need identifiers of users, we rely on Twitter as the source of datasets.", "labels": [], "entities": []}, {"text": "The dataset used in this work was constructed by collecting tweets and replies, i.e., responses to other tweets.", "labels": [], "entities": []}, {"text": "Aside from the user identity, to construct user-specific information, we retrieved individual public tweets from each account that are not replies to other tweets.", "labels": [], "entities": []}, {"text": "We assume that some selected representative words from the retrieved individual tweets are suitable as the user's information.", "labels": [], "entities": []}, {"text": "Therefore, we use two types of user-specific information: user identities and collections of users' representative words.", "labels": [], "entities": []}, {"text": "Unlike other tasks that can assume a finite set of expected outputs, e.g., machine translation, in response generation, an input utterance can elicit various responses.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7704620361328125}, {"text": "response generation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7306549996137619}]}, {"text": "Thus, measuring the quality of the output becomes a formidable issue.", "labels": [], "entities": []}, {"text": "To measure the quality of generated responses, we rely on human judgment.", "labels": [], "entities": []}, {"text": "Three evaluation criteria are provided to the judges: fluency, relevance, and style.", "labels": [], "entities": [{"text": "relevance", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9865363240242004}, {"text": "style", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9899449944496155}]}, {"text": "The results show that our model is significantly better than the baseline in relevance and style.", "labels": [], "entities": [{"text": "relevance", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9355741143226624}]}, {"text": "Some examples of generated responses from our model are shown in.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since our target is to incorporate and emphasize the response styles of actual human responders, we need to include user identification attributes in the datasets.", "labels": [], "entities": []}, {"text": "Therefore, for datasets, we collected tweets from Twitter API.", "labels": [], "entities": []}, {"text": "Then, we constructed two types of datasets: conversation dataset and user-info dataset.", "labels": [], "entities": []}, {"text": "This dataset is designated to train the model to generate a response to a given input utterance in general.", "labels": [], "entities": []}, {"text": "We extracted this dataset from Twitter, and retrieved only those tweets that satisfy the following conditions.", "labels": [], "entities": []}, {"text": "We set a filter to select only reply tweets, i.e., responses to other tweets, from users who had engaged in conversations with a minimum of three turns.", "labels": [], "entities": []}, {"text": "We paired each reply with the tweet that it is a response of, as response and input utterance, respectively.", "labels": [], "entities": []}, {"text": "We then used the responders' usernames as the user identification attribute, hence user embeddings.", "labels": [], "entities": []}, {"text": "Note that the user embeddings can only be obtained from this conversation dataset.", "labels": [], "entities": []}, {"text": "To improve data quality, we further cleaned up the retrieved tweets to remove some noises, such as tweets with non-ASCII characters, duplications, and non-English tweets.", "labels": [], "entities": []}, {"text": "We also removed URLs, hashtags, and mentions from tweets.", "labels": [], "entities": []}, {"text": "The final conversation dataset consists of around 230,000 pairs of input utterances and responses.", "labels": [], "entities": []}, {"text": "This dataset is an effort to capture more characteristic of the users and also to handle the unseen users in the training dataset.", "labels": [], "entities": []}, {"text": "User-info embeddings mentioned in Section 4.1 are derived from this dataset.", "labels": [], "entities": []}, {"text": "To construct user-info dataset, we retrieved tweets from the accounts of every username in the conversation dataset.", "labels": [], "entities": []}, {"text": "To ensure that this dataset is independent from the conversation dataset, we retrieved only individual tweets, i.e., non-replies as opposed to the reply tweets for conversation dataset.", "labels": [], "entities": []}, {"text": "We retrieved all public tweets, via Twitter API, from each account and then applied TF-IDF to find the most important words for each user.", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9084014892578125}]}, {"text": "For an individual user, we treated one tweet (sentence) as one document and hence computed the TF-IDF score for each word across all sentences.", "labels": [], "entities": [{"text": "TF-IDF score", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.9759873449802399}]}, {"text": "Then, we kept the top 50 words according to the TF-IDF scores.", "labels": [], "entities": [{"text": "TF-IDF scores", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.6623027622699738}]}, {"text": "The usage of this dataset is independent of the conversation dataset.", "labels": [], "entities": []}, {"text": "We can pair the user information in the user-info dataset with the one, the same user, in the conversation dataset or we can disregard the relationship.", "labels": [], "entities": []}, {"text": "Many previous studies on dialogue or response generation models () relied on BLEU () as their automatic evaluation metric.", "labels": [], "entities": [{"text": "dialogue or response generation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6437693908810616}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9972002506256104}]}, {"text": "To compute the score, BLEU measures the overlapping words or n-grams between the generated output (hypothesis) and the target output (reference).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9981404542922974}]}, {"text": "BLEU was initially intended for machine translation, which tends to have a finite target; therefore, it might not be suitable for evaluating conversation models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9743430018424988}, {"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.810287207365036}]}, {"text": "According to, BLEU is lowly correlated with human judgments of dialogue systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9987137317657471}]}, {"text": "Additionally, some other work on response generation () did not use BLEU for their evaluation method, relying on human judgment instead.", "labels": [], "entities": [{"text": "response generation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8895537853240967}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9962372779846191}]}, {"text": "Thus, we opted to use only human evaluation in our work.", "labels": [], "entities": []}, {"text": "We hired judges from Amazon Mechanical Turk (AMT) to evaluate the quality of our generated responses.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 21, "end_pos": 49, "type": "DATASET", "confidence": 0.8733220994472504}]}, {"text": "The following three judgment criteria were defined: \u2022 fluency or naturalness: Whether the response could be produced by (an English speaking) human.", "labels": [], "entities": []}, {"text": "\u2022 relevance or adequacy: Whether the response could be accepted as a suitable answer or contained useful information regarding the input utterance.", "labels": [], "entities": [{"text": "relevance", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.95610511302948}]}, {"text": "\u2022 style: Whether the response could be produced by the same person if some profile information was provided.", "labels": [], "entities": [{"text": "style", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9961143732070923}]}, {"text": "The rationale behind measuring these criteria is as follows.", "labels": [], "entities": []}, {"text": "Even though our goal is to integrate styles to the generated responses, we also want to assure that the generated responses are correct and useful to the input.", "labels": [], "entities": []}, {"text": "Since we supposed that style is significantly harder to evaluate, the evaluation task was done in two stages: the first stage was for fluency and relevance, and the second stage was for style.", "labels": [], "entities": [{"text": "style", "start_pos": 186, "end_pos": 191, "type": "METRIC", "confidence": 0.9389705657958984}]}, {"text": "We randomly picked 12 users from the conversation dataset and retrieved tweets that they replied to.", "labels": [], "entities": []}, {"text": "For each user, 5-10 tweets were obtained to be used as input utterances.", "labels": [], "entities": []}, {"text": "In total, 100 tweets were collected, and each pair of an input utterance and its response was then evaluated by 10 judges.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Human evaluation results for fluency and relevance, presented as raw score percentages. Our  UNK + Info model with unseen users gains 26.5% more for fluency and 36.5% more for relevance  compared to the baseline.", "labels": [], "entities": [{"text": "UNK + Info", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.8156387408574423}]}, {"text": " Table 4: Acceptable or \"Good enough\" results  with good and enough scores combined. seq2seq  tops fluency, but our model with unseen users gets  the highest relevance score.", "labels": [], "entities": [{"text": "Acceptable", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9983593821525574}]}]}