{"title": [{"text": "STEVENDU2018's system in VarDial 2018: Discriminating between Dutch and Flemish in Subtitles", "labels": [], "entities": [{"text": "STEVENDU2018", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7455295324325562}, {"text": "VarDial 2018", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.8532719910144806}]}], "abstractContent": [{"text": "This paper introduces the submitted system for team STEVENDU2018 during VarDial 2018 (Zampieri et al., 2018) Discriminating between Dutch and Flemish in Subtitles(DFS).", "labels": [], "entities": [{"text": "STEVENDU2018", "start_pos": 52, "end_pos": 64, "type": "METRIC", "confidence": 0.568267285823822}, {"text": "VarDial", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.582342267036438}, {"text": "Discriminating between Dutch and Flemish in Subtitles(DFS)", "start_pos": 109, "end_pos": 167, "type": "TASK", "confidence": 0.7252002537250519}]}, {"text": "Post evaluation analyses are also presented.", "labels": [], "entities": []}, {"text": "The results obtained indicate that it is a challenging task to discriminate between Dutch and Flemish.", "labels": [], "entities": []}], "introductionContent": [{"text": "The DFS task is a supervised learning task to classify text into Dutch or Flemish.", "labels": [], "entities": []}, {"text": "Dutch is the language spoken in the Netherlands and Flemish is a variant of Dutch language and also known as Belgian Dutch.", "labels": [], "entities": []}, {"text": "There are 300000 labeled training data, 500 labeled development data, 20000 on-hold test data (van der.", "labels": [], "entities": []}, {"text": "DUT in training data denotes Dutch, and BEL is the label for Flemish.", "labels": [], "entities": [{"text": "DUT in training data", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.6202685832977295}, {"text": "BEL", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9984408020973206}]}, {"text": "F1 score is the evaluation metric.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9819639325141907}]}, {"text": "This paper is structured as follows: first, a brief training data analysis will be given.", "labels": [], "entities": []}, {"text": "Then systems trained during the evaluation will be introduced.", "labels": [], "entities": []}, {"text": "Finally more systems will be explored for post evaluation analysis.", "labels": [], "entities": [{"text": "post evaluation analysis", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.6558615763982137}]}], "datasetContent": [{"text": "There are two systems trained during evaluation: a bag-of-ngram model and dual convolutional neural network model.: Statistics for the punctuation in training data set.", "labels": [], "entities": []}, {"text": "The score on the released test set range from 0.55 to 0.66 in, our bag-of-ngram, the most simple approach yields 0.623.", "labels": [], "entities": []}, {"text": "On the other hand the proposed Dual-CNN yields 0.621.", "labels": [], "entities": []}, {"text": "The test score correlated well with the local cross validation score, development set is not the right choice for model selection.", "labels": [], "entities": []}, {"text": "The best score is just 0.66, which implies that the DFS task is challenging.", "labels": [], "entities": [{"text": "DFS task", "start_pos": 52, "end_pos": 60, "type": "TASK", "confidence": 0.7081592082977295}]}, {"text": "Since the bag-of-ngram system only scores 0.623 on test set, to achieve better result a series of studies had been carryout after the evaluation.", "labels": [], "entities": []}, {"text": "These can be broadly divided into three groups: one group focus on finding the vector representation for the given text data, another group focus on deep learning approaches, third group utilize existing text classification framework.", "labels": [], "entities": [{"text": "text classification", "start_pos": 204, "end_pos": 223, "type": "TASK", "confidence": 0.7049166560173035}]}], "tableCaptions": [{"text": " Table 1: Statistics for the training data set.", "labels": [], "entities": [{"text": "training data set", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.7632211844126383}]}, {"text": " Table 2: Statistics for the punctuation in training data set.", "labels": [], "entities": [{"text": "training data set", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.7651497423648834}]}, {"text": " Table 4: F1 scores for mean word vector system", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9709424376487732}, {"text": "mean word vector", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.6254663268725077}]}, {"text": " Table 6: F1 scores for popular deep learning based approaches", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.999506950378418}]}]}