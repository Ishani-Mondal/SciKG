{"title": [{"text": "The WMT18 Morpheval test suites for", "labels": [], "entities": [{"text": "WMT18 Morpheval test", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.860443909962972}]}], "abstractContent": [{"text": "Progress in the quality of machine translation output calls for new automatic evaluation procedures and metrics.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.8260313073794047}]}, {"text": "In this paper, we extend the Morpheval protocol introduced by Burlot and Yvon (2017) for the English-to-Czech and English-to-Latvian translation directions to three additional language pairs, and report its use to analyze the results of WMT 2018's participants for these language pairs.", "labels": [], "entities": []}, {"text": "Considering additional, typologically varied source and target languages also enables us to draw some generalizations regarding this morphology-oriented evaluation procedure.", "labels": [], "entities": []}], "introductionContent": [{"text": "The success of rather opaque neural machine translation systems has called for more finegrained types of evaluation than traditional automatic evaluation metrics offer.", "labels": [], "entities": []}, {"text": "In particular, we would like to obtain more detailed information about systems performance than just one overall number (even if it correlates well with human judgement).", "labels": [], "entities": []}, {"text": "Evaluation metrics that focus on various aspects of the translation, such as syntax or morphology, rather than on general translation quality, have thus seen renewed interest.", "labels": [], "entities": []}, {"text": "This interest has spurred the inclusion of additional test suites into the WMT 2018 news translation task.", "labels": [], "entities": [{"text": "WMT 2018 news translation task", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.7239622831344604}]}, {"text": "in the following) present a test suite for evaluating the morphological competence of machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7284332513809204}]}, {"text": "They provide a set of sentence pairs in the source language that differ by one morphological contrast.", "labels": [], "entities": []}, {"text": "A sentence pair is considered correct if the morphological contrast is also conveyed in the target language translations of the two sentences of the pair.", "labels": [], "entities": []}, {"text": "B&Y developed their test suite for English-Czech and English-Latvian and applied it to a selection of MT systems that participated in WMT 2017.", "labels": [], "entities": [{"text": "B&Y", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9387028813362122}, {"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9650910496711731}, {"text": "WMT 2017", "start_pos": 134, "end_pos": 142, "type": "TASK", "confidence": 0.5593633502721786}]}, {"text": "For WMT 2018, we have extended the English-Czech test suite 1 and created similar Morpheval test suites for three additional translation directions: English-German, English-Finnish, and Turkish-English.", "labels": [], "entities": []}, {"text": "All primary WMT submissions of these translation directions were evaluated.", "labels": [], "entities": [{"text": "WMT submissions", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.8818116188049316}]}, {"text": "We start by summarizing the components of the Morpheval test suites and their language-specific implementations.", "labels": [], "entities": [{"text": "Morpheval test suites", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.9236660401026408}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: BLEU scores and human evaluation scores  computed on newstest-2018 for English-Czech.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988522529602051}, {"text": "newstest-2018", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9728102684020996}]}, {"text": " Table 3: BLEU scores and human evaluation scores  computed on newstest-2018 for English-German.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986691474914551}, {"text": "newstest-2018", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9750358462333679}]}, {"text": " Table 4: BLEU scores and human evaluation scores  computed on newstest-2018 for English-Finnish.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989257454872131}, {"text": "newstest-2018", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.972558856010437}]}, {"text": " Table 5:  BLEU scores computed on SETIMES2  and newstest-2018 and human evaluation scores on  newstest-2018 for Turkish-English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9994655251502991}, {"text": "SETIMES2", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.7013229131698608}, {"text": "newstest-2018", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.895269513130188}]}, {"text": " Table 6: Accuracy values for the English-Czech test suite (paradigm contrast features).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9971926808357239}]}, {"text": " Table 7: Accuracy values for the English-Czech test suite (agreement features).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9961808919906616}]}, {"text": " Table 8: Entropy values for the English-Czech test suite (consistency features).", "labels": [], "entities": [{"text": "Entropy", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9380254745483398}]}, {"text": " Table 6. Not taking  into account online systems whose architectures  are unknown, the table shows a contrast between  a Recurrent Neural Network model (uedin) and  a Transformer model (CUNI-Transformer). The former obtains slightly higher accuracies than the  latter. This is especially obvious in verb tasks  (past and conditional), as well as for noun num- ber. This might suggest that Transformer models  have more difficulty in conveying a morphological  feature from source to target. 12  However, we observe no such difference for  agreement features", "labels": [], "entities": []}, {"text": " Table 9: Accuracy values for the English-German test suite (paradigm contrast features).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9975783228874207}]}, {"text": " Table 10: Accuracy values for the English-German test suite (agreement features).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9969670176506042}]}, {"text": " Table 11: Entropy values for the English-German test suite (consistency features).", "labels": [], "entities": []}, {"text": " Table 12: Accuracy values for the English-Finnish test suite (paradigm completion features).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9950993657112122}]}, {"text": " Table 13: Accuracy values for the English-Finnish test suite (left: agreement features, right: rare word features).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9961013793945312}]}, {"text": " Table 14: Accuracy values for the English-Finnish test suite (stability features).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9980179071426392}]}, {"text": " Table 15: Accuracy values for the Turkish-English test suite.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9970899820327759}, {"text": "Turkish-English test suite", "start_pos": 35, "end_pos": 61, "type": "DATASET", "confidence": 0.8453075289726257}]}]}