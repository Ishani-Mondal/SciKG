{"title": [{"text": "Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment Analysis", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.9556982517242432}]}], "abstractContent": [{"text": "Multimodal machine learning is a core research area spanning the language, visual and acoustic modalities.", "labels": [], "entities": [{"text": "Multimodal machine learning", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7442076206207275}]}, {"text": "The central challenge in multimodal learning involves learning representations that can process and relate information from multiple modalities.", "labels": [], "entities": []}, {"text": "In this paper, we propose two methods for unsupervised learning of joint multimodal representations using sequence to sequence (Seq2Seq) methods: a Seq2Seq Modality Translation Model and a Hierarchical Seq2Seq Modality Translation Model.", "labels": [], "entities": [{"text": "Seq2Seq Modality Translation", "start_pos": 148, "end_pos": 176, "type": "TASK", "confidence": 0.5696458717187246}, {"text": "Hierarchical Seq2Seq Modality Translation", "start_pos": 189, "end_pos": 230, "type": "TASK", "confidence": 0.5489728152751923}]}, {"text": "We also explore multiple different variations on the multimodal inputs and outputs of these seq2seq models.", "labels": [], "entities": []}, {"text": "Our experiments on multimodal sentiment analysis using the CMU-MOSI dataset indicate that our methods learn informative mul-timodal representations that outperform the baselines and achieve improved performance on multimodal sentiment analysis , specifically in the Bimodal case where our model is able to improve F1 Score by twelve points.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.7922811706860861}, {"text": "CMU-MOSI dataset", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.9883873164653778}, {"text": "multimodal sentiment analysis", "start_pos": 214, "end_pos": 243, "type": "TASK", "confidence": 0.7348330219586691}, {"text": "F1 Score", "start_pos": 314, "end_pos": 322, "type": "METRIC", "confidence": 0.9669245183467865}]}, {"text": "We also discuss future directions for multimodal Seq2Seq methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis, which involves identifying a speaker's sentiment, is an open research problem.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9316640496253967}, {"text": "identifying a speaker's sentiment", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.6979283332824707}]}, {"text": "In this field, the majority of work done focused on unimodal methodologies -primarily textual analysis -where investigating was limited to identifying usage of words in positive and negative scenarios.", "labels": [], "entities": [{"text": "textual analysis", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7008478790521622}]}, {"text": "However, unimodal textual sentiment analysis through usage of words, phrases, and their interdependencies were found to be insufficient for extracting affective content from textual opinions.", "labels": [], "entities": [{"text": "unimodal textual sentiment analysis", "start_pos": 9, "end_pos": 44, "type": "TASK", "confidence": 0.6698598340153694}, {"text": "extracting affective content from textual opinions", "start_pos": 140, "end_pos": 190, "type": "TASK", "confidence": 0.7477805515130361}]}, {"text": "As a result, there has been a recent push towards using statistical methods to extract additional behavioral cues not present in the language modality from the video and audio modalities.", "labels": [], "entities": []}, {"text": "This research field is known as multimodal sentiment analysis and it extends the conventional text-based definition of sentiment analysis to a multimodal setup where different modalities contribute to modeling the sentiment of the speaker.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.7352044582366943}, {"text": "sentiment analysis", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.8786733448505402}]}, {"text": "For example, () explores modalities such as audio, while) explores a multimodal approach to predicting sentiment.", "labels": [], "entities": [{"text": "predicting sentiment", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.9351507723331451}]}, {"text": "This push has been further bolstered by the advent of multimodal social media platforms, such as YouTube, Facebook, and VideoLectures which are used to express personal opinions on a worldwide scale.", "labels": [], "entities": []}, {"text": "As a result, several multimodal datasets, such as CMU-MOSI () and later CMU-MOSEI (), ICT-MMMO () and, take advantage of the abundance of multimodal data on the Internet.", "labels": [], "entities": [{"text": "CMU-MOSI", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.9281507730484009}, {"text": "CMU-MOSEI", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.9286162257194519}]}, {"text": "At the same time, neural network based multimodal models have been proposed that are highly effective at learning multimodal representations for multimodal sentiment analysis.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 145, "end_pos": 174, "type": "TASK", "confidence": 0.7644954721132914}]}, {"text": "Recent progress has been limited to supervised learning using labeled data, and does not take advantage of the abundant unlabeled data on the Internet.", "labels": [], "entities": []}, {"text": "To address this gap, our work is primarily one of unsupervised representation learning.", "labels": [], "entities": []}, {"text": "We attempt to learn a multimodal representation of our data in a structured paradigm and explore whether a joint multimodal representation trained via unsupervised learning can improve the performance for multimodal sentiment analysis.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 205, "end_pos": 234, "type": "TASK", "confidence": 0.7583553989728292}]}, {"text": "While representation learning has been an area of rapid research in the past years, there has been limited work that explores multimodal setting.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.9588991105556488}]}, {"text": "To this end, we propose two methods: a Seq2Seq Modality Translation Model and a Hierarchical Seq2Seq Modality Translation Model for unsupervised learning of multimodal representations.", "labels": [], "entities": [{"text": "Seq2Seq Modality Translation", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.5669590731461843}, {"text": "Hierarchical Seq2Seq Modality Translation", "start_pos": 80, "end_pos": 121, "type": "TASK", "confidence": 0.546537846326828}]}, {"text": "Our results show that using multimodal representations learned from our Seq2Seq modality translation method outperforms the baselines and achieves improved performance on multimodal sentiment analysis.", "labels": [], "entities": [{"text": "Seq2Seq modality translation", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.6702316800753275}, {"text": "multimodal sentiment analysis", "start_pos": 171, "end_pos": 200, "type": "TASK", "confidence": 0.7176061073939005}]}], "datasetContent": [{"text": "We explored the applications of this model to the CMU-MOSI dataset ().", "labels": [], "entities": [{"text": "CMU-MOSI dataset", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.9302719831466675}]}, {"text": "We implemented a baseline LSTM model based off the work done in . Our implementation uses 66.67% of the data for training from which we take a 15.15% held-out set for validation, and the remaining 33.33% is used for testing.", "labels": [], "entities": []}, {"text": "Finally, we evaluated our proposed model against the baseline results generated by the implementation of ( ).", "labels": [], "entities": []}, {"text": "Here we compared our results against the various multimodal configurations evaluating our performance using precision, recall, and F1 scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.999715268611908}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9982543587684631}, {"text": "F1", "start_pos": 131, "end_pos": 133, "type": "METRIC", "confidence": 0.9998493194580078}]}, {"text": "The dataset that we use to explore applications of our model is the CMU Multimodal Opinion-level Sentiment Intensity dataset (CMU-MOSI).", "labels": [], "entities": [{"text": "CMU Multimodal Opinion-level Sentiment Intensity dataset (CMU-MOSI)", "start_pos": 68, "end_pos": 135, "type": "DATASET", "confidence": 0.7335757083363004}]}, {"text": "The dataset contains video, audio, and transcriptions of 89 different speakers in 93 different videos divided into 2199 separate opinion sentiments.", "labels": [], "entities": []}, {"text": "Each video has an associated sentiment label in the range from -3 to 3.", "labels": [], "entities": []}, {"text": "The low end of the spectrum (-3) indicates strongly negative sentiment, whereas the high end of the spectrum indicates strongly positive sentiment (+3), and ratings of 0 indicate neutral sentiment.", "labels": [], "entities": []}, {"text": "The CMU-MOSI dataset is currently subject to much research ( and the current state of the art is achieved by ) with an F1 score of 80.3 using a context aware model across entire videos.", "labels": [], "entities": [{"text": "CMU-MOSI dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9526392221450806}, {"text": "F1 score", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9801017045974731}]}, {"text": "The state of the art using only individual segments is achieved by) with an F1 score of 77.3.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9818859994411469}]}, {"text": "With respect to raw features that are being given as inputs to our model, we perform feature extraction in the same manner as described in ).", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.724187821149826}]}, {"text": "In the text domain, pretrained 300 dimensional GLoVe embeddings () were used to represent the textual tokens.", "labels": [], "entities": []}, {"text": "In the audio domain, low level acoustic features including 12 Mel-frequency cepstral coefficients (MFCCs), pitch tracking and voiced/unvoiced segmenting features (Drugman and Alwan, 2011), glottal source parameters; Alku, 1992;), peak slope parameters and maxima dispersion quotients were extracted automatically using COVAREP).", "labels": [], "entities": []}, {"text": "Finally, in the video domain, Facet (iMotions, 2017) is used to extract per-frame basic and advanced emotions and facial action units as indicators of facial muscle movement.", "labels": [], "entities": [{"text": "Facet (iMotions, 2017)", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.6374369512001673}]}, {"text": "In situations where the same time alignment between different modalities are required, we choose the granularity of the input to beat the level of words.", "labels": [], "entities": []}, {"text": "The words are aligned with audio using P2FA to get their exact utterance times.", "labels": [], "entities": []}, {"text": "The visual and acoustic modalities are aligned to words using these utterance times.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Unimodal baseline results with 3 metrics: Precision, Recall and F-Score (F1)", "labels": [], "entities": [{"text": "Precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.999211311340332}, {"text": "Recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9965630173683167}, {"text": "F-Score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9982307553291321}, {"text": "F1", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9879263639450073}]}, {"text": " Table 3: Trimodal results with 3 metrics: Precision, Recall and F-Score (F1)", "labels": [], "entities": [{"text": "Precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9992572665214539}, {"text": "Recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.996653139591217}, {"text": "F-Score", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9984086155891418}, {"text": "F1", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9914856553077698}]}]}