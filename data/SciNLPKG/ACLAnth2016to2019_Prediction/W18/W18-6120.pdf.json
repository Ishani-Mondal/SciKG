{"title": [], "abstractContent": [{"text": "We propose anew word embedding method called word-like character n-gram embedding , which learns distributed representations of words by embedding word-like character n-grams.", "labels": [], "entities": []}, {"text": "Our method is an extension of recently proposed segmentation-free word embedding, which directly embeds frequent character n-grams from a raw corpus.", "labels": [], "entities": []}, {"text": "However, its n-gram vocabulary tends to contain too many non-word n-grams.", "labels": [], "entities": []}, {"text": "We solved this problem by introducing an idea of expected word frequency.", "labels": [], "entities": []}, {"text": "Compared to the previously proposed methods , our method can embed more words, along with the words that are not included in a given basic word dictionary.", "labels": [], "entities": []}, {"text": "Since our method does not rely on word segmentation with rich word dictionaries, it is especially effective when the text in the corpus is in unsegmented language and contains many neologisms and informal words (e.g., Chinese SNS dataset).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7179478704929352}, {"text": "Chinese SNS dataset", "start_pos": 218, "end_pos": 237, "type": "DATASET", "confidence": 0.6749400198459625}]}, {"text": "Our experimental results on Sina Weibo (a Chinese microblog service) and Twitter show that the proposed method can embed more words and improve the performance of downstream tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most existing word embedding methods require word segmentation as a preprocessing step (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.716397300362587}]}, {"text": "The raw corpus is first converted into a sequence of words, and word co-occurrence in the segmented corpus is used to compute word vectors.", "labels": [], "entities": []}, {"text": "This conventional method is referred to as Segmented character N -gram Embedding (SNE) for making a distinction clear in the argument below.", "labels": [], "entities": []}, {"text": "Word segmentation is almost obvious for segmented languages (e.g., English), whose words are delimited by spaces.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6922953724861145}]}, {"text": "On the other hand, when dealing with unsegmented languages (e.g., Chinese and Japanese), whose word boundaries are not obviously indicated, word segmenta- tion tools are used to determine word boundaries in the raw corpus.", "labels": [], "entities": []}, {"text": "However, these segmenters require rich dictionaries for accurate segmentation, which are expensive to prepare and not always available.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.8278867602348328}]}, {"text": "Furthermore, when we deal with noisy texts (e.g., SNS data), which contain a lot of neologisms and informal words, using a word segmenter with a poor word dictionary results in significant segmentation errors, leading to degradation of the quality of learned word embeddings.", "labels": [], "entities": []}, {"text": "To avoid the difficulty, segmentation-free word embedding has been proposed.", "labels": [], "entities": [{"text": "segmentation-free word embedding", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.8415396014849345}]}, {"text": "It does not require word segmentation as a preprocessing step.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7454013228416443}]}, {"text": "Instead, it examines frequencies of all possible character n-grams in a given corpus to buildup frequent n-gram lattice.", "labels": [], "entities": []}, {"text": "Subsequently, it composes distributed representations of n-grams by feeding their co-occurrence information to existing word embedding models.", "labels": [], "entities": []}, {"text": "In this method, which we refer to as Frequent character N -gram Embedding (FNE), the top-K most frequent character n-grams are selected as n-gram vocabulary for embedding.", "labels": [], "entities": [{"text": "Frequent character N -gram Embedding (FNE)", "start_pos": 37, "end_pos": 79, "type": "METRIC", "confidence": 0.8407633437050713}]}, {"text": "Although FNE does not require any word dictionaries, the n-gram vocabulary tends to include avast amount of nonwords.", "labels": [], "entities": [{"text": "FNE", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.7836922407150269}]}, {"text": "For example, only 1.5% of the n-gram vocabulary is estimated as words at K = 2M in Experiment 1 (See Precision of FNE in  Since the vocabulary size K is limited, we would like to reduce the number of non-words in the vocabulary in order to embed more words.", "labels": [], "entities": [{"text": "FNE", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.566188633441925}]}, {"text": "To this end, we propose another segmentation-free word embedding method, called Word-like character N -gram Embedding (WNE).", "labels": [], "entities": []}, {"text": "While FNE only considers n-gram frequencies for constructing the n-gram vocabulary, WNE considers how likely each n-gram is a \"word\".", "labels": [], "entities": [{"text": "FNE", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.7705824971199036}, {"text": "WNE", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.48912671208381653}]}, {"text": "Specifically, we introduce the idea of expected word frequency (ewf ) in a stochastically segmented corpus (, and the top-K n-grams with the highest ewf are selected as n-gram vocabulary for embedding.", "labels": [], "entities": [{"text": "expected word frequency (ewf )", "start_pos": 39, "end_pos": 69, "type": "METRIC", "confidence": 0.7533122102419535}]}, {"text": "In WNE, ewf estimates the frequency of each n-gram appearing as a word in the corpus, while the raw frequency of the n-gram is used in FNE.", "labels": [], "entities": [{"text": "FNE", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.8162443041801453}]}, {"text": "As seen in and, WNE tends to include more dictionary words than FNE.", "labels": [], "entities": [{"text": "FNE", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.6003094911575317}]}, {"text": "WNE incorporates the advantage of dictionarybased SNE into FNE.", "labels": [], "entities": [{"text": "WNE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8854180574417114}, {"text": "FNE", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.815637469291687}]}, {"text": "In the calculation of ewf, we use a probabilistic predictor of word boundary.", "labels": [], "entities": []}, {"text": "We do not expect the predictor is very accurate-If it is good, SNE is preferred in the first place.", "labels": [], "entities": [{"text": "SNE", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9363909959793091}]}, {"text": "A naive predictor is sufficient forgiving low ewf score to the vast majority of non-words so that words, including neologisms, are easier to enter the vocabulary.", "labels": [], "entities": [{"text": "ewf score", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.8579813539981842}]}, {"text": "Although our idea seems somewhat simple, our experiments show that WNE significantly improves word coverage while achieving better performances on downstream tasks.", "labels": [], "entities": [{"text": "WNE", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.6578702330589294}, {"text": "word coverage", "start_pos": 94, "end_pos": 107, "type": "TASK", "confidence": 0.7706205248832703}]}], "datasetContent": [{"text": "We evaluate the three methods: SNE, FNE and WNE.", "labels": [], "entities": [{"text": "FNE", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9836808443069458}]}, {"text": "We use 100MB of SNS data, Sina Weibo 1 for Chinese and Twitter 2 for Japanese and Korean, as training corpora.", "labels": [], "entities": []}, {"text": "Although Korean has spacing, the word boundaries are not obviously determined by space.", "labels": [], "entities": []}, {"text": "The implementation of the proposed method is available on GitHub 3 .  We examine the number of words and non-words in the n-gram vocabulary.", "labels": [], "entities": []}, {"text": "The n-gram vocabularies of size K are prepared by the three methods.", "labels": [], "entities": []}, {"text": "For evaluating the vocabularies, we prepared three types of dictionaries for each language, namely, basic, rich 5 and noun.", "labels": [], "entities": []}, {"text": "basic is the standard dictionary for the word segmenters, and rich is a larger dictionary including neologisms.", "labels": [], "entities": [{"text": "word segmenters", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.726641982793808}]}, {"text": "noun is a word set consists of all noun words in Wikidata).", "labels": [], "entities": []}, {"text": "Each n-gram in a vocabulary is marked as For Japanese, Chinese, and Korean, respectively, basic dictionaries are IPADIC, jieba/dict.txt.small, mecab-ko-dic, and rich dictionaries are NEologd, jieba/dict.txt.big, NIADic \"word\" if it is included in a specified dictionary.", "labels": [], "entities": []}, {"text": "We then compute Precision as the ratio of marked words in the vocabulary and Recall as the ratio of marked words in the dictionary.", "labels": [], "entities": [{"text": "Precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9974931478500366}, {"text": "Recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.996464729309082}]}, {"text": "Precision-Recall curve is drawn by changing K from 1 to 1 \u00d7 10 7 .  We performed the noun category prediction task with the learned word vectors.", "labels": [], "entities": [{"text": "Precision-Recall", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9869762063026428}, {"text": "noun category prediction", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.8500295082728068}]}, {"text": "Most of the settings are the same as.", "labels": [], "entities": []}, {"text": "Noun words and their categories are extracted from Wikidata with the predetermined category set 6 . The word set is split into train (60%) and test (40%) sets.", "labels": [], "entities": []}, {"text": "The hyperparameters are tuned with 5-folds CV on the train set, and the performance is measured on the test set.", "labels": [], "entities": []}, {"text": "This is repeated 10 times for random splits, and the mean accuracies are reported.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.8919937610626221}]}, {"text": "C-SVM classifiers are trained to predict categories from the word vectors, where unseen words are skipped in training and treated as errors in testing.", "labels": [], "entities": []}, {"text": "We performed a grid search over (C, classifier) \u2208 {0.5, 1, 5, 10, 50} \u00d7 {1-vs.-1, 1-vs.-all} of linear SVM.", "labels": [], "entities": []}, {"text": "The vocabulary size is set to K = 2 \u00d7 10 6 for FNE and WNE, while K = K SNE is fixed at the maximum value, i.e., the number of unique segmented n-grams for SNE.", "labels": [], "entities": [{"text": "FNE", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8539682626724243}, {"text": "WNE", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.650884211063385}]}], "tableCaptions": [{"text": " Table 2: Classification accuracies [%] (Experiment 2)", "labels": [], "entities": [{"text": "Classification accuracies", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.5572091937065125}]}]}