{"title": [{"text": "MeSH-based dataset for measuring the relevance of text retrieval", "labels": [], "entities": [{"text": "MeSH-based dataset", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9321825802326202}, {"text": "text retrieval", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7382581532001495}]}], "abstractContent": [{"text": "Creating simulated search environments has been of a significant interest in information retrieval , in both general and biomedical search domains.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.803080826997757}]}, {"text": "Existing collections include modest number of queries and are constructed by manually evaluating retrieval results.", "labels": [], "entities": []}, {"text": "In this work we propose leveraging MeSH term assignments for creating synthetic test beds.", "labels": [], "entities": [{"text": "MeSH term assignments", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.5283085604508718}]}, {"text": "We select a suitable subset of MeSH terms as queries , and utilize MeSH term assignments as labels for retrieval evaluation.", "labels": [], "entities": []}, {"text": "Using well studied retrieval functions, we show that their performance on the proposed data is consistent with similar findings in previous work.", "labels": [], "entities": []}, {"text": "We further use the proposed retrieval evaluation framework to better understand how to combine heterogeneous sources of textual information .", "labels": [], "entities": []}], "introductionContent": [{"text": "PubMed is a search engine processing on average 3 million queries a day and is recognized as a primary tool for scholars in the biomedical field.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9349629282951355}]}, {"text": "PubMed provides access to a collection of approximately 28 million biomedical abstracts as of 2018, of which about 4.5 million have full text document available in PubMed Central.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9722139239311218}, {"text": "PubMed Central", "start_pos": 164, "end_pos": 178, "type": "DATASET", "confidence": 0.9152860641479492}]}, {"text": "With the growing availability of full-text articles, an essential question to consider is how to leverage full text information to improve PubMed retrieval?", "labels": [], "entities": []}, {"text": "While a number of studies have pointed out the benefits of full text for various text mining tasks, combining these two resources for information retrieval is not a trivial endeavor.", "labels": [], "entities": [{"text": "text mining", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.7254681587219238}, {"text": "information retrieval", "start_pos": 134, "end_pos": 155, "type": "TASK", "confidence": 0.7747774124145508}]}, {"text": "Na\u00efvely merging full text articles with abstract data, naturally increases the recall, but at a cost for precision, generally degrading the overall quality of combined search.", "labels": [], "entities": [{"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9998026490211487}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9989946484565735}]}, {"text": "Research is required to understand how to best combine abstracts and full texts, examine the relative importance of different sections in full text, investigate the performance of different scoring functions, etc.", "labels": [], "entities": []}, {"text": "A major obstacle in such efforts is the lack of large-scale gold standards for retrieval evaluation.", "labels": [], "entities": [{"text": "retrieval evaluation", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.9019895493984222}]}, {"text": "Hence, creating such large-scale retrieval evaluation framework is the goal of this work.", "labels": [], "entities": []}, {"text": "Gold standards are typically assembled by using human judgments, which are time consuming, expensive and not scalable.", "labels": [], "entities": []}, {"text": "Pioneering examples area TREC collection) and a BioASQ collection (.", "labels": [], "entities": [{"text": "TREC collection", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.7371794581413269}, {"text": "BioASQ collection", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.783859133720398}]}, {"text": "Simulating test collections for evaluating retrieval quality offers a viable alternative and has been explored in the literature.", "labels": [], "entities": []}, {"text": "In this work we create an evaluation framework based on MeSH term assignments, and use that framework to test the performance of several classic ranking functions.", "labels": [], "entities": [{"text": "MeSH term assignments", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.6899679899215698}]}, {"text": "We examine the utility of MeSH terms as query surrogates and MeSH term assignments as pseudo-relevance rankings.", "labels": [], "entities": [{"text": "MeSH term assignments", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.6834277311960856}]}, {"text": "We describe how we select a subset of MeSH terms as candidate MeSH queries and discuss the retrieval results using five different retrieval functions available in SOLR.", "labels": [], "entities": []}, {"text": "MeSH queries are representative of real user queries.", "labels": [], "entities": [{"text": "MeSH", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8754308819770813}]}, {"text": "This approach allows us to create a largescale relevance ranking framework that is based on human judgements and is publicly available.", "labels": [], "entities": []}, {"text": "MeSH queries are available for download at: https://ftp.ncbi.nlm.nih.gov/pub/wbur/mesh_que-ries/.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each paper indexed by MEDLINE \u00ae is manually assigned on average thirteen MeSH terms) by an indexer, who has access to both the abstract and full text of articles.", "labels": [], "entities": [{"text": "MEDLINE \u00ae", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.7572439908981323}, {"text": "MeSH", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9329470992088318}]}, {"text": "It is plausible to assume that MeSH terms assigned to a document are highly reflective of its topic, and the document is highly relevant to that MeSH term.", "labels": [], "entities": []}, {"text": "In this work we propose using a subset of MeSH terms as queries and rely on the assumption that documents with the MeSH terms assigned are relevant to the query.", "labels": [], "entities": []}, {"text": "As queries, we aim to select MeSH terms that satisfy certain frequency requirements, and those that are correlated with real user queries.", "labels": [], "entities": []}, {"text": "We will refer to the final set of MeSH terms that we use as queries as MeSH queries.", "labels": [], "entities": []}, {"text": "Using MeSH terms for evaluation of various NLP tasks has been described in the literature.", "labels": [], "entities": []}, {"text": "However, to our knowledge, using MeSH terms as query surrogates and MeSH assignments as relevance rankings has not been yet described.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Retrieval results for multi-word que- ries, based on the top 2K retrieved documents.  Presented are averages over 1,735 multi-word  MeSH queries.", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8654373288154602}]}, {"text": " Table 2. Comparison of PubMed and PMC  scores for multiword queries based on top 2K  retrieved documents. The counts are included  only for the articles for which both PubMed  and PMC versions exist, and one or both are in  the top 2K.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.9269442558288574}]}, {"text": " Table 3. The value of full text PMC articles  in the retrieval performance. In combined  retrieval, we assign each article the maxi- mum of its PubMed and PMC score and  evaluate based on that maximum.", "labels": [], "entities": [{"text": "PMC articles", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.833535224199295}, {"text": "maxi- mum", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.8631519873936971}, {"text": "PubMed and PMC score", "start_pos": 145, "end_pos": 165, "type": "DATASET", "confidence": 0.7521844655275345}]}]}