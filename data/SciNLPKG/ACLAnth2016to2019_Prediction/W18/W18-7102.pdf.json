{"title": [{"text": "Normalization in Context: Inter-Annotator Agreement for Meaning-Based Target Hypothesis Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "We explore the contribution of explicit task contexts in the annotation of word-level and sentence-level normalizations for learner language.", "labels": [], "entities": []}, {"text": "We present the annotation schemes and tools used to annotate both word-and sentence-level target hypotheses given an explicit task context for the Corpus of Reading Exercises in German (Ott et al., 2012) and discuss a range of inter-annotator agreement measures appropriate for evaluating target hypothesis and error annotation.", "labels": [], "entities": []}, {"text": "For learner answers to reading comprehension questions, we find that both the amount of task context and the correctness of the learner answer influence the inter-annotator agreement for word-level normalizations.", "labels": [], "entities": []}, {"text": "For sentence-level normalizations, the teachers' detailed assessments of the learner answer meaning provided in the corpus give indications of the difficulty of the target hypothesis annotation task.", "labels": [], "entities": [{"text": "sentence-level normalizations", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.6570454686880112}]}, {"text": "We provide a thorough evaluation of inter-annotator agreement for multiple aspects of meaning-based target hypothesis annotation in context and explore measures beyond inter-annotator agreement that can potentially be used to evaluate the quality of normalization annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learner language frequently contains noncanonical orthography and morphosyntactic constructions that present difficulties for natural language processing tools developed for standard language.", "labels": [], "entities": []}, {"text": "Since manually annotated learner corpora are often small and the high degree of variation in learner productions leads to data sparsity issues even for larger learner corpora, it is useful to consider methods that normalize non-standard aspects of learner language.", "labels": [], "entities": []}, {"text": "While normalization and applying standard language categories to learner language does not address the full spectrum of learner language analysis and fundamental concerns about analyzing learner language (cf., it can facilitate access to learner language in applications such as corpus search tools and computer-aided language learning systems.", "labels": [], "entities": [{"text": "learner language analysis", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.6935983498891195}]}, {"text": "Normalizations such as the minimal target hypothesis from the Falko German learner corpus) have been developed in order to provide aversion of a learner production that can be systematically searched and that is more appropriate for further manual or automatic analysis.", "labels": [], "entities": [{"text": "Falko German learner corpus", "start_pos": 62, "end_pos": 89, "type": "DATASET", "confidence": 0.9411630928516388}]}, {"text": "The minimal target hypothesis contains a minimal number of modifications that convert the learner sentence into a locally grammatical sentence.", "labels": [], "entities": []}, {"text": "As it may not be possible to determine exactly what the learner intended to say in an openended task such as an essay task, what constitutes a minimal change is based on grammatical properties, e.g., preserving a verb and modifying its arguments rather modifying the verb itself.", "labels": [], "entities": []}, {"text": "In terms of the difficulties an annotator may face while interpreting a learner utterance, consider the following learner utterance from the Hiroshima English Learners' Corpus (Miura, 1998): (1) I don't know he live were.", "labels": [], "entities": [{"text": "Hiroshima English Learners' Corpus (Miura, 1998)", "start_pos": 141, "end_pos": 189, "type": "DATASET", "confidence": 0.8944325115945604}]}, {"text": "It is possible to speculate about the intended meaning of this utterance, proposing multiple interpretations such as: (2) a.", "labels": [], "entities": []}, {"text": "I don't know if he was alive. b. I don't know where he lives.", "labels": [], "entities": []}, {"text": "Then consider (1) again within the task context: a translation task from Japanese into English of a sentence with the meaning I don't know where he lives.", "labels": [], "entities": []}, {"text": "This task context makes it extremely likely that the intended meaning is that of (2b).", "labels": [], "entities": []}, {"text": "Without annotation guidelines based on detailed grammatical properties such as for Falko, target hypothesis annotation and likewise error annotation for learner language in open-ended tasks has been shown to be difficult to perform reliably (e.g.,.", "labels": [], "entities": [{"text": "Falko", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.9101063013076782}]}, {"text": "As an example, report Cohen's \u03ba of 0.39 for the task of identifying which tokens should be edited in the NUCLE corpus of English student essays.", "labels": [], "entities": [{"text": "Cohen's \u03ba of 0.39", "start_pos": 22, "end_pos": 39, "type": "METRIC", "confidence": 0.7436322152614594}, {"text": "NUCLE corpus of English student essays", "start_pos": 105, "end_pos": 143, "type": "DATASET", "confidence": 0.9623947441577911}]}, {"text": "In contrast to open-ended tasks, a more explicit task context can provide more information about the potential meaning of a learner production, thereby facilitating a more reliable interpretation of the form and meaning and thus more reliable annotation of target hypotheses, which preserve the intended meaning instead of prioritizing particular grammatical features.", "labels": [], "entities": []}, {"text": "For the ComiGS corpus, which contains explicit task contexts in the form of comic strips used in picture description tasks, report \u03ba = 0.86 for the same task of identifying which tokens should be normalized.", "labels": [], "entities": [{"text": "ComiGS corpus", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.8457605242729187}, {"text": "picture description tasks", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.7737406094868978}]}, {"text": "In this paper, we systematically explore the dependence of normalization on task context through manual annotation studies, focusing on L2 learner responses in a reading comprehension task context.", "labels": [], "entities": []}, {"text": "We explore inter-annotator agreement measures for normalization and error annotation, considering the use of related evaluation metrics beyond inter-annotator agreement for the direct evaluation of normalization annotation.", "labels": [], "entities": [{"text": "normalization and error annotation", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.6800506338477135}]}], "datasetContent": [{"text": "To evaluate the role of context in non-word normalization, the correct answers from CREG-5K (binary assessment: appropriate) were annotated.", "labels": [], "entities": [{"text": "non-word normalization", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7351377457380295}, {"text": "CREG-5K", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.8825043439865112}]}, {"text": "There were 1152 potential non-words in 2574 answers to 877 questions about 98 reading texts.", "labels": [], "entities": []}, {"text": "Two annotators annotated the training instances (10%) and met to discuss disagreements and to re-: IAA for Non-Words: Context fine the annotation guidelines, then annotated the remaining instances (90%) independently.", "labels": [], "entities": [{"text": "IAA", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.42667269706726074}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "As discussed in section 2.2, the agreement for the normalizations is presented as percentage agreement on the exact form provided and the agreement for the context category using Krippendorff's \u03b1.", "labels": [], "entities": []}, {"text": "As a result of the fact that the number of categories is not identical across conditions, the \u03b1 values cannot be compared directly, however indicate moderate to substantial agreement on the context tags.", "labels": [], "entities": []}, {"text": "When only the student answer is available, annotators agree 74.8% of the time on the normalization.", "labels": [], "entities": []}, {"text": "This increases to 79.0% if the question is also available and to 83.8% if the question and reading text 2 are provided, showing that the presence of an explicit task context does enable a higher degree of reliability in normalization annotation.", "labels": [], "entities": [{"text": "reliability", "start_pos": 205, "end_pos": 216, "type": "METRIC", "confidence": 0.9723372459411621}]}, {"text": "For the annotations with the full context (60%, all six context tags are included), the confusion matrix for the context tags is shown in.", "labels": [], "entities": []}, {"text": "Some frequent sources of disagreement are rare inflections such as second person plural subjunctive forms (e.g., stehet 'would stand'), where one annotator annotated them as Real Word and the other normalized them to more frequent third person singular indicative forms (steht 'stand') with the category Answer, and instances where there are multiple, acceptable alternatives for prepositions in a particular context and one annotator consistently provided more alternatives, annotating such cases as Hard (vs. Answer for the other annotator).", "labels": [], "entities": []}, {"text": "In the second non-word normalization experiment, the role of appropriateness is considered.", "labels": [], "entities": []}, {"text": "The non-words consist of 529 non-words in 365 answers, presented to the annotator with the As the students answering the reading comprehension questions do not have access to the teacher target answers while responding, the target answers are not presented to the annotations as part of this experiment.: IAA for Non-Words: Appropriateness full reading text context.", "labels": [], "entities": [{"text": "IAA", "start_pos": 305, "end_pos": 308, "type": "METRIC", "confidence": 0.8888593912124634}, {"text": "Appropriateness", "start_pos": 324, "end_pos": 339, "type": "METRIC", "confidence": 0.9588550925254822}]}, {"text": "Since the appropriate answers from CREG-5K were annotated in the previous experiment, the appropriate answers come from CREG-1032 and other CREG subcorpora, while the inappropriate answers come from CREG-1032 and CREG-5K.", "labels": [], "entities": []}, {"text": "The two annotators from the previous experiment completed the annotation independently without any further training.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "When the answer meaning has been assessed as appropriate, annotators agree on a single normalization in 83.3% of instances, nearly 5% higher than when the answer is inappropriate.", "labels": [], "entities": []}, {"text": "Krippendorff's \u03b1, which is now comparable across both conditions since all six categories were used, is 0.678 for appropriate answers and drops to 0.588 for inappropriate answers, showing that annotators are more reliable in terms of the contribution of the task context for appropriate answers.", "labels": [], "entities": []}, {"text": "This maybe due to the fact that incorrect answers may include additional information that is not present in any part of the task context, so it maybe more difficult to choose a context annotation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Meaning Assessments in CREG-5K", "labels": [], "entities": [{"text": "Meaning Assessments", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.864687591791153}, {"text": "CREG-5K", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.8225192427635193}]}, {"text": " Table 2: IAA for Non-Words: Context", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8163682222366333}]}, {"text": " Table 3: Confusion Matrix: Non-Word with Full Con- text", "labels": [], "entities": []}, {"text": " Table 4: IAA for Non-Words: Appropriateness", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7857204675674438}, {"text": "Appropriateness", "start_pos": 29, "end_pos": 44, "type": "METRIC", "confidence": 0.9926227331161499}]}, {"text": " Table 6: IAA for Error Tag Identification", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9869460463523865}, {"text": "Error Tag Identification", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.6806836724281311}]}, {"text": " Table 7: IAA by Detailed Meaning Assessment", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5779258608818054}, {"text": "Detailed Meaning Assessment", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.7956237196922302}]}]}