{"title": [{"text": "The University of Cambridge's Machine Translation Systems for WMT18", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7366487383842468}, {"text": "WMT18", "start_pos": 62, "end_pos": 67, "type": "TASK", "confidence": 0.8452515602111816}]}], "abstractContent": [{"text": "The University of Cambridge submission to the WMT18 news translation task focuses on the combination of diverse models of translation.", "labels": [], "entities": [{"text": "WMT18 news translation task", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.7867916896939278}]}, {"text": "We compare recurrent, convolu-tional, and self-attention-based neural models on German-English, English-German, and Chinese-English.", "labels": [], "entities": []}, {"text": "Our final system combines all neural models together with a phrase-based SMT system in an MBR-based scheme.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.8676762580871582}]}, {"text": "We report small but consistent gains on top of strong Transformer ensembles.", "labels": [], "entities": []}], "introductionContent": [{"text": "Encoder-decoder networks are the current prevailing architecture for neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 69, "end_pos": 101, "type": "TASK", "confidence": 0.8609573642412821}]}, {"text": "Various architectures have been used in the general framework of encoder and decoder networks such as recursive auto-encoders, (attentional) recurrent models (, convolutional models, and, most recently, purely (self-)attention-based models ().", "labels": [], "entities": []}, {"text": "In the spirit of we devoted our WMT18 submission to exploring the three most commonly used architectures: recurrent, convolutional, and self-attentionbased models like the Transformer (.", "labels": [], "entities": [{"text": "WMT18 submission", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.7064658105373383}]}, {"text": "Our experiments suggest that self-attention is the superior architecture on the tested language pairs, but it can still benefit from model combination with the other two.", "labels": [], "entities": []}, {"text": "We show that using large batch sizes is crucial to Transformer training, and that the delayed SGD updates technique ( ) is useful to increase the batch size on limited GPU hardware.", "labels": [], "entities": [{"text": "Transformer training", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.913353681564331}]}, {"text": "Furthermore, we also report gains from MBR-based combination with a phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.848788321018219}]}, {"text": "We found this particularly striking as the SMT baselines are often more than 10 BLEU points below our strongest neural models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9833356142044067}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9987630844116211}]}, {"text": "Our final submission ranks second in terms of BLEU score in the WMT18 evaluation campaign on English-German and German-English, and outperforms all other systems on a variety of linguistic phenomena on German-English ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.98343625664711}, {"text": "WMT18", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.593180239200592}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Training data sizes for English-German  and German-English after filtering.", "labels": [], "entities": []}, {"text": " Table 2: Training data sizes for Chinese-English  after filtering.", "labels": [], "entities": []}, {"text": " Table 3: Number of model parameters.", "labels": [], "entities": []}, {"text": " Table 4: Impact of the effective batch size on  Transformer training on en-de news-test2017 after  3,276M training tokens, beam size 4.", "labels": [], "entities": []}, {"text": " Table 5: Training setups for our neural models on all language pairs.", "labels": [], "entities": []}, {"text": " Table 6: Single architecture results on all language pairs for single systems and 2-ensembles.", "labels": [], "entities": []}, {"text": " Table 7: Model combination with ensembling and MBR.Model scores are weighted with MERT and  combined (log-)linearly as described in Sec. 2.  *  : The LSTM and SliceNet models are 2-ensembles.", "labels": [], "entities": [{"text": "MBR.Model", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9014197587966919}, {"text": "MERT", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9967190623283386}]}, {"text": " Table 8: BLEU scores of the submitted systems  (row 10 in Tab. 7).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982922673225403}]}]}