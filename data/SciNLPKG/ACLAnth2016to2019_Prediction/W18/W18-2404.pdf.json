{"title": [], "abstractContent": [{"text": "The problem of sequence labelling in language understanding would benefit from approaches inspired by semantic priming phenomena.", "labels": [], "entities": [{"text": "sequence labelling", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.6785355359315872}, {"text": "language understanding", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.6989753991365433}]}, {"text": "We propose that an attention-based RNN architecture can be used to simulate semantic priming for sequence labelling.", "labels": [], "entities": []}, {"text": "Specifically, we employ pre-trained word embeddings to characterize the semantic relationship between utterances and labels.", "labels": [], "entities": []}, {"text": "We validate the approach using varying sizes of the ATIS and MEDIA datasets, and show up to 1.4-1.9% improvement in F1 score.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.7152000665664673}, {"text": "MEDIA datasets", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.9008725583553314}, {"text": "F1 score", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9717765748500824}]}, {"text": "The developed framework can enable more explainable and generalizable spoken language understanding systems.", "labels": [], "entities": [{"text": "generalizable spoken language understanding", "start_pos": 56, "end_pos": 99, "type": "TASK", "confidence": 0.6360290721058846}]}], "introductionContent": [{"text": "Priming () is a cognitive mechanism in which a primary stimulus (i.e. the prime) influences the response to a subsequent stimulus (i.e. the target) in an implicit and intuitive manner.", "labels": [], "entities": [{"text": "Priming", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8950625061988831}]}, {"text": "In the case of semantic priming, both the prime and the target typically belong to the same semantic category.", "labels": [], "entities": []}, {"text": "Semantic priming can be explained in terms of induced activation in associative neural networks.", "labels": [], "entities": []}, {"text": "Further, there is empirical evidence to suggest that the processing of words in natural language is influenced by preceding words that are semantically related.", "labels": [], "entities": []}, {"text": "Therefore, semantic priming approaches would enable improvements in sequence labelling.", "labels": [], "entities": []}, {"text": "Previous studies have leveraged contextual information in utterance sequences and dependencies between labels ( to improve performance in sequence labelling tasks.", "labels": [], "entities": []}, {"text": "However, there is limited work to use contextual information in utterances to inform inference of the subsequent labels through semantic priming.", "labels": [], "entities": []}, {"text": "For instance, \"I'd like to book ...\" not only suggests the next word(s), e.g., flight, but also the label of the next word(s), e.g., services.", "labels": [], "entities": []}, {"text": "We posit that systems employing this mode of cross-linked semantic priming could enhance performance in a variety of sequence labelling tasks.", "labels": [], "entities": [{"text": "sequence labelling tasks", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.7031773726145426}]}, {"text": "In this work, we hypothesize that semantic priming inhuman cognition can be simulated by means of an attention mechanism that uses word context to enhance the discriminating power of sequence labelling models.", "labels": [], "entities": []}, {"text": "We propose and explore the use of attention () in a deep learning architecture to simulate the semantic priming mechanism.", "labels": [], "entities": []}, {"text": "We apply this concept to slot filling, an example of sequence labelling in spoken language understanding, which aims to label the utterance sequences with a set of begin/in/out (BIO) tags.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.8171133995056152}, {"text": "sequence labelling in spoken language understanding", "start_pos": 53, "end_pos": 104, "type": "TASK", "confidence": 0.7078424493471781}]}, {"text": "Specifically, we use pre-trained word embeddings to characterise not only the context of words, but also the semantic relationship between words in utterances and words in labels.", "labels": [], "entities": []}, {"text": "Overall, we develop a semantic priming based approach for the task of slot-filling to associate utterances and label sequences.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: (1) We propose an approach that applies semantic priming to sequence labelling.", "labels": [], "entities": []}, {"text": "To capture semantic associations between utterance words and label words, we use three different strategies for deriving label embeddings from pre-trained embeddings.", "labels": [], "entities": []}, {"text": "(2) We implemented the approach in an LSTM-based architecture and validate the efficacy of the approach.", "labels": [], "entities": []}, {"text": "In Section 2 we review related work.", "labels": [], "entities": []}, {"text": "Section 3 elaborates the proposed approach.", "labels": [], "entities": []}, {"text": "An empirical evaluation is provided in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "To validate the efficacy of the architecture in, an empirical evaluation was performed and implemented in Keras . This section elaborates the experimental setup and presents our results.", "labels": [], "entities": [{"text": "Keras", "start_pos": 106, "end_pos": 111, "type": "DATASET", "confidence": 0.9216345548629761}]}, {"text": "Two datasets on spoken dialogues were used in the experiments, namely, the Air Travel Information System (ATIS) task ( and MEDIA, French dialogues collected by ELDA ().", "labels": [], "entities": [{"text": "Air Travel Information System (ATIS) task", "start_pos": 75, "end_pos": 116, "type": "DATASET", "confidence": 0.6166086830198765}, {"text": "MEDIA", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.9450116753578186}]}, {"text": "The statistics of the two datasets is given in.", "labels": [], "entities": []}, {"text": "For ME-DIA, using entities significantly impacts the performance.", "labels": [], "entities": []}, {"text": "Thus entities are used together with words in utterances, as implied by the size of vocabulary in.", "labels": [], "entities": []}, {"text": "Since bi-directional LSTM is used in the architecture in, no context word windows were used as additional inputs in the datasets.", "labels": [], "entities": []}, {"text": "The pre-trained 1 https://keras.io/ word embedding sources for the two datasets are GloVe (English) () and fastText (French) (, respectively.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.6873303651809692}]}, {"text": "In particular, we found that there are about 100 words missing in the fastText French word embedding.", "labels": [], "entities": [{"text": "fastText French word embedding", "start_pos": 70, "end_pos": 100, "type": "DATASET", "confidence": 0.9267086088657379}]}, {"text": "Some of the words, however, are due to original tokenization in MEDIA.", "labels": [], "entities": [{"text": "MEDIA", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.7494064569473267}]}], "tableCaptions": [{"text": " Table 1: Statistics of datasets.  \u2020 The vocabulary is  a mix of words and entities.", "labels": [], "entities": []}, {"text": " Table 2: F1 of the two datasets.  \u2020 CRF used.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9982812404632568}, {"text": "CRF", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.8468865156173706}]}, {"text": " Table 3: F1 of the reduced datasets.  \u2020 CRF used.  100% of the vocabulary in datasets are retained.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9982469081878662}, {"text": "CRF", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.8419768810272217}]}]}