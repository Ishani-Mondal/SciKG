{"title": [{"text": "Investigating NP-Chunking with Universal Dependencies for English", "labels": [], "entities": []}], "abstractContent": [{"text": "Chunking is a pre-processing task generally dedicated to improving constituency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.849666565656662}]}, {"text": "In this paper, we want to show that universal dependency (UD) parsing can also leverage the information provided by the task of chunk-ing even though annotated chunks are not provided with universal dependency trees.", "labels": [], "entities": [{"text": "universal dependency (UD) parsing", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.6478401372830073}]}, {"text": "In particular , we introduce the possibility of deducing noun-phrase (NP) chunks from universal dependencies, focusing on English as a first example.", "labels": [], "entities": []}, {"text": "We then demonstrate how the task of NP-chunking can benefit PoS-tagging in a multi-task learning setting-comparing two different strategies-and how it can be used as a feature for dependency parsing in order to learn enriched models.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 180, "end_pos": 198, "type": "TASK", "confidence": 0.7975597679615021}]}], "introductionContent": [{"text": "Syntactic chunking consists of identifying groups of (consecutive) words in a sentence that constitute phrases (e.g. noun-phrases, verb-phrases).", "labels": [], "entities": [{"text": "Syntactic chunking", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.906171590089798}]}, {"text": "It can be seen as a shallow parsing task between PoStagging and syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.6836248338222504}]}, {"text": "Chunking is known as being a relevant preprocessing step for syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8061168789863586}]}, {"text": "Chunking got a lot of attention when syntactic parsing was predominantly driven by constituency parsing and was highlighted, in particular, through the).", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7798825204372406}, {"text": "constituency parsing", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7000347077846527}]}, {"text": "Nowadays, studies) still compare chunking -as well as constituency parsing-performance on these same data from the Penn Treebank.", "labels": [], "entities": [{"text": "chunking", "start_pos": 33, "end_pos": 41, "type": "TASK", "confidence": 0.9751473069190979}, {"text": "constituency parsing-performance", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.8318084180355072}, {"text": "Penn Treebank", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.9937276244163513}]}, {"text": "While dependency parsing is spreading to different languages and domains (, chunking is restricted to old journalistic data.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.8697910308837891}]}, {"text": "Nevertheless, chunking can benefit dependency parsing as well as constituency parsing, but gold annotated chunks are not available for universal dependencies.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8346810042858124}, {"text": "constituency parsing", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.8689521253108978}]}, {"text": "We want to automatically deduce chunks from universal dependencies (UD) ( and investigate its benefit for other tasks such as Part-of-Speech (PoS) tagging and dependency parsing.", "labels": [], "entities": [{"text": "Part-of-Speech (PoS) tagging", "start_pos": 126, "end_pos": 154, "type": "TASK", "confidence": 0.6597313761711121}, {"text": "dependency parsing", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.7773311734199524}]}, {"text": "We focus on English, which has properties that make it a good candidate for chunking (low percentage of non-projective dependencies).", "labels": [], "entities": [{"text": "chunking", "start_pos": 76, "end_pos": 84, "type": "TASK", "confidence": 0.9601346254348755}]}, {"text": "As a first target, we also decide to restrict the task to the most common chunks: noun-phrases (NP).", "labels": [], "entities": []}, {"text": "We choose to see NP-chunking as a sequence labeling task where tags signal the beginning (B-NP), the inside (I-NP) or the outside (O) of chunks.", "labels": [], "entities": []}, {"text": "We thus propose to use multi-task learning for training chunking along with PoS-tagging and feature-tagging to show that the tasks can benefit from each other.", "labels": [], "entities": [{"text": "training chunking", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7064560949802399}]}, {"text": "We experiment with two different multi-task learning strategies (training parameters in parallel or sequentially).", "labels": [], "entities": []}, {"text": "We also intend to make parsing benefit from NP-chunking as a preprocessing task.", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9804614782333374}]}, {"text": "Accordingly, we propose to add NP-chunk tags as features for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.8408550322055817}]}, {"text": "We show how to (i) deduce NPchunks from universal dependencies for English in order to (ii) demonstrate the benefit of performing chunking along with PoS-tagging through multitask learning and (iii) evaluate the impact of using NP-chunks as features for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 254, "end_pos": 272, "type": "TASK", "confidence": 0.799473911523819}]}], "datasetContent": [{"text": "As a baseline for PoS-tagging, feature-tagging and NP-chunking, we first train our sequence tagger for each task separately.", "labels": [], "entities": []}, {"text": "We then train the tagger in a multi-task setting -with PoS-tagging as a main task-alternating the auxiliary tasks and the strategies (shared or stacked multi-task learning).", "labels": [], "entities": []}, {"text": "As a baseline for dependency parsing, we train the parser using only word and character embeddings as input to the bi-LSTM.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.838595062494278}]}, {"text": "We then add the PoS and NP-chunk embeddings, separately and simultaneously, for training enriched models.", "labels": [], "entities": []}, {"text": "As an upper bound, we also propose to run the experiments with \"gold\" NP-chunks, i.e. we feed the parser (for training and testing) with NP-chunks that were automatically deduced from the dependencies.", "labels": [], "entities": []}, {"text": "We evaluate all tasks on the three English treebanks included in the version 2.1 of the Universal Dependencies project ( : EWT (254k tokens), LinES (82k tokens) and Par-TUT (49k tokens).", "labels": [], "entities": [{"text": "EWT", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.5964505672454834}, {"text": "LinES", "start_pos": 142, "end_pos": 147, "type": "DATASET", "confidence": 0.640709400177002}]}, {"text": "In average, 3.8, 3.3 and 6.2 NP-chunks per sentence are deduced respectively for each treebank.", "labels": [], "entities": []}, {"text": "Note that the LinES treebank does not contain features (morpho-syntactic tags), so we exclude feature-tagging from the evaluation for this treebank.", "labels": [], "entities": [{"text": "LinES treebank", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.9656587541103363}]}, {"text": "We use the development data to tune our hyper-parameters and to determine the number of epochs (via early-stopping) for each experiment.", "labels": [], "entities": []}, {"text": "For sequence tagging, we use the RMSProp optimizer with a learning rate at 0.0005.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.825858861207962}]}, {"text": "Hidden layers of dimension 300 is used for ParTUT and 100 for EWT and LinES.", "labels": [], "entities": [{"text": "ParTUT", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.451945424079895}, {"text": "EWT", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8736637830734253}, {"text": "LinES", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.7887904047966003}]}, {"text": "We use a dropout of 0.2 on the hidden layers.", "labels": [], "entities": []}, {"text": "For dependency parsing, the hidden layer of the bi-LSTM has a dimension set at 125 and uses a dropout of 0.33.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8447943329811096}]}, {"text": "The dimension of the word and character embeddings are respectively 200 and 50.", "labels": [], "entities": []}, {"text": "For dependency parsing, embedding dimensions for PoS and NP-chunk tags are set respectively to 6 and 3.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8293956518173218}]}, {"text": "We average the scores on 5 runs for each experiment.", "labels": [], "entities": []}, {"text": "We evaluate accuracy on PoStagging and feature-tagging and F 1 3 on chunking.: Results of PoS-tagging (P), feature-tagging (F) and NP-chunking (C) trained as one task (baseline) or via multi-task learning (Shared vs Stacked strategies).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.999658465385437}]}, {"text": "Bold scores are the highest of each column.", "labels": [], "entities": []}, {"text": "Statistical significance (T-test>0.05) is marked with \u2020 .  For dependency parsing, we calculate the label accuracy (LA), the unlabeled attachment score (UAS) and the labeled attachment score (LAS).", "labels": [], "entities": [{"text": "T-test", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.8984889388084412}, {"text": "dependency parsing", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.8314630091190338}, {"text": "label accuracy (LA)", "start_pos": 100, "end_pos": 119, "type": "METRIC", "confidence": 0.8529042720794677}, {"text": "unlabeled attachment score (UAS)", "start_pos": 125, "end_pos": 157, "type": "METRIC", "confidence": 0.7904505580663681}, {"text": "labeled attachment score (LAS)", "start_pos": 166, "end_pos": 196, "type": "METRIC", "confidence": 0.8475940922896067}]}, {"text": "As for the, only universal dependency labels are taken into account (ignoring language-specific subtypes), i.e. we consider a predicted label correct if the main type of the gold label is the same, e.g. flat:name is correct if the gold label is flat.", "labels": [], "entities": []}, {"text": "We also exclude punctuations from the evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of PoS-tagging (P), feature-tagging (F) and NP-chunking (C) trained as one task (baseline) or  via multi-task learning (Shared vs Stacked strategies). Bold scores are the highest of each column. Statistical  significance (T-test>0.05) is marked with  \u2020 .", "labels": [], "entities": [{"text": "Statistical  significance", "start_pos": 213, "end_pos": 238, "type": "METRIC", "confidence": 0.8928278684616089}, {"text": "T-test>0.05)", "start_pos": 240, "end_pos": 252, "type": "METRIC", "confidence": 0.8731411546468735}]}, {"text": " Table 2: Results of dependency parsing using PoS (P) and/or NP-chunk (C) features.The baseline uses only word  and character embeddings. Highest scores are in bold.  \u2020 indicates statistical significance.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8526690304279327}, {"text": "statistical significance", "start_pos": 179, "end_pos": 203, "type": "METRIC", "confidence": 0.8648938834667206}]}]}