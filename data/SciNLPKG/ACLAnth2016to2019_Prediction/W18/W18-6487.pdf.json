{"title": [{"text": "The RWTH Aachen University Filtering System for the WMT 2018 Parallel Corpus Filtering Task", "labels": [], "entities": [{"text": "RWTH Aachen University Filtering", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.7862748056650162}, {"text": "WMT 2018 Parallel Corpus Filtering", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.6542214810848236}]}], "abstractContent": [{"text": "This paper describes the submission of RWTH Aachen University for the De\u2192En parallel corpus filtering task of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018).", "labels": [], "entities": [{"text": "De\u2192En parallel corpus filtering task of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018)", "start_pos": 70, "end_pos": 175, "type": "TASK", "confidence": 0.7516269832849503}]}, {"text": "We use several rule-based, heuristic methods to preselect sentence pairs.", "labels": [], "entities": []}, {"text": "These sentence pairs are scored with count-based and neural systems as language and translation models.", "labels": [], "entities": []}, {"text": "In addition to single sentence-pair scoring, we further implement a simple redundancy removing heuristic.", "labels": [], "entities": []}, {"text": "Our best performing corpus filtering system relies on recurrent neural language models and translation models based on the transformer architecture.", "labels": [], "entities": [{"text": "corpus filtering", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7227495312690735}]}, {"text": "A model trained on 10M randomly sampled tokens reaches a performance of 9.2% BLEU on newstest2018.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9995583891868591}, {"text": "newstest2018", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.9474013447761536}]}, {"text": "Using our filtering and ranking techniques we achieve 34.8% BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9997569918632507}]}], "introductionContent": [{"text": "In this work we describe the corpus filtering system of the RWTH Aachen University for the WMT 2018 parallel corpus filtering task.", "labels": [], "entities": [{"text": "corpus filtering", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7625864148139954}, {"text": "RWTH Aachen University", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.8572033643722534}, {"text": "WMT 2018 parallel corpus filtering task", "start_pos": 91, "end_pos": 130, "type": "TASK", "confidence": 0.7597943445046743}]}, {"text": "We decided to rank the data using a two-stage process.", "labels": [], "entities": []}, {"text": "During the first stage, we reduce the number of parallel sentences by applying basic rulebased heuristics each of whom can reject a sentence as described in Section 3.", "labels": [], "entities": []}, {"text": "Afterward, we apply a variety of models on the remaining sentences to assign a score to each sentence pair.", "labels": [], "entities": []}, {"text": "The details of those models, namely language models and translation models, can be found in Section 4.", "labels": [], "entities": [{"text": "translation", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.9631696939468384}]}, {"text": "Our final submission consists of three different systems on top of rule-based filtering: Two of them are based on scoring each sentence pair independently using either only count-based models or only neural models.", "labels": [], "entities": []}, {"text": "The third submission extends on the neural network-based submission by removing redundancies before ranking the sentences.", "labels": [], "entities": []}, {"text": "We compare the behavior of neural network based models to count-based models and find that the performance differs by more than 1.0 % BLEU on average across all test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9992619156837463}]}, {"text": "In total our best system reaches a performance of 34.8 % BLEU compared to 9.2 % BLEU using random sampling on newstest2018 of the news translation task with 10M token subsampled training data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9995623230934143}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.998989999294281}, {"text": "news translation task", "start_pos": 130, "end_pos": 151, "type": "TASK", "confidence": 0.7580331166585287}]}, {"text": "We report our findings and results in detail in Section 6.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.9116023182868958}]}], "datasetContent": [{"text": "To check the quality of a filtering approach, we train a transformer model on the top 10M respectively top 100M subwords of the scored training data.", "labels": [], "entities": []}, {"text": "We mainly focus on the 10M-subsampling results, as this scenario shows clearer differences in performance between different methods.", "labels": [], "entities": []}, {"text": "Like in Section 4.4 we use the Sockeye implementation of the transformer architecture but we train smaller models for evaluation purposes.", "labels": [], "entities": [{"text": "Sockeye", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9526244401931763}]}, {"text": "The decision to use small transformer networks was made as they give strong results in a much shorter amount of time (1 day compared to 5 days).", "labels": [], "entities": []}, {"text": "To verify the generality of the approach we cross-checked several experiments using recurrent neural network-based (RNN-based) translation systems from the Marian framework (Junczys-Dowmunt et al., 2018) and found their training behavior to be correlated.", "labels": [], "entities": []}, {"text": "We apply the provided subsampling script on the filtered data to extract training data.", "labels": [], "entities": []}, {"text": "Due to an error in our filtering setup the input data is tokenized and subworded.", "labels": [], "entities": []}, {"text": "Since both procedures increase the number of tokens per sentence, we extract less sentence pairs than intended.", "labels": [], "entities": []}, {"text": "Note that because of these two effects (transformer and lower subsampling rate) BLEU scores reported in this submission can vary in comparison to other submissions, if RNN-based systems are used.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9978571534156799}]}, {"text": "For the transformer model used for evaluation, we use only 3 layers in encoder and decoder and increase the batch size to 8,000 words.", "labels": [], "entities": []}, {"text": "We train for 100k updates evaluating a checkpoint every 10k updates for the 10M word experiments.", "labels": [], "entities": []}, {"text": "In the case of 100M words of training data, 200k update steps are performed with a checkpoint being written every 20k updates.", "labels": [], "entities": []}, {"text": "The best checkpoint is selected by computing BLEU () on newstest2015.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9971586465835571}, {"text": "newstest2015", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9665066599845886}]}, {"text": "The beam-size for translation is 12.", "labels": [], "entities": [{"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9875655770301819}]}, {"text": "We report BLEU scores using mteval from the Moses toolkit ( and TER scores) using TERcom.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981027245521545}, {"text": "Moses toolkit", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9225042164325714}, {"text": "TER scores", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9687124490737915}]}, {"text": "During system building most of our design decisions are based on results for the 10M-wordversion of the task, however, we observe very similar trends for the 100M-word subsampling.", "labels": [], "entities": []}, {"text": "For brevity we report most results only on the smaller subsamples.", "labels": [], "entities": []}, {"text": "In this section we report the results of our filtering experiments.", "labels": [], "entities": []}, {"text": "We use newstest2015 and newstest2017 as development sets and report the results on newstest2018.", "labels": [], "entities": []}, {"text": "For brevity, we shorten the names of newstestX to tstX in the header of several tables.", "labels": [], "entities": []}, {"text": "All BLEU and TER scores reported in this sections are obtained by using the system under consideration as filtering system and training the transformer system described in Section 5 on the resulting training data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9988377690315247}, {"text": "TER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9858027696609497}]}, {"text": "All processing steps and experiments are organized with as workflow manager.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sizes of datasets after applying the heuristic filtering methods. Sizes are given in sentence pairs, tokens  on German side and tokens on English side. Every heuristic is applied on top of the preceding heuristic. The  last two columns show the percentage (with respect to its input not the original corpus) respectively the absolute  number of lines removed by a heuristic.", "labels": [], "entities": []}, {"text": " Table 2: Model evaluation of 10M random sampling  from the datasets created by rule-based heuristic filter- ing.", "labels": [], "entities": []}, {"text": " Table 3: Model evaluation of 100M random sampling  from the datasets created by rule-based heuristic filter- ing.", "labels": [], "entities": []}, {"text": " Table 4: Results for 10M word subsampling when applying different scoring models on already filtered data.  All models are scoring the data that was filtered with methods described in Section 3.1 to 3.6. For model-based  filtering, both source and target sides are scored.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of language model perplexity  with its performance as data cleaning system as well  as the effect of CommonCrawl on LMs.  * For KenLM filtering results we combine the corre- sponding LM scores of source, target and two fixed  IBM1 scores.", "labels": [], "entities": [{"text": "KenLM filtering", "start_pos": 149, "end_pos": 164, "type": "TASK", "confidence": 0.8673617243766785}]}, {"text": " Table 6: Effect of using the Levenshtein distance heuristic (3.6) on count-based and neural scoring.", "labels": [], "entities": []}, {"text": " Table 7: Official submission result for each evaluation method. The scores report the average BLEU % across all  6 test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9996788501739502}]}]}