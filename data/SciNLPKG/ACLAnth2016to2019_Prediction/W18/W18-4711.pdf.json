{"title": [{"text": "Towards Understanding End-of-trip Instructions in a Taxi Ride Scenario", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce a dataset containing human-authored descriptions of target locations in an \"end-of-trip in a taxi ride\" scenario.", "labels": [], "entities": []}, {"text": "We describe our data collection method and a novel annotation scheme that supports understanding of such descriptions of target locations.", "labels": [], "entities": []}, {"text": "Our dataset contains target location descriptions for both synthetic and real-world images as well as visual annotations (ground truth labels, dimensions of vehicles and objects, coordinates of the target location, distance and direction of the target location from vehicles and objects) that can be used in various visual and language tasks.", "labels": [], "entities": []}, {"text": "We also perform a pilot experiment on how the corpus could be applied to visual reference resolution in this domain.", "labels": [], "entities": [{"text": "visual reference resolution", "start_pos": 73, "end_pos": 100, "type": "TASK", "confidence": 0.6868725220362345}]}], "introductionContent": [{"text": "The last few utterances in atypical taxi ride are the passengers directing the driver to stop their ride at the desired target location.", "labels": [], "entities": []}, {"text": "\"Stop right next to the white car\", \"behind the big tree should work\", \"drop me off in front of the second black pickup truck\" are all examples of such utterances.", "labels": [], "entities": []}, {"text": "Resolving these requests, while a simple task for the human drivers, assumes complex vision and language understanding capabilities.", "labels": [], "entities": []}, {"text": "The purpose of this work is to build a dataset that comprises of such utterances and build an annotation scheme supporting the understanding of such utterances.", "labels": [], "entities": []}, {"text": "We introduce a novel dataset which contains the human-authored natural language descriptions of the desired target location in an end-of-trip taxi ride scenario with synthetic images and real street images.", "labels": [], "entities": []}, {"text": "We describe the annotation scheme for these descriptions which comprises of referents, directional descriptions, and actions, and show that the inter-annotator agreement is high.", "labels": [], "entities": []}, {"text": "Our dataset contains the images with the ground-truth target location coordinates that are described by the users.", "labels": [], "entities": []}, {"text": "The image annotations also contain object ground-truth labels, coordinates, dimensions along with the distance and direction of the target location with respect to the objects that are present in the image.", "labels": [], "entities": []}, {"text": "We refer to the position of the target location as a function of 'r' and '\u2713' where 'r' is the magnitude of the vector, and \u2713 is the direction between the referent and the target location.", "labels": [], "entities": []}, {"text": "This quantification provides the capability to predict the target location coordinates using natural language sentences given the visual context.", "labels": [], "entities": []}, {"text": "shows an example where the combination of rand \u2713 determines the target location with respect to the referent(s).", "labels": [], "entities": []}, {"text": "The contributions of this work are: 1) A novel corpus containing user descriptions of target locations for synthetic and real-world street images.", "labels": [], "entities": []}, {"text": "2) The natural language description annotations along with the visual annotations for the task of target location prediction.", "labels": [], "entities": [{"text": "target location prediction", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.6579441428184509}]}, {"text": "3) A baseline model for the task of identification of referents from user descriptions.", "labels": [], "entities": [{"text": "identification of referents from user descriptions", "start_pos": 36, "end_pos": 86, "type": "TASK", "confidence": 0.8068953255812327}]}, {"text": "r: Example from the synthetic section of the dataset.", "labels": [], "entities": []}, {"text": "The annotation labels of rand \u2713 define the target location for an example utterance \"drop me off in front of the cop car\".", "labels": [], "entities": []}, {"text": "The green arrow shows the direction of motion of the taxi.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of the dataset collected across the synthetic and the real-world images. The \"com- bined\" section contains the total unique tokens and common tokens shared across the descriptions in the  synthetic and real-world. We observe that the difference in the language is mainly related to the referent  descriptions.", "labels": [], "entities": []}]}