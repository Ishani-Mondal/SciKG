{"title": [{"text": "Joint Part-of-Speech and Language ID Tagging for Code-Switched Data", "labels": [], "entities": [{"text": "Part-of-Speech and Language ID Tagging", "start_pos": 6, "end_pos": 44, "type": "TASK", "confidence": 0.6523692607879639}]}], "abstractContent": [{"text": "Code-switching is the fluent alternation between two or more languages in conversation between bilinguals.", "labels": [], "entities": []}, {"text": "Large populations of speakers code-switch during communication, but little effort has been made to develop tools for code-switching, including part-of-speech taggers.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.7299310863018036}]}, {"text": "In this paper, we propose an approach to POS tagging of code-switched English-Spanish data based on recurrent neural networks.", "labels": [], "entities": [{"text": "POS tagging of code-switched English-Spanish", "start_pos": 41, "end_pos": 85, "type": "TASK", "confidence": 0.8880454659461975}]}, {"text": "We test our model on known monolin-gual benchmarks to demonstrate that our neural POS tagging model is on par with state-of-the-art methods.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.734562337398529}]}, {"text": "We next test our code-switched methods on the Miami Bangor corpus of English-Spanish conversation , focusing on two types of experiments: POS tagging alone, for which we achieve 96.34% accuracy, and joint part-of-speech and language ID tagging, which achieves similar POS tagging accuracy (96.39%) and very high language ID accuracy (98.78%).", "labels": [], "entities": [{"text": "Miami Bangor corpus of English-Spanish conversation", "start_pos": 46, "end_pos": 97, "type": "DATASET", "confidence": 0.9583573540051779}, {"text": "POS tagging", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.6929617375135422}, {"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9875439405441284}, {"text": "language ID tagging", "start_pos": 224, "end_pos": 243, "type": "TASK", "confidence": 0.7104624609152476}, {"text": "POS tagging", "start_pos": 268, "end_pos": 279, "type": "TASK", "confidence": 0.632655918598175}, {"text": "accuracy", "start_pos": 280, "end_pos": 288, "type": "METRIC", "confidence": 0.7729997634887695}, {"text": "accuracy", "start_pos": 324, "end_pos": 332, "type": "METRIC", "confidence": 0.8953916430473328}]}, {"text": "Finally, we show that our proposed models outperform other state-of-the-art code-switched taggers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Code-switching (CS) is the phenomenon by which multilingual speakers switch between languages in written or spoken communication.", "labels": [], "entities": [{"text": "Code-switching (CS)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6331740543246269}]}, {"text": "For example, a English-Spanish speaker might say \"El teacher me dijo que Juanito is very good at math.\"", "labels": [], "entities": []}, {"text": "CS can be observed in various linguistic levels: phonological, morphological, lexical, and syntactic and can be classified as intra-sentential (if the switch occurs within the boundaries of a sentence or utterance), or inter-sentential (if the switch occurs between two sentences or utterances).", "labels": [], "entities": []}, {"text": "The importance of developing NLP technologies for CS data is immense.", "labels": [], "entities": []}, {"text": "In the US alone there is an estimated population of 56.6 million Hispanic people (US Census), of which 40 million are native speakers (US Census).", "labels": [], "entities": []}, {"text": "Most of these speakers routinely code-switch.", "labels": [], "entities": []}, {"text": "However, very little research has been done to develop NLP approaches to CS language, due largely to the lack of sufficient corpora of high-quality annotated data to train on.", "labels": [], "entities": []}, {"text": "Yet CS presents serious challenges to all language technologies, including part-of-speech (POS) tagging, parsing, language modeling, machine translation, and automatic speech recognition, since techniques developed on one language quickly breakdown when that language is mixed with another.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 75, "end_pos": 103, "type": "TASK", "confidence": 0.6870156049728393}, {"text": "parsing", "start_pos": 105, "end_pos": 112, "type": "TASK", "confidence": 0.954204261302948}, {"text": "language modeling", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.6988479346036911}, {"text": "machine translation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7610200643539429}, {"text": "automatic speech recognition", "start_pos": 158, "end_pos": 186, "type": "TASK", "confidence": 0.697349468866984}]}, {"text": "One of Artificial Intelligence's ultimate goals is to enable seamless natural language interactions between artificial agents and human users.", "labels": [], "entities": []}, {"text": "In order to achieve that goal, it is imperative that users be able to communicate with artificial agents as they do with other humans.", "labels": [], "entities": []}, {"text": "In addition to such real time interactions, CS language is also pervasive in social media).", "labels": [], "entities": []}, {"text": "So, any system which attempts to communicate with these users or to mine their social media content needs to deal with CS language.", "labels": [], "entities": []}, {"text": "POS tagging is a key component of any Natural Language Understanding system and one of the first researchers employ to process data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7234166860580444}]}, {"text": "As such, it is crucial that POS taggers be able to process CS content.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.76872918009758}]}, {"text": "Monolingual POS taggers stumble when processing CS sentences due to out-of-vocabulary words in one language, confusable words that exist in both language lexicons, and differences in the syntax of the two languages.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.8035092949867249}]}, {"text": "For example, when running monolingual English and Spanish taggers on the CS English-Spanish shown in, the English tagger erroneously tagged most Spanish: Example of an English-Spanish code-switched sentence.", "labels": [], "entities": []}, {"text": "The figure shows the original codeswitched sentence, English translations of each token, gold POS tags and the tagging output of an English tagger, a Spanish tagger, a tagger trained on English and Spanish sentences, and a tagger trained on a corpus of code-switched sentences, in that order.", "labels": [], "entities": []}, {"text": "Errors made by each tagger are underlined.", "labels": [], "entities": [{"text": "Errors", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9723353981971741}]}, {"text": "tokens, and similarly the Spanish tagger mistagged most English tokens.", "labels": [], "entities": []}, {"text": "A tagger trained on monolingual English and Spanish sentences (EN+ES tagger) fared better, making only two mistakes: on the word \"when\", where the switch occurs (confusing the subordinating conjunction for an adverb), and the word \"in\" (which exists in both vocabularies).", "labels": [], "entities": []}, {"text": "A tagger trained on CS instances of English-Spanish, however, was able to tag the whole sentence correctly.", "labels": [], "entities": []}], "datasetContent": [{"text": "Throughout our experiments we use three corpora for different purposes.", "labels": [], "entities": []}, {"text": "The Wall Street Journal (WSJ) corpus is used to demonstrate that our proposed Bi-LSTM POS tagger is on par with current state-of-the-art English POS taggers.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.9523757100105286}]}, {"text": "The Universal Dependencies (UD) corpus is used to train baseline monolingual POS taggers in English and Spanish that we can use to test on our CS data since both employ the Universal POS tagset ().", "labels": [], "entities": [{"text": "Universal Dependencies (UD) corpus", "start_pos": 4, "end_pos": 38, "type": "DATASET", "confidence": 0.595300480723381}]}, {"text": "The Miami Bangor corpus, which contains instances of inter-and intrasentential CS utterances in English and Spanish, is used for training and testing CS models and comparing these to monolingual models.", "labels": [], "entities": [{"text": "Miami Bangor corpus", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9638366103172302}]}, {"text": "shows the number of sentences/utterances and tokens in each dataset split.", "labels": [], "entities": []}, {"text": "For the MB corpus, Inter-CS refers to the subset of monolingual sentences and Intra-CS refers to the subset of CS sentences.", "labels": [], "entities": [{"text": "MB corpus", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.8770613074302673}, {"text": "Inter-CS", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8934962153434753}]}, {"text": "In this section, we present our experiments using the three Bi-LSTM models introduced in Section 3 and the datasets from Section 4.", "labels": [], "entities": []}, {"text": "Our goal is a) to show that the basic Bi-LSTM POS tagger performs very well against known POS tagging benchmarks; b) to obtain baseline performances for monolingual taggers when tested on CS data; and c) to train and test the proposed models on CS data and analyze their performance when trained on different proportions of monolingual and CS data.", "labels": [], "entities": [{"text": "Bi-LSTM POS tagger", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.48762012521425885}]}], "tableCaptions": [{"text": " Table 1: Datasets used for our experiments.", "labels": [], "entities": []}, {"text": " Table 2: Language composition (%) of the MB  corpus.", "labels": [], "entities": [{"text": "MB  corpus", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.96028071641922}]}, {"text": " Table 3: CS in the Miami Bangor Corpus. The top  subtable shows the number of switches, the aver- age number of switches per utterances, the amount  of switched words (word after which a switch oc- curs), and the amount of switched utterances in  each partition. The bottom subtable shows the per- centage of utterances that contain n switches.", "labels": [], "entities": [{"text": "Miami Bangor Corpus", "start_pos": 20, "end_pos": 39, "type": "DATASET", "confidence": 0.9405372341473898}]}, {"text": " Table 4: Bi-LSTM POS tagging accuracy (%) on  the Universal Dependency corpora. The left sub- table shows the accuracy on the UD dev and test  sets. The right subtables shows the accuracy on  the MB test set and on the subset of CS utterances.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.4443219006061554}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.917101263999939}, {"text": "Universal Dependency corpora", "start_pos": 51, "end_pos": 79, "type": "DATASET", "confidence": 0.7164024909337362}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9981245398521423}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9974048733711243}, {"text": "MB test set", "start_pos": 197, "end_pos": 208, "type": "DATASET", "confidence": 0.8267171184221903}]}, {"text": " Table 5: POS tagging accuracy (%) on the MB  corpus. Underlined font indicates best result in  test set by each training setting across different  tagging models. Bold results indicate best overall  result in that test set.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6688363254070282}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9427152872085571}, {"text": "MB  corpus", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.9405612945556641}]}, {"text": " Table 6: LID tagging accuracy by the Bi-LSTM  joint POS+LID Tagger on the MB corpus.", "labels": [], "entities": [{"text": "LID tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.5499978363513947}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9725396633148193}, {"text": "MB corpus", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.9616906046867371}]}, {"text": " Table 7: Out-of-vocabulary (OOV) rate, sen- tence (Sacc) and word accuracy (Wacc) at the  sentence level, fragment (CSFAcc) and word ac- curacy (CSFWacc) at the fragment level, aver- age minimum distance from tagging error to CSF  (AvgMinDistCSF), and percentage of errors that  occur within a CSF (%ErrorsInCSF).", "labels": [], "entities": [{"text": "Out-of-vocabulary (OOV) rate", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.7104547083377838}, {"text": "accuracy (Wacc)", "start_pos": 67, "end_pos": 82, "type": "METRIC", "confidence": 0.8976386487483978}, {"text": "word ac- curacy (CSFWacc)", "start_pos": 129, "end_pos": 154, "type": "METRIC", "confidence": 0.8442003386361259}, {"text": "aver- age minimum distance", "start_pos": 178, "end_pos": 204, "type": "METRIC", "confidence": 0.9497419595718384}]}]}