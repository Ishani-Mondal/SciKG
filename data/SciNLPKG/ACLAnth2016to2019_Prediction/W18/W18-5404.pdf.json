{"title": [{"text": "Nightmare attest time: How punctuation prevents parsers from generalizing", "labels": [], "entities": [{"text": "Nightmare attest time", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.7684773604075114}, {"text": "generalizing", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.8263821601867676}]}], "abstractContent": [{"text": "Punctuation is a strong indicator of syntactic structure, and parsers trained on text with punctuation often rely heavily on this signal.", "labels": [], "entities": [{"text": "syntactic structure", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7483232319355011}]}, {"text": "Punctuation is a diversion, however, since human language processing does not rely on punctuation to the same extent, and in informal texts, we therefore often leave out punctuation.", "labels": [], "entities": []}, {"text": "We also use punctuation ungrammati-cally for emphatic or creative purposes, or simply by mistake.", "labels": [], "entities": []}, {"text": "We show that (a) dependency parsers are sensitive to both absence of punctuation and to alternative uses; (b) neural parsers tend to be more sensitive than vintage parsers; (c) training neural parsers without punctuation outperforms all out-of-the-box parsers across all scenarios where punctuation departs from standard punctuation.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7069772183895111}]}, {"text": "Our main experiments are on synthetically corrupted data to study the effect of punctuation in isolation and avoid potential confounds, but we also show effects on out-of-domain data.", "labels": [], "entities": []}], "introductionContent": [{"text": "We study the sensitivity of modern dependency parsers to punctuation.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7331713438034058}]}, {"text": "While punctuation was originally motivated by reading aloud, serving the purpose of \"breath marks\" (, many modern-day punctuation systems are designed to facilitate grammatical disambiguation.", "labels": [], "entities": []}, {"text": "This paper aims to show that for this reason, punctuation can significantly hurt the generalization ability of state-of-the-art syntactic parsers.", "labels": [], "entities": []}, {"text": "In other words, syntactic parsers become too reliant on punctuation and therefore suffer from the absence or creative uses of punctuation.", "labels": [], "entities": []}, {"text": "Such uses are abundant; see for examples from Twitter.", "labels": [], "entities": []}, {"text": "Such situations, where highly predictive features are absent or distorted attest time, were referred to in as nightmare attest time.", "labels": [], "entities": []}, {"text": "Human reading is very robust to variation in punctuation (Baldwin and Coady,; so creative use of punctuation does not hurt human reading performance.", "labels": [], "entities": []}, {"text": "In effect, sensitivity to punctuation is a major obstacle that prevents our syntactic parser from achieving human-level robustness.", "labels": [], "entities": []}, {"text": "The generalization ability of a dependency parser is usually measured by evaluating its accuracy on held-out data, our yardstick to prevent over-fitting, i.e. we define the degree to which a parser has over-fitted to the training data as the difference between performance on training data and performance on the held-out data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9988467693328857}]}, {"text": "This practice is poor when data is not i.i.d., since the heldout data cannot be assumed to be representative; in such cases, little or no over-fitting does not guarantee our parsers have learned important linguistic generalizations: Rather, the parsers may have over-fitted to superficial cues that are present in both the training and test datasets.", "labels": [], "entities": []}, {"text": "We argue that punctuation signs are superficial cues preventing modern parsers from learning appropriately high-level abstractions from our datasets.", "labels": [], "entities": []}, {"text": "Contributions We evaluate three neural dependency parsers for English, as well as two older alternatives, on a standard benchmark, before and after stripping punctuation, as well as after injecting more punctuation signs in the benchmark.", "labels": [], "entities": []}, {"text": "We show that (a) projective parsers are, unsurprisingly, more sensitive to punctuation injection than non-projective ones, since punctuation injection may introduce crossing edges, and (b) neural parsers are more sensitive than vintage parsers.", "labels": [], "entities": []}, {"text": "The latter is our main contribution, but we also show that training a neural parser without punctuation outperforms all parsers trained in a regular fashion across all punctuation scenarios.", "labels": [], "entities": []}, {"text": "Our experiments are on semi-synthetic data to control for confounds, but we also show the parser trained without punctuation is superior on real data with non-standard punctuation.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes how we remove and inject punctuation (our perturbation maps), and details of the parsers used in our experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Labeled attachment scores with punctuation removed. All parsers suffer from absence of or additional  punctuation. The relative increase in error ( 1-BL  1-SYS \u2212 1; with BL performance on original text; SYS performance  under NO PUNCT and \u03b4 = 0.1, \u03ba = 0.1, resp.) for neural parsers is higher than for non-neural parsers. GWEB and  FOSTER scores are on development sentences (of at least five words) with no punctuation.", "labels": [], "entities": [{"text": "BL", "start_pos": 180, "end_pos": 182, "type": "METRIC", "confidence": 0.9871578812599182}, {"text": "FOSTER", "start_pos": 342, "end_pos": 348, "type": "METRIC", "confidence": 0.9958721995353699}]}]}