{"title": [{"text": "Phrase-based Unsupervised Machine Translation with Compositional Phrase Embeddings", "labels": [], "entities": [{"text": "Phrase-based Unsupervised Machine Translation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.730619266629219}]}], "abstractContent": [{"text": "This paper describes the University of Tartu's submission to the unsupervised machine translation track of WMT18 news translation shared task.", "labels": [], "entities": [{"text": "WMT18 news translation shared task", "start_pos": 107, "end_pos": 141, "type": "TASK", "confidence": 0.7468220353126526}]}, {"text": "We build several baseline translation systems for both directions of the English-Estonian language pair using mono-lingual data only; the systems belong to the phrase-based unsupervised machine translation paradigm where we experimented with phrase lengths of up to 3.", "labels": [], "entities": [{"text": "phrase-based unsupervised machine translation", "start_pos": 160, "end_pos": 205, "type": "TASK", "confidence": 0.5867524594068527}]}, {"text": "As a main contribution , we performed a set of standalone experiments with compositional phrase embed-dings as a substitute for phrases as individual vocabulary entries.", "labels": [], "entities": []}, {"text": "Results show that reasonable n-gram vectors can be obtained by simply summing up individual word vectors which retains or improves the performance of phrase-based unsupervised machine tranlation systems while avoiding limitations of atomic phrase vectors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most successful approaches to machine translation () rely on the availability of parallel corpora.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.826072096824646}]}, {"text": "Supervised neural machine translation (NMT) employs the encoderdecoder architecture, where the encoder reads the source sentence and produces its representation which is then fed to the decoder that tries to generate the target sentence word byword.", "labels": [], "entities": [{"text": "Supervised neural machine translation (NMT)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.88460944380079}]}, {"text": "Crossentropy loss is usually used as a training objective and beam search algorithm is used for inference.", "labels": [], "entities": []}, {"text": "These neural models show state-of-the art performance but rely on vast amounts of parallel data.", "labels": [], "entities": []}, {"text": "On the other hand, there is a Statistical Machine Translation paradigm that is based on phrase tables that are learned from parallel corpus.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.7506452401479086}]}, {"text": "These methods are currently replaced by neural counterparts for high-resource languages, but perform better in low-resource settings (.", "labels": [], "entities": []}, {"text": "For some language pairs the size of parallel corpus ranges from extremely low to almost zero.", "labels": [], "entities": []}, {"text": "These extremely low-resource language pairs were a motivation for the Unsupervised Machine Translation () that aims to translate language without usage of the parallel corpora for training.", "labels": [], "entities": [{"text": "Unsupervised Machine Translation", "start_pos": 70, "end_pos": 102, "type": "TASK", "confidence": 0.6446106533209482}]}, {"text": "The first step of the unsupervised approach to translation is the same for all methods: learning word level embedding spaces for source and target languages and then aligning these spaces.", "labels": [], "entities": [{"text": "translation", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.9666955471038818}]}, {"text": "Next, one of the unsupervised vector space mapping methods) is applied to align spaces together and perform word byword translation.", "labels": [], "entities": [{"text": "vector space mapping", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.6786852876345316}, {"text": "word byword translation", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.6449106236298879}]}, {"text": "This mapping is only possible because of the linear properties of the word embedding method that is used to get word vector representations.", "labels": [], "entities": []}, {"text": "Lastly, to improve system's performance, iterative refining using either neural network models or parts of SMT pipeline is done (.", "labels": [], "entities": [{"text": "SMT pipeline", "start_pos": 107, "end_pos": 119, "type": "TASK", "confidence": 0.8706745505332947}]}, {"text": "In this work we implement a system that is similar to the latter approach.", "labels": [], "entities": []}, {"text": "Statistical Machine Translation systems that are based on the phrase representations require smaller amounts of data to achieve reasonable performance.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7122195561726888}]}, {"text": "Phrase-based Unsupervised Machine Translation is motivated by the assumption that methods working without parallel data can benefit from the usage of phrases as the basic units.", "labels": [], "entities": [{"text": "Phrase-based Unsupervised Machine Translation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.836032971739769}]}, {"text": "In order for phrases to be used for Unsupervised Translation they have to be represented in a suitable way.", "labels": [], "entities": [{"text": "Unsupervised Translation", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6937477290630341}]}, {"text": "Current approach is to learn phrase embeddings as atomic vocabulary units (.", "labels": [], "entities": []}, {"text": "In this work, we implement systems for the English-Estonian language pair following the guidlines presented in ( show that simply using sum of individual word embeddings can produce reasonable phrase embeddings.", "labels": [], "entities": []}, {"text": "This eliminates the need of having huge n-gram vocabulary that might be hard to learn and use in unsupervised bilingual mapping.", "labels": [], "entities": []}, {"text": "It also allows to obtain embeddings for arbitrary phrases as opposite to using predefined limited set of phrases.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: In Section 2 we describe our unsupervised translation system baseline.", "labels": [], "entities": []}, {"text": "Section 3 describes our approach to computing phrase embeddings for arbitrary phrases for UMT.", "labels": [], "entities": []}, {"text": "In Section 4 we describe our experiments for compositional phrase embeddings standalone and in context of UMT.", "labels": [], "entities": []}, {"text": "Section 5 concludes the work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Final BLEU scores for the experiments with Unigram, Bigram/Trigram-atomic, and Bigram/Trigram-sum systems are presented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.998820960521698}]}, {"text": "Regarding Ngram-atomic series of experiments, shows that there is an advantage of using bigrams for both language directions.", "labels": [], "entities": []}, {"text": "However, for Estonian-English the advantage is much bigger (+3.5 against +0.59 BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9985201954841614}]}, {"text": "That might be due to the fact the the number of ngrams in English vocabulary is more then 3 times as big as the corresponding number for Estonian vocabulary.", "labels": [], "entities": []}, {"text": "Trigram experiment provides no significant increase or decrease over the bigram experiment.", "labels": [], "entities": []}, {"text": "The benefit of using bigrams and the trigram trend are consistent with the findings of Lample et al..", "labels": [], "entities": []}, {"text": "However, while Lample et al. reports the increase of about 1 BLEU point when using bigrams, we observe the increase of 3.5 for et-en.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.998977780342102}]}, {"text": "Note that we also use custom ngram extraction procedure as opposite to taking top N most frequent bigrams (.", "labels": [], "entities": [{"text": "ngram extraction", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.6129163801670074}]}, {"text": "In case of Ngram-sum experiment for et-en, the system outperforms unigram experiment suggesting that the compositional embeddings do have semantic power.", "labels": [], "entities": []}, {"text": "However, it is below the Bigramatomic and Trigram-atomic baselines which is expected since the predictivness of the compositional model is not perfect.", "labels": [], "entities": []}, {"text": "For en-et however, neither Bigram-sum nor Trigram-sum system outperforms atomic baseline suggesting that the topic needs additional dedicated research efforts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy results of explicit evaluation of com- positional models. Top 3 results among columns are in  bold.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9977449178695679}]}, {"text": " Table 2: BLEU scores for our baselines and systems  powered with compositional phrase embeddings for  Estonian-English and English-Estonian language di- rections.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988886713981628}]}]}