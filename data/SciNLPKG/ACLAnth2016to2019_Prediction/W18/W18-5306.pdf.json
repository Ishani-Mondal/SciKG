{"title": [{"text": "AttentionMeSH: Simple, Effective and Interpretable Automatic MeSH Indexer", "labels": [], "entities": []}], "abstractContent": [{"text": "The 3-hydroxyl-3-methylglutaryl Coenzyme A reductase (HMGCR) gene was examined for polymorphisms in Beijingyou chickens.", "labels": [], "entities": [{"text": "Beijingyou", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.9355139136314392}]}, {"text": "A \"T\" base insert was detected at nucleotide 2749 of the 3-UTR region of the HMGCR gene and was used as the basis for distinguishing a B allele, distinct from the A.", "labels": [], "entities": []}, {"text": "Serum and muscle contents of total cholesterol.", "labels": [], "entities": []}, {"text": "LDL-cholesterol in serum was significantly lower in AB birds and lowest in BB birds.", "labels": [], "entities": [{"text": "LDL-cholesterol", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9907805323600769}]}, {"text": "Realtime PCR showed that the same trends across genotypes occurred in an abundance of HMGCR transcripts in liver, but there was no difference in contents of HMGCR mRNA in breast or thigh muscles.", "labels": [], "entities": []}, {"text": "Hepatic expression and serum LDL-cholesterol were meaningfully correlated (partial, with total serum cholesterol held constant, r = 0.923).", "labels": [], "entities": [{"text": "Hepatic expression", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9490181803703308}]}, {"text": "In muscle, similar genotypic differences were found for the abundance of the LDL receptor (LDLR) transcript.", "labels": [], "entities": []}, {"text": "Cholesterol content in breast muscle related to LDLR expression (partial correlation with serum LDL-cholesterol held constant, r = 0.719); the equivalent partial correlation in thigh muscle was not significant.", "labels": [], "entities": []}, {"text": "The results indicated that the B allele significantly reduces hepatic abundance of HMGCR transcripts, probably accounting for genotypic differences in serum cholesterol.", "labels": [], "entities": []}, {"text": "In muscle, the cholesterol content appeared to reflect differences in LDLR expression with apparent mechanistic differences between breast and thigh.", "labels": [], "entities": []}], "introductionContent": [{"text": "MEDLINE is a database containing more than 24 million biomedical journal citations by 2018 1 . *These authors contribute equally to the paper.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9216694831848145}]}, {"text": "\u2020This work was done while the author was at CMU.", "labels": [], "entities": []}, {"text": "1 https://www.nlm.nih.gov/pubs/ factsheets/medline.html PubMed provides free access to MEDLINE for worldwide researchers.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.8893258571624756}, {"text": "MEDLINE", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.8921055793762207}]}, {"text": "To facilitate information storage and retrieval, curators at the National Library of Medicine (NLM) assign a set of Medical Subject Headings (MeSH) to each article.", "labels": [], "entities": [{"text": "information storage and retrieval", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.70408134162426}, {"text": "National Library of Medicine (NLM)", "start_pos": 65, "end_pos": 99, "type": "DATASET", "confidence": 0.957365265914372}]}, {"text": "MeSH 2 is a hierarchically-organized terminology developed by NLM for indexing and cataloging biomedical texts like MEDLINE articles.", "labels": [], "entities": [{"text": "indexing and cataloging biomedical texts like MEDLINE articles", "start_pos": 70, "end_pos": 132, "type": "TASK", "confidence": 0.6520579196512699}]}, {"text": "MeSH has about 28 thousand terms by 2018 3 , covering the fields from clinical medicine to information sciences.", "labels": [], "entities": [{"text": "MeSH", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9423342943191528}]}, {"text": "Indexers examine the full article and annotate it with MeSH terms according to rules set by NLM . Its estimated that indexing an article costs $9.4 on average, and there are more than 813,500 citations added to MEDLINE in 2017 . Indexing all citations manually would cost several million dollars in one year.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 211, "end_pos": 218, "type": "DATASET", "confidence": 0.9147371649742126}]}, {"text": "Thus, several automatic annotation models have been developed to improve the time-consuming and financially expensive manual annotation.", "labels": [], "entities": []}, {"text": "We will discuss these models in section 2.1.", "labels": [], "entities": []}, {"text": "Automatical annotating PubMed abstracts with MeSH terms is hard in several aspects: There are 28 thousand possible classes and even more of their combinations.", "labels": [], "entities": []}, {"text": "The frequencies of different MeSH terms also vary a lot: The most frequent MeSH term is 'Humans' and it is annotated to more than 8 million articles in the MEDLINE database; while the 20,000th frequent MeSH 'Hypnosis, Anesthetic' is indexed to only about 200 articles (.", "labels": [], "entities": [{"text": "MEDLINE database", "start_pos": 156, "end_pos": 172, "type": "DATASET", "confidence": 0.9734470844268799}]}, {"text": "It causes severe class imbalance problems.", "labels": [], "entities": []}, {"text": "Above difficulties are further complicated by the fact that indexers at the NLM usually inspect the whole articles to do the annotation, but the challenge only provides PubMed abstracts and titles, which might not be enough to find all MeSH terms.", "labels": [], "entities": []}, {"text": "We will discuss it more detailedly in section 5.", "labels": [], "entities": []}, {"text": "Deep learning is a subtype of machine learning that arranges the computational models in multiple processing layers to learn the representations of data with multiple levels of abstractions as well as the mapping from these features to the output (.", "labels": [], "entities": []}, {"text": "Attention is a strategy for deep learning models to learn both the mapping from input to output and the relevance between input parts and output parts ( ).", "labels": [], "entities": []}, {"text": "The learnt relevance helps improve the mapping performance as well as provide interpretability.", "labels": [], "entities": []}, {"text": "We will discuss relevant works of deep learning in automatic annotations in section 2.2.", "labels": [], "entities": []}, {"text": "Here we propose a novel model, AttentionMeSH, which utilizes deep learning and attention mechanism to index MeSH terms to biomedical texts and provides interpretation at the word level.", "labels": [], "entities": []}, {"text": "Each abstract, together with title and journal name, is tokenized to words, then the model feeds word vectors to a bidirectional gated recurrent unit (Bi-GRU) to derive word representations with contextual information).", "labels": [], "entities": []}, {"text": "We narrow down the MeSH term vocabulary for each abstract using a masking mechanism.", "labels": [], "entities": []}, {"text": "Then for each candidate MeSH term, the model calculates the attention attribution over words.", "labels": [], "entities": []}, {"text": "Next, each MeSH term gets a specific document representations by MeSH-specific attention-weighted sum of the word vectors.", "labels": [], "entities": []}, {"text": "Finally, the model uses nonlinear layers to classify each MeSH term using the learnt MeSH-specific document representation.", "labels": [], "entities": []}, {"text": "We participated in BioASQ Challenge Task6A while developing the model.", "labels": [], "entities": [{"text": "BioASQ Challenge Task6A", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.5346021354198456}]}, {"text": "We achieve close to state-of-the-art performance with an onconstruction model in the final week of the contest and with our final model after the contest.", "labels": [], "entities": []}, {"text": "The model also achieves high level of interpretability evaluated by human experts.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.9439472556114197}]}, {"text": "The main contributions of this work are summarized as follows: 1.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, AttentionMeSH is the first end-to-end deep learning model with soft-attention mechanism to index MeSH terms in such a large scale (millions of training data).", "labels": [], "entities": []}, {"text": "With this relatively simple model, we achieved close to stateof-the-art performance without any sophisticated feature engineering or preprocessing.", "labels": [], "entities": []}, {"text": "2. We develop a novel masking mechanism, which is aimed to handle multi-class classification problems with a large number of classes, like indexing MeSH.", "labels": [], "entities": [{"text": "indexing MeSH", "start_pos": 139, "end_pos": 152, "type": "TASK", "confidence": 0.7049494087696075}]}, {"text": "We also conduct extensive experiments on how the masking layer settings influence classification performance.", "labels": [], "entities": []}, {"text": "3. We believe AttentionMeSH is the first MeSH annotation model that is capable of providing textual evidence and interpretations of its predictions.", "labels": [], "entities": []}, {"text": "We argue that interpretability matters because humans are needed to complete the annotation task..", "labels": [], "entities": [{"text": "interpretability", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.96366286277771}]}, {"text": "BioASQ is an European Union-funded project that organizes tasks on biomedical semantic indexing and question answering (.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6229437589645386}, {"text": "biomedical semantic indexing", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.686466783285141}, {"text": "question answering", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.8602648079395294}]}, {"text": "In the task A of BioASQ, participants are asked to annotate un-indexed PubMed articles with MeSH terms using their models, before they are annotated by the human indexers.", "labels": [], "entities": []}, {"text": "The manual annotations are taken as ground truth to evaluate the participating models.", "labels": [], "entities": []}, {"text": "DeepMeSH () is the winner of the latest challenge, BioASQ task 5a, held in 2017.", "labels": [], "entities": [{"text": "DeepMeSH", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9130563735961914}]}, {"text": "DeepMeSH also uses a two-step strategy: the first step is to generate MeSH candidates and predict the number of output MeSH terms, and the second step is to rank the candidates and take the highestranked predicted number of MeSH terms as output.", "labels": [], "entities": []}, {"text": "DeepMeSH uses Term Frequency Inverse Document Frequency (TFIDF) and document to vector (D2V) schemes to represent each abstract and generate MeSH candidates using binary classifiers and k-nearest neighbor (KNN) methods over using these features.", "labels": [], "entities": []}, {"text": "TFIDF is a traditional weighted bag of word sparse representation of the text and D2V learns a deep semantic representation of the text.", "labels": [], "entities": []}, {"text": "Because state-of-the-art models have less than 0.7 Micro-F, automatic MeSH indexing systems can just serve to assist human indexers.", "labels": [], "entities": [{"text": "MeSH indexing", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.7089710235595703}]}, {"text": "Since human indexers usually add or delete MeSH terms based on the recommendations, interpretability of the automatic annotations is very important for them.", "labels": [], "entities": []}, {"text": "In this paper we adopt a local explanation view of model interpretability, and argue that a good system, in addition to being accurate, should also be able to tell which part of the input supports the indexed MeSH term.", "labels": [], "entities": []}, {"text": "This would allow human indexers to be more effective at annotating the article.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the dataset provided by BioASQ , which contains about 13.5 million manually annotated PubMed articles.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.9375051856040955}]}, {"text": "The dataset covers 28,340 MeSH terms in total, and each article is annotated 12.69 MeSH terms on average.", "labels": [], "entities": []}, {"text": "We selected 3 million articles from 2012 to 2017 for training.", "labels": [], "entities": []}, {"text": "The results reported in this paper are derived from two test sets: BioASQ Test Sets: During the challenge, BioASQ provides a test set of several thousands articles each week.", "labels": [], "entities": [{"text": "BioASQ Test Sets", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.8009180823961893}]}, {"text": "Ours: we use 100 thousand latest articles to test our model, and all other results are calculated by this dataset.", "labels": [], "entities": []}, {"text": "Since our test set is very large, the results will be precise.", "labels": [], "entities": []}, {"text": "The major metric for performance evaluation is Micro-F, which is a harmonic mean of microprecision (MiP) and micro-recall (MiR) , and is calculated as follows: In these equations, i is indexed for articles and j is indexed for MeSH terms, so Na is the number of articles in the test set, and N is the number of all MeSH terms.", "labels": [], "entities": []}, {"text": "y ij and\u02c6yand\u02c6 and\u02c6y ij are both binary encoded variables to denote whether MeSH term j is in article i in ground-truth and prediction, respectively.", "labels": [], "entities": []}, {"text": "Selecting relevant MeSH terms from neighbor articles can be regarded as a weak classifier itself, and high-recall setting is favored in this step.", "labels": [], "entities": []}, {"text": "We measure the micro-recall for different masking layer settings, and the results are shown in.", "labels": [], "entities": []}, {"text": "Basically, there are two hyperparameters for it: the number of neighbor articles K and the number of highest ranking MeSH terms selected M . A non-trivial baseline for K is 3M, i.e. the number of all training articles.", "labels": [], "entities": []}, {"text": "Under this circumstance, the ranked MeSH list is determined by global frequency, thus is non-specific to any article.", "labels": [], "entities": [{"text": "MeSH list", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.6879443377256393}]}, {"text": "We choose the number of nearest articles K =  1000 for it gives the highest recalls with the increase of mask size.", "labels": [], "entities": [{"text": "recalls", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.998461127281189}]}, {"text": "In fact, micro-recall at M = 1024 and K = 1000 is about 0.97, which almost guarantees that all true annotations are included as candidate fora document.", "labels": [], "entities": [{"text": "M", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.9254133105278015}]}, {"text": "Before fine-tuning on other hyperparameters and the thresholds of making predictions, we first train the model with different M , and report the results in.", "labels": [], "entities": []}, {"text": "While we were developing the model, we participated in the BioASQ Task6a challenge.", "labels": [], "entities": [{"text": "BioASQ Task6a challenge", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.569163978099823}]}, {"text": "During the challenge, there is a test set available each week.", "labels": [], "entities": []}, {"text": "Each test set contains several thousands of un-indexed PubMed citations.", "labels": [], "entities": []}, {"text": "Each citation has journal name, title, abstract information.", "labels": [], "entities": []}, {"text": "Participants will run their models on the test set and upload their predictions of MeSH annotations within a given time.", "labels": [], "entities": []}, {"text": "The organizers will then evaluate every participants' predictions and make the results available.", "labels": [], "entities": []}, {"text": "The results of the whole Challenges are showed in.", "labels": [], "entities": []}, {"text": "Furthermore, the results of the last week of the Challenge are showed in.", "labels": [], "entities": []}, {"text": "BioASQ are not our final model.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7522052526473999}]}, {"text": "We include the up-to-date results in 'NOW' in and report the ablation test results in.", "labels": [], "entities": [{"text": "NOW", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.7014970183372498}, {"text": "ablation", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9462640285491943}]}, {"text": "At inference time, attention matrix provides wordlevel interpretation: For each MeSH prediction, the model shows which words are given high attention.", "labels": [], "entities": [{"text": "wordlevel interpretation", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.7557225525379181}]}, {"text": "It helps the indexers to evaluate and proofread the indexing results of our model.", "labels": [], "entities": []}, {"text": "shows an example of attention for interpretation.", "labels": [], "entities": []}, {"text": "To qualitatively evaluate the interpretability of different models, the best way would be to measure the time efficiency of manual indexing with the assistance of different models.", "labels": [], "entities": []}, {"text": "However, this might require well-trained NLM indexers to evaluate.", "labels": [], "entities": []}, {"text": "Instead, we asked two independent researchers with Ph.D. degrees in related fields to label relevant words for 100 MeSH-article pairs.", "labels": [], "entities": []}, {"text": "Their intersected labels are regarded as groundtruth.", "labels": [], "entities": []}, {"text": "We model the interpretability evaluation as an information retrieval task, and evaluate each method's recall at different numbers of outputs in.", "labels": [], "entities": [{"text": "interpretability evaluation", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.9202944338321686}, {"text": "information retrieval task", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.7744307716687521}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.996437668800354}]}, {"text": "Since other models like DeepMeSH and MTI don't report how to interpret their model outputs, we use string-matching as a non-trivial baseline.: Interpretability Evaluation.", "labels": [], "entities": []}, {"text": "R@n: The average recall of ground-truth relevant words if the model outputs n words.", "labels": [], "entities": [{"text": "R", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9081704616546631}, {"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9905316829681396}]}, {"text": "AM Whole Model: The whole model of AttentionMeSH is used to get the attention matrix, and n words with highest attention weights will be the output; AM Embeddings: We only use the trained word and MeSH embeddings of AttentionMeSH model, and we output n words that have highest dot products with each specific MeSH.", "labels": [], "entities": [{"text": "AM Whole Model", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8468559781710306}]}, {"text": "String-Matching: A string matching method that takes all words in the abstracts that are same to any word in the MeSH name.", "labels": [], "entities": [{"text": "MeSH name", "start_pos": 113, "end_pos": 122, "type": "DATASET", "confidence": 0.8721858263015747}]}, {"text": "\u2020 : Significant differences with String-Matching; \u2021 : Significant differences with AM Embeddings.", "labels": [], "entities": []}, {"text": "Significance is defined by p < 0.05 in paired t tests.", "labels": [], "entities": [{"text": "Significance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9712550044059753}]}], "tableCaptions": [{"text": " Table 1: Parameter Values. For hyperparameters, we  highlight the optimal ones among all tried values.", "labels": [], "entities": []}, {"text": " Table 2: Model Performance with Different Mask Set- tings. n.n.: MeSH mask selected from nearest neighbor  articles (K = 1000); freq.: MeSH mask selected from  globally frequent MeSH terms; rd.: MeSH mask ran- domly selected. All results are averaged over models  trained by 3 random seeds.", "labels": [], "entities": []}, {"text": " Table 3: Model Performance of the Final BioASQ Test  Set. The models are ranked top-down from the lowest  average MiF to the highest one. Our on-construction  AttentionMeSH ranked second by average MiF.", "labels": [], "entities": [{"text": "Final BioASQ Test  Set", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.6477091535925865}]}, {"text": " Table 4: Model Performance with Ablations and Finer  Tuning. w/o: without; w/: with; t.t.: MeSH term  threshold tuning; w.e.: word embeddings. Ensembling  takes the average prediction of 8 models trained by dif- ferent seeds. All results, except the ensemble one, are  averaged over models trained by 3 random seeds.", "labels": [], "entities": [{"text": "MeSH term  threshold tuning", "start_pos": 92, "end_pos": 119, "type": "METRIC", "confidence": 0.8484113663434982}]}, {"text": " Table 5. Since other models like DeepMeSH and  MTI don't report how to interpret their model out- puts, we use string-matching as a non-trivial base- line.", "labels": [], "entities": []}]}