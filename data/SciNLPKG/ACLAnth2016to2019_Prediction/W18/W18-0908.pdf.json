{"title": [{"text": "An LSTM-CRF Based Approach to Token-Level Metaphor Detection", "labels": [], "entities": [{"text": "Token-Level Metaphor Detection", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.7125927805900574}]}], "abstractContent": [{"text": "Automatic processing of figurative languages is gaining popularity in NLP community for their ubiquitous nature and increasing volume.", "labels": [], "entities": []}, {"text": "In this era of web 2.0, automatic analysis of metaphors is important for their extensive usage.", "labels": [], "entities": [{"text": "automatic analysis of metaphors", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.7103757560253143}]}, {"text": "Metaphors area part of figurative language that compares different concepts, often on a cognitive level.", "labels": [], "entities": []}, {"text": "Many approaches have been proposed for automatic detection of metaphors, even using sequential models or neural networks.", "labels": [], "entities": [{"text": "automatic detection of metaphors", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.8326089680194855}]}, {"text": "In this paper, we propose a method for detection of metaphors at the token level using a hybrid model of Bidirectional-LSTM and CRF.", "labels": [], "entities": []}, {"text": "We used fewer features, as compared to the previous state-of-the-art sequential model.", "labels": [], "entities": []}, {"text": "On experimentation with VUAMC, our method obtained an F-score of 0.674.", "labels": [], "entities": [{"text": "VUAMC", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.8067010045051575}, {"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9997608065605164}]}], "introductionContent": [{"text": "A metaphor is a figure of speech that brings together different concepts, which are often distinct and seemingly unrelated.", "labels": [], "entities": []}, {"text": "A metaphor comprises a word or a phrase representing something else, where applying it in its literal sense is often not possible.", "labels": [], "entities": []}, {"text": "Metaphors bring in vivid imagery to our communications by drawing an analogy between one thing and another or between actions.", "labels": [], "entities": []}, {"text": "Metaphors also provide a fundamental cognitive and structural role.", "labels": [], "entities": []}, {"text": "introduced metaphor as a central cognitive device that gives structure to abstract conceptual domains, referred to as the 'target domains', which are described in terms of concrete domains, referred to as the 'source domains'.", "labels": [], "entities": []}, {"text": "In our work, we do not try to ascertain the source or target domains, rather we focus on determining the presence of metaphorically used tokens in any given sentence.", "labels": [], "entities": []}, {"text": "To estimate the frequency of occurrence of metaphors, conducted a study on a subset of the British National Corpus and manually annotated the metaphorical expressions in that data.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 91, "end_pos": 114, "type": "DATASET", "confidence": 0.9504744609196981}]}, {"text": "They found out that 241 sentences contained at least one metaphor among the 761 sentences considered.", "labels": [], "entities": []}, {"text": "Figurative uses of language are abundant in literature, but they are not restricted to the literary works.", "labels": [], "entities": []}, {"text": "Figurative elements of language, especially sarcasm and metaphor, are common in online product reviews, blogs, articles and posts in social networking sites.", "labels": [], "entities": []}, {"text": "With the increasing amount of textual data, the number of metaphorical instances is also increasing.", "labels": [], "entities": []}, {"text": "As the application of metaphors is pervasive, their interpretation in non-literal ways is required.", "labels": [], "entities": []}, {"text": "To process metaphors automatically, their detection is of foremost importance.", "labels": [], "entities": []}, {"text": "Their abundance in any language suggests that their detection would benefit the entire Natural Language Processing (NLP) community, for it would benefit methods like paraphrasing, summarization, machine translation, etc.", "labels": [], "entities": [{"text": "summarization", "start_pos": 180, "end_pos": 193, "type": "TASK", "confidence": 0.9847686290740967}, {"text": "machine translation", "start_pos": 195, "end_pos": 214, "type": "TASK", "confidence": 0.7368083149194717}]}, {"text": "As of now, most of the state of the art machine translations treat text literally and hence errors creep into the automated translations.", "labels": [], "entities": []}, {"text": "There has been an increasing interest in automated processing of metaphors in the NLP community for their pervasiveness in our communications.", "labels": [], "entities": []}, {"text": "To analyze and interpret a metaphor, it has to be identified first.", "labels": [], "entities": []}, {"text": "Some of the existing computational models for detection of metaphors use a hierarchical organization of conventional metaphors, or selectional restrictions as provided in lexical resources available or by using word embeddings, or conventional mappings of subjectverb, verb-object, subject-object.", "labels": [], "entities": [{"text": "detection of metaphors", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.8044531544049581}]}, {"text": "In this paper, we treat the problem of tokenlevel metaphor detection as a sequence tagging problem; and sequence tagging problems, like Parts Of Speech (POS) tagging and Named Entity Recognition (NER), have been long dealt in NLP.", "labels": [], "entities": [{"text": "tokenlevel metaphor detection", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.9209771951039633}, {"text": "sequence tagging", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.7421161234378815}, {"text": "Parts Of Speech (POS) tagging", "start_pos": 136, "end_pos": 165, "type": "TASK", "confidence": 0.6321708645139422}, {"text": "Named Entity Recognition (NER)", "start_pos": 170, "end_pos": 200, "type": "TASK", "confidence": 0.808881531159083}]}, {"text": "We approach token-level metaphor detection, with the help of Long Short-Term Memory (LSTM) and Conditional Random Fields (CRF).", "labels": [], "entities": [{"text": "token-level metaphor detection", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.8938088218371073}]}, {"text": "We try to identify the metaphors in a running text, irrespective of the type of the metaphor.", "labels": [], "entities": []}, {"text": "To observe the effectiveness of our proposed method, we have experimented on VUAMC (, an open domain text corpus, that has been hand-annotated for metaphors at the token level.", "labels": [], "entities": [{"text": "VUAMC", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.8636531829833984}]}, {"text": "Our method obtained the state-of-the-art results as compared to previously reported works on token level metaphor detection.", "labels": [], "entities": [{"text": "token level metaphor detection", "start_pos": 93, "end_pos": 123, "type": "TASK", "confidence": 0.8018792122602463}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We start in Section 2 by discussing existing literature on metaphor detection which compares to our work in at least one facet and compare these with our methodology.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.9514195621013641}]}, {"text": "Section 3 discusses the preliminaries.", "labels": [], "entities": [{"text": "preliminaries", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.9544514417648315}]}, {"text": "Section 4 presents the motivation behind proposing our method.", "labels": [], "entities": []}, {"text": "Section 5 provides information about the dataset used in the experiments and discusses the feature set considered.", "labels": [], "entities": []}, {"text": "Section 6 provides the experimental details.", "labels": [], "entities": []}, {"text": "Section 7 presents the results of our experiments along with some discussions.", "labels": [], "entities": []}, {"text": "Section 8 concludes the paper suggesting possible future works.", "labels": [], "entities": []}], "datasetContent": [{"text": "We considered all tokens, irrespective of their POS tag supplied with the VUAMC.", "labels": [], "entities": [{"text": "VUAMC", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9701477885246277}]}, {"text": "We ignored the punctuations like comma (,), exclamation mark (!), period (.), and quotation mark ('), as punctuation marks cannot be used metaphorically, to the best of our knowledge.", "labels": [], "entities": []}, {"text": "For each of the tokens considered, the feature vector was computed as described in section 5.", "labels": [], "entities": []}, {"text": "As the punctuation marks were not considered, the tokens belonging to a particular sentence were clubbed together, in the order they appear in the sentence in VUAMC.", "labels": [], "entities": [{"text": "VUAMC", "start_pos": 159, "end_pos": 164, "type": "DATASET", "confidence": 0.9642359614372253}]}, {"text": "As the label for metaphoricity, each token is marked as negative or positive representing literal and metaphorical tokens, respectively.", "labels": [], "entities": []}, {"text": "As sentences of the dataset are not of equal length, we padded them with constant vectors, labeled negative for metaphoricity.", "labels": [], "entities": []}, {"text": "Ina running text, if the end of sentences are not marked, an automatic processor for sentences can be used to mark them.", "labels": [], "entities": []}, {"text": "We used a Bi-LSTM-CRF architecture similar to the ones presented by, and.", "labels": [], "entities": []}, {"text": "Our architecture used a Bidirectional-LSTM with a layer of CRF above it.", "labels": [], "entities": []}, {"text": "Our model with back-propagation updated parameters with every batch.", "labels": [], "entities": []}, {"text": "We used a batch size of 128 while training.", "labels": [], "entities": []}, {"text": "We used a learning rate of 0.0005 and had set the gradient clipping to 5.", "labels": [], "entities": []}, {"text": "We used Adam () as our learning method with a dropout of 0.5.", "labels": [], "entities": []}, {"text": "Our model used a single LSTM layer for forward and a single LSTM layer for backward propagations.", "labels": [], "entities": []}, {"text": "Each of the layers had a dimension of 100.", "labels": [], "entities": []}, {"text": "It was observed that changing the dimensions did not significantly improve the results.", "labels": [], "entities": [{"text": "dimensions", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.979280948638916}]}, {"text": "The system is trained and tested on the complete corpus, leaving out the metadata of the genre they belong to in the British National Corpus (BNC).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 117, "end_pos": 146, "type": "DATASET", "confidence": 0.9723820288976034}]}, {"text": "We did a 10-fold cross validation on the entire dataset, with the order of the sentences changed randomly.", "labels": [], "entities": []}, {"text": "We rearranged the sentences so that the sentences belonging to the same genre did not necessarily get clubbed together as originally in the dataset.", "labels": [], "entities": []}, {"text": "The performance of the system with the suggested features is evaluated on the basis of Precision, Recall and F1-score.", "labels": [], "entities": [{"text": "Precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.999539852142334}, {"text": "Recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9971520900726318}, {"text": "F1-score", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9983396530151367}]}, {"text": "To check whether a feature contributes to the results, we also experimented on an incremental basis, i.e. adding features on top of the others.", "labels": [], "entities": []}, {"text": "We also checked separately for the features along with the word embeddings for the words (tokens).", "labels": [], "entities": []}, {"text": "We did this with a 10-fold cross-validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for complete VU Amsterdam Metaphor Corpus.", "labels": [], "entities": [{"text": "VU Amsterdam Metaphor Corpus", "start_pos": 31, "end_pos": 59, "type": "DATASET", "confidence": 0.8446162045001984}]}, {"text": " Table 2: Results for Feature Selection on the complete VU Amsterdam Metaphor Corpus with Bi- LSTM-CRF.", "labels": [], "entities": [{"text": "VU Amsterdam Metaphor Corpus", "start_pos": 56, "end_pos": 84, "type": "DATASET", "confidence": 0.880102813243866}, {"text": "Bi- LSTM-CRF", "start_pos": 90, "end_pos": 102, "type": "METRIC", "confidence": 0.8926259080568949}]}]}