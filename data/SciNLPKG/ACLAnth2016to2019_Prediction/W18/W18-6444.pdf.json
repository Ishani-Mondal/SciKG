{"title": [{"text": "Translation of Biomedical Documents with Focus on Spanish-English", "labels": [], "entities": [{"text": "Translation of Biomedical Documents", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8547672778367996}]}], "abstractContent": [{"text": "For the WMT 2018 shared task of translating documents pertaining to the Biomedical domain, we developed a scoring formula that uses an unsophisticated and effective method of weighting term frequencies and was integrated in a data selection pipeline.", "labels": [], "entities": [{"text": "WMT 2018 shared task", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.6091263741254807}]}, {"text": "The method was applied on five language pairs and it performed best on Portuguese-English, where a BLEU score of 41.84 placed it third out of seven runs submitted by three institutions.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9811875522136688}]}, {"text": "In this paper, we describe our method and results with a special focus on Spanish-English where we compare it against a state-of-the-art method.", "labels": [], "entities": []}, {"text": "Our contribution to the task lies in introducing a fast, unsupervised method for selecting domain-specific data for training models which obtain good results using only 10% of the general domain data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The 2018 Biomedical Translation Task, held as part of the Third Conference on Machine Translation, aims at evaluating systems on scientific publications from Medline ().", "labels": [], "entities": [{"text": "Biomedical Translation Task", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.9342900713284811}, {"text": "Machine Translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7343905568122864}, {"text": "Medline", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.6866318583488464}]}, {"text": "The task is particularly challenging as there is still not enough bilingual medical data available for training high quality Machine Translation (MT) systems.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 125, "end_pos": 149, "type": "TASK", "confidence": 0.8673038601875305}]}, {"text": "We develop and apply a data selection method on five out of the nine language pairs addressed by the task: English-Spanish, SpanishEnglish, English-Portuguese, Portuguese-English and English-Romanian.", "labels": [], "entities": []}, {"text": "Data selection, as a domain adaptation technique, exploits all available (bilingual) general domain corpora with the purpose of extracting sentences that have a strong relationship to a given in-domain.", "labels": [], "entities": [{"text": "Data selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7071597576141357}, {"text": "domain adaptation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7501501142978668}]}, {"text": "All sentences from the general domain pool are scored according to a similarity function/ algorithm/ method and after being sorted, the most similar ones are selected to take part in the MT training pipeline.", "labels": [], "entities": [{"text": "MT training pipeline", "start_pos": 187, "end_pos": 207, "type": "TASK", "confidence": 0.9114452997843424}]}, {"text": "The subsampling is usually done using a threshold, which is the number of sentence (pairs) or a percentage of the sentences to be considered in-domain.", "labels": [], "entities": []}, {"text": "We introduce a data selection method which is fast to apply and yields good results when compared with a strong baseline and a state-of-the-art method.", "labels": [], "entities": []}, {"text": "The simplicity of the method has at its core term frequencies and a newly developed similarity function.", "labels": [], "entities": []}, {"text": "On the one hand, no models need to be trained and the method is unsupervised, but on the other hand, the method does not consider the context of the words or their semantics.", "labels": [], "entities": []}, {"text": "However, the results are very encouraging with BLEU () scores between 31.05 and 41.84 for four language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9989470839500427}]}, {"text": "The paper is structured as follows: the next section briefly presents related work, Section 3 describes the experimental results along with a description of our algorithm, Section 4 gives an overview of the results obtained in the task and additional experiments and the last section presents conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the experimental settings including the corpora and the tools used, as well as the data selection algorithm we developed.", "labels": [], "entities": []}, {"text": "For Spanish-English, the best performing variant of our method was run 1 -using only the English side of the corpora in the algorithm.", "labels": [], "entities": []}, {"text": "We evaluated our DSTF-EN method against a strong baseline (that uses an interpolated LM), a baseline trained using only the in-domain data and the state-of-theart method MML for the Spanish-English language pair . Following recommendations from H. and standard practices, we tuned the systems three times and report in the averaged BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 332, "end_pos": 336, "type": "METRIC", "confidence": 0.9995061159133911}]}, {"text": "According to the BLEU scores, our method outperformed both baselines and gained almost 1 BLEU point over MML.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9990366697311401}, {"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9990096092224121}, {"text": "MML", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.7015253305435181}]}, {"text": "The strong baseline is very competitive with both data selection methods.", "labels": [], "entities": []}, {"text": "This can easily be explained, since the system relies on the same interpolated language model as DSTF-EN and MML.", "labels": [], "entities": [{"text": "MML", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.8528584837913513}]}, {"text": "There is a 3 BLEU points difference between our results and the baseline trained only the in-domain data and almost half a point BLEU score difference between the strong baseline and our method.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.999484658241272}, {"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9989482760429382}]}, {"text": "With respect to the ME-TEOR scores, our method again outperforms the state-of-the-art approach.", "labels": [], "entities": [{"text": "ME-TEOR", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9911318421363831}]}, {"text": "In order to determine whether our method (DSTF-EN) outperforms the state-of-the-art method (MML) from a statistical point of view, we applied paired bootstrap resampling).", "labels": [], "entities": []}, {"text": "The MTCompar-Eval tool () was used for this purpose where the source, reference and one or more system translations are used in the analysis.", "labels": [], "entities": [{"text": "MTCompar-Eval", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.683989405632019}]}, {"text": "For our analysis we selected the best translation of each system according to their BLEU scores . depicts the paired bootstrap resampling BLEU graph (left side) and the F-measure graph (right side).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9989957213401794}, {"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9856563806533813}]}, {"text": "The x-axis is represented by 1000 resamples of the test set and the y-axis represents the We tuned three times and averaged the BLEU scores difference in BLEU (respectively F-measure) between DSTF-EN and MML for all resamples.", "labels": [], "entities": [{"text": "BLEU scores difference", "start_pos": 128, "end_pos": 150, "type": "METRIC", "confidence": 0.9507598082224528}, {"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.9952970147132874}, {"text": "F-measure", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9915873408317566}, {"text": "DSTF-EN", "start_pos": 192, "end_pos": 199, "type": "DATASET", "confidence": 0.9597914218902588}, {"text": "MML", "start_pos": 204, "end_pos": 207, "type": "DATASET", "confidence": 0.8175671100616455}]}, {"text": "The p-value from the first graph in reports that in 11 cases out of the 1000 resamples, the state-ofthe-art method performed better in terms of BLEU than our method (marked with a small red area in the graph).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9991307854652405}]}, {"text": "A similar behaviour can be oserved in the right graph from wherein 34 cases out of 1000, MML outperformed DSTF-EN in terms of F-measure.", "labels": [], "entities": [{"text": "DSTF-EN", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.8982260227203369}, {"text": "F-measure", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9634519815444946}]}, {"text": "Therefore in 96.6% of the times our method wins over the state-of-the-art when using the F-measure and in 98.9% of the cases, our method is better than MML when evaluating with BLEU (large green areas in the graphs).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.8965651392936707}, {"text": "BLEU", "start_pos": 177, "end_pos": 181, "type": "METRIC", "confidence": 0.99793940782547}]}, {"text": "We conclude that our method has a statistical significant performance in comparison with the state-of-theart method when selecting the 10% of the general domain sentences that were most similar to the indomain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. We note that the differences between  each run, for every language pair, are insignifi- cant except for one language pair, therefore we  conclude that either one of the algorithm varia- tions can be successfully applied as a fast data  selection technique that yields good translations  (BLEU scores between 31 and 42 for four out of  five language pairs).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 298, "end_pos": 302, "type": "METRIC", "confidence": 0.998651921749115}]}, {"text": " Table 3: BLEU scores reported by WMT", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993564486503601}, {"text": "WMT", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7452486157417297}]}, {"text": " Table 4: averaged BLEU scores for Spanish- English", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9721049070358276}]}]}