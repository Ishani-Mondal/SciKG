{"title": [{"text": "BENGAL: An Automatic Benchmark Generator for Entity Recognition and Linking", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9602143168449402}, {"text": "Entity Recognition and Linking", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.7527290284633636}]}], "abstractContent": [{"text": "The manual creation of gold standards for named entity recognition and entity linking is time-and resource-intensive.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.6220528781414032}, {"text": "entity linking", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.7094369530677795}]}, {"text": "Moreover, recent works show that such gold standards contain a large proportion of mistakes in addition to being difficult to maintain.", "labels": [], "entities": []}, {"text": "We hence present BENGAL, a novel automatic generation of such gold standards as a complement to manually created benchmarks.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9938123226165771}]}, {"text": "The main advantage of our benchmarks is that they can be readily generated at anytime.", "labels": [], "entities": []}, {"text": "They are also cost-effective while being guaranteed to be free of annotation errors.", "labels": [], "entities": []}, {"text": "We compare the performance of 11 tools on benchmarks in English generated by BENGAL and on 16 benchmarks created manually.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.5177260041236877}]}, {"text": "We show that our approach can be ported easily across languages by presenting results achieved by 4 tools on both Brazilian Por-tuguese and Spanish.", "labels": [], "entities": []}, {"text": "Overall, our results suggest that our automatic benchmark generation approach can create varied benchmarks that have characteristics similar to those of existing benchmarks.", "labels": [], "entities": [{"text": "benchmark generation", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.8384418487548828}]}, {"text": "Our experimental results are available at http: //faturl.com/bengalexpinlg and the code at https://github.", "labels": [], "entities": []}, {"text": "com/dice-group/BENGAL.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9230517148971558}]}], "introductionContent": [{"text": "The creating of gold standard is of central importance for the objective assessment and development of approaches all around computer science.", "labels": [], "entities": []}, {"text": "For example, evaluation campaigns such as BioASQ () have led to an improvement of the F-measure achieved by biomedical question answering systems by more than 5%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9967862367630005}, {"text": "biomedical question answering", "start_pos": 108, "end_pos": 137, "type": "TASK", "confidence": 0.6130398511886597}]}, {"text": "While the manual creation of Named Entity Recognition (NER) and Entity Linking (EL) gold standards (also called benchmarks) has the advantage of yielding resources which reflect human processing, it also exhibits significant disadvantages: a) Annotation mistakes: Human annotators have to read through every sentence in the corpus and often (a) miss annotations or (b) assign wrong resources to entities for reasons as various as fatigue or lack of background knowledge (and this even when supported with annotation tools).", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.8174553612867991}]}, {"text": "For example, was able to determine that up to 38,453 of the annotations in commonly used benchmarks (see GER-BIL () fora list of these benchmarks) were erroneous.", "labels": [], "entities": [{"text": "GER-BIL", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.5634395480155945}]}, {"text": "A manual evaluation of 25 documents from the ACE2004 benchmark revealed that 195 annotations were missing and 14 of 306 annotations were incorrect.", "labels": [], "entities": [{"text": "ACE2004 benchmark", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.9646482765674591}]}, {"text": "Similar findings were reported for AIDA/CONLL) and OKE2015).", "labels": [], "entities": [{"text": "AIDA/CONLL)", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.8084483444690704}, {"text": "OKE2015", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9275822639465332}]}, {"text": "b) Volume: Manually created benchmarks are usually small (commonly < 2, 500 documents, see).", "labels": [], "entities": [{"text": "Volume", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.931346595287323}]}, {"text": "Hence, they are of little help when aiming to benchmark the scalability of existing solutions (especially when these solutions use caching).", "labels": [], "entities": []}, {"text": "c) Lack of updates: Manual benchmark generation approaches lead to static corpora which tend not to reflect the newest reference knowledge graphs (also called Knowledge Base (KB)s).", "labels": [], "entities": []}, {"text": "For example, several of the benchmarks presented in GERBIL () link to outdated versions of Wikipedia or DBpedia.", "labels": [], "entities": [{"text": "GERBIL", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.5093967318534851}]}, {"text": "d) Popularity bias: van show that manual benchmarks are often biased towards popular resources.", "labels": [], "entities": []}, {"text": "e) Lack of availability: The lack of benchmarks for resource-poor languages inhibits the development of corresponding NER and EL solutions.", "labels": [], "entities": []}, {"text": "Automatic methods area viable and supplementary approach for the generation of gold standards for NER and EL, especially as they address some of the weaknesses of the manual benchmark creation process.", "labels": [], "entities": [{"text": "NER", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.6372700929641724}, {"text": "EL", "start_pos": 106, "end_pos": 108, "type": "DATASET", "confidence": 0.6735902428627014}]}, {"text": "The main contribution of our paper is a novel approach for the automatic generation of benchmarks for NER and EL dubbed BENGAL.", "labels": [], "entities": [{"text": "NER", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.731428325176239}, {"text": "BENGAL", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9232739806175232}]}, {"text": "Our approach relies on the abundance of structured data in Resource Description Framework (RDF) on the Web and is based on Natural Language Generation (NLG) techniques which verbalize such data to generate automatically annotated natural language statements.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 123, "end_pos": 156, "type": "TASK", "confidence": 0.8114371597766876}]}, {"text": "Our automatic benchmark creation method addresses the drawbacks of manual benchmark generation aforementioned as follows: a) It alleviates the human annotation error problem by relying on data in RDF which explicitly contain the entities to find.", "labels": [], "entities": [{"text": "benchmark creation", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.690200537443161}, {"text": "benchmark generation", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.7635661661624908}]}, {"text": "b) BENGAL is able to generate arbitrarily large benchmarks.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.9828439950942993}]}, {"text": "Hence, it can enhance the measurement of both the accuracy and the scalability of approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9994612336158752}]}, {"text": "c) BENGAL can be updated easily to reflect the newest terminology and reference KBs.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.995187520980835}]}, {"text": "Hence, it can generate corpora that reflect the newest KBs.", "labels": [], "entities": []}, {"text": "d) BENGAL is not biased towards popular resources as it can choose entities to include in the benchmark generated following a uniform distribution.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.9682181477546692}]}, {"text": "e) BENGAL can be ported to any token-based language.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.9778304696083069}]}, {"text": "This is exemplified by porting BENGAL to Portuguese and Spanish.", "labels": [], "entities": [{"text": "porting BENGAL", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.7180928587913513}]}], "datasetContent": [{"text": "This last strategy is a specialization of the star graph generation where the set of triples to a resource is not chosen randomly.", "labels": [], "entities": []}, {"text": "Instead, for each class (e.g., :Person) of the input KB, we begin by filtering the set of properties and only consider properties that (1) have the said class as domain and (2) achieve a coverage above a user-set threshold (60% in our experiments) (e.g., :birthPlace, :deathPlace, :spouse).", "labels": [], "entities": []}, {"text": "We then build a property co-occurence graph for the said class in which the nodes are the properties selected in the preceding step and the co-occurence of two properties p 1 and p 2 is the instance r of the input class where \u2203o 1 , o 2 : (r, The resulting graph is then clustered (e.g., by using the approach presented by Ngonga).", "labels": [], "entities": []}, {"text": "We finally select the clusters which contain the properties with the highest frequencies in K that allow the selection of at least d triples from K.", "labels": [], "entities": []}, {"text": "For example, if :birthPlace (frequency = 10), :deathPlace (frequency = 10) were in the same cluster while :spouse (frequency = 8) were in its own cluster, we would choose the pair (:birthPlace, :deathPlace) and return the corresponding triples for our input resource.", "labels": [], "entities": []}, {"text": "Hence, we would return Listing 4 for our running example.", "labels": [], "entities": []}, {"text": "We generated 13 datasets in English (B1-B13), 4 datasets in Brazilian Portuguese and 4 datasets in Spanish to evaluate our approach.", "labels": [], "entities": []}, {"text": "6 B1 to B10 were generated by running our five sub-graph generation methods with and without paraphrasing.", "labels": [], "entities": []}, {"text": "The number of documents was set to 100 while (d min , d max ) was set to (1, 5).", "labels": [], "entities": []}, {"text": "B11 shows how BENGAL can be used to evaluate the scalability of approaches.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9971741437911987}]}, {"text": "Here, we used the hybrid generation strategy to generate 10,000 documents.", "labels": [], "entities": []}, {"text": "B12 and B13 comprise 10 longer documents each with d min set to 90.", "labels": [], "entities": [{"text": "B12", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8111140727996826}]}, {"text": "For B12, we focused on generating a high number of entities in the documents while B13 contains less entities but the same number of documents.", "labels": [], "entities": []}, {"text": "We compared B1-B13 with the 16 manually created gold standards for English found in GER-BIL.", "labels": [], "entities": [{"text": "B1-B13", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.8828244209289551}, {"text": "GER-BIL", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9320060610771179}]}, {"text": "The comparison was carried out in two ways.", "labels": [], "entities": [{"text": "comparison", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9736409187316895}]}, {"text": "First, we assessed the features of the datasets.", "labels": [], "entities": []}, {"text": "Then, we compared the micro F-measure of 11 NER and EL frameworks on the manually and automatically generated datasets.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.8734586834907532}]}, {"text": "We chose to use these 11 frameworks because they are included in GERBIL.", "labels": [], "entities": [{"text": "GERBIL", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.9317759871482849}]}, {"text": "This inclusion ensures that their interfaces are compatible and their results comparable.", "labels": [], "entities": []}, {"text": "In addition, we assessed the performance of multilingual NER and EL systems on the datasets P1-P4 to show that BENGAL can be easily ported to languages other than English.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.801011860370636}]}, {"text": "The first aim of our evaluation was to quantify the variability of the datasets B1-B13 generated by BENGAL.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.6470902562141418}]}, {"text": "To this end, we compared the distribution of the part of speech (POS) tags of the BENGAL datasets with those of the 16 benchmark datasets.", "labels": [], "entities": [{"text": "BENGAL datasets", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.7500633001327515}]}, {"text": "An analysis of the Pearson correlation of these distributions revealed that the manually created datasets (D1-D16) have a high correlation (0.88 on average) with a minimum of 0.61 (D10-D16).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 19, "end_pos": 38, "type": "METRIC", "confidence": 0.9516421854496002}]}, {"text": "The correlation of the POS tag distributions between BENGAL datasets and a manually created dataset vary between 0.34 (D7-B11) and 0.89 (D14-B9) with an average of 0.67.", "labels": [], "entities": [{"text": "BENGAL datasets", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.8578636646270752}]}, {"text": "This shows that BENGAL datasets can be generated to be similar to manually created datasets (D14-B9) as well as to be very different to them (D7-B11).", "labels": [], "entities": [{"text": "BENGAL datasets", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.9139991402626038}]}, {"text": "Hence, BENGAL can be used for testing sentence structures that are not common in the current manually generated benchmarks.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9901139736175537}]}, {"text": "We also studied the distribution of entities and tokens across the datasets in our evaluation.", "labels": [], "entities": []}, {"text": "gives an overview of these distributions, where E is the set of entities in the corpus C.", "labels": [], "entities": []}, {"text": "The distribution of values for the different features is very diverse across the different manually created datasets.", "labels": [], "entities": []}, {"text": "This is mainly due to (1) different ways to annotate entities and (2) the domains of the datasets (news, description of entities, microposts).", "labels": [], "entities": []}, {"text": "As shown in, BENGAL can be easily configured to generate a wide variety of datasets with similar quality and number of documents to those of real datasets.", "labels": [], "entities": [{"text": "BENGAL", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9400569796562195}]}, {"text": "This is mainly due to our approach being able to generate benchmarks ranging from (1) benchmarks with sentences containing a large number of entities without any filler terms (high entity density) to (2) benchmarks which contain more information pertaining to entity types and literals (low entity density).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Excerpt of the features of the datasets used in our evaluation. The datasets B4, B6, B8 and B10  are paraphrased versions of B3, B5, B7 resp. B9 and share similar characteristics.", "labels": [], "entities": []}, {"text": " Table 3: Excerpt of micro F1-scores of the annotators for the A2KB experiments on chosen datasets.  N/A means that the annotator stopped with an error.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.5155525207519531}, {"text": "A2KB experiments", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.8739269971847534}, {"text": "N/A", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9246375759442648}]}]}