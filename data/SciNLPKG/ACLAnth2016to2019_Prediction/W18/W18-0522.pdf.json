{"title": [{"text": "The Whole is Greater than the Sum of its Parts: Towards the Effectiveness of Voting Ensemble Classifiers for Complex Word Identification", "labels": [], "entities": [{"text": "Complex Word Identification", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.6176780064900717}]}], "abstractContent": [{"text": "In this paper, we present an effective system using voting ensemble classifiers to detect contextually complex words for non-native English speakers.", "labels": [], "entities": []}, {"text": "To make the final decision, we channel a set of eight calibrated classifiers based on lexical, size and vocabulary features and train our model with annotated datasets collected from a mixture of native and non-native speakers.", "labels": [], "entities": []}, {"text": "Thereafter, we test our system on three datasets namely NEWS, WIKINEWS, and WIKIPEDIA and report competitive results with an F1-Score ranging between 0.777 to 0.855 for each of the datasets.", "labels": [], "entities": [{"text": "NEWS", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9720078110694885}, {"text": "WIKINEWS", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.854499101638794}, {"text": "WIKIPEDIA", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.9140018820762634}, {"text": "F1-Score", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9995761513710022}]}, {"text": "Our system outperforms multiple other models and falls within 0.042 to 0.026 percent of the best-performing model's score in the shared task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Complex Word Identification (CWI) is an essential sub-task for Lexical Simplification.", "labels": [], "entities": [{"text": "Complex Word Identification (CWI)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7478577544291815}, {"text": "Lexical Simplification", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.908934086561203}]}, {"text": "Lexical Simplification involves substituting a complicated word in the text with a more straightforward synonym.", "labels": [], "entities": [{"text": "Lexical Simplification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9174807667732239}]}, {"text": "shows the pipeline for Lexical Simplification systems.", "labels": [], "entities": [{"text": "Lexical Simplification", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.9220307767391205}]}, {"text": "It is geared for target population like non-native speakers, second-language learners, young learners, and people with language disabilities (like Aphasia and Alexia), with the aim of allowing them to comprehend the presented text completely.", "labels": [], "entities": []}, {"text": "The goal of the shared task is as follows: Given a target word (or phrase) and its context, we are to computationally determine if the target word is complex or not.", "labels": [], "entities": []}, {"text": "Unlike the SemEval 2016 shared task, the target words here could have more than one word (e.g., teenage girl), and the context could stretch over multiple sentences.", "labels": [], "entities": [{"text": "SemEval 2016 shared task", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.8046832382678986}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we mention related work in the area of Complex Word Identification -in particular, the previous shared task at SemEval 2016.", "labels": [], "entities": [{"text": "Complex Word Identification", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6169997056325277}]}, {"text": "Section 3 describes the dataset of NLP BEA'S CWI shared task at NAACL 2018.", "labels": [], "entities": [{"text": "NLP BEA'S CWI shared task at NAACL 2018", "start_pos": 35, "end_pos": 74, "type": "DATASET", "confidence": 0.6972805298864841}]}, {"text": "In Section 4, we describe our system, the features used, and our classification methodology.", "labels": [], "entities": []}, {"text": "Moving along we then report our competitive results in Section 5 and discuss them in Section 6.", "labels": [], "entities": []}, {"text": "We conclude by recapitulating our paper in Section 7 and identify future work that will be done.", "labels": [], "entities": []}], "datasetContent": [{"text": "For this shared task (, we used only the English monolingual dataset, which made use of data from a number of sources, such as News articles, WikiNews and Wikipedia articles.", "labels": [], "entities": []}, {"text": "shows details such as total sentences and the number of unique sentences that we computed across all the three datasets.", "labels": [], "entities": []}, {"text": "The Wikipedia dataset consisted of sentences from Wikipedia articles.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9642362892627716}]}, {"text": "Likewise, the WIKINEWS dataset and the NEWS dataset contained sentences from news articles.", "labels": [], "entities": [{"text": "WIKINEWS dataset", "start_pos": 14, "end_pos": 30, "type": "DATASET", "confidence": 0.9780142307281494}, {"text": "NEWS dataset", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9805307686328888}]}, {"text": "However, the difference between the two is that the articles in the NEWS dataset were written by professional journalists, while lesser experienced writers wrote those in the WIKINEWS dataset.", "labels": [], "entities": [{"text": "NEWS dataset", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9873857498168945}, {"text": "WIKINEWS dataset", "start_pos": 175, "end_pos": 191, "type": "DATASET", "confidence": 0.9778434634208679}]}, {"text": "Ina majority of instances, the target words were just a single word.", "labels": [], "entities": []}, {"text": "However, there were a few target words that were over a word long.", "labels": [], "entities": []}, {"text": "Similarly, inmost cases, the context was only one sentence, except fora few instances in which the context was as long as 3 -4 sentences.", "labels": [], "entities": []}, {"text": "The training datasets were annotated by 10 native and 10 non-native English speakers.", "labels": [], "entities": []}, {"text": "Even if one amongst them found the word to be difficult, it was annotated as complex.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Description of the Dataset. The first column  gives the dataset. The next column gives the total num- ber of sentences. The last column gives the number of  unique sentences.", "labels": [], "entities": []}, {"text": " Table 2: Results of ten-fold cross-validation on the  training for each of the classifiers on the complex class  only. This was used to choose our top classifiers.", "labels": [], "entities": []}, {"text": " Table 4: F1-Score for each of the datasets for the top  10 teams on the corresponding test dataset. The high- lighted row corresponds to our submission.", "labels": [], "entities": [{"text": "F1-Score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991279244422913}]}]}