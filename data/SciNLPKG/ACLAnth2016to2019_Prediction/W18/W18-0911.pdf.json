{"title": [{"text": "Bigrams and BiLSTMs Two neural networks for sequential metaphor detection", "labels": [], "entities": [{"text": "sequential metaphor detection", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.8597104549407959}]}], "abstractContent": [{"text": "We present and compare two alternative deep neural architectures to perform word-level metaphor detection on text: a bi-LSTM model and anew structure based on recursive feed-forward concatenation of the input.", "labels": [], "entities": [{"text": "word-level metaphor detection", "start_pos": 76, "end_pos": 105, "type": "TASK", "confidence": 0.7952429254849752}]}, {"text": "We discuss different versions of such models and the effect that input manipulation-specifically, reducing the length of sentences and introducing concreteness scores for words-have on their performance.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Interestingly adding explicit semantic information such as concreteness ratings in our input -which means, somehow, reverting to feature engineering -did produce better results for the composition architecture, but not yet for our Bi-LSTM.", "labels": [], "entities": []}, {"text": "show the results of our best performing models when the concreteness of the individual token was explicitly added to the embeddings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: F1 for different windows of concatenation (N)  in the composition model. N=1 is equivalent to no con- catenation.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9982221722602844}]}, {"text": " Table 4: Parameter tuning, testing both deeper and wider settings of the model. We write in parenthesis the  dimensions each layer: for example Dense(20) is a fully connected layer with an output space of dimensionality  20.", "labels": [], "entities": [{"text": "Parameter tuning", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7515773177146912}]}, {"text": " Table 6: Results for different models using sentence breaking to 20 (any sentence longer than 20 tokens is split in  two parts treated as complete different sentences). The first line works as baseline showing a model without input  manipulation. Concat(n=) represents our compositional model, Chunk signifies the usage of sentence breaking.", "labels": [], "entities": [{"text": "sentence breaking", "start_pos": 324, "end_pos": 341, "type": "TASK", "confidence": 0.7619635462760925}]}, {"text": " Table 7: Results for different models using embeddings enriched with explicit information regarding word con- creteness and sentence breaking to 20 (any sentence longer than 20 tokens is split in two parts treated as complete  different sentences). The first lines work as baselines showing the performance of previous models (without any  input manipulation, only chunking, only concreteness scores). Concat(n=) represents our compositional model,  Chunk signifies the usage of sentence breaking, Conc represents the usage of concreteness scores.", "labels": [], "entities": []}, {"text": " Table 8: Results for the evaluation set from the shared dataset competition (NAACL 2018). We used sentence  breaking and concreteness information.", "labels": [], "entities": [{"text": "NAACL 2018)", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8810992439587911}, {"text": "sentence  breaking", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.7529684007167816}]}]}