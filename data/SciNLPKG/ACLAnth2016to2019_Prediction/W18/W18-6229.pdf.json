{"title": [{"text": "HGSGNLP at IEST 2018: An Ensemble of Machine Learning and Deep Neural Architectures for Implicit Emotion Classification in Tweets", "labels": [], "entities": [{"text": "HGSGNLP at IEST 2018", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.8528412282466888}, {"text": "Implicit Emotion Classification in Tweets", "start_pos": 88, "end_pos": 129, "type": "TASK", "confidence": 0.7509138107299804}]}], "abstractContent": [{"text": "This paper describes our system designed for the WASSA-2018 Implicit Emotion Shared Task (IEST).", "labels": [], "entities": [{"text": "WASSA-2018 Implicit Emotion Shared Task (IEST)", "start_pos": 49, "end_pos": 95, "type": "TASK", "confidence": 0.8642315044999123}]}, {"text": "The task is to predict the emotion category expressed in a tweet by removing the terms angry, afraid, happy, sad, surprised , disgusted and their synonyms.", "labels": [], "entities": []}, {"text": "Our final submission is an ensemble of one supervised learning model and three deep neural network based models, where each model approaches the problem from essentially different directions.", "labels": [], "entities": []}, {"text": "Our system achieves the macro F1 score of 65.8%, which is a 5.9% performance improvement over the baseline and is ranked 12 out of 30 participating teams.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9803228676319122}]}], "introductionContent": [{"text": "In Natural Language Processing, emotion recognition is concerning of associating words, phrases or documents with predefined emotion categories, such as Anger, Anticipation and Sadness).", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7506096959114075}]}, {"text": "Most of previous research works on emotion recognition ( presumes emotion words or their representations are accessible.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8085159063339233}]}, {"text": "Such models might fail to learn associations for more subtle descriptions and therefore fail to predict the emotion when overt emotion words are not available.", "labels": [], "entities": []}, {"text": "The WASSA-2018 Implicit Emotion Shared Task (IEST) () aims to predict the emotion category of a given tweet when the explicit emotion word, or trigger words, is removed.", "labels": [], "entities": [{"text": "WASSA-2018 Implicit Emotion Shared Task (IEST)", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.7181652188301086}]}, {"text": "The emotion category can be one of six classes: Anger, Disgust, Fear, Joy, Sadness and Surprise.", "labels": [], "entities": [{"text": "Joy", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9612663388252258}]}, {"text": "\"It's when you feel like you are invisible to others.\"", "labels": [], "entities": []}, {"text": "2. \"We are so that people must think we are on good drugs or just really good actors.\"", "labels": [], "entities": []}, {"text": "In the above 2 examples, with the help of commonsense or world knowledge, implicit emotion still can be inferred from context as Sadness and Joy.", "labels": [], "entities": []}, {"text": "The tokens in the examples indicate the position of the removed word in the given tweet.", "labels": [], "entities": []}, {"text": "Our submitted system is an ensemble of four broad sets of approaches combined using a weighted average of the separate predictions.", "labels": [], "entities": []}, {"text": "One approach uses traditional lexicon-based method to train a logistic regression classifier, while the remaining three approaches rely on representing the input tweet as a word vector and using neural network based architectures to give the emotion category for the tweet.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the features used in our system.", "labels": [], "entities": []}, {"text": "Section 3 explains the various approaches used by our ensemble model and the way we combined the predictions.", "labels": [], "entities": []}, {"text": "Section 4 states the experiment results and discusses the implications of those results.", "labels": [], "entities": []}, {"text": "We conclude our work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The task organizers provide a training dataset (i.e. 153k instances) and a small blind trial dataset (i.e. 9.6k instances) for system building.", "labels": [], "entities": [{"text": "system building", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.7808518409729004}]}, {"text": "Then a period of 1 week is given for submitting the predictions on a blind test dataset (i.e. 29k instances).", "labels": [], "entities": []}, {"text": "Macro-averaged F1 score is chosen to be the official evaluation metric.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9751023352146149}]}], "tableCaptions": [{"text": " Table 3: Performance comparison between individual models and ensemble model on trial data. Our final  ensemble model includes lexicon, fastText, CNN and CNN-LSTM models.", "labels": [], "entities": []}, {"text": " Table 4: Official results for our submission.", "labels": [], "entities": []}]}