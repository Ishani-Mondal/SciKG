{"title": [{"text": "Linguistic Features of Helpfulness in Automated Support for Creative Writing", "labels": [], "entities": []}], "abstractContent": [{"text": "We examine an emerging NLP application that supports creative writing by automatically suggesting continuing sentences in a story.", "labels": [], "entities": []}, {"text": "The application tracks users' modifications to generated sentences, which can be used to quantify their \"helpfulness\" in advancing the story.", "labels": [], "entities": []}, {"text": "We explore the task of predicting help-fulness based on automatically detected linguistic features of the suggestions.", "labels": [], "entities": [{"text": "predicting help-fulness", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8109537065029144}]}, {"text": "We illustrate this analysis on a set of user interactions with the application using an initial selection of features relevant to story generation.", "labels": [], "entities": [{"text": "story generation", "start_pos": 130, "end_pos": 146, "type": "TASK", "confidence": 0.728785902261734}]}], "introductionContent": [{"text": "At the intersection between natural language generation, computational creativity, and humancomputer interaction research is the vision of tools that directly collaborate with people in authoring creative content.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.7129885951677958}]}, {"text": "With recent work on automatically generating creative language (, this vision has started to come to fruition.", "labels": [], "entities": []}, {"text": "One such application focuses on providing automated support to human authors for story writing.", "labels": [], "entities": [{"text": "story writing", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.8195004761219025}]}, {"text": "In particular, Roemmele and Gordon (2015),,, and have developed systems that automatically generate suggestions for new sentences to continue an ongoing story.", "labels": [], "entities": []}, {"text": "As with other interactive language generation tasks, there is no obvious approach to evaluating these systems.", "labels": [], "entities": [{"text": "language generation tasks", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.8106041749318441}]}, {"text": "The number of acceptable continuations that can be generated fora given story is open-ended, so measures that strictly rely on similarity to a constrained set of gold standard sentences, e.g. BLEU score (), are not ideal.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 192, "end_pos": 202, "type": "METRIC", "confidence": 0.9870874881744385}]}, {"text": "Moreover, the focus of evaluation in interactive applications should be on users' judgments of the quality of the interaction.", "labels": [], "entities": []}, {"text": "While it is straightforward to ask users to rate generated content, self-reported ratings for global dimensions of quality (e.g. \"on a scale of 1-5, how coherent is this sentence in this story?\") do not necessarily provide insight into the specific characteristics that influenced these judgments, which users might not even be explicitly aware of.", "labels": [], "entities": []}, {"text": "It is more useful to examine users' judgment on an implicit level: for example, by allowing them to adapt generated sequences.", "labels": [], "entities": []}, {"text": "This is related to rewriting tasks in other domains like grammatical error correction, where annotators edit sentences to improve their perceived quality.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.5824133058389028}]}, {"text": "This enables the features of the modified sequence to be compared to those of the original.", "labels": [], "entities": []}, {"text": "In this work, we analyze a set of user interactions with the application Creative Help, where users make 'help' requests to automatically suggest new sentences in a story, which they can then freely modify.", "labels": [], "entities": []}, {"text": "We take advantage of Creative Help's functionality that tracks authors' edits to generated sentences, resulting in an alignment between each original suggestion and its modified form.", "labels": [], "entities": []}, {"text": "Previous work on this application compared different generation models according to the similarity between suggestions and corresponding modifications, based on the idea that more helpful suggestions will receive fewer edits.", "labels": [], "entities": []}, {"text": "Here, we focus on quantifying suggestions according to a set of linguistic features shown by existing research to be relevant to story generation.", "labels": [], "entities": [{"text": "story generation", "start_pos": 129, "end_pos": 145, "type": "TASK", "confidence": 0.8051574230194092}]}, {"text": "We examine whether these features can be used to predict how much authors modify the suggestions.", "labels": [], "entities": []}, {"text": "We propose that this type of analysis is useful for identifying the aspects of generated content authors implicitly find most helpful for writing.", "labels": [], "entities": []}, {"text": "It can inform the evaluation of future creativity support systems in terms of how well they maximize features associated with helpfulness.", "labels": [], "entities": []}], "datasetContent": [{"text": "We recruited people via social media, email, and Amazon Mechanical Turk to interact with Creative Help 4 for at least fifteen minutes.", "labels": [], "entities": [{"text": "Creative Help 4", "start_pos": 89, "end_pos": 104, "type": "DATASET", "confidence": 0.8111310799916586}]}, {"text": "Participants were asked to write a story of their choice.", "labels": [], "entities": []}, {"text": "They were told the objective of the task was to experiment with asking for \\help\\ but they were not required to make a certain number of help requests.", "labels": [], "entities": []}, {"text": "They could choose to edit, add to, or delete a suggestion just like any other text in their story, without any requirement to change the suggestion at all.", "labels": [], "entities": []}, {"text": "Ultimately, 139 users participated in the task, resulting in suggestion-modification pairs for 940 help requests, which includes pairs where the suggestion and modification are equivalent because no edits were made.", "labels": [], "entities": []}, {"text": "Given this dataset of pairs, we first quantified Initial Story: I knew it wasn't a good idea to put the alligator in the bathtub.", "labels": [], "entities": [{"text": "Initial Story", "start_pos": 49, "end_pos": 62, "type": "METRIC", "confidence": 0.8227843642234802}]}, {"text": "The problem was that there was nowhere else waterproof in the house, and Dale was going to be home in twenty minutes.", "labels": [], "entities": []}, {"text": "Suggested: I needed to know, too, and I was glad I was feeling it.", "labels": [], "entities": []}, {"text": "Modified: I needed to know how upset he would be if he found out about my adoption spree.", "labels": [], "entities": [{"text": "my adoption spree", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.6915730834007263}]}, {"text": "Initial Story: My brother was a quiet boy.", "labels": [], "entities": []}, {"text": "He liked to spend time by himself in his room and away from others.", "labels": [], "entities": []}, {"text": "It wasn't such a bad thing, as it allowed him to focus on his more creative side.", "labels": [], "entities": []}, {"text": "He would write books, draw comics, and write lyrics for songs that he would learn to play as he got older.", "labels": [], "entities": []}, {"text": "Suggested: He'd have to learn to get in touch with my father.", "labels": [], "entities": []}, {"text": "Modified: He had an ok relationship with my parents, but mostly because they supported his separation.: Examples of generated suggestions and corresponding modifications with their preceding context the degree to which authors edited the suggestions.", "labels": [], "entities": []}, {"text": "In particular, we calculated the similarity between each suggestion and corresponding modification in terms of Levenshtein edit distance: 1 \u2212 dist(sug,mod) max(|sug|,|mod|) , where higher values indicate more similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9848870635032654}, {"text": "Levenshtein edit distance", "start_pos": 111, "end_pos": 136, "type": "METRIC", "confidence": 0.5878295799096426}]}, {"text": "The mean similarity score for this dataset was 0.695 (SD=0.346), indicating that authors most often chose to retain large parts of the suggestions instead of fully rewriting them.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 9, "end_pos": 25, "type": "METRIC", "confidence": 0.8743161857128143}]}, {"text": "We investigated whether these similarity scores could be predicted by the linguistic features of the suggestions.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 30, "end_pos": 47, "type": "METRIC", "confidence": 0.9582920670509338}]}, {"text": "Features that significantly correlate with Levenshtein similarity can be interpreted as being 'helpful' in influencing authors to make use of the original suggestion in their story.", "labels": [], "entities": [{"text": "similarity", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.4568789005279541}]}, {"text": "It is certainly possible to use other similarity metrics to quantify helpfulness, such as similarity in terms of word embeddings.", "labels": [], "entities": []}, {"text": "These measures may model similarity below the surface text of the suggestion, in which the modification may use different words to alternatively express the same story event or idea.", "labels": [], "entities": []}, {"text": "With this approach, given a metric for any feature, the helpfulness of that feature can be quantified.", "labels": [], "entities": []}, {"text": "Here, we selected some features used in previous work on story generation and evaluating writing quality.", "labels": [], "entities": [{"text": "story generation", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.8366810083389282}]}, {"text": "In particular, we included some features used in systems applied to the Story Cloze Test, which involves selecting the most likely ending fora given story from a provided set of candidates.", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.7584788203239441}]}, {"text": "also explored some of these metrics to compare different models for sentence-based story continuation in an offline framework.", "labels": [], "entities": [{"text": "sentence-based story continuation", "start_pos": 68, "end_pos": 101, "type": "TASK", "confidence": 0.6585092147191366}]}, {"text": "Our metrics consist of those that analyze the individual features of a sentence by itself (story-independent, Metrics 1-7 below), and those that analyze the sentence with reference to the story context that precedes the suggestion (story-dependent, Metrics 8-14 below).", "labels": [], "entities": []}, {"text": "For the story-dependent metrics, we only considered suggestions that did not appear as the first sentence in the story (910 suggestions).", "labels": [], "entities": []}, {"text": "Sentence Length: The length of a candidate ending in the Story Cloze Test was found to predict its correctness (.", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.8141829371452332}]}, {"text": "We measured the length of suggestion in terms of its number of words (Metric 1).", "labels": [], "entities": []}, {"text": "Grammaticality: Grammaticality is an obvious feature of high-quality writing.", "labels": [], "entities": []}, {"text": "We used Language Tool 5, a rule-based system that detects various grammatical errors.", "labels": [], "entities": []}, {"text": "This system computed an overall grammaticality score for each sentence, equal to the proportion of total words in the sentence deemed to be grammatically correct (Metric 2).", "labels": [], "entities": []}, {"text": "Lexical Frequency: Writing quality has been found to correlate with the use of unique words).", "labels": [], "entities": []}, {"text": "We computed the average frequency of the words in each suggestion according to their GoodTuring smoothed counts in the Reddit Comment Corpus 6 (Metric 3).", "labels": [], "entities": [{"text": "GoodTuring smoothed counts", "start_pos": 85, "end_pos": 111, "type": "METRIC", "confidence": 0.6239765087763468}, {"text": "Reddit Comment Corpus 6", "start_pos": 119, "end_pos": 142, "type": "DATASET", "confidence": 0.9123525470495224}]}, {"text": "Syntactic Complexity: Writing quality is also associated with greater syntactic complexity.", "labels": [], "entities": []}, {"text": "We examined this feature in terms of the number and length of syntactic phrases in the generated sentences.", "labels": [], "entities": []}, {"text": "Phrase length was approximated by the number of children under each head verb/noun as given by the dependency parse.", "labels": [], "entities": [{"text": "Phrase length", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.6409523785114288}]}, {"text": "We counted the total number of noun phrases (Metric 4) and words per noun phrase (Metric 5), and likewise the number of verb phrases (Metric 6) and words per verb phrase (Metric 7).", "labels": [], "entities": []}, {"text": "These metrics were all normalized by sentence length.", "labels": [], "entities": []}, {"text": "Lexical Cohesion: Correct endings in the Story Cloze Test tend to have higher lexical similarity to their contexts according to statistical measures of similarity ().", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.850354532400767}]}, {"text": "We analyzed lexical cohesion be-tween the context and suggestion in terms of their Jaccard similarity (proportion of overlapping words; Metric 8), GloVe word embeddings 7 trained on the Common Crawl corpus (Metric 9), and sentence (skip-thought) vectors 8 () trained on the BookCorpus (Metric 10).", "labels": [], "entities": [{"text": "Jaccard similarity", "start_pos": 83, "end_pos": 101, "type": "METRIC", "confidence": 0.9025667607784271}, {"text": "Common Crawl corpus (Metric 9", "start_pos": 186, "end_pos": 215, "type": "DATASET", "confidence": 0.8817424575487772}, {"text": "BookCorpus", "start_pos": 274, "end_pos": 284, "type": "DATASET", "confidence": 0.988146960735321}]}, {"text": "For the latter two, the score was the cosine similarity between the means of the context and suggestion vectors, respectively.", "labels": [], "entities": []}, {"text": "Style Consistency: Automated measures of writing style have been used to predict the success of fiction novels).", "labels": [], "entities": []}, {"text": "Moreover, found that simple n-gram style features could distinguish between correct and incorrect endings in the Story Cloze Test.", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.9063958724339803}]}, {"text": "We examined the similarity in style between the context and suggestion in terms of their distributions of coarse-grained partof-speech tags, using the same approach as.", "labels": [], "entities": []}, {"text": "The similarity between the context c and suggestion s for each POS category was quantified as 1 \u2212 |posc\u2212poss| posc+poss , where pos is the proportion of words with that tag.", "labels": [], "entities": []}, {"text": "We averaged the scores across all POS categories (Metric 11).", "labels": [], "entities": []}, {"text": "We also looked at style in terms of the Jaccard similarity between the POS trigrams in the context and suggestion (Metric 12).", "labels": [], "entities": []}, {"text": "Sentiment Similarity: The relation between the sentiment of a story and a candidate ending in the Story Cloze Test can be used to predict its correctness ().", "labels": [], "entities": [{"text": "Sentiment Similarity", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8762108981609344}]}, {"text": "We applied sentiment analysis to the context and suggestion using the tool 9 described in, which provides a valence score for 11 emotions.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8994542360305786}]}, {"text": "For each emotion, we computed the inverse distance 1 (1+|ec\u2212es|) between the context and suggestion scores e c and e s , respectively.", "labels": [], "entities": [{"text": "inverse distance 1", "start_pos": 34, "end_pos": 52, "type": "METRIC", "confidence": 0.9654612938563029}]}, {"text": "We averaged these values across all emotions to get one overall sentiment similarity score (Metric 13).", "labels": [], "entities": [{"text": "sentiment similarity score", "start_pos": 64, "end_pos": 90, "type": "METRIC", "confidence": 0.7602377931276957}]}, {"text": "Entity Coreference: Events in stories are linked by common entities (e.g. characters, locations, and objects), so coreference between entity mentions is particularly important for establishing the coherence of a story.", "labels": [], "entities": []}, {"text": "We calculated the proportion of entities in each suggestion that coreferred to an entity in the corresponding context shows the Spearman correlation coefficient (\u03c1) between the suggestion scores for each metric and their Levenshtein similarity to the resulting modifications.", "labels": [], "entities": [{"text": "Spearman correlation coefficient (\u03c1)", "start_pos": 128, "end_pos": 164, "type": "METRIC", "confidence": 0.8848162591457367}]}, {"text": "This coefficient indicates the degree to which the corresponding feature predicted authors' modifications, where higher coefficients mean that authors applied fewer edits.", "labels": [], "entities": []}, {"text": "Statistically significant correlations (p < 0.005) are highlighted in gray, indicating that suggestions with higher scores on these metrics were particularly helpful to authors.", "labels": [], "entities": []}, {"text": "Suggestion length did not have a significant impact, but grammaticality emerged as a helpful feature.", "labels": [], "entities": [{"text": "Suggestion length", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.6551109552383423}]}, {"text": "The frequency scores of the words in the suggestions did not significantly influence their helpfulness.", "labels": [], "entities": []}, {"text": "In terms of syntactic complexity, suggestions with more noun phrases were edited less often, but verb complexity was not influential.", "labels": [], "entities": []}, {"text": "For lexical cohesion, the number of overlapping words between the suggestion and its context (Jaccard similarity) was not predictive, but vector-based similarity was an indicator of helpfulness.", "labels": [], "entities": [{"text": "Jaccard similarity)", "start_pos": 94, "end_pos": 113, "type": "METRIC", "confidence": 0.85447758436203}]}, {"text": "Similarity in terms of sentence (skip-thought) vectors was the most helpful feature overall, which suggests these representations are indeed useful for modeling coherence between neighboring sentences in a story.", "labels": [], "entities": []}, {"text": "Analogously, and found that these representations were very effective for encoding story sentences in the Story Cloze Test in order to predict correct endings.", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 106, "end_pos": 122, "type": "DATASET", "confidence": 0.8683488170305887}]}, {"text": "Neither metric for style similarity predicted authors' edits, but sentiment similarity between the suggestion and context was significantly helpful.", "labels": [], "entities": []}, {"text": "Finally, suggestions that more frequently coreferred to entities introduced in the context were more helpful.", "labels": [], "entities": []}, {"text": "These results describe this particular sample of Creative Help interactions fora selected set of features relevant to story generation, but this analysis can be scaled to determine the influence of any feature in an automated writing support framework where authors can adapt generated content.", "labels": [], "entities": [{"text": "story generation", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.7574293911457062}]}, {"text": "The objective of this approach is to leverage data from user interactions with the system to establish an automated feedback loop for evaluation, by which features that emerge as helpful can be promoted in future systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Correlation \u03c1 between metric scores for sug- gestions and similarity to modifications", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.941372275352478}]}]}