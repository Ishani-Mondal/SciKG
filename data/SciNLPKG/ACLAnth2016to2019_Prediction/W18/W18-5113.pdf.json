{"title": [{"text": "Comparative Studies of Detecting Abusive Language on Twitter", "labels": [], "entities": [{"text": "Detecting Abusive Language", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.9264993270238241}]}], "abstractContent": [{"text": "The context-dependent nature of online aggression makes annotating large collections of data extremely difficult.", "labels": [], "entities": []}, {"text": "Previously studied datasets in abusive language detection have been insufficient in size to efficiently train deep learning models.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6882426738739014}]}, {"text": "Recently, Hate and Abusive Speech on Twitter, a dataset much greater in size and reliability, has been released.", "labels": [], "entities": [{"text": "Hate and Abusive Speech", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7553462535142899}, {"text": "reliability", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.9855189323425293}]}, {"text": "However, this dataset has not been comprehensively studied to its potential.", "labels": [], "entities": []}, {"text": "In this paper, we conduct the first comparative study of various learning models on Hate and Abusive Speech on Twitter, and discuss the possibility of using additional features and context data for improvements.", "labels": [], "entities": [{"text": "Hate and Abusive Speech", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.7151580601930618}]}, {"text": "Experimental results show that bidirectional GRU networks trained on word-level features, with Latent Topic Clustering modules, is the most accurate model scoring 0.805 F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 169, "end_pos": 171, "type": "METRIC", "confidence": 0.9985495209693909}]}], "introductionContent": [{"text": "Abusive language refers to any type of insult, vulgarity, or profanity that debases the target; it also can be anything that causes aggravation.", "labels": [], "entities": []}, {"text": "Abusive language is often reframed as, but not limited to, offensive language (, cyberbullying (, othering language, and hate speech (.", "labels": [], "entities": []}, {"text": "Recently, an increasing number of users have been subjected to harassment, or have witnessed offensive behaviors online.", "labels": [], "entities": []}, {"text": "Major social media companies (i.e. Facebook, Twitter) have utilized multiple resources-artificial intelligence, human reviewers, user reporting processes, etc.-in effort to censor offensive language, yet it seems nearly impossible to successfully resolve the issue.", "labels": [], "entities": []}, {"text": "The major reason of the failure in abusive language detection comes from its subjectivity and context-dependent characteristics (.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.637355109055837}]}, {"text": "For instance, a message can be regarded as harmless on its own, but when taking previous threads into account it maybe seen as abusive, and vice versa.", "labels": [], "entities": []}, {"text": "This aspect makes detecting abusive language extremely laborious even for human annotators; therefore it is difficult to build a large and reliable dataset (.", "labels": [], "entities": []}, {"text": "Previously, datasets openly available in abusive language detection research on Twitter ranged from 10K to 35K in size (.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.6869975527127584}]}, {"text": "This quantity is not sufficient to train the significant number of parameters in deep learning models.", "labels": [], "entities": []}, {"text": "Due to this reason, these datasets have been mainly studied by traditional machine learning methods.", "labels": [], "entities": []}, {"text": "Most recently, introduced Hate and Abusive Speech on Twitter, a dataset containing 100K tweets with cross-validated labels.", "labels": [], "entities": [{"text": "Hate and Abusive Speech", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7327074706554413}]}, {"text": "Although this corpus has great potential in training deep models with its significant size, there are no baseline reports to date.", "labels": [], "entities": []}, {"text": "This paper investigates the efficacy of different learning models in detecting abusive language.", "labels": [], "entities": [{"text": "detecting abusive language", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.8357145587603251}]}, {"text": "We compare accuracy using the most frequently studied machine learning classifiers as well as recent neural network models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9993085861206055}]}, {"text": "Reliable baseline results are presented with the first comparative study on this dataset.", "labels": [], "entities": []}, {"text": "Additionally, we demonstrate the effect of different features and variants, and describe the possibility for further improvements with the use of ensemble models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Hate and Abusive Speech on Twitter (Founta et al., 2018) classifies tweets into 4 labels, \"normal\", \"spam\", \"hateful\" and \"abusive\".", "labels": [], "entities": [{"text": "Hate and Abusive Speech", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6246882528066635}]}, {"text": "We were only able to crawl 70,904 tweets out of 99,996 tweet IDs, mainly because the tweet was deleted or the user account had been suspended.", "labels": [], "entities": []}, {"text": "shows the distribution of labels of the crawled data.", "labels": [], "entities": []}, {"text": "In training the feature engineering based machine learning classifiers, we truncate vector representations according to the TF-IDF values (the top 14,000 and 53,000 for word-level and characterlevel representations, respectively) to avoid overfitting.", "labels": [], "entities": []}, {"text": "For neural network models, words that appear only once are replaced as unknown tokens.: Experimental results of learning models and their variants, followed by the context tweet models.", "labels": [], "entities": []}, {"text": "The top 2 scores are marked as bold for each metric.", "labels": [], "entities": []}, {"text": "Since the dataset used is not split into train, development, and test sets, we perform 10-fold cross validation, obtaining the average of 5 tries; we divide the dataset randomly by a ratio of 85:5:10, respectively.", "labels": [], "entities": []}, {"text": "In order to evaluate the overall performance, we calculate the weighted average of precision, recall, and F1 scores of all four labels, \"normal\", \"spam\", \"hateful\", and \"abusive\".", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9975524544715881}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9969730377197266}, {"text": "F1", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.9997634291648865}]}], "tableCaptions": [{"text": " Table 1: Label distribution of crawled tweets", "labels": [], "entities": [{"text": "Label distribution of crawled tweets", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.7281422555446625}]}, {"text": " Table 2: Experimental results of learning models and their variants, followed by the context tweet models. The  top 2 scores are marked as bold for each metric.", "labels": [], "entities": []}]}