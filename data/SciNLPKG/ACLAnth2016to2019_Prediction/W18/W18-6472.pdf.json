{"title": [], "abstractContent": [{"text": "Our entry to the parallel corpus filtering task uses a two-step strategy.", "labels": [], "entities": [{"text": "parallel corpus filtering task", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.7185706868767738}]}, {"text": "The first step uses a series of pragmatic hard 'rules' to remove the worst example sentences.", "labels": [], "entities": []}, {"text": "This first step reduces the effective corpus size down from the initial 1 billion to 160 million tokens.", "labels": [], "entities": []}, {"text": "The second step uses four different heuristics weighted to produce a score that is then used for further filtering down to 100 or 10 million tokens.", "labels": [], "entities": []}, {"text": "Our final system produces competitive results without requiring excessive fine tuning to the exact task or language pair.", "labels": [], "entities": []}, {"text": "The first step in isolation provides a very fast filter that gives most of the gains of the final system.", "labels": [], "entities": []}], "introductionContent": [{"text": "This task asks for applicants to provide a score for each sentence pair in a 1-billion-word Machine Translation (MT) training corpus that is considered to be 'very noisy', such that those scores can be used to filter the corpus down into 10 million and 100 million words subsets.", "labels": [], "entities": [{"text": "Machine Translation (MT) training", "start_pos": 92, "end_pos": 125, "type": "TASK", "confidence": 0.8659816881020864}]}, {"text": "The quality of the output is measured by BLEU score obtained by training standard systems on these two subsets of data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9730177223682404}]}, {"text": "We consider this task to comprise of two primary components, namely (a) removing sentences that do not represent good examples of translation from one language to the other ('junk') and (b) distilling the remaining data down to a smaller training footprint without losing quality or diversity and then attaching scores to those sentences.", "labels": [], "entities": []}, {"text": "These two components are somewhat related; however, we chose to use a two-pass system to tackle them independently, so our system could be used to tackle the two components separately if required by a 'real-world' use case.", "labels": [], "entities": []}, {"text": "There are various approaches to this task that have previously been reported and we have attempted to select the most pragmatically useful of these to incorporate into our final system.", "labels": [], "entities": []}, {"text": "Our philosophy in choosing what to put into our system was to make it as general as possible, such that it could be used for other language pairs and different datasets, rather than specifically tuning for this task.", "labels": [], "entities": []}, {"text": "That then allows us to use the system more widely across our efforts in the field of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7971206605434418}]}, {"text": "We have also chosen to use an array of different metrics to produce a final score, rather than a single score, to gain the benefits of multiple models that approach the problem in different ways.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Percentages of the 1 billion word and dev  corpora removed by each of the initial filtering  rules.", "labels": [], "entities": []}]}