{"title": [{"text": "Neural Character-based Composition Models for Abuse Detection", "labels": [], "entities": [{"text": "Neural Character-based Composition", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6791011889775594}, {"text": "Abuse Detection", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.8191314935684204}]}], "abstractContent": [{"text": "The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc.", "labels": [], "entities": []}, {"text": "In light of this, there have been several efforts to automate the detection and moderation of such abusive content.", "labels": [], "entities": []}, {"text": "However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts.", "labels": [], "entities": []}, {"text": "The current state of the art approaches to abusive language detection , based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.6322435239950815}, {"text": "OOV", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.9187073111534119}]}, {"text": "However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words.", "labels": [], "entities": []}, {"text": "In this paper , we address this problem by designing a model that can compose embeddings for unseen words.", "labels": [], "entities": []}, {"text": "We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.", "labels": [], "entities": [{"text": "abuse detection", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.7494949102401733}, {"text": "Wikipedia talk page", "start_pos": 178, "end_pos": 197, "type": "DATASET", "confidence": 0.9337806502978007}]}], "introductionContent": [{"text": "Pew Research Center has recently uncovered several disturbing trends in communications on the Internet.", "labels": [], "entities": [{"text": "Pew Research Center", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.9229839046796163}]}, {"text": "As per their report), 40% of adult Internet users have personally experienced harassment online, and 60% have witnessed the use of offensive names and expletives.", "labels": [], "entities": []}, {"text": "Expectedly, the majority (66%) of those who have personally faced harassment have had their most recent incident occur on asocial networking website or app.", "labels": [], "entities": []}, {"text": "While most of these websites and apps provide ways of flagging offensive and hateful content, only 8.8% of the victims have actually considered using such provisions.", "labels": [], "entities": []}, {"text": "Two conclusions can be drawn from these statistics: (i) abuse (a term we use henceforth to collectively refer to toxic language, hate speech, etc.) is prevalent in social media, and (ii) passive and/or manual techniques for curbing its propagation (such as flagging) are neither effective nor easily scalable (.", "labels": [], "entities": []}, {"text": "Consequently, the efforts to automate the detection and moderation of such content have been gaining popularity ().", "labels": [], "entities": []}, {"text": "In their work, describe the task of achieving effective automation as an inherently difficult one due to several ingrained complexities; a prominent one they highlight is the deliberate structural obfuscation of words (for example, fcukk, w0m3n, banislam, etc.) by users to evade detection.", "labels": [], "entities": []}, {"text": "Simple spelling correction techniques and edit-distance procedures fail to provide information about such obfuscations because: (i) words maybe excessively fudged (e.g., a55h0le, n1gg3r) or concatenated (e.g., stupidbitch, feminismishate), and (ii) they fail to take into account the fact that some character sequences like musl and wom are more frequent and more indicative of abuse than others (.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.9156704843044281}]}, {"text": "goon to show that simple character n-gram features prove to be highly promising for supervised classification approaches to abuse detection due to their robustness to spelling variations; however, they do not address obfuscations explicitly. and also use character ngrams to attain impressive results on their respective datasets.", "labels": [], "entities": [{"text": "abuse detection", "start_pos": 124, "end_pos": 139, "type": "TASK", "confidence": 0.733414426445961}]}, {"text": "That said, the current state of the art methods do not exploit character-level information, but instead utilize recurrent neural network (RNN) models operating on word embeddings alone (.", "labels": [], "entities": []}, {"text": "Since the problem of deliberately noisy input is not explicitly accounted for, these approaches resort to the use of a generic OOV (out of vocabulary) embedding for words not seen in the training phase.", "labels": [], "entities": [{"text": "OOV", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.968045175075531}]}, {"text": "However, in using a single embedding for all unseen words, such approaches lose the ability to distinguish obfuscated words from non-obfuscated or rare ones.", "labels": [], "entities": []}, {"text": "Recently, and, working with the same Twitter dataset as we do, reported that many of the misclassifications by their RNN-based methods happen due to intentional misspellings and/or rare words.", "labels": [], "entities": []}, {"text": "Our contributions are two-fold: first, we experimentally demonstrate that character n-gram features are complementary to the current state of the art RNN approaches to abusive language detection and can strengthen their performance.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 168, "end_pos": 194, "type": "TASK", "confidence": 0.6482877731323242}]}, {"text": "We then explicitly address the problem of deliberately noisy input by constructing a model that operates at the character level and learns to predict embeddings for unseen words.", "labels": [], "entities": []}, {"text": "We show that the integration of this model with the character-enhanced RNN methods further advances the state of the art in abuse detection on three datasets from two different domains, namely Twitter and Wikipedia talk page.", "labels": [], "entities": [{"text": "abuse detection", "start_pos": 124, "end_pos": 139, "type": "TASK", "confidence": 0.7493971288204193}, {"text": "Wikipedia talk page", "start_pos": 205, "end_pos": 224, "type": "DATASET", "confidence": 0.9199589093526205}]}, {"text": "To the best of our knowledge, this is the first work to use character-based word composition models for abuse detection. were among the first ones to apply supervised learning to the task of abuse detection.", "labels": [], "entities": [{"text": "abuse detection.", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.787979245185852}, {"text": "abuse detection", "start_pos": 191, "end_pos": 206, "type": "TASK", "confidence": 0.7792943120002747}]}, {"text": "They worked with a linear support vector machine trained on local (e.g., n-grams), contextual (e.g., similarity of a post to its neighboring posts), and sentiment-based (e.g., presence of expletives) features to recognize posts involving harassment.", "labels": [], "entities": []}, {"text": "worked with comments taken from the Yahoo Finance portal and demonstrated that distributional representations of comments learned using the paragraph2vec framework () can outperform simpler bag-of-words BOW features under supervised classification settings for hate speech detection.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 261, "end_pos": 282, "type": "TASK", "confidence": 0.7544741829236349}]}, {"text": "improved upon the results of Djuric et al. by training their classifier on an amalgamation of features derived from four different categories: linguistic (e.g., count of insult words), syntactic (e.g. part-of-speech POS tags), distributional semantic (e.g., word and comment embeddings) and n-gram based (e.g., word bi-grams).", "labels": [], "entities": []}], "datasetContent": [{"text": "Following the proceedings of the 1 st Workshop on Abusive Language Online (, we use three datasets from two different domains.", "labels": [], "entities": [{"text": "Abusive Language Online", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.7770941456158956}]}, {"text": "We normalize the input by lowercasing all words and removing stop words.", "labels": [], "entities": []}, {"text": "For the GRU architecture, we use exactly the same hyper-parameters as, i.e., 128 hidden units, Glorot initialization, cross-entropy loss, and Adam optimizer (.", "labels": [], "entities": []}, {"text": "also use the same settings except they have fewer hidden units.", "labels": [], "entities": []}, {"text": "The LSTM in our character-based word composition model has 256 hidden units while that in our encoder has 64; the CNN has filters of widths varying from 1 to 4.", "labels": [], "entities": [{"text": "CNN", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.9148356914520264}]}, {"text": "The results we report are with an LSTM-based word composition model.", "labels": [], "entities": [{"text": "LSTM-based word composition", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.618446926275889}]}, {"text": "In all the models, besides dropout regularization (), we holdout a small part of the training set as validation data to prevent over-fitting.", "labels": [], "entities": [{"text": "dropout regularization", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.5499994605779648}]}, {"text": "We use 300d embeddings and 1 to 5 character ngrams for Wikipedia and 200d embeddings and 1 to 4 character n-grams for Twitter.", "labels": [], "entities": []}, {"text": "We implement the models in with Theano back-end.", "labels": [], "entities": []}, {"text": "We employ Lightgbm ( as our GDBT classifier and tune its hyper-parameters using 5-fold grid search.", "labels": [], "entities": [{"text": "Lightgbm", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.9083426594734192}]}], "tableCaptions": [{"text": " Table 1: Results on the Twitter dataset. The meth- ods we propose are denoted by  \u2020 . Our best method  (AUGMENTED WS + CNG) significantly outper- forms all other methods.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.8806679248809814}, {"text": "AUGMENTED WS + CNG", "start_pos": 105, "end_pos": 123, "type": "METRIC", "confidence": 0.8892717957496643}]}, {"text": " Table 3: Macro F 1 scores on the two Wikipedia  datasets. The current state of the art method for  these datasets is HS.  \u2020 denotes the methods we  propose. Our best method (CONTEXT HS + CNG)  outperforms all the other methods.", "labels": [], "entities": [{"text": "Wikipedia  datasets", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.9580130577087402}, {"text": "HS", "start_pos": 118, "end_pos": 120, "type": "METRIC", "confidence": 0.8872146010398865}]}, {"text": " Table 4: The current state of the art baseline (HS)  vs. our best methods (  \u2020 ) on the abusive classes of  W-TOX and W-ATT.", "labels": [], "entities": [{"text": "HS)", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9186198115348816}]}]}