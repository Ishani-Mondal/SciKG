{"title": [{"text": "Weighting Model Based on Group Dynamics to Measure Convergence in Multi-party Dialogue", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes anew weighting method for extending a dyad-level measure of convergence to multi-party dialogues by considering group dynamics instead of simply averaging.", "labels": [], "entities": []}, {"text": "Experiments indicate the usefulness of the proposed weighted measure and also show that in general a proper weighting of the dyad-level measures performs better than non-weighted averaging in multiple tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entrainment is the tendency of speakers to begin behaving like one another in conversation.", "labels": [], "entities": []}, {"text": "The development of methods for automatically quantifying entrainment in text and speech data is an active research area, as entrainment has been shown to correlate with outcomes such as success measures and social variables fora variety of phenomena, e.g., acoustic-prosodic, lexical, and syntactic (.", "labels": [], "entities": []}, {"text": "One of the main measures of entrainment is convergence which is the main focus of this paper.", "labels": [], "entities": [{"text": "convergence", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.9908700585365295}]}, {"text": "Within a conversation, convergence measures the amount of increase in similarity of speakers overtime in terms of linguistic features (.", "labels": [], "entities": [{"text": "convergence", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9727219343185425}]}, {"text": "While most research has focused on quantifying the amount of entrainment between speaker pairs (i.e., dyads), recent studies have started to develop measures for quantifying entrainment between larger groups of speakers (; Danescu-Niculescu-Mizil et al.,).", "labels": [], "entities": []}, {"text": "To date, mainly simple methods such as unweighted averaging have been used to move from dyads to groups.", "labels": [], "entities": []}, {"text": "However, because multi-party interactions are more complicated than dyad-level interactions, it is not clear that the contribution of all group members should be weighted equally.", "labels": [], "entities": []}, {"text": "For example, to account for participation differences, Friedberg et al. proposed a weighting method based on the number of uttered words of each dyad, although this did not yield performance improvements compared to simple averaging.) provided examples of group-specific behaviors that were not properly quantified using simple averaging.", "labels": [], "entities": []}, {"text": "While this case study nicely identified potential problems with prior measures, their observations were only based on a few example dialogues and no solutions were proposed.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew weighting method to normalize the contribution of speakers based on group dynamics.", "labels": [], "entities": []}, {"text": "We explore the effect of our method, participation weighting, and simple averaging when calculating group convergence from dyads.", "labels": [], "entities": []}, {"text": "We conclude that our proposed weighted convergence measure performs significantly better on multiple benchmark prediction and regression tasks that have been used to evaluate convergence in prior studies).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experimental evaluations use two tasks that have been used for convergence measure evaluations in previous studies.", "labels": [], "entities": []}, {"text": "Predicting Social Outcomes: Our first task examines how the NW, PW, and GDW measures of acoustic-prosodic convergence (independent variables) relate to the social outcome measures (dependent variables) from Section 3.", "labels": [], "entities": [{"text": "GDW", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.8990463614463806}]}, {"text": "This is similar to prior studies which have evaluated convergence in terms of predicting outcomes).", "labels": [], "entities": []}, {"text": "We hypothesize that the group-dynamic weighted convergence measure will outperform the nonweighted and participation-based measures.", "labels": [], "entities": []}, {"text": "First, we train a hierarchical multiple regression with each of the three groups of convergence measures, added once in the first level and the other time in the second, to measure if the second level predictors significantly improve the explanation of variance.", "labels": [], "entities": []}, {"text": "We only keep predictors with significant coefficients when presenting the models.", "labels": [], "entities": []}, {"text": "For Process Conflict, the results show that all NW, PW, and GDW predictor groups are as good as each other; no matter which group is entered in the first level, the predictors in the second level do not significantly improve model fit.", "labels": [], "entities": []}, {"text": "For Favorable, neither PW nor NW in the second level significantly improves performance.", "labels": [], "entities": [{"text": "Favorable", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.669144332408905}]}, {"text": "However, shows that adding the GDW measures at the second level significantly improves a model with only NW features at the first level.", "labels": [], "entities": [{"text": "GDW", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.5993063449859619}]}, {"text": "The amount of variance explained in Model 2 is significantly above and beyond Model 1, \u2206R 2 = 0.048, \u2206F (2, 119) = 3.179, p = 0.045.", "labels": [], "entities": [{"text": "variance", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.8015909790992737}, {"text": "F", "start_pos": 102, "end_pos": 103, "type": "METRIC", "confidence": 0.9911609888076782}]}, {"text": "The reverse order, GDW at first level and NW at the second level, shows that the improvement at the second level is not significant, \u2206R 2 = 0.031, \u2206F (2, 119) = 2.068, p = 0.131.", "labels": [], "entities": [{"text": "GDW", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9808262586593628}, {"text": "F", "start_pos": 148, "end_pos": 149, "type": "METRIC", "confidence": 0.9915245175361633}]}, {"text": "These results indicate that the proposed weighted (GDW) convergence (for intensity max and SD) are the best  predictors of the favorable social outcome compared with the other two measures of convergence.", "labels": [], "entities": [{"text": "GDW) convergence", "start_pos": 51, "end_pos": 67, "type": "METRIC", "confidence": 0.7832891941070557}, {"text": "intensity max", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9290577173233032}]}, {"text": "Next, we reduce the task from regression to a binary classification by splitting the two social outcome variables at the median.", "labels": [], "entities": []}, {"text": "We perform Leave-One-Out Cross-Validations (LOOCV) using a logistic regression (L2) algorithm and all eight acoustic-prosodic features to predict binary outcomes.", "labels": [], "entities": []}, {"text": "The results in show that the GWD model significantly 6 outperforms both PW and NW models to predict the favorable social outcome.", "labels": [], "entities": []}, {"text": "In the prediction of process conflict, the PW model outperforms both NW and GDW models and its improvement over GDW is trending.", "labels": [], "entities": []}, {"text": "In sum, the results in both tables support our hypothesis for the favorable social outcome, where the proposed GDW convergence measure is a better predictor of the outcome.", "labels": [], "entities": [{"text": "GDW convergence measure", "start_pos": 111, "end_pos": 134, "type": "METRIC", "confidence": 0.6327507396539053}]}, {"text": "For process conflict, we do not see any significant difference.", "labels": [], "entities": []}, {"text": "Predicting Real Dialogues: The existence of entrainment should not be incidental.", "labels": [], "entities": []}, {"text": "To evaluate this criteria, we use permuted versus real conversations as in).", "labels": [], "entities": []}, {"text": "We hypothesize that GDW will be the best convergence measure for distin-6 Corrected paired t-test was performed to address instance dependency from both games).", "labels": [], "entities": [{"text": "GDW", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.8528825044631958}]}, {"text": "guishing real versus permuted dialogues.", "labels": [], "entities": []}, {"text": "For each of the 124 game sessions, we construct artificially permuted versions of the real dialogues as follows.", "labels": [], "entities": []}, {"text": "For each speaker, we randomly permute the silence and speech intervals extracted by Praat.", "labels": [], "entities": []}, {"text": "Next, we measure convergence for all the groups with permuted audios.", "labels": [], "entities": [{"text": "convergence", "start_pos": 17, "end_pos": 28, "type": "METRIC", "confidence": 0.9918799996376038}]}, {"text": "We perform a leave-one-out cross-validation experiment to predict real conversations using the convergence measures.", "labels": [], "entities": []}, {"text": "We examined several classification algorithms including logistic regression; linear SVM was the only one that showed significant results.", "labels": [], "entities": []}, {"text": "The \"All\" results in show that none of the models significantly outperform the majority baseline.", "labels": [], "entities": [{"text": "All", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.9757158756256104}]}, {"text": "To diagnose the issue, we perform the prediction on each game separately.", "labels": [], "entities": []}, {"text": "The proposed GDW model significantly outperforms other models for Game 1.", "labels": [], "entities": []}, {"text": "However, for Game 2, none of the results are significantly different.", "labels": [], "entities": []}, {"text": "One reason might be that convergence occurs quickly during Game 1, and there is not much convergence occurring at Game 2.", "labels": [], "entities": []}, {"text": "Thus, there is no significant difference between permuted and not permuted convergence for any of the features during Game 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hierarchical regression results with inten- sity max and SD convergence as independent, and  Favorable as dependent, variables. The NW mea- sures are added in the first level and GDW mea- sures in the second level. Significant / trending  results if p-value is < 0.05 (*) or < 0.1 (+).", "labels": [], "entities": [{"text": "NW mea- sures", "start_pos": 142, "end_pos": 155, "type": "METRIC", "confidence": 0.8934636861085892}, {"text": "GDW mea- sures", "start_pos": 189, "end_pos": 203, "type": "METRIC", "confidence": 0.6298575177788734}]}, {"text": " Table 2: LOOCV prediction accuracies of bi- nary favorable social outcome and process conflict  variables. (**) indicates GWD model significantly  outperforms both PW and NW models. (+) indi- cates PW improvement over GDW is trending.", "labels": [], "entities": [{"text": "LOOCV", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9079722762107849}]}, {"text": " Table 3: Accuracies using the linear SVM models  and LOOCV to predict real conversations. (+) in- dicates GWD outperforms NW with p = 0.06 , (*)  indicates GWD outperforms PW with p = 0.004.", "labels": [], "entities": [{"text": "LOOCV", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9066848158836365}]}]}