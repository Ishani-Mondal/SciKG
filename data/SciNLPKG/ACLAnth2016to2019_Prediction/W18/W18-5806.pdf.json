{"title": [{"text": "Complementary Strategies for Low Resourced Morphological Modeling", "labels": [], "entities": [{"text": "Low Resourced Morphological Modeling", "start_pos": 29, "end_pos": 65, "type": "TASK", "confidence": 0.5977435857057571}]}], "abstractContent": [{"text": "Morphologically rich languages are challenging for natural language processing tasks due to data sparsity.", "labels": [], "entities": []}, {"text": "This can be addressed either by introducing out-of-context morphological knowledge, or by developing machine learning architectures that specifically target data spar-sity and/or morphological information.", "labels": [], "entities": []}, {"text": "We find these approaches to complement each other in a morphological paradigm modeling task in Modern Standard Arabic, which, in addition to being morphologically complex, features ubiquitous ambiguity, exacerbating spar-sity with noise.", "labels": [], "entities": []}, {"text": "Given a small number of out-of-context rules describing closed class morphology , we combine them with word embed-dings leveraging subword strings and noise reduction techniques.", "labels": [], "entities": []}, {"text": "The combination outper-forms both approaches individually by about 20% absolute.", "labels": [], "entities": []}, {"text": "While morphological resources already exist for Modern Standard Arabic, our results inform how comparable resources might be constructed for non-standard dialects or any morphologically rich, low resourced language, given scarcity of time and funding.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphologically rich languages pose many challenges for natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.7263036668300629}]}, {"text": "This often takes the shape of data sparsity, as the increase in the number of possible inflections for any given core concept leads to a lower average word frequency of individual (i.e., unique) word types.", "labels": [], "entities": []}, {"text": "Hence, models have fewer chances to learn about types based on their in-context behavior.", "labels": [], "entities": []}, {"text": "One common, albeit time consuming response to this challenge is to introduce outof-context morphological knowledge, hand crafting rules to relate forms inflected from the same lemma.", "labels": [], "entities": []}, {"text": "The other common response is to adopt machine learning architectures specifically targeting data sparsity and/or morphological information.", "labels": [], "entities": []}, {"text": "We find these two responses to be complementary in a paradigm modeling task for Modern Standard Arabic (MSA).", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 80, "end_pos": 108, "type": "DATASET", "confidence": 0.889399399360021}]}, {"text": "MSA is characterized by morphological richness and extreme orthographic ambiguity, compounding the issue of data sparsity with noise.", "labels": [], "entities": []}, {"text": "Despite its challenges, MSA is relatively well resourced, with many solutions for morphological analysis and disambiguation leveraging large amounts of annotated data, hand crafted rules, and/or sophisticated neural architectures (.", "labels": [], "entities": []}, {"text": "Such resources and techniques, however, are not available or not viable for the many under resourced and often mutually unintelligible dialects of Arabic (DA), which are similarly morphologically rich and highly ambiguous (;).", "labels": [], "entities": []}, {"text": "Many recent efforts seek to develop morphological resources for DA, but most are underdeveloped or specific to one dialect (.", "labels": [], "entities": [{"text": "DA", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.950150191783905}]}, {"text": "This work does not aim to develop a full morphological analysis and disambiguation resource, but to inform how one might be most efficiently developed for any DA variety or similarly low resourced language, given scarcity of time and funding.", "labels": [], "entities": []}, {"text": "For such a resource to be practical and easily extendable to new DA varieties, it must take as input the natural, highly ambiguous orthography.", "labels": [], "entities": []}, {"text": "Thus, we do not rely on constructed phonological representations to clarify ambiguities, as is common practice when modeling morphology for its own sake (.", "labels": [], "entities": []}, {"text": "To in-form how such a resource should be developed, we evaluate minimally rule based and unsupervised techniques for clustering words that belong to the same paradigm in MSA.", "labels": [], "entities": []}, {"text": "We primarily use pre-existing MSA resources only for evaluation, constraining resource availability to emulate DA settings during training, as we lack the resources to evaluate our techniques in DA.", "labels": [], "entities": [{"text": "DA", "start_pos": 195, "end_pos": 197, "type": "TASK", "confidence": 0.9115553498268127}]}, {"text": "Our best system combines a minimal set of rules describing closed class morphology with word embeddings that leverage subword strings and noise reduction strategies.", "labels": [], "entities": []}, {"text": "The former, despite being cheaper and easier to produce than other rule-based systems, provides valuable out-of-context morphological knowledge, which the latter complements by modeling the in-context behavior of words and morphemes.", "labels": [], "entities": []}, {"text": "Combining the techniques outperforms either individually by about 20% absolute.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the data, design, and models used in our experiments.", "labels": [], "entities": []}, {"text": "The linguistic concept of derivational family differs from ours in that it does not require any ambiguous forms to be shared by derivationally related paradigms.", "labels": [], "entities": []}, {"text": "However, identifying such derivational families automatically is non-trivial.", "labels": [], "entities": []}, {"text": "Even if the shared root can be identified, it can be difficult to determine whether the root is mono or polysemous, e.g., \u0161 \u03c2 r could refer to hair, poetry, or feeling.", "labels": [], "entities": []}, {"text": "Regardless, our definition of family better serves our investigation into the effects of ambiguity.", "labels": [], "entities": []}, {"text": "We build single and multi prototype representations of the entire vocabulary, then examine how well they reflect the paradigms in EVAL.", "labels": [], "entities": []}, {"text": "Each representation can bethought of as a tree where each word is a leaf at depth 0, i.e., W 1 , W 2 , and W 3 in.", "labels": [], "entities": []}, {"text": "Descending down the tree, words are clustered with other words' branches at subsequent depths until the clustering algorithm finishes or the root is reached where all words in the vocabulary are clustered together.", "labels": [], "entities": []}, {"text": "All trees use some model of word similarity to guide clustering.", "labels": [], "entities": []}, {"text": "In multi prototype representations, a word's leaf prototype at depth 0 can be copied and grafted onto other words' branches at non-zero depths before those branches are clustered to its own.", "labels": [], "entities": []}, {"text": "Such is the case of W 2 , which is copied as W 2 at depth 1 of W 3 's branch before W 3 's branch connects to W 2 's.", "labels": [], "entities": []}, {"text": "This enables partially overlapping paradigms to be modeled, like those in.", "labels": [], "entities": []}, {"text": "We evaluate the trees via average maximum Fscore.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9930791258811951}]}, {"text": "For each word in EVAL, we descend from its leaf, at each depth calculating an F-score for the overlap between the words that have been clustered to the leaf's branch so far and the leaf word's known paradigm mates, i.e., the set of words sharing at least one lemma with the leaf.", "labels": [], "entities": [{"text": "F-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9989343285560608}]}, {"text": "Thus, paradigms are soft clusters in our representation, in that, for each word in a paradigm, its set of proposed paradigm mates need not be consistent with any of its proposed paradigm mates' sets of proposed paradigm mates.", "labels": [], "entities": []}, {"text": "We then take the best F-score for each leaf word in EVAL, regardless of the depth level at which it was achieved, and average these maximum F-scores.", "labels": [], "entities": [{"text": "F-score", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9974325299263}, {"text": "EVAL", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.7939884662628174}, {"text": "F-scores", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9845833778381348}]}, {"text": "This reflects how cohesively paradigms are represented in the tree.", "labels": [], "entities": []}, {"text": "Additionally, we report the average depth at which templatic and concatenatively related paradigm mates are added.", "labels": [], "entities": []}, {"text": "Because we evaluate via average maximum Fscore, this metric represents the potential performance of any given model.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9917254447937012}]}, {"text": "Future work will address predicting the depth level where average maximum F-score is achieved fora given leaf word via rule-based and/or empirical techniques that have proven successful for related tasks).", "labels": [], "entities": [{"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9739752411842346}]}], "tableCaptions": [{"text": " Table 1: Statistics from the EVAL set. Morpholog- ical structures by level of abstraction. Ambiguous  structures contain at least one lemma ambiguous  form. Non-derivationally ambiguous structures  contain at least one coincidentally lemma ambigu- ous form.", "labels": [], "entities": [{"text": "EVAL set", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.918285071849823}]}, {"text": " Table 2: Scores for clustering words with their paradigm mates in tree representations built from different  models of word similarity. Scores are calculated as described in Section 3.2, with precision and recall  extracted from the depth that maximizes F and then averaged over all words in EVAL. Join depths refer to  the average depth at which templatic or concatenatively related paradigm mates are added to the branch.", "labels": [], "entities": [{"text": "precision", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9992803931236267}, {"text": "recall", "start_pos": 207, "end_pos": 213, "type": "METRIC", "confidence": 0.9979178309440613}, {"text": "F", "start_pos": 255, "end_pos": 256, "type": "METRIC", "confidence": 0.9840391278266907}, {"text": "EVAL", "start_pos": 293, "end_pos": 297, "type": "DATASET", "confidence": 0.8738945126533508}]}]}