{"title": [{"text": "USI-IR at IEST 2018: Sequence Modeling and Pseudo-Relevance Feedback for Implicit Emotion Detection", "labels": [], "entities": [{"text": "USI-IR at IEST 2018", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.7981878072023392}, {"text": "Sequence Modeling", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.9536460638046265}, {"text": "Implicit Emotion Detection", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.7373618880907694}]}], "abstractContent": [{"text": "This paper describes the participation of USI-IR in WASSA 2018 Implicit Emotion Shared Task.", "labels": [], "entities": [{"text": "USI-IR", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.8192216157913208}, {"text": "WASSA 2018 Implicit Emotion Shared Task", "start_pos": 52, "end_pos": 91, "type": "TASK", "confidence": 0.7851921021938324}]}, {"text": "We propose a relevance feedback approach employing a sequential model (biLSTM) and word embeddings derived from a large collection of tweets.", "labels": [], "entities": []}, {"text": "To this end, we assume that the top-k predictions produce at a first classification step are correct (based on the model accuracy) and use them as new examples to retrain the network.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen the rapid growth of social media platforms (e.g., Facebook, Twitter, several blogs) that has changed the way that people communicate.", "labels": [], "entities": []}, {"text": "Many people express their opinion and emotions on blogs, forums or microblogs.", "labels": [], "entities": []}, {"text": "Detecting the emotions that are expressed in social media is a very important problem fora wide variety of applications.", "labels": [], "entities": [{"text": "Detecting the emotions that are expressed in social media", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.7932171093093024}]}, {"text": "For example, enterprises can detect complains of customers about their products or services and act promptly.", "labels": [], "entities": []}, {"text": "Emotion detection aims at identifying various emotions from text.", "labels": [], "entities": [{"text": "Emotion detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9248922765254974}]}, {"text": "According to there are eight basic emotions: anger, joy, sadness, fear, trust, surprise, disgust and anticipation.", "labels": [], "entities": [{"text": "surprise", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9485023617744446}, {"text": "anticipation", "start_pos": 101, "end_pos": 113, "type": "METRIC", "confidence": 0.9647214412689209}]}, {"text": "Considering the abundance of opinions and emotions expressed in microblogs, emotion and sentiment analysis in Twitter has attracted the interest of the research community (.", "labels": [], "entities": [{"text": "emotion and sentiment analysis", "start_pos": 76, "end_pos": 106, "type": "TASK", "confidence": 0.643892228603363}]}, {"text": "In particular, Implicit Emotion Shared Task (IEST) is a shared task by WASSA 2018 that focuses on emotion analysis.", "labels": [], "entities": [{"text": "Implicit Emotion Shared Task (IEST)", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.7254327833652496}, {"text": "emotion analysis", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.7284712940454483}]}, {"text": "In this task, participants are asked to develop tools that can predict the emotions in tweets from which a certain emotion word is removed.", "labels": [], "entities": []}, {"text": "This is a very challenging problem since the emotion analysis needs to be done without access to an explicit mention of an emotion word and consequently taking advantage of context that surrounds the target word.", "labels": [], "entities": []}, {"text": "In this paper, we describe our submitted system to the IEST: WASSA-2018 Implicit Emotion Shared Task.", "labels": [], "entities": [{"text": "IEST: WASSA-2018 Implicit Emotion Shared Task", "start_pos": 55, "end_pos": 100, "type": "TASK", "confidence": 0.5974904213632856}]}, {"text": "Our system is based on a bidirectional Long Short-Term Memory (biLSTM) network on top of word embeddings which is later inserted in a pseudo-relevance feedback schema.", "labels": [], "entities": []}, {"text": "Our results show that even though the model still need more refinement it offers interesting capabilities to address the task at hand.", "labels": [], "entities": []}], "datasetContent": [{"text": "To train our model, we employ the dataset provided within the shared task.", "labels": [], "entities": []}, {"text": "It is worth mentioning that no other external datasets are used during the training and development phases.", "labels": [], "entities": []}, {"text": "There are roughly 153K tweets in the training set, 10K in the development set and 28K in the test set.", "labels": [], "entities": []}, {"text": "Each data instance includes the tweet and the emotion class of the word which has been extracted from the text.", "labels": [], "entities": []}, {"text": "The test set's golden labels were provided only after the evaluation period.", "labels": [], "entities": []}, {"text": "The complete description of the dataset can be found in (  Preprocessing and tokenization are crucial steps of the pipeline involved in the development of a model: the output produced has an immediate effect in the features learned by the model.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.9363575577735901}, {"text": "tokenization", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.9639382362365723}]}, {"text": "This task could turn to be particularly challenging in Twitter since the vocabulary results quite unstable overtime and the way that users expressed does not follow traditional patterns.", "labels": [], "entities": []}, {"text": "In order to preprocess and tokenize the collection of tweets we employ a python library 1 developed for that purpose which applies different regular expressions to extract particular units, such as hashtags, and separates them from the rest of the tokens.", "labels": [], "entities": []}, {"text": "We only conserve words, hashtags, mentions, emojis and smileys.", "labels": [], "entities": []}, {"text": "The remaining tokens outside these categories are discarded given that their inclusion did not prove to be useful for the task.", "labels": [], "entities": []}, {"text": "Some examples of such tokens are URLs (they were originally replaced with http://url.removed), numbers and the unit.", "labels": [], "entities": []}, {"text": "Furthermore, we remove the hash symbols from the hashtags and split the words when possible using the Viterbi algorithm.", "labels": [], "entities": []}, {"text": "The prior probabilities are obtained from word statistics from Google Ngram corpus.", "labels": [], "entities": [{"text": "Google Ngram corpus", "start_pos": 63, "end_pos": 82, "type": "DATASET", "confidence": 0.9134888251622518}]}, {"text": "In par-ticular, we observed a positive impact on the training accuracy of the network.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9915705919265747}]}, {"text": "One possible reason could be that the terms contained in the hashtags were probably present in the word embeddings but not as a singular unit (e.g., #classyCouple).", "labels": [], "entities": []}, {"text": "All the tokens are transformed to lowercase and words which were completely in capitals, emulating a yell in the social media language, were doubled.", "labels": [], "entities": []}, {"text": "It is important to remark that stopwords are not removed.", "labels": [], "entities": []}, {"text": "The model is comprised of an embedding layer, a biLSTM layer and a softmax layer.", "labels": [], "entities": []}, {"text": "It receives as input a tokenized twitter message treated as a sequence of words.", "labels": [], "entities": []}, {"text": "Since the length of different tweets can vary, we set the length of each message to 99 (the maximum message length across training and development data according to the operations performed in the preprocessing step).", "labels": [], "entities": []}, {"text": "Tweets that are shorter than this length are zero-padded.", "labels": [], "entities": []}, {"text": "It should be noted that the network will ignore everything that goes beyond the last word in the text, i.e., the padding.", "labels": [], "entities": []}, {"text": "The weights of the embedding layer are initialized using word2vec) embeddings trained on 400 million tweets () from the ACL W-NUT share task (.", "labels": [], "entities": []}, {"text": "We also tried to use the 300-dimensional pre-trained vector trained on Google News dataset 2 combined with emojis pre-trained embeddings.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.9233690102895101}]}, {"text": "However, the performance was slightly worse and for that reason we decided not to employ them.", "labels": [], "entities": []}, {"text": "Words out of the embeddings are conserved, albeit their weights are randomly initialize and learn from scratch.", "labels": [], "entities": []}, {"text": "A single biLSTM layer with a hidden layer size of 128 neurons follows in the architecture and feeds a softmax layer in order to obtain the final prediction.", "labels": [], "entities": []}, {"text": "The network parameters are learned by minimizing Cross-Entropy and by backpropagating the error through layers over 5 epochs, with a batch size of 128, using RMSprop optimization algorithm.", "labels": [], "entities": []}, {"text": "Moreover, a dropout rate () of 0.5 is used to address overfitting issues.", "labels": [], "entities": []}, {"text": "The aforementioned model was implemented in Python using Tensorflow library (.", "labels": [], "entities": []}, {"text": "Lastly, as introduced in Section 3, we propose a pseudo-relevance feedback scheme as follows: (a) A first instance of the network is trained using the training and development sets; (b) Sub- sequently, the k percentage of the tweets with the highest class probability is extracted with the corresponding labels to create anew set examples; (c) Finally, the training and development sets along with the new examples are used to re-train the model from scratch and obtain the final predictions.", "labels": [], "entities": []}, {"text": "It should be noted that the same hyperparameters are employed at both training and retraining steps.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: LIWC selected categories for the six emotions. The values represent percentages over total words.", "labels": [], "entities": []}, {"text": " Table 2: Pseudo-relevance feedback schema results on  the test set", "labels": [], "entities": []}]}