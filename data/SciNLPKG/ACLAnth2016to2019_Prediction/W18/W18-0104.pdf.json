{"title": [{"text": "Phonological (un)certainty weights lexical activation", "labels": [], "entities": []}], "abstractContent": [{"text": "Spoken word recognition involves at least two basic computations.", "labels": [], "entities": [{"text": "Spoken word recognition", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8664368192354838}]}, {"text": "First is matching acoustic input to phonological categories (e.g. /b/, /p/, /d/).", "labels": [], "entities": []}, {"text": "Second is activating words consistent with those phono-logical categories.", "labels": [], "entities": []}, {"text": "Here we test the hypothesis that the listener's probability distribution over lexical items is weighted by the outcome of both computations: uncertainty about phonological discreti-sation and the frequency of the selected word(s).", "labels": [], "entities": []}, {"text": "To test this, we record neural responses in auditory cortex using magneto-encephalography, and model this activity as a function of the size and relative activation of lexical candidates.", "labels": [], "entities": []}, {"text": "Our findings indicate that towards the beginning of a word, the processing system indeed weights lexical candidates by both phono-logical certainty and lexical frequency; however, later into the word, activation is weighted by frequency alone.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is mounting evidence for the predictive nature of language comprehension.", "labels": [], "entities": []}, {"text": "Response times and neural activity are reduced in response to more predictable linguistic input.", "labels": [], "entities": []}, {"text": "This indicates that the brain forms probabilistic hypotheses about current and future linguistic content, which manifest in expectations of phonemes, morphemes, words and syntactic structures;.", "labels": [], "entities": []}, {"text": "In speech comprehension, the brain's task is to correctly determine a word's identity as quickly as possible.", "labels": [], "entities": []}, {"text": "It is not optimal to always wait until word ending, because the target maybe correctly identifiable earlier.", "labels": [], "entities": []}, {"text": "For example, after hearing hippopotamu-the final /s/ provides very little additional information.", "labels": [], "entities": []}, {"text": "Indeed, one could even stop at hippot-and still identify the target word correctly most of the time.", "labels": [], "entities": []}, {"text": "Upon hearing the beginning of a lexical item, the brain activates the cohort of words that are consistent with the acoustic signal.", "labels": [], "entities": []}, {"text": "Words in the cohort are activated relative to their match to the phoneme sequence and frequency of occurrence.", "labels": [], "entities": []}, {"text": "With each subsequent phoneme, the cohort is reduced as items cease to be consistent with the provided input, until one item prevails (see).", "labels": [], "entities": []}, {"text": "This process is consistent with the highly influential cohort model of spoken word recognition, and has been associated with activity in left superior temporal gyrus (STG) (.", "labels": [], "entities": [{"text": "spoken word recognition", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.6785138646761576}]}, {"text": "In practice though, phoneme identity is often uncertain: the acoustic signal maybe consistent with both a [b] and a, for example.", "labels": [], "entities": []}, {"text": "This phonetic uncertainty, and its effect on lexical activation, is not addressed by the cohort model.", "labels": [], "entities": []}, {"text": "However, there is evidence suggesting that phonetic uncertainty affects lexical and sentential processing (.", "labels": [], "entities": []}, {"text": "Here we build upon this previous work in order to understand the neural computations underlying lexical activation, in service to spoken word recognition.", "labels": [], "entities": [{"text": "spoken word recognition", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.710164209206899}]}, {"text": "Concretely, we ask: How does finegrained acoustic information (below the phonological level) serve to activate lexical hypotheses and estimate their probabilities?", "labels": [], "entities": []}, {"text": "Can this integration between phonological and lexical levels of description be readout from the STG?: Schematic depiction of cohort activation under each of the two models, for the first five phonemes of the word palate.", "labels": [], "entities": []}, {"text": "The onset b-p symbol represents that the onset phoneme was 75% consistent with a /b/ and 25% consistent with a /p/.", "labels": [], "entities": []}, {"text": "Transparency reflects relative word activation.", "labels": [], "entities": []}, {"text": "Note that the change in transparency between the two accounts reflects the actual probabilities predicted by each model -because there are more words activated in the Acoustic-Weighted account, less normalised probability is assigned to each item.", "labels": [], "entities": []}, {"text": "To address these questions, we model neural responses in STG, time-locked to each phoneme in a word, as a function of two computational models.", "labels": [], "entities": []}, {"text": "One model assumes that the activation of a lexical candidate is gradiently weighted by the acoustic evidence in favour of that candidate: e.g., balloon is activated in proportion to how /b/-like the initial sound of the word was, even if that sound was more likely to represent a different phoneme (e.g., /p/).", "labels": [], "entities": []}, {"text": "We refer to this model, in which phonetic uncertainty is carried over to the word recognition process, as the acoustic-weighted model.", "labels": [], "entities": [{"text": "word recognition process", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.8409506678581238}]}, {"text": "The other model assumes that acoustic information serves as a switch: a lexical item is either fully activated or not activated at all, as a result of a discrete decision made at the phonetic level.", "labels": [], "entities": []}, {"text": "This model, which we refer to as the switch-based model, is most consistent with the traditional cohort model -the system commits to whichever phoneme is more likely, and this is used to form predictions at the lexical level (see).", "labels": [], "entities": []}, {"text": "A subset of the data reported here are also published in.", "labels": [], "entities": []}], "datasetContent": [{"text": "Native English participants (n = 25) listened to each of the 103 \u00d7 5 words in isolation, and in 20% of trials (randomly distributed) made an auditoryto-visual word matching judgment.", "labels": [], "entities": [{"text": "auditoryto-visual word matching", "start_pos": 141, "end_pos": 172, "type": "TASK", "confidence": 0.6144758959611257}]}, {"text": "While completing the task, neural responses were recorded using a 208-sensor KIT magnetoencephalography (MEG) system.", "labels": [], "entities": []}, {"text": "Data were sam-  pled at 1000 Hz, which provided a measure of neural activity at each millisecond.", "labels": [], "entities": []}, {"text": "In order to test responses to specific phonemes in a word, the data were cut into a series of 700 ms epochs, where the time at 0 ms corresponds to the onset of a phoneme.", "labels": [], "entities": []}, {"text": "Note that the phonemes were shorter than 700 ms, so the epochs overlapped in time.", "labels": [], "entities": []}, {"text": "The activity recorded from MEG sensors was localised using MNE-Python software (, and averaged over the left STG.", "labels": [], "entities": [{"text": "MNE-Python", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.8503881692886353}, {"text": "STG", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.8968409299850464}]}, {"text": "This provided one datapoint per millisecond (700) per phoneme (4370) per participant (25).", "labels": [], "entities": []}], "tableCaptions": []}