{"title": [{"text": "Two-Step Training and Mixed Encoding-Decoding for Implementing a Generative Chatbot with a Small Dialogue Corpus", "labels": [], "entities": []}], "abstractContent": [{"text": "Generative chatbot models based on sequence to sequence networks can generate natural conversation interactions if a huge dialogue corpus is used as training data.", "labels": [], "entities": []}, {"text": "However, except fora few languages such as English and Chinese, it remains difficult to collect a large dialogue corpus.", "labels": [], "entities": []}, {"text": "To address this problem, we propose a chatbot model using a mixture of words and syllables as encoding-decoding units.", "labels": [], "entities": []}, {"text": "In addition , we propose a two-step training method , involving pre-training using a large non-dialogue corpus and retraining using a small dialogue corpus.", "labels": [], "entities": []}, {"text": "In our experiments , the mixture units were shown to help reduce out-of-vocabulary (OOV) problems.", "labels": [], "entities": []}, {"text": "Moreover, the two-step training method was effective in reducing grammatical and semantical errors in responses when the chatbot was trained using a small dialogue corpus (533,997 sentence pairs).", "labels": [], "entities": []}], "introductionContent": [{"text": "Chatbots (also known as conversational agents, such as Alexa, Siri, and Cortana) are software programs that mimic written or spoken human speech for interactions with real people.", "labels": [], "entities": []}, {"text": "Chatbot models are divided into two types: retrieval-based and generative models.", "labels": [], "entities": []}, {"text": "The retrieval-based models match an input query against predefined queries, select one query with the highest matching score, and return a response paired with the selected query.", "labels": [], "entities": []}, {"text": "They simply pick responses from a repository of query-response pairs, and therefore the responses do not contain any unplanned grammatical errors.", "labels": [], "entities": []}, {"text": "However, the response coverage is restricted, because retrieval-based models cannot handle unseen queries for which predefined responses do not exist.", "labels": [], "entities": []}, {"text": "To overcome this problem, generative models have been proposed with the increasing development of deep learning techniques.", "labels": [], "entities": [{"text": "generative", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.9685543775558472}]}, {"text": "Generative models do not rely on predefined responses, but rather generate new responses using well-trained neural networks.", "labels": [], "entities": []}, {"text": "Therefore, they have an ability to cope effectively with unseen queries.", "labels": [], "entities": []}, {"text": "However, they require a large training corpus, in the form of queryresponse pairs.", "labels": [], "entities": []}, {"text": "If the training corpus is not sufficient, then they make grammatical errors, especially in longer sentences.", "labels": [], "entities": []}, {"text": "Many previous studies on generative chatbot models are based on sequence-to-sequence networks called encoder-decoder models.", "labels": [], "entities": [{"text": "generative chatbot", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9040566086769104}]}, {"text": "To furnish a chatbot with personal characteristics, proposed a persona-based model in which individual characteristics of speakers are encoded.", "labels": [], "entities": []}, {"text": "However, the persona-based model required a large speaker-specific dialogue corpus for model training.", "labels": [], "entities": []}, {"text": "To resolve this problem, proposed a speaker-role adaptation model based on auto-encoding methods using a non-dialogue corpus.", "labels": [], "entities": [{"text": "speaker-role adaptation", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7586491107940674}]}, {"text": "To improve the performances of chatbots, proposed a hybrid model that generates answers by selecting the most suitable among those retrieved.", "labels": [], "entities": []}, {"text": "These previous models require a huge single-turn dialogue corpus (about ten million paired sentences) for training.", "labels": [], "entities": []}, {"text": "For most languages, excluding a few such as English and Chinese, it is not easy to collect a high-quality dialogue corpus with millions of entries.", "labels": [], "entities": []}, {"text": "To reduce this problem, we propose a two-step training method for efficiently training a generative chatbot model based on a sequence-to-sequence neural network.", "labels": [], "entities": []}, {"text": "In the first step, the proposed model is pre-trained using a large amount of non-dialogue text, such as novel texts and news articles.", "labels": [], "entities": []}, {"text": "We call this the language learning step.", "labels": [], "entities": [{"text": "language learning", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.886346697807312}]}, {"text": "In the second step, it is finaly trained using a comparably small single-turn dialogue corpus.", "labels": [], "entities": []}, {"text": "We call this the dialogue learning step.", "labels": [], "entities": [{"text": "dialogue learning", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8988878428936005}]}, {"text": "Previous models face difficulties in dealing effectively with out-of-vocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "To reduce this problem, we propose an encoding-decoding method using a mixture of words and syllables as encoding-decoding units.", "labels": [], "entities": []}, {"text": "The proposed model encodes and decodes closed words (i.e., general nouns and verbs) into word forms.", "labels": [], "entities": []}, {"text": "Then, it encodes and decodes open words (i.e., proper nouns and OOV words) into syllable forms.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we collected two kinds of).", "labels": [], "entities": []}, {"text": "Thus, to supplement these evaluation measures, we manually examined outputs of the proposed chatbot from grammatical and semantic viewpoints.", "labels": [], "entities": []}, {"text": "For the manual evaluation, we collected 100 new queries from four university students who were not involved in the research.", "labels": [], "entities": []}, {"text": "The four students input the queries to the chatbot.", "labels": [], "entities": []}, {"text": "Then, they assigned scores of 0~2 points to each response generated by the chatbot, from both syntactic and semantic viewpoints, as shown in.", "labels": [], "entities": []}, {"text": "The first experiment was designed to show the usefulness of the proposed architecture, where mixtures of words and syllables are used as input and output sequences, from the aspect of OOV problems.", "labels": [], "entities": []}, {"text": "In, Mixture is the proposed model.", "labels": [], "entities": [{"text": "Mixture", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9658851027488708}]}, {"text": "WordOnly and Syllable-Only represent chatbots that use only words and syllables, respectively, as encoding-decoding units.", "labels": [], "entities": [{"text": "WordOnly", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9711576104164124}]}, {"text": "The parenthesized scores are the performances when functional words (i.e., ending words, postpositional words, and so on) in Korean are excluded from the performance evaluations.", "labels": [], "entities": []}, {"text": "In other words, they are the performances with respect to generated content words (i.e., nouns, verbs, and so on).", "labels": [], "entities": []}, {"text": "All of the models were trained using only the dialogue training corpus, like conventional chatbot models.", "labels": [], "entities": []}, {"text": "As shown in, Mixture exhibited a better performance than Word-Only for all measures.", "labels": [], "entities": [{"text": "Mixture", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.919003427028656}]}, {"text": "Although Mixture achieved an inferior performance to SyllableOnly for the measures with respect to all types of words, it outperformed Syllable-Only for the measures with respect to just content words.", "labels": [], "entities": []}, {"text": "We found that Word-Only and Syllable-Only made many mistakes in generating content words that were unseen in the training data.", "labels": [], "entities": []}, {"text": "This fact indirectly shows that the proposed architecture may contribute to reducing OOV problems.", "labels": [], "entities": [{"text": "OOV", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9041134715080261}]}, {"text": "We analyzed the cases in which Mixture showed lower syntactic and semantic scores than Syllable-Only.", "labels": [], "entities": []}, {"text": "The reasons are as follows: Syllable-Only showed relatively high syntactic and semantic scores because it often returns short and general responses like \"Okay\" and \"Yes, I see\".", "labels": [], "entities": []}, {"text": "Moreover, SyllableOnly more correctly generated functional words than Word-Only and Mixture did.", "labels": [], "entities": []}, {"text": "As a result, Syllable-Only obtained higher syntactic and semantic scores.", "labels": [], "entities": []}, {"text": "Although Mixture well generated content words, it showed lower syntactic and semantic scores than Syllable-Only because it less correctly generated functional words.", "labels": [], "entities": []}, {"text": "The second experiment was designed to show the effectiveness of the proposed training method.", "labels": [], "entities": []}, {"text": "shows how the performance of the proposed chatbot varies according to different training methods.", "labels": [], "entities": []}, {"text": "In, Single-Step Training is a conventional training method in which a chatbot is trained using only the dialogue training corpus.", "labels": [], "entities": []}, {"text": "LM Training is the training method proposed by.", "labels": [], "entities": [{"text": "LM Training", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.7511425018310547}]}, {"text": "In LM Training, a chatbot is pre-trained using a language model, and re-trained using the dialogue training corpus.", "labels": [], "entities": []}, {"text": "TwoStep Training is the proposed method, in which the chatbot is pre-trained using the non-dialogue corpus and re-trained using the dialogue training corpus.", "labels": [], "entities": []}, {"text": "The parenthesized scores give the performances when functional words are excluded from the performance evaluations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance comparison according to  different encoding and decoding units", "labels": [], "entities": []}, {"text": " Table 3: Performance comparison according to  different training methods", "labels": [], "entities": []}]}