{"title": [{"text": "Neural Response Ranking for Social Conversation: A Data-Efficient Approach", "labels": [], "entities": [{"text": "Neural Response", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.826242059469223}, {"text": "Social Conversation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7264491766691208}]}], "abstractContent": [{"text": "The overall objective of 'social' dialogue systems is to support engaging, entertaining, and lengthy conversations on a wide variety of topics , including social chitchat.", "labels": [], "entities": []}, {"text": "Apart from raw dialogue data, user-provided ratings are the most common signal used to train such systems to produce engaging responses.", "labels": [], "entities": []}, {"text": "In this paper we show that social dialogue systems can be trained effectively from raw unanno-tated data.", "labels": [], "entities": []}, {"text": "Using a dataset of real conversations collected in the 2017 Alexa Prize challenge , we developed a neural ranker 1 for selecting 'good' system responses to user utterances , i.e. responses which are likely to lead to long and engaging conversations.", "labels": [], "entities": [{"text": "Alexa Prize challenge", "start_pos": 60, "end_pos": 81, "type": "DATASET", "confidence": 0.9352700511614481}]}, {"text": "We show that (1) our neural ranker consistently outper-forms several strong baselines when trained to optimise for user ratings; (2) when trained on larger amounts of data and only using conversation length as the objective, the ranker performs better than the one trained using ratings ultimately reaching a Precision@1 of 0.87.", "labels": [], "entities": [{"text": "Precision@1", "start_pos": 309, "end_pos": 320, "type": "METRIC", "confidence": 0.9845751921335856}]}, {"text": "This advance will make data collection for social conversational agents simpler and less expensive in the future.", "labels": [], "entities": [{"text": "data collection", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7381367981433868}]}], "introductionContent": [{"text": "Chatbots, or socialbots, are dialogue systems aimed at maintaining an open-domain conversation with the user spanning a wide range of topics, with the main objective of being engaging, entertaining, and natural.", "labels": [], "entities": []}, {"text": "Under one of the current approaches to such systems, the bot ensemble), a collection, or ensemble, of different bots is used, each of which proposes a candidate response to the user's input, and a response ranker selects the best response for the final system output to be uttered to the user.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the task of finding the best supervision signal for training a response ranker for ensemble systems.", "labels": [], "entities": []}, {"text": "Our contribution is twofold: first, we present a neural ranker for ensemble-based dialogue systems and evaluate its level of performance using an annotation type which is often used in open-domain dialogue and was provided to the Alexa Prize 2017 participants by: per-dialogue user ratings.", "labels": [], "entities": [{"text": "Alexa Prize 2017 participants", "start_pos": 230, "end_pos": 259, "type": "DATASET", "confidence": 0.9152181595563889}]}, {"text": "Second and most importantly, we explore an alternative way of assessing social conversations simply via their length, thus removing the need for any user-provided ratings.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to tune the neural rankers, we performed a grid search over the shared encoder GRU layer size and the Predictor topology.", "labels": [], "entities": []}, {"text": "The best configurations are determined by the loss on the development sets.", "labels": [], "entities": []}, {"text": "For evaluation, we used an independent dataset.", "labels": [], "entities": []}, {"text": "At the evaluation stage, we check how well the rankers can distinguish between good responses and bad ones.", "labels": [], "entities": []}, {"text": "The criterion for 'goodness' that we use here is chosen to be independent from both training signals.", "labels": [], "entities": []}, {"text": "Specifically, we collected an evaluation set composed of dialogue turns followed by explicit user feedback, e.g. \"great, thank you\", \"that was interesting\" (we refer to it as the User feedback dataset).", "labels": [], "entities": [{"text": "User feedback dataset", "start_pos": 179, "end_pos": 200, "type": "DATASET", "confidence": 0.7662540078163147}]}, {"text": "Our 'bad' response candidates are randomly sampled across the dataset.", "labels": [], "entities": []}, {"text": "The user feedback turns were identified using sentiment analysis in combination with a whitelist and a blacklist of hand-picked phrases, so that in total we used 605 unique utterances, e.g. \"that's pretty cool\", \"you're funny\", \"gee thanks\", \"interesting fact\", \"funny alexa you're funny\".", "labels": [], "entities": []}, {"text": "'Goodness' defined in this way allows us to evaluate how well our two approximated training signals can optimize for the user's satisfaction as explicitly expressed at the turn level, thus leading 3 Some extremely long dialogues are due to users repeating themselves over and over, and so this filter removes these bad dialogues from the dataset.", "labels": [], "entities": []}, {"text": "Dialogues less than 3 turns long are often where the user accidentally triggered the chatbot.", "labels": [], "entities": []}, {"text": "These outliers amounted to about 14% of our data.", "labels": [], "entities": []}, {"text": "Using more extreme thresholds did not produce enough data while less ones did not provide adequate training signal.", "labels": [], "entities": []}, {"text": "We tested GRU sizes of 64, 128, 256 and Predictor layers number/sizes of,,.", "labels": [], "entities": [{"text": "Predictor", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9909331798553467}]}, {"text": "to our desired behaviour, i.e., producing long and engaging dialogues.", "labels": [], "entities": []}, {"text": "The User feedback dataset contains 24,982 context, good response, bad response tuples in total.", "labels": [], "entities": [{"text": "User feedback dataset", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.5923593540986379}]}, {"text": "To evaluate the rankers on this dataset, we use precision@k, which is commonly used for information retrieval system evaluation (Eq. 3).", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9953409433364868}, {"text": "information retrieval system evaluation", "start_pos": 88, "end_pos": 127, "type": "TASK", "confidence": 0.8760693669319153}]}, {"text": "where c is dialogue context, R is response candidates list, and Relevant is a binary predicate indicating whether a particular response is relevant to the context.", "labels": [], "entities": [{"text": "Relevant", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9964786171913147}]}, {"text": "Precision is typically used together with recall and F-measure.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9552648663520813}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9994814991950989}, {"text": "F-measure", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9957762360572815}]}, {"text": "However, since our dialogue data is extremely sparse so that it is hard to find multiple good responses for the same exact dialogue context, recall and F-measure cannot be applied to this setting.", "labels": [], "entities": [{"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9992689490318298}, {"text": "F-measure", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9980265498161316}]}, {"text": "Therefore, since we only perform pairwise ranking, we use precision@1 to check that the good answer is the top-ranked one.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9975384473800659}]}, {"text": "Also due to data sparsity, we only perform this evaluation with gold positive responses and sampled negative ones -it is typically not possible to find a good response with exactly the same context as a given bad response.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Ranking models evaluation: pairwise ranking  precision on the independent User feedback dataset and  loss on the Length/Rating test sets (Section 5) for the  corresponding trainset sizes of 500,000.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9763084053993225}, {"text": "Length/Rating test sets", "start_pos": 123, "end_pos": 146, "type": "DATASET", "confidence": 0.7546270847320556}]}]}