{"title": [{"text": "EmoNLP at IEST 2018: An Ensemble of Deep Learning Models and Gradient Boosting Regression Tree for Implicit Emotion Prediction in Tweets", "labels": [], "entities": [{"text": "EmoNLP at IEST 2018", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.6996769458055496}, {"text": "Implicit Emotion Prediction", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.6675740083058676}]}], "abstractContent": [{"text": "This paper describes our system submitted to IEST 2018, a shared task (Klinger et al., 2018) to predict the emotion types.", "labels": [], "entities": []}, {"text": "Six emotion types are involved: anger, joy, fear, surprise , disgust and sad.", "labels": [], "entities": []}, {"text": "We perform three different approaches: feed forward neural network (FFNN), convolutional BLSTM (Con-BLSTM) and Gradient Boosting Regression Tree Method (GBM).", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.7255202531814575}]}, {"text": "Word embeddings used in convolutional BLSTM are pre-trained on 470 million tweets which are filtered using the emotional words and emojis.", "labels": [], "entities": []}, {"text": "In addition , broad sets of features (i.e. syntactic features, lexicon features, cluster features) are adopted to train GBM and FFNN.", "labels": [], "entities": [{"text": "GBM", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.8196842670440674}, {"text": "FFNN", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.8586313128471375}]}, {"text": "The three approaches are finally ensembled by the weighted average of predicted probabilities of each emotion label.", "labels": [], "entities": []}], "introductionContent": [{"text": "Twitter is an active social networking platform.", "labels": [], "entities": []}, {"text": "It is estimated that nearly 500 million tweets are sent per day . As a short message where people can convey their emotions, twitter data is particular interesting for emotional detection.", "labels": [], "entities": [{"text": "emotional detection", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.7398828715085983}]}, {"text": "The task of WASSA 2018 Implicit Emotion Shared Task is aimed to predict the emotion underlying in the tweets.", "labels": [], "entities": [{"text": "WASSA 2018 Implicit Emotion Shared Task", "start_pos": 12, "end_pos": 51, "type": "TASK", "confidence": 0.6947672069072723}]}, {"text": "The emotional types that are supposed to predict are \"Anger, Fear, Sadness, Joy, Surprise, Disgust\".", "labels": [], "entities": []}, {"text": "In each tweet, the emotional expression is implicit, that is, a certain emotional word is removed.", "labels": [], "entities": []}, {"text": "The removed emotional words could be one of the following:\"sad\", \"happy\", \"disgusted\", \"surprised\", \"angry\", \"afraid\" or a synonym of one of them.", "labels": [], "entities": []}, {"text": "For example, given the tweet \"It's when you feel like you are invisible to others.\", the system should predict the label of this tweet as \"Sadness\".", "labels": [], "entities": []}, {"text": "Moreover, the 1 https://en.wikipedia.org/wiki/Twitter system cannot only be useful for implicit emotion detection but also for various NLP applications.", "labels": [], "entities": [{"text": "implicit emotion detection", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.6535303692022959}]}, {"text": "For example, this system can be used to detect the emotions in movie reviews which do not have the sentimental word but actually express sentimental polarities.", "labels": [], "entities": []}, {"text": "In this paper, we describe our approaches and experiments to solve this problem.", "labels": [], "entities": []}, {"text": "Our system is an ensemble of three classification approaches combined with a weighted average of predicted probabilities.", "labels": [], "entities": []}, {"text": "Whilst, two of the three approaches are neural network models and the other is a gradient boosting regression tree model (Section 3).", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: Section 2 discusses in brief the dataset for the task.", "labels": [], "entities": []}, {"text": "Section 3 gives an explanation about the details of various approaches used in our system.", "labels": [], "entities": []}, {"text": "Section 4 shows the results and discussions about them.", "labels": [], "entities": []}, {"text": "Finally, we draw the conclusion about our participation in the Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Test and Development results on our system.", "labels": [], "entities": []}, {"text": " Table 2: Results on different labels.", "labels": [], "entities": []}]}