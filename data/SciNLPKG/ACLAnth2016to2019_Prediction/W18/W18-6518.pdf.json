{"title": [], "abstractContent": [{"text": "We study the problem of opinion question generation from sentences with the help of community-based question answering systems.", "labels": [], "entities": [{"text": "opinion question generation from sentences", "start_pos": 24, "end_pos": 66, "type": "TASK", "confidence": 0.768492466211319}, {"text": "question answering", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7481914162635803}]}, {"text": "For this purpose, we use a sequence to sequence attentional model, and we adopt coverage mechanism to prevent sentences from repeating themselves.", "labels": [], "entities": [{"text": "coverage", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9476438760757446}]}, {"text": "Experimental results on the Amazon ques-tion/answer dataset show an improvement in automatic evaluation metrics as well as human evaluations from the state-of-the-art question generation systems.", "labels": [], "entities": [{"text": "Amazon ques-tion/answer dataset", "start_pos": 28, "end_pos": 59, "type": "DATASET", "confidence": 0.7329905271530152}]}], "introductionContent": [{"text": "Question generation (QG) can be considered as a task which affects many aspects of people's lives.", "labels": [], "entities": [{"text": "Question generation (QG)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8570731401443481}]}, {"text": "One of the main significance of the question generation is its capability to improve one's learning ability.", "labels": [], "entities": [{"text": "question generation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7340832352638245}]}, {"text": "Studies have shown that asking questions can help students realize their knowledge deficits and encourages them to look for information to compensate for those deficits.", "labels": [], "entities": []}, {"text": "Additionally, QG can be used as an aid to search engines by providing suggestions regarding the users' queries (.", "labels": [], "entities": []}, {"text": "This way, the users can either choose one of those suggestions or obtain a better idea on how to modify their query to get better results.", "labels": [], "entities": []}, {"text": "Moreover, QG can assist the reading comprehension task and the question answering community by providing a robust input for their systems.", "labels": [], "entities": [{"text": "question answering community", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.8455702861150106}]}, {"text": "In this work, we propose a sequence to sequence model that uses attention and coverage mechanisms for addressing the question generation problem at the sentence level.", "labels": [], "entities": [{"text": "question generation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7262639552354813}]}, {"text": "The attention and coverage mechanisms prevent language generation systems from generating the same word over and over again, and have been shown to improve a system's output (.", "labels": [], "entities": []}, {"text": "We benefit from the community-based question answering systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7813321650028229}]}, {"text": "Specifically, we use the Amazon question/answer dataset.", "labels": [], "entities": [{"text": "Amazon question/answer dataset", "start_pos": 25, "end_pos": 55, "type": "DATASET", "confidence": 0.920808982849121}]}, {"text": "The sentences are mostly informal and sometimes do not follow the correct grammatical structure.", "labels": [], "entities": []}, {"text": "We utilize the answers that people post on the community question answering system as inputs to our model; hence, proposing an opinion question generation system which could be used as an interface to online forums helping users in browsing and querying them by making questions as suggestions.", "labels": [], "entities": [{"text": "opinion question generation", "start_pos": 127, "end_pos": 154, "type": "TASK", "confidence": 0.6828591028849283}]}, {"text": "In the subsequent section, we describe the related works to QG.", "labels": [], "entities": [{"text": "QG", "start_pos": 60, "end_pos": 62, "type": "DATASET", "confidence": 0.8650065064430237}]}, {"text": "The next section is on the task definition, followed by the demonstration of the model structure.", "labels": [], "entities": []}, {"text": "After that, we discuss the experimental settings and at the end provide a thorough discussion of our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Amazon question/answer dataset.", "labels": [], "entities": [{"text": "Amazon question/answer dataset", "start_pos": 11, "end_pos": 41, "type": "DATASET", "confidence": 0.8739305257797241}]}, {"text": "We set the minimum length of the questions to 4 tokens, including the question mark to filter out poorly structured sentences.", "labels": [], "entities": []}, {"text": "The answers must beat least 10 tokens long.", "labels": [], "entities": []}, {"text": "Moreover, we set the maximum length of the questions and the answers to 20 and 35 tokens, respectively.", "labels": [], "entities": []}, {"text": "As there are many URLs in the dataset, we replace them with a URL token to reduce the vocabulary size.", "labels": [], "entities": []}, {"text": "We lower-case the entire dataset and use the NLTK toolkit 1 for sentence tokenization.", "labels": [], "entities": [{"text": "NLTK toolkit 1", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.925804078578949}, {"text": "sentence tokenization", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.6339334696531296}]}, {"text": "There can be many examples where the questions are not grammatically correct.", "labels": [], "entities": []}, {"text": "People may just ask: \"Waterproof ?\".", "labels": [], "entities": []}, {"text": "The same problem occurs with the answers: the answer might be a single \"Yes\".", "labels": [], "entities": []}, {"text": "We use 80% of the dataset as the training set, and the rest is divided between the validation set and the test set.", "labels": [], "entities": []}, {"text": "shows the total number of examples in each dataset after removing very long or very short sentences from the training and the validation datasets.", "labels": [], "entities": []}, {"text": "Our base model is from OpenNMT system (, and we use the PyTorch 2 library, a deep learning framework that provides maximum flexibility and speed.", "labels": [], "entities": [{"text": "PyTorch 2 library", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.8502124349276224}]}, {"text": "It accelerates the computation on both CPU and GPU by a great amount, and the memory usage is extremely efficient in PyTorch compared to other options.", "labels": [], "entities": []}, {"text": "We fix the size of the answer and the question vocabularies to 50k.", "labels": [], "entities": []}, {"text": "Only the most frequent words are kept, and the rest are replaced with the UNK token.", "labels": [], "entities": [{"text": "UNK token", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.9236624240875244}]}, {"text": "We set the word embedding dimension to 300 and we use glove.840B.300d) as the pre-trained word embedding on both the encoder and the decoder sides.", "labels": [], "entities": []}, {"text": "These embeddings are updated during training.", "labels": [], "entities": []}, {"text": "The LSTM hidden unit size is set to 600 and we set the number of layers to 2.", "labels": [], "entities": []}, {"text": "We employ the stochastic gradient descent (SGD) as the optimization method with an initial learning rate of 1.0 and halve the learning rate after 10 epochs.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.7517169813315073}]}, {"text": "The training continues for 20 epochs with the batch size of 64 and dropout probability of 0.3.", "labels": [], "entities": [{"text": "dropout probability", "start_pos": 67, "end_pos": 86, "type": "METRIC", "confidence": 0.9513991177082062}]}, {"text": "The hyperparameter \u03bb that is used for weighting the coverage loss is set to 1 3 . The decoding is done using the beam search with the beam size of 5, and the generation is stopped when we reach the EOS token.", "labels": [], "entities": [{"text": "EOS token", "start_pos": 198, "end_pos": 207, "type": "DATASET", "confidence": 0.9583896696567535}]}, {"text": "In the end, we choose the model with the lowest perplexity on the validation set.", "labels": [], "entities": []}, {"text": "For evaluating our system automatically, we use three different evaluation metrics.", "labels": [], "entities": []}, {"text": "The first one is BLEU () that uses the ngram similarity between a prediction and a set of references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.996747612953186}]}, {"text": "We calculate BLEU score for unigrams and bigrams.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9731256365776062}]}, {"text": "The next one is METEOR, which scores predictions by aligning them to ground truth sentences with the help of stemming, synonyms and paraphrases.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.963360607624054}]}, {"text": "The last evaluation metric is Rouge.", "labels": [], "entities": []}, {"text": "It compares the generated sentences with the references based on n-gram.", "labels": [], "entities": []}, {"text": "For this task, we use ROUGE L , which reports the results based on the longest common subsequence.", "labels": [], "entities": [{"text": "ROUGE L", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9656802117824554}]}, {"text": "We use the evaluation package by. shows the results of our system and the baseline.", "labels": [], "entities": []}, {"text": "Our model improves the BLEU 1 score by at least 1.5 points.", "labels": [], "entities": [{"text": "BLEU 1 score", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9747294386227926}]}, {"text": "It also achieves a better result regarding the BLEU 2 and the ME-TEOR whereas the ROUGE is lower than the baseline.", "labels": [], "entities": [{"text": "BLEU 2", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9846172034740448}, {"text": "ME-TEOR", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.981823742389679}, {"text": "ROUGE", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9988501071929932}]}, {"text": "If we consider the results reported in, we notice that the BLEU scores are much higher compared to our work.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.999595582485199}]}, {"text": "The reason is that they use the SQuAD dataset, which is a human-generated corpus.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.88346728682518}]}, {"text": "The sentences are well-structured, grammatically correct with fewer unnecessary punctuation and colloquialism.", "labels": [], "entities": []}, {"text": "However, when working with the community-based question answering systems, the structure of sentences do not always follow the correct path.", "labels": [], "entities": [{"text": "question answering", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.720735713839531}]}, {"text": "These sentences often contain useless information and symbols.", "labels": [], "entities": []}, {"text": "Another problem is that multiple questions can be generated from a single sentence.", "labels": [], "entities": []}, {"text": "The system may generate a question which is correct both semantically and grammatically and also asks about accurate information in the sentence.", "labels": [], "entities": []}, {"text": "However, if it is not the same as the ground-truth, the results will be affected.", "labels": [], "entities": []}, {"text": "shows some examples generated by our system and, where the coverage mechanism becomes useful and prevents the model from generating the same word 'material' twice.", "labels": [], "entities": []}, {"text": "To further assess the performance of our system, we performed human evaluations on the results.", "labels": [], "entities": []}, {"text": "Three English-speaker students were asked to give a score from 1 (very poor) to 5 (very good) to the questions generated from both systems according to two criteria: syntactic correctness and relevance.", "labels": [], "entities": []}, {"text": "Syntactic correctness indicates the grammaticality and the fluency and relevance demonstrates whether the question is meaningful and related to the sentence it is generated from.", "labels": [], "entities": []}, {"text": "The three assessors performed the evaluations on 100 randomly selected question and answer pairs from the results.", "labels": [], "entities": []}, {"text": "The comparison of human evaluations between our system and the model is shown in.", "labels": [], "entities": []}, {"text": "Bold numbers demonstrate the best performing system for each evaluation criteria, and we see that our system outperforms the", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU 1-2, METEOR and ROUGE L  scores on the test set. Bold numbers demonstrate  the best performing system for each evaluation  metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993630051612854}, {"text": "METEOR", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9962654709815979}, {"text": "ROUGE L", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9369595646858215}]}, {"text": " Table 3. Bold numbers demonstrate  the best performing system for each evaluation cri- teria, and we see that our system outperforms the", "labels": [], "entities": []}, {"text": " Table 3: Human evaluation results for the syntac- tic correctness and relevance between our model  and Du et al. (2017).", "labels": [], "entities": [{"text": "syntac- tic correctness", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.6384342014789581}]}]}