{"title": [{"text": "Retrieve and Refine: Improved Sequence Generation Models For Dialogue", "labels": [], "entities": [{"text": "Retrieve", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9503356218338013}, {"text": "Sequence Generation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8525651693344116}]}], "abstractContent": [{"text": "Sequence generation models for dialogue are known to have several problems: they tend to produce short, generic sentences that are un-informative and unengaging.", "labels": [], "entities": [{"text": "Sequence generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9306690394878387}]}, {"text": "Retrieval models on the other hand can surface interesting responses , but are restricted to the given retrieval set leading to erroneous replies that cannot be tuned to the specific context.", "labels": [], "entities": []}, {"text": "In this work we develop a model that combines the two approaches to avoid both their deficiencies: first retrieve a response and then refine it-the final sequence generator treating the retrieval as additional context.", "labels": [], "entities": []}, {"text": "We show on the recent CON-VAI2 challenge task our approach produces responses superior to both standard retrieval and generation models inhuman evaluations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence generation models like Seq2Seq () are increasingly popular for tasks such as machine translation (MT) and summarization, where generation is suitably constrained by the source sentence.", "labels": [], "entities": [{"text": "Sequence generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8743918538093567}, {"text": "machine translation (MT)", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.8337908625602722}, {"text": "summarization", "start_pos": 115, "end_pos": 128, "type": "TASK", "confidence": 0.9661138653755188}]}, {"text": "However, obtaining good performance on dialogue tasks, where the context still allows many interpretations, remains an open problem despite much recent work ( . Several authors report the issue that they produce short, generic sentences containing frequent wordsthe so-called \"I don't know\" problem -as that response can work as a reply in many instances, but is uninformative and unengaging.", "labels": [], "entities": []}, {"text": "Retrieval models () do not have this problem, but instead either produce engaging responses or else completely erroneous ones which they cannot Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI tune to the specific context, as they can only produce a valid reply if it is in the retrieval set.", "labels": [], "entities": [{"text": "EMNLP Workshop SCAI", "start_pos": 168, "end_pos": 187, "type": "DATASET", "confidence": 0.8992257714271545}]}, {"text": "In this work we propose a Retrieve and Refine model to gain the advantages of both methods, and avoid both their disadvantages.", "labels": [], "entities": [{"text": "Retrieve", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.7990930676460266}]}, {"text": "Models that produce an initial prediction and then refine it are growing in traction in NLP.", "labels": [], "entities": []}, {"text": "They have been used in MT and summarization either for refinement of initial predictions or combining with retrieval (, as well as for sentence correction or refinement without context (.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9978646636009216}, {"text": "summarization", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.7520988583564758}, {"text": "sentence correction", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.745065450668335}]}, {"text": "There is little work in applying these methods to dialogue; one work we are aware of has been done concurrently with ours is.", "labels": [], "entities": []}, {"text": "The usefulness of our approach is shown with detailed experiments on the ConvAI2 dataset 1 which is a chit-chat task to get to know the other speaker's profile, obtaining generations superior to both retrieval and sequence generation models inhuman evaluations.", "labels": [], "entities": [{"text": "ConvAI2 dataset 1", "start_pos": 73, "end_pos": 90, "type": "DATASET", "confidence": 0.9210750857988993}]}], "datasetContent": [{"text": "We conduct experiments on the recent ConvAI2 challenge dataset which uses a modified version of the PersonaChat dataset (Zhang et al., 2018) (larger, and with different processing).", "labels": [], "entities": [{"text": "ConvAI2 challenge dataset", "start_pos": 37, "end_pos": 62, "type": "DATASET", "confidence": 0.8536028067270914}, {"text": "PersonaChat dataset", "start_pos": 100, "end_pos": 119, "type": "DATASET", "confidence": 0.9129748940467834}]}, {"text": "The dataset consists of conversations between crowdworkers who were randomly paired and asked to act the part of a given persona (randomly assigned from 1155 possible personas, created by another set of workers), chat naturally, and get to know each other during the conversation.", "labels": [], "entities": []}, {"text": "There are around 160,000 utterances in around 11,000 dialogues, with 2000 dialogues for validation and test, which use non-overlapping personas.", "labels": [], "entities": []}, {"text": "Perplexity Dialogue is known to be notoriously hard to evaluate with automated metrics (.", "labels": [], "entities": [{"text": "Perplexity Dialogue", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.852924108505249}]}, {"text": "In contrast to machine translation, there is much less constraint on the output with many valid answers with little word overlap, e.g. there are many answers to \"what are you doing tonight?\".", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7073927074670792}]}, {"text": "Nevertheless many recent papers report perplexity results in addition to human judgments.", "labels": [], "entities": []}, {"text": "For the retrieve and refine case, perplexity evaluation is particularly flawed: if the retrieval points the model to a response that is very different from (but equally valid as) the true response, the model might focus on refining that and get poor perplexity.", "labels": [], "entities": [{"text": "perplexity evaluation", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.865362286567688}]}, {"text": "We therefore test our model by considering various types of retrieval methods: (i) the best performing existing retriever model, the Memory Network approach from (retrieving from the training set), (ii) a retriever that returns a random utterance from the training set, (iii) the true label given in the test set, and (iv) the closest nearest neighbor from the training set utterances to the true label, as measured by the embedding space of the Memory Network retriever model.", "labels": [], "entities": []}, {"text": "While (iii) and (iv) cannot be used in a deployed system as they are unknown, they can be   used as a sanity check: a useful retrieve and refine should improve perplexity if given these as input.", "labels": [], "entities": []}, {"text": "We also compare to a standard Seq2Seq model, i.e. no retrieval.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "They show that the RetNRef model can indeed improve perplexity with label neighbors or the label itself.", "labels": [], "entities": []}, {"text": "However, surprisingly there is almost no difference between using no retrieval, random labels or our best retriever.", "labels": [], "entities": []}, {"text": "The RetNRef ++ model -that truncates the dialogue history and focuses more on the retrieval utterance -does even worse in terms of perplexity: 48.4 using the Memory Network retriever.", "labels": [], "entities": []}, {"text": "However, poor perplexity does not mean human judgments of the generated sequences will not improve; in fact we will see that they do in the next section.", "labels": [], "entities": []}, {"text": "How to automatically evaluate these kinds of models still remains an open problem.", "labels": [], "entities": []}, {"text": "Word Statistics Another way to measure the salience of a generation model is to compare it to human utterances in terms of word statistics.", "labels": [], "entities": []}, {"text": "We analyze the word statistics of our models in.", "labels": [], "entities": []}, {"text": "Seq2Seq models are known to produce short sentences with more common words than humans.", "labels": [], "entities": []}, {"text": "The statistics on the ConvAI2 dataset bear this out, where the Seq2Seq model responses have lower word and character counts and use fewer rare words than the human responses.", "labels": [], "entities": [{"text": "ConvAI2 dataset", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.9759925603866577}]}, {"text": "The RetNRef model (using the Memory Network retriever, retrieving from the training set) makes some improvements in this regard, e.g. doubling the use of rare words (with frequency less than 100) and smaller gains for words with frequency less than 1000, but are still not close to human statistics.", "labels": [], "entities": []}, {"text": "The RetNRef ++ model which boosts the use of the retrieval does better in this regard, making the statistics much closer to human ones.", "labels": [], "entities": []}, {"text": "Of course these metrics do not measure whether the utterances are semantically coherent, but it is encouraging to see a model using rare words as without this we believe it is hard for it to be engaging.", "labels": [], "entities": []}, {"text": "compares the word overlap between retrieved and generated sentences in the RetNRef variants in order to measure if RetNRef is either ignoring the retriever, or else paying too much attention to and copying it.", "labels": [], "entities": []}, {"text": "As comparison, the first row also shows the overlap between the retriever and vanilla Seq2Seq which does not retrieve at all.", "labels": [], "entities": []}, {"text": "The results show that RetNRef ++ has >80% word overlap with the retriever output around half (53%) of the time, whereas Seq2Seq and RetNRef very rarely overlap with the retriever (3% and 8% of the time respectively have >80% overlap).", "labels": [], "entities": []}, {"text": "This shows that our improved model RetNRef ++ does use the retriever, but can also generate novel content when it wants to, which a standard retriever cannot.", "labels": [], "entities": []}, {"text": "Following the protocol in, we asked humans to conduct short dialogues with our models (100 dialogues each of 10-20 turns, so 600 dialogues in total), and then measure the engagingness, consistency, and fluency (all scored out of 5) as well as to try to detect the persona that the model is using, given the choice between that and a random persona.", "labels": [], "entities": [{"text": "consistency", "start_pos": 185, "end_pos": 196, "type": "METRIC", "confidence": 0.9925988912582397}]}, {"text": "Consistency measures the coherence of the dialogue, e.g. \"I have a dog\" followed by \"I have no pets\" is not consistent.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "They show engagingness scores superior to Seq2Seq for all RetNRef variants, and with RetNRef ++ slightly outperforming the retriever which it conditions on.", "labels": [], "entities": []}, {"text": "Importantly however, it maintains this performance whilst still being able to generate text which a retrieval model cannot.", "labels": [], "entities": []}, {"text": "It also performs well in the other metrics, although like the Memory Network model, it is weaker at using the persona than Seq2Seq.", "labels": [], "entities": []}, {"text": "Seq2Seq is inferior to the    Memory Network Retriever in terms of engagement, inline with previous results.", "labels": [], "entities": [{"text": "engagement", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9799790382385254}]}, {"text": "We also tried overtraining the Seq2Seq for 100 epochs instead of early stopping by validation on perplexity as it may overfit training sentences and act more as a retriever, but this did not help.", "labels": [], "entities": []}, {"text": "Some example dialogues of the RetNRef ++ model performing well (as scored by the evaluators) are shown in.", "labels": [], "entities": []}, {"text": "Longer sentences from the bot (person 2) with more nuanced entity information typically come from attending to the retriever, whereas the generator can also produce shorter replies independent of the retriever that fit the context well.", "labels": [], "entities": []}, {"text": "There are still issues however, such as repeated phrases by the generator, and some tendency to copy the speaking partner's phrases that could be improved.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexity on the ConvAI2 task test set with  different types of retriever for RetNRef, see text.", "labels": [], "entities": [{"text": "ConvAI2 task test set", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.8086003810167313}, {"text": "RetNRef", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.7893990278244019}]}, {"text": " Table 2: Output sequence statistics for the methods.  Seq2Seq generates shorter sentences with more com- mon words than humans, which RetNRef alleviates.", "labels": [], "entities": [{"text": "Output", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.950739860534668}]}, {"text": " Table 3: Word overlap between retrieved and gener- ated utterances in RetNRef, and between Seq2Seq and  the Memory Network retriever (first row).", "labels": [], "entities": [{"text": "Word overlap", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.8276024162769318}, {"text": "RetNRef", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9233804941177368}]}, {"text": " Table  2. Seq2Seq models are known to produce short  sentences with more common words than humans.  The statistics on the ConvAI2 dataset bear this out,  where the Seq2Seq model responses have lower  word and character counts and use fewer rare  words than the human responses. The RetNRef", "labels": [], "entities": [{"text": "ConvAI2 dataset", "start_pos": 123, "end_pos": 138, "type": "DATASET", "confidence": 0.988038569688797}, {"text": "RetNRef", "start_pos": 283, "end_pos": 290, "type": "DATASET", "confidence": 0.806602954864502}]}, {"text": " Table 4: Human Evaluation scores for the models,scoring fluency, engagingness, consistency and persona detec- tion, with standard deviation in parentheses. We consider engagingness to be the most important metric.", "labels": [], "entities": [{"text": "consistency", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9992550015449524}]}, {"text": " Table 6: Example dialogues (left) where RetNRef outperforms MemNet, and (right) where RetNRef outperforms  Seq2Seq -by either paying attention to the retriever or not. The first two rows are the previous two dialogue turns  by Person 1 & 2, the following rows are the predictions for the next utterance of Person 1 by the various models.", "labels": [], "entities": [{"text": "MemNet", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.9083025455474854}]}]}