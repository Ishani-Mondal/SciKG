{"title": [{"text": "Neural sentence generation from formal semantics", "labels": [], "entities": [{"text": "Neural sentence generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7725983460744222}]}], "abstractContent": [{"text": "Sequence-to-sequence models have shown strong performance in a wide range of NLP tasks, yet their applications to sentence generation from logical representations are underdeveloped.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7581736445426941}]}, {"text": "In this paper , we present a sequence-to-sequence model for generating sentences from logical meaning representations based on event semantics.", "labels": [], "entities": []}, {"text": "We use a semantic parsing system based on Combinatory Categorial Grammar (CCG) to obtain data annotated with logical formulas.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7514313757419586}]}, {"text": "We augment our sequence-to-sequence model with masking for predicates to constrain output sentences.", "labels": [], "entities": []}, {"text": "We also propose a novel evaluation method for generation using Recognizing Textual Entailment (RTE).", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 63, "end_pos": 99, "type": "TASK", "confidence": 0.5824328362941742}]}, {"text": "Combining parsing and generation, we test whether or not the output sentence entails the original text and vice versa.", "labels": [], "entities": []}, {"text": "Experiments showed that our model outper-formed a baseline with respect to both BLEU scores and accuracies in RTE.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9774690866470337}]}], "introductionContent": [{"text": "In recent years, syntactic and semantic parsing has been developed and improved significantly.", "labels": [], "entities": [{"text": "syntactic and semantic parsing", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.6574614718556404}]}, {"text": "Syntactic parsing based on syntactic theories has been accomplishing reasonable accuracy to support various application tasks.", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7039775550365448}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9989882111549377}]}, {"text": "Mapping sentences to logical formulas automatically has also been studied in depth, so there are semantic parsing systems that can produce high quality formulas.", "labels": [], "entities": [{"text": "Mapping sentences to logical formulas automatically", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8135204513867696}, {"text": "semantic parsing", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.7449425160884857}]}, {"text": "* This work was done prior to joining Amazon.", "labels": [], "entities": [{"text": "Amazon", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.8176220655441284}]}, {"text": "One advantage of using logical formulas in semantic parsing is that they have expressive power that goes beyond simple representations such as predicate-argument structures.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7209222465753555}]}, {"text": "More specifically, logical formulas can capture aspects of sentence meanings that arise from complex syntactic structures such as coordination, functional words such as negation and quantifiers, and the scope of interactions between them.", "labels": [], "entities": []}, {"text": "In combination with the restricted use of higherorder logic (HOL) developed informal semantics, those logical formulas have recently been used for RTE ( and Semantic Textual Similarity (STS) () and achieved high accuracy.", "labels": [], "entities": [{"text": "RTE", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.9687226414680481}, {"text": "Semantic Textual Similarity (STS)", "start_pos": 157, "end_pos": 190, "type": "TASK", "confidence": 0.734104519089063}, {"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9959264993667603}]}, {"text": "Compared with these recent developments in syntactic and semantic parsing, automatic generation of sentences from expressive logical formulas has received relatively less attention, despite along and venerable tradition of work on surface realization, including those based on Minimal Recursion Semantics (MRS)) and CCG.", "labels": [], "entities": [{"text": "syntactic and semantic parsing", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.6773244217038155}, {"text": "automatic generation of sentences from expressive logical formulas", "start_pos": 75, "end_pos": 141, "type": "TASK", "confidence": 0.8519355691969395}, {"text": "surface realization", "start_pos": 231, "end_pos": 250, "type": "TASK", "confidence": 0.7599219977855682}, {"text": "Minimal Recursion Semantics (MRS))", "start_pos": 277, "end_pos": 311, "type": "TASK", "confidence": 0.6903731524944305}]}, {"text": "If one can generate sentences from formulas, it would be possible to perform other NLP tasks in combination with RTE, including those challenging tasks such as paraphrase extraction ( and sentence splitting and rephrasing.", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 160, "end_pos": 181, "type": "TASK", "confidence": 0.8655796647071838}, {"text": "sentence splitting and rephrasing", "start_pos": 188, "end_pos": 221, "type": "TASK", "confidence": 0.7395184338092804}]}, {"text": "Meanwhile, sequence-to-sequence models showed high performance in machine translation and many other areas in NLP (), yet their applications to sentence generation from logical meaning representations are still underdeveloped, mainly due to alack of data and the structural complexity of meaning representations (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8239181339740753}, {"text": "sentence generation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7670884430408478}]}, {"text": "To address this challenge, we introduce a first sequence-tosequence model for sentence generation from logical formulas.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7214304208755493}]}, {"text": "We use the semantic parsing system ccg2lambda 1 to obtain data annotated with logical formulas including higher-order ones.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7483307421207428}]}, {"text": "Since the distinction between content words and function words plays an important role in parsing and generation, we augment the sequence-to-sequence model with masking for predicates, so that content words in input logical formulas occur in output sentences with a list of function words utilized in the parsing system.", "labels": [], "entities": [{"text": "parsing and generation", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7573983073234558}]}, {"text": "We also propose a novel evaluation method for sentence generation.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.780913382768631}]}, {"text": "BLEU () is widely used to evaluate the quality of decoded sentences, but it has difficulties in assessing finegrained meaning relations between sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9802221655845642}]}, {"text": "Instead, we use an RTE system for evaluation.", "labels": [], "entities": []}, {"text": "We test whether or not the output sentence entails the original text and vice versa.", "labels": [], "entities": []}, {"text": "This idea is motivated by the assumption that unlike surfacebased methods such as BLEU, textual entailment is sensitive to syntactic and semantic aspects of sentences, thus making it possible to distinguish fine-grained meaning relations between original and output sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.6875670552253723}]}, {"text": "RTE has also been shown to be effective for evaluation of machine translation (.", "labels": [], "entities": [{"text": "RTE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7241466045379639}, {"text": "machine translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.8026412427425385}]}, {"text": "Experiments show that our model outperforms a baseline with respect to both BLEU scores and accuracy in RTE.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.9769384860992432}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9990717172622681}]}], "datasetContent": [{"text": "We create a dataset annotated with logical formulas from the SNLI corpus, a collection of 570,000 English sentence pairs manually labeled with an entailment relation.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.8269478976726532}]}, {"text": "We use 50,000 hypothesis sentences from its training portion and split them into 42,000, 4,000, and 4,000 sentences for our training, development, and test sets, respectively.", "labels": [], "entities": []}, {"text": "We map the sentences into logical formulas using ccg2lambda.", "labels": [], "entities": []}, {"text": "We use C&C parser for converting tokenized sentences into CCG trees.", "labels": [], "entities": []}, {"text": "shows the number of words in the constructed corpus (vocab), the max length (maxlen) and average length (ave-len) of sequences obtained for the token-based (token) and graphbased methods (graph).", "labels": [], "entities": [{"text": "average length (ave-len)", "start_pos": 89, "end_pos": 113, "type": "METRIC", "confidence": 0.8395787477493286}]}, {"text": "Here output shows information on the output sentences.", "labels": [], "entities": []}, {"text": "As a baseline, we use Treebank Semantics (Butler, 2016) 2 , a rule-based system for parsing and generation with logical formulas based on event semantics.", "labels": [], "entities": [{"text": "Treebank Semantics (Butler, 2016) 2", "start_pos": 22, "end_pos": 57, "type": "DATASET", "confidence": 0.7096386849880219}, {"text": "parsing and generation", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.8333247701327006}]}, {"text": "For evaluation, AMR generation tasks () use BLEU, which does not directly consider the meaning and structure of a sentence.", "labels": [], "entities": [{"text": "AMR generation tasks", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.9496326446533203}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9986709356307983}]}, {"text": "For instance, two sentences No one visited the old man to greet him and Someone visited the old man to greet him are similar but differ in meaning.", "labels": [], "entities": []}, {"text": "To avoid this problem, we propose an evaluation method using parsing and RTE.", "labels": [], "entities": [{"text": "parsing", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.9690529704093933}]}, {"text": "Namely, we first vocab max-len ave-len  parse an input sentence S 1 to obtain a formula P and then generate a sentence S 2 from the formula P . Finally, we check whether S 1 entails S 2 and vice versa.", "labels": [], "entities": []}, {"text": "Our method based on RTE can detect differences in meaning in cases like the above.", "labels": [], "entities": []}, {"text": "We measure the accuracy of RTE for unidirectional and bidirectional entailments: S 1 \u21d2 S 2 , S 2 \u21d2 S 1 and S 1 \u21d4 S 2 . We use ccg2lambda for parsing original and generated sentences and proving entailment relations between them.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9995648264884949}, {"text": "parsing original and generated sentences", "start_pos": 141, "end_pos": 181, "type": "TASK", "confidence": 0.8317520499229432}]}, {"text": "We use 400 pairs of sentences taken from the test set for RTE experiments.", "labels": [], "entities": [{"text": "RTE", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.837746262550354}]}, {"text": "The inference system outputs yes (entailment), no (contradiction) or unknown.", "labels": [], "entities": []}, {"text": "The gold answer is set to yes.", "labels": [], "entities": []}, {"text": "The parsing and inference system of ccg2lambda achieved high precision in RTE tasks;  reported that the precision was nearly 100% for the SICK dataset).", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9609786868095398}, {"text": "ccg2lambda", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.94119793176651}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.998722493648529}, {"text": "RTE tasks", "start_pos": 74, "end_pos": 83, "type": "TASK", "confidence": 0.7966918051242828}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9991238713264465}, {"text": "SICK dataset", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.866945892572403}]}, {"text": "Thus, a predicted entailment (yes) judgement can serve as a reliable measure for evaluating the entailment relation between S 1 and S 2 . shows BLEU scores and RTE accuracy.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 144, "end_pos": 155, "type": "METRIC", "confidence": 0.9749594628810883}, {"text": "RTE", "start_pos": 160, "end_pos": 163, "type": "METRIC", "confidence": 0.9928025603294373}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.7373859286308289}]}, {"text": "Here, token and graph show the results fora token-based model with attention and the graphbased model with attention, respectively, and +mask means the model with masking.", "labels": [], "entities": []}, {"text": "The baseline is shown by rule, which is the performance of Treebank Semantics.", "labels": [], "entities": []}, {"text": "As shown here, all the models outperformed the baseline with respect to both BLEU score and RTE accuracy.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9739296734333038}, {"text": "RTE", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9951465725898743}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.7090420722961426}]}, {"text": "For the RTE accuracy, the increase in the score of the graph + mask model was slightly larger than the increase for the token + mask model.", "labels": [], "entities": [{"text": "RTE", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.8087767362594604}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.8614048957824707}]}, {"text": "shows examples of decoded sentences Input sentence (S 1 ) Decoded sentence (S 2 ) (1) the girls are swimming in the ocean.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Information about sequences.", "labels": [], "entities": []}]}