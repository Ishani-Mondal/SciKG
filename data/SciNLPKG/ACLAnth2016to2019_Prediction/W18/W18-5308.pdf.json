{"title": [{"text": "UNCC QA: A Biomedical Question Answering System", "labels": [], "entities": [{"text": "UNCC QA", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.712764173746109}, {"text": "Biomedical Question Answering", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.6638201971848806}]}], "abstractContent": [{"text": "In this paper, we detail our submission to the BioASQ competition's Biomedical Semantic Question and Answering task.", "labels": [], "entities": [{"text": "BioASQ competition's Biomedical Semantic Question and Answering task", "start_pos": 47, "end_pos": 115, "type": "TASK", "confidence": 0.6319354871908823}]}, {"text": "Our system uses extractive summarization techniques to generate answers and has scored highest ROUGE-2 and Rogue-SU4 in all test batch sets.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9976410865783691}]}, {"text": "Our contributions are named-entity based method for answering factoid and list questions , and an extractive summarization techniques for building paragraph-sized summaries , based on lexical chains.", "labels": [], "entities": [{"text": "answering factoid and list questions", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.816618025302887}]}, {"text": "Our system got highest ROUGE-2 and ROUGE-SU4 scores for ideal-type answers in all test batch sets.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9984738230705261}, {"text": "ROUGE-SU4", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9968908429145813}]}, {"text": "We also discuss the limitations of the described system, such lack of the evaluation on other criteria (e.g. manual).", "labels": [], "entities": []}, {"text": "Also, for factoid-and list-type question our system got low accuracy (which suggests that our algorithm needs to improve in the ranking of entities).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9994090795516968}]}], "introductionContent": [{"text": "Most of the recent question answering (QA) systems produce either factoid type answers (typically, a phrase or a short sentence) or a summary (typically, returning a few sentences or passages from the text).", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.8671031713485717}]}, {"text": "Creating a natural language answer from relevant passages is still an open problem.", "labels": [], "entities": []}, {"text": "Our paper presents is also about providing factoid and summary answers in BioASQ.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.7408498525619507}]}, {"text": "BioASQ is a research competition which is organized by tracks in the biomedical domain.", "labels": [], "entities": []}, {"text": "Namely, large-scale online biomedical semantic indexing, biomedical semantic question answering, and information extraction from biomedical literature.", "labels": [], "entities": [{"text": "large-scale online biomedical semantic indexing", "start_pos": 8, "end_pos": 55, "type": "TASK", "confidence": 0.6456436455249787}, {"text": "biomedical semantic question answering", "start_pos": 57, "end_pos": 95, "type": "TASK", "confidence": 0.7293395400047302}, {"text": "information extraction from biomedical literature", "start_pos": 101, "end_pos": 150, "type": "TASK", "confidence": 0.8298570156097412}]}, {"text": "The biomedical QA task is organized in two phases.", "labels": [], "entities": [{"text": "biomedical QA task", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8128507137298584}]}, {"text": "Phase A deals with retrieval of the relevant document, snippets, concepts, and RDF triples, and phase B deals with exact and ideal answer generations.", "labels": [], "entities": []}, {"text": "Exact answer generation is required for factoid, list, and yes/no type question.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.7318886667490005}]}, {"text": "ideal answer is required for all the question.", "labels": [], "entities": []}, {"text": "An ideal answer is a paragraph-sized summary of snippets.", "labels": [], "entities": []}, {"text": "BioASQ competition provides the train and test dataset.", "labels": [], "entities": [{"text": "BioASQ competition", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.8687595427036285}]}, {"text": "The training dataset consists of questions, golden standard documents, concepts, and ideal answers.", "labels": [], "entities": []}, {"text": "The test dataset is split between phase A and phase B.", "labels": [], "entities": []}, {"text": "The phase A dataset consists of the questions, unique ids, question types.", "labels": [], "entities": []}, {"text": "The phase B dataset consists of the questions, golden standard documents and snippets, unique ids, and question types.", "labels": [], "entities": []}, {"text": "Exact answers for factoid type questions are evaluated using strict accuracy, lenient accuracy, and MRR (Mean Reciprocal Rank).", "labels": [], "entities": [{"text": "factoid type questions", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.8148971001307169}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9625211954116821}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9847277402877808}, {"text": "MRR", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9986851811408997}, {"text": "Mean Reciprocal Rank)", "start_pos": 105, "end_pos": 126, "type": "METRIC", "confidence": 0.9074223041534424}]}, {"text": "Answers for the list type question are evaluated based on precision, recall, and F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.999663233757019}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9995083808898926}, {"text": "F-measure", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9984918832778931}]}, {"text": "ideal answers are evaluated using automatic and manual scores.", "labels": [], "entities": []}, {"text": "Automatic evaluation scores consist of ROUGE-2 and ROUGE-SU4 and manual evaluation is done by measuring readability, repetition, recall, and precision.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9893187880516052}, {"text": "ROUGE-SU4", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.8924005627632141}, {"text": "repetition", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.9986522793769836}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9992208480834961}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.998675525188446}]}, {"text": "In this paper, we present our submission for BioASQ competition.", "labels": [], "entities": [{"text": "BioASQ competition", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.5415610373020172}]}, {"text": "We describe two methods, evaluated on two BioASQ tasks: ideal answer and factoid type questions.", "labels": [], "entities": []}, {"text": "Both methods use conceptual representations based on MetaMap and UMLS.", "labels": [], "entities": []}, {"text": "We compute answers by choosing sentences with the concept chains that are similar to concepts in the question.", "labels": [], "entities": []}, {"text": "In factoid questions, additionally, our method selects the entities with the highest idf scores.", "labels": [], "entities": []}, {"text": "The first method obtains the best rank for test batch 2,3 and 5 of Phase B of Task 6B.", "labels": [], "entities": []}, {"text": "The second method was evaluated on previous year tests with mediocre results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of ideal answers on task 4B and 5B  test batch sets using BioASQ oracle. The results are  arranged from most recent to least. The table shows  ROUGE-2 and ROUGE-SU4 scores.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 161, "end_pos": 168, "type": "METRIC", "confidence": 0.9863640666007996}, {"text": "ROUGE-SU4", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9235318899154663}]}, {"text": " Table 2: Results of ideal answers on task 6B test  batch sets using BioASQ oracle. The table shows  ROUGE-2 and ROUGE-SU4 scores of UNCC System  1 and 2. UNCC System 1 submitted the summary cre- ated with 200 word limit. UNCC System 2 submitted  summary created by selecting only top sentence.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.9943371415138245}, {"text": "ROUGE-SU4", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9851741194725037}, {"text": "UNCC System  1", "start_pos": 133, "end_pos": 147, "type": "DATASET", "confidence": 0.9072227478027344}, {"text": "UNCC System 2", "start_pos": 222, "end_pos": 235, "type": "DATASET", "confidence": 0.9087305068969727}]}, {"text": " Table 3: Result of factoid type question on task 5B  of BioASQ. The scores include the Lenient accuracy  LAcc, strict accuracy SAcc, and MRR(Mean Recipro- cal Rank).", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.8734144568443298}, {"text": "Lenient accuracy  LAcc", "start_pos": 88, "end_pos": 110, "type": "METRIC", "confidence": 0.6893138488133749}, {"text": "strict accuracy SAcc", "start_pos": 112, "end_pos": 132, "type": "METRIC", "confidence": 0.8564814726511637}, {"text": "MRR", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.9985805749893188}, {"text": "Mean Recipro- cal Rank)", "start_pos": 142, "end_pos": 165, "type": "METRIC", "confidence": 0.8652669688065847}]}, {"text": " Table 4: Result of list type question on task 5B of  BioASQ. The results are evaluated on Mean Precision,  Recall, and F-measure.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.7211105227470398}, {"text": "Mean", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9922507405281067}, {"text": "Precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.5029597282409668}, {"text": "Recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.985158383846283}, {"text": "F-measure", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9971729516983032}]}, {"text": " Table 5: Results of ideal answers on task 3B, 2B and  1B test batch sets using BioASQ oracle. The results  are arranged from most recent to least. The table shows  ROUGE-2 and ROUGE-SU4 scores.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 165, "end_pos": 172, "type": "METRIC", "confidence": 0.984119176864624}, {"text": "ROUGE-SU4", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9362871646881104}]}]}