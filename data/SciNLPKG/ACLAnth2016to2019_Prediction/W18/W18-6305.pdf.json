{"title": [{"text": "Discourse-Related Language Contrasts in English-Croatian Human and Machine Translation", "labels": [], "entities": [{"text": "Discourse-Related Language Contrasts in English-Croatian Human and Machine Translation", "start_pos": 0, "end_pos": 86, "type": "TASK", "confidence": 0.617306176159117}]}], "abstractContent": [{"text": "We present an analysis of a number of coref-erence phenomena in English-Croatian human and machine translations.", "labels": [], "entities": [{"text": "English-Croatian human and machine translations", "start_pos": 64, "end_pos": 111, "type": "TASK", "confidence": 0.6127466797828675}]}, {"text": "The aim is to shed light on the differences in the way these structurally different languages make use of discourse information and provide insights for discourse-aware machine translation system development.", "labels": [], "entities": [{"text": "discourse-aware machine translation system development", "start_pos": 153, "end_pos": 207, "type": "TASK", "confidence": 0.7340228736400605}]}, {"text": "The phenomena are automatically identified in parallel data using annotation produced by parsers and word alignment tools, enabling us to pinpoint patterns of interest in both languages.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.7214258164167404}]}, {"text": "We make the analysis more fine-grained by including three corpora pertaining to three different registers.", "labels": [], "entities": []}, {"text": "Ina second step, we create a test set with the challenging linguistic constructions and use it to evaluate the performance of three MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.9878861904144287}]}, {"text": "We show that both SMT and NMT systems struggle with handling these discourse phenomena, even though NMT tends to perform somewhat better than SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9637106657028198}, {"text": "SMT", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.867756724357605}]}, {"text": "By providing an overview of patterns frequently occurring in actual language use, as well as by pointing out the weaknesses of current MT systems that commonly mistranslate them, we hope to contribute to the effort of resolving the issue of discourse phenomena in MT applications.", "labels": [], "entities": [{"text": "MT", "start_pos": 135, "end_pos": 137, "type": "TASK", "confidence": 0.9692487120628357}, {"text": "MT", "start_pos": 264, "end_pos": 266, "type": "TASK", "confidence": 0.9831059575080872}]}], "introductionContent": [{"text": "Every natural language has means of marking elements belonging to the same coreference chain in order to achieve cohesion and coherence in running text.", "labels": [], "entities": []}, {"text": "These discourse phenomena are crucial for understanding texts and their misrepresentation harms text intelligibility.", "labels": [], "entities": []}, {"text": "Despite their significance, machine translation (MT) systems have been known to struggle with adequately transferring coreference relations from the source to the target language, which is partly due to the great differences in the way languages express these relations.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8428443014621735}]}, {"text": "In our approach, we extend the framework introduced by, who identify discourse discrepancies in parallel data for English and German by predefining and automatically extracting discourse patterns of interest, and then utilize word alignment information to determine which of the extracted phenomena lack the equivalent counterpart in the other language.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 226, "end_pos": 240, "type": "TASK", "confidence": 0.7090582251548767}]}, {"text": "We use the same procedure to automatically extract phenomena, but extend the methodology to include cases where the phenomenon does have an equivalent construction in the other language, despite the alignment data suggesting that it is more frequently left unaligned.", "labels": [], "entities": []}, {"text": "In this research, we perform an in-depth study of the way in which diverse discourse phenomena are handled in translation from English to Croatian.", "labels": [], "entities": []}, {"text": "We investigate both human translation and the output of different types of MT systems.", "labels": [], "entities": [{"text": "human translation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.6974731236696243}, {"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9900070428848267}]}, {"text": "In the first step, we use the extended methodology of Lapshinova- to extract interesting diverging discourse patterns that commonly occur in the parallel data.", "labels": [], "entities": []}, {"text": "While reflections on the relevant linguistic intuitions are given as a reference, the selection of the phenomena chosen for further examination is primarily based on the data obtained from corpora.", "labels": [], "entities": []}, {"text": "This makes our approach strongly usage-based and provides ample space for making observations unconstrained by a particular theoretical framework.", "labels": [], "entities": []}, {"text": "In the second step, we construct a dataset with sentences containing challenging discourse phenomena identified in the analysis of human translations.", "labels": [], "entities": [{"text": "analysis of human translations", "start_pos": 119, "end_pos": 149, "type": "TASK", "confidence": 0.6149256750941277}]}, {"text": "The constructed dataset can be used for further research in the field of corpus linguistics and translation studies, but it is also useful for gaining insight about language contrasts that is of relevance to MT researchers.", "labels": [], "entities": [{"text": "translation studies", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.9125606417655945}, {"text": "MT", "start_pos": 208, "end_pos": 210, "type": "TASK", "confidence": 0.9829195737838745}]}, {"text": "We therefore use it to test and evaluate the performance of several types of MT systems and to that end devise a weighted error classification, tailored to accommodate the complexity of the problem at hand.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.988781213760376}]}, {"text": "The paper is structured as follows: in Section 2 we explain the motivation for the research and 36 in Section 3 we give an overview of related work.", "labels": [], "entities": []}, {"text": "In Section 4 we describe the used parallel corpora and present the approach to and findings of their analysis.", "labels": [], "entities": []}, {"text": "In Section 5 we describe the MT experiment and our approach to error classification.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9867447018623352}, {"text": "error classification", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.6564055532217026}]}, {"text": "Section 6 contains a discussion of the obtained results and the paper ends with concluding remarks and ideas for future research in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "After analysing the parallel data and identifying interesting tendencies regarding the coreference Encoder: 3-layer bidirectional LSTM, hidden size 500.", "labels": [], "entities": []}, {"text": "Decoder: 3-layer LSTM, hidden size 500.", "labels": [], "entities": []}, {"text": "phenomena, we wanted to see how they were handled by different types of MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9786943793296814}]}, {"text": "Using our linguistic patterns, we extracted a subset of sentences, targeting the constructions that are handled differently by the two languages.", "labels": [], "entities": []}, {"text": "The number of sentences per phenomenon corresponds to the overall frequency of their occurrence, while the proportion of sentences taken from each corpus roughly reflects the differences in corpus sizes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overall number of occurrences of each phenomenon in the respective language per corpus.", "labels": [], "entities": []}, {"text": " Table 2: MT systems -training configurations.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9718170166015625}]}, {"text": " Table 3: MT error classification.", "labels": [], "entities": [{"text": "MT error classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.826863706111908}]}, {"text": " Table 4: Percentage of acceptable translations out of the total", "labels": [], "entities": []}, {"text": " Table 5: Overall number of acceptable and unacceptable", "labels": [], "entities": []}, {"text": " Table 6: Total scores and percentages of acceptable translations for each system per phenomenon.", "labels": [], "entities": []}]}