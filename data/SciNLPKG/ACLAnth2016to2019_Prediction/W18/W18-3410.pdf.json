{"title": [], "abstractContent": [{"text": "Various common deep learning architec-tures, such as LSTMs, GRUs, Resnets and Highway Networks, employ state passthrough connections that support training with high feed-forward depth or recurrence over many time steps.", "labels": [], "entities": []}, {"text": "These \"Passthrough Networks\" architec-tures also enable the decoupling of the network state size from the number of parameters of the network, a possibility has been studied by Sak et al.", "labels": [], "entities": []}, {"text": "(2014) with their low-rank parametrization of the LSTM.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.8812017440795898}]}, {"text": "In this work we extend this line of research, proposing effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity.", "labels": [], "entities": []}, {"text": "This is particularly beneficial in low-resource settings as it supports expressive models with a compact parametrization less susceptible to overfitting.", "labels": [], "entities": []}, {"text": "We present competitive experimental results on several tasks, including language modeling and a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7948750555515289}, {"text": "MNIST classification", "start_pos": 157, "end_pos": 177, "type": "TASK", "confidence": 0.8203689157962799}]}], "introductionContent": [], "datasetContent": [{"text": "We applied the Low-rank GRU (LR-GRU) and Low-rank plus diagonal GRU (LRD-GRU) architectures to a subset of sequential benchmarks described in the Unitary Evolution Recurrent Neural Networks (uRNN) article by, specifically the memory task, the addition task and the sequential randomly permuted MNIST task.", "labels": [], "entities": []}, {"text": "For the memory tasks, we also considered two different variants proposed by and which are hard for the uRNN architecture.", "labels": [], "entities": []}, {"text": "We chose to compare against the uRNN architecture because it set state of the art results in terms of both data complexity and accuracy and because it is an architecture with similar design objectives as low-rank passthrough architectures, namely a lowdimensional parametrization and the mitigation of the vanishing gradient problem, but it is based on quite different principles.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9976599216461182}]}, {"text": "We also applied these architectures to a character-level language modeling task on the Penn Treebank corpus.", "labels": [], "entities": [{"text": "character-level language modeling", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.6742478211720785}, {"text": "Penn Treebank corpus", "start_pos": 87, "end_pos": 107, "type": "DATASET", "confidence": 0.9952289859453837}]}, {"text": "For the language modeling task, we also experimented with Low-rank plus diagonal LSTMs.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.8243632912635803}]}], "tableCaptions": [{"text": " Table 1: Sequential permuted MNIST results", "labels": [], "entities": [{"text": "Sequential permuted MNIST", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7875710129737854}]}, {"text": " Table 2: Character-level language modeling results", "labels": [], "entities": [{"text": "Character-level language modeling", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.7490835189819336}]}]}