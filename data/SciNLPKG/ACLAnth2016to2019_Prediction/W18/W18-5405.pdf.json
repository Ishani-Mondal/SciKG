{"title": [{"text": "Evaluating Textual Representations through Image Generation", "labels": [], "entities": [{"text": "Evaluating Textual Representations", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7957542141278585}]}], "abstractContent": [{"text": "We present a methodology for determining the quality of textual representations through the ability to generate images from them.", "labels": [], "entities": []}, {"text": "Continuous representations of textual input are ubiquitous in modern Natural Language Processing techniques either at the core of machine learning algorithms or as the by-product at any given layer of a neural network.", "labels": [], "entities": []}, {"text": "While current techniques to evaluate such representations focus on their performance on particular tasks, they don't provide a clear understanding of the level of informational detail that is stored within them, especially their ability to represent spatial information.", "labels": [], "entities": []}, {"text": "The central premise of this paper is that visual inspection or analysis is the most convenient method to quickly and accurately determine information content.", "labels": [], "entities": []}, {"text": "Through the use of text-to-image neural networks, we propose anew technique to compare the quality of textual representations by visualizing their information content.", "labels": [], "entities": []}, {"text": "The method is illustrated on a medical dataset where the correct representation of spatial information and shorthands are of particular importance.", "labels": [], "entities": []}, {"text": "For four different well-known tex-tual representations, we show with a quantitative analysis that some representations are consistently able to deliver higher quality vi-sualizations of the information content.", "labels": [], "entities": []}, {"text": "Additionally , we show that the quantitative analysis technique correlates with the judgment of a human expert evaluator in terms of alignment.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, a method is proposed to evaluate the quality of a textual representation by conditioning an image generation network on it.", "labels": [], "entities": []}, {"text": "Neural networks implicitly construct representations of a textual input by learning which features are important for the task at hand.", "labels": [], "entities": []}, {"text": "It is not immediately possible however to assess the level of detail and structure that is retained in such a representation.", "labels": [], "entities": [{"text": "detail", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9882424473762512}]}, {"text": "Many systems often complement or replace the input with pre-trained representations that have the advantage of being constructed with a larger unlabeled corpus.", "labels": [], "entities": []}, {"text": "Depending on the task, this practice sometimes significantly improves the performance of the network ().", "labels": [], "entities": []}, {"text": "On the one hand, this is due to the use of a larger unlabeled corpus which reduces data sparsity and thus improves generalization accuracy.", "labels": [], "entities": [{"text": "generalization", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.9471083879470825}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.8202673196792603}]}, {"text": "On the other hand, representations often contain higherlevel features that are fundamental for the task they are trained for.", "labels": [], "entities": []}, {"text": "A neural network in a separate task can thus rely on those features without having to discover them allover again.", "labels": [], "entities": []}, {"text": "As the field of Natural Language Processing advances and machine learning models expand to include multimodal information, the importance of understanding the level of detail and information that is retained in a textual representation only grows.", "labels": [], "entities": []}, {"text": "Obtained representations can be employed in additional tasks (for example generation, translation, summarization, etc.) depending on their ability to capture certain types of information.", "labels": [], "entities": [{"text": "translation", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.6646373271942139}, {"text": "summarization", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.7087841629981995}]}, {"text": "The medical domain in particular might benefit from a better understanding of representations as the industry moves to adopt deep learning methods in increasingly intricate applications and researchers attempt to extract and utilize more complex information structures.", "labels": [], "entities": []}, {"text": "An example is spatial information which is an important quantity in many natural language applications, yet no explicit methodology exists that indicates to what extent that information is present in textual representations.", "labels": [], "entities": []}, {"text": "In many medical settings, a correct understanding and representation of such information is crucial.", "labels": [], "entities": []}, {"text": "In thorax radiography, which is the focus of this paper, textual captions often include detailed findings which relate to specific areas in an X-Ray.", "labels": [], "entities": []}, {"text": "Clinical texts in general, add an extra level of com-plexity as they often lack syntactic structure and employ many shorthands.", "labels": [], "entities": []}, {"text": "Images differ from texts in the sense that the retained information and generalization of a representation are immediately apparent fora human observer.", "labels": [], "entities": []}, {"text": "It is not surprising that the 'human perceptual score' is a frequently used metric to evaluate image generation systems.", "labels": [], "entities": [{"text": "image generation", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7245083451271057}]}, {"text": "In this paper we propose a novel method to assess the quality of textual representations.", "labels": [], "entities": []}, {"text": "By creating images from different textual representations we show that some representations lack the necessary information to lead to detailed high-quality images.", "labels": [], "entities": []}, {"text": "The textual representations are evaluated both by comparing the quality of the produced images compared to the images in the test data, as well as the alignment between images and captions.", "labels": [], "entities": []}, {"text": "The outcome is determined both by a qualitative (human perceptual scores) as well as a quantitative (divergence scores) measure.", "labels": [], "entities": []}, {"text": "To calculate the divergence scores, we rely on the methodology that estimates distance between two distributions as introduced by and extend it to estimate how well image and text are aligned in the generated content.", "labels": [], "entities": []}, {"text": "As we show in the results, text-to-image architectures are indeed suitable to get an immediate visual estimate of the quality of the representation and the information contained within.", "labels": [], "entities": []}, {"text": "We will evaluate several common textual representations that were constructed with unsupervised learning techniques on both a relatively straightforward conditional GAN as well as on a more advanced StackGan ( ) which uses several stages and a conditioning mechanism that augments the textual representation.", "labels": [], "entities": []}, {"text": "The contributions of this paper are: \u2022 The formulation of a methodology to visualize and evaluate the information and quality of different textual representations.", "labels": [], "entities": []}, {"text": "\u2022 The extension of a GAN evaluation measure to evaluate alignment of output with conditional information.", "labels": [], "entities": []}], "datasetContent": [{"text": "As we produce images from text to determine the quality of the textual representations, accurate evaluation measures are needed to assess the generated images.", "labels": [], "entities": []}, {"text": "We focus on evaluation measures for GANs as it is the only type of architecture that is used to create images in this paper.", "labels": [], "entities": [{"text": "GANs", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.9573779702186584}]}, {"text": "Besides human perceptual scores, some recent advances have been made to assess the quality of the distribution of the generated output of GANs.", "labels": [], "entities": []}, {"text": "Some of the most widely adopted measures are the Inception Score (IS) ( and the Fr\u00e9chet Inception Distance (FID) (.", "labels": [], "entities": [{"text": "Inception Score (IS)", "start_pos": 49, "end_pos": 69, "type": "METRIC", "confidence": 0.9362948656082153}, {"text": "Fr\u00e9chet Inception Distance (FID)", "start_pos": 80, "end_pos": 112, "type": "METRIC", "confidence": 0.8348358869552612}]}, {"text": "Both measures have a reasonable correlation with image quality but also contain undesirable properties as explained by.", "labels": [], "entities": []}, {"text": "One large problem is that both use a third-party network that was trained on a different dataset to measure the quality of the generated data.", "labels": [], "entities": []}, {"text": "It therefore assumes that the distribution of the dataset used in the generation task is similar to the dataset that the third-party network was trained on.", "labels": [], "entities": []}, {"text": "This assumption is often not fulfilled, particularly if specialized medical datasets are used.", "labels": [], "entities": []}, {"text": "To solve these issues, propose using divergence and distance functions that are normally used for training a GAN.", "labels": [], "entities": [{"text": "GAN", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9018255472183228}]}, {"text": "show that these metrics exhibit consistency across various models and find that they better reflect human perceptual scores than the IS and FID.", "labels": [], "entities": [{"text": "FID", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9637376666069031}]}, {"text": "To calculate how well the generated distribution has approached the data distribution, an independent critic is trained until convergence to distinguish between generated samples and samples from the validation set.", "labels": [], "entities": []}, {"text": "The WGAN loss is used and the weights of the original generator are no longer updated.", "labels": [], "entities": [{"text": "WGAN loss", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.5971121788024902}]}, {"text": "When applied to output images, the Wasserstein distance thus can give an estimate of the divergence between the generated and real images.", "labels": [], "entities": []}, {"text": "This quantity is expressed as W qual image in Equation 2.", "labels": [], "entities": [{"text": "W qual image", "start_pos": 30, "end_pos": 42, "type": "METRIC", "confidence": 0.8829602599143982}]}, {"text": "where P r,v refers to the real distribution of the validation data.", "labels": [], "entities": []}, {"text": "Additionally, by evaluating the model that is trained in Equation 2 on the training and test set, suggest a method to estimate whether overfitting has occurred.", "labels": [], "entities": []}, {"text": "Indeed, if the model generalizes well to the unseen examples in the testset, the expected values in Equation 3 should be roughly the same.", "labels": [], "entities": [{"text": "Equation", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8987132906913757}]}, {"text": "In this equation P r,te and P r,tr refer to the real distributions of the test and training set respectively.", "labels": [], "entities": []}, {"text": "While this method allows us to judge the output quality of the images, and by extension the textual representations, in the following section we will explain how our methodology extends this approach in order to evaluate the alignment between image and text.", "labels": [], "entities": []}, {"text": "The used dataset is the chest X- words are padded to equal length, with a maximum of 30 words.", "labels": [], "entities": []}, {"text": "All words are lowercase and words with a frequency of less than 5 occurrences are removed and replaced by an out-of-vocabulary marker.", "labels": [], "entities": []}, {"text": "While the dataset also contains diagnosis labels for each image, they are not used in this paper.", "labels": [], "entities": []}, {"text": "The dataset is divided into training, validation and test set with 80%, 10% and 10% of the data respectively.", "labels": [], "entities": []}, {"text": "For the experiments we first create four different textual representations on the captions of the training set, as detailed in section 3.1.", "labels": [], "entities": []}, {"text": "Those representations are referred to as word2vec (sum), word2vec (concat), autoencoder and ARAE.", "labels": [], "entities": [{"text": "ARAE", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.5550822615623474}]}, {"text": "To illustrate the methodology, we set the fixed dimension of each representation to 300, which is a standard dimension for such embeddings, initially used by in their analysis of distributed vectors.", "labels": [], "entities": []}, {"text": "For the autoencoder and the ARAE, training is stopped when the validation error of the reconstruction is minimal.", "labels": [], "entities": [{"text": "ARAE", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.6971320509910583}]}, {"text": "To generate images from the text, the TTI-GAN and StackGAN models are used as explained in section 3.2.", "labels": [], "entities": [{"text": "TTI-GAN", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.6456714272499084}]}, {"text": "The latter produces images with higher resolution than the former approach.", "labels": [], "entities": []}, {"text": "This is important as a higher resolution is required to make an accurate assessment about the alignment of the X-Ray images to the captions.", "labels": [], "entities": []}, {"text": "The expected outcome is that a textual representation that maintains sequential information performs better than one that does not.", "labels": [], "entities": []}, {"text": "Additionally we expect a code that lies on a regularized smooth space, such as the code produced by the ARAE, to be more useful than a code that does not.", "labels": [], "entities": [{"text": "ARAE", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.8517105579376221}]}, {"text": "Finally, we perform two types of experiments, for which the concrete setup is as follows.", "labels": [], "entities": []}, {"text": "1. As GAN training can be unstable, the TTI-GAN is trained 10 times for each represen-  2.", "labels": [], "entities": [{"text": "TTI-GAN", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.6403365731239319}]}, {"text": "For the StackGAN, we train one model for each representation, and train an independent critic 5 times for each model.", "labels": [], "entities": []}, {"text": "As GAN training can be quite unstable, this experiment does not allow us to judge the value of the representations from just one run.", "labels": [], "entities": [{"text": "GAN", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.968329668045044}]}, {"text": "However, we compare our estimates for W qual image and W align im txt to the evaluation of a trained clinician, to confirm that our methodology correlates with human judgment, both in terms of quality and alignment.", "labels": [], "entities": []}, {"text": "For the first stage of the StackGAN we produce 64x64 pixel images, while the second stage outputs higher resolution 256x256 pixel images.", "labels": [], "entities": []}, {"text": "For this experiment, \u03bb was set to 0.05 and c was set to 0.01.", "labels": [], "entities": []}, {"text": "The text-to-image architectures are each trained during 120 epochs for each of the textual representations of the captions in the training set.", "labels": [], "entities": []}, {"text": "The image quality is then assessed on the images that are generated from the captions of the validation and test set.", "labels": [], "entities": []}, {"text": "This ensures that we check whether the learned representations can generalize well to captions that were never seen during their construction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Quantitative results of 10 runs for the  TTI-GAN visualization method for each of the  representations. A lower W qual image implies a  better image quality. (*) For both the autoencoder  and ARAE, an outlier was removed.", "labels": [], "entities": [{"text": "Quantitative", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9776906371116638}, {"text": "ARAE", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.6080158948898315}]}, {"text": " Table 2. Quantitative results for the trained Stage- 2 StackGAN visualization method for each of  the representations. A lower W qual image and  W align im txt imply a better image quality and  alignment respectively.", "labels": [], "entities": [{"text": "alignment", "start_pos": 195, "end_pos": 204, "type": "METRIC", "confidence": 0.9028422236442566}]}]}