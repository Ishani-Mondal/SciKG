{"title": [{"text": "Inducing Grammars with and for Neural Machine Translation", "labels": [], "entities": [{"text": "Inducing Grammars", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9157138764858246}, {"text": "Neural Machine Translation", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.7503871520360311}]}], "abstractContent": [{"text": "Machine translation systems require semantic knowledge and grammatical understanding.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.741655558347702}]}, {"text": "Neural machine translation (NMT) systems often assume this information is captured by an attention mechanism and a decoder that ensures fluency.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8063290218512217}]}, {"text": "Recent work has shown that incorporating explicit syntax alleviates the burden of modeling both types of knowledge.", "labels": [], "entities": []}, {"text": "However, requiring parses is expensive and does not explore the question of what syntax a model needs during translation.", "labels": [], "entities": []}, {"text": "To address both of these issues we introduce a model that simultaneously translates while inducing dependency trees.", "labels": [], "entities": []}, {"text": "In this way, we leverage the benefits of structure while investigating what syntax NMT must induce to maximize performance.", "labels": [], "entities": []}, {"text": "We show that our dependency trees are 1.", "labels": [], "entities": []}, {"text": "language pair dependent and 2.", "labels": [], "entities": []}, {"text": "1 Motivation Language has syntactic structure and translation models need to understand grammatical dependencies to resolve the semantics of a sentence and preserve agreement (e.g., number, gender, etc).", "labels": [], "entities": []}, {"text": "Many current approaches to MT have been able to avoid explicitly providing structural information by relying on advances in sequence to sequence (seq2seq) models.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9949501752853394}]}, {"text": "The most famous advances include attention mechanisms (Bahdanau et al., 2015) and gating in Long Short-Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997).", "labels": [], "entities": []}, {"text": "In this work we aim to benefit from syntactic structure, without providing it to the model, and to disentangle the semantic and syntactic components of translation, by introducing a gating mechanism which controls when syntax should be used.", "labels": [], "entities": []}, {"text": "The boy sitting next to the girls ordered a coffee Figure 1: Our model aims to capture both: syntactic (verb ordered \u2192 subj/obj boy, coffee) alignment (noun girls \u2192 determiner the) attention.", "labels": [], "entities": []}, {"text": "Consider the process of translating the sentence \"The boy sitting next to the girls ordered a coffee .\" (Figure 1) from English to German.", "labels": [], "entities": [{"text": "translating", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9710790514945984}]}, {"text": "In Ger-man, translating ordered, requires knowledge of its subject boy to correctly predict the verb's number bestellte instead of bestellten.", "labels": [], "entities": [{"text": "translating ordered", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.9010013937950134}]}, {"text": "This is a case where syntactic agreement requires long-distance information.", "labels": [], "entities": [{"text": "syntactic agreement", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8307429850101471}]}, {"text": "On the other hand, next can be translated in isolation.", "labels": [], "entities": []}, {"text": "The model should uncover these relationships and decide when and which aspects of syntax are necessary.", "labels": [], "entities": []}, {"text": "While in principle decoders can utilize previously predicted words (e.g., the translation of boy) to reason about subject-verb agreement, in practice LSTMs still struggle with long-distance dependencies.", "labels": [], "entities": [{"text": "translation of boy)", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7673027068376541}]}, {"text": "Moreover, Belinkov et al.", "labels": [], "entities": []}, {"text": "(2017) showed that using attention reduces the decoder's capacity to learn target side syntax.", "labels": [], "entities": []}, {"text": "In addition to demonstrating improvements in translation quality, we are also interested in analyzing the predicted dependency trees discovered by our models.", "labels": [], "entities": []}, {"text": "Recent work has begun analyzing task-specific latent trees (Williams et al., 2018).", "labels": [], "entities": []}, {"text": "We present the first results on learning latent trees with a joint syntactic-semantic objective.", "labels": [], "entities": []}, {"text": "We do this in the service of machine translation which inherently requires access to both aspects of a sentence.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7274982780218124}]}, {"text": "Further , our results indicate that language pairs with rich morphology require and therefore induce more complex syntactic structure.", "labels": [], "entities": []}, {"text": "Our use of a structured self attention encoder (\u00a74) that predicts a non-projective dependency tree", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Next we will discuss our experimental setup and report results for English\u2194German (En\u2194De), English\u2194Russian (En\u2194Ru), and Russian\u2192Arabic (Ru\u2192Ar) translation models.", "labels": [], "entities": [{"text": "Russian\u2192Arabic (Ru\u2192Ar) translation", "start_pos": 120, "end_pos": 154, "type": "TASK", "confidence": 0.5598446627457937}]}], "tableCaptions": [{"text": " Table 2: Results for translating En\u2194De, En\u2194Ru, and Ru\u2192Ar. Statistical significances are marked as   \u2020 p < 0.05 and  \u2021 p < 0.01 when compared against the baselines and / when compared against the  FA-NMT (no-shared). The results indicate the strength of our proposed shared-attention for NMT.", "labels": [], "entities": [{"text": "translating En\u2194De", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8359692096710205}, {"text": "FA-NMT", "start_pos": 197, "end_pos": 203, "type": "METRIC", "confidence": 0.9704923629760742}]}, {"text": " Table 3: Directed and Undirected (DA/UA) model accuracy (without punctuation) compared to branching  baselines: left (L), right (R) and undirected (Un). Our results show an intriguing effect of the target  language on induction. Note the accuracy discrepancy between translating RU to EN versus AR.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9945966005325317}, {"text": "accuracy", "start_pos": 239, "end_pos": 247, "type": "METRIC", "confidence": 0.9986478686332703}]}]}