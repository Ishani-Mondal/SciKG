{"title": [{"text": "Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation", "labels": [], "entities": [{"text": "Collecting Diverse Natural Language Inference", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.9161084651947021}, {"text": "Sentence Representation Evaluation", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.9595040678977966}]}], "abstractContent": [{"text": "We present a large scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a sentence representation encoded by a neural network captures distinct types of reasoning.", "labels": [], "entities": []}, {"text": "The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total.", "labels": [], "entities": []}, {"text": "Our collection of diverse datasets is available at http://www.decomp.net/, and will grow overtime as additional resources are recast and added from novel sources.", "labels": [], "entities": []}], "introductionContent": [{"text": "A plethora of new natural language inference (NLI) 1 datasets has been created in recent years (.", "labels": [], "entities": []}, {"text": "However, these datasets do not provide clear insight into what type of reasoning or inference a model maybe performing.", "labels": [], "entities": []}, {"text": "For example, these datasets cannot be used to evaluate whether competitive NLI models can determine if an event occurred, correctly differentiate between figurative and literal language, or accurately identify and categorize named entities.", "labels": [], "entities": []}, {"text": "Consequently, these datasets cannot answer how well sentence representation learning models capture distinct semantic phenomena necessary for general natural language understanding (NLU).", "labels": [], "entities": [{"text": "general natural language understanding (NLU)", "start_pos": 142, "end_pos": 186, "type": "TASK", "confidence": 0.7423153604779925}]}, {"text": "To answer these questions, we introduce the Diverse NLI Collection (DNC), a large-scale NLI dataset that tests a model's ability to perform diverse types of reasoning.", "labels": [], "entities": []}, {"text": "DNC is a collection of NLI problems, each requiring a model to perform a unique type of reasoning.", "labels": [], "entities": []}, {"text": "Each NLI dataset contains labeled context-hypothesis pairs that we re- The task of determining if a hypothesis would likely be inferred from a context, or premise; also known as Recognizing Textual Entailment (RTE) ( cast from semantic annotations for specific structured prediction tasks.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 178, "end_pos": 214, "type": "TASK", "confidence": 0.6815748612085978}]}, {"text": "includes a sample of NLI pairs that test specific types of reasoning.", "labels": [], "entities": []}, {"text": "We extend various prior works on challenge NLI datasets (, and define recasting as leveraging existing datasets to create NLI examples.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.7235123217105865}]}, {"text": "We recast annotations from a total of 13 datasets across 7 NLP tasks into labeled NLI examples.", "labels": [], "entities": []}, {"text": "The tasks include event factuality, named entity recognition, gendered anaphora resolution, sentiment analysis, relationship extraction, pun detection, and lexicosyntactic inference.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6508089105288187}, {"text": "gendered anaphora resolution", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.6288709839185079}, {"text": "sentiment analysis", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.9610291421413422}, {"text": "relationship extraction", "start_pos": 112, "end_pos": 135, "type": "TASK", "confidence": 0.8277316093444824}, {"text": "pun detection", "start_pos": 137, "end_pos": 150, "type": "TASK", "confidence": 0.72547747194767}]}, {"text": "Currently, DNC contains over half a million labeled examples that can be used to probe a model's ability to capture different types of semantic reasoning necessary for general NLU.", "labels": [], "entities": []}, {"text": "In short, this work answers a recent plea to the community to test \"more kinds of inference\" than in previous challenge sets ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}