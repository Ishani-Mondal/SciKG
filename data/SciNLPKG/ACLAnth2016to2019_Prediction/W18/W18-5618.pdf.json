{"title": [{"text": "In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition", "labels": [], "entities": [{"text": "Improve Biomedical Named Entity Recognition", "start_pos": 41, "end_pos": 84, "type": "TASK", "confidence": 0.7195547580718994}]}], "abstractContent": [{"text": "Rapidly expanding volume of publications in the biomedical domain makes it increasingly difficult fora timely evaluation of the latest literature.", "labels": [], "entities": []}, {"text": "That, along with a push for automated evaluation of clinical reports, present opportunities for effective natural language processing methods.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.6813755631446838}]}, {"text": "In this study we target the problem of named entity recognition, where texts are processed to annotate terms that are relevant for biomedical studies.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.6187154551347097}]}, {"text": "Terms of interest in the domain include gene and protein names, and cell lines and types.", "labels": [], "entities": []}, {"text": "Here we report on a pipeline built on Embeddings from Language Models (ELMo) and a deep learning package for natural language processing (Al-lenNLP).", "labels": [], "entities": []}, {"text": "We trained context-aware token em-beddings on a dataset of biomedical papers using ELMo, and incorporated these embeddings in the LSTM-CRF model used by AllenNLP for named entity recognition.", "labels": [], "entities": [{"text": "AllenNLP", "start_pos": 153, "end_pos": 161, "type": "DATASET", "confidence": 0.9684933423995972}, {"text": "named entity recognition", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.6209686398506165}]}, {"text": "We show these representations improve named entity recognition for different types of biomedical named entities.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6344190140565237}]}, {"text": "We also achieve anew state of the art in gene mention detection on the BioCreative II gene mention shared task.", "labels": [], "entities": [{"text": "gene mention detection", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7951973477999369}, {"text": "BioCreative II gene mention shared task", "start_pos": 71, "end_pos": 110, "type": "TASK", "confidence": 0.5417315463225046}]}], "introductionContent": [{"text": "Last decade witnessed substantial improvements in machine learning methods and their application to natural language processing tasks.", "labels": [], "entities": []}, {"text": "Recently, introduced ELMo (Embeddings from Language Models), a system for deep contextualized word representation, and showed how it can be used in existing task-specific deep neural networks.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.847851574420929}, {"text": "deep contextualized word representation", "start_pos": 74, "end_pos": 113, "type": "TASK", "confidence": 0.6416966691613197}]}, {"text": "The method improves the state of the art over a variety of NLP tasks such as question answering, word sense disambiguation, sentiment analysis, and named entity recognition.", "labels": [], "entities": [{"text": "question answering", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.831570953130722}, {"text": "word sense disambiguation", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.7018498182296753}, {"text": "sentiment analysis", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.9594195187091827}, {"text": "named entity recognition", "start_pos": 148, "end_pos": 172, "type": "TASK", "confidence": 0.6500519712766012}]}, {"text": "The developers of the tool also provide an ELMo model pre-trained on the Billion-word Language Model (LM) dataset) as an off-theshelf tool for use in a wide variety of NLP tasks and domains.", "labels": [], "entities": [{"text": "Billion-word Language Model (LM) dataset", "start_pos": 73, "end_pos": 113, "type": "DATASET", "confidence": 0.6057554951735905}]}, {"text": "This begs the question of how the performance of downstream analysis would improve if the model were to be adapted to work with domainspecific texts.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the effect of an in-domain training set for ELMo in Named Entity Recognition (NER) applications.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 82, "end_pos": 112, "type": "TASK", "confidence": 0.821028600136439}]}, {"text": "Our contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "Off-the-shelf ELMo has room for improvement in domain-specific applications 2.", "labels": [], "entities": []}, {"text": "ELMo consistently improves biomedical named entity recognition when trained on indomain data 3.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6783115863800049}, {"text": "biomedical named entity recognition", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.6545889377593994}]}, {"text": "Such improvement can be achieved even when the in-domain training dataset is smaller than the Billion-word LM data.", "labels": [], "entities": [{"text": "Billion-word LM data", "start_pos": 94, "end_pos": 114, "type": "DATASET", "confidence": 0.5610645214716593}]}, {"text": "4. The resulting model achieves the highest precision/recall/F1 scores so far on BioCreative II Gene mention detection shared task (BC2GM).", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9991094470024109}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.8549370169639587}, {"text": "F1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.8657853603363037}, {"text": "BioCreative II Gene mention detection shared task (BC2GM)", "start_pos": 81, "end_pos": 138, "type": "TASK", "confidence": 0.7344330728054047}]}, {"text": "We explain ELMo and AllenNER, the named entity recognizer we used, in sections 2 and section 3.", "labels": [], "entities": [{"text": "AllenNER", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.9734703302383423}]}, {"text": "Then, we describe our datasets in section 4, and we move onto report the results in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected a focused domain-specific subset of PubMed Central (PMC) documents, and used them for training ELMo.", "labels": [], "entities": [{"text": "PubMed Central (PMC) documents", "start_pos": 49, "end_pos": 79, "type": "DATASET", "confidence": 0.7513408064842224}]}, {"text": "This dataset is described in detail in section 4.1.", "labels": [], "entities": []}, {"text": "We report results on two benchmark datasets, which we describe in sections 4.2 and 4.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Leading results in the literature (up) in comparison with our results (down) on BC2GM dataset", "labels": [], "entities": [{"text": "BC2GM dataset", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.9268246591091156}]}, {"text": " Table 2: F1-scores (%) for different entity types in JNLPBA dataset", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9988905787467957}, {"text": "JNLPBA dataset", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.9440982043743134}]}]}