{"title": [{"text": "A Comparison of Character Neural Language Model and Bootstrapping for Language Identification in Multilingual Noisy Texts", "labels": [], "entities": [{"text": "Language Identification", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.7502589821815491}]}], "abstractContent": [{"text": "This paper seeks to examine the effect of including background knowledge in the form of character pre-trained neural language model (LM), and data bootstrapping to overcome the problem of unbalanced limited resources.", "labels": [], "entities": []}, {"text": "As a test, we explore the task of language identification in mixed-language short non-edited texts with an under-resourced language, namely the case of Algerian Arabic for which both labelled and unlabelled data are limited.", "labels": [], "entities": [{"text": "language identification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7035832852125168}]}, {"text": "We compare the performance of two traditional machine learning methods and a deep neural networks (DNNs) model.", "labels": [], "entities": []}, {"text": "The results show that overall DNNs perform better on labelled data for the majority categories and struggle with the minority ones.", "labels": [], "entities": []}, {"text": "While the effect of the untokenised and unlabelled data encoded as LM differs for each category, bootstrap-ping, however, improves the performance of all systems and all categories.", "labels": [], "entities": []}, {"text": "These methods are language independent and could be gener-alised to other under-resourced languages for which a small labelled data and a larger unla-belled data are available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most Natural Language Processing (NLP) tools are generally designed to deal with monolingual texts with more or less standardised spelling.", "labels": [], "entities": []}, {"text": "However, users in social media, especially in multilingual societies, generate multilingual nonedited material whereat least two languages or language varieties are used.", "labels": [], "entities": []}, {"text": "This phenomenon is linguistically referred to as language (code) mixing where code-switching and borrowing, among others, are the most studied phenomena.", "labels": [], "entities": [{"text": "language (code) mixing", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.6583597302436829}]}, {"text": "defined borrowing as a morphological or a phonological adaptation of a word from one language to another and code-switching as the use of a foreign word, as it is in its original language, to express something in another language.", "labels": [], "entities": [{"text": "borrowing as a morphological or a phonological adaptation of a word from one language to another and code-switching as the use of a foreign word, as it is in its original language, to express something in another language", "start_pos": 8, "end_pos": 229, "type": "Description", "confidence": 0.8504963263869285}]}, {"text": "However, the literature does not make it clear whether the use of different script is counted as borrowing, or code-switching or something else.", "labels": [], "entities": []}, {"text": "For instance, there is no linguistic wellmotivated theory about how to classify languages written in other scripts, like French written in Arabic script which is frequently the casein North Africa.", "labels": [], "entities": []}, {"text": "This theoretical gap could be explained by the fact that this fairly recent phenomenon has emerged with the widespread of the new technologies.", "labels": [], "entities": []}, {"text": "In this paper, we consider both codeswitching and borrowing and refer to them collectively as language mixing.", "labels": [], "entities": []}, {"text": "Our motivation in doing so is to offer to sociolinguists a linguistically informative tool to analyse and study the language contact behaviour in the included languages.", "labels": [], "entities": []}, {"text": "The task of identifying languages in mixedlanguage texts is a useful pre-processing tool where sequences belonging to different languages/varieties are identified.", "labels": [], "entities": []}, {"text": "They are then processed by further language/variety-specific tools and models.", "labels": [], "entities": []}, {"text": "This task itself has neither been well studied for situations when many languages are mixed nor has it been explored as a main or an auxiliary task in multi-task learning (see Section 2).", "labels": [], "entities": []}], "datasetContent": [{"text": "Deep learning has become the leading approach to solving linguistic tasks.", "labels": [], "entities": [{"text": "solving linguistic tasks", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.8223868012428284}]}, {"text": "However deep neural networks (DNNs) used in a supervised and unsupervised learning scenario usually require large datasets in order for the trained models to perform well.", "labels": [], "entities": []}, {"text": "For example, estimates that the size of the training dataset for character-level DNNs for text classification task should range from hundreds of thousands to several million of examples.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.8726377685864767}]}, {"text": "The limits imposed by the lack of labelled datasets have been countered by combining structural learning and semi-supervised learning (Ando and).", "labels": [], "entities": []}, {"text": "Contrary to the supervised approach where a labelled dataset is used to train a model, in structural learning, the learner first learns underlying structures from either labelled or unlabelled data.", "labels": [], "entities": []}, {"text": "If the model is trained on labelled data, it should be possible to reuse the knowledge encoded in the relations of the predictive features in this auxiliary task, if properly trained, to solve other related tasks.", "labels": [], "entities": []}, {"text": "If the model is trained on unlabelled data, the model captures the underlying structures of words or characters in a language as a language model (LM), i.e., model the probabilistic distribution of words and characters of a text.", "labels": [], "entities": []}, {"text": "Such pre-trained LM should be useful for various supervised tasks assuming that linguistic structures are predictive of the labels used in these tasks.", "labels": [], "entities": []}, {"text": "Approaches like this are known as transfer learning or multi-task learning (MTL) and are classified as a semi-supervised approaches (with no bootstrapping) ().", "labels": [], "entities": [{"text": "transfer learning or multi-task learning (MTL)", "start_pos": 34, "end_pos": 80, "type": "TASK", "confidence": 0.7098511382937431}]}, {"text": "There is an increasing interest in evaluating different frameworks ( and comparing neural network models (.", "labels": [], "entities": []}, {"text": "Some studies have shown that MTL is useful for certain tasks) while others reported that it is not always effective.", "labels": [], "entities": [{"text": "MTL", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9348562359809875}]}, {"text": "Bootstrapping () is a gen-eral and commonly used method of countering the limits of labelled datasets for learning.", "labels": [], "entities": []}, {"text": "It is a semi-supervised method where a well-performing model is used to automatically label new data which is subsequently used as a training data for another model.", "labels": [], "entities": []}, {"text": "This helps to enhance supervised learning.", "labels": [], "entities": []}, {"text": "However, this is also not always effective.", "labels": [], "entities": []}, {"text": "For example, and show that bootstrapping degraded the performance of some classifiers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Information about datasets.", "labels": [], "entities": []}, {"text": " Table 2: Performance of the models on labelled data.", "labels": [], "entities": []}, {"text": " Table 3. For the DNN, we only report the results  of the (best-performing) GRU models.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the models with background  knowledge.", "labels": [], "entities": []}, {"text": " Table 2. The use of the bootstrap- ping and the language model (5) leads to no signif- icant difference in performance in respect to (4).  Overall, it appears that the usage of the language  model has no strong effect. This could be caused  by the noise in the data, and adding more unla- belled data makes it hard for the language model  to learn all the data irregularities. Maybe the sys- tem requires more training data.", "labels": [], "entities": []}]}