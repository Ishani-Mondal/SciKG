{"title": [{"text": "Di-LSTM Contrast : A Deep Neural Network for Metaphor Detection", "labels": [], "entities": [{"text": "Metaphor Detection", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.756574809551239}]}], "abstractContent": [{"text": "The contrast between the contextual and general meaning of a word serves as an important clue for detecting its metaphoricity.", "labels": [], "entities": []}, {"text": "In this paper , we present a deep neural architecture for metaphor detection which exploits this contrast.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.9698074162006378}]}, {"text": "Additionally, we also use cost-sensitive learning by re-weighting examples, and base-line features like concreteness ratings, POS and WordNet-based features.", "labels": [], "entities": [{"text": "WordNet-based", "start_pos": 134, "end_pos": 147, "type": "DATASET", "confidence": 0.906156599521637}]}, {"text": "The best performing system of ours achieves an overall F1 score of 0.570 on All POS category and 0.605 on the Verbs category at the Metaphor Shared Task 2018.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9861311316490173}]}], "introductionContent": [{"text": "Lakoff (1993) defines a metaphorical expression as a linguistic expression which is the surface realization of a cross-domain mapping in a conceptual system.", "labels": [], "entities": []}, {"text": "On one hand, metaphors play a significant role in making a language more creative.", "labels": [], "entities": []}, {"text": "On the other, they also make language understanding difficult for artificial systems.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7519104480743408}]}, {"text": "Metaphor Shared Task 2018 () aims to explore various approaches for word-level metaphor detection in sentences.", "labels": [], "entities": [{"text": "word-level metaphor detection in sentences", "start_pos": 68, "end_pos": 110, "type": "TASK", "confidence": 0.8153597354888916}]}, {"text": "The task is to predict whether the target word in the given sentence is metaphoric or not.", "labels": [], "entities": []}, {"text": "There are two categories for this shared task.", "labels": [], "entities": []}, {"text": "The first one, All POS, tests the models for content words from all types of POS among nouns, adjectives, adverbs and verbs, while the second category, Verbs, tests the models only for verbs.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present evaluation results for our model.", "labels": [], "entities": []}, {"text": "shows their comparison on the test set using F1 score as the metric for evaluation.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9878317415714264}]}, {"text": "Experimental results indicate that our model generalizes well on the tests for both the task categories and the performance trends on tests are consistent with those on validation.", "labels": [], "entities": []}, {"text": "also shows the performance comparison of the variants of our model with the baseline results for the shares task provided by the organizers.", "labels": [], "entities": []}, {"text": "Our best performing model surpasses the baseline results on the Verbs category, while it achieves a lesser but comparable performance with the baseline on: Analysis of our best performing system on the Test Sets (both categories).", "labels": [], "entities": []}, {"text": "R = Recall, F = F1 Score All POS category.", "labels": [], "entities": [{"text": "R =", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9293439984321594}, {"text": "Recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.6813209056854248}, {"text": "F", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.9833123087882996}, {"text": "F1 Score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.967511773109436}, {"text": "POS", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.8380988836288452}]}, {"text": "We experiment with the combining function of the hidden states of forward and backward LSTM (in section 4.1) using both averaging and concatenation.", "labels": [], "entities": []}, {"text": "The validation results on both the categories show that concatenation performs much better than averaging.", "labels": [], "entities": []}, {"text": "This observation is supported by the fact that concatenation followed by a fully connected layer allows more parameterized interactions between the two states than averaging.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of data statistics for All POS cat- egory (Content Tokens: nouns, adjectives, adverbs and  verbs)", "labels": [], "entities": []}, {"text": " Table 2: Summary of data statistics for Verbs category  (Content Tokens: verbs)", "labels": [], "entities": []}, {"text": " Table 3: Comparision of F1 scores on Validation, All POS (Test) and Verbs (Test) scores between the various  approaches. DC = DiLSTM Contrast with concatenation, DC (avg) = DiLSTM Contrast with averaging, R = Re- weighting of Examples, L = Additional Linguistic Features (Baseline), Task Baseline = The baseline system used  by the task organizers", "labels": [], "entities": [{"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9952203631401062}, {"text": "Verbs", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.951842188835144}]}, {"text": " Table 5: Analysis of our best performing system on the Test Sets (both categories). P = Precision. R = Recall, F  = F1 Score", "labels": [], "entities": [{"text": "Test Sets", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.8700478672981262}, {"text": "Precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9893429279327393}, {"text": "R =", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9342176914215088}, {"text": "Recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.6408335566520691}, {"text": "F", "start_pos": 112, "end_pos": 113, "type": "METRIC", "confidence": 0.977397084236145}, {"text": "F1 Score", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9497991800308228}]}]}