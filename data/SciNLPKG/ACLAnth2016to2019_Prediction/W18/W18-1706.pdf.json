{"title": [{"text": "Efficient Graph-based Word Sense Induction by Distributional Inclusion Vector Embeddings", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.6011088987191519}]}], "abstractContent": [{"text": "Word sense induction (WSI), which addresses polysemy by unsupervised discovery of multiple word senses, resolves ambiguities for downstream NLP tasks and also makes word representations more interpretable.", "labels": [], "entities": [{"text": "Word sense induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7560887883106867}]}, {"text": "This paper proposes an accurate and efficient graph-based method for WSI that builds a global non-negative vector embedding basis (which are interpretable like topics) and clusters the basis indexes in the ego network of each poly-semous word.", "labels": [], "entities": [{"text": "WSI", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.841041088104248}]}, {"text": "By adopting distributional inclusion vector embeddings as our basis formation model, we avoid the expensive step of nearest neighbor search that plagues other graph-based methods without sacrificing the quality of sense clusters.", "labels": [], "entities": []}, {"text": "Experiments on three datasets show that our proposed method produces similar or better sense clusters and em-beddings compared with previous state-of-the-art methods while being significantly more efficient .", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense induction (WSI) is a challenging task of natural language processing whose goal is to categorize and identify multiple senses of polysemous words from raw text without the help of predefined sense inventory like WordNet.", "labels": [], "entities": [{"text": "Word sense induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8258795191844305}, {"text": "natural language processing", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.7161475618680319}, {"text": "WordNet", "start_pos": 223, "end_pos": 230, "type": "DATASET", "confidence": 0.9544757604598999}]}, {"text": "The problem is sometimes also called unsupervised word sense disambiguation ().", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6697456240653992}]}, {"text": "An effective WSI has wide applications.", "labels": [], "entities": [{"text": "WSI", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9488083124160767}]}, {"text": "For example, we can compare different induced senses in different documents to detect novel senses overtime () or analyze sense difference in multiple corpora (.", "labels": [], "entities": []}, {"text": "WSI could also be used to group and diversify the documents retrieved from search engine (.", "labels": [], "entities": []}, {"text": "After identifying senses, we can train an embedding for each sense of a word.", "labels": [], "entities": []}, {"text": "demonstrate that this multiprototype word embedding is useful in several downstream applications including part-of-speech (POS) tagging, relation extraction, and sentence relatedness tasks.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.6303099513053894}, {"text": "relation extraction", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.8784611225128174}, {"text": "sentence relatedness tasks", "start_pos": 162, "end_pos": 188, "type": "TASK", "confidence": 0.8101761142412821}]}, {"text": "also show that word sense disambiguation could be successfully applied to sentiment analysis.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.7278758883476257}, {"text": "sentiment analysis", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.9630445539951324}]}, {"text": "Since word sense induction (WSI) methods are unsupervised, the senses are typically derived from the results of different clustering techniques.", "labels": [], "entities": [{"text": "word sense induction (WSI)", "start_pos": 6, "end_pos": 32, "type": "TASK", "confidence": 0.8306658764680227}]}, {"text": "Like most of the clustering problems, it is usually challenging to predetermine the number of clusters/senses each word should have.", "labels": [], "entities": []}, {"text": "In fact, for many words, the \"correct\" number of senses is not unique.", "labels": [], "entities": []}, {"text": "Setting the number of clusters differently can capture different resolutions of senses.", "labels": [], "entities": []}, {"text": "For instance, race in the car context could share the same sense with the race in the game context because they all mean contest, but the race in the car context actually refers to the specific contest of speed.", "labels": [], "entities": []}, {"text": "Therefore, they can also be separated into two different senses, depending on the level of granularity we would like to model.", "labels": [], "entities": []}, {"text": "For graph-based clustering methods, it is easy and natural to model the multiple resolutions of senses in a consistent way by hierarchical clustering and defer the difficult problem of choosing the number of clusters to the end.", "labels": [], "entities": []}, {"text": "This makes it easier to incorporate other information, such as users' resolution preference on each hierarchical sense tree.", "labels": [], "entities": []}, {"text": "The flexibility is one of the reasons why graph-based methods are widely studied and applied to many downstream applications (.", "labels": [], "entities": []}, {"text": "Nevertheless, graph-based WSI methods usually require a substantial amount of computational resources.", "labels": [], "entities": [{"text": "WSI", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9090825319290161}]}, {"text": "For example, build the graph by finding the nearest neighbors of the target word in the word embedding space (i.e., ego network).", "labels": [], "entities": []}, {"text": "Thus, constructing ego networks for all the words takes at least O(|V | 2 ) time, where |V | is the size of the vocabulary, unless some approximation is made (e.g., approximate nearest neighbor search such as k-d tree).", "labels": [], "entities": [{"text": "O", "start_pos": 65, "end_pos": 66, "type": "METRIC", "confidence": 0.9894969463348389}]}, {"text": "Next, if our goals include finding less common senses, the method needs to construct a large graph by including more nearest neighbors.", "labels": [], "entities": []}, {"text": "For each target word, computing the pairwise distances between nodes in the large graph is also computationally intensive.", "labels": [], "entities": []}, {"text": "To overcome the limitations and make graphbased WSI more practical, we propose a novel WSI algorithm that first groups words into a set of basis indexes (i.e., a set of topics) efficiently and then, constructs the graph where each node corresponds to a basis index (i.e., a topic) instead of a word.", "labels": [], "entities": []}, {"text": "The motivation behind the approach is that different senses of a word usually appear in different topics.", "labels": [], "entities": []}, {"text": "For example, food and technology will beat least two distinct topics inmost of the topic models, so we can find senses by clustering corresponding basis indexes safely when the target word is apple.", "labels": [], "entities": []}, {"text": "If one word could have distinct senses in one topic, humans will constantly face difficult word sense disambiguation tasks while reading a document.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.6870451668898264}]}, {"text": "Although the main idea is simple, improving the efficiency significantly without sacrificing the quality is difficult.", "labels": [], "entities": []}, {"text": "One of the challenges is that similarity between two basis indexes changes given different target words.", "labels": [], "entities": []}, {"text": "For example, a country topic should be clustered together with a city topic if the target word is place.", "labels": [], "entities": []}, {"text": "However, if the query word is bank, it makes more sense to group the country topic with the money topic into one sense so that the bank mention in Bank of America will belong to the sense.", "labels": [], "entities": [{"text": "Bank of America", "start_pos": 147, "end_pos": 162, "type": "DATASET", "confidence": 0.9255793889363607}]}, {"text": "This means we want to focus on the geographical meaning of country when the target word is more about geography, while focus on the economic meaning of country when the target word is more about economics.", "labels": [], "entities": []}, {"text": "In order to tackle the issue, we adopt a recently proposed approach called distributional inclusion vector embedding (DIVE) (.", "labels": [], "entities": []}, {"text": "DIVE compresses the sparse bag-of-words while preserving the co-occurrence frequency order, so DIVE is able to model not only the possibility of observing one target word in a topic as typical topic models but also the possibility of observing one topic of a sentence containing a target word mention.", "labels": [], "entities": []}, {"text": "This allows us to efficiently identify the topics relevant to each target word, and only focus on an aspect of each of these topics composed of the words relevant to both the topic and the target word.", "labels": [], "entities": []}, {"text": "Experiments show that our method performs similarly compared with, a state-of-the-art graph-based WSI method, without the need of expensive nearest neighbor search.", "labels": [], "entities": []}, {"text": "Our method is even better for the words without a dominating sense.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first conduct a qualitative experiment to verify that our clustering algorithm performs well on some typical polysemy, and show the results in Table 1.", "labels": [], "entities": []}, {"text": "As we can see, our method cannot only separate two senses in very different contexts but also can distinguish more subtle sense difference such as identifying the car context and competition context as two different senses of the target word race.", "labels": [], "entities": []}, {"text": "Intuitively speaking, our method could be especially useful when it comes to increase the recall of less common senses (like discovering the educational meaning of core), but it is hard to verify the claim using existing WSI benchmarks because the common senses, especially the most frequent sense, often dominate in the benchmarks unless using the datasets where the bias is removed.", "labels": [], "entities": [{"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9964808821678162}, {"text": "WSI benchmarks", "start_pos": 221, "end_pos": 235, "type": "DATASET", "confidence": 0.7639762759208679}]}, {"text": "In the following sections, we will first introduce the setup and then the experiments on 3 datasets.", "labels": [], "entities": []}, {"text": "We train DIVE on first 51.2 million tokens of WaCkypedia (, the dataset suggested by, and the default hyper-parameter setting is used except the number of embedding dimensions L (i.e., number of basis indexes).", "labels": [], "entities": [{"text": "WaCkypedia", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.9202723503112793}]}, {"text": "We train two DIVEs, one with 100 dimensions and the other with 300 dimensions to study how the granularity of basis affects the performance.", "labels": [], "entities": []}, {"text": "For all other steps or baselines, we train them on the whole WaCkypedia where the stop words are removed.", "labels": [], "entities": [{"text": "WaCkypedia", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.908027708530426}]}, {"text": "For our clustering module, we use all the default hyper-parameters of the spectral clustering library in Scikit-learn 0.18.2 (Pedregosa et al., 2011) except the number of clusters is fixed at 2.", "labels": [], "entities": []}, {"text": "Setting a number larger than 2 makes it harder to compare with the results generated from other baselines whose default hyper-parameters usually make average number of senses between 1 and 2.", "labels": [], "entities": []}, {"text": "During EM, we train the skip-gram embedding on the whole WaCkypedia where we treat every consecutive 20 tokens as a sentence, and the refinement stops after 3 EM iteration.", "labels": [], "entities": [{"text": "WaCkypedia", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.9336910247802734}]}, {"text": "In the tables of this section, our methods using DIVE with 100 and 300 dimensions are denoted as DIVE (100) and DIVE (300), respectively.", "labels": [], "entities": [{"text": "DIVE", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.8927921652793884}, {"text": "DIVE (300)", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9129140377044678}]}, {"text": "In all quantitative experiments, we compare our method with, a state-of-theart graph-based clustering which builds ego graphs based on words similar to the target words, so we call it word graph (WG).", "labels": [], "entities": []}, {"text": "To train the model, we first train skip-gram on whole WaCkypedia and use all the default hyper-parameters in their released code to get sense embeddings.", "labels": [], "entities": [{"text": "WaCkypedia", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.9180458188056946}]}, {"text": "We also apply our EM refinement step to their output embedding to make the comparison fair and call this variation WG+EM.", "labels": [], "entities": []}, {"text": "We also compare our method with the baseline which randomly assigns two senses to every token and performs EM to refine the embedding (i.e., only adopting our post-processing step).", "labels": [], "entities": [{"text": "EM", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9484689235687256}]}, {"text": "The method is similar to multiple-sense skip-gram (), so we call it MSSG in our tables.", "labels": [], "entities": []}, {"text": "In all datasets, evaluation involves the similarity measurement between a sense of the target word and a context.", "labels": [], "entities": []}, {"text": "For each query, we compute cosine similarity between the context embedding and the sense embedding of the target word, where the context embedding e c is the average embedding of word in the context.", "labels": [], "entities": []}, {"text": "Notice that each word in the context could also be polysemous.", "labels": [], "entities": []}, {"text": "In these cases, we adopt the sense embedding of the context word that is closest to the sense of the target word (i.e., highest cosine similarity).", "labels": [], "entities": []}, {"text": "The Turk bootstrap Word Sense Inventory (TWSI) task  which consists of 1,012 nouns accompanied with 145,140 context sentences.", "labels": [], "entities": [{"text": "Turk bootstrap Word Sense Inventory (TWSI) task", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.6918434765603807}]}, {"text": "The task is to identify the correct sense of the target nouns, and all WSI algorithms choose the sense whose embedding is most similar to the context embedding.", "labels": [], "entities": []}, {"text": "Dataset is heavily skewed with 79% of contexts being assigned to the most frequent senses.", "labels": [], "entities": []}, {"text": "To remove this bias, we follow the procedure described in to create balanced TWSI.", "labels": [], "entities": []}, {"text": "Specifically, we only keep the first 5 contexts of each sense of every target word to make every sense count equally.", "labels": [], "entities": []}, {"text": "The procedure yields 8710 pairs of senses and contexts.: Results obtained on the SemEval 2013 task (%), where JI is Jaccard Index, FNMI is Fuzzy NMI, and FB-C is Fuzzy B-Cubed.", "labels": [], "entities": [{"text": "SemEval 2013 task", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.6392290790875753}, {"text": "JI", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9305229783058167}, {"text": "FNMI", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.947608232498169}, {"text": "FB-C", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.861462414264679}]}, {"text": "All-1 is to assign all senses to be the same and Rnd is to randomly assign all senses to 2 groups.", "labels": [], "entities": []}, {"text": "When evaluating on TWSI, each method needs to represent the sense by a sparse bag-of-word context feature called sense inventory.", "labels": [], "entities": []}, {"text": "The evaluation script 3 first maps each sense predicted by each algorithm to aground truth sense.", "labels": [], "entities": []}, {"text": "Then, the problem becomes a classification task, which can be evaluated by precision, recall, and F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9996684789657593}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9994645714759827}, {"text": "F1", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9995313882827759}]}, {"text": "In, we can see that DIVE performs slightly worse than WG () in full TWSI, but becomes slightly better in balanced TWSI.", "labels": [], "entities": []}, {"text": "We suspect this is because our number of sense is 2 but the WG generates the output where the average number of senses is around 1.5, which might do better when a sense of each word occurs most of the time.", "labels": [], "entities": []}, {"text": "Notice that the comparison in balanced TWSI is fair because the experiments in show that WG performs worse when increasing number of clusters.", "labels": [], "entities": []}, {"text": "The results also suggest that a sufficient number of basis vectors seldom group two senses together (otherwise, increasing the resolution/dimension of DIVE should be helpful).", "labels": [], "entities": [{"text": "resolution", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9914644360542297}]}, {"text": "SemEval-2013 task 13) provides a smaller dataset which consists of 50 words which include nouns, verbs, and adjectives.", "labels": [], "entities": []}, {"text": "The context prediction is done in the same way as TWSI, and the meaning of each metric could be found in.", "labels": [], "entities": [{"text": "context prediction", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7231263965368271}, {"text": "TWSI", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.5539724230766296}]}, {"text": "In, we can see our method performs roughly the same compared with other baselines.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of sense clusters on polysemous words. When the number of clusters is set to be 2, we present  the top 4 basis indexes b j in each sense cluster, which have the highest values on the target word embedding w q [b j ].  Otherwise, the top 2 basis indexes are presented. CID refers to sense cluster ID. The top 5 words with the highest  values of each basis index w[b j ] are presented.", "labels": [], "entities": []}, {"text": " Table 2: Precision@1 on the WCR R1 (%).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9947305917739868}, {"text": "WCR R1", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9154944121837616}]}, {"text": " Table 3: Results obtained on the TWSI task (%), where  P is precision and R is recall. MSSG rnd and DIVE  rnd are baselines which randomly assign sense given  inventory built by MSSG and DIVE, respectively.", "labels": [], "entities": [{"text": "TWSI task", "start_pos": 34, "end_pos": 43, "type": "TASK", "confidence": 0.5845282673835754}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.999137282371521}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9992496371269226}]}, {"text": " Table 4: Results obtained on the SemEval 2013 task  (%), where JI is Jaccard Index, FNMI is Fuzzy NMI,  and FB-C is Fuzzy B-Cubed. All-1 is to assign all  senses to be the same and Rnd is to randomly assign  all senses to 2 groups.", "labels": [], "entities": [{"text": "SemEval 2013 task", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.6912934978802999}, {"text": "Jaccard Index", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.6692290604114532}, {"text": "FNMI", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9531008005142212}, {"text": "FB-C", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9565846920013428}]}]}