{"title": [{"text": "UTFPR at WMT 2018: Minimalistic Supervised Corpora Filtering for Machine Translation", "labels": [], "entities": [{"text": "UTFPR at WMT 2018", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.6428281515836716}, {"text": "Minimalistic Supervised Corpora Filtering", "start_pos": 19, "end_pos": 60, "type": "TASK", "confidence": 0.7615215480327606}, {"text": "Machine Translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7817741334438324}]}], "abstractContent": [{"text": "We present the UTFPR systems at the WMT 2018 parallel corpus filtering task.", "labels": [], "entities": [{"text": "WMT 2018 parallel corpus filtering task", "start_pos": 36, "end_pos": 75, "type": "TASK", "confidence": 0.6893464575211207}]}, {"text": "Our supervised approach discerns between good and bad translations by training classic binary classification models over an artificially produced binary classification dataset derived from a high-quality translation set, and a minimalistic set of 6 semantic distance features that rely only on easy-to-gather resources.", "labels": [], "entities": []}, {"text": "We rank translations by their probability for the \"good\" label.", "labels": [], "entities": []}, {"text": "Our results show that logistic regression pairs best with our approach, yielding more consistent results throughout the different settings evaluated.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is no secret that Machine Translation (MT) systems have a wide array of applications, which range from translating news to multiple languages in order to more widely spread useful information, to producing translated transcriptions of real-time audio so that people from different places can communicate more easily.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.849625962972641}]}, {"text": "MT systems have evolved considerably throughout recent years due mainly to the widespread adoption of neural machine translation (NMT) approaches.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9818035364151001}, {"text": "neural machine translation (NMT)", "start_pos": 102, "end_pos": 134, "type": "TASK", "confidence": 0.8275501926740011}]}, {"text": "Attention-based encoder-decoders () and neural semantic encoders are just some examples of recurrent neural network architectures that have achieved great success in this task.", "labels": [], "entities": []}, {"text": "But regardless of how much MT approaches have evolved from a modelling standpoint, both modern and legacy approaches learn from the same type of information: parallel data containing handcrafted translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9872291088104248}]}, {"text": "This data usually takes the form of millions (sometimes billions) of parallel original-to-translated sentences, and are often extracted from translated versions of documents, such as news articles, and subtitles (.", "labels": [], "entities": []}, {"text": "Despite being hand-crafted, sometimes these datasets contain a lot of spurious translation examples that would not necessarily teach anything useful to an MT model, potentially compromising its performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 155, "end_pos": 157, "type": "TASK", "confidence": 0.9633316397666931}]}, {"text": "Consequently, it is important to filter these datasets in order to maximise the model's performance. and effectively filter large parallel corpora extracted from subtitles by using unsupervised metrics that combine features such as translation probabilities, language model probabilities, etc.", "labels": [], "entities": []}, {"text": "In this contribution, we attempt to elaborate on the ideas of and by using such features as input to supervised machine learning models.", "labels": [], "entities": []}, {"text": "In what follows, we present the UTFPR systems for the WMT 2018 parallel corpus filtering task: A minimalistic approach that aims at combining easy-to-harvest features with classic supervised binary classification models to create efficient translation filters.", "labels": [], "entities": [{"text": "WMT 2018 parallel corpus filtering task", "start_pos": 54, "end_pos": 93, "type": "TASK", "confidence": 0.6858058919509252}]}], "datasetContent": [{"text": "As mentioned in Section 2, we submit our results to the parallel corpus filtering shared task of WMT 2018, of which the test set contains roughly one billion unfiltered parallel English-German translations.", "labels": [], "entities": [{"text": "parallel corpus filtering shared task", "start_pos": 56, "end_pos": 93, "type": "TASK", "confidence": 0.7231704473495484}, {"text": "WMT 2018", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.6913366317749023}]}, {"text": "To train our supervised model, we use the Europarl v7 parallel corpus, which contains 1, 920, 209 translations.", "labels": [], "entities": [{"text": "Europarl v7 parallel corpus", "start_pos": 42, "end_pos": 69, "type": "DATASET", "confidence": 0.9309701770544052}]}, {"text": "For learning, we experiment with three classification models: Logistic Regression (UTFPR-LR), Decision Trees (UTFPR-DT), and Random Forests (UTFPR-RF).", "labels": [], "entities": []}, {"text": "We chose them because they use a varying array of learning methods, and can be trained efficiently even when presented with hundreds of millions of input instances.", "labels": [], "entities": []}, {"text": "To evaluate our approach, the shared task organizers first created two sub-sampled sets of parallel translations containing the 10 million and 100 million highest scoring translations in the test set.", "labels": [], "entities": []}, {"text": "They then used these sets to train both statistical (SMT) and neural MT (NMT) models using the Moses (", "labels": [], "entities": [{"text": "SMT) and neural MT (NMT)", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.6991321258246899}]}], "tableCaptions": [{"text": " Table 1: Parallel corpus filtering results with respect to the average BLEU-c scores obtained over the datasets described in  Section 4. The first and last five lines feature, respectively, the five systems that achieved the highest and lowest average  BLEU-c scores in the task. Boldface numbers highlight the highest BLEU-c scores achieved among the UTFPR systems.", "labels": [], "entities": [{"text": "Parallel corpus filtering", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6569547156492869}, {"text": "BLEU-c", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9959477782249451}, {"text": "BLEU-c", "start_pos": 254, "end_pos": 260, "type": "METRIC", "confidence": 0.9867362380027771}, {"text": "BLEU-c", "start_pos": 320, "end_pos": 326, "type": "METRIC", "confidence": 0.9969261288642883}, {"text": "UTFPR", "start_pos": 353, "end_pos": 358, "type": "DATASET", "confidence": 0.8435243964195251}]}]}