{"title": [], "abstractContent": [{"text": "Developing computational models of spatial prepositions (such as on, in, above, etc.) is crucial for such tasks as human-machine collaboration , story understanding, and 3D model generation from descriptions.", "labels": [], "entities": [{"text": "story understanding", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.8261526525020599}, {"text": "3D model generation from descriptions", "start_pos": 170, "end_pos": 207, "type": "TASK", "confidence": 0.74489067196846}]}, {"text": "However, these prepositions are notoriously vague and ambiguous , with meanings depending on the types, shapes and sizes of entities in the argument positions, the physical and task context , and other factors.", "labels": [], "entities": []}, {"text": "As a result truth value judgments for prepositional relations are often uncertain and variable.", "labels": [], "entities": []}, {"text": "In this paper we treat the modeling task as calling for assignment of probabilities to such relations as a function of multiple factors, where such probabilities can be viewed as estimates of whether humans would judge the relations to hold in given circumstances.", "labels": [], "entities": []}, {"text": "We implemented our models in a 3D blocks world and a room world in a computer graphics setting, and found that true/false judgments based on these models do not differ much more from human judgments that the latter differ from one another.", "labels": [], "entities": []}, {"text": "However, what really matters pragmatically is not the accuracy of truth value judgments but whether, for instance, the computer models suffice for identifying objects described in terms of prepositional relations, (e.g., the box to the left of the table, where there are multiple boxes).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9978060126304626}]}, {"text": "For such tasks, our models achieved accuracies above 90% for most relations.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9816816449165344}]}], "introductionContent": [{"text": "Spatial prepositions are pervasive in natural languages and, therefore, interpretation and understanding of their meaning is critical to tasks involving NLP.", "labels": [], "entities": []}, {"text": "The computational challenges are aggravated by the versatility and vagueness of these prepositions, and their sensitivity to miscellaneous factors such as shapes, sizes and salience of the relata, part-of relations, typicality, etc.", "labels": [], "entities": []}, {"text": "On provides a good example of such semantically rich prepositions.", "labels": [], "entities": []}, {"text": "When we say that one object is on another one, we strongly imply the relation of physical support between them.", "labels": [], "entities": []}, {"text": "But support relations can be quite subtle, and can occur in diverse physical configurations: In and over provide additional examples of semantically subtle, versatile prepositions.", "labels": [], "entities": []}, {"text": "While it is conceivable that the diverse meanings of these prepositions are unrelated and arose from disparate communicative and historical pressures, there are strong arguments that this is not the case.", "labels": [], "entities": []}, {"text": "In fact, it is very likely that all or most of the different meanings associated with a preposition are based on some underlying primary meaning from which they all originated.", "labels": [], "entities": []}, {"text": "In the case of on, it seems plausible that the initial meaning was essentially support by a more or less horizontal surface, which was then extended to further support relations, and metaphorized to nonspatial relations during the evolution of language.", "labels": [], "entities": []}, {"text": "Because of this richness, it seems that no single criterion can capture all the instances where relations such as on, in, over, etc., hold.", "labels": [], "entities": []}, {"text": "However, there are three considerations that prompted us to proceed with the design of intuitive computational models of some of the most prevalent spatial prepositions: First, while no simple mathematical criterion can characterize anyone of these relations, we can identify prototypical cases where the relations hold, and by considering such cases one by one, we can also zero in on non-geometrical factors that affect \"truth\" judgments in these cases.", "labels": [], "entities": []}, {"text": "Second, people's judgments about whether a prepositional relation holds in a given case can be quite variable; therefore it should suffice to provide models that estimate the probability that arbitrary judges would consider the relation to hold.", "labels": [], "entities": []}, {"text": "This approach is aligned with a view of predicate vagueness as variability in applicability judgments, enabling Bayesian interpretation.", "labels": [], "entities": [{"text": "Bayesian interpretation", "start_pos": 112, "end_pos": 135, "type": "TASK", "confidence": 0.6159107834100723}]}, {"text": "And third, the ultimate success criterion in assessing models of prepositional predicates should be pragmatic; i.e, in physical settings we often use such predicates to identify a referent (the blue book in front of the laptop) or to specify a goal (put the laptop on the table), so our models should allow a natural language system to interpret such usages as a human would.", "labels": [], "entities": []}, {"text": "Our results for referent identification suggest that our current models are nearly good enough for such purposes in various \"blocks world\" and \"room world\" configurations.", "labels": [], "entities": [{"text": "referent identification", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.8937473893165588}]}, {"text": "In developing a conceptual framework for modeling several common prepositional relations, we tried to achieve a trade-off: On one hand, we tried to avoid overcomplicating the model, keeping the number of primitive concepts used in the framework to a minimum.", "labels": [], "entities": []}, {"text": "On the other, we strove to make the framework general enough to cover a wide range of objects and configurations.", "labels": [], "entities": []}, {"text": "In the following sections, we discuss related work, and then outline our modeling framework by examining the primitive concepts that are used as building blocks, and showing how these concepts come together in modeling a specific preposition.", "labels": [], "entities": []}, {"text": "We then evaluate our approach in two test domains, a blocks world and a \"room world\", making use of Blender graphics software.", "labels": [], "entities": []}, {"text": "We show that our computational models judge the chosen prepositional relations accurately enough in both worlds to enable rather good referent identification in relation to independent human judgments.", "labels": [], "entities": [{"text": "referent identification", "start_pos": 134, "end_pos": 157, "type": "TASK", "confidence": 0.8636832535266876}]}, {"text": "We summarize our contributions, and directions for future work, in the concluding section.", "labels": [], "entities": []}], "datasetContent": [{"text": "The model was evaluated as follows.", "labels": [], "entities": []}, {"text": "For the truthjudgment task, the model was used to evaluate the given relation and its arguments.", "labels": [], "entities": []}, {"text": "Both the numerical answer provided by the model and the annotator's answer were then transformed to the ordinal scale to compute the agreement coefficient.", "labels": [], "entities": []}, {"text": "The human responses were converted from the Likert scale \"YES\", \"RATHER YES\", \"UNCERTAIN\", \"RATHER NO\", \"NO\" into integers 5 to 1, respectively.", "labels": [], "entities": [{"text": "Likert scale \"YES", "start_pos": 44, "end_pos": 61, "type": "METRIC", "confidence": 0.6936012953519821}, {"text": "RATHER YES", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.8037989437580109}, {"text": "UNCERTAIN", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9612962007522583}, {"text": "RATHER NO\"", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.8616798321406046}, {"text": "NO", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.6607282161712646}]}, {"text": "The metric value generated by the model was transformed as follows: Values in [0, 0.2) correspond to 1, those in [0.2, 0.4) to 2, ..., those in [0.8, 1] to 5.", "labels": [], "entities": []}, {"text": "For the description task, given a human description of a target object in relation to a reference object, the model was given the reference object and relation, and was required to identify the object being described.", "labels": [], "entities": []}, {"text": "We used both standard and weighted versions of Cohen's Kappa as an inter-annotator agreement metric with the weighting penalty w(i, j) = i \u2212 j, where i and j are the ordinal conversions of the responses of human annotators and our system.", "labels": [], "entities": []}, {"text": "The agreement values were computed as follows.", "labels": [], "entities": []}, {"text": "First, all pairwise agreement values between annotators and between each annotator and the system were computed.", "labels": [], "entities": []}, {"text": "Next, the corresponding averages (of human-human and human-system pairs, respectively) were found.", "labels": [], "entities": []}, {"text": "For the initial data set (the part used to some extent to tune the model parameters), the accuracy breakdown was as follows.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9994097948074341}]}, {"text": "For weighed Kappa, the average pairwise human-human interannotator agreement value was 0.717, whereas the average pairwise system-human agreement metric was 0.682.", "labels": [], "entities": []}, {"text": "For standard Kappa, the respective values were 0.536 and 0.479.", "labels": [], "entities": []}, {"text": "For an independent data set used for final evaluation, the values were: human-human agreement, weighted Kappa -0.76, human-system agreement, weighted Kappa -0.71, human-human agreement, standard Kappa -0.52, human-system agreement, standard Kappa -0.49.", "labels": [], "entities": []}, {"text": "Again, all these numbers are pairwise averages.", "labels": [], "entities": []}, {"text": "As expected, interannotator agreement was not very high.", "labels": [], "entities": [{"text": "interannotator", "start_pos": 13, "end_pos": 27, "type": "METRIC", "confidence": 0.5578164458274841}, {"text": "agreement", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.5305118560791016}]}, {"text": "The somewhat lower system-human agreement is still close enough to human-human agreement to indicate the plausibility of our models.", "labels": [], "entities": []}, {"text": "Since humans manage to identify referents perfectly well using spatial relations, despite the vagueness of these relations, the key question then was how well our models would do for such usages.", "labels": [], "entities": []}, {"text": "For the description task we computed the accuracy in terms of the percentage of tests with correctly identified objects.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9996750354766846}]}, {"text": "The overall system accuracy on the testing data was about 93%; while imperfect, this is an encouraging result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9981878399848938}]}, {"text": "The detailed breakdown for separate relations is provided in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Fourteen relations, together with the to- tal occurrences within the dataset used for tuning  (different annotation) and accuracy per relation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9991263747215271}]}, {"text": " Table 2: Fourteen relations, together with the total  occurrences within the dataset used for final test- ing (different annotation) and accuracy per rela- tion.", "labels": [], "entities": [{"text": "final test- ing", "start_pos": 95, "end_pos": 110, "type": "METRIC", "confidence": 0.749586246907711}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9994248151779175}]}]}