{"title": [{"text": "Domain Adapted Word Embeddings for Improved Sentiment Classification", "labels": [], "entities": [{"text": "Improved Sentiment Classification", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.7057868242263794}]}], "abstractContent": [{"text": "Generic word embeddings are trained on large-scale generic corpora; Domain Specific (DS) word embeddings are trained only on data from a domain of interest.", "labels": [], "entities": []}, {"text": "This paper proposes a method to combine the breadth of generic embeddings with the specificity of domain specific embed-dings.", "labels": [], "entities": [{"text": "breadth", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9452487230300903}]}, {"text": "The resulting embeddings, called Domain Adapted (DA) word embeddings, are formed by first aligning corresponding word vectors using Canonical Correlation Analysis (CCA) or the related non-linear Kernel CCA (KCCA) and then combining them via convex optimization.", "labels": [], "entities": []}, {"text": "Results from evaluation on sentiment classification tasks show that the DA embeddings substantially outperform both generic, DS embeddings when used as input features to standard or state-of-the-art sentence encoding algorithms for classification.", "labels": [], "entities": [{"text": "sentiment classification tasks", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.9380436340967814}]}], "introductionContent": [{"text": "Generic word embeddings such as Glove and word2vec) which are pre-trained on large bodies of raw text, have demonstrated remarkable success when used as features for supervised learning problems.", "labels": [], "entities": []}, {"text": "There are, however, many applications with domain specific vocabularies and relatively small amounts of data.", "labels": [], "entities": []}, {"text": "The performance of generic word embeddings in such applications is limited, since word embeddings pre-trained on generic corpora do not capture domain specific semantics/knowledge, while embeddings learned on small data sets are of low quality.", "labels": [], "entities": []}, {"text": "A concrete example of a small-sized domain specific corpus is the Substances User Disorders (SUDs) data set (, which contains messages on discussion forums for people with substance addictions.", "labels": [], "entities": [{"text": "Substances User Disorders (SUDs) data set", "start_pos": 66, "end_pos": 107, "type": "TASK", "confidence": 0.7265108078718185}]}, {"text": "These forums are part of mobile health intervention treatments that encourages participants to engage in sobriety-related discussions.", "labels": [], "entities": []}, {"text": "The goal of such treatments is to analyze content of participants' digital media content and provide human intervention via machine learning algorithms.", "labels": [], "entities": []}, {"text": "This data is both domain specific and limited in size.", "labels": [], "entities": []}, {"text": "Other examples include customer support tickets reporting issues with taxi-cab services, product reviews, reviews of restaurants and movies, discussions by special interest groups and political surveys.", "labels": [], "entities": []}, {"text": "In general they are common in domains where words have different sentiment from what they would have elsewhere.", "labels": [], "entities": []}, {"text": "These data sets present significant challenges for word embedding learning algorithms.", "labels": [], "entities": [{"text": "word embedding learning", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.8037814696629842}]}, {"text": "First, words in data on specific topics have a different distribution than words from generic corpora.", "labels": [], "entities": []}, {"text": "Hence using generic word embeddings obtained from algorithms trained on a corpus such as Wikipedia, would introduce considerable errors in performance metrics on specific downstream tasks such as sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 196, "end_pos": 220, "type": "TASK", "confidence": 0.9541719853878021}]}, {"text": "For example, in SUDs, discussions are focused on topics related to recovery and addiction; the sentiment behind the word 'party' maybe very different in a dating context than in a substance abuse context.", "labels": [], "entities": [{"text": "SUDs", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.9747217297554016}]}, {"text": "Thus domain specific vocabularies and word semantics maybe a problem for pre-trained sentiment classification models (.", "labels": [], "entities": [{"text": "word semantics", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.6868817955255508}, {"text": "sentiment classification", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.7824872434139252}]}, {"text": "Second, there is insufficient data to completely retrain anew set of word embeddings.", "labels": [], "entities": []}, {"text": "The SUD data set consists of a few hundred people and only a fraction of these are active (,.", "labels": [], "entities": [{"text": "SUD data set", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9287436604499817}]}, {"text": "This results in a small data set of text messages available for analysis.", "labels": [], "entities": []}, {"text": "Furthermore, these messages are unstructured and the language used is informal.", "labels": [], "entities": []}, {"text": "Fine-tuning the generic word embedding also leads to noisy outputs due to the highly non-convex training objective and the small amount of the data.", "labels": [], "entities": []}, {"text": "Since such data sets are common, a simple and effective method to adapt word embedding approaches is highly valuable.", "labels": [], "entities": []}, {"text": "While existing work (e.g) combines word embeddings from different algorithms to improve upon intrinsic tasks such as similarities, analogies etc, there does not exist a concrete method to combine multiple embeddings for extrinsic tasks.", "labels": [], "entities": []}, {"text": "This paper proposes a method for obtaining high quality domain adapted word embeddings that capture domain specific semantics and are suitable for tasks on the specific domain.", "labels": [], "entities": []}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}], "datasetContent": [{"text": "In 2017) with DA word embeddings to obtain sentence embeddings.", "labels": [], "entities": []}, {"text": "Encoded sentences are then classified using a logistic regressor.", "labels": [], "entities": []}, {"text": "Performance metrics reported are average precision, F-score and AUC.", "labels": [], "entities": [{"text": "average", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9851506352424622}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.7960432171821594}, {"text": "F-score", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.998794674873352}, {"text": "AUC", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9975157976150513}]}, {"text": "All hyperparameters are tuned via 10 fold cross validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: This table shows results from the classi- fication task using sentence embeddings obtained  from weighted averaging of word embeddings.  Metrics reported are average Precision, F-score  and AUC and the corresponding standard devi- ations. Best performing embeddings and corre- sponding metrics are highlighted in boldface.", "labels": [], "entities": [{"text": "Precision", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.996792733669281}, {"text": "F-score", "start_pos": 187, "end_pos": 194, "type": "METRIC", "confidence": 0.9895875453948975}, {"text": "AUC", "start_pos": 200, "end_pos": 203, "type": "METRIC", "confidence": 0.9951649904251099}]}, {"text": " Table 2: This table shows results obtained by ini- tializing InferSent encoder with different embed- dings in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard de- viations. Best performing embeddings and corre- sponding metrics are highlighted in boldface We  use \u03b1 = 0.5 for all of our experiments here.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 115, "end_pos": 144, "type": "TASK", "confidence": 0.8864448467890421}, {"text": "Precision", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9985333681106567}, {"text": "F-score", "start_pos": 188, "end_pos": 195, "type": "METRIC", "confidence": 0.9837527275085449}, {"text": "AUC", "start_pos": 201, "end_pos": 204, "type": "METRIC", "confidence": 0.9948391318321228}]}]}