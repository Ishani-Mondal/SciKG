{"title": [{"text": "Beyond Weight Tying: Learning Joint Input-Output Embeddings for Neural Machine Translation", "labels": [], "entities": [{"text": "Weight Tying", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.6658200025558472}, {"text": "Neural Machine Translation", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.7070650060971578}]}], "abstractContent": [{"text": "Tying the weights of the target word em-beddings with the target word classifiers of neural machine translation models leads to faster training and often to better translation quality.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.7519457340240479}]}, {"text": "Given the success of this parameter sharing, we investigate other forms of sharing in between no sharing and hard equality of parameters.", "labels": [], "entities": []}, {"text": "In particular, we propose a structure-aware output layer which captures the semantic structure of the output space of words within a joint input-output embedding.", "labels": [], "entities": []}, {"text": "The model is a generalized form of weight tying which shares parameters but allows learning a more flexible relationship with input word embeddings and allows the effective capacity of the output layer to be controlled.", "labels": [], "entities": [{"text": "weight tying", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.7452751696109772}]}, {"text": "In addition, the model shares weights across output classifiers and translation contexts which allows it to better leverage prior knowledge about them.", "labels": [], "entities": []}, {"text": "Our evaluation on English-to-Finnish and English-to-German datasets shows the effectiveness of the method against strong encoder-decoder base-lines trained with or without weight tying.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) predicts the target sentence one word at a time, and thus models the task as a sequence classification problem where the classes correspond to words.", "labels": [], "entities": [{"text": "Neural machine translation (NMT) predicts the target sentence one word", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.8295348783334097}, {"text": "sequence classification", "start_pos": 112, "end_pos": 135, "type": "TASK", "confidence": 0.7309808433055878}]}, {"text": "Typically, words are treated as categorical variables which lack description and semantics.", "labels": [], "entities": []}, {"text": "This makes training speed and parametrization dependent on the size of the target vocabulary (.", "labels": [], "entities": []}, {"text": "Previous studies overcome this problem by truncating the vocabulary to limit its size and mapping out-of-vocabulary words to a single \"unknown\" token.", "labels": [], "entities": []}, {"text": "Other approaches attempt to use a limited number of frequent words plus sub-word units, the combination of which can cover the full vocabulary, or to perform character-level modeling (; with the former being the most effective between the two.", "labels": [], "entities": [{"text": "character-level modeling", "start_pos": 158, "end_pos": 182, "type": "TASK", "confidence": 0.7345674633979797}]}, {"text": "The idea behind these alternatives is to overcome the vocabulary size issue by modeling the morphology of rare words.", "labels": [], "entities": []}, {"text": "One limitation, however, is that semantic information of words or sub-word units learned by the input embedding are not considered when learning to predict output words.", "labels": [], "entities": []}, {"text": "Hence, they rely on a large amount of examples per class to learn proper word or sub-word unit output classifiers.", "labels": [], "entities": []}, {"text": "One way to consider information learned by input embeddings, albeit restrictively, is with weight tying i.e. sharing the parameters of the input embeddings with those of the output classifiers) which is effective for language modeling and machine translation (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 217, "end_pos": 234, "type": "TASK", "confidence": 0.7454966604709625}, {"text": "machine translation", "start_pos": 239, "end_pos": 258, "type": "TASK", "confidence": 0.8252044320106506}]}, {"text": "Despite its usefulness, we find that weight tying has three limitations: (a) It biases all the words with similar input embeddings to have a similar chance to be generated, which may not always be the case (see for examples).", "labels": [], "entities": [{"text": "weight tying", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.8538984954357147}]}, {"text": "Ideally, it would be better to learn distinct relationships useful for encoding and decoding without forcing any general bias.", "labels": [], "entities": []}, {"text": "(b) The relationship between outputs is only implicitly captured by weight tying because there is no parameter sharing across output classifiers.", "labels": [], "entities": []}, {"text": "(c) It requires that the size of the translation context vector and the input embeddings are the same, which in practice makes it difficult to control the output layer capacity.", "labels": [], "entities": []}, {"text": "In this study, we propose a structure-aware output layer which overcomes the limitations of previous output layers of NMT models.", "labels": [], "entities": []}, {"text": "To achieve this, we treat words and subwords as units with textual descriptions and semantics.: Top-5 most similar input and output representations to two query words based on cosine similarity for an NMT trained without (NMT) or with weight tying (NMT-tied) and our structure-aware output layer (NMT-joint) on De-En (|V| \u2248 32K).", "labels": [], "entities": []}, {"text": "Our model learns representations useful for encoding and generation which are more consistent to the dominant semantic and syntactic relations of the query such as verbs in past tense, adjectives and nouns (inconsistent words are marked in red). and output classifiers, but also shares parameters across output classifiers and translation contexts to better capture the similarity structure of the output space and leverage prior knowledge about this similarity.", "labels": [], "entities": [{"text": "encoding and generation", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.6376179953416189}]}, {"text": "This flexible sharing allows it to distinguish between features of words which are useful for encoding, generating, or both.", "labels": [], "entities": []}, {"text": "shows examples of the proposed model's input and output representations, compared to those of a softmax linear unit with or without weight tying.", "labels": [], "entities": []}, {"text": "This proposal is inspired by joint input-output models for zero-shot text classification (), but innovates in three important directions, namely in learning complex non-linear relationships, controlling the effective capacity of the output layer and handling structured prediction problems.", "labels": [], "entities": [{"text": "zero-shot text classification", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.6566391189893087}]}, {"text": "Our contributions are summarized as follows: \u2022 We identify key theoretical and practical limitations of existing output layer parametrizations such as softmax linear units with or without weight tying and relate the latter to joint input-output models.", "labels": [], "entities": []}, {"text": "\u2022 We propose a novel structure-aware output layer which has flexible parametrization for neural MT and demonstrate that its mathematical form is a generalization of existing output layer parametrizations.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.8698951601982117}]}, {"text": "\u2022 We provide empirical evidence of the superiority of the proposed structure-aware output layer on morphologically simple and complex languages as targets, including under challenging conditions, namely varying vocabulary sizes, architecture depth, and output frequency.", "labels": [], "entities": []}, {"text": "The evaluation is performed on 4 translation pairs, namely English-German and English-Finnish in both directions using BPE ( of varying operations to investigate the effect of the vocabulary size to each model.", "labels": [], "entities": [{"text": "BPE", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9536769390106201}]}, {"text": "The main baseline is a strong LSTM encoder-decoder model with 2 layers on each side (4 layers) trained with or without weight tying on the target side, but we also experiment with deeper models with up to 4 layers on each side (8 layers).", "labels": [], "entities": []}, {"text": "To improve efficiency on large vocabulary sizes we make use of negative sampling as in ( show that the proposed model is the most robust to such approximate training among the alternatives.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the NMT-joint model to two strong NMT baselines trained with and without weight tying over four large parallel corpora which include morphologically rich languages as targets (Finnish and German), but also morphologically less rich languages as targets (English) from WMT 2017 (Bojar et al., 2017) 2 . We examine the behavior of the proposed model under challenging conditions, namely varying vocabulary sizes, architecture depth, and output frequency.", "labels": [], "entities": [{"text": "WMT 2017", "start_pos": 279, "end_pos": 287, "type": "DATASET", "confidence": 0.9315749108791351}]}, {"text": "The English-Finnish corpus contains 2.5M sentence pairs for training, 1.3K for development (Newstest2015), and 3K for testing (Newstest2016), and the English-German corpus 5.8M for training, 3K for development (Newstest2014), and 3K for testing (Newstest2015).", "labels": [], "entities": []}, {"text": "We preprocess the texts using the BPE algorithm) with 32K, 64K and 128K operations.", "labels": [], "entities": []}, {"text": "Following the standard evaluation practices in the field (, the translation quality is measured using BLEU score () (multi-blue) on tokenized text and the significance is measured with the paired bootstrap re-sampling method proposed by.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9821216464042664}]}, {"text": "The quality on infrequent words is measured with METEOR) which has originally been proposed to measure performance on function words.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9869703054428101}]}, {"text": "To adapt it for our purposes on English-German pairs (|V| \u2248 32K), we set as function words different sets of words grouped according to three frequency bins, each of them containing |V| 3 words of high, medium and low frequency respectively and set its parameters to {0.85, 0.2, 0.6, 0.} and {0.95, 1.0, 0.55, 0.} when evaluating on English and German respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: BLEU scores on De \u2192 En (|V| \u2248 32K)  for the ablation analysis of NMT-joint.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994268417358398}]}, {"text": " Table 4: Target tokens processed per second during  training with negative sampling on En \u2192 De pair  with a large BPE vocabulary |V| \u2248 128K.", "labels": [], "entities": [{"text": "BPE vocabulary |V", "start_pos": 115, "end_pos": 132, "type": "METRIC", "confidence": 0.9162180125713348}]}, {"text": " Table 5: BLEU scores on De \u2192 En (|V| \u2248 32K) for the NMT-joint with d j = 512 against baselines  when varying the depth of both the encoder and the decoder of the NMT model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999406099319458}]}]}