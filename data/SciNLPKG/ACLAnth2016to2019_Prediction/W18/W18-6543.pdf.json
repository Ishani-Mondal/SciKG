{"title": [{"text": "Handling Rare Items in Data-to-Text Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Neural approaches to data-to-text generation generally handle rare input items using either delexicalisation or a copy mechanism.", "labels": [], "entities": [{"text": "data-to-text generation", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.763504296541214}]}, {"text": "We investigate the relative impact of these two methods on two datasets (E2E and WebNLG) and using two evaluation settings.", "labels": [], "entities": [{"text": "E2E", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.9362547993659973}, {"text": "WebNLG", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.8573123216629028}]}, {"text": "We show (i) that rare items strongly impact performance; (ii) that combining delexicalisation and copying yields the strongest improvement; (iii) that copying underperforms for rare and unseen items and (iv) that the impact of these two mechanisms greatly varies depending on how the dataset is constructed and on how it is split into train, dev and test 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "The input to data-to-text generation often contains rare items, i.e. low frequency items such as names, locations and dates.", "labels": [], "entities": []}, {"text": "This makes it difficult for neural models to predict their verbalisation.", "labels": [], "entities": []}, {"text": "To address these issues, neural approaches to datato-text generation typically resort either to delexicalisation () or to a copy mechanism . Character-based encodings ( and byte pair encodings have also been used.", "labels": [], "entities": [{"text": "datato-text generation", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7743233740329742}]}, {"text": "However, when applying a character-based approach within a standard sequence-to-sequence model to the WebNLG and E2E datasets, the results were low.", "labels": [], "entities": [{"text": "WebNLG and E2E datasets", "start_pos": 102, "end_pos": 125, "type": "DATASET", "confidence": 0.7526886016130447}]}, {"text": "Hence we chose not to discuss them.", "labels": [], "entities": []}, {"text": "When using delexicalisation, the data is preprocessed to replace rare items with placeholders and the generated text is post-processed to replace these placeholders with appropriate values based on a mapping between placeholders and initial values built during preprocessing.", "labels": [], "entities": []}, {"text": "While this method is often used, it has several drawbacks.", "labels": [], "entities": []}, {"text": "It requires an additional pre-and post-processing step.", "labels": [], "entities": []}, {"text": "These processing steps must be re-implemented for each new data-to-text application.", "labels": [], "entities": []}, {"text": "The matching procedure needed to correctly match a rare input item (e.g., Barack Obama) with the corresponding part in the output text (e.g., the former President of the United States) maybe quite complex which may result in incorrect or incomplete delexicalisations.", "labels": [], "entities": []}, {"text": "In contrast, the copy mechanisms standardly used in neural approaches to summarisation, paraphrasing , answer generation ( and data-to-text generation ( ) is a generic technique which is easy to integrate in the encoder-decoder framework and can be used independently of the particular domain and application.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.9810845851898193}, {"text": "answer generation", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.8073353469371796}, {"text": "data-to-text generation", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.7575861811637878}]}, {"text": "In this paper, we investigate the impact of copying and delexicalisation on the quality of generated texts using two sequence-to-sequence models with attention: one using the copy and coverage mechanism of, the other using delexicalisation.", "labels": [], "entities": []}, {"text": "We evaluate their respective output on two data-to-text datasets, namely the E2E () and the WebNLG (Gardent et al., 2017a) datasets.", "labels": [], "entities": [{"text": "E2E", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.8981914520263672}, {"text": "WebNLG (Gardent et al., 2017a) datasets", "start_pos": 92, "end_pos": 131, "type": "DATASET", "confidence": 0.9264439543088278}]}, {"text": "We also compare the two methods in two different settings: the original train/dev/test partition produced by the E2E and by the WebNLG challenge vs. a more constrained train/dev/test split which aims to further minimise the amount of redundancy between train, dev and test data.", "labels": [], "entities": [{"text": "E2E", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.967471182346344}]}, {"text": "This latter experimental setting is in- Original: Bakewell pudding, also called bakewell tart, originates from the Derbyshire Dales.", "labels": [], "entities": []}, {"text": "Classified as a dessert which can be served warm or cold, its main ingredients are ground almond, jam, butter and eggs.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two recently released corpora for data-to-text generation served as experimental datasets for our study: the E2E () and the WebNLG () datasets.", "labels": [], "entities": [{"text": "data-to-text generation", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7310684770345688}, {"text": "WebNLG () datasets", "start_pos": 124, "end_pos": 142, "type": "DATASET", "confidence": 0.8703073660532633}]}, {"text": "In the E2E dataset, the input to generation is a dialogue act consisting of three to eight slot-value pairs describing a restaurant, while the output is a restaurant recommendation verbalising this input.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9745232164859772}]}, {"text": "shows an example with an input consisting of six slot-value pairs.", "labels": [], "entities": []}, {"text": "In average, each input is associated with 8.1 references.", "labels": [], "entities": []}, {"text": "The number of possible values for each slot ranges from two (binary slots) to 34 (restaurant name).", "labels": [], "entities": []}, {"text": "summarise the statistics of the E2E dataset.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.9781074225902557}]}, {"text": "In WebNLG, the aim is to verbalise a set of RDF (Resource Description Framework) triples describing entities of different categories.", "labels": [], "entities": []}, {"text": "An RDF triple is of the form (subject, property, object) where subject and object denotes entities or values and property denotes a binary relation holding between subject and object.", "labels": [], "entities": []}, {"text": "The inputs consist of sets of (one to seven) triples and the entities belong to fifteen distinct DBpedia categories 2 . Both dataset releases gave rise to a shared task in NLG in 2017 . Note though that for WebNLG, the present study relies on the final release data (version 2) , which is a larger dataset than that used for the WebNLG Challenge 2017.", "labels": [], "entities": [{"text": "WebNLG Challenge 2017", "start_pos": 329, "end_pos": 350, "type": "DATASET", "confidence": 0.8884823520978292}]}, {"text": "We derive delexicalised datasets from the original E2E and WebNLG datasets as follows.", "labels": [], "entities": [{"text": "WebNLG datasets", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.8901249170303345}]}, {"text": "For each dataset, we replicated the delexicalisation procedure which was applied to the baseline systems developed for the E2E ( and for the WebNLG challenge (Gardent et al., 2017b) respectively 5 . As shown in Table 1, both input data and output text were delexicalised.", "labels": [], "entities": [{"text": "E2E", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.9378024339675903}, {"text": "WebNLG challenge (Gardent et al., 2017b)", "start_pos": 141, "end_pos": 181, "type": "DATASET", "confidence": 0.9056845837169223}]}, {"text": "In E2E, only the name and near slots were delexicalised (because contrary to the other slots,   they have a large number of distinct values).", "labels": [], "entities": [{"text": "E2E", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.9360171556472778}]}, {"text": "In WebNLG, delexicalisation was done on the subjects and objects of RDF triples.", "labels": [], "entities": []}, {"text": "While delexicalisation was flawless in E2E, WebNLG data poses additional challenges as the subject and object values in the input do not necessarily match the corresponding text fragment in the output.", "labels": [], "entities": []}, {"text": "As a result, not all subjects and objects were delexicalised.", "labels": [], "entities": []}, {"text": "In the delexicalised E2E corpus, placeholders constitute 5.7% of all tokens, while they reach 15.7% in the WebNLG data.", "labels": [], "entities": [{"text": "E2E corpus", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.8393016457557678}, {"text": "WebNLG data", "start_pos": 107, "end_pos": 118, "type": "DATASET", "confidence": 0.9834510684013367}]}, {"text": "The train/dev/test split is often constrained to ensure that there is no overlap in terms of input between the training, the development and the test set.", "labels": [], "entities": []}, {"text": "As recently showed however, this may result in a setup where certain input fragments (in that case, subject and object entities present in the input set of RDF triples) are present so often in the test set that models built on this standard split, overfit and memorise rather than learn.", "labels": [], "entities": []}, {"text": "Thus, in the split-andrephrase application they studied, observed that, given some input containing the entity e and some set of facts T (e) about this entity, the model will regularly output a text which mentions e but is unrelated to the set of facts T (e).", "labels": [], "entities": []}, {"text": "That is, instead of learning to generate text from data, the model learns to associate a text with an entity.", "labels": [], "entities": []}, {"text": "To better assess the impact of delexicalisation and copying on the output of data-to-text generation models, we therefore consider two ways of partitioning the corpus into train, dev and test: the traditional way (Unconstrained) where the overlapping constraint applies to entire inputs (i.e., sets of RDF triples in WebNLG and dialogue moves in E2E) and a more challenging split (Constrained) where the no-overlap constraint applies to input fragments (i.e., RDF triples in WebNLG and slotvalues in E2E).", "labels": [], "entities": []}, {"text": "shows the statistics for both splits for each dataset.", "labels": [], "entities": []}, {"text": "Unconstrained The unconstrained split is the original split provided by the challenge organisers.", "labels": [], "entities": []}, {"text": "The E2E dataset was split into training, validation and test sets following a 76.5/8.5/15 ratio.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9516633152961731}]}, {"text": "It was ensured that the input were distinct for all three sets and that a similar distribution of input and reference text lengths was kept.", "labels": [], "entities": []}, {"text": "We found 1,430 identical (data, text) pairs in the original E2E data.", "labels": [], "entities": [{"text": "E2E data", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9439310133457184}]}, {"text": "They were deleted for the subsequent experiments.", "labels": [], "entities": []}, {"text": "In WebNLG, the original split follows an 80/10/10 ratio.", "labels": [], "entities": [{"text": "WebNLG", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.9564671516418457}]}, {"text": "As with the E2E dataset, there is a null intersection in terms of input between train, dev and test.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.9650093615055084}]}, {"text": "In addition, sets of triples of different sizes and sets of triples of different categories were proportionally distributed between training, dev and test sets.", "labels": [], "entities": []}, {"text": "Constrained We consider a second partitioning where we aim to minimise the overlap between train, dev and test in terms of input fragments.", "labels": [], "entities": []}, {"text": "As shown in, in the E2E dataset, most of the slots have under eight possible values.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.9843409657478333}]}, {"text": "As these few values appear with a large number of distinct slot-value combinations (49,966 input-text instances), they are unlikely to trigger fact memorisation.", "labels": [], "entities": [{"text": "fact memorisation", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.7519292533397675}]}, {"text": "We therefore focus on those slots which have a higher number of values and restrict the test set using restaurant names, a slot with 34 possible values.", "labels": [], "entities": []}, {"text": "Four restaurant names were selected to occur only in the test data and to support a distribution of inputs types and text length similar to that of the original train/dev/test (cf..", "labels": [], "entities": []}, {"text": "Nonetheless, it is worth noting that the E2E dataset was constructed in such away that a specific restaurant name may have mutually exclusive values in different inputs, such as low customer rating and high customer rating.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9257685542106628}]}, {"text": "This might result in weak association between restaurant names and specific inputs and therefore, in little risk of memorising facts related to a specific restaurant name.", "labels": [], "entities": []}, {"text": "As we shall see in Section 3, this intuition is confirmed by the results which show little differences, for the E2E data, in terms of both automatic and human-based metrics between the Constrained and the Unconstrained setting.", "labels": [], "entities": [{"text": "E2E data", "start_pos": 112, "end_pos": 120, "type": "DATASET", "confidence": 0.9622513949871063}]}, {"text": "Note also that since the E2E Constrained split is defined with respect to a slot value (restaurant names) which is delexicalised, the constrained vs. unconstrained split distinction loses its impact in the delexicalised setting.", "labels": [], "entities": []}, {"text": "For the WebNLG dataset, the constraint on the train/dev/test partition is in terms of triples.", "labels": [], "entities": [{"text": "WebNLG dataset", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.9835913777351379}]}, {"text": "In addition to the exclusion from the test set all inputs (set of RDF triples) which occur in either the dev or the train set, we require that no RDF triple occurs in two of these sets.", "labels": [], "entities": []}, {"text": "Let t = (s, p, o) bean RDF-triple, with pa property and s, o subject and object RDF resources.", "labels": [], "entities": []}, {"text": "In the constrained dataset, if, t is in the test set, then t may not be in either the dev or the training set but variants such as (s , p, o), (s, p, o ), (s , p, o ) or (s, p , o) may (with s = s , p = p and o = o ).", "labels": [], "entities": []}, {"text": "In this way, models can be trained which must learn to verbalise properties independently of their arguments.", "labels": [], "entities": []}, {"text": "Again, care was taken to keep the distribution in terms of input length similar to that of the original split (cf.).", "labels": [], "entities": []}, {"text": "Automatic Evaluation Systems were evaluated using four automatic corpus-based metrics: BLEU (), NIST), METEOR), ROUGE L ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9988871216773987}, {"text": "METEOR", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9851877689361572}, {"text": "ROUGE L", "start_pos": 112, "end_pos": 119, "type": "METRIC", "confidence": 0.9721933305263519}]}, {"text": "We made use of the scripts used for the E2E Challenge evaluation . The first three metrics were originally developed for machine translation, the last one for summarisation.", "labels": [], "entities": [{"text": "E2E Challenge evaluation", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.8393419782320658}, {"text": "machine translation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7842970788478851}, {"text": "summarisation", "start_pos": 159, "end_pos": 172, "type": "TASK", "confidence": 0.9871677756309509}]}, {"text": "Roughly speaking, BLEU calculates the n-gram precision; NIST is based on BLEU, but adds more weight to rarer n-grams; METEOR computes the harmonic mean of precision and recall, featuring also stem and synonymy matching; ROUGE L calculates recall for common longest subsequences in a reference and candidate text.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9970195889472961}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9601316452026367}, {"text": "NIST", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.857263445854187}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9950942993164062}, {"text": "METEOR", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9637166261672974}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9986686706542969}, {"text": "recall", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.9968240261077881}, {"text": "ROUGE L", "start_pos": 220, "end_pos": 227, "type": "METRIC", "confidence": 0.9392990171909332}, {"text": "recall", "start_pos": 239, "end_pos": 245, "type": "METRIC", "confidence": 0.9737197160720825}]}, {"text": "Given our taskhandling rare items (or named entities in the corpora in question)-we also applied the slot-error rate (SER) to evaluate outputs which seems to be more suitable for evaluating the presence of named entities.", "labels": [], "entities": [{"text": "slot-error rate (SER)", "start_pos": 101, "end_pos": 122, "type": "METRIC", "confidence": 0.9771367073059082}]}, {"text": "SER was calculated by exact matching slot values in the candidate texts, where S is a number of substitutions, Dis a number of deletions, I is a number of insertions, and N is a total number of slots in the reference.", "labels": [], "entities": [{"text": "SER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.90534508228302}]}, {"text": "The resulting SER is an average of SER for each prediction.", "labels": [], "entities": [{"text": "SER", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9955446124076843}, {"text": "SER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9983757734298706}]}, {"text": "While computing SER for the dialogue slot-based E2E corpus is straightforward (the binary slot familyFriendly was excluded), it results in some noise for WebNLG where subjects and objects are numerous (3,648 vs. 79 values in E2E) and where they were rephrased in references (cf. also Section 2.2).", "labels": [], "entities": [{"text": "SER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9602909088134766}]}, {"text": "Manual Annotation To allow comparisons between constrained and unconstrained settings, we intersected inputs of constrained and unconstrained test sets and gathered corresponding predictions from them for all the models.", "labels": [], "entities": []}, {"text": "The intersection between the two test sets has 40 inputs in the E2E corpus and 153 in WebNLG.", "labels": [], "entities": [{"text": "E2E corpus", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.9743698537349701}, {"text": "WebNLG", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.987191915512085}]}, {"text": "For E2E, we manually evaluated all 40 predictions available for each system (constrained and unconstrained); for WebNLG, we chose 44 predictions ensuring the presence of different sizes and categories.", "labels": [], "entities": [{"text": "WebNLG", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.9356908798217773}]}, {"text": "By manually assessing outputs for the same inputs for all the systems, contrasts between constrained and unconstrained settings are better highlighted.", "labels": [], "entities": []}, {"text": "Manual inspection of outputs revealed that most of generated predictions did not encounter issues with grammar or fluency.", "labels": [], "entities": []}, {"text": "For this reason, we chose to focus on semantic adequacy of predicted texts.", "labels": [], "entities": []}, {"text": "The evaluation was done by one human judge.", "labels": [], "entities": []}, {"text": "After the evaluation was finished, the human annotator confirmed that, except for one system (see Section 3), all system outputs demonstrated fluent and grammatical English sentences.", "labels": [], "entities": []}, {"text": "Once presented with an input and a corresponding prediction text, a human judge was asked to evaluate semantic information present in the prediction.", "labels": [], "entities": []}, {"text": "A minimal unit of analysis was a slotvalue pair in E2E and an RDF triple element (sub-  ject, object, or property) in WebNLG.", "labels": [], "entities": [{"text": "WebNLG", "start_pos": 118, "end_pos": 124, "type": "DATASET", "confidence": 0.9359914660453796}]}, {"text": "For each semantic unit, the judge indicated if it was rendered correctly (right) or incorrectly (wrong) in the text.", "labels": [], "entities": []}, {"text": "If the unit was missing, it was noted as missed; new semantic content, not present in the source input, was labelled as added.", "labels": [], "entities": []}, {"text": "Then, the number of each type of annotations was calculated for each input and converted to percentage with respect to the number of slot-value pairs (E2E) or number of triple constituents (WebNLG).", "labels": [], "entities": []}, {"text": "Given the E2E example in, statistics about the example is the following: right: 2, wrong: 1, added: 1, missed: 1 (near[Burger King] was omitted).", "labels": [], "entities": [{"text": "Burger King]", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.9291525880495707}]}, {"text": "Total number of slots being 4, the performance in the percentage is then right: 50%, wrong: 25%, added: 25%, missed: 25%.", "labels": [], "entities": [{"text": "missed", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9682379961013794}]}, {"text": "WebNLG example annotations were done taking into account the three parts of a triple.", "labels": [], "entities": [{"text": "WebNLG", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9380119442939758}]}, {"text": "If a property was not translated correctly, we considered that a model missed out that information.", "labels": [], "entities": []}, {"text": "While a subject or object was not rendered correctly, they were annotated as wrong.", "labels": [], "entities": []}, {"text": "All the semantic information beyond the size of initial set of triples was evaluated as added.", "labels": [], "entities": []}, {"text": "The WebNLG example in received the following scores, the total number of constituents being three: right: 2 (66%), wrong: 1 (33%), missed: 0 (0%), added: 1 (33%).", "labels": [], "entities": []}, {"text": "If semantic information was repeated, it was rated as added.", "labels": [], "entities": []}, {"text": "The human evaluation analysis presented above is modest due to the lack of resources.", "labels": [], "entities": []}, {"text": "To justify it, we argue that our focus is solely on semantic adequacy which is a more objective parameter in evaluations than, say, fluency or grammaticality.", "labels": [], "entities": []}, {"text": "With no intent to question the documented unreliability of automatic metrics in NLG, we attribute such high correlations to the design of our configurations which cover some extreme cases where models are supposed to show a drastic drop in performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics on attribute values in E2E (left) and on RDF-triple constituents in WebNLG (right).", "labels": [], "entities": [{"text": "E2E", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9330340623855591}, {"text": "WebNLG", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.941349446773529}]}, {"text": " Table 3: Training/development/test sets statistics in E2E and WebNLG in original (unconstrained) and  constrained splits. Instances count is a number of (data, text) pairs; MRs count is a number of unique  data inputs.", "labels": [], "entities": [{"text": "E2E", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.9399559497833252}, {"text": "WebNLG", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.880160391330719}, {"text": "MRs count", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9677121043205261}]}, {"text": " Table 5: E2E dataset (D: Delexicalisation, D+C: delexicalisation and copying, C: copy and coverage,  NIL: Neither copy nor delexicalisation). The upper half of the table presents automatic evaluation results;  the lower half-human evaluation results. Best scores are in bold.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.9306747019290924}, {"text": "NIL", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9497405290603638}]}, {"text": " Table 6: WebNLG dataset (D: Delexicalisation, D+C: delexicalisation and copying, C: copy and cover- age, NIL: Neither copy nor delexicalisation). The upper half of the table presents automatic evaluation  results; the lower half-human evaluation results. Best scores are in bold. * -word repetitions present  in predictions.", "labels": [], "entities": [{"text": "WebNLG dataset", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.9791140854358673}, {"text": "NIL", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.912350058555603}]}]}