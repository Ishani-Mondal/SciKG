{"title": [{"text": "Named Entity Recognition for Hindi-English Code-Mixed Social Media Text", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7086930721998215}]}], "abstractContent": [{"text": "Named Entity Recognition (NER) is a major task in the field of Natural Language Processing (NLP), and also is a sub-task of Information Extraction.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7665409644444784}, {"text": "Information Extraction", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.8233038187026978}]}, {"text": "The challenge of NER for tweets lies in the insufficient information available in a tweet.", "labels": [], "entities": [{"text": "NER", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9331585168838501}]}, {"text": "There has been a significant amount of work done related to entity extraction, but only for resource-rich languages and domains such as the newswire.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8408233523368835}]}, {"text": "Entity extraction is, in general, a challenging task for such an informal text, and code-mixed text further complicates the process with it's unstructured and incomplete information.", "labels": [], "entities": [{"text": "Entity extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8221777975559235}]}, {"text": "We propose experiments with different machine learning classification algorithms with word, character and lexical features.", "labels": [], "entities": [{"text": "machine learning classification", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.6686415672302246}]}, {"text": "The algorithms we experimented with are Decision tree, Long Short-Term Memory (LSTM), and Conditional Random Field (CRF).", "labels": [], "entities": []}, {"text": "In this paper, we present a corpus for NER in Hindi-English Code-Mixed along with extensive experiments on our machine learning models which achieved the best f1-score of 0.95 with both CRF and LSTM.", "labels": [], "entities": [{"text": "NER", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9670273065567017}, {"text": "CRF", "start_pos": 186, "end_pos": 189, "type": "DATASET", "confidence": 0.8322114944458008}]}], "introductionContent": [{"text": "Multilingual speakers often switchback and forth between languages when speaking or writing, mostly in informal settings.", "labels": [], "entities": []}, {"text": "This language interchange involves complex grammar, and the terms \"code-switching\" and \"code-mixing\" are used to describe it Lipski.", "labels": [], "entities": []}, {"text": "Code-mixing refers to the use of linguistic units from different languages in a single utterance or sentence, whereas codeswitching refers to the co-occurrence of speech extracts belonging to two different grammatical systems Gumperz.", "labels": [], "entities": []}, {"text": "As both phenomena are frequently observed on social media platforms in similar contexts, we use only the code-mixing scenario in this work.", "labels": [], "entities": []}, {"text": "Following are some instances from a Twitter corpus of Hindi-English code-mixed texts also transliterated in English.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section present the experiments we performed with different combinations of features and systems.", "labels": [], "entities": []}, {"text": "In order to determine the effect of each feature and parameter of different models we performed several experiments with some set of feature vectors at a time and all at a time simultaneously changing the values of the parameters of our models like criterion ('Information gain', 'gini'), maximum depth of the tree for Decision tree model, optimization algorithms, loss functions in LSTM, regularization parameters and algorithms of optimization for CRF like 'L-BFGS' 5 , 'L2 regularization' 6 , 'Avg.", "labels": [], "entities": []}, {"text": "In all the models we mentioned above we validated our classification models with 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "which we arrived at after fine empirical tuning.", "labels": [], "entities": []}, {"text": "provides the experiments on CRF model.", "labels": [], "entities": []}, {"text": "The c1 and c2 parameters for CRF model refers to L1 regularization and L2 regularization.", "labels": [], "entities": []}, {"text": "These two regularization parameters are used to restrict our estimation of w * as mentioned in Section 5.1.", "labels": [], "entities": []}, {"text": "When experimented with algorithm of optimization as 'L2 regularization' or 'Average Perceptron' there is not any significant change in the results of our observation both in the per class statistics as well as the overall.", "labels": [], "entities": []}, {"text": "We arrived at these values of c1 and c2 after fine empirical tuning. and 5 refers to this observation.", "labels": [], "entities": []}, {"text": "Next we move to our experiments with LSTM model.", "labels": [], "entities": []}, {"text": "Here we experimented with the optimizer, activation function along with the number of units as well as number of epochs.", "labels": [], "entities": []}, {"text": "The best result that we came through was with using 'softmax' as activation function, 'adam' as optimizer and 'sparse categorical cross-entropy' for our loss function.", "labels": [], "entities": []}, {"text": "shows the statistics of running LSTM on our Dataset with 5-fold cross-validation having validation-split of 0.2 with our char, word and lexical feature set of our tokens.", "labels": [], "entities": []}, {"text": "shows one prediction instance of our LSTM model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tags and their Count in Corpus", "labels": [], "entities": [{"text": "Count", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.985559344291687}, {"text": "Corpus", "start_pos": 34, "end_pos": 40, "type": "TASK", "confidence": 0.367418497800827}]}, {"text": " Table 2: Inter Annotator Agreement.", "labels": [], "entities": [{"text": "Inter Annotator Agreement", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.8173224131266276}]}, {"text": " Table 3:  Decision Tree Model with 'max- depth=32'", "labels": [], "entities": [{"text": "max- depth", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9286834994951884}]}, {"text": " Table 4: CRF Model with 'c1=0.1' and 'c2=0.1'  and 'L-BFGS' algorithm", "labels": [], "entities": []}, {"text": " Table 5: CRF Model with 'Avg. Perceptron' Al- gorithm", "labels": [], "entities": []}, {"text": " Table 6: An Example Prediction of our LSTM  Model", "labels": [], "entities": []}, {"text": " Table 7: LSTM model with \"optimizer='adam'\"", "labels": [], "entities": []}]}