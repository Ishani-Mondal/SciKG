{"title": [], "abstractContent": [{"text": "The paper presents our participation in the WMT 2018 Metrics Shared Task.", "labels": [], "entities": [{"text": "WMT 2018 Metrics Shared Task", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.7281331300735474}]}, {"text": "We propose an improved version of Translation Edit/Error Rate (TER).", "labels": [], "entities": [{"text": "Translation Edit/Error Rate (TER)", "start_pos": 34, "end_pos": 67, "type": "METRIC", "confidence": 0.861676037311554}]}, {"text": "In addition to including the basic edit operations in TER, namely-insertion, deletion, substitution and shift, our metric also allows stem matching, optimizable edit costs and better normalization so as to correlate better with human judgement scores.", "labels": [], "entities": [{"text": "TER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.6599485874176025}, {"text": "stem matching", "start_pos": 134, "end_pos": 147, "type": "TASK", "confidence": 0.7928095459938049}]}, {"text": "The proposed metric shows much higher correlation with human judgments than TER.", "labels": [], "entities": [{"text": "TER", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9940701127052307}]}], "introductionContent": [{"text": "There has been several efforts to introduce better automatic evaluation metrics that can help towards the growth of machine translation (MT) systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.8421687543392181}]}, {"text": "Human evaluation is slow and expensive and thereby efficient automatic MT evaluation metrics are required which are faster and correlate strongly with human judgements.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.9552019536495209}]}, {"text": "Over the years a number of automatic MT evaluation metrics have been proposed like BLEU (), METEOR), Translation Edit Rate (), NIST, etc., which are widely used in the MT research and development community.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9291278421878815}, {"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9989866614341736}, {"text": "METEOR", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9922335147857666}, {"text": "Translation Edit Rate ()", "start_pos": 101, "end_pos": 125, "type": "METRIC", "confidence": 0.7914844751358032}, {"text": "NIST", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.4750654101371765}, {"text": "MT research and development", "start_pos": 168, "end_pos": 195, "type": "TASK", "confidence": 0.8889471888542175}]}, {"text": "However, due to its due to its simplicity and easier interpretability, Translation Edit Rate, or Translation Error Rate (TER), is one of the most commonly used MT evaluation metrics and often it is used as a baseline evaluation metric by MT researchers.", "labels": [], "entities": [{"text": "Translation Edit Rate", "start_pos": 71, "end_pos": 92, "type": "METRIC", "confidence": 0.7124577065308889}, {"text": "Translation Error Rate (TER)", "start_pos": 97, "end_pos": 125, "type": "METRIC", "confidence": 0.9153741896152496}, {"text": "MT evaluation", "start_pos": 160, "end_pos": 173, "type": "TASK", "confidence": 0.9112510979175568}, {"text": "MT", "start_pos": 238, "end_pos": 240, "type": "TASK", "confidence": 0.969588041305542}]}, {"text": "In this work, we propose anew MT \uf02a Work done while at Jadavpur University.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9894221425056458}]}, {"text": "evaluation metric which provides improvements over TER and achieves better correlation with human judgement scores on the segment-level for various language pairs.", "labels": [], "entities": [{"text": "TER", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9320814609527588}]}], "datasetContent": [{"text": "We tuned our metric on the training datasets of the WMT15 and obtained the following optimal sets of edit costs presented in..", "labels": [], "entities": [{"text": "WMT15", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.8357174396514893}]}, {"text": "As can be seen from, the proposed metric provides much higher correlation (9.62% \u2212 32.50%) for every language pair and target language than TER.", "labels": [], "entities": [{"text": "correlation", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.972984790802002}, {"text": "TER", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.7733665704727173}]}, {"text": "The fact that even for the enru language direction, the metric shows significant improvement in correlation without the stem matching component, indicates that most of the improvements are due to the optimal edit costs.", "labels": [], "entities": []}, {"text": "Apart from TER, we compared our results with the top performers of WMT16 segment level metrics (cf..", "labels": [], "entities": [{"text": "TER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.8843685388565063}, {"text": "WMT16 segment level metrics", "start_pos": 67, "end_pos": 94, "type": "DATASET", "confidence": 0.6188798397779465}]}, {"text": "sentBLEU is the segment level version of BLEU, MPEDA was developed on the basis of METEOR and METRICSF is a combination of three metrics, namely, BLEU, METEOR and UPF-COBALT (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9931628704071045}, {"text": "MPEDA", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.689673900604248}, {"text": "METEOR", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.8105706572532654}, {"text": "METRICSF", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9047818779945374}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9969623684883118}]}, {"text": "It can be inferred from that ITER performs significantly better than TER and it is among the top few performers.", "labels": [], "entities": [{"text": "ITER", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.5712929368019104}, {"text": "TER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9621605277061462}]}, {"text": "Specifically for ru-en, ITER provides the best result and surpasses all other metrics.", "labels": [], "entities": [{"text": "ITER", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.7766424417495728}]}, {"text": "We participated in the WMT 2018 Metrics Shared Task and submitted results for the \"no hybrids\" (newstest2018+testsuites) test set.", "labels": [], "entities": [{"text": "WMT 2018 Metrics Shared Task", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6052535533905029}, {"text": "newstest2018+testsuites) test set", "start_pos": 96, "end_pos": 129, "type": "DATASET", "confidence": 0.762906496723493}]}, {"text": "Due to resource constraints, we could not evaluate the \"hybrids\" test set which contain artificially created 10K+ system outputs per language pair and test set.", "labels": [], "entities": []}, {"text": "To establish better confidence intervals for system-level evaluation, the WMT18 metric task organizers computed system level scores for 10K hybrid super-sampled systems from our non-hybrid segment level scores using simple arithmetic average.", "labels": [], "entities": [{"text": "WMT18 metric task", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.8973405957221985}]}, {"text": "The results of our participation in the WMT 2018 Metrics Shared Task are reported in ().", "labels": [], "entities": [{"text": "WMT 2018 Metrics Shared Task", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6027946054935456}]}], "tableCaptions": [{"text": " Table 1: Optimal sets of edit costs obtained after  training ITER on WMT15 datasets (DAseg-wmt- newstest2015). Here, D_cost, I_cost, Sh_cost and  Sub_cost refer to the cost of deletion, insertion,  shifting and substitution, respectively.", "labels": [], "entities": [{"text": "WMT15 datasets", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.9815622568130493}]}]}