{"title": [{"text": "Addressing the Winograd Schema Challenge as a Sequence Ranking Task", "labels": [], "entities": [{"text": "Winograd Schema Challenge", "start_pos": 15, "end_pos": 40, "type": "DATASET", "confidence": 0.8290335337320963}]}], "abstractContent": [{"text": "The Winograd Schema Challenge targets pronominal anaphora resolution problems which require the application of cognitive inference in combination with world knowledge.", "labels": [], "entities": [{"text": "Winograd Schema Challenge", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.7311416864395142}, {"text": "pronominal anaphora resolution", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.6698821485042572}]}, {"text": "These problems are easy to solve for humans but most difficult to solve for machines.", "labels": [], "entities": []}, {"text": "Computational models that previously addressed this task rely on syntactic preprocessing and incorporation of external knowledge by manually crafted features.", "labels": [], "entities": []}, {"text": "We address the Winograd Schema Challenge from anew perspective as a sequence ranking task, and design a Siamese neural sequence ranking model which performs significantly better than a random baseline, even when solely trained on sequences of words.", "labels": [], "entities": [{"text": "Siamese neural sequence ranking", "start_pos": 104, "end_pos": 135, "type": "TASK", "confidence": 0.7041478902101517}]}, {"text": "We evaluate against a baseline and a state-of-the-art system on two data sets and show that anonymization of noun phrase candidates strongly helps our model to generalize.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Winograd Schema Challenge (WSC) targets difficult pronoun resolution problems which are easy to resolve for humans, but represent a great challenge for AI systems because they require the application of cognitive inferencing in combination with world knowledge (.", "labels": [], "entities": [{"text": "Winograd Schema Challenge (WSC)", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.6195349792639414}, {"text": "pronoun resolution", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.844510555267334}]}, {"text": "It has been argued that a computer that is able to solve WS problems with human-like accuracy must be able to perform \"human-like\" reasoning and that the WSC can be seen as an alternative to the Turing test.", "labels": [], "entities": [{"text": "WS problems", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9261883497238159}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9436037540435791}]}, {"text": "Consider the following Winograd Schema (WS): Example 1.1 The city councilmen refused the demonstrators a permit because they feared violence.", "labels": [], "entities": [{"text": "Winograd Schema (WS)", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.5432542443275452}]}, {"text": "Both city councilmen and demonstrators agree in number and gender and even in semantic type, as both mentions refer to groups of humans (with political interests).", "labels": [], "entities": []}, {"text": "While we could imagine a city with councilmen who approve violence and hence forbid a demonstration by peaceful protesters, this reading may appear nonsensical to most readers.", "labels": [], "entities": []}, {"text": "Most humans will straightforwardly resolve the pronoun they to corefer with the city councilmen.", "labels": [], "entities": []}, {"text": "Now consider the outcome of replacing a single word -the predicate feared -with the semantically related predicate advocated, yielding its twin sentence: Example 1.", "labels": [], "entities": []}, {"text": "The city councilmen refused the demonstrators a permit because they advocated violence.", "labels": [], "entities": []}, {"text": "With this change, the resolution is reversed: now they refers to the demonstrators.", "labels": [], "entities": []}, {"text": "Humans may reason that city councilmen are naturally concerned with the well-being of their city and thus they are not in favor of a demonstration by protesters who advocate violence.", "labels": [], "entities": []}, {"text": "Winograd problems as displayed in Examples 1.1 and 1.2 occur very rarely in natural language texts and cannot be properly resolved by traditional coreference resolution (CR) systems.", "labels": [], "entities": [{"text": "coreference resolution (CR)", "start_pos": 146, "end_pos": 173, "type": "TASK", "confidence": 0.8549448490142822}]}, {"text": "The primary reason is that standard CR systems heavily rely on features such as gender or number agreement or mention-distance information.", "labels": [], "entities": []}, {"text": "However, such features do not giveaway any knowledge that would be useful for resolving WS problems.", "labels": [], "entities": [{"text": "WS", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.7674306035041809}]}, {"text": "Given a random baseline of 0.5 accuracy, the Stanford resolver (, winner of the), achieves a sobering accuracy of 0.53 when facing Winograd Schema problems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.998683750629425}, {"text": "sobering accuracy", "start_pos": 93, "end_pos": 110, "type": "METRIC", "confidence": 0.7463521063327789}]}, {"text": "describe a state-of-the-art neural system for general neural coreference resolution and observe that, while trained on much more data than is available in the WSC, their system shows little advance in the uphill battle of resolving hard pronoun coreference problems that require world knowledge.", "labels": [], "entities": [{"text": "general neural coreference resolution", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.6908012628555298}, {"text": "WSC", "start_pos": 159, "end_pos": 162, "type": "DATASET", "confidence": 0.737359881401062}]}, {"text": "As our main contribution we are proposing a novel and very general take on the WSC task that we formulate as a sequence ranking task in a neural Siamese sequence ranking model.", "labels": [], "entities": [{"text": "WSC task", "start_pos": 79, "end_pos": 87, "type": "TASK", "confidence": 0.906331866979599}, {"text": "Siamese sequence ranking", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.6212788621584574}]}, {"text": "Moreover, we design features derived from manually designed knowledge bases and show how they can be integrated in this model.", "labels": [], "entities": []}, {"text": "We investigate anonymization of noun phrase candidates that significantly enhances the generalization capacity of the model.", "labels": [], "entities": []}, {"text": "We evaluate against baselines and a state-of-the-art (SOTA) system with special focus on the impact of different features and propose connotation frames as a novel feature for the WSC task.", "labels": [], "entities": [{"text": "WSC task", "start_pos": 180, "end_pos": 188, "type": "TASK", "confidence": 0.9327190518379211}]}, {"text": "All Siamese model variants, even those trained on word sequences only, show significant improvements over the baseline on our main testing set.", "labels": [], "entities": []}, {"text": "Our best performing model achieves 0.63 accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9940468072891235}]}], "datasetContent": [{"text": "Strict Data is Scarce: WSCL.", "labels": [], "entities": [{"text": "WSCL", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.8684235215187073}]}, {"text": "Starting with the work by, a collection of (currently) 282 strict WS problems is maintained online 1 , which will henceforth be referred to as We make a distinction between strict and relaxed Winograd Schemata.", "labels": [], "entities": []}, {"text": "Relaxed Winograd Schemata are problems which can be solved by computing simple corpus statistics.", "labels": [], "entities": [{"text": "Relaxed Winograd Schemata", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5696992675463358}]}, {"text": "E.g., The chimpanzee couldn't use Linux because it is an animal is of the relaxed type because a simple google query returns significantly more results for chimpanzee is an animal than Linux is an animal (19,700 vs. 3 hits).", "labels": [], "entities": []}, {"text": "Such relaxed, easyto-solve examples are not contained in the WSCL data set, but do occur in the WSCR data set, described below.", "labels": [], "entities": [{"text": "WSCL data set", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9802944461504618}, {"text": "WSCR data set", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.985466718673706}]}, {"text": "The problems in WSCL have an average length of 18 tokens.", "labels": [], "entities": [{"text": "WSCL", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.733507513999939}]}, {"text": "Some problems may consist of more than one sentence and require understanding across sentence boundaries.", "labels": [], "entities": []}, {"text": "The main dataset used in this work 3 , which we refer to as WSCR, was published by.", "labels": [], "entities": [{"text": "WSCR", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.9242346882820129}]}, {"text": "The data was created by 30 undergraduate students.", "labels": [], "entities": []}, {"text": "It comprises 943 twin sentences and comes already divided into training (70%) and test set (30%).", "labels": [], "entities": []}, {"text": "As opposed to the WSCL data, WSCR comprises both strict and relaxed Winograd schemata.", "labels": [], "entities": [{"text": "WSCL data", "start_pos": 18, "end_pos": 27, "type": "DATASET", "confidence": 0.9460380673408508}, {"text": "WSCR", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8644497990608215}]}, {"text": "We found that it also contains sentences with no straightforward resolution, as in Ex.", "labels": [], "entities": []}, {"text": "2.1 and 2.2 (with gold antecedents underlined): Example 2.1 Bob likes to play with Jimbo because he loves playing.", "labels": [], "entities": [{"text": "Jimbo", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.9435110688209534}]}, {"text": "Example 2.2 The bus driver yelled at the kid after she drove her vehicle.", "labels": [], "entities": []}, {"text": "When we presented these problems to a class of students, close to half of them voted for the other reading in Example 2.1 (more than half in Example 2.2).", "labels": [], "entities": [{"text": "Example", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.8925309181213379}]}, {"text": "This is reasonable, since the alternative reading (Jimbo loves playing) can be inferred from the fact that generally people like to play with someone who likes to play -rather than with someone who does not like to play.", "labels": [], "entities": []}, {"text": "The alternative reading of Example 2.2 could be even more likely, since it makes perfect sense that when a kid tries to drive the bus driver's vehicle, the bus driver will get angry and might yell at the kid.", "labels": [], "entities": [{"text": "Example 2.2", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.8718501925468445}]}, {"text": "When inspecting the data, we found that while notably having lower quality than WSCL, most sentences have a clearly preferred reading, which coheres with the gold annotation.", "labels": [], "entities": [{"text": "WSCL", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.8872127532958984}]}, {"text": "The problems in WSCR seem less diverse as all consist of exactly one sentence and in every sentence we find at least one discourse connector or a comma connecting a main clause with the antecedent candidates to a sub-clause that contains the pronoun.", "labels": [], "entities": [{"text": "WSCR", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.7133980393409729}]}, {"text": "Together with the WSCR data set, Rahman and Ng (2012) also publicized the description of a linear ranking system that achieves 73% accuracy on the published data.", "labels": [], "entities": [{"text": "WSCR data set", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.9848931630452474}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9986903071403503}]}, {"text": "The system relies on 8 features, which it uses to fit a SVM ranking model.", "labels": [], "entities": []}, {"text": "Contrary to our work, all features depend on syntactic dependency annotation.", "labels": [], "entities": []}, {"text": "While incorporating complex external knowledge resources such as FrameNet () or narrative chains, the most helpful feature turned out to be simple Google-queries, it significantly outperformed the random baseline with a considerable margin of 6% to the next best single feature.", "labels": [], "entities": []}, {"text": "attempted to replicate parts of the system, selecting five features.", "labels": [], "entities": []}, {"text": "Some of them were implemented 43 differently, e.g. instead of querying Google directly, the Google n-gram dataset) was used.", "labels": [], "entities": [{"text": "Google n-gram dataset", "start_pos": 92, "end_pos": 113, "type": "DATASET", "confidence": 0.8481897513071696}]}, {"text": "The authors present a system that extracts representative examples from the web.", "labels": [], "entities": []}, {"text": "Both systems were tested on a subset of the WSCR test set (for the problems where web examples were found).", "labels": [], "entities": [{"text": "WSCR test set", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.964426318804423}]}, {"text": "The reimplemented system yielded 0.56 accuracy while their own approach yielded 0.69 accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9984763264656067}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9978534579277039}]}, {"text": "Integer Linear Program (ILP).", "labels": [], "entities": []}, {"text": "use an ILP inference approach with a novel way of knowledge representation.", "labels": [], "entities": [{"text": "knowledge representation", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.725281685590744}]}, {"text": "Their system yields 0.76 accuracy on WSCR, which is the current state-of-the-art result on this data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9992995262145996}, {"text": "WSCR", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.6259236931800842}]}, {"text": "In their approach \"Predicate Schemas\" are instantiated and scored using knowledge acquired from external knowledge bases compiled into constraints fora decision.", "labels": [], "entities": []}, {"text": "Consider 'The bee landed on the flower because it {was hungry, had pollen}', where the gold resolution is that (i) the bee was hungry and (ii) the flower had pollen.", "labels": [], "entities": []}, {"text": "A simple predicate schema for this problem is instantiated as hungry(bee) vs. hungry(f lower) and has pollen(f lower) vs. has pollen(bee).", "labels": [], "entities": []}, {"text": "Scores for the instantiated predicates are then gathered from external knowledge sources such as Google . Other) on the generated abstract representations for reasoning about the correct antecedent.", "labels": [], "entities": []}, {"text": "for evaluation considers only causal attributive and direct causal events and performs experiments with only 4 twin problems for demonstration purposes.", "labels": [], "entities": []}, {"text": "We conclude that (i) all examined prior work focuses on either a specific subset of Winograd problems or/and is tested on only one specific data set, WSCL or WSCR but never both.", "labels": [], "entities": [{"text": "WSCL", "start_pos": 150, "end_pos": 154, "type": "DATASET", "confidence": 0.9163128733634949}, {"text": "WSCR", "start_pos": 158, "end_pos": 162, "type": "DATASET", "confidence": 0.8187035918235779}]}, {"text": "Also (ii) we are the first to present an end-to-end WSC system which, contrary to all prior methods, does not rely on sophisticated preprocessing or linguistic annotation.", "labels": [], "entities": [{"text": "WSC", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8971144556999207}]}, {"text": "(iii) We avoid heavy reliance on Google searches, which we argue the approaches of both Rahman and Ng (2012) and suffer from.", "labels": [], "entities": []}, {"text": "This is mainly due to two reasons: 1., Google has restricted automatic access to their search engine, making it difficult to solve more than a handful of pronoun resolution problems in short time without payment and, even more importantly 2., reproduction of results is impossible due to the nature of Google's search-algorithm as a black box -one cannot ensure to retrieve the exact same or even similar query results as previous authors.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.8008613884449005}]}, {"text": "Our work, by contrast, does not rely on non-reproducible features and will be the first to present an end-to-end neural approach for addressing the WSC.", "labels": [], "entities": [{"text": "addressing the WSC", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.5855338374773661}]}, {"text": "Unlike most other research on WSC, we test our models on both data sets discussed above -WSCL, the smaller data set of higher quality (282 examples) with strict and mostly unambiguous WSC cases, which we exclusively use for testing and WSCR, which comes in a predefined split of 1322 training and 564 testing problems, but which is of slightly reduced quality for the reasons discussed in Section 3.", "labels": [], "entities": [{"text": "WSC", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9700038433074951}, {"text": "WSCL", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.7200778722763062}]}, {"text": "Note that, as in previous work, we do not exploit the fact that each Winograd problem has a twin.", "labels": [], "entities": []}, {"text": "Given that the WS problems in WSCL and WSCR come in pairs with alternative resolutions to first vs. second antecedent candidate, we apply a random process as the baseline with 0.5 probability of achieving the correct guess.", "labels": [], "entities": [{"text": "WS", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9607668519020081}, {"text": "WSCL", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9234133362770081}, {"text": "WSCR", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.8692522644996643}]}, {"text": "Since the problem can be seen as a binary classification task, we calculate binomial tests to assess the probability of the zero-hypothesis that a random process achieves the same amount or more correct predictions than the evaluated system.", "labels": [], "entities": []}, {"text": "We also downloaded the state of the art system of, which the authors made publicly available 8 . However, it is important to note that the publicized system had been retrained on both training and testing data of WSCR 9 , making it difficult to re-evaluate it under the original experimental conditions.", "labels": [], "entities": [{"text": "WSCR 9", "start_pos": 213, "end_pos": 219, "type": "DATASET", "confidence": 0.8768887221813202}]}, {"text": "When evaluating the system with anonymized candidates, we only select cases where the integrated mention detection was able to detect both (and only both) candidates and linked the pronoun to one of those.", "labels": [], "entities": []}, {"text": "All other cases we have to treat as unresolved.", "labels": [], "entities": []}, {"text": "The downloaded system yields an accuracy of 0.99 (397 correct, 3 incorrect, 164 unresolved) on WSCR.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9994226694107056}, {"text": "WSCR", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.9886934161186218}]}, {"text": "In our evaluation Table we present the result from their paper: SOTA) As an additional baseline we use as a representation of the input sentences the representations predicted by a trained sentence embedding model, here InferSent (.", "labels": [], "entities": []}, {"text": "InferSent has been been trained on large-scale natural language inference tasks and therefore may have internalized valuable information about whether sentence readings are coherent or rather nonsensical.", "labels": [], "entities": []}, {"text": "We infer 4096-dimensional sentence vectors with the trained model provided by the authors and fit a linear ranker SVM, using randomly sampled development data to find a suitable regularization parameter.", "labels": [], "entities": []}, {"text": "We evaluate our models in two testing scenarios: (i) Train:WSCR+Test:WSCL: In this setup we train the model on the full WSCR data and test on the unseen WSCL data, to test the generalization capability of our models across data sets.", "labels": [], "entities": [{"text": "WSCR data", "start_pos": 120, "end_pos": 129, "type": "DATASET", "confidence": 0.9189906418323517}, {"text": "WSCL data", "start_pos": 153, "end_pos": 162, "type": "DATASET", "confidence": 0.9005440175533295}]}, {"text": "(ii) Train+Test:WSCR In the second scenario we use the predefined split of the WSCR data for training and evaluation.", "labels": [], "entities": [{"text": "WSCR", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.7843742370605469}, {"text": "WSCR data", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.9057023227214813}]}, {"text": "Since both scenarios do not involve a development set, we randomly split off 100 twin pair problems (200 examples) from the training data for development purposes.", "labels": [], "entities": []}, {"text": "Since there is much stochasticity in the models (stochastic gradient descent, parameter sampling, training-development split, etc.), we do five random initializations with different seeds.", "labels": [], "entities": []}, {"text": "We choose the model parameterizations from the epochs where they performed best on the development set.", "labels": [], "entities": []}, {"text": "These models predict the test set and we compute mean and standard deviation of accuracy.", "labels": [], "entities": [{"text": "standard", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.965339720249176}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.8627654314041138}]}, {"text": "We also introduce two ensembles, the na\u00a8\u0131vena\u00a8\u0131ve ensemble (Na\u00a8\u0131veENa\u00a8\u0131veE) and the Siamese ensemble (SiamE), which are majority voters informed by the predictions of the five different random seed models.", "labels": [], "entities": []}, {"text": "We examine the Na\u00a8\u0131veNa\u00a8\u0131ve model and the Siamese model, using all discussed features and pretrained, fixed 300 dimensional GloVe word embeddings ().", "labels": [], "entities": []}, {"text": "Dependency edge embeddings with 10 dimensions are initialized randomly from N 10 (0, 1).", "labels": [], "entities": []}, {"text": "The two embeddings for the anonymized mentions are drawn from N 300 (0, 1).", "labels": [], "entities": []}, {"text": "The Bi-LSTMs have 32 hidden units each, the weight matrix used for the linear transformation of the inputs is initialized according to, who proposed this initialization scheme to bring substantially faster convergence.", "labels": [], "entities": []}, {"text": "The weight matrix used for the linear transformation of the recurrent state is initialized as a random orthonormal matrix ( and the biases are initialized with zeros.", "labels": [], "entities": []}, {"text": "Parameters are searched with RMSProp (learning rate 0.001) and mini-batches of size 128 over 1,000 epochs.", "labels": [], "entities": [{"text": "RMSProp", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.8481440544128418}, {"text": "learning rate 0.001", "start_pos": 38, "end_pos": 57, "type": "METRIC", "confidence": 0.9127068320910136}]}, {"text": "displays our main results in the two experiment settings, with WSCL and WSCR as testing data.", "labels": [], "entities": [{"text": "WSCL", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.794319212436676}, {"text": "WSCR", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.8449434041976929}]}, {"text": "Surprisingly, when we test the SOTA system of on the strict WSCL data, the model fails to generalize.", "labels": [], "entities": [{"text": "WSCL data", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.9265615046024323}]}, {"text": "Again considering only the examples where the mention detection detected both and only both candidates and the pronoun was linked to one of them, it makes 24 correct and 22 false predictions and does not significantly outperform the random baseline (p=0.44).", "labels": [], "entities": []}, {"text": "Our model experiences the same problem when trained on WSCR and tested on WSCL -a random process produces more or the same amount of correct predictions with p=0.14.", "labels": [], "entities": [{"text": "WSCR", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9253584742546082}, {"text": "WSCL", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.946338415145874}]}, {"text": "The InferSent model, being pre-trained on large-scale NLI tasks proved to be a strong baseline and outperformed the baseline on both datasets by a notable margin, achieving the best result on WSCL (0.56 accuracy, significant on level p<0.05, non-significant for p<0.005).", "labels": [], "entities": [{"text": "WSCL", "start_pos": 192, "end_pos": 196, "type": "DATASET", "confidence": 0.5534769296646118}, {"text": "accuracy", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.9899511933326721}]}, {"text": "When trained on the WSCR training data and tested on the WSCR testing data, however our neural model significantly outperforms the random baseline by an observable margin of 9 percentage points (pp.) for Siam and 13 pp. for SiamE.", "labels": [], "entities": [{"text": "WSCR training data", "start_pos": 20, "end_pos": 38, "type": "DATASET", "confidence": 0.9274695515632629}, {"text": "WSCR testing data", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.969710648059845}, {"text": "SiamE", "start_pos": 224, "end_pos": 229, "type": "DATASET", "confidence": 0.8794004321098328}]}, {"text": "A traditional coreference system and winner of the) is significantly outperformed by our neural model by 10 pp.", "labels": [], "entities": []}], "tableCaptions": []}