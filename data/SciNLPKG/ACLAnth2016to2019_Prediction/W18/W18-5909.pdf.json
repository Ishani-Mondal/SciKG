{"title": [{"text": "Detecting Tweets Mentioning Drug Name and Adverse Drug Reaction with Hierarchical Tweet Representation and Multi-Head Self-Attention", "labels": [], "entities": [{"text": "Detecting Tweets Mentioning Drug Name and Adverse Drug Reaction", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.8364100787374709}]}], "abstractContent": [{"text": "This paper describes our system for the first and third shared tasks of the third Social Media Mining for Health Applications (SMM4H) workshop, which aims to detect the tweets mentioning drug names and adverse drug reactions.", "labels": [], "entities": [{"text": "Social Media Mining for Health Applications (SMM4H) workshop", "start_pos": 82, "end_pos": 142, "type": "TASK", "confidence": 0.7784757494926453}]}, {"text": "In our system we propose a neural approach with hierarchical tweet representation and multi-head self-attention (HTR-MSA) for both tasks.", "labels": [], "entities": []}, {"text": "Our system achieved the first place in both the first and third shared tasks of SMM4H with an F-score of 91.83% and 52.20% respectively.", "labels": [], "entities": [{"text": "SMM4H", "start_pos": 80, "end_pos": 85, "type": "TASK", "confidence": 0.6271771788597107}, {"text": "F-score", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9997262358665466}]}], "introductionContent": [{"text": "Social media services such as Twitter have become important platforms for information sharing and dissemination.", "labels": [], "entities": [{"text": "information sharing and dissemination", "start_pos": 74, "end_pos": 111, "type": "TASK", "confidence": 0.7159742787480354}]}, {"text": "Automatically detecting tweets which mentions drug names (DNs) and adverse drug reactions (ADRs) at a large scale is an interesting research topic and has many important applications such as pharmacovigilance.", "labels": [], "entities": [{"text": "Automatically detecting tweets which mentions drug names (DNs) and adverse drug reactions (ADRs)", "start_pos": 0, "end_pos": 96, "type": "TASK", "confidence": 0.8499770480043748}]}, {"text": "However, tweets are very noisy and informal, and full of misspellings (e.g., \"aspirn\" for \"aspirin\") and user-created abbreviations (e.g., \"COC\" for \"Cocaine\").", "labels": [], "entities": []}, {"text": "In addition, many DN and ADR mentions are context-dependent.", "labels": [], "entities": [{"text": "DN and ADR mentions", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.6442930474877357}]}, {"text": "For example, \"I take Vitamin C after meals\" is a tweet mentioning drug name, but the tweet \"Vitamin C is good for health\" is not.", "labels": [], "entities": []}, {"text": "Thus, the detection of DN and ADR mentioning tweets is very challenging.", "labels": [], "entities": [{"text": "DN and ADR mentioning tweets", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6169378459453583}]}, {"text": "In order to facilitate the research on automatic detection of tweets mentioning DN and ADR, two related shared tasks were released by the third Social Media Mining for Health Applications (SMM4H) workshop.", "labels": [], "entities": [{"text": "automatic detection of tweets mentioning DN and ADR", "start_pos": 39, "end_pos": 90, "type": "TASK", "confidence": 0.7743143998086452}, {"text": "Social Media Mining for Health Applications (SMM4H)", "start_pos": 144, "end_pos": 195, "type": "TASK", "confidence": 0.7400896085633172}]}, {"text": "Task 1 aims to classify whether a tweet mentions any drug names or dietary supplement.", "labels": [], "entities": []}, {"text": "1 https://healthlanguageprocessing.org/smm4h/ Task 3 aims to classify whether a tweet contains adverse drug reaction mention.", "labels": [], "entities": []}, {"text": "We designed a neural approach with hierarchical tweet representation and multi-head self-attention (HTR-MSA) to participate in these two tasks.", "labels": [], "entities": []}, {"text": "Our hierarchical tweet representation model first learns word representations from characters using convolutional neural network (CNN) and then learns tweet representations from words using a combination of Bidirectional long-short term memory (Bi-LSTM) network and CNN.", "labels": [], "entities": [{"text": "tweet representation", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.6873550415039062}]}, {"text": "In addition, we incorporated additional features to enhance the word representations, including pre-trained word embedding, part-of-speech (POS) tag embedding, sentiment features based on sentiment lexicons and lexicon features extracted from medical lexicons.", "labels": [], "entities": []}, {"text": "Besides, we applied multi-head self-attention mechanism to our approach to enhance the contextual representations of words by capturing the interactions between all words in tweets.", "labels": [], "entities": []}, {"text": "Our system achieved 91.83% F-score in Task 1 and 52.20% F-score in Task 3, and ranked 1st in both task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9759827852249146}, {"text": "F-score", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9957155585289001}]}, {"text": "The codes of our system are publicly available 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "The datasets provided by Task 1 and Task 3 in the shared tasks of the third SMM4H workshop ( were used in our experiments.", "labels": [], "entities": [{"text": "SMM4H workshop", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7069040238857269}]}, {"text": "The first one is for detection of tweets mentioning DNs (denoted as DN In our experiments, we use the 400-dim pretrained word embeddings released by.", "labels": [], "entities": []}, {"text": "The Bi-LSTM network has 2 \u00d7 200 units.", "labels": [], "entities": []}, {"text": "The CNN network has 400 filters with window size of 3.", "labels": [], "entities": [{"text": "CNN network", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9625612199306488}]}, {"text": "There are 16 heads in the multi-head self-attention network, and the output dimension of each head is 16.", "labels": [], "entities": []}, {"text": "RMSProp is selected as the optimizer.", "labels": [], "entities": [{"text": "RMSProp", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8105072975158691}]}, {"text": "Since the negative samples are dominant in the ADR dataset, we use the over-sampling strategy to balance the number of positive and negative samples.", "labels": [], "entities": [{"text": "ADR dataset", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.8153904676437378}]}, {"text": "Besides, in order to further improve the performance of our approach, we incorporate the ensemble strategy by independently training our model for 10 times and using the average prediction results.", "labels": [], "entities": []}, {"text": "The performance metric is F-score on positive samples.", "labels": [], "entities": [{"text": "F-score", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9988491535186768}]}, {"text": "In this section, we evaluate the performance of our approach by comparing it with baseline methods, including: (1) SVM, support vector machine with word unigram features; HTR, our basic hierarchical tweet representation model without self-attention; (7) HTR-MSA, our hierarchical tweet representation model with multi-head self-attention; (8) HTR-MSA-ens, using an ensemble of our HTR-MSA models.", "labels": [], "entities": [{"text": "HTR-MSA-ens", "start_pos": 343, "end_pos": 354, "type": "DATASET", "confidence": 0.8670556545257568}]}, {"text": "For fair comparisons, we use the same additional word features with our approach in all baseline methods.", "labels": [], "entities": []}, {"text": "We conducted 10-fold crossvalidation on the labeled tweets and the results are summarized in.", "labels": [], "entities": []}, {"text": "According to Table 1, our approach can outperform all the baseline methods.", "labels": [], "entities": []}, {"text": "This maybe because in our approach we learn word representations from not only the word embeddings but also the characters in words.", "labels": [], "entities": []}], "tableCaptions": []}