{"title": [{"text": "Automatically Tailoring Unsupervised Morphological Segmentation to the Language", "labels": [], "entities": [{"text": "Automatically Tailoring Unsupervised Morphological Segmentation", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.5516702830791473}]}], "abstractContent": [{"text": "Morphological segmentation is beneficial for several natural language processing tasks dealing with large vocabularies.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9191393554210663}]}, {"text": "Unsuper-vised methods for morphological segmen-tation are essential for handling a diverse set of languages, including low-resource languages.", "labels": [], "entities": []}, {"text": "(2016) introduced a Language Independent Morphological Segmenter (LIMS) using Adaptor Grammars (AG) based on the best-on-average performing AG configuration.", "labels": [], "entities": [{"text": "Language Independent Morphological Segmenter (LIMS)", "start_pos": 20, "end_pos": 71, "type": "TASK", "confidence": 0.734493442944118}]}, {"text": "However, while LIMS worked best on average and outper-forms other state-of-the-art unsupervised morphological segmentation approaches, it did not provide the optimal AG configuration for five out of the six languages.", "labels": [], "entities": []}, {"text": "We propose two language-independent classi-fiers that enable the selection of the optimal or nearly-optimal configuration for the morphological segmentation of unseen languages .", "labels": [], "entities": [{"text": "morphological segmentation of unseen languages", "start_pos": 130, "end_pos": 176, "type": "TASK", "confidence": 0.8085767745971679}]}], "introductionContent": [{"text": "As natural language processing becomes more interested in many languages, including lowresource languages, unsupervised morphological segmentation remains an important area of study.", "labels": [], "entities": [{"text": "unsupervised morphological segmentation", "start_pos": 107, "end_pos": 146, "type": "TASK", "confidence": 0.7113355398178101}]}, {"text": "For most of the languages of the world, we do not have morphologically annotated resources.", "labels": [], "entities": []}, {"text": "However, many human language technologies profit from morphological segmentation, for example machine translation () and speech recognition (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7543512880802155}, {"text": "speech recognition", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7788411676883698}]}, {"text": "In this paper, we build on previous work on unsupervised morphological segmentation using Adaptor Grammars (AGs), a type of nonparametric Bayesian models that generalize probabilistic context-free grammars (PCFGs), where the PCFG is typically a morphological grammar that specifies the word structure.", "labels": [], "entities": []}, {"text": "Specifically, we extend the research proposed by, who investigate a large space of parameters when using Adaptor Grammars related to (i) the underlying context-free grammar and (ii) the use of a \"Cascaded\" system in which one grammar chooses affixes to be seeded into another in order to simulate the situation where scholar-knowledge is available.", "labels": [], "entities": []}, {"text": "Their results on a development set of 6 languages (English, German, Finish, Turkish, Estonian and Zulu) show that the best performing AG-based configuration (grammar and learning setup) differ from language to language.", "labels": [], "entities": []}, {"text": "For processing unseen languages, proposed the Language-Independent Morphological Segmenter (LIMS) based on the best-on-average performing configuration when running leaveone-out cross validation on the development languages.", "labels": [], "entities": []}, {"text": "However, while LIMS works best on average and has been shown to outperform other stateof-the-art unsupervised morphological segmentation systems, it is not the optimal configuration for any of the development languages except Zulu.", "labels": [], "entities": []}, {"text": "Thus, in this paper we propose an approach to automatically select the optimal or nearly-optimal languageindependent configuration for the morphological segmentation of unseen languages.", "labels": [], "entities": [{"text": "morphological segmentation of unseen languages", "start_pos": 139, "end_pos": 185, "type": "TASK", "confidence": 0.8018834710121154}]}, {"text": "We train two classifiers on the development languages used by to make choices for unseen languages (Section 3).", "labels": [], "entities": []}, {"text": "We show that we can choose the best parameter settings for the six development languages in a leave-oneout cross validation, and also on an unseen test language (Arabic).", "labels": [], "entities": []}], "datasetContent": [{"text": "Adaptor Grammars (AGs) have been used successfully for unsupervised morphological seg- Morph = Affix/Morph representation as a sequence of morphs.", "labels": [], "entities": []}, {"text": "SubMorph (SM) = Lower level representation of characters as a sequence of sub-morphs.", "labels": [], "entities": []}, {"text": "\"+\" denotes one or more and \"?\" denotes optional.", "labels": [], "entities": []}, {"text": "mentation), which is the task of breaking down words in a language into a sequence of morphs.", "labels": [], "entities": []}, {"text": "An AG model typically has two main components: a PCFG and an adaptor that adapts the probabilities assigned to individual subtrees in the grammar.", "labels": [], "entities": []}, {"text": "For the task of morphological segmentation, a PCFG is typically a morphological grammar that specifies word structure.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.853978157043457}]}, {"text": "Given a list of input strings, AGs can learn latent tree structures.", "labels": [], "entities": []}, {"text": "developed several AG models based on different underlying contextfree grammars and learning settings, which we briefly introduce below.", "labels": [], "entities": []}, {"text": "introduce a set of 9 grammars (see) designed based on three dimensions: 1) how the grammar generates the prefix, stem and suffix (morph vs. tripartite), 2) the levels which are represented in nonterminals (e.g., compounds, morphs and sub-morphs) and 3) the levels at which the segmentation into output morphs is produced.", "labels": [], "entities": []}, {"text": "For example, in the PrStSu+SM grammar a word is modeled as a prefix, a stem and a suffix, where the prefix and suffix are sequences of zero or more morphs, while a morph is a sequence of sub-morphs, and the segmentation is based on the prefix, suffix and stem level.", "labels": [], "entities": [{"text": "PrStSu+SM grammar", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.6093362867832184}]}, {"text": "The PrStSu2a+SM grammar is similar, but a word is modeled as a prefix and stem-suffix sequence, where the prefix is optional, and stem-suffix is either a stem or a stem and a suffix (see for more details).", "labels": [], "entities": [{"text": "PrStSu2a+SM grammar", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6055554002523422}]}, {"text": "shows the trees for segmenting the word replayings using the PrStSu+SM and PrStSu2a+SM grammars.", "labels": [], "entities": [{"text": "segmenting the word replayings", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.6507834121584892}]}, {"text": "consider three learning settings: Standard (Std), Scholar-Seeded Knowledge (Sch) and Cascaded (Casc).", "labels": [], "entities": []}, {"text": "In the Standard setting, no scholar knowledge is introduced in the grammars, while in the Scholar-Seeded Knowledge setting the grammars are augmented with scholar knowledge in the form of information about affixes gathered from grammar books (before learning happens).", "labels": [], "entities": []}, {"text": "The Cascaded setting approximates the effect of scholar-seeded knowledge by first using a high-precision AG to derive a set of affixes and then insert those affixes into the grammars used in a second learning step.", "labels": [], "entities": []}, {"text": "show that the segmentation performance differs significantly across the different grammars, learning settings and languages.", "labels": [], "entities": []}, {"text": "For instance, the best performance for German is obtained by running the Standard PrStSu+SM configuration, while the Cascaded PrStSu2a+SM configuration produces the best segmentation for Finnish.", "labels": [], "entities": []}, {"text": "That means, there is no setup that yields the optimal segmentation for all languages.", "labels": [], "entities": []}, {"text": "For the processing of an unseen language (i.e., not part of the development), recommend using the Cascaded PrStSu+SM configuration (referred to as LIMS: Language-Independent Morphological Segmenter), as it is the best-on-average performing one when running leave-one-out cross validation on the development languages.", "labels": [], "entities": []}, {"text": "While LIMS works best on average, it is not the optimal configuration for any of the development languages except Zulu.", "labels": [], "entities": []}, {"text": "Thus, in this paper, we address the problem of automatically selecting the optimal or nearly-optimal language-independent (Standard or Cascaded) configuration for the morphological segmentation of unseen languages.", "labels": [], "entities": [{"text": "morphological segmentation of unseen languages", "start_pos": 167, "end_pos": 213, "type": "TASK", "confidence": 0.8062282919883728}]}, {"text": "We use the 6 development languages used by as well as Arabic as a fully unseen language.", "labels": [], "entities": []}, {"text": "The data for English, German, Finnish, Turkish and Estonian is from Morpho Challenge 1 , and the data for Zulu is from the Ukwabelana corpus ).", "labels": [], "entities": [{"text": "Morpho Challenge 1", "start_pos": 68, "end_pos": 86, "type": "DATASET", "confidence": 0.8535587787628174}, {"text": "Ukwabelana corpus", "start_pos": 123, "end_pos": 140, "type": "DATASET", "confidence": 0.9267253875732422}]}, {"text": "For the  unseen language we choose Arabic as it belongs to the Semitic family, while none of the development languages does.", "labels": [], "entities": []}, {"text": "We obtain the Arabic data by randomly selecting 50K words from the PATB corpus ().", "labels": [], "entities": [{"text": "PATB corpus", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.960014134645462}]}, {"text": "lists the sources and sizes of our corpora.", "labels": [], "entities": []}, {"text": "We report results using the EMMA F-measure score.", "labels": [], "entities": [{"text": "EMMA", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.5129494667053223}, {"text": "F-measure score", "start_pos": 33, "end_pos": 48, "type": "METRIC", "confidence": 0.8252964913845062}]}, {"text": "Results on an unseen language.", "labels": [], "entities": []}, {"text": "We evaluate our system on Arabic, a language that is not part of the development of the system.", "labels": [], "entities": []}, {"text": "Arabic also belongs to the Semitic family, where none of the development languages does.", "labels": [], "entities": []}, {"text": "For an unseen language, we first run the Standard PrStSu+SM configuration for 50 optimization iterations to obtain the morphological features.", "labels": [], "entities": []}, {"text": "We then run the KNN classifier on those features in order to obtain the final AG configuration.", "labels": [], "entities": []}, {"text": "Table 6 lists the EMMA F-scores for Arabic for all grammars in both the Standard and Cascaded setups.", "labels": [], "entities": [{"text": "EMMA", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.4933263957500458}, {"text": "F-scores", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.6312921047210693}]}, {"text": "Our KNN classifier picks the Standard PrStSu+SM configuration, which yields the best segmentation among all the configurations with an EMMA F-score of 0.701.", "labels": [], "entities": [{"text": "EMMA", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.6344569325447083}, {"text": "F-score", "start_pos": 140, "end_pos": 147, "type": "METRIC", "confidence": 0.52308189868927}]}, {"text": "Comparison with existing unsupervised approaches.", "labels": [], "entities": []}, {"text": "compares the performance of the selected configurations of our system (Table 5) to three other systems;   chosen).", "labels": [], "entities": []}, {"text": "Our system has EMMA F-score error reductions of 17.1%, 29.2% and 6.3% over Morfessor, MorphoChain 2 and LIMS, respectively, on average across the development languages and Arabic.", "labels": [], "entities": [{"text": "EMMA F-score error reductions", "start_pos": 15, "end_pos": 44, "type": "METRIC", "confidence": 0.8399835675954819}]}, {"text": "It is also only 0.003 of average EMMA Fscore behind an oracle system, where the best configuration is always selected (indicated as Best).", "labels": [], "entities": [{"text": "Fscore", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.44878751039505005}]}, {"text": "We are notable to compare versus the system presented by as neither their system nor their data is currently available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data source and size information. TRAIN  = training corpus, DEV = development corpus  and TEST = test corpus.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9966859221458435}, {"text": "DEV", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8789730072021484}, {"text": "TEST", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9621570706367493}]}, {"text": " Table 5: Overall system output. KNN = K-Nearest Neigh-", "labels": [], "entities": [{"text": "KNN", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.965884804725647}, {"text": "K-Nearest Neigh-", "start_pos": 39, "end_pos": 55, "type": "METRIC", "confidence": 0.7706168095270792}]}, {"text": " Table 6: Adaptor-grammar results (Emma F-scores) for the", "labels": [], "entities": []}, {"text": " Table 7: The performance of our system (Ours) compared", "labels": [], "entities": []}]}