{"title": [{"text": "Training and Prediction Data Discrepancies: Challenges of Text Classification with Noisy, Historical Data", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7258422523736954}]}], "abstractContent": [{"text": "Industry datasets used for text classification are rarely created for that purpose.", "labels": [], "entities": [{"text": "text classification", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8631332218647003}]}, {"text": "In most cases, the data and target predictions area by-product of accumulated historical data, typically fraught with noise, present in both the text-based document, as well as in the targeted labels.", "labels": [], "entities": []}, {"text": "In this work, we address the question of how well performance metrics computed on noisy, historical data reflect the performance on the intended future machine learning model input.", "labels": [], "entities": []}, {"text": "The results demonstrate the utility of dirty training datasets used to build prediction models for cleaner (and different) prediction inputs.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many benchmark text classification datasets gold-standard labels are available without additional annotation effort (e.g. a star rating fora product review, editorial keywords associated with a news article, etc.).", "labels": [], "entities": []}, {"text": "Large, manually annotated datasets for text classification are less common, especially in industry settings.", "labels": [], "entities": [{"text": "text classification", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8348081409931183}]}, {"text": "The cost and complexity of a large-scale industry labeling project (with specialized and/or confidential text) could be prohibitive.", "labels": [], "entities": []}, {"text": "As a result, industry data used for supervised Machine Learning (ML) was rarely created for that purpose.", "labels": [], "entities": [{"text": "supervised Machine Learning (ML)", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.7907461921374003}]}, {"text": "Instead, labels are often derived from secondary sources.", "labels": [], "entities": []}, {"text": "For example, text labels maybe derived from associated medical billing or diagnosis codes, from an outcome of a litigation, from a monetary value associated with a case, etc.", "labels": [], "entities": []}, {"text": "In most cases, the data and labels area by-product of accumulated historical data.", "labels": [], "entities": []}, {"text": "As such, noise is intrinsically present in the data fora variety of incidental reasons, interacting over along period of time.", "labels": [], "entities": []}, {"text": "In the case of text-based data for document classification, noise could be present in both the textbased document, as well as in the targeted labels.", "labels": [], "entities": [{"text": "document classification", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.707888275384903}]}, {"text": "There are numerous reasons that could explain the presence of text document noise.", "labels": [], "entities": []}, {"text": "For example, industry data based on scanned documents accumulated overtime is a common challenge.", "labels": [], "entities": []}, {"text": "In some cases, the original image could be lost or unavailable and one is left with the result of OCR engines with varying quality, that could also have changed overtime.", "labels": [], "entities": []}, {"text": "As various IT personnel handle the data over the years, unaware of its future use, data can be truncated or purged per storage/retention policies.", "labels": [], "entities": []}, {"text": "Similarly, character-encoding data transformation bugs and inconsistencies area common occurrence.", "labels": [], "entities": [{"text": "character-encoding data transformation", "start_pos": 11, "end_pos": 49, "type": "TASK", "confidence": 0.7134077548980713}]}, {"text": "In addition, the text data that contains the information needed for correct labelling could be interspersed with irrelevant text snippets, such as system generated messages or human entered notes used for different purposes.", "labels": [], "entities": []}, {"text": "The reasons for the noise in the targeted labels are also abundant.", "labels": [], "entities": []}, {"text": "In cases where the labels are created via human data entry / coding, the reasons could be as mundane as human error or inexperience.", "labels": [], "entities": []}, {"text": "In large organizations, department personnel training and management could differ and varying workflows can result in inconsistent labeling.", "labels": [], "entities": []}, {"text": "Labeling practices could also evolve overtime both at the organization, department, or individual employee levels.", "labels": [], "entities": []}, {"text": "Labeling could also be affected by external business reasons.", "labels": [], "entities": [{"text": "Labeling", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.958535373210907}]}, {"text": "For example, the coding scheme for medical billing codes could have evolved from ICD-9 coding to ICD-10 coding.", "labels": [], "entities": [{"text": "medical billing codes", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7110559741655985}]}, {"text": "The billing coding rules themselves could have changed fora variety of accounting and financial reasons, unrelated to the content of the corresponding textual data.", "labels": [], "entities": [{"text": "billing coding", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9495577216148376}]}, {"text": "Updates in data entry applications could result in a different set of dropdown or checkbox options, and, as a result, coding/labeling could change because of coincidental software updates.", "labels": [], "entities": []}, {"text": "In industry settings, the job of the data scientist often involves exploring, understanding, and utilizing historical datasets for the purposes building prediction models to consume current, and presumably cleaner, document inputs.", "labels": [], "entities": []}, {"text": "In such settings, the disparity between the training data and the data used for predictions poses a challenge.", "labels": [], "entities": []}, {"text": "Evaluations of various ML approaches should involve performance metrics using not necessarily the available historical training data, but different document inputs.", "labels": [], "entities": [{"text": "ML", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9847783446311951}]}, {"text": "In practice, however, data scientists often ignore the fact that the training data differs substantially from the data the ML model will take as input.", "labels": [], "entities": []}, {"text": "Algorithm selections, tuning, and performance metrics are often computed on historical data only.", "labels": [], "entities": []}, {"text": "In this work, we attempt to address the question of how well performance metrics computed on noisy, historical data reflect the performance on the intended future ML model input.", "labels": [], "entities": []}], "datasetContent": [{"text": "We focused on several document classification datasets varying in size, in number of training examples, in document length and document content/structure, as well as in the number of label categories.", "labels": [], "entities": [{"text": "document classification", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.6909091919660568}]}, {"text": "We utilized two common benchmark document classification datasets, and built a third artificial dataset utilizing 5 independent document datasets.", "labels": [], "entities": []}, {"text": "summarizes the datasets used in our experiments.", "labels": [], "entities": []}, {"text": "The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents (forum messages), partitioned across 20 different newsgroups.", "labels": [], "entities": [{"text": "Newsgroups dataset", "start_pos": 7, "end_pos": 25, "type": "DATASET", "confidence": 0.7531204223632812}]}, {"text": "The 2016 Yelp reviews dataset consists of more than 1 million user reviews accompanied with 1 to 5-star business rating (used as document labels).", "labels": [], "entities": [{"text": "Yelp reviews dataset", "start_pos": 9, "end_pos": 29, "type": "DATASET", "confidence": 0.8951130906740824}]}, {"text": "The dataset consists of all available Yelp user reviews dated 2016.", "labels": [], "entities": [{"text": "Yelp user reviews dated 2016", "start_pos": 38, "end_pos": 66, "type": "DATASET", "confidence": 0.9241703867912292}]}, {"text": "Both of the above dataset are relatively clean.", "labels": [], "entities": []}, {"text": "However, they both rely on user-entered labels.", "labels": [], "entities": []}, {"text": "This inevitably leads to some level of noise.", "labels": [], "entities": []}, {"text": "For example, in some cases, the content of the user review might not necessarily reflect the user-entered business rating.", "labels": [], "entities": []}, {"text": "Similarly, in the case of 20 Newsgroups, a user could send a message to a user group that doesn't necessarily reflect the best message category.", "labels": [], "entities": []}, {"text": "To measure accurately the effects of noise on various algorithm performance, we also created an artificially clean dataset (referred to as Synthetic).", "labels": [], "entities": []}, {"text": "To create the dataset we utilized 5 different document collections.", "labels": [], "entities": []}, {"text": "They include the 20 Newsgroups and a portion of the Yelp reviews dated 2016, described above.", "labels": [], "entities": [{"text": "Yelp reviews dated 2016", "start_pos": 52, "end_pos": 75, "type": "DATASET", "confidence": 0.9715301692485809}]}, {"text": "We also included the Reuters-21578 collection, a dataset consisting of over 21,000 Reuters news articles from 1987; a Farm Advertisements dataset (Mesterharm and Pazzani, 2011) consisting of over 4,000 website text advertisements on various farm animal related topics; a dataset of text abstracts describing National Science Foundation awards for basic research (Lichman, 2013).", "labels": [], "entities": [{"text": "Reuters-21578 collection", "start_pos": 21, "end_pos": 45, "type": "DATASET", "confidence": 0.9668140411376953}, {"text": "Farm Advertisements dataset", "start_pos": 118, "end_pos": 145, "type": "DATASET", "confidence": 0.7723369797070821}]}, {"text": "The label for each document correspond to the source dataset, i.e. the labels are newsgroup message, review, news article, website ad, v.s. grant abstract.", "labels": [], "entities": []}, {"text": "It is trivial fora human annotator to distinguish between the different document categories, and, at the same time, the classification decision involves some understanding of the document text and structure.", "labels": [], "entities": []}, {"text": "In addition, one of our noise-introducing techniques involves interspersing documents with irrelevant text.", "labels": [], "entities": []}, {"text": "To introduce irrelevant text snippets in the 20 Newsgroups dataset we utilized texts from the Yelp Reviews dataset and vice versa.", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.8520633578300476}, {"text": "Yelp Reviews dataset", "start_pos": 94, "end_pos": 114, "type": "DATASET", "confidence": 0.9816389481226603}]}, {"text": "To introduce noice in our synthetic dataset we utilized a dataset containing legal cases from the Federal Court of Australia (.", "labels": [], "entities": []}, {"text": "While a thorough comparison of supervised document classification algorithms and architectures is beyond the scope of this work, we experimented with a small number of commonly used in practice document classification algorithms: bag-of-words SVM (, a word-level convolutional neural network (CNN), and fastText (Joulin et al., 2016).", "labels": [], "entities": [{"text": "document classification", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7382039129734039}, {"text": "document classification", "start_pos": 194, "end_pos": 217, "type": "TASK", "confidence": 0.750562995672226}]}, {"text": "We set aside 30% of the available data from all three datasets as clean test sets.", "labels": [], "entities": []}, {"text": "These test sets represents the intended prediction input of the ML model.", "labels": [], "entities": [{"text": "ML", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9297685623168945}]}, {"text": "The rest 70% of the data was used for training.", "labels": [], "entities": []}, {"text": "Noise was gradually introduced into the training sets, which represents dirty, historical training data input that differs from the intended prediction input.", "labels": [], "entities": []}, {"text": "At each step, the different types of noise: Summary of the clean test dataset performance (orange line in).", "labels": [], "entities": [{"text": "Summary", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9760968089103699}]}, {"text": "The 3d and 4th columns show the slope of the performance degradation at noise levels 0.5 and 0.25.", "labels": [], "entities": []}, {"text": "For example, at 50% levels of noise, 50% of the documents were truncated by 50% of the length of the text; 50% of the documents were interspersed with 50% irrelevant text; for 50% of the set of the set of labels 50% of the document labels were randomly flipped; for 50% of the set of labels, 50% of the documents were replicated and their labels were randomly flipped.", "labels": [], "entities": []}, {"text": "The algorithm performance on the noisy training set was measured via cross-validation.", "labels": [], "entities": []}, {"text": "In all cases, training was performed without parameter tuning targeting the training dataset (clean or dirty versions).", "labels": [], "entities": []}, {"text": "In all cases, text normalization involved only converting the text to lowercase.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7860645055770874}]}, {"text": "The SVM classifiers were built using unigram bag-of-words, limiting the vocabulary to tokens that appear more than 5 times and in less than 50% of all documents.", "labels": [], "entities": []}, {"text": "For the two smaller dataset (20 Newsgroups and Synthetic) we utilized Wikipedia pre-trained word embeddings of size 100 for both the word-level CNN and fastText classifiers.", "labels": [], "entities": []}, {"text": "For the large Yelp Review dataset pre-trained embeddings were not used.", "labels": [], "entities": [{"text": "Yelp Review dataset", "start_pos": 14, "end_pos": 33, "type": "DATASET", "confidence": 0.9616297284762064}]}, {"text": "fastText was run using the default training parameters.", "labels": [], "entities": [{"text": "fastText", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9082331657409668}]}, {"text": "We experimented with two different word-level CNN architectures both producing comparable results 1 . illustrates the performance of the algorithms on the three datasets.", "labels": [], "entities": []}, {"text": "In all cases, the textclassification algorithms demonstrate resilience to noise.", "labels": [], "entities": []}, {"text": "The clean dataset performance (orange line) consistently outperforms cross-validation results on the dirty training dataset (blue line) . shows performance as noise is introduced into the dataset from 0 to 100%.", "labels": [], "entities": []}, {"text": "In practice, however, dirty historical data used for supervised ML, contains lower levels of noise (otherwise the dataset would be practically unusable).", "labels": [], "entities": [{"text": "ML", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9207785129547119}]}, {"text": "To compare performance of various algorithms in this more realistic setting, we measured the slope of the clean dataset accuracy as noise is introduced from 0 to 0.5, and from 0 to 0.25.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9861217141151428}]}, {"text": "Slope values closer to 0 indicates small performance degradation, while larger negative values correspond to greater performance degradation.", "labels": [], "entities": [{"text": "Slope", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9746178388595581}]}, {"text": "Results vary across datasets and algorithms, however, in all cases, the slope of the degradation of performance on a clean test set is small, indicating that all algorithms are able to successfully ignore noise signals at various degrees.", "labels": [], "entities": []}, {"text": "The wordlevel CNN classifier appears to be particularly resilient to relatively small amounts of noise and is the top performer for the 2 larger datasets (Yelp 2016 and Synthetic).", "labels": [], "entities": []}, {"text": "also shows the relative performance gain of results on the clean test set compared to results on the dirty training datasets at noise levels 0.5 and 0.25.", "labels": [], "entities": [{"text": "clean test set", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.7549203634262085}]}, {"text": "Results measured on the clean set outperform results measured on the dirty training dataset by an average of 25% at noise level 0.5 and an average of 11% at 0.25 noise levels.", "labels": [], "entities": [{"text": "dirty training dataset", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.5994307398796082}]}, {"text": "For the large dataset (Yelp 2016), word-level CNN results show the most significant performance gain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Summary of the datasets.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.5163995623588562}]}, {"text": " Table 3: Summary of the clean test dataset performance (orange line in", "labels": [], "entities": []}]}