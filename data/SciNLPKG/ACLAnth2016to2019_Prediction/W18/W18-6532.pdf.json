{"title": [{"text": "Generation of Company descriptions using concept-to-text and text-to-text deep models: dataset collection and systems evaluation", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we study the performance of several state-of-the-art sequence-to-sequence models applied to generation of short company descriptions.", "labels": [], "entities": [{"text": "generation of short company descriptions", "start_pos": 106, "end_pos": 146, "type": "TASK", "confidence": 0.7734196066856385}]}, {"text": "The models are evaluated on a newly created and publicly available company dataset that has been collected from Wikipedia.", "labels": [], "entities": []}, {"text": "The dataset consists of around 51K company descriptions that can be used for both concept-to-text and text-to-text generation tasks.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.7293685972690582}]}, {"text": "Automatic metrics and human evaluation scores computed on the generated company descriptions show promising results despite the difficulty of the task as the dataset (like most available datasets) has not been originally designed for machine learning.", "labels": [], "entities": []}, {"text": "In addition, we perform correlation analysis between automatic metrics and human evaluations and show that certain automatic metrics are more correlated to human judgments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditional approaches of Natural Language Generation (NLG) consist in creating specific algorithms in the consensual NLG pipeline.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.8137830793857574}]}, {"text": "However, recently there has been a very quick and strong interest in End-to-End (E2E) NLG systems in particular in the Dialogue community) which are data-driven NLG methods jointly learning sentence planning and surface realization.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 212, "end_pos": 231, "type": "TASK", "confidence": 0.7290902435779572}]}, {"text": "Probably the most well known current effort is the E2E NLG Challenge () which has generated a high number of submissions and whose task was to perform sentence planing and realization from dialogue actbased Meaning Representation (MR) on unaligned data.", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.8573211232821146}, {"text": "sentence planing and realization from dialogue actbased Meaning Representation (MR)", "start_pos": 151, "end_pos": 234, "type": "TASK", "confidence": 0.7922615061203638}]}, {"text": "This challenge was a great success as it gathered the community around this problem of data-driven NLG models and showed the diversity of techniques that has been proposed to deal with the proposed task.", "labels": [], "entities": []}, {"text": "The challenge also revealed that sequence-to-sequence (seq2seq) attention models such as TGEN are competitive, yet, other simpler templatebased approaches can still be effective.", "labels": [], "entities": []}, {"text": "It also showed that although automatic metrics are useful for learning, they cannot be blindly used to predict human performances in NLG (.", "labels": [], "entities": []}, {"text": "Furthermore, the E2E data contained a lot of redundancy of structure and a limited amount of concepts plus a least 5 references for the same MR input.", "labels": [], "entities": [{"text": "E2E data", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.9165599048137665}]}, {"text": "This is an ideal case for machine learning but is it the one that is encountered in all E2E NLG applications?", "labels": [], "entities": [{"text": "machine learning", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.729461133480072}]}, {"text": "In this work, we are interested in applying E2E models in areal world application in which there is a low amount of resources and whose output quality must beat human-level.", "labels": [], "entities": []}, {"text": "The task is to produce a short description of a company given either a semi-structured set of slots (MR) or a textual document.", "labels": [], "entities": []}, {"text": "This work is performed in the context of a research project with the Skopai company whose aim is to use AI technique to support startup description for attracting investors.", "labels": [], "entities": []}, {"text": "More precisely, the task will be to generate an abstract for the article that contains the main factual information about a company.", "labels": [], "entities": []}, {"text": "In this research, we focus on seq2seq models in order to generate a summary for an article for two approaches: concept-to-text and text-to-text.", "labels": [], "entities": []}, {"text": "As emphasized by, there seem to be a convergence of NLG and summarization techniques, that is why for both approaches were recently applied in the text-to-text domain.", "labels": [], "entities": [{"text": "summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.9832684397697449}]}, {"text": "Furthermore, text-to-text approaches have to explicitly deal with content selection and document planning, tasks that were not the focus of the E2E challenge.", "labels": [], "entities": [{"text": "content selection", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.6938440650701523}, {"text": "document planning", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.687729999423027}]}, {"text": "In the literature, there has been few work related to our objective.", "labels": [], "entities": []}, {"text": "For instance, in, authors used a neural model to generate short biographical summaries from structured data (concepts) using a dataset collected from Wikipedia.", "labels": [], "entities": []}, {"text": "Similarly, in) a sequenceto-sequence autoencoder with attention was used to generate biographies from Wikipedia.", "labels": [], "entities": []}, {"text": "However, both of these work concentrate on generating mainly one sentence summaries, while our aim is to be able to generate longer summaries.", "labels": [], "entities": []}, {"text": "In addition, since their generated summaries are extremely short, the information in the summaries is almost always is guaranteed to be present in the concepts, while in our dataset this is not the case.", "labels": [], "entities": []}, {"text": "Finally, most Wikipedia biographies have very similar and repetitive writing styles which makes it easier for models to learn them, but in the case of company descriptions, the task is more challenging since most summaries are written in a different style.", "labels": [], "entities": []}, {"text": "The contribution of the paper can be summarized as follow: \u2022 a collection of a realistic dataset for conceptto-text and text-to-text task that is made available to the community; \u2022 the implementation and evaluation of several E2E models for the two tasks; \u2022 an evaluation by naive human subjects and a comparison with the automatic metrics.", "labels": [], "entities": []}, {"text": "The paper starts by describing the dataset collection in Section 2 and then the seq2seq methods in Section 3.", "labels": [], "entities": []}, {"text": "Corpus-based experiments and human evaluation are described in Section 4 and 5 respectively.", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7002265751361847}]}, {"text": "The paper ends with a short discussion of the findings and an outlook for further work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The task understudy is to investigate the power of deep models on a specific task: the generation of company summaries.", "labels": [], "entities": [{"text": "generation of company summaries", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.802082359790802}]}, {"text": "However, no large dataset corresponding to this task was available when the research started.", "labels": [], "entities": []}, {"text": "Thus, a dataset about company descriptions has been collected, cleaned up, and organized for the task.", "labels": [], "entities": []}, {"text": "At the end of the process, although the dataset is faithful to the information found in Wikipedia, the dataset is not ideal for machine learning since the abstract, the body and the infobox are only loosely correlated.", "labels": [], "entities": []}, {"text": "For instance, shows an abstract which is not based on information provided in the body text.", "labels": [], "entities": []}, {"text": "Moreover, shows  In short, the problem found in the dataset can be summarized as follow.", "labels": [], "entities": []}, {"text": "Given a article a =< s, b, i > where sis the abstract b the body text, and i the infobox, the following problems exist: \u2022 sis not guaranteed to be built from b.", "labels": [], "entities": []}, {"text": "\u2022 sand i does not always contains the same information.", "labels": [], "entities": []}, {"text": "\u2022 s, i and b vary greatly in terms of size and none of the size is correlated.", "labels": [], "entities": []}, {"text": "Often, one or two of the sections are empty.", "labels": [], "entities": []}, {"text": "\u2022 there is only one version of s.", "labels": [], "entities": []}, {"text": "\u2022 there is no information about what was the objective of the writer(s) when producing s.", "labels": [], "entities": []}, {"text": "However, despites these problems, we believe this dataset represents a valuable resource since it represents the kind of data that can be found in real situations and that End2End systems must deal within order to make a significant impact in society.", "labels": [], "entities": []}, {"text": "The dataset is available for download 1 .   The original dataset has been through a limited amount of preprocessing for machine learning.", "labels": [], "entities": []}, {"text": "For the C2T approaches, the dataset presented in Section 2 is filtered to contain only companies having abstracts of at least 7 words and at most 105 words.", "labels": [], "entities": []}, {"text": "As a result of this process, 43681 companies are retained.", "labels": [], "entities": []}, {"text": "Finally the dataset is partitioned to learning (35384), dev(3929) and test(4368) sets.", "labels": [], "entities": []}, {"text": "For the T2T approaches, the dataset is filtered at first to keep only the companies having abstracts with less than 105 tokens and bodies greater than 100 tokens while having the size of the abstract smaller than the size of the body text.", "labels": [], "entities": []}, {"text": "As a result, 28034 are kept.", "labels": [], "entities": []}, {"text": "The dataset is then splitted into three sets: training (21309), dev (2357) and test.", "labels": [], "entities": []}, {"text": "In all the experiment the test set of 4368 companies is the same.", "labels": [], "entities": []}, {"text": "For the C2T and C2T char experiments, we used the seq2seq model by Google 3 , while for the C2T+pg, C2T+pg+cv, T2T+pg and T2T+pg+cv experiments, the Pointer-Generator Network implementation of was used.", "labels": [], "entities": []}, {"text": "In addition, a baseline model called lead4 was also implemented.", "labels": [], "entities": []}, {"text": "This baseline generates summaries by extracting the first 4 sentences from the article's body text.", "labels": [], "entities": []}, {"text": "The seq2seq model architecture has 2 layers of bidirectional LSTM trained using Adam optimization with learning rate of 0.001.", "labels": [], "entities": []}, {"text": "As for the Pointer-Generator Network, it uses a single layer of bidirectional LSTM trained with AdaGrad and learning rate of 0.15.", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.8624283671379089}, {"text": "learning rate", "start_pos": 108, "end_pos": 121, "type": "METRIC", "confidence": 0.9651784896850586}]}, {"text": "Both models have 256 hidden units for the encoder, decoders and embedding layers and a vocabulary size 50K (only for word models).", "labels": [], "entities": []}, {"text": "The choice of hyper-parameters were determined by tunning the models on the dev set.", "labels": [], "entities": []}, {"text": "Seq2seq models were trained until the loss on the dev set stops decreasing for several consecutive iterations.", "labels": [], "entities": []}, {"text": "As for the Pointer-Generator Network and coverage models, we followed the strategy suggest in, i.e., to train the models with highly-truncated sequences then increase them during the training process until the maximum length is reached.", "labels": [], "entities": []}, {"text": "Then the coverage mechanism is added and training is continued from the last training point of the Pointer-Generator Network.", "labels": [], "entities": []}, {"text": "Standard automatic measures BLEU (), ROUGE-L (,) and CIDEr ( were computed using the E2E challenge script.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.942025899887085}, {"text": "ROUGE-L", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9968116879463196}, {"text": "CIDEr", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.8828238248825073}, {"text": "E2E challenge script", "start_pos": 85, "end_pos": 105, "type": "DATASET", "confidence": 0.8653035958607992}]}, {"text": "shows evaluation results on both the dev and test sets for the lead4 (baseline), C2T and T2T tasks.", "labels": [], "entities": []}, {"text": "The best system is difficult to extract from these results since there are close.", "labels": [], "entities": []}, {"text": "However, C2T char exhibits the best results for BLEU, ROUGE-L and CIDEr on the dev set while C2T+pg exhibits the best results for ROUGE-L and CIDEr on the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9985001087188721}]}, {"text": "T2T+pg+cv shows the best NIST on both sets while lead4 is unbeatable from the METEOR perspective.", "labels": [], "entities": [{"text": "NIST", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9463188052177429}, {"text": "METEOR", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.7316017746925354}]}, {"text": "With respect to the results reported in the literature, such as the ones of the E2E challenge (for which the baseline system reaches: BLEU=0.6593; NIST=8.6094; METEOR=0.4483, ROUGE-L=0.6850, CIDEr=2.2338) these results are very low except for ROUGE-L.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9994458556175232}, {"text": "NIST", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9913700819015503}, {"text": "METEOR", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9907930493354797}, {"text": "ROUGE-L", "start_pos": 175, "end_pos": 182, "type": "METRIC", "confidence": 0.9929438233375549}, {"text": "CIDEr", "start_pos": 191, "end_pos": 196, "type": "METRIC", "confidence": 0.9638815522193909}, {"text": "ROUGE-L", "start_pos": 243, "end_pos": 250, "type": "METRIC", "confidence": 0.9773619771003723}]}, {"text": "However, the main reason for such a large difference is that in the E2E challenge there are several references for each instance of the data.", "labels": [], "entities": []}, {"text": "This leads to a higher ratio of match between the generated sentence words and the references ones, and thus, higher scores.", "labels": [], "entities": []}, {"text": "In order to verify this, we conducted few tests using our C2T char model on the E2E challenge data without any parameter tunning.", "labels": [], "entities": [{"text": "E2E challenge data", "start_pos": 80, "end_pos": 98, "type": "DATASET", "confidence": 0.9525535901387533}]}, {"text": "The results showed that when only a single reference is counted, our model was able to achieve a score of 0.29 and 0.47 for BLEU and ROUGE-L respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9985349178314209}, {"text": "ROUGE-L", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9792126417160034}]}, {"text": "However, when multiple references were included, the scores increased to 0.51 and 0.61 for BLEU and ROUGE-L.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9960642457008362}, {"text": "ROUGE-L", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9520291686058044}]}, {"text": "This clearly shows that the E2E challenge dataset is closer to the ideal case for machine learning than our case.", "labels": [], "entities": [{"text": "E2E challenge dataset", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.8185447255770365}]}, {"text": "Thus our results should not be directly compared with the E2E challenge.", "labels": [], "entities": []}, {"text": "However, we also computed the F1 of ROUGE 1, 2 and L score using the pyrouge package and compared to recent summarization methods . In that case C2T+pg exhibits the best results for ROUGE-1 (.3346) ROUGE-2 (.1701) and ROUGE-L (.3132) on the test set.", "labels": [], "entities": [{"text": "F1", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9994106292724609}]}, {"text": "These results are comparable to the abstractive method of (Nal- However, they were both tested on the CNN/Daily Mail test set, for which no problem of content mismatch between documents and summaries were reported while it is a difficulty of our dataset.", "labels": [], "entities": [{"text": "CNN/Daily Mail test set", "start_pos": 102, "end_pos": 125, "type": "DATASET", "confidence": 0.9338160951932272}]}, {"text": "Furthermore, the difference with the E2E challenge is important since our dataset contains a large vocabulary, a large number of named entities and only one -not always reliable-reference summary.", "labels": [], "entities": []}, {"text": "The few number of reference summaries give fewer opportunity for the models output to match ngrams in the references than when multiple references are available.", "labels": [], "entities": []}, {"text": "Although such metrics suggest our models are far from achieving satisfying results, they give in fact little insight about the actual weakness of the models.", "labels": [], "entities": []}, {"text": "Moreover correlation between automatic and human-based metrics in NLG is still debatable (.", "labels": [], "entities": []}, {"text": "That is why we conducted a human evaluation as well.", "labels": [], "entities": []}, {"text": "In order to gain more insight about the generation properties of each model a human evaluation with 19 human subjects was performed.", "labels": [], "entities": []}, {"text": "We setup a web-based experiment which was circulated inside the lab but to people who were not involved in this project.", "labels": [], "entities": []}, {"text": "The 4 questions below were asked on a 5-point Lickert scale: Q1 How do you judge the Information Coverage of the company summary : 1 no information, 5 contains everything Q2 How do you judge the Non-Redundancy of Information in the company summary.", "labels": [], "entities": []}, {"text": "1: means lots of repeated information, 5: no repetition.", "labels": [], "entities": [{"text": "repetition", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9761072397232056}]}, {"text": "Q3 How do you judge the Semantic Adequacy of the company summary?", "labels": [], "entities": []}, {"text": "1: lots of semantical mistakes, 5: semantically very correct.", "labels": [], "entities": []}, {"text": "Q4 How do you judge the Grammatical Correctness of the company summary?", "labels": [], "entities": [{"text": "Grammatical Correctness", "start_pos": 24, "end_pos": 47, "type": "METRIC", "confidence": 0.7094241082668304}]}, {"text": "1: very incorrect, 5: very good We did not include fluency in the question since it is often correlated with grammar and because participants have difficulty to judge this property.", "labels": [], "entities": []}, {"text": "Q1 to 4 were specifically designed to measure the recurrent weakness of seq2seq models: content selection, repetition, hallucination and bad segment connection.", "labels": [], "entities": [{"text": "repetition", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.9259310960769653}]}, {"text": "Participants were exposed to a screen where a background (extract of the original Wikipedia body text, cut to 400 max), an infobox and a summary were visible all together in the screen.", "labels": [], "entities": [{"text": "Wikipedia body text", "start_pos": 82, "end_pos": 101, "type": "DATASET", "confidence": 0.874263862768809}]}, {"text": "After reading the background, the infobox and the summary, the participant could answer the question by scrolling down.", "labels": [], "entities": []}, {"text": "Not limit of time was imposed.", "labels": [], "entities": []}, {"text": "A first example was given for training, then each participant had to treat 10 summaries.", "labels": [], "entities": []}, {"text": "The participant could not go to the next step without explicitly answering all questions.", "labels": [], "entities": []}, {"text": "In average one session last 15 minutes.", "labels": [], "entities": []}, {"text": "At no time participants have been aware that one of the summary was human generated (i.e., the Wikipedia abstract).", "labels": [], "entities": []}, {"text": "30 companies were selected from the 4368 companies of the test set.", "labels": [], "entities": []}, {"text": "They were selected based on the number of views during the month preceding the experiment.", "labels": [], "entities": []}, {"text": "The less viewed one were retained to avoid participants judging well known companies.", "labels": [], "entities": []}, {"text": "Results of the human experiment are reported in.", "labels": [], "entities": []}, {"text": "The first line report the result of the reference (i.e., the Wikipedia abstract) for comparison.", "labels": [], "entities": []}, {"text": "It is clear from the coverage metric that no system nor the reference was seen as doing a good job at conveying the information.", "labels": [], "entities": []}, {"text": "It is a known problem of the Wikipedia dataset and the systems were notable to do better than the reference.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.9788739085197449}]}, {"text": "Nonredundancy metric gives a more contrasted view of the systems.", "labels": [], "entities": []}, {"text": "C2T+pg was judged to be the least repetitive after the reference, while C2T char to be the most repetitive.", "labels": [], "entities": []}, {"text": "Regarding semantic correctness, C2T+pg is clearly above the others again including the reference.", "labels": [], "entities": []}, {"text": "Same observation can be made for grammatical correctness.", "labels": [], "entities": [{"text": "grammatical correctness", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7197825312614441}]}, {"text": "These results of human evaluation were compared to those of the automatic metrics (excluding the reference one).", "labels": [], "entities": []}, {"text": "The correlation matrix is given in.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9906726479530334}]}, {"text": "It can be seen that among automatic metrics, METEOR, ROUGE-L and CIDEr are highly correlated.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9591991305351257}, {"text": "ROUGE-L", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9959976673126221}, {"text": "CIDEr", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.8662876486778259}]}, {"text": "When it comes to human vs automatic metrics, it is obvious that CIDEr has a highest correlation with semantic and grammar.", "labels": [], "entities": []}, {"text": "It is worth noting that ROUGE-L is is also highly correlated to semantic and grammar.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9668068885803223}]}, {"text": "C2T char: rgb entertainment is an argentine television production company based in argentina.", "labels": [], "entities": [{"text": "C2T char: rgb entertainment", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.8540879368782044}]}, {"text": "the company was founded in 2000 by gustavo yankelevich yankelevich yankelevich y victor gonzalez in 2000.", "labels": [], "entities": []}, {"text": "C2T+pg: rgb entertainment is a television production company in argentina . T2T+pg: the argentine channel productions is an american film production and distribution company . the company was founded in 2000 by gustavo yankelevich and victor gonzalez in buenos , argentina . it is owned by ideas group.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Systems results on dev and test set using the E2E challenge metrics scripts provided with the  baseline", "labels": [], "entities": []}, {"text": " Table 2: Results of the human evaluation per sys- tem.", "labels": [], "entities": []}]}