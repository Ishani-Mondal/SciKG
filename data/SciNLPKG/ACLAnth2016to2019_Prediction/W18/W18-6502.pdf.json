{"title": [], "abstractContent": [{"text": "We aim to automatically generate natural language descriptions about an input structured knowledge base (KB).", "labels": [], "entities": []}, {"text": "We build our generation framework based on a pointer network which can copy facts from the input KB, and add two attention mechanisms: (i) slot-aware attention to capture the association between a slot type and its corresponding slot value; and (ii) anew table position self-attention to capture the inter-dependencies among related slots.", "labels": [], "entities": []}, {"text": "For evaluation, besides standard metrics including BLEU, METEOR, and ROUGE, we propose a KB reconstruction based metric by extracting a KB from the generation output and comparing it with the input KB.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9984421133995056}, {"text": "METEOR", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9345909357070923}, {"text": "ROUGE", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9943800568580627}]}, {"text": "We also create anew data set which includes 106,216 pairs of struc-tured KBs and their corresponding natural language descriptions for two distinct entity types.", "labels": [], "entities": []}, {"text": "Experiments show that our approach significantly outperforms state-of-the-art methods.", "labels": [], "entities": []}, {"text": "The reconstructed KB achieves 68.8%-72.6% F-score.", "labels": [], "entities": [{"text": "KB", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.5510967373847961}, {"text": "F-score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.991734504699707}]}], "introductionContent": [{"text": "Show and tell, showing an audience something and telling them about it, is a common classroom activity for early elementary school kids.", "labels": [], "entities": []}, {"text": "As a similar practice for knowledge propagation, we often need to describe and/or explain the information in a structured knowledge base (KB) in natural language, in order to make the knowledge elements and their connections easier to comprehend.", "labels": [], "entities": [{"text": "knowledge propagation", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.732609823346138}]}, {"text": "For example, () presents a natural language generation system to convert structured medical records to natural language text descriptions, which enables more effective communication between healthcare providers and their patients and among healthcare providers themselves.", "labels": [], "entities": []}, {"text": "Moreover, 51% of entity attributes in the current English Wikipedia Infoboxes are not described in English articles in the Wikipedia dump of April 1, 2018.", "labels": [], "entities": [{"text": "English Wikipedia Infoboxes", "start_pos": 50, "end_pos": 77, "type": "DATASET", "confidence": 0.6608512997627258}, {"text": "Wikipedia dump of April 1", "start_pos": 123, "end_pos": 148, "type": "DATASET", "confidence": 0.8932878255844117}]}, {"text": "The availability of vast amounts of Linked Open Data (LOD) and Wikipedia derived resources such as DBPedia, WikiData and YAGO encourages pursuing anew direction of knowledge-driven ( or semantically oriented) Natural Language Generation (NLG).", "labels": [], "entities": [{"text": "semantically oriented) Natural Language Generation (NLG)", "start_pos": 186, "end_pos": 242, "type": "TASK", "confidence": 0.7547077801492479}]}, {"text": "We aim to fill in this knowledge gap by developing a system that can take a KB (consisted of a set of slot types and their values) about an entity as input (see example in), and automatically generate a natural language description ( plates and styles which human use to describe the same slot type.", "labels": [], "entities": []}, {"text": "For example, to describe a football player's membership with a team, we can use various phrases including member of, traded to, drafted by, played for, face of, loaned to and signed for.", "labels": [], "entities": []}, {"text": "Instead of manually crafting patterns for each slot type, we leverage the existing pairs of structured slots from Wikipedia infoboxes and Wikidata) and the corresponding sentences describing these slots in Wikipedia articles as our training data, to learn a deep neural network based generator.", "labels": [], "entities": []}, {"text": "Pointer network to copy over facts.", "labels": [], "entities": []}, {"text": "The previous work ( ) considers the slot type and slot value as two sequences and applies a sequence to sequence (seq2seq) framework) for generation.", "labels": [], "entities": []}, {"text": "However, the task of describing structured knowledge is fundamentally different from creative writing, because we need to cover the knowledge elements contained in the input KB, and the goal of generation is mainly to clearly describe the semantic connections among these knowledge elements in an accurate and coherent way.", "labels": [], "entities": []}, {"text": "The seq2seq model fails to capture such connections and tends to generate wrong information (e.g., Thailand in).", "labels": [], "entities": []}, {"text": "To address this challenge, we choose a pointer network to copy slot values directly from the input KB.", "labels": [], "entities": []}, {"text": "However, the copying mechanism in the pointer network is notable to capture the alignment between a slot type and its slot value, and thus it often assigns facts to wrong slots.", "labels": [], "entities": []}, {"text": "For example, 22 in should be the number of matches instead of birth date.", "labels": [], "entities": []}, {"text": "It also tends to repeat the same slot value based on language model, e.g., \"Uroplatus ebenaui is a of gecko endemic to Madagascar.", "labels": [], "entities": []}, {"text": "The Uroplatus is a member of the species of the genus Madagascar.\".", "labels": [], "entities": []}, {"text": "We propose a Slot-aware Attention mechanism to compute slot type attention and slot value attention simultaneously and capture their correlation.", "labels": [], "entities": []}, {"text": "Attention mechanism in deep neural networks) is inspired from human visual attention, which refers to human's capability to focus on a certain region of an image with high resolution while perceiving the surrounding image in low resolution.", "labels": [], "entities": []}, {"text": "It allows the neural network to have access to the hidden state of the encoder, and thus learn what to attend to.", "labels": [], "entities": []}, {"text": "For example, fora Date of Birth slot type, words such as born may receive higher attention than female.", "labels": [], "entities": []}, {"text": "As we can see in (+Type), the output with slot type attention contains more precise slots.", "labels": [], "entities": []}, {"text": "Multiple slots are often interdependent.", "labels": [], "entities": []}, {"text": "For example, a football player may join multiple teams, with each team associated with a certain number of points, goals, scores and games participated.", "labels": [], "entities": []}, {"text": "We design anew table position based self-attention to capture correlations among interdependent slots and put them in the same sentence.", "labels": [], "entities": []}, {"text": "For example, our model successfully associates the number of matches 22 with the Israel women's national football team as shown in.", "labels": [], "entities": []}, {"text": "The major contributions of this paper are: \u2022 For the first time, we propose anew table position attention which proves to be effective at capturing inter-dependencies among facts.", "labels": [], "entities": [{"text": "table position attention", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.8263144294420878}]}, {"text": "This new approach achieves 2.5%-7.8% Fscore gain at KB reconstruction.", "labels": [], "entities": [{"text": "Fscore gain", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.980067104101181}, {"text": "KB reconstruction", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.6956909894943237}]}, {"text": "\u2022 We propose a KB reconstruction based metric to evaluate how many facts are correctly expressed in the generation output.", "labels": [], "entities": []}, {"text": "\u2022 We create a large dataset of KBs paired with natural language descriptions for 106,216 entities, which can serve as anew benchmark.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply the standard BLEU (), METEOR, and ROUGE) metrics to evaluate the generation performance, because they can measure the content overlap between system output and ground-truth and also check whether the system output is written in sufficiently good English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9991083741188049}, {"text": "METEOR", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.977679431438446}, {"text": "ROUGE", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9963088631629944}]}, {"text": "In addition, we can also consider natural language as the most expressive way for knowledge transmission via a noisy channel.", "labels": [], "entities": [{"text": "knowledge transmission", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7099436223506927}]}, {"text": "If we are able to reconstruct the input KB from the generated description, our generator achieves a 100% success rate at knowledge propagation.", "labels": [], "entities": [{"text": "knowledge propagation", "start_pos": 121, "end_pos": 142, "type": "TASK", "confidence": 0.7908993661403656}]}, {"text": "We propose a KB reconstruction based metric as follows: for each entity, construct a KB from the generated paragraph, and compute precision, recall and F-score by comparing it with the input KB from two aspects: (1).", "labels": [], "entities": [{"text": "KB reconstruction", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7553313374519348}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9993593096733093}, {"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9992669224739075}, {"text": "F-score", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.9981545805931091}]}, {"text": "Overall Slot Filling: If a pair of slot type and its slot value exists in both of the reconstructed KB and the input KB, it's considered as a correct slot.", "labels": [], "entities": [{"text": "Slot Filling", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.7740397751331329}]}, {"text": "Inter-dependent Slot Filling: If a row that consists one or multiple slot types and their slot values exist in both of the reconstructed KB and the input KB, it's considered as a correct row.", "labels": [], "entities": [{"text": "Slot Filling", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.7804248034954071}]}, {"text": "If the same slot/row is correctly described multiple times in the system generation output, it's only counted as correct once, i.e., redundant descriptions will be penalized.", "labels": [], "entities": []}, {"text": "This metric is further illustrated in.", "labels": [], "entities": []}, {"text": "It's similar to the relation extraction based generation evaluation metric proposed by) and entity/event extraction based metric proposed by.", "labels": [], "entities": [{"text": "relation extraction based generation evaluation", "start_pos": 20, "end_pos": 67, "type": "TASK", "confidence": 0.8552163004875183}, {"text": "entity/event extraction", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.5906497687101364}]}, {"text": "They compared automatic Information Extraction results from the reference description and the system generation output.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6320854276418686}]}, {"text": "However, the performance of state-of-theart open-domain slot filling () is still far from satisfactory to serve as an automatic extraction tool for evaluating generation results.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.7163737863302231}]}, {"text": "Therefore for the pilot study in this paper we manually reconstruct KBs from the generation output for evaluation.", "labels": [], "entities": []}, {"text": "Notably none of the above automatic metrics is sufficient to capture adequacy, grammaticality and fluency of the generated descriptions.", "labels": [], "entities": []}, {"text": "However extrinsic metrics such as system purpose and user task are expensive, while cheaper metrics such as human rating do not correlate with extrinsic metrics.", "labels": [], "entities": []}, {"text": "Moreover the task we address in this paper requires essential domain knowledge fora human user to assess the generated descriptions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Generation Performance based on Standard Metrics %)", "labels": [], "entities": []}, {"text": " Table 6: Overall Slot Filling Precision (P), Recall  (R), F-score (F1) (%)", "labels": [], "entities": [{"text": "Overall Slot Filling Precision (P)", "start_pos": 10, "end_pos": 44, "type": "METRIC", "confidence": 0.8548814398901803}, {"text": "Recall  (R)", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9716569632291794}, {"text": "F-score (F1)", "start_pos": 59, "end_pos": 71, "type": "METRIC", "confidence": 0.8928457051515579}]}, {"text": " Table 7: Inter-dependent Slot Filling Precision (P),  Recall (R), F-score (F1) (%)", "labels": [], "entities": [{"text": "Inter-dependent Slot Filling Precision (P)", "start_pos": 10, "end_pos": 52, "type": "METRIC", "confidence": 0.8826964156968253}, {"text": "Recall (R)", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9680440872907639}, {"text": "F-score (F1)", "start_pos": 67, "end_pos": 79, "type": "METRIC", "confidence": 0.9273447096347809}]}]}