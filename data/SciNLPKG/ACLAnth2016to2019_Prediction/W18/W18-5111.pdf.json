{"title": [{"text": "Interpreting Neural Network Hate Speech Classifiers", "labels": [], "entities": [{"text": "Interpreting Neural Network Hate Speech Classifiers", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8024069468180338}]}], "abstractContent": [{"text": "Deep neural networks have been applied to hate speech detection with apparent success, but they have limited practical applicability without transparency into the predictions they make.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.8569902976353964}]}, {"text": "In this paper, we perform several experiments to visualize and understand a state-of-the-art neural network classifier for hate speech (Zhang et al., 2018).", "labels": [], "entities": []}, {"text": "We adapt techniques from computer vision to visualize sensitive regions of the input stimuli and identify the features learned by individual neurons.", "labels": [], "entities": []}, {"text": "We also introduce a method to discover the keywords that are most predictive of hate speech.", "labels": [], "entities": []}, {"text": "Our analyses explain the aspects of neural networks that work well and point out areas for further improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "We define hate speech as \"language that is used to express hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group\" (.", "labels": [], "entities": [{"text": "hate speech as \"language that is used to express hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group", "start_pos": 10, "end_pos": 175, "type": "Description", "confidence": 0.7991717701608484}]}, {"text": "This definition importantly does not include all instances of offensive language, reflecting the challenges of automated detection in practice.", "labels": [], "entities": [{"text": "automated detection", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7501625120639801}]}, {"text": "For instance, in the following examples from Twitter (1) clearly expresses homophobic sentiment, while (2) uses the same term self-referentially: (1) Being gay aint cool ...", "labels": [], "entities": []}, {"text": "yall just be confused and hurt ...", "labels": [], "entities": []}, {"text": "fags dont make it to Heaven (2) me showing up in heaven after everyone told me god hates fags As in many other natural language tasks, deep neural networks have become increasingly popular and effective within the realm of hate speech research.", "labels": [], "entities": [{"text": "hate speech research", "start_pos": 223, "end_pos": 243, "type": "TASK", "confidence": 0.7122986018657684}]}, {"text": "However, few attempts have been made to explain the underlying features that contribute to their performance, essentially rendering them black-boxes.", "labels": [], "entities": []}, {"text": "Given the significant social, moral, and legal consequences of incorrect predictions, interpretability is critical for deploying and improving these models.", "labels": [], "entities": []}, {"text": "To address this, we contribute three ways of visualizing and understanding neural networks for text classification and conduct a case study on a state-of-the-art model for generalized hate speech.", "labels": [], "entities": [{"text": "text classification", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.8136075437068939}]}, {"text": "We 1) perform occlusion tests to investigate regions of model sensitivity in the inputs, 2) identify maximal activations of network units to visualize learned features, and 3) identify the unigrams most strongly associated with hate speech.", "labels": [], "entities": []}, {"text": "Our analyses explore the bases of the neural network's predictions and discuss common classes of errors that remain to be addressed by future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distribution of class labels in the dataset.", "labels": [], "entities": []}, {"text": " Table 3: Comparison between our method and a ran- dom baseline on recall of Hatebase lexicon terms and  tweets containing Hatebase terms. The random base- line is averaged over 10,000 trials.", "labels": [], "entities": [{"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9898377656936646}, {"text": "Hatebase lexicon terms", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.8642052213350931}, {"text": "Hatebase", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.49930062890052795}]}]}