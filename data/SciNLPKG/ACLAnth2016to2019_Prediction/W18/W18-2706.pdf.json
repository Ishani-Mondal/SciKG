{"title": [], "abstractContent": [{"text": "Current models for document summarization disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.6729315221309662}]}, {"text": "We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs.", "labels": [], "entities": []}, {"text": "With user input, our system can produce high quality summaries that follow user preferences.", "labels": [], "entities": []}, {"text": "Without user input , we set the control variables automatically-on the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 F1-ROUGE and human evaluation).", "labels": [], "entities": [{"text": "CNN-Dailymail dataset", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.978238582611084}, {"text": "F1-ROUGE1", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9950869679450989}, {"text": "F1-ROUGE", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.9783559441566467}]}], "introductionContent": [{"text": "Summarization condenses a document into a short paragraph or a single sentence while retaining core information.", "labels": [], "entities": [{"text": "Summarization condenses a document into a short paragraph", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.8877302184700966}]}, {"text": "Summarization algorithms are either extractive or abstractive.", "labels": [], "entities": []}, {"text": "Extractive algorithms form summaries by pasting together relevant portions of the input, while abstractive algorithms may generate new text that is not present in the initial document.", "labels": [], "entities": []}, {"text": "This work focuses on abstractive summarization and, in contrast to previous work, describes mechanisms that enable the reader to control important aspects of the generated summary.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.5714035034179688}]}, {"text": "The reader can select the desired length of the summary depending on how detailed they would like the summary to be.", "labels": [], "entities": [{"text": "length", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9742935299873352}]}, {"text": "The reader can require the text to focus on entities they have a particular interest in.", "labels": [], "entities": []}, {"text": "We let the reader choose the style of the summary based on their favorite source of information, e.g., a particular news source.", "labels": [], "entities": []}, {"text": "Finally, we allow the reader to specify that they only want to summarize a portion of the article, for example the remaining paragraphs they haven't read.", "labels": [], "entities": []}, {"text": "Our work builds upon sequence-to-sequence models, which have been extensively applied to abstractive summarization (.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.5771766901016235}]}, {"text": "These conditional language models use an encoder network to build a representation of the input document and a decoder network to generate a summary by attending to the source representation ().", "labels": [], "entities": []}, {"text": "We introduce a straightforward and extensible controllable summarization model to enable personalized generation and fully leverage that automatic summaries are generated at the reader's request.", "labels": [], "entities": [{"text": "summarization", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.9508254528045654}, {"text": "personalized generation", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.7897001802921295}]}, {"text": "We show that (1) our generated summaries follow the specified preferences and (2) these control variables guide the learning process and improve generation even when they are set automatically during inference.", "labels": [], "entities": []}, {"text": "Our comparison with state-of-the-art models on the standard CNN/DailyMail benchmark (), a multi-sentence summarization news corpus, highlights the advantage of our approach.", "labels": [], "entities": [{"text": "CNN/DailyMail benchmark", "start_pos": 60, "end_pos": 83, "type": "DATASET", "confidence": 0.9208649396896362}]}, {"text": "On both the entity-anonymized (+0.76 F1-ROUGE1) and full text versions (+0.85 F1-ROUGE1) of the dataset, we outperform previous pointer-based models trained with maximum likelihood despite the relative simplicity of our model.", "labels": [], "entities": [{"text": "F1-ROUGE1", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9892773628234863}, {"text": "F1-ROUGE1", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9710090160369873}]}, {"text": "Further, we demonstrate in a blind human evaluation study that our model generates summaries preferred by human readers. are deep convolutional networks (.", "labels": [], "entities": []}, {"text": "Both start with a word embedding layer followed by alternating convolutions with Gated Linear Units (GLU) ( ).", "labels": [], "entities": []}, {"text": "The decoder is connected to the encoder through attention modules () that performs a weighted sum of the encoder outputs.", "labels": [], "entities": []}, {"text": "The weights are predicted from the current decoder states, allowing the decoder to emphasize the parts of the input document which are the most relevant for generating the next token.", "labels": [], "entities": []}, {"text": "We use multi-hop attention, i.e. attention is applied at each layer of the decoder.", "labels": [], "entities": []}, {"text": "In addition to attending over encoder states, we also use intra-attention in the decoder to enable the model to refer back to previously generated words.", "labels": [], "entities": []}, {"text": "This allows the decoder to keep track of its progress and reduces the generation of repeated information (.", "labels": [], "entities": []}, {"text": "To combine encoder and decoder attention, we alternate between each type of attention at every layer.", "labels": [], "entities": []}, {"text": "Much prior work on the CNN-Dailymail benchmark employed pointer networks to copy rare entities from the input (, which introduces additional complexity to the model.", "labels": [], "entities": [{"text": "CNN-Dailymail benchmark", "start_pos": 23, "end_pos": 46, "type": "DATASET", "confidence": 0.8956647217273712}]}, {"text": "Instead, we rely on sub-word tokenization and weight sharing.", "labels": [], "entities": []}, {"text": "We show this simple approach is very effective.", "labels": [], "entities": []}, {"text": "Specifically, we use byte-pair-encoding (BPE) for tokenization, a proven strategy that has been shown to improve the generation of proper nouns in translation ().", "labels": [], "entities": [{"text": "tokenization", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.9703254103660583}]}, {"text": "We share the representation of the tokens in the encoder and decoder embeddings and in the last decoder layer.", "labels": [], "entities": []}], "datasetContent": [{"text": "Further, we compare length control with (Kikuchi et al., 2016) on DUC-2004 single-sentence summarization task.", "labels": [], "entities": [{"text": "length control", "start_pos": 20, "end_pos": 34, "type": "METRIC", "confidence": 0.9428006410598755}, {"text": "DUC-2004 single-sentence summarization task", "start_pos": 66, "end_pos": 109, "type": "TASK", "confidence": 0.7245965600013733}]}, {"text": "We train on English Gigaword following the protocol of.", "labels": [], "entities": [{"text": "English Gigaword", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.8452204465866089}]}, {"text": "The data consist of 3.6 million pairs (first sentence, headline of news articles).", "labels": [], "entities": []}, {"text": "Following (, we evaluate on the 500 documents in the DUC2004 task-1.", "labels": [], "entities": [{"text": "DUC2004 task-1", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9081850051879883}]}, {"text": "We use a source and target vocabulary of 30k words.", "labels": [], "entities": []}, {"text": "Architecture, Training, and Generation: We implement models with the fairseq library 1 . For CNN-Dailymail, our model has 8 layers in the encoder and decoder, each with kernel width 3.", "labels": [], "entities": [{"text": "CNN-Dailymail", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.9060545563697815}]}, {"text": "We use 512 hidden units for each layer, embeddings of size 340, and dropout 0.2.", "labels": [], "entities": []}, {"text": "For DUC, we have 6 layers in the encoder and decoder with 256 hidden units.", "labels": [], "entities": []}, {"text": "Similar to, we train using Nesterov accelerated gradient method (Sutskever et al., 2013) with gradient clipping 0.1 (, momentum 0.99, and learning rate 0.2.", "labels": [], "entities": [{"text": "momentum", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9556334614753723}, {"text": "learning rate", "start_pos": 138, "end_pos": 151, "type": "METRIC", "confidence": 0.9729703664779663}]}, {"text": "We reduce the learning rate by an order of magnitude when the validation perplexity ceases to improve, and end training when the learning rate drops below 10 \u22125 . Summaries are generated using beam search with beam size 5.", "labels": [], "entities": []}, {"text": "To avoid repetition, we prevent the decoder from generating the same trigram more than once, following.", "labels": [], "entities": [{"text": "repetition", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9217464327812195}]}, {"text": "Evaluation: On the CNN-Dailymail benchmark, our automatic evaluation reports F1-ROUGE scores for ROUGE-1, ROUGE-2, and ROUGE-L.", "labels": [], "entities": [{"text": "CNN-Dailymail benchmark", "start_pos": 19, "end_pos": 42, "type": "DATASET", "confidence": 0.9765354096889496}, {"text": "F1-ROUGE", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9952282905578613}, {"text": "ROUGE-1", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9737987518310547}, {"text": "ROUGE-2", "start_pos": 106, "end_pos": 113, "type": "METRIC", "confidence": 0.9319420456886292}, {"text": "ROUGE-L", "start_pos": 119, "end_pos": 126, "type": "METRIC", "confidence": 0.9476898908615112}]}, {"text": "We compare to existing abstractive baselines).", "labels": [], "entities": []}, {"text": "We also compare with Lead-3 which selects the first three sentences of the article as the summary.", "labels": [], "entities": [{"text": "Lead-3", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.9436351656913757}]}, {"text": "Note that, although simple, this baseline is not outperformed by all models.", "labels": [], "entities": []}, {"text": "For human evaluation, we conduct a human evaluation study using Amazon Mechanical Turk and the test set generation output of.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.965821107228597}]}, {"text": "500 articles from the test set were randomly selected and evaluated by 5 raters.", "labels": [], "entities": []}, {"text": "The raters were presented with the first 400 words of each news article and asked to select the summarization output they preferred.", "labels": [], "entities": []}, {"text": "For the DUC-2004, we report recall ROUGE for ROUGE-1, ROUGE-2, and ROUGE-L at 30, 50, and 75 byte lengths following.", "labels": [], "entities": [{"text": "DUC-2004", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.9608450531959534}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9952150583267212}, {"text": "ROUGE", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.6529420018196106}, {"text": "ROUGE-1", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.8676031231880188}, {"text": "ROUGE-2", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.8387630581855774}]}, {"text": "Our study compares summarization with fixed value control variables on full text CNN-Dailymail with (See et al., 2017).", "labels": [], "entities": [{"text": "summarization", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.9700134992599487}, {"text": "CNN-Dailymail", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.796341061592102}]}, {"text": "shows that human raters prefer our model about 59% of the time based 2.5k judgments.", "labels": [], "entities": []}, {"text": "Our model can therefore improve summary quality in a discernible way.", "labels": [], "entities": []}, {"text": "As an aside, we find that ROUGE and ratings agree in two-thirds of the cases, whereat least four out of five humans agree.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9979037046432495}, {"text": "ratings", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.8717377781867981}]}], "tableCaptions": [{"text": " Table 1: Baseline without control variables. Each row  add a feature on top of the previous row features.", "labels": [], "entities": []}, {"text": " Table 2: Summarization with oracle control to  simulate user preference.", "labels": [], "entities": []}, {"text": " Table 3: Fixed control variables on entity-anonymized  text. Even with fixed variables, the controllable model  improves ROUGE compared to ML alternatives.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.9973616003990173}]}, {"text": " Table 4: Summarization with fixed control variables  on original text. Even with a fixed setting, the  controlled summarization model improves ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 144, "end_pos": 149, "type": "METRIC", "confidence": 0.9968640804290771}]}, {"text": " Table 5: ROUGE for fixed length control variable on DUC-2004 Task 1.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9937663078308105}, {"text": "DUC-2004 Task 1", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.9305543502171835}]}, {"text": " Table 6: Fraction of requested entity actually  occurring in decoded summaries. Entities originate  either from lead-3 or from the full document.", "labels": [], "entities": []}, {"text": " Table 8: Summaries with various settings for user control variables and remainder summarization.", "labels": [], "entities": []}, {"text": " Table 9: Human evaluation: 59% of ratings prefer our  summaries (500 CNN-DM test articles, 5 raters each).", "labels": [], "entities": [{"text": "CNN-DM test articles", "start_pos": 70, "end_pos": 90, "type": "DATASET", "confidence": 0.927320679028829}]}]}