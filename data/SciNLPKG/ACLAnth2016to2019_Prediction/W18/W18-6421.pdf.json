{"title": [{"text": "NTT's Neural Machine Translation Systems for WMT 2018", "labels": [], "entities": [{"text": "NTT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7456633448600769}, {"text": "Neural Machine Translation", "start_pos": 6, "end_pos": 32, "type": "TASK", "confidence": 0.6130934755007426}, {"text": "WMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.6940839886665344}]}], "abstractContent": [{"text": "This paper describes NTT's neural machine translation systems submitted to the WMT 2018 English-German and German-English news translation tasks.", "labels": [], "entities": [{"text": "NTT's neural machine translation", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.613608855009079}, {"text": "WMT 2018 English-German and German-English news translation tasks", "start_pos": 79, "end_pos": 144, "type": "TASK", "confidence": 0.7716282084584236}]}, {"text": "Our submission has three main components: the Transformer model, corpus cleaning, and right-to-left n-best re-ranking techniques.", "labels": [], "entities": [{"text": "corpus cleaning", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7764303684234619}]}, {"text": "Through our experiments , we identified two keys for improving accuracy: filtering noisy training sentences and right-to-left re-ranking.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9968235492706299}]}, {"text": "We also found that the Transformer model requires more training data than the RNN-based model, and the RNN-based model sometimes achieves better accuracy than the Transformer model when the corpus is small.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9978641867637634}]}], "introductionContent": [{"text": "This paper describes NTT's submission to the WMT 2018 news translation task ( . This year, we participated in English-toGerman (En-De) and German-to-English (De-En) translation tasks.", "labels": [], "entities": [{"text": "NTT", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.7473270893096924}, {"text": "WMT 2018 news translation task", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.7123419880867005}]}, {"text": "The starting point of our system is the Transformer model (, which recently established better performance than conventional RNN-based models.", "labels": [], "entities": []}, {"text": "We incorporated a parallel corpus cleaning technique (Section 3.1) and a right-to-left n-best re-ranking technique (Section 3.4) and also used a synthetic corpus to exploit monolingual data.", "labels": [], "entities": []}, {"text": "To maintain the quality of the synthetic corpus, we checked its back-translation BLEU scores and filtered out the noisy data with low scores (Section 3.2).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9862484335899353}]}, {"text": "Through experiments, we evaluated how each feature affects accuracy (Section 4).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9990882873535156}]}, {"text": "Compared with the RNN-based system, we also identified when the Transformer model works effectively (Section 4.3.3).", "labels": [], "entities": []}, {"text": "\u21e4 His current affiliation is Tohoku University.", "labels": [], "entities": [{"text": "Tohoku University", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.9765858948230743}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of sentences in datasets", "labels": [], "entities": []}, {"text": " Table 2: Cased BLEU scores of our submission and baseline systems", "labels": [], "entities": [{"text": "Cased", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9771721959114075}, {"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9076710939407349}]}]}