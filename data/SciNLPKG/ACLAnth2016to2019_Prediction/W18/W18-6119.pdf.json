{"title": [{"text": "Preferred Answer Selection in Stack Overflow: Better Text Representations ... and Metadata, Metadata, Metadata", "labels": [], "entities": [{"text": "Preferred Answer Selection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6696876386801401}]}], "abstractContent": [{"text": "Community question answering (cQA) forums provide a rich source of data for facilitating non-factoid question answering over many technical domains.", "labels": [], "entities": [{"text": "Community question answering (cQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7369228601455688}, {"text": "question answering", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7257217317819595}]}, {"text": "Given this, there is considerable interest in answer retrieval from these kinds of forums.", "labels": [], "entities": [{"text": "answer retrieval", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.9127639532089233}]}, {"text": "However this is a difficult task as the structure of these forums is very rich, and both metadata and text features are important for successful retrieval.", "labels": [], "entities": []}, {"text": "While there has recently been a lot of work on solving this problem using deep learning models applied to question/answer text, this work has not looked at how to make use of the rich metadata available in cQA forums.", "labels": [], "entities": []}, {"text": "We propose an attention-based model which achieves state-of-the-art results for text-based answer selection alone, and by making use of complementary meta-data, achieves a substantially higher result over two reference datasets novel to this work.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.7246765345335007}]}], "introductionContent": [{"text": "Community question answering (\"cQA\") forums such as Stack Overflow have become a staple source of information for technical searches on the web.", "labels": [], "entities": [{"text": "Community question answering (\"cQA\")", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7469492157300314}]}, {"text": "However, often a given query will match against multiple questions each with multiple answers.", "labels": [], "entities": []}, {"text": "This complicates technical information retrieval, as any kind of search or questionanswering engine must decide how to rank these answers.", "labels": [], "entities": [{"text": "technical information retrieval", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.6681273380915324}]}, {"text": "Therefore, it would be beneficial to be able to automatically determine which questions in a cQA forum are most relevant to a given query question, and which answers to these questions best answer the query question.", "labels": [], "entities": []}, {"text": "One of the challenges in addressing this problem is that cQA threads tend to have a very rich and specific kind of structure and associated metadata.", "labels": [], "entities": []}, {"text": "There is also often rich metadata associated with each question (e.g. number of views or community-assigned tags), each answer (e.g. creation and edit timestamps), along with every user who has participated in the thread -both explicit (e.g. badges or their reputation level) and implicit (e.g. activity data from other threads they have participated in, types of questions they have posted, or the types of answers they posted which were accepted).", "labels": [], "entities": []}, {"text": "Our research is aimed at improving the ability to automatically identify the best answer within a thread fora given question, as an initial step towards cross-thread answer ranking/selection.", "labels": [], "entities": [{"text": "cross-thread answer ranking/selection", "start_pos": 153, "end_pos": 190, "type": "TASK", "confidence": 0.6686501681804657}]}, {"text": "In this work we use Stack Overflow as our source of cQA threads.", "labels": [], "entities": []}, {"text": "More concretely, given a Stack Overflow cQA thread with at least four answers, we attempt to automatically determine which of the answers was chosen by the user posting the original question as the \"preferred answer\".", "labels": [], "entities": []}, {"text": "A secondary goal of our research is learning how to leverage both the question/answer text in cQA threads, along with the associated metadata.", "labels": [], "entities": []}, {"text": "We show how to create effective representations of both the thread text and the metadata, and we investigate the relative strength of each as well as their complementarity for preferred answer selection.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 186, "end_pos": 202, "type": "TASK", "confidence": 0.7083529382944107}]}, {"text": "By leveraging this metadata and using an attentional model for constructing question/answer pair representations, we are able to obtain greatly improved results over an existing state-of-the-art method for answer retrieval.", "labels": [], "entities": [{"text": "answer retrieval", "start_pos": 206, "end_pos": 222, "type": "TASK", "confidence": 0.8814328908920288}]}, {"text": "The contributions of our research are as follows: \u2022 we develop two novel benchmark datasets for cQA answer ranking/selection; \u2022 we adapt a deep learning method proposed for near-duplicate/paraphrase detection, and achieve state-of-the-art results for text-based answer selection; and \u2022 we demonstrate that metadata is critical in identifying preferred answers, but at the same time text-based representations complement metadata to achieve the best overall results for the task.", "labels": [], "entities": [{"text": "cQA answer ranking/selection", "start_pos": 96, "end_pos": 124, "type": "TASK", "confidence": 0.707326865196228}, {"text": "paraphrase detection", "start_pos": 188, "end_pos": 208, "type": "TASK", "confidence": 0.7230345606803894}, {"text": "text-based answer selection", "start_pos": 251, "end_pos": 278, "type": "TASK", "confidence": 0.6452025572458903}]}, {"text": "The data and code used in this research will be made available on acceptance.", "labels": [], "entities": [{"text": "acceptance", "start_pos": 66, "end_pos": 76, "type": "TASK", "confidence": 0.6174512505531311}]}], "datasetContent": [{"text": "We developed two datasets based on Stack Overflow question-answer threads, along with a background corpus for pre-training models.", "labels": [], "entities": []}, {"text": "The evaluation datasets were created by sampling from threads with at least four answers, where one of those answers had been selected as \"best\" by the question asker.", "labels": [], "entities": []}, {"text": "The process for constructing our dataset was modelled on the 10,000 \"how\" question corpus), similar to Bogdanova and Foster.", "labels": [], "entities": []}, {"text": "The two evaluation datasets, which we denote as \"SMALL\" and \"LARGE\", contain 10K and 70K questions, respectively, each with a predefined 50/25/25 split between train, val, and test questions.", "labels": [], "entities": [{"text": "SMALL", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.6585732698440552}, {"text": "LARGE", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9883231520652771}]}, {"text": "On average, there are approximately six answers per question.", "labels": [], "entities": []}, {"text": "In addition to the sampled sub-sets, we also used the full Stack Overflow dump (containing a full month of questions and answers) for pretraining; we will refer to this dataset as \"FULL\".", "labels": [], "entities": [{"text": "FULL", "start_pos": 181, "end_pos": 185, "type": "METRIC", "confidence": 0.9847843647003174}]}, {"text": "This full dataset consists of approximately 300K questions and 1M answers.", "labels": [], "entities": []}, {"text": "In all cases, we tokenised the text using Stanford CoreNLP).", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9512439072132111}]}, {"text": "Stack Overflow contains rich metadata, including user-level information and question-and answer-specific data.", "labels": [], "entities": []}, {"text": "We leverage this metadata in our model, as detailed in Section 4.2.", "labels": [], "entities": []}, {"text": "Summary statistics of SMALL, LARGE and FULL are presented in.", "labels": [], "entities": [{"text": "SMALL", "start_pos": 22, "end_pos": 27, "type": "TASK", "confidence": 0.4441855251789093}, {"text": "LARGE", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9234654903411865}, {"text": "FULL", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9876739382743835}]}, {"text": "In addition to the Stack Overflow dataset, we also experiment with an additional complementary dataset: the SEMEVAL 2017 Task 3A QuestionComment reranking dataset ( We include this dataset to establish the competitiveness of our proposed text processing networks (noting that the data contains very little metadata to be able to evaluate our metadata-based model).", "labels": [], "entities": [{"text": "SEMEVAL 2017 Task 3A QuestionComment reranking dataset", "start_pos": 108, "end_pos": 162, "type": "DATASET", "confidence": 0.6568630167416164}]}, {"text": "We used the 2016 test set as validation, the 2017 test set as test.", "labels": [], "entities": [{"text": "2016 test set", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.8103964527448019}, {"text": "2017 test set", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.7999706069628397}]}, {"text": "Note that there are 3 classes in SEMEVAL: Good, PotentiallyUseful, and Bad, but we collapse PotentiallyUseful and Bad into a single class, following most competition entries.", "labels": [], "entities": [{"text": "SEMEVAL", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.7382526397705078}]}, {"text": "To train our models, we used the Adam Optimiser ().", "labels": [], "entities": []}, {"text": "For decatt, we used dropout over F, G, H after every dense layer.", "labels": [], "entities": []}, {"text": "For the doc2vec MLP, we included batch normalisation before, and dropout after, each dense layer.", "labels": [], "entities": [{"text": "doc2vec MLP", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.7261653244495392}]}, {"text": "For testing, we picked the best model according to the validation results after the end of each epoch.", "labels": [], "entities": []}, {"text": "The parameters for decatt were initialised with a Gaussian distribution of mean 0 and variance 0.01, and for the doc2vec MLP we used Glorot normal initialization.", "labels": [], "entities": [{"text": "MLP", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.5060527920722961}]}, {"text": "For Stack Overflow, the parameters for Word embeddings were pretrained using GloVe () with the FULL data (by combining all questions and answers in the sequence they appeared) for 50 epochs.", "labels": [], "entities": [{"text": "Stack Overflow", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.8766754269599915}, {"text": "GloVe", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9207441210746765}, {"text": "FULL", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9847798943519592}]}, {"text": "Word embeddings were set to 150 dimensions.", "labels": [], "entities": []}, {"text": "The co-occurrence weighting function's maximum value x max was kept at the default of 10.", "labels": [], "entities": []}, {"text": "For SEMEVAL, we used pretrained Common Crawl cased embeddings with 840G tokens and 300 dimensions ().", "labels": [], "entities": [{"text": "SEMEVAL", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9329712986946106}]}, {"text": "To train the decatt model for Stack Overflow we split the data into 3 partitions based on the size of the question/answer text, with separate partitions where the total length of the question/answer text was: (1) \u2264 500 words; (2) > 500 and \u2264 2000 words; and (3) > 2000 words.", "labels": [], "entities": [{"text": "Stack Overflow", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.8989357650279999}]}, {"text": "We used a different batch size for each partition (32, 8, and 4 respectively).", "labels": [], "entities": []}, {"text": "Examples were shuffled within each partition after every epoch.", "labels": [], "entities": []}, {"text": "For SEMEVAL we did not QTags n Probability distribution over top-n tags for question (\u00d7n).", "labels": [], "entities": [{"text": "SEMEVAL", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.8544817566871643}]}], "tableCaptions": [{"text": " Table 1: Details of the three Stack Overflow datasets.", "labels": [], "entities": [{"text": "Stack Overflow datasets", "start_pos": 31, "end_pos": 54, "type": "DATASET", "confidence": 0.7492953141530355}]}, {"text": " Table 3: Hyperparameter settings used for each model and corpora. \"LR\" = learning rate; \"N/A\" indicates that the  hyperparameter is not relevant for the given model. All models were trained for 40 epochs.", "labels": [], "entities": [{"text": "LR", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.982609748840332}, {"text": "learning rate", "start_pos": 74, "end_pos": 87, "type": "METRIC", "confidence": 0.943650871515274}, {"text": "N/A\"", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.7294468134641647}]}, {"text": " Table 4: Results for doc2vec, metadata and  decatt models on both Stack Overflow datasets  (P@1) and SEMEVAL (MAP).", "labels": [], "entities": []}, {"text": " Table 4. To investigate the relative im-", "labels": [], "entities": []}]}