{"title": [{"text": "DialCrowd: A toolkit for easy dialog system assessment", "labels": [], "entities": []}], "abstractContent": [{"text": "When creating a dialog system, developers need to test each version to ensure that it is performing correctly.", "labels": [], "entities": []}, {"text": "Recently the trend has been to test on large datasets or to ask many users to tryout a system.", "labels": [], "entities": []}, {"text": "Crowd-sourcing has solved the issue of finding users, but it presents new challenges such as how to use a crowdsourcing platform and what type of testis appropriate.", "labels": [], "entities": []}, {"text": "Di-alCrowd makes system assessment using crowdsourcing easier by providing tools, templates and analytics.", "labels": [], "entities": [{"text": "system assessment", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.6596160531044006}]}, {"text": "This paper describes the services that DialCrowd provides and how it works.", "labels": [], "entities": []}, {"text": "It also describes a test of DialCrowd by a group of dialog system developers.", "labels": [], "entities": []}], "introductionContent": [{"text": "The development of a spoken dialog system involves many steps and always ends in system tests.", "labels": [], "entities": []}, {"text": "As our systems have become more complicated and the statistical methods we use demand more and more data, proper system assessment becomes an increasingly difficult challenge.", "labels": [], "entities": [{"text": "system assessment", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.6431283950805664}]}, {"text": "One of the easier approaches to goal-oriented system assessment is to employ user simulation (.", "labels": [], "entities": []}, {"text": "It aims at the overall assessment of the system by measuring goal completion.", "labels": [], "entities": []}, {"text": "While this is a useful first approach, it can't reveal what a human user would actually say.", "labels": [], "entities": []}, {"text": "Thus this approach is usually used as a first approximation, quickly followed up with some assessment using humans.", "labels": [], "entities": []}, {"text": "SOme chatbot systems use machine learning metrics to compare a model-generated response to a golden standard response.", "labels": [], "entities": []}, {"text": "However, those metrics assume that a valid response has a significant word overlap with the golden response, which is often not the case.", "labels": [], "entities": []}, {"text": "showed that these metrics correlate very weakly with human judgment.", "labels": [], "entities": []}, {"text": "Other approaches used to assess nontask oriented dialog systems include word similarity metrics, next utterance classification, word perplexity, and response diversity (.", "labels": [], "entities": [{"text": "next utterance classification", "start_pos": 97, "end_pos": 126, "type": "TASK", "confidence": 0.7581873734792074}]}, {"text": "They are limited since they can't reproduce the variety found in actual user behavior.", "labels": [], "entities": []}, {"text": "Crowdsourcing platforms, such as Amazon Mechanical Turk (AMT), have shown promise in assessing spoken dialog systems ().", "labels": [], "entities": []}, {"text": "But for most developers it is not trivial to setup the crowdsourcing process and obtain usable results.", "labels": [], "entities": []}, {"text": "noted that this process must be cheap to operate and easy to use.", "labels": [], "entities": []}, {"text": "Researchers (the requesters) have to overcome the following difficulites: learning how to use the crowdsourcing entity interface, learning how to create an understandable and attractive task, deciding on the correct form that the task should take (the template), connecting the dialog systems that are to be assessed to the crowdsourcing platform, paying the workers, assessing the quality of the workers' production, getting solid final results.", "labels": [], "entities": []}, {"text": "To solve the connection issue, researchers have used the telephone to connect their dialog systems, relying on a crowdsourcing web interface to present the task, then sending the worker to the dialog system and finally bringing them back to the interface to collect their production and schedule payment.", "labels": [], "entities": []}, {"text": "This connection issue is one example of these hurdles.", "labels": [], "entities": []}, {"text": "Researchers are also faced with the choice of the form of assessment.", "labels": [], "entities": []}, {"text": "The types of tests may vary.", "labels": [], "entities": []}, {"text": "One form that is often found in the literature is to compare two versions of the same system (A/B text).", "labels": [], "entities": []}, {"text": "The literature shows that a small number of test types covers most publications.", "labels": [], "entities": []}, {"text": "DialCrowd (https://dialrc.org/dialcrowd.html) is a toolkit that makes crowdsourced evalua-tion studies easy to run.", "labels": [], "entities": []}, {"text": "We have identified a small number of standard evaluation experiment types and provided templates that generate web interfaces for these studies in a crowdsourcing environment.", "labels": [], "entities": []}, {"text": "The DialCrowd interface first has the researcher choose the type of study (or she can makeup her own).", "labels": [], "entities": []}, {"text": "Once the type is chosen, the corresponding template appears and is filled in.", "labels": [], "entities": []}, {"text": "This generates the task (HIT on AMT) that the worker will see.", "labels": [], "entities": [{"text": "HIT", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9887853860855103}]}, {"text": "This considerably lowers preparation time, and guides those who are new to the field to commonly-accepted study types.", "labels": [], "entities": []}, {"text": "DialCrowd presently has a small set of templates which will soon expand to include those suggested by our users or that we find in the literature.", "labels": [], "entities": [{"text": "DialCrowd", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9495834708213806}]}, {"text": "Other aspects of crowdsourced assessment that DialCrowd presently addresses are: \u2022 Explaining the overall goal of the assessment to the worker \u2022 Instructing the worker on how to accomplish the task \u2022 Reminding a requester to post a consent form for explicit permission to use the data \u2022 Helping calculate how much to pay fora HIT \u2022 How to make a HIT less susceptible to BOTs \u2022 Help in designing the appearance of the HIT.", "labels": [], "entities": [{"text": "BOTs", "start_pos": 370, "end_pos": 374, "type": "METRIC", "confidence": 0.9141875505447388}]}, {"text": "Going forward, DialCrowd will also provide tools to: \u2022 Assess an individual worker \u2022 Create a golden data set \u2022 Assess the final outcome with basic analytics \u2022 Ensure that results are collected ethically and are made available to the community with as few restrictions as possible that do not compromise the worker's privacy.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}