{"title": [{"text": "Deep Learning Architecture for Complex Word Identification", "labels": [], "entities": [{"text": "Complex Word Identification", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6107415755589803}]}], "abstractContent": [{"text": "We describe a system for the CWI-task that includes information on 5 aspects of the (com-plex) lexical item, namely distributional information of the item itself, morphological structure , psychological measures, corpus-counts and topical information.", "labels": [], "entities": []}, {"text": "We constructed a deep learning architecture that combines those features and apply it to the probabilistic and binary classification task for all English sets and Spanish.", "labels": [], "entities": []}, {"text": "We achieved reasonable performance on all sets with best performances seen on the probabilistic task, particularly on the English news set (MAE 0.054 and F1-score of 0.872).", "labels": [], "entities": [{"text": "English news set", "start_pos": 122, "end_pos": 138, "type": "DATASET", "confidence": 0.9547034700711569}, {"text": "MAE", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9630825519561768}, {"text": "F1-score", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9975892305374146}]}, {"text": "An analysis of the results shows that reasonable performance can be achieved with a single architecture without any domain-specific tweaking of the parameter settings and that distributional features capture almost all of the information also found in hand-crafted features.", "labels": [], "entities": []}], "introductionContent": [{"text": "In general, complex word identification (CWI) aims to identify words that are perceived as difficult fora given target audience.", "labels": [], "entities": [{"text": "complex word identification (CWI)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8054970403512319}]}, {"text": "As such, children, foreign language learners) and readers suffering from aphasia), dyslexia () or autism spectrum disorder) will struggle with different words.", "labels": [], "entities": []}, {"text": "The goal of the current CWI shared task) is to predict which words can be difficult fora non-native speaker, based on annotations collected from a mixture of native and nonnative speakers.", "labels": [], "entities": []}, {"text": "The instructions for the English dataset are formulated so that the annotator marks the words he thinks are problematic for children, non-native speakers, or people with language disabilities.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.6939176172018051}]}, {"text": "Having such a diverse target audience requires a system that includes a variety of information at different levels of linguistic description.", "labels": [], "entities": []}, {"text": "We include information that covers 5 aspects of the lexical item at hand, namely distributional information of the item itself, morphological structure, psychological measures, corpus-counts and topical information.", "labels": [], "entities": []}, {"text": "With the exception of the psychological measures, all can be readily trained by an appropriate neural network architecture and/or acquired from large-scale corpora.", "labels": [], "entities": []}, {"text": "We train a neural network to integrate said sources of information and apply it to the probabilistic and the binary complexity assessment for the three English datasets and the Spanish one.", "labels": [], "entities": [{"text": "English datasets", "start_pos": 152, "end_pos": 168, "type": "DATASET", "confidence": 0.8230702579021454}]}], "datasetContent": [{"text": "Result  The results in show reasonably good performance for all tasks.", "labels": [], "entities": [{"text": "Result", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9489825367927551}]}, {"text": "Our architecture seems to work especially well for the regression task, but shows its aptitude for the classification task as well.", "labels": [], "entities": []}, {"text": "The size of the training data seems to play a direct role in the system's ability for accurate predictions.", "labels": [], "entities": [{"text": "accurate predictions", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.5363718271255493}]}, {"text": "This is inline with other deep learning literature.", "labels": [], "entities": []}, {"text": "This does not hold for the Spanish set however, which might be due to a slight difference in apprehension during the data collection phase.", "labels": [], "entities": []}, {"text": "The inclusion of corpus-counts and pre-trained embeddings from a general corpus, rather than a wikipedia corpus shows directly 330  in the performance of the respective tasks.", "labels": [], "entities": []}, {"text": "Using a wikipedia corpus will probably positively influence the results for those particular sets.", "labels": [], "entities": []}, {"text": "Yet, the inclusion of general corpus-information proves to be a valid alternative in lack of specialized corpora.", "labels": [], "entities": []}, {"text": "The inclusion of the engineered features does not seem to affect the obtained scores much.", "labels": [], "entities": []}, {"text": "provides an overview of the relative contribution of each input layer to the final result for the English news dataset.", "labels": [], "entities": [{"text": "English news dataset", "start_pos": 98, "end_pos": 118, "type": "DATASET", "confidence": 0.8491866191228231}]}, {"text": "The models were trained for 50 epochs.", "labels": [], "entities": []}, {"text": "Considering each input layer separately, the word embeddings are the best estimator for the complexity task, followed closely by the character embeddings.", "labels": [], "entities": []}, {"text": "Engineered features capture some information on the word's complexity, yet not as much as the embedding layers.", "labels": [], "entities": []}, {"text": "Interestingly, sentence information does not outperform the baseline.", "labels": [], "entities": []}, {"text": "The combination of input layers shows the relative improvement that can be achieved by adding more information to the best performing input layer.", "labels": [], "entities": []}, {"text": "The results indicate that combining information only marginally improves performance.", "labels": [], "entities": []}, {"text": "They also confirm that the engineered features in combination with the embeddings do not contribute much to the final score.", "labels": [], "entities": []}, {"text": "This leads to the following conclusions for the current dataset.", "labels": [], "entities": []}, {"text": "First, complexity is best determined by including focused information of the target word itself.", "labels": [], "entities": []}, {"text": "The inclusion of contextual, topical information does not show any noticeable advantage.", "labels": [], "entities": []}, {"text": "Looking at the combination of input layers, we can derive that the engineered features only add marginally different information from other input sources.", "labels": [], "entities": []}, {"text": "This could be due to the limited number of words that are actually covered by the psychological dataset, but it also implies that the information from the corpus-counts is indirectly captured by the embeddings and from the word length by the character encodings.", "labels": [], "entities": []}, {"text": "It is a casein point for replacing manual feature engineering byword and character embeddings.", "labels": [], "entities": []}, {"text": "Based on these results we cannot conclude whether the word embeddings' better performance over the character embeddings is due to pre-training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: CWI training, development and test sets", "labels": [], "entities": []}, {"text": " Table 2: Results, Rank and Maximum scores for the  CWI identification task", "labels": [], "entities": [{"text": "CWI identification", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.8346913754940033}]}, {"text": " Table 3: Precision and Recall for different input layers", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9910590052604675}, {"text": "Recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9928680658340454}]}]}