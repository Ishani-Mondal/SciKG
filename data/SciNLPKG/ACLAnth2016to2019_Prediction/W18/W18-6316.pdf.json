{"title": [{"text": "Neural Machine Translation into Language Varieties", "labels": [], "entities": [{"text": "Neural Machine Translation into Language Varieties", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8326780100663503}]}], "abstractContent": [{"text": "Both research and commercial machine translation have so far neglected the importance of properly handling the spelling, lexical and grammar divergences occurring among language varieties.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7893170416355133}, {"text": "spelling, lexical and grammar divergences occurring among language varieties", "start_pos": 111, "end_pos": 187, "type": "TASK", "confidence": 0.6974311321973801}]}, {"text": "Notable cases are standard national varieties such as Brazilian and Euro-pean Portuguese, and Canadian and European French, which popular online machine translation services are not keeping distinct.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7702183425426483}]}, {"text": "We show that an evident side effect of modeling such varieties as unique classes is the generation of inconsistent translations.", "labels": [], "entities": []}, {"text": "In this work, we investigate the problem of training neural machine translation from English to specific pairs of language varieties, assuming both labeled and unlabeled parallel texts, and low-resource conditions.", "labels": [], "entities": [{"text": "training neural machine translation from English to specific pairs of language varieties", "start_pos": 44, "end_pos": 132, "type": "TASK", "confidence": 0.7658742517232895}]}, {"text": "We report experiments from En-glish to two pairs of dialects, European-Brazilian Portuguese and European-Canadian French, and two pairs of standardized varieties , Croatian-Serbian and Indonesian-Malay.", "labels": [], "entities": []}, {"text": "We show significant BLEU score improvements over baseline systems when translation into similar languages is learned as a multilingual task with shared representations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9738166034221649}]}], "introductionContent": [{"text": "The field of machine translation (MT) is making amazing progress, thanks to the advent of neural models and deep learning.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.8626042306423187}]}, {"text": "While just few years ago research in MT was struggling to achieve useful translations for the most requested and highresourced languages, the level of translation quality reached today has raised the demand and interest for less-resourced languages and the solution of more subtle and interesting translation tasks (.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9609010815620422}]}, {"text": "If the goal of machine translation is to help worldwide communication, then the time has come to also cope with dialects or more generally language varieties . Remarkably, up to now, even standard national language varieties, such as Brazilian and European Portuguese, or Canadian and European French, which are used by relatively large populations have been quite neglected both by research and industry.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7161383479833603}]}, {"text": "Prominent online commercial MT services, such as Google Translate and Bing, are currently not offering any variety of Portuguese and French.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.972383975982666}]}, {"text": "Even worse, systems offering such languages tend to produce inconsistent outputs, like mixing lexical items from different Portuguese (see for instance the translations shown in).", "labels": [], "entities": []}, {"text": "Clearly, in the perspective of delivering high-quality MT to professional post-editors and final users, this problem urges to be fixed.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9847339987754822}]}, {"text": "While machine translation from many to one varieties is intuitively simpler to approach 2 , it is the opposite direction that presents the most relevant problems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.7620838582515717}]}, {"text": "First, languages varieties such as dialects might significantly overlap thus making differences among their texts quite subtle (e.g., particular grammatical constructs or lexical divergences like the ones reported in the example).", "labels": [], "entities": []}, {"text": "Second, parallel data are not always labeled at the level of language variety, making it hard to develop specific NMT engines.", "labels": [], "entities": []}, {"text": "Finally, training data might be very unbalanced among different varieties, due to the population sizes of their respective speakers or for other reasons.", "labels": [], "entities": []}, {"text": "This clearly makes it harder to model the lower-resourced varieties (.", "labels": [], "entities": []}, {"text": "In this work we present our initial effort to systematically investigate ways to approach NMT from English into four pairs of language varieties: I'm going to the gym before breakfast.", "labels": [], "entities": [{"text": "NMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9182473421096802}]}, {"text": "No, I'm not going to the gym.", "labels": [], "entities": []}, {"text": "pt Eu estou indo para a academia antes do caf\u00e9 da manh\u00e3.", "labels": [], "entities": []}, {"text": "N\u00e3o, eu n\u00e3o vou ao gin\u00e1sio.", "labels": [], "entities": []}, {"text": "pt-BR (M-C2) Eu vo\u00fa a academia antes do caf\u00e9 da manh\u00e3.", "labels": [], "entities": []}, {"text": "N\u00e3o, eu n\u00e3o vo\u00f9 a academia.", "labels": [], "entities": []}, {"text": "pt-EU (M-C2) Vou para o gin\u00e1sio antes do pequeno-almo\u00e7o.", "labels": [], "entities": []}, {"text": "N\u00e3o, n\u00e3o vou para o gin\u00e0sio.", "labels": [], "entities": []}, {"text": "Vou ao gin\u00e1sio antes do pequeno-almo\u00e7o.", "labels": [], "entities": []}, {"text": "N\u00e3o, n\u00e3o vou ao gin\u00e1sio.", "labels": [], "entities": []}, {"text": "Portuguese European -Portuguese Brazilian, European French -Canadian French, Serbian -Croatian, and Indonesian -Malay 3 . For each couple of varieties, we assume to have both parallel text labeled with the corresponding couple member, and parallel text without such information.", "labels": [], "entities": []}, {"text": "Moreover, the considered target pairs, while all being mutually intelligible, present different levels of linguistic similarity and also different proportions of available training data.", "labels": [], "entities": []}, {"text": "For our tasks we rely on the WIT 3 TED Talks collection 4 , used for the International Workshop of Spoken Language Translation, and OpenSubtitles2018, a corpus of subtitles available from the OPUS collection 5 . After presenting related work (Section 2) on NLP and MT of dialects and related languages, we introduce (in Section 3) baseline NMT systems, either language/dialect specific or generic, and multilingual NMT systems, either trained with fully supervised (or labeled) data or with partially supervised data.", "labels": [], "entities": [{"text": "WIT 3 TED Talks collection", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.8406841039657593}, {"text": "International Workshop of Spoken Language Translation", "start_pos": 73, "end_pos": 126, "type": "TASK", "confidence": 0.6256469388802847}, {"text": "OPUS collection", "start_pos": 192, "end_pos": 207, "type": "DATASET", "confidence": 0.9300220906734467}, {"text": "MT of dialects and related languages", "start_pos": 265, "end_pos": 301, "type": "TASK", "confidence": 0.8429693977038065}]}, {"text": "In Section 4, we introduce our datasets, NMT set-ups based on the Transformer architecture, and then present the results for each evaluated system.", "labels": [], "entities": []}, {"text": "We conclude the paper with a discussion and conclusion in Sections 5 and 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experimental setting consists of eight target varieties and English as source.", "labels": [], "entities": []}, {"text": "We use publicly available datasets from the WIT 3 TED corpus ().", "labels": [], "entities": [{"text": "WIT 3 TED corpus", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.8311062306165695}]}, {"text": "The summary of the partitioned training, dev, and test sets are given in, where Tr.", "labels": [], "entities": [{"text": "Tr", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9445732831954956}]}, {"text": "2/3 is the labeled portion of the training set used to train the semi-supervised models, while the other 1/3 are either held out as unlabeled (M-U) or classified automatically (M-C2, M-C3).", "labels": [], "entities": []}, {"text": "In the preprocessing stages, we tokenize the corpora and remove lines longer than 70 tokens.", "labels": [], "entities": []}, {"text": "The Serbian corpus written in Cyrillic is transliterated into Latin script with CyrTranslit . In addition, to also run a large-data experiment,  we expand the English\u2212European/Brazilian Portuguese data with the corresponding OpenSubtitles2018 datasets from the OPUS corpus.", "labels": [], "entities": [{"text": "OpenSubtitles2018 datasets", "start_pos": 225, "end_pos": 251, "type": "DATASET", "confidence": 0.9020580649375916}, {"text": "OPUS corpus", "start_pos": 261, "end_pos": 272, "type": "DATASET", "confidence": 0.9707992374897003}]}, {"text": "summarizes the augmented training data, while keeping the same dev and test sets.", "labels": [], "entities": []}, {"text": "We trained all systems using the Transformer model 8 ().", "labels": [], "entities": []}, {"text": "We use the Adam optimizer () with an initial learning rate of 0.2 and a dropout also set to 0.2.", "labels": [], "entities": []}, {"text": "A shared source and target vocabulary of size 16k is generated via sub-word segmentation (Wu et al., 2016).", "labels": [], "entities": []}, {"text": "The choice for the vocabulary size follows the recommendations in Denkowski and Neubig (2017) regarding training of NMT systems on TED Talks data.", "labels": [], "entities": [{"text": "TED Talks data", "start_pos": 131, "end_pos": 145, "type": "DATASET", "confidence": 0.8078285058339437}]}, {"text": "Overall we use a uniform setting for all our models, with a 512 embedding dimension and hidden units, and 6 layers of selfattention encoder-decoder network.", "labels": [], "entities": []}, {"text": "The training batch size is of 6144 sub-word tokens and the max length after segmentation is set to 70.", "labels": [], "entities": [{"text": "max length", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.8971963822841644}]}, {"text": "Following and fora fair comparison, experiments are run for 100k training steps, i.e., in the low-resource settings all models are observed to converge within these steps.", "labels": [], "entities": []}, {"text": "Adaptation experiments are run to convergence, which requires roughly half of the steps (i.e., 50k) required to train the generic low-resource model.", "labels": [], "entities": [{"text": "convergence", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.929142951965332}]}, {"text": "On the other hand, large-data systems are trained for up to 800k steps, which also showed to be a convergence point.", "labels": [], "entities": []}, {"text": "For the final evaluation we take the best performing checkpoint on the dev set.", "labels": [], "entities": []}, {"text": "All models are trained using Tesla V100-pcie-16gb on a single GPU.: Performance of language identification on the low-resource and high-resource (pt L) settings", "labels": [], "entities": [{"text": "language identification", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.7501218318939209}]}], "tableCaptions": [{"text": " Table 2: Number of parallel sentences of the TED Talks  used for training, development and testing. At the bot- tom, the large-data set-up which uses the OpenSubtitles  (pt-BR L and pt-PT L) as additional training set.", "labels": [], "entities": []}, {"text": " Table 3: Performance of language identification on the  low-resource and high-resource (pt L) settings", "labels": [], "entities": [{"text": "language identification", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7478596270084381}]}, {"text": " Table 4: BLEU scores of the presented models, trained  with unsupervised, supervised and semi-supervised  data, from English to Brazilian Portuguese (pt-BR) and  European Portuguese (pt-EU), Canadian French (fr- CA) and European French (fr-EU), Croatian (hr) and  Serbian (sr), and Indonesian (id) and Malay (ms). Ar- rows \u2193\u2191 indicate statistically significant differences cal- culated against Mul using bootstrap resampling with  \u03b1 = 0.05", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981343150138855}, {"text": "Ar- rows", "start_pos": 315, "end_pos": 323, "type": "METRIC", "confidence": 0.9656690955162048}]}, {"text": " Table 5: BLEU score on the test set of models trained  with large-scale data, from English to Brazilian Por- tuguese (pt-BR) and European Portuguese (pt-EU). Ar- rows \u2193\u2191 indicate statistically significant differences cal- culated against the Mul model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992942810058594}, {"text": "Ar- rows", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9660526911417643}]}]}