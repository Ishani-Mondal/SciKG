{"title": [{"text": "A Treebank for the Healthcare Domain", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a treebank for the healthcare domain developed at ezDI.", "labels": [], "entities": []}, {"text": "The treebank is created from a wide array of clinical health record documents across hospitals.", "labels": [], "entities": []}, {"text": "The data has been de-identified and annotated for constituent syntactic structure.", "labels": [], "entities": []}, {"text": "The treebank contains a total of 52053 sentences that have been sampled for subdomains as well as linguistic variations.", "labels": [], "entities": []}, {"text": "The paper outlines the sampling process followed to ensure a better domain representation in the corpus, the annotation process and challenges, and corpus statistics.", "labels": [], "entities": []}, {"text": "The Penn Treebank tagset and guidelines were largely followed, but there were many syntactic contexts that warranted adaptation of the guidelines.", "labels": [], "entities": [{"text": "Penn Treebank tagset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9927731156349182}]}, {"text": "The treebank created was used to retrain the Berkeley parser and the Stanford parser.", "labels": [], "entities": []}, {"text": "These parsers were also trained with the GENIA treebank for comparative quality assessment.", "labels": [], "entities": [{"text": "GENIA treebank", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.9854001700878143}]}, {"text": "Our treebank yielded greater accuracy on both parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9995217323303223}]}, {"text": "Berkeley parser performed better on our treebank with an average F1 measure of 91 across 5-folds.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9926217198371887}]}, {"text": "This was a significant jump from the out-of-the-box F1 score of 70 on Berkeley parser's default grammar.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9801773130893707}]}], "introductionContent": [{"text": "There is severe paucity of data in healthcare due to the confidentiality regulations entailed.", "labels": [], "entities": []}, {"text": "However, the importance of domain specific training data cannot be denied.", "labels": [], "entities": []}, {"text": "It is a well acknowledged fact that systems trained on the general domain do not perform well in highly specialized domains like healthcare ().", "labels": [], "entities": []}, {"text": "The research is further hindered for tasks that require a large volume of annotated data such as syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.7855483293533325}]}, {"text": "Parsing is one of the complex natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9552618265151978}]}, {"text": "Its complexity is inherited from syntax.", "labels": [], "entities": []}, {"text": "Syntactic annotation is based on phrase structure grammar which posits a universal framework based on well-formedness conditions.", "labels": [], "entities": [{"text": "Syntactic annotation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8786362111568451}, {"text": "phrase structure grammar", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.8158938686052958}]}, {"text": "However, these frameworks are modeled on formal language and therefore fail to account for ungrammaticality or variations in style.", "labels": [], "entities": []}, {"text": "A universal syntactic framework even fora well-studied language like English is not established due to these reasons.", "labels": [], "entities": []}, {"text": "Clinical healthcare data is an apposite example.", "labels": [], "entities": []}, {"text": "It is populated with ungrammatical fragments and domain specific idiosyncrasies that cannot be accounted by standard grammatical rules.", "labels": [], "entities": []}, {"text": "Therefore, the annotation task involves a high level of complexity and subjectivity.", "labels": [], "entities": []}, {"text": "This paper showcases specific examples that justified adoption of new rules that are not postulated under the Penn Treebank guidelines (.", "labels": [], "entities": [{"text": "Penn Treebank guidelines", "start_pos": 110, "end_pos": 134, "type": "DATASET", "confidence": 0.9922058780988058}]}, {"text": "This is domain specific annotation.", "labels": [], "entities": []}, {"text": "This approach has been rewarding.", "labels": [], "entities": []}, {"text": "The Berkeley parser () trained on this domain specific treebank gave a high F1 score of 91.58 using ParsEval () method of evaluation.", "labels": [], "entities": [{"text": "F1", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9995661377906799}]}, {"text": "This is a remarkable improvement from the F1 of 70 that was attained on the parser's default grammar model.", "labels": [], "entities": [{"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9978965520858765}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Number and type of documents from each category included in Dataset 2 from Database 2", "labels": [], "entities": []}, {"text": " Table 3: Source databases, methods and resulting datasets", "labels": [], "entities": []}, {"text": " Table 4: Inter-annotator agreement calculated on 500 sentences from each annotator", "labels": [], "entities": []}, {"text": " Table 5: Comparison of inter-annotator agreement for various treebanks", "labels": [], "entities": []}, {"text": " Table 6: Comparative percentage of non-terminal nodes in three treebanks", "labels": [], "entities": []}, {"text": " Table 7: Comparison of treebanks trained on different parsers. Results in F1 score using ParsEval", "labels": [], "entities": [{"text": "F1 score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9703230261802673}, {"text": "ParsEval", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.4243590831756592}]}]}