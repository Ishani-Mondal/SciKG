{"title": [{"text": "Exploring word embeddings and phonological similarity for the unsupervised correction of language learner errors", "labels": [], "entities": []}], "abstractContent": [{"text": "The presence of misspellings and other errors or non-standard word forms poses a considerable challenge for NLP systems.", "labels": [], "entities": []}, {"text": "Although several supervised approaches have been proposed previously to normalize these, annotated training data is scarce for many languages.", "labels": [], "entities": []}, {"text": "We investigate , therefore, an unsupervised method where correction candidates for Swedish language learners' errors are retrieved from word embeddings.", "labels": [], "entities": []}, {"text": "Furthermore, we compare the usefulness of combining cosine similarity with orthographic and phonological similarity based on a neural grapheme-to-phoneme conversion system we train for this purpose.", "labels": [], "entities": []}, {"text": "Although combinations of similarity measures have been explored for finding correction candidates, it remains unclear how these measures relate to each other and how much they contribute individually to identifying the correct alternative.", "labels": [], "entities": []}, {"text": "We experiment with different combinations of these and find that integrating phonological information is especially useful when the majority of learner errors are related to misspellings, but less so when errors are of a variety of types including, e.g. grammatical errors.", "labels": [], "entities": []}], "introductionContent": [{"text": "During the language acquisition process, learners often use word forms which deviate, in one way or another, from a standard that native (L1) speakers usually adhere to.", "labels": [], "entities": []}, {"text": "These deviations, also referred to as errors or non-normative forms, result often in differences in spelling which can lead to forms that might not exist in a language (non-word errors).", "labels": [], "entities": []}, {"text": "Inferring the intended meaning and the correct word form for such errors can be challenging both for humans and for machines.", "labels": [], "entities": []}, {"text": "In previous work, a number of approaches have been tested for the automatic correction of language learner errors, which rely often on annotated data (.", "labels": [], "entities": [{"text": "automatic correction of language learner errors", "start_pos": 66, "end_pos": 113, "type": "TASK", "confidence": 0.7637386322021484}]}, {"text": "For normalizing native speakers' non-standard use of spelling, however, a recent direction being explored is the use of word embeddings as an unsupervised solution.", "labels": [], "entities": [{"text": "normalizing native speakers' non-standard use of spelling", "start_pos": 4, "end_pos": 61, "type": "TASK", "confidence": 0.8692909904888698}]}, {"text": "A major advantage of this approach is that it does not require annotated training data.", "labels": [], "entities": []}, {"text": "Such methods rely on the intuition that semantically similar words are grouped close to each other in the vector space.", "labels": [], "entities": []}, {"text": "Incorporating character n-grams when building word representations completes this type of similarity with orthographic and morphological relatedness ().", "labels": [], "entities": []}, {"text": "This can be useful for detecting correction candidates for those types of errors that involve only a slight variation of word forms, such as spelling and inflectional errors.", "labels": [], "entities": [{"text": "detecting correction", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.8942877054214478}]}, {"text": "Cosine similarity is often combined with other lexical similarity measures, their individual contribution and interaction, however, remains less explored.", "labels": [], "entities": []}, {"text": "The research questions that we address in this article are: \u2022 RQ1: How useful are word embeddings based on character n-grams for retrieving correction candidates for language learners' errors?", "labels": [], "entities": [{"text": "RQ1", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.7236059308052063}]}, {"text": "\u2022 RQ2: Does capturing phonological similarities between sounds provide helpful additional information for identifying corrections?", "labels": [], "entities": [{"text": "identifying corrections", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.8710109293460846}]}, {"text": "\u2022 RQ3: What combination of different similarity measures is most efficient in this context?", "labels": [], "entities": [{"text": "RQ3", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.5598123073577881}]}, {"text": "The usefulness of word embeddings with character n-grams has not been previously explored specifically for correcting errors made by second or foreign language (L2) learners.", "labels": [], "entities": [{"text": "correcting errors made by second or foreign language (L2) learners", "start_pos": 107, "end_pos": 173, "type": "TASK", "confidence": 0.7456142256657282}]}, {"text": "While there might be similarities between the type of spelling errors that L1 and L2 speakers make (e.g. based on the position of keys while typing), in the case of the latter category grammatical errors may also occur.", "labels": [], "entities": []}, {"text": "Moreover, L2 spelling errors maybe often induced also by sound similarity or orthographic differences between L1 and L2.", "labels": [], "entities": []}, {"text": "Since data annotated with information about L2 errors and their correction is scarce for most languages, exploring unsupervised methods in this context is particularly valuable.", "labels": [], "entities": []}, {"text": "This is the case also for Swedish, the target language of our experiments.", "labels": [], "entities": []}, {"text": "As an unsupervised solution, we evaluate embeddings where words are represented as the sum of their character n-grams for L2 error correction by inspecting how often correction candidates occur among the most similar words based on cosine similarity.", "labels": [], "entities": [{"text": "L2 error correction", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.5642911692460378}]}, {"text": "We compare embeddings created using a large collection of Wikipedia articles with embeddings trained on a smaller amount of specialized corpora related to L2 learning combined with blog texts.", "labels": [], "entities": []}, {"text": "We find that the latter alternative provides twice more often the correction among the set of most similar words than the former.", "labels": [], "entities": [{"text": "correction", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9887443780899048}]}, {"text": "One of the obstacles of spelling words correctly is that different graphemes can be used to encode the same sound in a certain language.", "labels": [], "entities": []}, {"text": "This similarly in pronunciation, however goes beyond the type of information that a similarity measure based on character n-grams and orthography could provide.", "labels": [], "entities": []}, {"text": "Therefore, we propose a phonological similarity measure for capturing this aspect.", "labels": [], "entities": []}, {"text": "To address RQ2, we train a grapheme-to-phoneme conversion system based on neural networks to be able to map even out-ofvocabulary (OOV) words, such as non-word errors, to a phonological representation.", "labels": [], "entities": [{"text": "RQ2", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.884503185749054}]}, {"text": "We show that this system improves on the results of a previous rule-based attempt at solving this task for Swedish.", "labels": [], "entities": []}, {"text": "We then compute the similarity based on a Levenshtein distance discounted for phonological relatedness between the phonological representation of each non-word error and that of the intended word.", "labels": [], "entities": [{"text": "similarity", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9870548248291016}]}, {"text": "Furthermore, we investigate the correlation between different similarity measures and find that phonological similarity correlates less with cosine and orthographic similarity than these two with each other.", "labels": [], "entities": []}, {"text": "In relation to RQ3, we compare cosine, orthographic and phonological similarity and investigate their interaction for finding the intended word for an L2 error.", "labels": [], "entities": []}, {"text": "We aggregate cosine similarity and the other two similarity measures individually and in combination and observe how the ranking of the correct word changes compared to other most similar words retrieved from word embeddings.", "labels": [], "entities": []}, {"text": "Our results indicate that combining cosine and orthographic similarity worked best for the variety of errors found in learner essays, whist for errors collected from spelling exercises summing all three measures, or combining cosine and phonological similarity was more efficient.", "labels": [], "entities": []}, {"text": "This work has also a number of engineering contributions, which are being made freely available.", "labels": [], "entities": []}, {"text": "1 This includes (i) a word embedding incorporating character n-grams, trained on a combination of L2 relevant corpora and blog texts; (ii) a grapheme-to-phoneme conversion system for Swedish based on a large lexical database and deep learning methods; (iii) the implementation of a phonological similarity measure based on binary phonological features rooted in linguistic theory.", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 141, "end_pos": 171, "type": "TASK", "confidence": 0.7402172386646271}]}, {"text": "This article is structured as follows.", "labels": [], "entities": []}, {"text": "In section 2, we present previous work related to error correction, which is followed by the description of a small evaluation dataset (Section 3) used to measure the usefulness of the proposed techniques.", "labels": [], "entities": [{"text": "error correction", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.8367162048816681}]}, {"text": "We then describe the word embeddings (Section 4) and the grapheme-to-phoneme conversion system (Section 5) created, which are at the basis of the similarity measures used for finding optimal correction candidates.", "labels": [], "entities": []}, {"text": "Section 6 details the results of our experiments on the usefulness of the similarity measures.", "labels": [], "entities": []}, {"text": "Finally, we conclude our paper in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the usefulness of word embeddings and similarity measures, we combined L2 Swedish learner error data from two different sources: a corpus of learner essays and logged spelling exercises.", "labels": [], "entities": []}, {"text": "From the SweLL learner corpus (, we collected errors from 24 randomly sampled essays between CEFR levels A1 -B1 from the SpIn sub-corpus.", "labels": [], "entities": [{"text": "SweLL learner corpus", "start_pos": 9, "end_pos": 29, "type": "DATASET", "confidence": 0.8016423384348551}, {"text": "CEFR levels A1 -B1", "start_pos": 93, "end_pos": 111, "type": "METRIC", "confidence": 0.7047831654548645}]}, {"text": "Learners' native languages included Romanian, Vietnamese, Somali, Tigrinya, Dari, Latvian, Thai, Mandarin Chinese, Kurdish, Swahili, Albanian and Arabic.", "labels": [], "entities": []}, {"text": "During the manual error correction, each non-lemmatized token was analyzed and, if they were errors, they were manually corrected.", "labels": [], "entities": []}, {"text": "Only errors for which the intended token could be unambiguously determined were included.", "labels": [], "entities": []}, {"text": "Errors with capitalization, foreign words and containing @, signaling unintelligible handwritten characters, were excluded.", "labels": [], "entities": []}, {"text": "Besides essays, we collected errors also from spelling exercise logs (SpellEx) from a Swedish language learning platform, L\u00e4rka 3 . We collected non-word errors from the responses of the 10 language learners (ranging from beginner to advanced level) who participated in a previous evaluation of the platform.", "labels": [], "entities": []}, {"text": "Word segmentation errors and other errors consisting of valid inflectional forms were not considered.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6555159986019135}]}, {"text": "The size of the dataset in terms of number of L2 errors and their distribution across data sources and CEFR levels is presented in It is worth noting that, while the spelling exercise log part of the data consisted mainly of L2 spelling errors, the errors collected from the essays displayed a wider variety of error types including grammatical and vocabulary errors.", "labels": [], "entities": [{"text": "CEFR", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.878157913684845}]}, {"text": "Duplicate pairs of error and intended word have been removed from the dataset, thus the numbers in refer to unique error types.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Digits in parenthesis  for SpIn indicate the number of individual essays.", "labels": [], "entities": []}, {"text": " Table 1: The number of corrected non-word L2 errors in the dataset.", "labels": [], "entities": []}, {"text": " Table 2: The performance of the Swedish g2p conversion system.", "labels": [], "entities": [{"text": "Swedish g2p conversion", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7025032639503479}]}, {"text": " Table 3: Percentage of corrections occurring among the k most similar words.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9625420570373535}]}, {"text": " Table 7: Effect of combining similarity measures.", "labels": [], "entities": [{"text": "similarity", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9454578757286072}]}]}