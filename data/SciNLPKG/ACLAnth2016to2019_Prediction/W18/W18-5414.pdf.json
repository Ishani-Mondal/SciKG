{"title": [{"text": "Evaluating the Ability of LSTMs to Learn Context-Free Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "While long short-term memory (LSTM) neu-ral net architectures are designed to capture sequence information, human language is generally composed of hierarchical structures.", "labels": [], "entities": []}, {"text": "This raises the question as to whether LSTMs can learn hierarchical structures.", "labels": [], "entities": []}, {"text": "We explore this question with a well-formed bracket prediction task using two types of brackets modeled by an LSTM.", "labels": [], "entities": [{"text": "bracket prediction", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7335213422775269}]}, {"text": "Demonstrating that such a system is learnable by an LSTM is the first step in demonstrating that the entire class of CFLs is also learnable.", "labels": [], "entities": []}, {"text": "We observe that the model requires exponential memory in terms of the number of characters and embedded depth, where a sub-linear memory should suffice.", "labels": [], "entities": []}, {"text": "Still, the model does more than memorize the training input.", "labels": [], "entities": []}, {"text": "It learns how to distinguish between relevant and irrelevant information.", "labels": [], "entities": []}, {"text": "On the other hand, we also observe that the model does not generalize well.", "labels": [], "entities": []}, {"text": "We conclude that LSTMs do not learn the relevant underlying context-free rules, suggesting the good overall performance is attained rather by an efficient way of evaluating nuisance variables.", "labels": [], "entities": []}, {"text": "LSTMs area way to quickly reach good results for many natural language tasks, but to understand and generate natural language one has to investigate other concepts that can make more direct use of natural language's structural nature.", "labels": [], "entities": []}], "introductionContent": [{"text": "Composing hierarchical structure for natural language is an extremely powerful tool for human language generation.", "labels": [], "entities": [{"text": "human language generation", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.6981808145840963}]}, {"text": "These structures are of great importance in order to extract semantic interpretation and enable us to produce avast repertoire of sentences via a very small set of rules.", "labels": [], "entities": [{"text": "extract semantic interpretation", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.6157782077789307}]}, {"text": "Having acquired such a set of rules, it is easy to construct new structures without having previously seen similar examples.", "labels": [], "entities": []}, {"text": "For purposes of external communication, the syntactic structures generated by grammars must be \"flattened\" or linearized into a sequential output form (e.g. written, signed, or spoken).", "labels": [], "entities": []}, {"text": "When reading such a (linearized) text, hearing a spoken sentence or observing a signed language, the structure has to be recovered implicitly to recover the original meaning (i.e., parsing).", "labels": [], "entities": []}, {"text": "In this study, we investigate whether Long Short-Term Memory (LSTM) models) possess this same ability as humans do: inferring rule-based structure from a linear representation.", "labels": [], "entities": []}, {"text": "show clearly that there are phenomena inhuman language that can only be understood by taking the underlying hierarchical structure into account.", "labels": [], "entities": []}, {"text": "For neural networks to do the same, it is therefore essential to acquire the underlying structure of sentences.", "labels": [], "entities": []}, {"text": "Recurrent neural networks are often used for tasks like language modeling (), parsing, machine translation (, and morphological compositions (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7287994772195816}, {"text": "parsing", "start_pos": 78, "end_pos": 85, "type": "TASK", "confidence": 0.9749729633331299}, {"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7915962338447571}]}, {"text": "LSTMs are inherently sequential models.", "labels": [], "entities": []}, {"text": "Since the hierarchical structures appearing in natural language often correlate with sequential statistical features, it can be difficult to evaluate whether an LSTM learns the underlying rules of the sentence's syntax or alternatively simply learns sequential statistical correlations.", "labels": [], "entities": []}, {"text": "In this paper we carryout experiments to determine this.", "labels": [], "entities": []}, {"text": "We setup our experiments by posing the LSTM with a bracket completion problem having two possible bracket types, a so-called Dyck Language.", "labels": [], "entities": []}, {"text": "A model which recognizes this language has to infer rules of the underlying structure.", "labels": [], "entities": []}, {"text": "Furthermore, a system that can solve this task is able to recognize every context-free grammar (see section 3 regarding Dyck Languages via the Chomsky-Sch\u00fctzenberger theorem for why this is so).", "labels": [], "entities": []}, {"text": "By analyzing the intermediate states of the corresponding LSTM networks, observing generalization behaviours, and evaluating the memory demands of the model we investigate whether LSTMs acquire rules as opposed to statistical regularities.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}