{"title": [], "abstractContent": [{"text": "We describe the Enron People Assignment (EPA) dataset, in which tasks that are described in emails are associated with the person(s) responsible for carrying out these tasks.", "labels": [], "entities": [{"text": "Enron People Assignment (EPA) dataset", "start_pos": 16, "end_pos": 53, "type": "DATASET", "confidence": 0.545308530330658}]}, {"text": "We identify tasks and the responsible people in the Enron email dataset.", "labels": [], "entities": [{"text": "Enron email dataset", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.9465040365854899}]}, {"text": "We define evaluation methods for this challenge and report scores for our model and na\u00efve baselines.", "labels": [], "entities": []}, {"text": "The resulting model enables a user experience operating within a commercial email service: given a person and a task, it determines if the person should be notified of the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The initial motivation for our dataset 1 is to enable development of a commercial email service that helps individuals track tasks that are assigned or that they have agreed to perform.", "labels": [], "entities": []}, {"text": "To that end, tasks are identified automatically from email text; when such an email is sent or received by an individual, they can be notified or reminded of any resulting tasks should they be responsible for carrying them out.", "labels": [], "entities": []}, {"text": "Thus, for the commercial email service there are two parts to the problem: (a) detecting tasks from text, and (b) associating them to specific individuals given a list of affiliated people.", "labels": [], "entities": [{"text": "detecting tasks from text", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.8632352650165558}]}, {"text": "The Sender, To and Cc list from the email provide a mostly comprehensive list of individuals.", "labels": [], "entities": []}, {"text": "The latter step of selecting responsible individuals is an example of addressee tagging, also referred to as addressee recognition.", "labels": [], "entities": [{"text": "addressee tagging", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.6915901154279709}, {"text": "addressee recognition", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.7045296430587769}]}, {"text": "For example, in, the task is to complete a draft report, and the people responsible are Anna and Brad.", "labels": [], "entities": []}, {"text": "However, addressee tagging is needed to match the \"you\" in the second sentence to \"Anna\".", "labels": [], "entities": [{"text": "addressee tagging", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7055866867303848}]}, {"text": "As this example demonstrates, often more than one person is responsible for carrying out a task; the task notification must be provided to each responsible party who recieves the email, though not to others who receive the email but are not responsible for the task.", "labels": [], "entities": []}, {"text": "There maybe no explicit mention of the person responsible, as in.", "labels": [], "entities": []}, {"text": "In this example, the context is found not in the user-typed text, but rather in the email client-generated metadata.", "labels": [], "entities": []}, {"text": "To complicate matters, the text may use implicit second party references.", "labels": [], "entities": []}, {"text": "Consider: \"Please complete a draft by Friday\".", "labels": [], "entities": []}, {"text": "Here, the imperative \"complete\" has an implicit \"you\" that can refer to either singular or plural recipients.", "labels": [], "entities": []}, {"text": "Finally, there exist cases in which a task intent is detected, but no one in the To/Cc list is responsible.", "labels": [], "entities": []}, {"text": "For example: \"Brad will complete the draft report\" has a task intent, but if Brad is not in the list of available individuals, we should not erroneously assign one of the recipients.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset consists of a set of tasks in email and the associated people.", "labels": [], "entities": []}, {"text": "Within each email, a task is indicated by a special <mark> tag (one per HIT); a set of email recipients (one or more per email) is also provided.", "labels": [], "entities": []}, {"text": "The subset of recipients (possibly empty) who are responsible for the marked task is also provided.", "labels": [], "entities": []}, {"text": "Each recipient is identified by their email address.", "labels": [], "entities": []}, {"text": "Sometimes recipients are email groups(\"some_group\"@enron.com); they are referred to implicitly by the sender of the email.", "labels": [], "entities": []}, {"text": "In this section, we provide qualitative and quantitative analysis of the dataset.", "labels": [], "entities": []}, {"text": "We also compare the dataset being released to a similarly created dataset from the Avocado email corpus.", "labels": [], "entities": [{"text": "Avocado email corpus", "start_pos": 83, "end_pos": 103, "type": "DATASET", "confidence": 0.9361449678738912}]}, {"text": "We cannot release the Avocado version of the dataset due to licensing restrictions.", "labels": [], "entities": [{"text": "Avocado version of the dataset", "start_pos": 22, "end_pos": 52, "type": "DATASET", "confidence": 0.9501134514808655}]}, {"text": "We use two different agreement calculations since the task seems to be surprisingly subjective and noisy, even after multiple rounds of annotator training.", "labels": [], "entities": []}, {"text": "The first is a perfect agreement metric where we simply calculate the number of universally agreed upon label for each (email, task, recipient) tuple (from a total of 15,649 tuples).", "labels": [], "entities": []}, {"text": "We found there is a 71.07% perfect agreement rate on the recipient level in the dataset.", "labels": [], "entities": [{"text": "perfect agreement rate", "start_pos": 27, "end_pos": 49, "type": "METRIC", "confidence": 0.8361156781514486}]}, {"text": "To understand rates of inter-annotator agreement better, we calculated Krippendorff's alpha (\u03b1), a general and robust reliability measure.", "labels": [], "entities": [{"text": "Krippendorff's alpha (\u03b1)", "start_pos": 71, "end_pos": 95, "type": "METRIC", "confidence": 0.7892620662848154}]}, {"text": "Our \u03b1 value is 0.6123.", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9852279424667358}]}, {"text": "When interpreting magnitude of agreement measures, Krippendorf suggests \u03b1 \u2265 0.800, and the threshold for tentative conclusions at 0.667.", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 72, "end_pos": 73, "type": "METRIC", "confidence": 0.9721435308456421}]}, {"text": "However, he goes onto say that there is no \"magic number\" other than perhaps a perfect consensus, and the appropriate \u03b1 must be determined through experimentation and empirical evidence.", "labels": [], "entities": []}, {"text": "In this regard we are still determining an acceptable \u03b1 for the production scenario.", "labels": [], "entities": []}, {"text": "Theoretically the best \u03b1 would be 1.0, but as we have seen, if we take only data with perfect consensus, we lose up to 28.93% of the collected data (many of which still have a majority consensus).", "labels": [], "entities": []}, {"text": "In, we compare these results on Enron with corresponding annotations over the Avocado dataset.", "labels": [], "entities": [{"text": "Enron", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.9663379788398743}, {"text": "Avocado dataset", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.9854753017425537}]}, {"text": "As we can see, the agreement and reliability of the Avocado set is substantially higher.", "labels": [], "entities": [{"text": "agreement", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9947524070739746}, {"text": "reliability", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9855884909629822}, {"text": "Avocado set", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.9597161412239075}]}, {"text": "When asking the managed annotators if they felt there was any difference between the two sets, and by looking at the data ourselves, the biggest differences seem to be: 1.", "labels": [], "entities": []}, {"text": "2. Avocado formatting is more consistent, and the annotators find it easier to parse.", "labels": [], "entities": [{"text": "Avocado formatting", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.5698987394571304}]}, {"text": "3. Avocado sentence separation is cleaner; due to simpler formatting and line breaks.", "labels": [], "entities": [{"text": "Avocado sentence separation", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7081070343653361}]}, {"text": "Future work on the Enron dataset may include additional pre-processing to place less burden on the annotators.", "labels": [], "entities": [{"text": "Enron dataset", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.966698557138443}]}, {"text": "In addition to the agreement and reliability metrics, we have calculated several other statistics in the universally agreed annotated Enron data.", "labels": [], "entities": [{"text": "agreement", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9643772840499878}, {"text": "reliability", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.6586136817932129}, {"text": "Enron data", "start_pos": 134, "end_pos": 144, "type": "DATASET", "confidence": 0.9264644384384155}]}, {"text": "Similarly, for email+task combinations with multiple recipients, we report basic statistics on distribution of recipients in.", "labels": [], "entities": []}, {"text": "In the production scenario, performance is measured by the precision and recall of task assignment to a recipient on the recipient list.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9990979433059692}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9957364797592163}]}, {"text": "The other two metrics we looked at were precision and recall of single recipient vs multi recipient emails, and the distribution of precision and recall for each email.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.999202311038971}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.972276508808136}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.99489426612854}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9859400987625122}]}, {"text": "The calculation of the precision and recall is based on the simple binary label assigned to the (email, task, recipient) tuple.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9995614886283875}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9992669224739075}]}, {"text": "To train the model, we are currently using the data collected from the Avocado training set.", "labels": [], "entities": [{"text": "Avocado training set", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.9631786743799845}]}, {"text": "We have trained on 3872 emails, and we took the majority consensus HIT (otherwise random).", "labels": [], "entities": [{"text": "HIT", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.7102280855178833}]}, {"text": "In the future we can try to incorporate annotator reliability metrics to allow us to filter for more data.", "labels": [], "entities": []}, {"text": "The model is trained using logistic regression with a set of handcrafted features (described in supplementary material).", "labels": [], "entities": []}, {"text": "The best feature contributions come from token replacement and encoding out-of-sentence token information.", "labels": [], "entities": [{"text": "token replacement", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.887892872095108}]}], "tableCaptions": [{"text": " Table 2. Rate of perfect agreement and reliability.", "labels": [], "entities": [{"text": "Rate", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9943947792053223}, {"text": "perfect agreement", "start_pos": 18, "end_pos": 35, "type": "METRIC", "confidence": 0.7637943029403687}, {"text": "reliability", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9879516959190369}]}, {"text": " Table 5. Baseline performance for single and multiple  recipients.", "labels": [], "entities": []}]}