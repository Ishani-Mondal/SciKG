{"title": [{"text": "Character-level Supervision for Low-resource POS Tagging", "labels": [], "entities": [{"text": "Character-level Supervision", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5923669338226318}, {"text": "POS Tagging", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.8123070895671844}]}], "abstractContent": [{"text": "Neural part-of-speech (POS) taggers are known to not perform well with little training data.", "labels": [], "entities": [{"text": "Neural part-of-speech (POS) taggers", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5658629139264425}]}, {"text": "As a step towards overcoming this problem, we present an architecture for learning more robust neural POS tag-gers by jointly training a hierarchical, recurrent model and a recurrent character-based sequence-to-sequence network supervised using an auxiliary objective.", "labels": [], "entities": []}, {"text": "This way, we introduce stronger character-level supervision into the model, which enables better generalization to unseen words and provides regularization, making our encoding less prone to overfitting.", "labels": [], "entities": []}, {"text": "We experiment with three auxiliary tasks: lemma-tization, character-based word autoencod-ing, and character-based random string autoencoding.", "labels": [], "entities": []}, {"text": "Experiments with minimal amounts of labeled data on 34 languages show that our new architecture out-performs a single-task baseline and, surprisingly , that, on average, raw text au-toencoding can be as beneficial for low-resource POS tagging as using lemma information.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 231, "end_pos": 242, "type": "TASK", "confidence": 0.868211418390274}]}, {"text": "Our neural POS tagger closes the gap to a state-of-the-art POS tagger (MarMoT) for low-resource scenarios by 43%, even outperforming it on languages with templatic morphology, e.g., Arabic, Hebrew, and Turkish, by some margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "POS tagging, i.e., assigning syntactic categories to tokens in context, is an important first step when developing language technology for low-resource languages.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7910475730895996}]}, {"text": "POS tags can provide an efficient inductive bias for modeling downstream tasks, especially if training data for these tasks are limited.", "labels": [], "entities": []}, {"text": "However, POS tagging can be very challenging if only a few labeled sentences are available.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9280839264392853}]}, {"text": "Previous work on POS tagging with limited or no annotated data comes in three flavors, e.g.,: unsupervised POS induction, crosslingual transfer, or, if some suitable data are available, supervised induction from small labeled corpora or dictionaries.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.8933366537094116}, {"text": "POS induction", "start_pos": 107, "end_pos": 120, "type": "TASK", "confidence": 0.8169922530651093}, {"text": "crosslingual transfer", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.7835191488265991}]}, {"text": "This work focuses on the latter: We explore the effect of multi-task learning for building robust POS taggers for low-resource languages from small amounts of annotated data.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.7918414175510406}]}, {"text": "In low-resource settings, neural POS taggers have been observed to perform poorly compared to log-linear models.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.6957875937223434}]}, {"text": "This is unfortunate, since neural POS taggers have other advantages, including being easily integrable into multi-task learning architectures, sidestepping feature engineering, and providing compact word-level and sentence-level representations.", "labels": [], "entities": [{"text": "neural POS taggers", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.6441014508406321}]}, {"text": "In this paper, we therefore take steps to bridge the gap to state-of-the-art taggers in such scenarios.", "labels": [], "entities": []}, {"text": "Specifically, we consider training neural POS taggers from 478 annotated tokens (the size of the smallest treebank in UD 2.0 1 ).", "labels": [], "entities": [{"text": "training neural POS taggers", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.6094700843095779}]}, {"text": "In such a setting, it is often useful to leverage data from other, related tasks ( , if available.", "labels": [], "entities": []}, {"text": "However, since for many low-resource languages such data is hard to find, we consider multi-task learning scenarios with no other sequence labeling data at hand: (i) a scenario in which type-based morphological information is available, e.g., wordlemma pairs as can be found in standard dictionaries or UniMorph, (ii) a scenario where we only rely on raw text corpora in the language, and (iii) a scenario where we do not assume any additional data, but construct a synthetic auxiliary task instead.", "labels": [], "entities": []}, {"text": "In order to include secondary information such as word-lemma pairs into our model, we integrate a character-based recurrent sequence-to-sequence model into a hierarchical long short-term memory (LSTM) sequence tagger (cf.).", "labels": [], "entities": [{"text": "hierarchical long short-term memory (LSTM) sequence tagger", "start_pos": 158, "end_pos": 216, "type": "TASK", "confidence": 0.6388952864540948}]}, {"text": "By formulating suitable auxiliary tasks (lemmatization, word autoencoding or random string autoencoding, respectively), we can include additional character-level supervision into our model via multi-task training.", "labels": [], "entities": []}, {"text": "We present a novel architecture for inducing more robust neural POS taggers from small samples of annotated data in low-resource languages, combining a hierarchical, deep bi-LSTM sequence tagger with a character-based sequence-to-sequence model.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.6514894664287567}]}, {"text": "Furthermore, we experiment with different choices of external resources and corresponding auxiliary tasks and show that autoencoding can be as efficient as an auxiliary task for low-resource POS tagging as lemmatization.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 191, "end_pos": 202, "type": "TASK", "confidence": 0.8220337927341461}]}, {"text": "Finally, we evaluate our models on 34 typologically diverse languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we will describe our experiments, including data, baselines, and hyperparameters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: F-score deltas between the neural single-task baseline (POS) and our multi-task systems.", "labels": [], "entities": [{"text": "F-score deltas", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9585189819335938}]}, {"text": " Table 3: Average character embedding distances,  averaged over all languages.", "labels": [], "entities": []}, {"text": " Table 4: Minimum character embedding dis- tances, averaged over all languages.", "labels": [], "entities": []}]}