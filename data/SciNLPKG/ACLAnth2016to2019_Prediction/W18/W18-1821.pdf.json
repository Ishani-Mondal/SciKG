{"title": [{"text": "Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7950283586978912}]}], "abstractContent": [{"text": "SGNMT is a decoding platform for machine translation which allows paring various modern neural models of translation with different kinds of constraints and symbolic models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7724480926990509}]}], "introductionContent": [{"text": "The rate of innovation in machine translation (MT) has gathered impressive momentum over the recent years.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8558347225189209}]}, {"text": "The discovery and maturation of the neural machine translation (NMT) paradigm) has led to steady and substantial improvements of translation performance.", "labels": [], "entities": [{"text": "neural machine translation (NMT) paradigm", "start_pos": 36, "end_pos": 77, "type": "TASK", "confidence": 0.8453586271830967}]}, {"text": "shows that this progress is often driven by significant changes in the network architecture.", "labels": [], "entities": []}, {"text": "This volatility poses major challenges in MT-related research, teaching, and industry.", "labels": [], "entities": [{"text": "MT-related research", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.9402574300765991}]}, {"text": "Researchers potentially spend a lot of time implementing to keep their setups up-to-date with the latest models, teaching needs to identify suitable material in a changing environment, and the industry faces demanding speed requirements on its deployment processes.", "labels": [], "entities": []}, {"text": "Another practical challenge many researchers are struggling with is the large number of available NMT tools.", "labels": [], "entities": []}, {"text": "Committing to one particular NMT tool bears the risk of being outdated soon, as keeping up with the pace of research is especially costly for NMT software developers.", "labels": [], "entities": []}, {"text": "The open-source SGNMT (Syntactically Guided Neural Machine Translation) decoder 2 () is our attempt to mediate the effects of the rapid progress in: Best systems on the English-German WMT news-test2014 test set over the years (BLEU script: Moses' multi-bleu.pl).", "labels": [], "entities": [{"text": "SGNMT (Syntactically Guided Neural Machine Translation)", "start_pos": 16, "end_pos": 71, "type": "TASK", "confidence": 0.5721779949963093}, {"text": "WMT news-test2014 test set", "start_pos": 184, "end_pos": 210, "type": "DATASET", "confidence": 0.8351533338427544}, {"text": "BLEU", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.9044175744056702}]}, {"text": "MT and the diversity of available NMT software.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.6376379728317261}]}, {"text": "SGNMT introduces the concept of predictors as abstract scoring modules with left-to-right semantics.", "labels": [], "entities": []}, {"text": "We can think of a predictor as an interface to a particular neural model or NMT tool.", "labels": [], "entities": []}, {"text": "However, the interface also allows to implement constraints like in lattice or n-best list rescoring, and symbolic models such as n-gram language models or counting models as predictors.", "labels": [], "entities": []}, {"text": "Our software architecture is designed to facilitate the implementation of new predictors.", "labels": [], "entities": []}, {"text": "Therefore, SGNMT can be extended to anew model or tool with very limited coding effort because rather than reimplementing models it is often enough to access APIs within an adapter predictor.", "labels": [], "entities": [{"text": "SGNMT", "start_pos": 11, "end_pos": 16, "type": "TASK", "confidence": 0.8931283354759216}]}, {"text": "3 Software packages which are not written in Python can be exposed in SGNMT if they have a Python interface.", "labels": [], "entities": []}, {"text": "Once anew predictor is implemented, it can be directly combined with all other predictors which are already available in SGNMT.", "labels": [], "entities": []}, {"text": "Therefore, general techniques like lattice and n-best list rescoring (, ensembling, MBR-based NMT (), etc.", "labels": [], "entities": []}, {"text": "only need to be implemented once (as predictor), and are automatically available for all models.", "labels": [], "entities": []}, {"text": "This does not only speedup the transition to anew NMT toolkit, it also allows the combination of different NMT implementations, eg. ensembling a Theano-based NMT model) with a TensorFlow-based Tensor2Tensor (Google, 2017) model.", "labels": [], "entities": []}, {"text": "demonstrated the versatility of SGNMT by combining five very different models (RNN LM, feedforward NPLM, Kneser-Ney LM, bag-to-seq model, seq-to-seq model) and a bag-of-words constraint using predictors.", "labels": [], "entities": []}, {"text": "Not only the way scores are assigned to translations is open for extension in SGNMT (via predictors), but also the search strategy (decoder) itself.", "labels": [], "entities": []}, {"text": "Decoders in SGNMT are defined upon the predictor abstraction, which means that any search strategy is compatible with any predictor constellation.", "labels": [], "entities": []}, {"text": "Therefore, common search procedures like beam search do not need to be reimplemented for every new model or toolkit.", "labels": [], "entities": [{"text": "beam search", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.849726676940918}]}, {"text": "2 to 4 describe central concepts in SGNMT like predictors and decoders briefly and outline some common use cases.", "labels": [], "entities": [{"text": "SGNMT", "start_pos": 36, "end_pos": 41, "type": "TASK", "confidence": 0.9583815336227417}]}, {"text": "5 shows that the SGNMT software architecture has proven to be very well suited for our research as new directions can be quickly prototyped, and new NMT toolkits can be introduced without breaking old code.", "labels": [], "entities": []}, {"text": "7 discuss the benefits of SGNMT in teaching and industry, respectively.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BPE-level SMT lattice rescoring with different search strategies. The BLEU score  does not benefit from less search errors due to modeling errors.", "labels": [], "entities": [{"text": "BPE-level", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.5211967825889587}, {"text": "SMT lattice rescoring", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7806265155474345}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9989590644836426}]}, {"text": " Table 2: BLEU scores of SGNMT with different NMT back ends on the complete KFTT test  set (Neubig, 2011) computed with multi-bleu.pl. All neural systems are BPE-based (Sen- nrich et al., 2016) with vocabulary sizes of 30K. The SMT baseline achieves 18.1 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990637898445129}, {"text": "KFTT test  set", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.9514259696006775}, {"text": "SMT", "start_pos": 228, "end_pos": 231, "type": "TASK", "confidence": 0.964531660079956}, {"text": "BLEU", "start_pos": 255, "end_pos": 259, "type": "METRIC", "confidence": 0.9986805319786072}]}]}