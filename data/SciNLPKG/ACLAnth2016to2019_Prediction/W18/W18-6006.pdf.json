{"title": [{"text": "Mind the Gap: Data Enrichment in Dependency Parsing of Elliptical Constructions", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we focus on parsing rare and non-trivial constructions, in particular ellip-sis.", "labels": [], "entities": []}, {"text": "We report on several experiments in enrichment of training data for this specific construction, evaluated on five languages: Czech, English, Finnish, Russian and Slovak.", "labels": [], "entities": []}, {"text": "These data enrichment methods draw upon self-training and tri-training, combined with a stratified sampling method mimicking the structural complexity of the original treebank.", "labels": [], "entities": []}, {"text": "In addition, using these same methods, we also demonstrate small improvements over the CoNLL-17 parsing shared task winning system for four of the five languages, not only restricted to the elliptical constructions.", "labels": [], "entities": [{"text": "CoNLL-17 parsing shared task winning", "start_pos": 87, "end_pos": 123, "type": "TASK", "confidence": 0.8034634947776794}]}], "introductionContent": [{"text": "Dependency parsing of natural language text may seem like a solved problem, at least for resourcerich languages and domains, where state-of-theart parsers attack or surpass 90% labeled attachment score (LAS) ( . However, certain syntactic phenomena such as coordination and ellipsis are notoriously hard and even stateof-the-art parsers could benefit from better models of these constructions.", "labels": [], "entities": [{"text": "Dependency parsing of natural language text", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.873333732287089}, {"text": "labeled attachment score (LAS)", "start_pos": 177, "end_pos": 207, "type": "METRIC", "confidence": 0.8809170722961426}]}, {"text": "Our work focuses on one such construction that combines both coordination and ellipsis: gapping, an omission of a repeated predicate which can be understood from context.", "labels": [], "entities": []}, {"text": "For example, in Mary won gold and Peter bronze, the second instance of the verb is omitted, as the meaning is evident from the context.", "labels": [], "entities": []}, {"text": "In dependency parsing this creates a situation where the parent node is missing (omitted verb won) while its dependents are still present (Peter and bronze).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8289921879768372}]}, {"text": "In the Universal Dependencies annotation scheme () gapping constructions are analyzed by promoting one of the orphaned dependents to the position of its missing parent, and connecting all remaining core arguments to that promoted one with the orphan relation (see).", "labels": [], "entities": []}, {"text": "Therefore the dependency parser must learn to predict relations between words that should not usually be connected.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.7202186584472656}]}, {"text": "Gapping has been studied extensively in theoretical works.", "labels": [], "entities": [{"text": "Gapping", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.789032518863678}]}, {"text": "However, it received almost no attention in NLP works, neither concerned with parsing nor with corpora creation.", "labels": [], "entities": []}, {"text": "Among the recent papers, proposed a one-endpoint-crossing graph parser able to recover a range of null elements and trace types, and proposed two methods to recover elided predicates in sentences with gapping.", "labels": [], "entities": []}, {"text": "The aforementioned lack of corpora that would pay attention to gapping, as well as natural relative rarity of gapping, leads to its underrepresentation in training corpora: they do not provide enough examples for the parser to learn gapping.", "labels": [], "entities": []}, {"text": "Therefore we investigate methods of enriching the training data with new material from large raw corpora.", "labels": [], "entities": []}, {"text": "The present work consist of two parts.", "labels": [], "entities": []}, {"text": "In the first part, we experiment on enriching data in general, without a specific focus on gapping constructions.", "labels": [], "entities": []}, {"text": "This part builds upon self-training and tritraining related work known from the literature, but also develops and tests a stratified approach for selecting a structurally balanced subcorpus.", "labels": [], "entities": []}, {"text": "In the second part, we focus on elliptical sentences, comparing general enrichment of training data with enrichment using elliptical sentences artificially constructed by removal of a coordinated element.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, we set out to evaluate the overall quality of the trees in the raw enrichment dataset produced by our self-training variant by parsing and filtering web crawl data.", "labels": [], "entities": []}, {"text": "In our baseline experiments we train parsers (Dozat et al., 2017) using purely the new self-training data.", "labels": [], "entities": []}, {"text": "From the full self-training dataset we sample datasets comparable to the sizes of the original treebanks to train parsers.", "labels": [], "entities": []}, {"text": "These parsers are then evaluated using the original test set of the corresponding treebank.", "labels": [], "entities": []}, {"text": "This gives us an overall estimate of the self-training data quality compared to the original treebanks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The size of the artificial data", "labels": [], "entities": []}, {"text": " Table 2: Training data sizes after each sampling strategy compared to the original treebank training section (TB),  sentences/tokens.", "labels": [], "entities": []}, {"text": " Table 3: Results of the baseline parsing experiments,  using only automatically collected data, reported in  terms of LAS%. Random T: random sample, same  amount of tokens as in the Random S samples; Random  S: random sample, same amount of sentences as in the  original treebanks; Identical: identical sample, imitates  the distribution of trees in the original treebanks. For  comparison, the TB column shows the LAS of a parser  trained on the original treebank training data.", "labels": [], "entities": [{"text": "LAS", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9918393492698669}, {"text": "LAS", "start_pos": 416, "end_pos": 419, "type": "METRIC", "confidence": 0.9917266368865967}]}]}