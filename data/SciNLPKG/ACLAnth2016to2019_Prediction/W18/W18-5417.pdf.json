{"title": [{"text": "Indicatements that character language models learn English morpho-syntactic units and regularities", "labels": [], "entities": []}], "abstractContent": [{"text": "Character language models have access to surface morphological patterns, but it is not clear whether or how they learn abstract morphological regularities.", "labels": [], "entities": []}, {"text": "We instrument a character language model with several probes, finding that it can develop a specific unit to identify word boundaries and, by extension, morpheme boundaries, which allows it to capture linguistic properties and regularities of these units.", "labels": [], "entities": []}, {"text": "Our language model proves surprisingly good at identifying the selectional restrictions of English derivational morphemes, a task that requires both morphological and syntactic awareness.", "labels": [], "entities": []}, {"text": "Thus we conclude that, when morphemes overlap extensively with the words of a language, a character language model can perform morphological abstraction.", "labels": [], "entities": []}], "introductionContent": [{"text": "Character-level language models) are appealing because they enable openvocabulary generation of language, and conditional character language models have now been convincingly used in speech recognition) and machine translation ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.7422165423631668}, {"text": "machine translation", "start_pos": 207, "end_pos": 226, "type": "TASK", "confidence": 0.8352292478084564}]}, {"text": "They succeed due to parameter-sharing between frequent, rare, and even unobserved training words, prompting claims that they learn morphosyntactic properties of words.", "labels": [], "entities": []}, {"text": "For example, claim that character language models yield \"better modelling rare morphological variants\" while claim that \"Character-level models obviate the need for morphological tagging or manual feature engineering.\"", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 165, "end_pos": 186, "type": "TASK", "confidence": 0.7455779314041138}]}, {"text": "But these claims of morphological awareness are backed more by intuition than direct empirical evidence.", "labels": [], "entities": [{"text": "morphological awareness", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.8605062067508698}]}, {"text": "What do these models really learn about morphology?", "labels": [], "entities": []}, {"text": "And, to the extent that they learn about morphology, how do they learn it?", "labels": [], "entities": []}, {"text": "Our goal is to shed light on these questions, and to that end, we study the behavior of a characterlevel language model (hereafter LM) applied to English.", "labels": [], "entities": []}, {"text": "We observe that, when generating text, the LM applies certain morphological processes of English productively, i.e. in novel contexts ( \u00a73).", "labels": [], "entities": []}, {"text": "This rather surprising finding suggests that the model can identify the morphemes relevant to these processes.", "labels": [], "entities": []}, {"text": "An analysis of the LM's hidden units presents a possible explanation: there appears to be one particular unit that fires at morpheme and word boundaries ( \u00a74).", "labels": [], "entities": []}, {"text": "Further experiments reveal that the LM learns morpheme boundaries through extrapolation from word boundaries ( \u00a75).", "labels": [], "entities": []}, {"text": "In addition to morphology, the LM appears to encode syntactic information about words, i.e. their part of speech ( \u00a76).", "labels": [], "entities": []}, {"text": "With access to both morphology and syntax, the model should also be able to learn linguistic phenomena at the intersection of the two domains, which we indeed find to be the case: the LM captures the (syntactic) selectional restrictions of English derivational morphemes, albeit with some incorrect generalizations ( \u00a77).", "labels": [], "entities": []}, {"text": "The conclusions of this work can thus be summarized in two main points-a character-level language model can: 1.", "labels": [], "entities": []}, {"text": "learn to identify linguistic units of higher order, such as morphemes and words.", "labels": [], "entities": []}, {"text": "2. learn some underlying linguistic properties and regularities of said units.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: C2M Performance. WE stand for word edge,  EOW for end of word, and PREF for prefix.", "labels": [], "entities": [{"text": "WE", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9962479472160339}, {"text": "EOW", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9933579564094543}, {"text": "PREF", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9788899421691895}]}]}