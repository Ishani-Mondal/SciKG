{"title": [{"text": "UFRGS Participation on the WMT Biomedical Translation Shared Task", "labels": [], "entities": [{"text": "UFRGS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8494215607643127}, {"text": "WMT Biomedical Translation Shared", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.8592166751623154}]}], "abstractContent": [{"text": "This paper describes the machine translation systems developed by the Universidade Federal do Rio Grande do Sul (UFRGS) team for the biomedical translation shared task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7456535696983337}, {"text": "biomedical translation shared task", "start_pos": 133, "end_pos": 167, "type": "TASK", "confidence": 0.8579007387161255}]}, {"text": "Our systems are based on statistical machine translation and neural machine translation, using the Moses and OpenNMT toolkits, respectively.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6604684690634409}, {"text": "neural machine translation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.721098800500234}]}, {"text": "We participated in four translation directions for the English/Spanish and En-glish/Portuguese language pairs.", "labels": [], "entities": []}, {"text": "To create our training data, we concatenated several parallel corpora, both from in-domain and out-of-domain sources, as well as terminological resources from UMLS.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 159, "end_pos": 163, "type": "DATASET", "confidence": 0.9454530477523804}]}, {"text": "Our systems achieved the best BLEU scores according to the official shared task evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9988940358161926}]}], "introductionContent": [{"text": "In this paper, we present the system developed at the Universidade Federal do Rio Grande do Sul (UFRGS) for the Biomedical Translation shared task in the Third Conference on Machine Translation (WMT18), which consists in translating scientific texts from the biological and health domain.", "labels": [], "entities": [{"text": "Biomedical Translation shared task in the Third Conference on Machine Translation (WMT18)", "start_pos": 112, "end_pos": 201, "type": "TASK", "confidence": 0.7655683990035739}]}, {"text": "In this edition of the shared task, six language pairs are considered: English/Chinese, English/French, English/German, English/Portuguese, English/Romanian, and English/Spanish.", "labels": [], "entities": []}, {"text": "Our participation in this task considered the English/Portuguese and English/Spanish language pairs, with translations in both directions.", "labels": [], "entities": []}, {"text": "For that matter, we developed two machine translation (MT) systems: one based on statistical machine translation (SMT), using Moses (, and one using neural machine translation (NMT), using OpenNMT (.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.8441253960132599}, {"text": "statistical machine translation (SMT)", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.7731412351131439}]}, {"text": "This paper is structured as follows: Section 3 details the language resources used to train our translation models.", "labels": [], "entities": []}, {"text": "Section 4 contains the description of the experimental settings of our SMT and NMT models, including the pre-processing step performed to comply with the shared task guidelines.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9631270170211792}]}, {"text": "In Section 5 we present the results and briefly discuss the main findings.", "labels": [], "entities": []}, {"text": "Section 6 contains the conclusions and directions of future works to improve our models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we detail the pre-processing steps employed as well as the architecture of the SMT and NMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9577184319496155}]}, {"text": "We now detail the results achieved by our SMT and NMT systems on the official test data used in the shared task.", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.8594083189964294}]}, {"text": "shows the BLEU scores () for both systems and for the submissions made by other teams.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999068558216095}]}, {"text": "Our submissions achieved the best results for all translation directions we participated, with remarkable BLEU scores for the ES/EN and PT/EN pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9996522665023804}]}, {"text": "When compared to the other teams, our results presented similar behavior, with higher scores when English was the target language, which maybe explained by the poor English morphosyntactic system.", "labels": [], "entities": []}, {"text": "For the English/Spanish pair, the SMT system presented slightly better results than the NMT one, probably due to the dictionary size used in the NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9889996647834778}, {"text": "NMT", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.937859058380127}, {"text": "NMT", "start_pos": 145, "end_pos": 148, "type": "DATASET", "confidence": 0.9527867436408997}]}, {"text": "Regarding the superior results achieved, we expect that the large parallel corpora used in our experiments played an essential role.", "labels": [], "entities": []}, {"text": "Although we did not use the provided Scielo abstracts corpus (, we used a newer parallel corpus also from Scielo, but comprised of full-text articles (), which overlaps with the abstracts, but contains more data.", "labels": [], "entities": [{"text": "Scielo abstracts corpus", "start_pos": 37, "end_pos": 60, "type": "DATASET", "confidence": 0.6777942379315695}]}, {"text": "In addition to the biomedical and health corpora, we employed two out-of-domain corpora that we assumed to have a similar structure to scientific texts: the books and the JRC-Acquis (  large Europarl corpus (, since it is comprised of speeches transcripts, which do not follow the usual structure of scientific texts.", "labels": [], "entities": [{"text": "JRC-Acquis", "start_pos": 171, "end_pos": 181, "type": "DATASET", "confidence": 0.9295530915260315}, {"text": "Europarl corpus", "start_pos": 191, "end_pos": 206, "type": "DATASET", "confidence": 0.901884138584137}]}], "tableCaptions": [{"text": " Table 1 depicts the original number of paral- lel segments according to each corpora source. In  Section 3.1, we detail the pre-processing steps per- formed on the data to comply with the task evalu- ation.", "labels": [], "entities": []}, {"text": " Table 4: Official BLEU scores for the English/Spanish and English/Portuguese language pairs in both translation  directions. Bold numbers indicate the best result for each direction.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9636298418045044}]}]}