{"title": [{"text": "Does Syntactic Knowledge in Multilingual Language Models Transfer Across Languages?", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent work has shown that neural models can be successfully trained on multiple languages simultaneously.", "labels": [], "entities": []}, {"text": "We investigate whether such models learn to share and exploit common syntactic knowledge among the languages on which they are trained.", "labels": [], "entities": []}, {"text": "This extended abstract presents our preliminary results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work has shown that state-of-the-art neural models of language and translation can be successfully trained on multiple languages simultaneously without changing the model architecture.", "labels": [], "entities": []}, {"text": "In some cases this leads to improved performance compared to models only trained on a specific language, suggesting that multilingual models learn to share useful knowledge crosslingually through their learned representations.", "labels": [], "entities": []}, {"text": "While a large body of research exists on the multilingual mind, the mechanisms explaining knowledge sharing in computational multilingual models remain largely unknown: What kind of knowledge is shared among languages?", "labels": [], "entities": []}, {"text": "Do multilingual models mostly benefit from a better modeling of lexical entries or do they also learn to share more abstract linguistic categories?", "labels": [], "entities": []}, {"text": "We focus on the case of language models (LM) trained on two languages, one of which (L1) is over-resourced with respect to the other (L2), and investigate whether the syntactic knowledge learned for L1 is transferred to L2.", "labels": [], "entities": []}, {"text": "To this end we use the long-distance agreement benchmark recently introduced by.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider the scenario where L1 is overresourced compared to L2 and train our bilingual models by joint training on a mixed L1/L2 corpus so that supervision is provided simultaneously in the two languages.", "labels": [], "entities": []}, {"text": "We leave the evaluation of pre-training (or transfer learning) methods () to future work.", "labels": [], "entities": []}, {"text": "The monolingual LM is trained on a small L2 corpus (LM L2 ).", "labels": [], "entities": []}, {"text": "The bilingual LM is trained on a shuffled mix of the same small L2 corpus and a large L1 corpus, where L2 is oversampled to approximately match the amount of L1 sentences (LM L1+L2 ).", "labels": [], "entities": []}, {"text": "See for the actual training sizes.", "labels": [], "entities": []}, {"text": "For our preliminary experiments we have chosen French as the helper language (L1) and Italian as the target language (L2).", "labels": [], "entities": []}, {"text": "Since French and Italian share many morphosyntactic patterns, accuracy on the Italian agreement tasks is expected to benefit from adding French sentences to the training data if syntactic transfer occurs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9987913966178894}, {"text": "syntactic transfer", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.7355864644050598}]}, {"text": "Data and training details: We train our LMs on French and Italian Wikipedia articles extracted using the WikiExtractor tool.", "labels": [], "entities": []}, {"text": "1 For each language, we maintain a vocabulary of the 50k most frequent tokens, and replace the remaining tokens by <unk>.", "labels": [], "entities": []}, {"text": "For the bilingual LM, all words are prepended with a language tag so that vocabularies are completely disjoint.", "labels": [], "entities": []}, {"text": "Their union (100K types) is used to train the model.", "labels": [], "entities": []}, {"text": "This is the least optimistic scenario for linguistic transfer but also the most controlled one.", "labels": [], "entities": [{"text": "linguistic transfer", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8120544850826263}]}, {"text": "In future experiments we plan to study how transfer is affected by varying degrees of vocabulary overlap.", "labels": [], "entities": []}, {"text": "Following the setup of, we train 2-layer LSTM models with embedding and hidden layers of 650 dimensions for 40 epochs.", "labels": [], "entities": []}, {"text": "The trained models are evaluated on the Italian section of the syntactic benchmark provided by, which includes various non-trivial number agreement constructions.", "labels": [], "entities": []}, {"text": "Note that all models are trained on a regular corpus likelihood objective and do not receive any specific supervision for the syntactic tasks.", "labels": [], "entities": []}, {"text": "shows the results of our preliminary experiments.", "labels": [], "entities": []}, {"text": "The unigram baseline simply picks, for each sentence, the most frequent word form between singular or plural.", "labels": [], "entities": []}, {"text": "As an upper-bound we report the agreement accuracy obtained by a monolingual model trained on a large L2 corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9046766757965088}]}, {"text": "The effect of mixing the small Italian corpus with the large French one does not appear to be major.", "labels": [], "entities": []}, {"text": "Agreement accuracy increases slightly in the original sentences, where the model is free to rely on collocational cues, but decreases slightly in the nonce sentences, where the model must rely on pure grammatical knowledge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9372335076332092}]}, {"text": "Thus there is currently no evidence that syntactic transfer occurs in our setup.", "labels": [], "entities": [{"text": "syntactic transfer", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7944766581058502}]}, {"text": "A possible explanation is that the bilingual model has to fit the knowledge from two language systems into the same number of hidden layer parameters and this may cancel out the benefits of being exposed to a more diverse set of sentences.", "labels": [], "entities": []}, {"text": "In fact, the bilingual model achieves a considerably worse perplexity than the monolingual one (69.9 vs 55.62) on an Italian-only held-out set.", "labels": [], "entities": []}, {"text": "For comparison, \u00a8 Ostling and Tiedemann (2017) observed slightly better perplexities when mixing a small number of related languages, however their setup was considerably different (characterlevel LSTM with highly overlapping vocabulary).", "labels": [], "entities": []}, {"text": "This is work in progress.", "labels": [], "entities": []}, {"text": "We are currently looking fora bilingual LM configuration that will result in better target language perplexity and, possibly, better agreement accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9557554125785828}]}, {"text": "We also plan to extend the evaluation to other, less related, language pairs and different multilingual training techniques.", "labels": [], "entities": []}, {"text": "Finally, we plan to examine whether lexical syntactic categories (POS) are represented in a shared space among the two languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy on the Italian agreement set by the  unigram baseline, monolingual and bilingual LMs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9973013997077942}]}]}