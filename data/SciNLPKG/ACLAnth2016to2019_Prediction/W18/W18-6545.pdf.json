{"title": [{"text": "Adapting Neural Single-Document Summarization Model for Abstractive Multi-Document Summarization: A Pilot Study", "labels": [], "entities": [{"text": "Adapting Neural Single-Document Summarization", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.872848704457283}, {"text": "Abstractive Multi-Document Summarization", "start_pos": 56, "end_pos": 96, "type": "TASK", "confidence": 0.6190307140350342}]}], "abstractContent": [{"text": "Till now, neural abstractive summarization methods have achieved great success for single document summarization (SDS).", "labels": [], "entities": [{"text": "neural abstractive summarization", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6664760112762451}, {"text": "single document summarization (SDS)", "start_pos": 83, "end_pos": 118, "type": "TASK", "confidence": 0.7819332977135977}]}, {"text": "However, due to the lack of large scale multi-document summaries, such methods can be hardly applied to multi-document summarization (MDS).", "labels": [], "entities": [{"text": "multi-document summarization (MDS)", "start_pos": 104, "end_pos": 138, "type": "TASK", "confidence": 0.8296592831611633}]}, {"text": "In this paper, we investigate neural abstractive methods for MDS by adapting a state-of-the-art neural abstractive summarization model for SDS.", "labels": [], "entities": []}, {"text": "We propose an approach to extend the neural abstractive model trained on large scale SDS data to the MDS task.", "labels": [], "entities": []}, {"text": "Our approach only makes use of a small number of multi-document summaries for fine tuning.", "labels": [], "entities": []}, {"text": "Experimental results on two benchmark DUC datasets demonstrate that our approach can outperform a variety of base-line neural models.", "labels": [], "entities": [{"text": "DUC datasets", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.7723763585090637}]}], "introductionContent": [{"text": "Document summarization is a task of automatically producing a summary forgiven documents.", "labels": [], "entities": [{"text": "Document summarization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9139465093612671}]}, {"text": "Different from Single Document Summarization (SDS) which generates a summary for each given document, Multi-Document Summarization (MDS) aims to generate a summary fora set of topic-related documents.", "labels": [], "entities": [{"text": "Single Document Summarization (SDS)", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.8080355326334635}]}, {"text": "Previous approaches to document summarization can be generally categorized to extractive methods and abstractive methods.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.5447946041822433}]}, {"text": "Extractive methods produce a summary by extracting and merging sentences from the original document(s), while abstractive methods generate a summary using arbitrary words and expressions based on understanding the document(s).", "labels": [], "entities": []}, {"text": "Due to the difficulty of natural language understanding and generation, previous research on document summarization is more focused on extractive methods (.", "labels": [], "entities": [{"text": "natural language understanding and generation", "start_pos": 25, "end_pos": 70, "type": "TASK", "confidence": 0.6827130198478699}, {"text": "document summarization", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.594354659318924}]}, {"text": "However, extractive methods suffer from the inherent drawbacks of discourse incoherence and long, redundant sentences, which hampers its application in reality (.", "labels": [], "entities": []}, {"text": "Recently, with the success of sequence-to-sequence (seq2seq) models in natural language generation tasks including machine translation () and dialog systems (, abstractive summarization methods has received increasing attention.", "labels": [], "entities": [{"text": "natural language generation tasks", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.7436736673116684}, {"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7668807804584503}, {"text": "abstractive summarization", "start_pos": 160, "end_pos": 185, "type": "TASK", "confidence": 0.5459846556186676}]}, {"text": "With the resource of large-scale corpus of human summaries, it is able to train an abstractive summarization model in an end-to-end framework.", "labels": [], "entities": []}, {"text": "Neural abstractive summarization models ( have surpass the performance of extractive methods on single document summarization task with abundant training data.", "labels": [], "entities": [{"text": "Neural abstractive summarization", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5998299022515615}, {"text": "single document summarization task", "start_pos": 96, "end_pos": 130, "type": "TASK", "confidence": 0.7278580367565155}]}, {"text": "Unfortunately, the extension of seq2seq models to MDS is not straightforward.", "labels": [], "entities": []}, {"text": "Neural abstractive summarization models are usually trained on about hundreds of thousands of gold summaries, but there are usually very few human summaries available for the MDS task.", "labels": [], "entities": [{"text": "Neural abstractive summarization", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8281002640724182}]}, {"text": "More specifically, in the news domain, there is only a few hundred multi-document summaries provided by DUC and TAC conferences in total, which are largely insufficient for training neural abstractive models.", "labels": [], "entities": [{"text": "DUC and TAC conferences", "start_pos": 104, "end_pos": 127, "type": "DATASET", "confidence": 0.7150215730071068}]}, {"text": "Apart from insufficient training data, neural models for abstractive MDS also face the challenge of much more input content, and the study is still in the primary stage.", "labels": [], "entities": [{"text": "abstractive MDS", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.5339818000793457}]}, {"text": "In this study, we investigate applying seq2seq models to the MDS task.", "labels": [], "entities": [{"text": "MDS task", "start_pos": 61, "end_pos": 69, "type": "TASK", "confidence": 0.7842065095901489}]}, {"text": "We attempt various ways of extending neural abstractive summarization models pre-trained on the SDS data to the MDS task, and reveal that neural abstractive summarization models do not transfer well on a different dataset.", "labels": [], "entities": []}, {"text": "Then we study the factors which affect the transfer performance, and propose methods to adapt the pre-trained model to the MDS task.", "labels": [], "entities": [{"text": "MDS task", "start_pos": 123, "end_pos": 131, "type": "TASK", "confidence": 0.871666818857193}]}, {"text": "We also study leveraging the few MDS training data to further improve the pre-trained model.", "labels": [], "entities": []}, {"text": "We conduct experiment on the benchmark DUC datasets, and experiment results demonstrate our approach is able to achieve considerable improvement over a variety of neural baselines.", "labels": [], "entities": [{"text": "DUC datasets", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.8029019236564636}]}, {"text": "The contributions of this study are summarized as follows: \u2022 To the best of our knowledge, our work is one of the very few pioneering works to investigate adapting neural abstractive summarization models of single document summarization to the task of multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 252, "end_pos": 280, "type": "TASK", "confidence": 0.5697835385799408}]}, {"text": "\u2022 We propose a novel approach to adapt the neural model trained on the SDS data to the MDS task, and leverage the few MDS training data to further improve the pre-trained model.", "labels": [], "entities": [{"text": "MDS task", "start_pos": 87, "end_pos": 95, "type": "TASK", "confidence": 0.7558198869228363}]}, {"text": "\u2022 Evaluation results demonstrate the efficacy of our proposed approach, which outperforms a variety of neural baselines.", "labels": [], "entities": []}, {"text": "We organize the paper as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we introduce related work.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the previous neural abstractive summarization model.", "labels": [], "entities": [{"text": "neural abstractive summarization", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.6442869305610657}]}, {"text": "Then we introduce our proposed approach in Section 4.", "labels": [], "entities": []}, {"text": "Experiment results and discussion are presented in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we conclude this paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on the DUC datasets which are widely used in document summarization.", "labels": [], "entities": [{"text": "DUC datasets", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9706180095672607}, {"text": "document summarization", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.6471014022827148}]}, {"text": "We use the MDS tasks of DUC 2002 and 2004 as test sets, which contain 50 document sets and 59 document sets, respectively.", "labels": [], "entities": [{"text": "MDS tasks of DUC 2002", "start_pos": 11, "end_pos": 32, "type": "DATASET", "confidence": 0.6771995961666107}]}, {"text": "When evaluating on the DUC 2004 dataset, the datasets are used for tuning the model, and datasets are used when testing on the DUC 2002 dataset.", "labels": [], "entities": [{"text": "DUC 2004 dataset", "start_pos": 23, "end_pos": 39, "type": "DATASET", "confidence": 0.9871620138486227}, {"text": "DUC 2002 dataset", "start_pos": 127, "end_pos": 143, "type": "DATASET", "confidence": 0.985027551651001}]}, {"text": "The MDS tasks of DUC 2005-2007 are query focused summarization, but we ignore the query since these datasets are only used for training.", "labels": [], "entities": [{"text": "DUC 2005-2007", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.8771732747554779}]}, {"text": "There are on average 10 documents per set in DUC 2004 and 9.58 documents per set in DUC 2002.", "labels": [], "entities": [{"text": "DUC 2004", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9689028859138489}, {"text": "DUC 2002", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9741076231002808}]}, {"text": "For the datasets of DUC 2005-2007 we use only the top 10 documents which are most similar to the topic of a document set.", "labels": [], "entities": [{"text": "DUC 2005-2007", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.9302424490451813}]}, {"text": "ROUGE: We use ROUGE-1.5.5 (Lin and Hovy, 2003) toolkit and report the Rouge-1, Rouge-2 and Rouge-SU4 F1-scores, which has been widely adopted by DUC and TAC for automatic summary quality evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9239458441734314}, {"text": "F1-scores", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.7723565101623535}, {"text": "DUC", "start_pos": 145, "end_pos": 148, "type": "DATASET", "confidence": 0.880252480506897}, {"text": "TAC", "start_pos": 153, "end_pos": 156, "type": "DATASET", "confidence": 0.5684025287628174}]}, {"text": "It measured summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary.", "labels": [], "entities": []}, {"text": "We randomly sample 10 document sets from the DUC 2002 dataset and another 10 document sets from the DUC 2004 dataset for human evaluation.", "labels": [], "entities": [{"text": "DUC 2002 dataset", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9719654122988383}, {"text": "DUC 2004 dataset", "start_pos": 100, "end_pos": 116, "type": "DATASET", "confidence": 0.9779526789983114}]}, {"text": "Three volunteers who are fluent in English were asked to perform manual ratings on three dimensions: Coherence, Non-Redundancy (N.R. for short) and Readability.", "labels": [], "entities": []}, {"text": "The ratings are in the format of 1-5 numerical scores (not necessarily integral), with higher scores denote better quality.", "labels": [], "entities": []}, {"text": "The average results are shown in.", "labels": [], "entities": []}, {"text": "It can be observed that our system also outperforms other abstractive summarization approaches inhuman evaluation, achieving good coherence and readability.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison results with abstractive base- lines on the DUC 2002 test set.", "labels": [], "entities": [{"text": "DUC 2002 test set", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.9847456067800522}]}, {"text": " Table 2: Comparison results with abstractive base- lines on the DUC 2004 test set.", "labels": [], "entities": [{"text": "DUC 2004 test set", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.9863428622484207}]}, {"text": " Table 3: Details of model validation.", "labels": [], "entities": [{"text": "model validation", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.66764035820961}]}, {"text": " Table 4: Model validation results on DUC 2002.", "labels": [], "entities": [{"text": "DUC 2002", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9599678814411163}]}, {"text": " Table 5. As seen from", "labels": [], "entities": []}, {"text": " Table 5: Model validation results on DUC 2004.", "labels": [], "entities": [{"text": "DUC 2004", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.951526552438736}]}, {"text": " Table 6: Human evaluation results on 20 samples  from the DUC 2002 and DUC 2004 datasets.", "labels": [], "entities": [{"text": "DUC 2002 and DUC 2004 datasets", "start_pos": 59, "end_pos": 89, "type": "DATASET", "confidence": 0.9208319286505381}]}]}