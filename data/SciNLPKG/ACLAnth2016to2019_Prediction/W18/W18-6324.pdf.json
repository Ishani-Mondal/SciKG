{"title": [{"text": "Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation", "labels": [], "entities": [{"text": "Low-Resource Target Language Translation", "start_pos": 45, "end_pos": 85, "type": "TASK", "confidence": 0.6558251231908798}]}], "abstractContent": [{"text": "We work on translation from rich-resource languages to low-resource languages.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.975562334060669}]}, {"text": "The main challenges we identify are the lack of low-resource language data, effective methods for cross-lingual transfer, and the variable-binding problem that is common in neural systems.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.7572342455387115}]}, {"text": "We build a translation system that addresses these challenges using eight European language families as our test ground.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9697012305259705}]}, {"text": "Firstly, we add the source and the target family labels and study intra-family and inter-family influences for effective cross-lingual transfer.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.8051561713218689}]}, {"text": "We achieve an improvement of +9.9 in BLEU score for English-Swedish translation using eight families compared to the single-family multi-source multi-target baseline.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9811732769012451}]}, {"text": "Moreover, we find that training on two neighboring families closest to the low-resource language is often enough.", "labels": [], "entities": []}, {"text": "Secondly, we construct an ab-lation study and find that reasonably good results can be achieved even with considerably less target data.", "labels": [], "entities": []}, {"text": "Thirdly, we address the variable-binding problem by building an order-preserving named entity translation model.", "labels": [], "entities": [{"text": "order-preserving named entity translation", "start_pos": 64, "end_pos": 105, "type": "TASK", "confidence": 0.6909011453390121}]}, {"text": "We obtain 60.6% accuracy in qualitative evaluation where our translations are akin to human translations in a preliminary study.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9995706677436829}]}], "introductionContent": [{"text": "We work on translation from a rich-resource language to a low-resource language.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9724757671356201}]}, {"text": "There is usually little low-resource language data, much less parallel data available; Despite of the challenges of little data and few human experts, it has many useful applications.", "labels": [], "entities": []}, {"text": "Applications include translating water, sanitation and hygiene (WASH) guidelines to protect Indian tribal children against waterborne diseases, introducing earthquake preparedness techniques to Indonesian tribal groups living near volcanoes and delivering information to the disabled or the elderly in low-resource language communities (.", "labels": [], "entities": [{"text": "translating water, sanitation and hygiene (WASH)", "start_pos": 21, "end_pos": 69, "type": "TASK", "confidence": 0.8446212808291117}]}, {"text": "These are useful examples of translating a closed text known in advance to the lowresource language.", "labels": [], "entities": []}, {"text": "There are three main challenges.", "labels": [], "entities": []}, {"text": "Firstly, most of previous works research on individual languages instead of collective families.", "labels": [], "entities": []}, {"text": "Cross-lingual impacts and similarities are very helpful when there is little data in low-resource language.", "labels": [], "entities": []}, {"text": "Secondly, most of the multilingual Neural Machine Translation (NMT) works assume the same amount of training data for all languages.", "labels": [], "entities": [{"text": "multilingual Neural Machine Translation (NMT)", "start_pos": 22, "end_pos": 67, "type": "TASK", "confidence": 0.7858938149043492}]}, {"text": "In the low-resource case, it is important to exploit low or partial data in low-resource language to produce high quality translation.", "labels": [], "entities": []}, {"text": "The third issue is the variable-binding problem that is common in neural systems, where \"John calls Mary\" is treated the same way as \"Mary calls John\".", "labels": [], "entities": []}, {"text": "It is more challenging when both \"Mary\" and \"John\" are rare words.", "labels": [], "entities": []}, {"text": "Solving the binding problem is crucial because the mistakes in named entities change the meaning of the translation.", "labels": [], "entities": []}, {"text": "It is especially challenging in the lowresource case because many words are rare words.", "labels": [], "entities": []}, {"text": "Our contribution in addressing these issues is three-fold, extending from multi-source multitarget attentional NMT.", "labels": [], "entities": []}, {"text": "Firstly, to examine intrafamily and inter-family influences, we add source and target language family labels in training.", "labels": [], "entities": []}, {"text": "Training on multiple families improves BLEU score significantly; moreover, we find training on two neighboring families closest to the lowresource language gives reasonably good BLEU scores, and we define neighboring families closely in Section 3.2.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9714493751525879}, {"text": "BLEU", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.9990683197975159}]}, {"text": "Secondly, we conduct an ablation study to explore how generalization changes with different amounts of data and find that we only need a small amount of low-resource language data to produce reasonably good BLEU scores.", "labels": [], "entities": [{"text": "generalization", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.9625345468521118}, {"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9975625276565552}]}, {"text": "We use full data except for the ablation study.", "labels": [], "entities": []}, {"text": "Finally, to address the variable-binding problem, we build a parallel lexicon table across twenty-three European languages and devise a novel method of order-preserving named entity translation method.", "labels": [], "entities": [{"text": "order-preserving named entity translation", "start_pos": 152, "end_pos": 193, "type": "TASK", "confidence": 0.6712906807661057}]}, {"text": "Our method works in translation of any text with a fixed set of named entities known in advance.", "labels": [], "entities": [{"text": "translation of", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.9033676981925964}]}, {"text": "Our goal is to minimize manual labor, but not to fully automate to ensure the correct translation of named entities and their ordering.", "labels": [], "entities": [{"text": "translation of named entities and their ordering", "start_pos": 86, "end_pos": 134, "type": "TASK", "confidence": 0.7044226569788796}]}, {"text": "In this paper, we begin with introduction and related work in Section 1 and 2.", "labels": [], "entities": []}, {"text": "We introduce our methods in addressing three issues that are important for translation into low-resource language in Section 3.2, as proposed extensions to our baseline in Section 3.1.", "labels": [], "entities": [{"text": "translation", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.9766449928283691}]}, {"text": "Finally, we present our results in Section 4 and conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We choose the Bible corpus as a test ground for our proposed extensions because the Bible is the most translated text that exists and is freely accessible.", "labels": [], "entities": [{"text": "Bible corpus", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.88446444272995}]}, {"text": "Though it has limitations, it does not have copyright issues like most of literary works that are translated into many languages do.", "labels": [], "entities": []}, {"text": "There are many research works done using the Bible).", "labels": [], "entities": []}, {"text": "Unlike many past research works where only New Testament is used, we use both Old Testament and New Testament in our Bible corpus.", "labels": [], "entities": []}, {"text": "We align all Bible verses across different languages.", "labels": [], "entities": []}, {"text": "We train our proposed model on twenty-three European languages across eight families on a parallel Bible corpus.", "labels": [], "entities": [{"text": "Bible corpus", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.8582336008548737}]}, {"text": "For our purpose, we treat Swedish as our hypothetical low-resource target language, English as our rich-resource language in the single-source single-target case and all other Germanic languages as our rich-resource languages in the multi-source multi-target case.", "labels": [], "entities": []}, {"text": "Firstly, we present our data and training parameters.", "labels": [], "entities": []}, {"text": "Secondly, we add family tags in different configurations of language families showing intra-family and inter-family effects.", "labels": [], "entities": []}, {"text": "Thirdly, we conduct an ablation study and plot the generalization curves by varying the amount of training data in Swedish, and we show that training on one fifth of the data give reasonably good BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 196, "end_pos": 200, "type": "METRIC", "confidence": 0.9994248151779175}]}, {"text": "Lastly, we devise an order-preserving lexicon translation method by building a parallel lexicon table across twenty-three European languages and tagging named entities in order.", "labels": [], "entities": [{"text": "order-preserving lexicon translation", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.6515597601731619}]}], "tableCaptions": [{"text": " Table 2: (Baseline model) Germanic family multi-source", "labels": [], "entities": []}, {"text": " Table 3: Inter-family and intra-family effects on BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.988109827041626}]}, {"text": " Table 4: Effects of adding family labels on BLEU scores with", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9963938593864441}]}, {"text": " Table 3.  GSl: Germanic, Slavic family with family labels.  GRl: Germanic, Romance family with family labels.  3Fl: Germanic, Slavic, Romance family with family labels.  8Fl: all 8 European families together with family labels", "labels": [], "entities": []}, {"text": " Table 5: Ablation Study on Germanic Family. #w is the word count of unique sentences in Swedish data.", "labels": [], "entities": [{"text": "Ablation Study on Germanic Family", "start_pos": 10, "end_pos": 43, "type": "DATASET", "confidence": 0.6170227944850921}, {"text": "Swedish data", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.8467971384525299}]}, {"text": " Table 6: A few examples from the parallel lexicon table.", "labels": [], "entities": []}, {"text": " Table 7: Summary of order-preserving lexicon translation.", "labels": [], "entities": [{"text": "order-preserving lexicon translation", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.5771992107232412}]}]}