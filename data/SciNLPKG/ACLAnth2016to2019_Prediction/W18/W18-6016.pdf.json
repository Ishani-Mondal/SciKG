{"title": [{"text": "The Hebrew Universal Dependency Treebank: Past, Present and Future", "labels": [], "entities": [{"text": "Hebrew Universal Dependency Treebank", "start_pos": 4, "end_pos": 40, "type": "DATASET", "confidence": 0.8364133983850479}]}], "abstractContent": [{"text": "The Hebrew treebank (HTB), consisting of 6221 morpho-syntactically annotated newspaper sentences, has been the only resource for training and validating statistical parsers and taggers for Hebrew, for almost two decades now.", "labels": [], "entities": [{"text": "Hebrew treebank (HTB)", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.8295310497283935}, {"text": "validating statistical parsers and taggers", "start_pos": 142, "end_pos": 184, "type": "TASK", "confidence": 0.6621970236301422}]}, {"text": "During these decades, the HTB has gone through a trajectory of automatic and semi-automatic conversions, until arriving at its UDv2 form.", "labels": [], "entities": [{"text": "HTB", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.7483471632003784}]}, {"text": "In this work we manually validate the UDv2 version of the HTB, and, according to our findings, we apply scheme changes that bring the UD HTB to the same theoretical grounds as the rest of UD.", "labels": [], "entities": [{"text": "UD HTB", "start_pos": 134, "end_pos": 140, "type": "DATASET", "confidence": 0.7542855441570282}]}, {"text": "Our experimental parsing results with UDv2New confirm that improving the coherence and internal consistency of the UD HTB indeed leads to improved parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9678699374198914}, {"text": "UDv2New", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.8669834733009338}, {"text": "UD HTB", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.7965599596500397}, {"text": "parsing", "start_pos": 147, "end_pos": 154, "type": "TASK", "confidence": 0.9619287252426147}]}, {"text": "At the same time, our analysis demonstrates that there is more to be done at the point of intersection of UD with other linguistic processing layers, in particular, at the points where UD interfaces external morphological and lexical resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Hebrew Treebank (HTB), initially introduced by, is the first, and so far only, gold standard for morphologically and syntactically annotated sentences in Modern Hebrew.", "labels": [], "entities": [{"text": "Hebrew Treebank (HTB)", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.8882108211517334}]}, {"text": "It was created with the main goal in mind to enable the development of statistical models for morphological and syntactic parsing for Hebrew, but also to facilitate linguistic investigations into the structure and distribution of linguistic Semitic phenomena.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7483212947845459}]}, {"text": "The pilot version of has been minimal -it consisted of 500 sentences, morphologically and syntactically annotated by hand.", "labels": [], "entities": []}, {"text": "This modest start, however, defined linguistic conventions and annotation principles that would continue to affect many treebank versions derived from the HTB for many years, including the universal dependencies (UD) HTB version.", "labels": [], "entities": []}, {"text": "During these two decades, the HTB has expanded from 500 to 6221 sentences and changed several forms.", "labels": [], "entities": [{"text": "HTB", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.4494176506996155}]}, {"text": "The different versions of the treebank reflect different theories and formal representation types, that in turn reflect different, and sometimes contradictory, linguistic annotation principles.", "labels": [], "entities": []}, {"text": "The reasons for these differences were sometimes practical, e.g., anew version was derived to answer an emerging technological need, and sometimes socio-academic, e.g., because different teams adopted different linguistic theories as their underlying annotation principles.", "labels": [], "entities": []}, {"text": "The HTB thus enabled the development of many statistical morphological and syntactic processing models, but these models were trained on vastly different versions of the treebank, obeying different theories and annotation schemes, which then rendered the reported results mostly non-comparable.", "labels": [], "entities": [{"text": "HTB", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8451749086380005}]}, {"text": "Hebrew dependency parsing presents an acute version of this syndrome.", "labels": [], "entities": [{"text": "Hebrew dependency parsing", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5921589732170105}]}, {"text": "Studies such as Goldberg and Elhadad (2011),,, as well as the SPMRL shared tasks (), all present attachment scores on Hebrew dependency parsing.", "labels": [], "entities": [{"text": "SPMRL shared tasks", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.5295820236206055}, {"text": "Hebrew dependency parsing", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.6983818908532461}]}, {"text": "But for reporting these scores they use HTB versions that reflect distinct schemes, sometime reporting different metrics, which makes the numerical comparison between the respective results meaningless).", "labels": [], "entities": []}, {"text": "This is why the UD initiative comes as a blessing, not only for the cross-linguistic parsing community but also for the Hebrew NLP community -by presenting a unique opportunity to standardize the resources and metrics used for Hebrew parsing.", "labels": [], "entities": [{"text": "cross-linguistic parsing", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6786651164293289}, {"text": "Hebrew parsing", "start_pos": 227, "end_pos": 241, "type": "TASK", "confidence": 0.575753465294838}]}, {"text": "Ideally, the current UDv2 version would make for such a standard Hebrew resource.", "labels": [], "entities": []}, {"text": "Unfortunately though, many of the conversion processes since to the present UDv2 have been automatic or semi-automatic, with no point of systematic qualitative validation.", "labels": [], "entities": [{"text": "UDv2", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.8628566861152649}]}, {"text": "This resulted in odd, and sometime plain wrong, dependency structures, with respect to the UD scheme.", "labels": [], "entities": [{"text": "UD scheme", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.8161171674728394}]}, {"text": "In this work we take the opportunity to validate the UDv2 HTB, by manually going through the published trees, identifying systematic errors or annotation inconsistencies, and locating cases where the annotated structures contradict the UD guidelines (or spirit).", "labels": [], "entities": [{"text": "UDv2 HTB", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.918474942445755}]}, {"text": "We identified and corrected three main points of failure in the UD HTB: (i) the classification of argument types, deriving from the classification in the original HTB (ii) a mix-up of morphological and syntactic properties, where morphological features serve as syntactic sub-relations and vice versa, and (iii) a mix up of language-specific versus universal phenomena, where label sub-typing is exploited to indicate a supposedly language-specific phenomenon, which in fact has a designated universal label elsewhere.", "labels": [], "entities": [{"text": "UD HTB", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.8818191885948181}]}, {"text": "Based on these corrections, we present a revised version of the HTB that we call UDv2New.", "labels": [], "entities": [{"text": "HTB", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.9403854608535767}, {"text": "UDv2New", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9296444654464722}]}, {"text": "We use UDv2 and UDv2New to train a morphosyntactic parser (More et al., In Press) and provide baseline results on Hebrew UD parsing, in both ideal and realistic scenarios.", "labels": [], "entities": [{"text": "Hebrew UD parsing", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.58232910434405}]}, {"text": "Comparing our Hebrew parsing results on UDv2 and UDv2New, we verify that the improvement of linguistic coherence and annotation consistency has also led to improved parsing performance.", "labels": [], "entities": [{"text": "Hebrew parsing", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.6192043125629425}]}, {"text": "Lessons learned from our empirical analysis concern the systematic organization of natural language grammar in UD, and in particular (i) the need to standardize the interface of UD treebanks to external morphological and lexical resources, and (ii) the need to organize the form-function mapping in a language-specific vs. family-specific vs. strictly-universal relations taxonomy, within and across treebanks.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the trajectory of the HTB from its inception to UDv2.", "labels": [], "entities": [{"text": "HTB", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.7784777879714966}, {"text": "UDv2", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9523079991340637}]}, {"text": "In Section 3 we present our validation process and the scheme changes we applied.", "labels": [], "entities": [{"text": "validation", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.9465884566307068}]}, {"text": "In Section 4 we present raw-to-dependencies Hebrew parsing results and in Section 5 we share our future plans and lessons learned.", "labels": [], "entities": [{"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.9744018316268921}]}, {"text": "Finally, in Section 6 we conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "Goal: We wish to examine the empirical impact of our effort to correct the treebank and retain linguistic (as well as cross-treebank) coherence in its annotation scheme.", "labels": [], "entities": []}, {"text": "Indeed, ease of parsing should not be the indication for selecting one scheme over another, but the hypothesis is that, within one and the same set of guidelines, aversion that presents better coherence and consistency will also be more suitable for statistical training and will yield better results.", "labels": [], "entities": [{"text": "parsing", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.9711918234825134}, {"text": "consistency", "start_pos": 207, "end_pos": 218, "type": "METRIC", "confidence": 0.9868025183677673}]}, {"text": "Settings: To gauge the effect of our revision we conducted two sets of experiments: one with the HTB UDv2 version used in the recent shared task, and another our revised UDv2New.", "labels": [], "entities": [{"text": "HTB UDv2 version", "start_pos": 97, "end_pos": 113, "type": "DATASET", "confidence": 0.8845702807108561}, {"text": "UDv2New", "start_pos": 170, "end_pos": 177, "type": "DATASET", "confidence": 0.8673444986343384}]}, {"text": "We use the syntactic evaluation script provided by the CoNLL shared task 2018.", "labels": [], "entities": [{"text": "CoNLL shared task 2018", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.8772402852773666}]}, {"text": "We train on the portion defined as train set and report results on the dev set.", "labels": [], "entities": []}, {"text": "For training and parsing we used yap, 13 a transitionbased morphosyntactic parser written in go, which includes a morphological analyzer, a morphological disambiguator, and syntactic parser.", "labels": [], "entities": []}, {"text": "In previous work yap was shown to obtain state of the art results on Hebrew parsing using the SPMRL version of the treebank (More et al., In Press).", "labels": [], "entities": [{"text": "Hebrew parsing", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.6863473057746887}, {"text": "In Press)", "start_pos": 138, "end_pos": 147, "type": "DATASET", "confidence": 0.9270498355229696}]}, {"text": "Here we report its performance on the UD HTB.", "labels": [], "entities": [{"text": "UD HTB", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.9464475810527802}]}, {"text": "Scenarios: Because of its rich morphology and orthographic convention to attach or fuse adpositions and pronominals onto open-class categories, there is severe ambiguity in the morphological analysis of the Hebrew input tokens.", "labels": [], "entities": []}, {"text": "This is further magnified by the lack of diacritics in Hebrew written texts.", "labels": [], "entities": []}, {"text": "Hence, it is unknown upfront how many morphemes (in the HTB terminology) or syntactic words (in the UD terminology) are in the space-delimited tokens.", "labels": [], "entities": []}, {"text": "We examine two kinds of scenarios: \u2022 ideal: assuming gold morphological analysis and disambiguation given by an oracle.", "labels": [], "entities": []}, {"text": "\u2022 realistic: assuming automatically predicted morphological analysis and disambiguation.", "labels": [], "entities": []}, {"text": "We use yap for predicting morphological analysis (MA) and morphological disambiguation, and we contrast the use of a data-driven lexicon baselinelex with an external broad-coverage lexicon HebLex.", "labels": [], "entities": [{"text": "predicting morphological analysis (MA)", "start_pos": 15, "end_pos": 53, "type": "TASK", "confidence": 0.9204133749008179}, {"text": "morphological disambiguation", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.7254655510187149}, {"text": "HebLex", "start_pos": 189, "end_pos": 195, "type": "DATASET", "confidence": 0.8843449354171753}]}, {"text": "To gauge the effect of the lexical coverage of the morphological resource, we contrast each variant with an infused scenario, where the correct analysis is injected into the lattice.", "labels": [], "entities": []}, {"text": "Note that the input in the infused cases is still high as there are many MA alternatives.", "labels": [], "entities": [{"text": "MA", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.8795106410980225}]}, {"text": "However, the correct morphological disambiguation is guaranteed to be one of the morphological MA provided to the system as input.", "labels": [], "entities": []}, {"text": "Results: shows the parsing results in an ideal scenario, assuming gold morphology.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9782936573028564}]}, {"text": "Here we see that there is a consistent improvement for all metrics.", "labels": [], "entities": []}, {"text": "This supports our conjecture that a more consistent and coherent annotation of the treebank will benefit parsing, and it corroborates a wider conjecture, that, when it comes to supervised learning, the quality of the annotated data is as important as the learning algorithm (and maybe more important).", "labels": [], "entities": [{"text": "parsing", "start_pos": 105, "end_pos": 112, "type": "TASK", "confidence": 0.9660855531692505}]}, {"text": "shows the parsing results in realistic scenarios, where we assume automatically predicted morphological analysis and disambiguation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9674860239028931}]}, {"text": "As expected, the results substantially drop relative to the ideal scenario.", "labels": [], "entities": []}, {"text": "Also expected is the result that assuming an external broad-coverage lexicon substantially improves the results relative to a data-driven lexicon learned from the treebank.", "labels": [], "entities": []}, {"text": "The result that seems less expected here is that, as opposed to the ideal scenario, we see no improvement in the results of UDv2New relative to UDv2.", "labels": [], "entities": [{"text": "UDv2New", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.8063177466392517}, {"text": "UDv2", "start_pos": 144, "end_pos": 148, "type": "DATASET", "confidence": 0.9038739204406738}]}, {"text": "For some of the metrics the results slightly drop.", "labels": [], "entities": []}, {"text": "This drop could be either due to parser errors, or due to the lack of lexical coverage of the lexicon with respect to our revised UDv2New scheme.", "labels": [], "entities": [{"text": "UDv2New scheme", "start_pos": 130, "end_pos": 144, "type": "DATASET", "confidence": 0.9301720559597015}]}, {"text": "To test this, we execute an infused scenario where the morphological analysis lattices are guaranteed to also include the correct analysis.", "labels": [], "entities": []}, {"text": "Here we see a substantial improvement for both types of lexica, on all the different metrics, for the UDv2New version.", "labels": [], "entities": [{"text": "UDv2New", "start_pos": 102, "end_pos": 109, "type": "DATASET", "confidence": 0.8885825276374817}]}, {"text": "This result suggests that the drop has indeed been due to the insufficient lexical coverage of the resources, or due to mismatches between the lexicon and the new scheme.", "labels": [], "entities": []}, {"text": "As far as the statistical components for morphological and syntactic analysis and disambiguation go, the revised version helps the parser obtain better disambiguation, inline of our results in the gold experiments.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7097085267305374}]}], "tableCaptions": [{"text": " Table 2: Parsing Results of the HTB dev set for UDv2  vs UDv2New, in an ideal parsing scenario assuming  GOLD morphology.", "labels": [], "entities": [{"text": "HTB dev set", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.7665165066719055}]}, {"text": " Table 3: Parsing Results of the HTB dev set for UDv2  vs UDv2New, in a realistic parsing scenario assuming  PREDICTED morphology. We compare a data-driven  baseline lexicon with an external lexicon, heblex, and  we contrast uninfused or infused setting for both", "labels": [], "entities": []}]}