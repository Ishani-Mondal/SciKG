{"title": [{"text": "Uni-DUE Student Team: Tackling fact checking through decomposable attention neural network", "labels": [], "entities": [{"text": "Tackling fact checking", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.8825069665908813}]}], "abstractContent": [{"text": "In this paper we present our system for the FEVER Challenge.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.8338901400566101}]}, {"text": "The task of this challenge is to verify claims by extracting information from Wikipedia.", "labels": [], "entities": []}, {"text": "Our system has two parts.", "labels": [], "entities": []}, {"text": "In the first part it performs a search for candidate sentences by treating the claims as query.", "labels": [], "entities": []}, {"text": "In the second part it filters out noise from these candidates and uses the remaining ones to decide whether they support or refute or entail not enough information to verify the claim.", "labels": [], "entities": []}, {"text": "We show that this system achieves a FEVER score of 0.3927 on the FEVER shared task development data set which is a 25.5% improvement over the baseline score.", "labels": [], "entities": [{"text": "FEVER score", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9861001670360565}, {"text": "FEVER shared task development data set", "start_pos": 65, "end_pos": 103, "type": "DATASET", "confidence": 0.7356883535782496}]}], "introductionContent": [{"text": "In this paper we present our system for the FEVER Challenge . The FEVER Challenge is a shared task on fact extraction and claim verification.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9034637212753296}, {"text": "FEVER", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.862055778503418}, {"text": "fact extraction", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.776097983121872}, {"text": "claim verification", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.694623589515686}]}, {"text": "Initially created an annotated corpus of 185, 445 claims and proposed a baseline system to predict the correct labels as well as the pieces of evidence for the claims.", "labels": [], "entities": []}, {"text": "Our system consist of two parts.", "labels": [], "entities": []}, {"text": "In the first part we retrieve sentences that are relevant to a claim.", "labels": [], "entities": []}, {"text": "The claim is used as query and is submitted to Lucene search API.", "labels": [], "entities": [{"text": "Lucene search API", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9495901465415955}]}, {"text": "The sentences found are candidates for pieces of evidence for the claim.", "labels": [], "entities": []}, {"text": "Next in the second part we run a modified version of the Decomposable Attention network ( to predict the textual entailment between a claim and the candidate sentences found through searching but also between claim and all candidate sentences merged into one long text.", "labels": [], "entities": []}, {"text": "This step gives us entailment probabilities.", "labels": [], "entities": []}, {"text": "We also use a point system to filter out some noise (irrelevant sentences).", "labels": [], "entities": []}, {"text": "Based on the remaining candidates we perform label prediction, i.e. whether the claim is http://fever.ai supported, refuted or there is not enough evidence.", "labels": [], "entities": [{"text": "label prediction", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7405027747154236}]}, {"text": "Our system achieves a FEVER score of 0.3927 on the FEVER shared task development data set which is a 25.5% improvement over the baseline score.", "labels": [], "entities": [{"text": "FEVER score", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9868556559085846}, {"text": "FEVER shared task development data set", "start_pos": 51, "end_pos": 89, "type": "DATASET", "confidence": 0.7459597736597061}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Confusion matrix of the full system prediction.  Columns are predictions and rows the true labels.", "labels": [], "entities": []}, {"text": " Table 6: Contribution of each feature. Label refers to  the label accuracy, while Recall refers to the evidence  recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.8606633543968201}, {"text": "Recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9946693778038025}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9473640322685242}]}]}