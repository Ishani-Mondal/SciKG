{"title": [{"text": "DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications", "labels": [], "entities": [{"text": "DuReader: a Chinese Machine Reading Comprehension Dataset", "start_pos": 0, "end_pos": 57, "type": "DATASET", "confidence": 0.5985979475080967}]}], "abstractContent": [{"text": "This paper introduces DuReader, anew large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real-world MRC.", "labels": [], "entities": [{"text": "Chinese machine reading comprehension (MRC)", "start_pos": 62, "end_pos": 105, "type": "TASK", "confidence": 0.6380420710359301}]}, {"text": "DuReader has three advantages over previous MRC datasets: (1) data sources: questions and documents are based on Baidu Search and Baidu Zhi-dao 1 ; answers are manually generated.", "labels": [], "entities": [{"text": "DuReader", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9212638139724731}, {"text": "MRC datasets", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.8899668753147125}, {"text": "Baidu Search", "start_pos": 113, "end_pos": 125, "type": "DATASET", "confidence": 0.9232321977615356}]}, {"text": "(2) question types: it provides rich annotations for more question types, especially yes-no and opinion questions, that leaves more opportunity for the research community.", "labels": [], "entities": []}, {"text": "(3) scale: it contains 200K questions, 420K answers and 1M documents; it is the largest Chinese MRC dataset so far.", "labels": [], "entities": [{"text": "scale", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9720497727394104}, {"text": "Chinese MRC dataset", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.7441907326380411}]}, {"text": "Experiments show that human performance is well above current state-of-the-art baseline systems, leaving plenty of room for the community to make improvements.", "labels": [], "entities": []}, {"text": "To help the community make these improvements, both DuReader 2 and baseline systems 3 have been posted online.", "labels": [], "entities": []}, {"text": "We also organize a shared competition to encourage the exploration of more models.", "labels": [], "entities": []}, {"text": "Since the release of the task, there are significant improvements over the baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of machine reading comprehension (MRC) aims to empower machines to answer questions after reading articles (Rajpurkar et al., Lang #Que.", "labels": [], "entities": [{"text": "machine reading comprehension (MRC)", "start_pos": 12, "end_pos": 47, "type": "TASK", "confidence": 0.8423835138479868}]}, {"text": "ventory of questions than previous datasets.", "labels": [], "entities": []}, {"text": "Each question was manually annotated as either Entity, Description or YesNo and one of Fact or Opinion.", "labels": [], "entities": []}, {"text": "In particular, it annotates yes-no and opinion questions that take a large proportion in real user's questions.", "labels": [], "entities": []}, {"text": "Prior work has largely emphasized facts, but DuReader are full of opinions as well as facts.", "labels": [], "entities": []}, {"text": "Much of the work on question answering involves span selection, methods that answer questions by returning a single substring extracted from a single document.", "labels": [], "entities": [{"text": "question answering", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.8689420521259308}, {"text": "span selection", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.750763863325119}]}, {"text": "Span selection may work well for factoids (entities), but it is less appropriate for yes-no questions and opinion questions (especially when the answer involves a summary computed over several different documents).", "labels": [], "entities": [{"text": "Span selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8810314834117889}]}, {"text": "document sources: DuReader collects documents from the search results of Baidu Search as well as Baidu Zhidao.", "labels": [], "entities": [{"text": "Baidu Search", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9253891706466675}, {"text": "Baidu Zhidao", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9449340999126434}]}, {"text": "All the content in Baidu Zhidao is generated by users, making it different from the common web pages.", "labels": [], "entities": [{"text": "Baidu Zhidao", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.876884788274765}]}, {"text": "It is interesting to see if solutions designed for one scenario (search) transfer easily to another scenario (question answering community).", "labels": [], "entities": [{"text": "question answering community", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.7511252562204996}]}, {"text": "Additionally, previous work provides only a single paragraph () or a few passages) to extractor generate answers, while DuReader provides multiple full documents (that contains a lot of paragraphs or passages) for each question to generate answers.", "labels": [], "entities": [{"text": "DuReader", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.9288272857666016}]}, {"text": "This will raise paragraph selection (i.e. select the paragraphs likely containing answers) an important challenge as shown in Section 4.", "labels": [], "entities": [{"text": "paragraph selection", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.825379490852356}]}, {"text": "data scale: The first release of DuReader contains 200K questions, 1M documents and more than 420K human-summarized answers.", "labels": [], "entities": [{"text": "DuReader", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9472838640213013}]}, {"text": "To the best of our knowledge, DuReader is the largest Chinese MRC dataset so far.", "labels": [], "entities": [{"text": "DuReader", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.9673833847045898}, {"text": "Chinese MRC dataset", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.7520674268404642}]}], "datasetContent": [{"text": "In this section, we implement and evaluate the baseline systems with two state-of-the-art models.", "labels": [], "entities": []}, {"text": "Furthermore, with the rich annotations in our dataset, we conduct comprehensive evaluations from different perspectives.", "labels": [], "entities": []}, {"text": "Considering the characteristics of YesNo questions, we found that it's not suitable to directly use BLEU or Rouge to evaluate the performance on these questions, because these metrics could not reflect the agreement between answers.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9973282814025879}]}, {"text": "For example, two contradictory answers like \"You can do it\" and \"You can't do it\" get high agreement scores with these metrics.", "labels": [], "entities": []}, {"text": "A natural idea is to formulate this subtask as a classification problem.", "labels": [], "entities": []}, {"text": "However, as described in Section 3, multiple different judgments could be made based on the evidence collected from different documents, especially when the question is of opinion type.", "labels": [], "entities": []}, {"text": "In real-world settings, we don't want a smart model to give an arbitrary answer for such questions as Yes or No. To tackle this, we propose a novel opinionaware evaluation method that requires the evaluated system to not only output an answer in natural language, but also give it an opinion label.", "labels": [], "entities": []}, {"text": "We also have the annotators provide the opinion label for each answer they generated.", "labels": [], "entities": []}, {"text": "In such cases, every answer is paired with an opinion label (Yes, No or Depend) so that we can categorize the answers by their labels.", "labels": [], "entities": []}, {"text": "Finally, the predicted answers are evaluated via Blue or Rouge against only the reference answers with the same opinion label.", "labels": [], "entities": []}, {"text": "By using this opinion-aware evaluation method, a model that can predict a good answer in natural language and give it an opinion label correctly will get a higher score.", "labels": [], "entities": []}, {"text": "In order to classify the answers into different opinion polarities, we add a classifier.", "labels": [], "entities": []}, {"text": "We slightly change the Match-LSTM model, in which the final pointer network layer is replaced with a fully connected layer.", "labels": [], "entities": [{"text": "Match-LSTM", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9420750141143799}]}, {"text": "This classifier is trained with the gold answers and their corresponding opinion labels.", "labels": [], "entities": []}, {"text": "We compare a reading comprehension system equipped with such an opinion classifier with a pure reading comprehension system without it, and the results are demonstrated in.", "labels": [], "entities": []}, {"text": "We can see that doing opinion classification does help under our evaluation method.", "labels": [], "entities": [{"text": "opinion classification", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7262684106826782}]}, {"text": "Also, classifying the answers correctly is much harder for the questions of opinion type than for those of fact type.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of the six types of questions in Chinese (with glosses in English). Previous datasets  have focused on fact-entity and fact-description, though all six types are common in search logs.", "labels": [], "entities": []}, {"text": " Table 3: Pilot Study found that all six types  of question queries are common in search logs.  Previous MRC datasets have emphasized span- selection methods. Such methods are appropriate  for fact-entity and fact-description. Opinions and  yes-no leave big opportunities (about 33.8% and  15.6% of the sample, respectively).", "labels": [], "entities": [{"text": "MRC datasets", "start_pos": 105, "end_pos": 117, "type": "DATASET", "confidence": 0.8158764541149139}]}, {"text": " Table 4: The distribution of question types in  DuReader is similar to (but different from) the Pi- lot Study", "labels": [], "entities": [{"text": "DuReader", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9493844509124756}]}, {"text": " Table 5: Examples from DuReader. Annotations for these questions include both the answers, as well as  supporting sentences.", "labels": [], "entities": [{"text": "DuReader", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.9622867703437805}]}, {"text": " Table 6: Performance of typical MRC systems on the DuReader.", "labels": [], "entities": [{"text": "MRC", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9792819023132324}, {"text": "DuReader", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9880304932594299}]}, {"text": " Table 7: Model performance with gold paragraph.  The use of gold paragraphs could significantly  boosts the overall performance.", "labels": [], "entities": []}, {"text": " Table 8: Performance on various question types. Current MRC models achieve impressive improvements  compared with the selected paragraph baseline. However, there is a large gap between these models and  human.", "labels": [], "entities": []}, {"text": " Table 9: Performance of opinion-aware model on YesNo questions.", "labels": [], "entities": [{"text": "YesNo questions", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.8800588548183441}]}]}