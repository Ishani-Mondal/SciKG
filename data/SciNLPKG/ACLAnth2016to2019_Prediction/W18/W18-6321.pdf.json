{"title": [{"text": "Simple Fusion: Return of the Language Model", "labels": [], "entities": [{"text": "Simple Fusion", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6768338531255722}]}], "abstractContent": [{"text": "Neural Machine Translation (NMT) typically leverages monolingual data in training through backtranslation.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8476981619993845}]}, {"text": "We investigate an alternative simple method to use monolingual data for NMT training: We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 72, "end_pos": 84, "type": "TASK", "confidence": 0.9265361130237579}]}, {"text": "To achieve that, we train the translation model to predict the residual probability of the training data added to the prediction of the LM.", "labels": [], "entities": []}, {"text": "This enables the TM to focus its capacity on modeling the source sentence since it can rely on the LM for fluency.", "labels": [], "entities": []}, {"text": "We show that our method outperforms previous approaches to integrate LMs into NMT while the architecture is simpler as it does not require gating networks to balance TM and LM.", "labels": [], "entities": []}, {"text": "We observe gains of between +0.24 and +2.36 BLEU on all four test sets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top of ensembles without LM.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9987120628356934}]}, {"text": "We compare our method with alternative ways to utilize monolingual data such as backtranslation, shallow fusion, and cold fusion.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation (MT) relies on parallel training data, which is difficult to acquire.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9133395791053772}]}, {"text": "In contrast, monolingual data is abundant for most languages and domains.", "labels": [], "entities": []}, {"text": "Traditional statistical machine translation (SMT) effectively leverages monolingual data using language models (LMs) (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT", "start_pos": 12, "end_pos": 48, "type": "TASK", "confidence": 0.7448354363441467}]}, {"text": "The combination of LM and TM in SMT can be traced back to the noisy-channel model which applies the Bayes rule to decompose a translation system (: where x = (x 1 , . .", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9872065782546997}]}, {"text": ", x m ) is the source sentence, y = (y 1 , . .", "labels": [], "entities": []}, {"text": ", y n ) is the target sentence, and PT M (\u00b7) and P LM (\u00b7) are translation model and language model probabilities.", "labels": [], "entities": [{"text": "PT M (\u00b7)", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9613725741704305}]}, {"text": "In contrast, NMT) uses a discriminative model and learns the distribution P (y|x) directly end-to-end.", "labels": [], "entities": []}, {"text": "Therefore, the vanilla training regimen for NMT is not amenable to integrating an LM or monoglingual data in a straightforward manner.", "labels": [], "entities": []}, {"text": "An early attempt to use LMs for NMT, also known as shallow fusion, combines LM and NMT scores at inference time in a log-linear model.", "labels": [], "entities": [{"text": "shallow fusion", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.6708913594484329}]}, {"text": "In contrast, we integrate the LM scores during NMT training.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.8886647820472717}]}, {"text": "Our training procedure first trains an LM on a large monolingual corpus.", "labels": [], "entities": []}, {"text": "We then hold the LM fixed and train the NMT system to optimize the combined score of LM and NMT on the parallel training set.", "labels": [], "entities": []}, {"text": "This allows the NMT model to focus on modeling the source sentence, while the LM handles the generation based on the targetside history.", "labels": [], "entities": []}, {"text": "explored a similar idea for speech recognition using a gating network for controlling the relative contribution of the LM.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8359779119491577}]}, {"text": "We show that our simpler architecture without an explicit control mechanism is effective for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8103581070899963}]}, {"text": "We observe gains of up to more than 2 BLEU points from adding the LM to TM training.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9990164041519165}]}, {"text": "We also show that our method can be combined with backtranslation), yielding further gains over systems without LM.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on a variety of publicly available and proprietary data sets.", "labels": [], "entities": []}, {"text": "For our Turkish-English (tr-en), English-Turkish (entr), and Estonian-English (et-en) experiments we use all available parallel data from the WMT18 evaluation campaign to train the translation models.", "labels": [], "entities": [{"text": "WMT18 evaluation campaign", "start_pos": 142, "end_pos": 167, "type": "DATASET", "confidence": 0.8140491048494974}]}, {"text": "Our language models are trained on News Crawl 2017.", "labels": [], "entities": [{"text": "News Crawl 2017", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9749357501665751}]}, {"text": "We use news-test2017 as development (\"dev\") set and news-test2018 as test set.", "labels": [], "entities": []}, {"text": "Additionally, we collected our own proprietary corpus of public posts on Facebook.", "labels": [], "entities": []}, {"text": "We refer to it as 'INTERNAL' data set.", "labels": [], "entities": [{"text": "INTERNAL' data set", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.8331849128007889}]}, {"text": "This corpus consists of monolingual English in-domain sentences and parallel data in Xhosa-English.", "labels": [], "entities": []}, {"text": "Training set sizes are summarized in.", "labels": [], "entities": []}, {"text": "Our preprocessing consists of lower-casing, tokenization, and subword-segmentation using joint) with dot-product attention () trained with our PyTorch Translate library.", "labels": [], "entities": [{"text": "PyTorch Translate library", "start_pos": 143, "end_pos": 168, "type": "DATASET", "confidence": 0.8412744402885437}]}, {"text": "3 Both decoder and encoder consist of two 512-dimensional LSTM layers and 256-dimensional embeddings.", "labels": [], "entities": []}, {"text": "The first encoder layer is bidirectional, the second one runs from right to left.", "labels": [], "entities": []}, {"text": "Our training and architecture hyperparameters are summarized in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.9749121963977814}]}, {"text": "3. Our LSTM-based LMs have the same size and architecture as the decoder networks, but do not use attention and do not condition on the source sentence.", "labels": [], "entities": []}, {"text": "We run beam search with beam size of 6 in all our experiments.", "labels": [], "entities": [{"text": "beam search", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.891640692949295}]}, {"text": "For each setup we train five models using SGD (batch size of 32 sentences) with learning rate decay and label smoothing, and either select the best one (single system) or ensemble the four best models based on dev set BLEU score.", "labels": [], "entities": [{"text": "label smoothing", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.6566696166992188}, {"text": "BLEU", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.987286388874054}]}], "tableCaptions": [{"text": " Table 1: Parallel training data.", "labels": [], "entities": []}, {"text": " Table 2: Monolingual training data.", "labels": [], "entities": []}, {"text": " Table 3: Summary of NMT settings for all models.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7195766568183899}]}, {"text": " Table 4: Comparison of our PRENORM and POST- NORM combination strategies with shallow fu- sion (Gulcehre et al., 2015) and cold fusion (Sriram  et al., 2017) under an RNN-LM.", "labels": [], "entities": []}, {"text": " Table 5: Comparison between using a recurrent LM  (RNN) and an n-gram based feedforward LM (FFN)  on English-Turkish.", "labels": [], "entities": []}, {"text": " Table 6: Combining an RNN-LM and a feedforward  LM with the translation model using the POSTNORM  strategy.", "labels": [], "entities": []}, {"text": " Table 7: Perplexity and average entropies of the dis- tributions generated by our systems on the English- Turkish dev set.", "labels": [], "entities": [{"text": "English- Turkish dev set", "start_pos": 98, "end_pos": 122, "type": "DATASET", "confidence": 0.7972410082817077}]}, {"text": " Table 8: BLEU n-gram precisions for Estonian-English.", "labels": [], "entities": [{"text": "BLEU n-gram precisions", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8390211264292399}]}]}