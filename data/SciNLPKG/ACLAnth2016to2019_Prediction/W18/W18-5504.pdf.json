{"title": [{"text": "Retrieve and Re-rank: A Simple and Effective IR Approach to Simple Question Answering over Knowledge Graphs", "labels": [], "entities": [{"text": "Retrieve", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.8892563581466675}, {"text": "Simple Question Answering", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.6234716673692068}]}], "abstractContent": [{"text": "SimpleQuestions is a commonly used benchmark for single-factoid question answering (QA) over Knowledge Graphs (KG).", "labels": [], "entities": [{"text": "single-factoid question answering (QA)", "start_pos": 49, "end_pos": 87, "type": "TASK", "confidence": 0.7907346685727438}]}, {"text": "Existing QA systems rely on various components to solve different sub-tasks of the problem (such as entity detection, entity linking, relation prediction and evidence integration).", "labels": [], "entities": [{"text": "entity detection", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.7654763460159302}, {"text": "entity linking", "start_pos": 118, "end_pos": 132, "type": "TASK", "confidence": 0.7819393575191498}, {"text": "relation prediction", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.8238008320331573}, {"text": "evidence integration", "start_pos": 158, "end_pos": 178, "type": "TASK", "confidence": 0.7008979320526123}]}, {"text": "In this work, we propose a different approach to the problem and present an information retrieval style solution for it.", "labels": [], "entities": []}, {"text": "We adopt a two-phase approach: candidate generation and candidate re-ranking to answer questions.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.7854326069355011}]}, {"text": "We propose a Triplet-Siamese-Hybrid CNN (TSHCNN) to re-rank candidate answers.", "labels": [], "entities": []}, {"text": "Our approach achieves an accuracy of 80% which sets anew state-of-the-art on the SimpleQuestions dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995248317718506}, {"text": "SimpleQuestions dataset", "start_pos": 81, "end_pos": 104, "type": "DATASET", "confidence": 0.9367464482784271}]}], "introductionContent": [], "datasetContent": [{"text": "We report results using the standard evaluation criteria (, in terms of path-level accuracy, which is the percentage of questions for which the top-ranked candidate fact is correct.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9281671643257141}]}, {"text": "A prediction is correct if the system retrieves the correct subject and predicate.", "labels": [], "entities": []}, {"text": "Network parameters and decisions are presented in.", "labels": [], "entities": []}, {"text": "We use top-200 candidates as input to the re-ranking step.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: End-to-End Answer Accuracy for English  Questions", "labels": [], "entities": [{"text": "End-to-End Answer Accuracy", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.5569069981575012}]}, {"text": " Table 3: Candidate generation results: Recall of top-k  answer candidates.", "labels": [], "entities": [{"text": "Candidate generation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7427829504013062}]}, {"text": " Table 4: Candidate Re-ranking: Ablation Study. CQT:  Additional inputs, concatenate question and tuple ,  SCNS: Solr Candidates as Negative Samples", "labels": [], "entities": [{"text": "Ablation", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9867035150527954}]}]}