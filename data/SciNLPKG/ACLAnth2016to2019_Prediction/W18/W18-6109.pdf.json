{"title": [{"text": "Paraphrase Detection on Noisy Subtitles in Six Languages", "labels": [], "entities": [{"text": "Paraphrase Detection on Noisy Subtitles", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.808789324760437}]}], "abstractContent": [{"text": "We perform automatic paraphrase detection on subtitle data from the Opusparcus corpus comprising six European languages: German, En-glish, Finnish, French, Russian, and Swedish.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7410353422164917}, {"text": "Opusparcus corpus", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.9505632519721985}]}, {"text": "We train two types of supervised sentence embedding models: a word-averaging (WA) model and a gated recurrent averaging network (GRAN) model.", "labels": [], "entities": []}, {"text": "We find out that GRAN outperforms WA and is more robust to noisy training data.", "labels": [], "entities": [{"text": "GRAN", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9530507922172546}, {"text": "WA", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.6542942523956299}]}, {"text": "Better results are obtained with more and noisier data than less and cleaner data.", "labels": [], "entities": []}, {"text": "Additionally, we experiment on other datasets, without reaching the same level of performance, because of domain mismatch between training and test data.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper studies automatic paraphrase detection on subtitle data for six European languages.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8101434111595154}]}, {"text": "Paraphrases area set of phrases or full sentences in the same language that mean approximately the same thing.", "labels": [], "entities": []}, {"text": "Automatically finding out when two phrases mean the same thing is interesting from both a theoretical and practical perspective.", "labels": [], "entities": []}, {"text": "Theoretically, within the field of distributional, compositional semantics, there is currently a significant amount of interest in models and representations that capture the meaning of not just single words, but sequences of words.", "labels": [], "entities": []}, {"text": "There are also practical implementations, such as providing multiple alternative correct translations when evaluating the accuracy of machine translation systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9964397549629211}, {"text": "machine translation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7206826508045197}]}, {"text": "To our knowledge, the present work is the first published study of automatic paraphrase detection based on data from Opusparcus, a recently published paraphrase corpus . Opusparcus consists of sentential paraphrases, that is, pairs of full sentences that convey approximately the same meaning.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.8181571066379547}]}, {"text": "Opusparcus provides data for six European languages: German, English, Finnish, French, Russian, and Swedish.", "labels": [], "entities": [{"text": "Opusparcus", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9373599886894226}]}, {"text": "The data sets have been extracted from OpenSubtitles2016 (, which is a collection of translated movie and TV subtitles.", "labels": [], "entities": []}, {"text": "In addition to Opusparcus, experiments are performed on other well known paraphrase resources: (1) PPDB, the Paraphrase Database (, (2) MSRPC, the Microsoft Research Paraphrase Corpus (), (3) SICK (), and (4) STS14 ().", "labels": [], "entities": [{"text": "Opusparcus", "start_pos": 15, "end_pos": 25, "type": "DATASET", "confidence": 0.9324605464935303}, {"text": "Microsoft Research Paraphrase Corpus", "start_pos": 147, "end_pos": 183, "type": "DATASET", "confidence": 0.8404251635074615}]}, {"text": "We are interested in movie and TV subtitles because of their conversational nature.", "labels": [], "entities": []}, {"text": "This makes subtitle data ideal for exploring dialogue phenomena and properties of everyday, colloquial language (.", "labels": [], "entities": []}, {"text": "We would also like to stress the importance of working on other languages beside English.", "labels": [], "entities": []}, {"text": "Unfortunately, many language resources contain English data only, such as MSRPC and SICK.", "labels": [], "entities": [{"text": "MSRPC", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.8945571780204773}]}, {"text": "In other datasets, the quality of the English data surpasses that of the other languages to a considerable extent, as in the mutilingual version of PPDB ().", "labels": [], "entities": []}, {"text": "Although our subtitle data is very interesting data, it is also noisy data, in several respects.", "labels": [], "entities": []}, {"text": "Since the subtitles are user-contributed data, there are misspellings both due to human mistake and due to errors in optical character recognition (OCR).", "labels": [], "entities": [{"text": "optical character recognition (OCR)", "start_pos": 117, "end_pos": 152, "type": "TASK", "confidence": 0.7477852801481882}]}, {"text": "OCR errors emerge when textual subtitle files are 2 OpenSubtitles2016 is extracted from www.", "labels": [], "entities": []}, {"text": "OpenSubtitles2016 is in itself a subset of the larger OPUS collection (\"...", "labels": [], "entities": [{"text": "OpenSubtitles2016", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.895751953125}, {"text": "OPUS collection", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9314445853233337}]}, {"text": "the open parallel corpus\"): opus.lingfil.uu.se, and provides a large number of sentence-aligned parallel corpora in 65 languages.", "labels": [], "entities": []}, {"text": "produced by \"ripping\" (scanning) the subtitle text from DVDs using various tools.", "labels": [], "entities": [{"text": "ripping\" (scanning) the subtitle text from DVDs", "start_pos": 13, "end_pos": 60, "type": "TASK", "confidence": 0.7205356180667877}]}, {"text": "Furthermore, movies are sometimes not tagged with the correct language, they are encoded in various character encodings, and they come in various formats.", "labels": [], "entities": []}, {"text": "A different type of errors emerge because of misalignments and issues with sentence segmentation.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.7166487127542496}]}, {"text": "Opusparcus has been constructed by finding pairs of sentences in one language that have a common translation in at least one other language.", "labels": [], "entities": []}, {"text": "For example, English \"Have a seat.\" is potentially a paraphrase of \"Sit down.\" because both can be translated to French \"Asseyez-vous.\"", "labels": [], "entities": []}, {"text": "(Creutz, 2018) To figure out that \"Have a seat.\" is a translation of \"Asseyez-vous.\", English and French subtitles for the same movie can be used.", "labels": [], "entities": []}, {"text": "English and French text that occur at the same time in the movie are assumed to be translations of each other.", "labels": [], "entities": []}, {"text": "However, there are many complications involved: Subtitles are not necessarily shown as entire sentences, but as snippets of text that fit on the screen.", "labels": [], "entities": []}, {"text": "There are numerous partial overlaps when comparing the contents of subtitle screens across different languages, and the reconstruction of proper sentences maybe difficult.", "labels": [], "entities": [{"text": "reconstruction of proper sentences", "start_pos": 120, "end_pos": 154, "type": "TASK", "confidence": 0.8389878422021866}]}, {"text": "There may also be timing differences, because of different subtitle speeds and different time offsets for starting the subtitles.)", "labels": [], "entities": [{"text": "timing", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9846912026405334}]}, {"text": "Furthermore, argue that \" should better be viewed as boiled down transcriptions of the same conversations across several languages.", "labels": [], "entities": []}, {"text": "Subtitles will inevitably differ in how they 'compress' the conversations, notably due to structural divergences between languages, cultural differences and disparities in subtitling traditions/conventions.", "labels": [], "entities": []}, {"text": "As a consequence, sentence alignments extracted from subtitles often have a higher degree of insertions and deletions compared to alignments derived from other sources.\"", "labels": [], "entities": []}, {"text": "We tackle the paraphrase detection task using a sentence embedding approach.", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.9445572098096212}]}, {"text": "We experiment with sentence encoding models that take as input a single sentence and produce a vector representing the semantics of the sentence.", "labels": [], "entities": []}, {"text": "While models that rely on sentence pairs as input are able to use additional information, such as attention between the sentences, the sentence embedding approach has its advantages: Embeddings can be calculated also when no sentence pair is available, and large numbers of embeddings can be precalculated, which allows for fast comparisons in huge datasets.", "labels": [], "entities": []}, {"text": "Sentence representation learning has been a topic of growing interest recently.", "labels": [], "entities": [{"text": "Sentence representation learning", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9512404004732767}]}, {"text": "Much of this work has been done in the context of generalpurpose sentence embeddings using unsupervised approaches inspired by work on word embeddings () as well as approaches relying on supervised training objectives (.", "labels": [], "entities": []}, {"text": "While the paraphrase detection task is potentially useful for learning general purpose embeddings, we are mainly interested in paraphrastic sentence embeddings for paraphrase detection and semantic similarity tasks.", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8788015246391296}, {"text": "paraphrase detection", "start_pos": 164, "end_pos": 184, "type": "TASK", "confidence": 0.8763853013515472}]}, {"text": "Closest to the present work is that of, who study sentence representation learning using multiple encoding architectures and two different sources of training data.", "labels": [], "entities": [{"text": "sentence representation learning", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.8180539210637411}]}, {"text": "It was found that certain models benefit significantly from using full sentences (SimpWiki) instead of short phrases (PPDB) as training data.", "labels": [], "entities": []}, {"text": "However, the SimpWiki data set is relatively small, and this leaves open the question how much the approaches could benefit from very large corpora of sentential paraphrases.", "labels": [], "entities": [{"text": "SimpWiki data set", "start_pos": 13, "end_pos": 30, "type": "DATASET", "confidence": 0.8555840253829956}]}, {"text": "It is also unclear how well the approaches generalize to languages other than English.", "labels": [], "entities": []}, {"text": "The current paper takes a step forward in that experiments are performed on five other languages in addition to English.", "labels": [], "entities": []}, {"text": "We also study the effects of noise in the training data sets.", "labels": [], "entities": [{"text": "training data sets", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.7950361569722494}]}], "datasetContent": [{"text": "Our initial experiment addresses the effects of unsupervised morphological segmentation on the results of the paraphrase detection task.", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.9232568939526876}]}, {"text": "Next, we tackle our main question on the tradeoff between the amount of noise in the training data and the data size.", "labels": [], "entities": []}, {"text": "In particular, we try to see if an optimal amount of noise can be found, and whether the different models have different demands in this respect.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the English-language models on out-of-domain semantic similarity and paraphrase detection tasks.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.8385474979877472}]}, {"text": "All evaluations on the Opusparcus are conducted in the following manner: Each sentence in the sentence pair is embedded using the sentence encoding model.", "labels": [], "entities": [{"text": "Opusparcus", "start_pos": 23, "end_pos": 33, "type": "DATASET", "confidence": 0.9278438091278076}]}, {"text": "The resulting vectors are concatenated and passed onto a multi-layer perceptron classifier with a single hidden layer of 200 units.", "labels": [], "entities": []}, {"text": "The classifier is trained on the development set, and the final results are reported on the unseen test set in terms of classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9134674072265625}]}], "tableCaptions": [{"text": " Table 3: Results on Opusparcus for GRAN (all languages) and WA (English only). The first six rows show the  accuracies of the GRAN model at different estimated levels of correctly labeled positive training pairs: 80%, 70%,  and 60%. In each entry in the table, the first number is the classification accuracy and the number in brackets is  the number of assumed positive training pairs in millions. For comparison, the 1M column to the left repeats the  values from", "labels": [], "entities": [{"text": "GRAN", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8699710369110107}, {"text": "WA", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.7623363137245178}, {"text": "accuracy", "start_pos": 301, "end_pos": 309, "type": "METRIC", "confidence": 0.9112848043441772}]}, {"text": " Table 6: Results on Opusparcus test sets for models  trained on PPDB.", "labels": [], "entities": [{"text": "Opusparcus test sets", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.9355002045631409}, {"text": "PPDB", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.82369065284729}]}]}