{"title": [{"text": "Char2char Generation with Reranking for the E2E NLG Challenge", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.9574958880742391}]}], "abstractContent": [{"text": "This paper describes our submission to the E2E NLG Challenge.", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.8459269007047018}]}, {"text": "Recently, neural seq2seq approaches have become mainstream in NLG, often resorting to pre-(respectively post-) processing delex-icalization (relexicalization) steps at the word-level to handle rare words.", "labels": [], "entities": []}, {"text": "By contrast, we train a simple character level seq2seq model, which requires no pre/post-processing (delexicalization, tok-enization or even lowercasing), with surprisingly good results.", "labels": [], "entities": []}, {"text": "For further improvement , we explore two re-ranking approaches for scoring candidates.", "labels": [], "entities": []}, {"text": "We also introduce a synthetic dataset creation procedure , which opens up anew way of creating artificial datasets for Natural Language Generation.", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 119, "end_pos": 146, "type": "TASK", "confidence": 0.6408798098564148}]}], "introductionContent": [{"text": "Natural Language Generation from Dialogue Acts involves generating human understandable utterances from slot-value pairs in a Meaning Representation (MR).", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6330675780773163}]}, {"text": "This is a component in Spoken Dialogue Systems, where recent advances in Deep Learning are stimulating interest towards using end-to-end models.", "labels": [], "entities": [{"text": "Spoken Dialogue Systems", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8685283462206522}]}, {"text": "Traditionally, the Natural Language Generation (NLG) component in Spoken Dialogue Systems has been rule-based, involving a two stage pipeline: 'sentence planning' (deciding the overall structure of the sentence) and 'surface realization' (which renders actual utterances using this structure).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.8217080434163412}]}, {"text": "The resulting utterances using these rule-based systems tend * Work done during internship at Naver Labs (Previously Xerox Research Centre Europe.)", "labels": [], "entities": [{"text": "Xerox Research Centre Europe", "start_pos": 117, "end_pos": 145, "type": "DATASET", "confidence": 0.7650084793567657}]}, {"text": "\u2020 Previously Xerox Research Centre Europe.", "labels": [], "entities": [{"text": "Xerox Research Centre Europe", "start_pos": 13, "end_pos": 41, "type": "DATASET", "confidence": 0.9293019771575928}]}, {"text": "to be rigid, repetitive and limited in scope.", "labels": [], "entities": []}, {"text": "Recent approaches in dialogue generation tend to directly learn the utterances from data (.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8660809993743896}]}, {"text": "Recurrent Neural Networks with gated cell variants such as LSTMs and GRUs) are now extensively used to model sequential data.", "labels": [], "entities": []}, {"text": "This class of neural networks when integrated in a Sequence to) framework have produced state-of-art results in Machine Translation (, Conversational Modeling (, Semantic Parsing ( and Natural Language Generation (.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8819491267204285}, {"text": "Semantic Parsing", "start_pos": 162, "end_pos": 178, "type": "TASK", "confidence": 0.8348644971847534}, {"text": "Natural Language Generation", "start_pos": 185, "end_pos": 212, "type": "TASK", "confidence": 0.6561490197976431}]}, {"text": "While these models were initially developed to be used at word level in NLP related tasks, there has been a recent interest to use character level sequences, as in Machine Translation (.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.852501392364502}]}, {"text": "Neural seq2seq approaches to Natural Language Generation (NLG) are typically wordbased, and resort to delexicalization (a process in which named entities (slot values) are replaced with special 'placeholders') to handle rare or unknown words (out-of-vocabulary (OOV) words, even with a large vocabulary).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.8243379890918732}]}, {"text": "It can be argued that this de-lexicalization is unable to account for phenomena such as morphological agreement (gender, numbers) in the generated text).", "labels": [], "entities": []}, {"text": "However, and Agarwal and Dymetman (2017) employ a char-based seq2seq model where the input MR is simply represented as a character sequence, and the output is also generated char-by-char; avoiding the rare word problem, as the character vocabulary is very small.", "labels": [], "entities": []}, {"text": "This work builds on top of the formulation of and describes our submission for the E2E NLG challenge (.", "labels": [], "entities": [{"text": "E2E NLG challenge", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.9026135007540385}]}, {"text": "We further explore re-ranking techniques in order to identify the perfect 'oracle prediction' utterance.", "labels": [], "entities": [{"text": "oracle prediction' utterance", "start_pos": 75, "end_pos": 103, "type": "TASK", "confidence": 0.7977176606655121}]}, {"text": "One of the strategies for reranking uses an approach similar to the 'inverted generation' technique of., and have also trained a reverse model for back translation in Machine Translation and NLG.", "labels": [], "entities": [{"text": "back translation", "start_pos": 147, "end_pos": 163, "type": "TASK", "confidence": 0.7164370119571686}, {"text": "Machine Translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.7942703366279602}]}, {"text": "A synthetic data creation technique is used by and but as far as we know, our protocol is novel.", "labels": [], "entities": []}, {"text": "Our contributions in this paper and challenge can, thus, be summarized as: 1.", "labels": [], "entities": []}, {"text": "We show how a vanilla character-based sequence-to-sequence model performs successfully on the challenge test dataset in terms of BLEU score, while having a tendency to omit semantic material.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.9835028350353241}]}, {"text": "As far as we know, we are the only team using character based seq2seq for the challenge.", "labels": [], "entities": []}, {"text": "2. We propose a novel data augmentation technique in Natural Language Generation (NLG) which consists of 'editing' the Meaning Representation (MR) and using the original ReFerences (RF).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.823802669843038}]}, {"text": "This fabricated dataset helps us in extracting features (to detect errors), used for re-ranking the generated candidates (Section 2.2).", "labels": [], "entities": []}, {"text": "3. We introduce two different re-ranking strategies corresponding to our primary and secondary submission (in the challenge), defined in Section 2.3. 1", "labels": [], "entities": []}], "datasetContent": [{"text": "We artificially create a training set for the classifier (defined in Section 2.3.2) to detect errors (primarily omission of content) in generated utterances, by a data augmentation technique.", "labels": [], "entities": []}, {"text": "The systematic structure of the slots in MR gives us freedom to naturally augment data for our use case.", "labels": [], "entities": [{"text": "MR", "start_pos": 41, "end_pos": 43, "type": "DATASET", "confidence": 0.6372247338294983}]}, {"text": "To the best of our knowledge, this is the first approach of using data augmentation in the proposed fashion and opens up new directions to create artificial datasets for NLG.", "labels": [], "entities": []}, {"text": "We first define the procedure for creating a dataset to detect omission and then show how a similar approach can be used to create a synthetic dataset to detect additions.", "labels": [], "entities": []}, {"text": "This approach assumes that originally there are no omissions in RF fora given MR (in the training dataset).", "labels": [], "entities": [{"text": "MR", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.7235486507415771}]}, {"text": "These can be considered as positive pairs when detecting omissions.", "labels": [], "entities": []}, {"text": "Now if we artificially add another slot to the original MR and use the same RF for this new (constructed) MR, naturally the original RF tends to show omission of this added slot.", "labels": [], "entities": []}, {"text": "This is a two stage procedure: (a) Select a slot to add.", "labels": [], "entities": []}, {"text": "(b) Select a corresponding slot value.", "labels": [], "entities": []}, {"text": "Instead of sampling a particular slot in step (a), we add all the slots one by one (that could be augmented in MR apart from currently present slots).", "labels": [], "entities": []}, {"text": "Having chosen the slot type to be added, we add the slot value according to probability distribution of the slot values for that slot type.", "labels": [], "entities": []}, {"text": "The original (MR original ,RF) pair is assigned a class label of 1 and the new artificial pairs (MR new ,RF) a label of 0, denoting a case of omission (first line of).", "labels": [], "entities": []}, {"text": "Thus, these triplets (MR, RF, Class Label) allow us to treat this as a classification task.", "labels": [], "entities": []}, {"text": "In order to create a dataset which can be used for training our model to detect additions, we proceed in a similar way.", "labels": [], "entities": []}, {"text": "The difference is that now we systematically remove one slot in the original MR to create the new MRs (second line of).", "labels": [], "entities": []}, {"text": "In both cases, we control the procedure by manipulating MRs instead of the Natural Language RF.", "labels": [], "entities": [{"text": "MRs", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9049919247627258}]}, {"text": "This kind of augmented dataset opens up the possibility of using any classifier to detect the above mentioned errors.", "labels": [], "entities": []}, {"text": "The updated challenge dataset comprises 50K canonically ordered and systematically structured (MR,RF) pairs, collected following the crowdsourcing protocol explained in.", "labels": [], "entities": []}, {"text": "Consisting of 8 different slots (and their respective different values), note that the statistics in the test set differ significantly from the training set.", "labels": [], "entities": []}, {"text": "We used the open source tf-seq2seq framework 2 , built over TensorFlow ( and provided along with (, with some standard configurations.", "labels": [], "entities": []}, {"text": "We experimented with different numbers of layers in the encoder and decoder as well as different beam widths, while using the bi-directional encoder with an \"additive\" attention mechanism.", "labels": [], "entities": []}, {"text": "In terms of BLEU, our best performing model had the following configuration: encoder 1 layer, decoder 2 layers, GRU cell, beam-width 20, length penalty 1.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.99553382396698}, {"text": "length", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9712321758270264}]}, {"text": "We chose our primary system to be the re-ranker using the classifier.", "labels": [], "entities": []}, {"text": "summarizes our ranking among all the 60+ submissions (primary as well as additional) on the test set.", "labels": [], "entities": []}, {"text": "In terms of BLEU, two of our systems were in the top 5 among all 60+ submissions to the challenge.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9991371035575867}]}, {"text": "Results for human evaluation, as released by the challenge organizers, are summarized in Table 2 of (Du\u0161ek et al., 2018).", "labels": [], "entities": []}, {"text": "They followed the TrueSkill algorithm () judging all the primary systems on Quality and Naturalness.", "labels": [], "entities": []}, {"text": "We obtained competitive results in terms of both metrics, our system being in the 2nd cluster out of 5 (for both evaluations).", "labels": [], "entities": []}, {"text": "On the other hand, most systems ranked high on quality tended to have lower ranks for naturalness and vice versa.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic BLEU evaluations released by  organizers on the final challenge submission. We  had 3 submissions as described in Section 2. Two  of our systems were in the top 5 among all 60+  submissions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9446245431900024}]}, {"text": " Table 2: Human evaluation was crowd-sourced on  the primary system according to the TrueSkill al- gorithm (Sakaguchi et al., 2014)", "labels": [], "entities": [{"text": "TrueSkill al- gorithm (Sakaguchi et al., 2014)", "start_pos": 85, "end_pos": 131, "type": "DATASET", "confidence": 0.8820496255701239}]}]}