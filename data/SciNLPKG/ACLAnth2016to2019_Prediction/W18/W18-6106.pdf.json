{"title": [{"text": "A POS Tagging Model Designed for Learner English", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 2, "end_pos": 13, "type": "TASK", "confidence": 0.7651397883892059}, {"text": "Learner English", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.768674373626709}]}], "abstractContent": [{"text": "There has been very limited work on the adaptation of Part-Of-Speech (POS) tagging to learner English despite the fact that POS tagging is widely used in related tasks.", "labels": [], "entities": [{"text": "Part-Of-Speech (POS) tagging", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.6433111906051636}, {"text": "POS tagging", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.732153981924057}]}, {"text": "In this paper, we explore how we can adapt POS tagging to learner English efficiently and effectively.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.8360685408115387}]}, {"text": "Based on the discussion of possible causes of POS tagging errors in learner En-glish, we show that deep neural models are particularly suitable for this.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.8618814051151276}]}, {"text": "Considering the previous findings and the discussion, we introduce the design of our model based on bidi-rectional Long Short-Term Memory.", "labels": [], "entities": []}, {"text": "In addition , we describe how to adapt it to a wide variety of native languages (potentially, hundreds of them).", "labels": [], "entities": []}, {"text": "In the evaluation section, we empirically show that it is effective for POS tagging in learner English, achieving an accuracy of 0.964, which significantly outperforms the state-of-the-art POS-tagger.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.8646303415298462}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9991523027420044}]}, {"text": "We further investigate the tagging results in detail, revealing which part of the model design does or does not improve the performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Although Part-Of-Speech (POS) tagging is widely used in Natural Language Processing (NLP), there has been little work on its adaptation to learner English . It is often done by simply adding a manually-POS-tagged learner corpus to the training data.", "labels": [], "entities": [{"text": "Part-Of-Speech (POS) tagging", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.662395977973938}]}, {"text": "Probably only one exception is the work by who proposed to solve POS tagging and spelling error correction simultaneously.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.7790690362453461}, {"text": "spelling error correction", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.6831039190292358}]}, {"text": "However, their method also requires a POSannotated learner as training data.", "labels": [], "entities": []}, {"text": "The availability of POS-labeled learner corpora is still very limited even after the efforts researchers (e.g.,; van; Foster (2007b,a); ;) have made.", "labels": [], "entities": []}, {"text": "Because of this limitation, POS taggers designed for canonical English (i.e., native English) are normally used in related tasks including grammatical error correction ( and its automated evaluation, automated essay scoring (, and analyses of learner English), to name a few.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.7844865322113037}, {"text": "grammatical error correction", "start_pos": 139, "end_pos": 167, "type": "TASK", "confidence": 0.6279722452163696}, {"text": "automated essay scoring", "start_pos": 200, "end_pos": 223, "type": "TASK", "confidence": 0.5916666984558105}]}, {"text": "Unfortunately, however, the discrepancy between a POS tagger and its target text often results in POS-tagging errors, which in turn leads to performance degradation in related tasks as and show.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.6656364649534225}]}, {"text": "Specifically, a wide variety of characteristic phenomena that potentially degrade POS tagging performance appear in learner English.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.8588122725486755}]}, {"text": "Section 2 shows that there exist a wide variety of potential causes of POS-tagging errors.", "labels": [], "entities": []}, {"text": "For the time being, let us consider the following erroneous sentence: where mistakenly-tagged tokens are written in bold type . It reveals that several POS-tagging errors occur because of orthographic and grammatical errors.", "labels": [], "entities": []}, {"text": "Besides, and demonstrate that learner English exhibits characteristic POS sequence patterns depending on the writers' native languages.", "labels": [], "entities": [{"text": "POS sequence patterns", "start_pos": 70, "end_pos": 91, "type": "METRIC", "confidence": 0.9200672308603922}]}, {"text": "All these phenomena suggest that the adaptation of POS tagging to learner English will reduce their influence and thus contribute to achieving better performance in the related tasks.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.7822417318820953}]}, {"text": "In view of this background, in this paper, we explore how we can adapt POS tagging to learner English effectively.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.7987766861915588}]}, {"text": "We first discuss potential causes of POS-tagging errors in learner English.", "labels": [], "entities": []}, {"text": "Based on this, we then describe how deep neural models, which have been successfully applied to sequence labeling (, are particularly suitable for our purpose.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7085472941398621}]}, {"text": "Considering the previous findings and our discussion on the possible causes of POS-tagging errors, we present the design of our model based on Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) (LSTM).", "labels": [], "entities": []}, {"text": "Our model is equipped with a word token-based and character-based bidirectional LSTMs (BLSTMs) whose inputs are respectively word embeddings and character embeddings obtained from learner corpora.", "labels": [], "entities": []}, {"text": "In addition, we describe how to adapt it to a wide variety of native languages (potentially, hundreds of them) through native language vectors.", "labels": [], "entities": []}, {"text": "In the evaluation section, we empirically show that it is effective in adapting POS tagging to learner English, achieving an accuracy of 0.964 on Treebank of Learner English (TLE;), which is significantly better than that of Turbo tagger, one of the state-of-the-art POS taggers for native English.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.7123221457004547}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9992425441741943}]}, {"text": "We further investigate the tagging results in detail, revealing why the word tokenbased and character-based BLSTMs contribute to improving the performance while native language vectors do not.", "labels": [], "entities": []}], "datasetContent": [{"text": "The presented model was first tested on KJ to investigate how well it performs on learner English without a POS-labeled learner corpus (and therefore without the native language-based module).", "labels": [], "entities": []}, {"text": "It was trained on sections 00-18 of Penn Treebank Wall Street Journal (WSJ 9 ).", "labels": [], "entities": [{"text": "Penn Treebank Wall Street Journal (WSJ 9 )", "start_pos": 36, "end_pos": 78, "type": "DATASET", "confidence": 0.9741400082906088}]}, {"text": "For example, in I went swimming in the see, the word see can be interpreted as a verb from its form and also as a noun from its context.", "labels": [], "entities": []}, {"text": "The former and latter are referred to as superficial and distributional POS tags, respectively.", "labels": [], "entities": []}, {"text": "9 Marcus, Mitchell, et al.", "labels": [], "entities": []}, {"text": "Linguistic Data Consortium, 1999.", "labels": [], "entities": [{"text": "Linguistic Data Consortium, 1999", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.8534969210624694}]}, {"text": "The maximum number of epochs was set to 20 and the one that maximized the performance on the development set was chosen.", "labels": [], "entities": []}, {"text": "The other hyperparameters were determined as follows: The dimensions of word and character embeddings: 200 and 50.", "labels": [], "entities": []}, {"text": "The native language module consisted of an embedding layer of dimension 512 and a linear layer of dimension 200.", "labels": [], "entities": []}, {"text": "The dropout rate for BLSTM was 0.5; Adam was used for optimization (step size: 0.01, the first and second moment: 0.9 and 0.999, respectively was used to produce word and character embeddings.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.6039777398109436}]}, {"text": "The hyperparameters were determined by using the TLE development set as follows: The dimensions of word and character embeddings were 200 and 50, respectively; the window size was set to five in both cases; the other hyperparameters were set as shown in the footnote.", "labels": [], "entities": [{"text": "TLE development set", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.9263121088345846}]}, {"text": "Performance was measured by accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9996410608291626}]}, {"text": "For comparison, a Conditional Random Field () (CRF)-based method was implemented using the same training and development sets.", "labels": [], "entities": []}, {"text": "The features are: superficial, lemma, prefix, and suffix of tokens and presence of specific characters (numbers, uppercase, and symbols) with a window size of five tokens to the left and right of the token in question.", "labels": [], "entities": []}, {"text": "The first-order Markov model features were used to encode inter-label dependencies.", "labels": [], "entities": []}, {"text": "The presented model was then tested on TLE with the same evaluation settings as in which reports on the performance of the Turbo Tagger.", "labels": [], "entities": [{"text": "TLE", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.6933061480522156}]}, {"text": "It was trained on the data consisting of the training portions of TLE and EWT.", "labels": [], "entities": [{"text": "TLE", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.4952797591686249}, {"text": "EWT", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.887603759765625}]}, {"text": "Its hyperparameters, including the number of learning epochs, were determined using the development set of TLE.", "labels": [], "entities": [{"text": "TLE", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.6214873194694519}]}, {"text": "Word and character embeddings were obtained from the same corpora above.", "labels": [], "entities": []}, {"text": "The resulting model was tested on the test portion of TLE.", "labels": [], "entities": [{"text": "TLE", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.7430667877197266}]}, {"text": "shows POS-tagging accuracy of our model and the CRF-based method on KJ both for superficial and distributional POS.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9860060214996338}]}, {"text": "It includes the results where all spelling errors were fully corrected to investigate the influence from spelling errors.", "labels": [], "entities": []}, {"text": "It reveals that the presented model without the information about correct spellings outperforms even the CRF-based method fully exploiting the information; the differences between our model with original spellings and the CRF-based method with correct spellings are statistically significant both in surface (at the 99% confidence level) and distributional POS accuracy (at the 95% confidence level) (test for difference in population portion).", "labels": [], "entities": [{"text": "POS accuracy", "start_pos": 357, "end_pos": 369, "type": "METRIC", "confidence": 0.74781534075737}]}, {"text": "This shows that the presented model successfully absorbs the influence from spelling errors and also other linguistic phenomena particular to learner English.", "labels": [], "entities": []}, {"text": "shows the results on TLE including accuracy of the Turbo Tagger, which is cited from the work.", "labels": [], "entities": [{"text": "TLE", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.42403319478034973}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.999789297580719}, {"text": "Turbo Tagger", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.6351594924926758}]}, {"text": "It shows that the presented model outperforms the Turbo Tagger; the difference is statistically significant at the 99% confidence level (test for difference in population proportion).", "labels": [], "entities": []}, {"text": "Note that the Turbo Tagger was trained on the same POS-labeled native and learner corpus as in the presented model.", "labels": [], "entities": [{"text": "Turbo Tagger", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.5936111211776733}]}, {"text": "Nevertheless it does not perform as well as the presented models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on Target Learner Corpora.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy on KJ: All models are trained on sections 00-18 of WSJ.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989522695541382}, {"text": "WSJ", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8954194188117981}]}, {"text": " Table 3: Accuracy on TLE Test Set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9982349872589111}, {"text": "TLE Test Set", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.8048975268999735}]}]}