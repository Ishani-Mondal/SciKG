{"title": [{"text": "CUNI System for the WMT18 Multimodal Translation Task", "labels": [], "entities": [{"text": "WMT18 Multimodal Translation", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.6516835788885752}]}], "abstractContent": [{"text": "We present our submission to the WMT18 Multimodal Translation Task.", "labels": [], "entities": [{"text": "WMT18 Multimodal Translation Task", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.750052846968174}]}, {"text": "The main feature of our submission is applying a self-attentive network instead of a recurrent neu-ral network.", "labels": [], "entities": []}, {"text": "We evaluate two methods of incorporating the visual features in the model: first, we include the image representation as another input to the network; second, we train the model to predict the visual features and use it as an auxiliary objective.", "labels": [], "entities": []}, {"text": "For our submission , we acquired both textual and multimodal additional data.", "labels": [], "entities": []}, {"text": "Both of the proposed methods yield significant improvements over recurrent networks and self-attentive textual baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimodal Machine Translation (MMT) is one of the tasks that seek ways of capturing the relation of texts in different languages given a shared \"grounding\" information in a different (e.g. visual) modality.", "labels": [], "entities": [{"text": "Multimodal Machine Translation (MMT)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8354723552862803}]}, {"text": "The goal of the MMT shared task is to generate an image description (caption) in the target language using a caption in the source language and the image itself.", "labels": [], "entities": [{"text": "MMT shared task", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.9258458614349365}]}, {"text": "The main motivation for this task is the development of models that can exploit the visual information for meaning disambiguation and thus model the denotation of words.", "labels": [], "entities": [{"text": "meaning disambiguation", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.7247277349233627}]}, {"text": "During the last years, MMT was addressed as a subtask of neural machine translation (NMT).", "labels": [], "entities": [{"text": "MMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9721097350120544}, {"text": "neural machine translation (NMT)", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.8374126156171163}]}, {"text": "It was thoroughly studied within the framework of recurrent neural networks (RNNs) ( . Recently, the architectures based on self-attention such as the Transformer () became state-of-theart in NMT.", "labels": [], "entities": []}, {"text": "In this work, we present our submission based on the Transformer model.", "labels": [], "entities": []}, {"text": "We propose two ways of extending the model.", "labels": [], "entities": []}, {"text": "First, we tweak the architecture such that it is able to process both modalities in a multi-source learning scenario.", "labels": [], "entities": []}, {"text": "Second, we leave the model architecture intact, but add another training objective and train the textual encoder to be able to predict the visual features of the image described by the text.", "labels": [], "entities": []}, {"text": "This training component has been introduced in RNNs by and is called the \"imagination\".", "labels": [], "entities": []}, {"text": "We find that with self-attentive networks, we are able to improve over a strong textual baseline by including the visual information in the model.", "labels": [], "entities": []}, {"text": "This has been proven challenging in the previous RNN-based submissions, where there was only a minor difference in performance between textual and multimodal models.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 summarizes the previous submissions and related work.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the proposed methods.", "labels": [], "entities": []}, {"text": "The details of the datasets used for the training are given in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 describes the conducted experiments.", "labels": [], "entities": []}, {"text": "We discuss the results in Section 6 and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this year's round, two variants of the MMT tasks were announced.", "labels": [], "entities": [{"text": "MMT tasks", "start_pos": 42, "end_pos": 51, "type": "TASK", "confidence": 0.9242373406887054}]}, {"text": "As in the previous years, the goal of Task 1 is to translate an English caption into the target language given the image.", "labels": [], "entities": [{"text": "translate an English caption", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.7033912390470505}]}, {"text": "The target languages are German, French and Czech.", "labels": [], "entities": []}, {"text": "In Task 1a, the model receives the image and its captions in English, German, and French and is trained to produce the Czech translation.", "labels": [], "entities": []}, {"text": "In our submission, we focus only on Task 1.", "labels": [], "entities": []}, {"text": "In our submission, we experiment with three distinct architectures.", "labels": [], "entities": []}, {"text": "First, in textual architectures, we leave out the images from the training altogether.", "labels": [], "entities": []}, {"text": "We use this as a strong baseline for the multimodal experiments.", "labels": [], "entities": []}, {"text": "Second, multimodal experiments use the doubly attentive Transformer decoder described in Section 3.1.", "labels": [], "entities": []}, {"text": "Third, the experiments referred to as imagination employ the imagination component as described in Section 3.2.", "labels": [], "entities": []}, {"text": "We train the models in constrained and unconstrained setups.", "labels": [], "entities": []}, {"text": "In the constrained setup, only the Multi30k dataset is used for training.", "labels": [], "entities": [{"text": "Multi30k dataset", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.9481948912143707}]}, {"text": "In the unconstrained setup, we train the model using the additional data described in Section 4.", "labels": [], "entities": []}, {"text": "We run the multimodal experiments only in the constrained setup.", "labels": [], "entities": []}, {"text": "In the unconstrained variant of the imagination experiments, the dataset consists of examples that can miss either the textual target values (MSCOCO extension), or the image (additional en-cs   parallel data).", "labels": [], "entities": []}, {"text": "In these cases, we train only the decoding component with specified target value (i.e. imagination component on visual features, or the Transformer decoder on the textual data).", "labels": [], "entities": []}, {"text": "As said in Section 3.2, we train both components by summing the losses when both the image and the target sentence are available in a training example.", "labels": [], "entities": []}, {"text": "In all experiments, we use the Transformer network with 6 layers with model dimension of 512 and feed-forward hidden layer dimension of 4096 units.", "labels": [], "entities": []}, {"text": "The embedding matrix is shared between the encoder and decoder and its transposition is reused as the output projection matrix.", "labels": [], "entities": []}, {"text": "For each language pair, we use a vocabulary of approximately 15k wordpieces (.", "labels": [], "entities": []}, {"text": "We extract the vocabulary and train the model on lower-cased text without any further pre-processing steps applied.", "labels": [], "entities": []}, {"text": "We tokenize the text using the algorithm bundled with the tensor2tensor library (.", "labels": [], "entities": []}, {"text": "The tokenization algorithm splits the sentence to groups of alphanumeric and non-alphanumeric groups, throwing away single spaces that occur inside the sentence.", "labels": [], "entities": []}, {"text": "We conduct the experiments using the Neural Monkey toolkit . For image pre-processing, we use ResNet-50 () with identity mappings).", "labels": [], "entities": []}, {"text": "In the doubly-attentive model, we use the outputs of the last convolutional layer before applying the activation function with dimensionality of 8 \u00d7 8 \u00d7 2048.", "labels": [], "entities": []}, {"text": "We apply a trainable linear projection to the maps into 512 dimensions to fit the Transformer model dimension.", "labels": [], "entities": []}, {"text": "In the imagination experiments, we use average-pooled maps with 2048 dimensions.", "labels": [], "entities": []}, {"text": "Following, we set the margin parameter \u03b1 from Equation 5 to 0.1. 1 https://github.com/ufal/neuralmonkey For each model, we keep 10 sets of parameters that achieve the best BLEU scores () on the validation set.", "labels": [], "entities": [{"text": "margin parameter \u03b1", "start_pos": 22, "end_pos": 40, "type": "METRIC", "confidence": 0.9024304151535034}, {"text": "Equation", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9420139789581299}, {"text": "BLEU", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9987860321998596}]}, {"text": "We experiment with weight averaging and model ensembling.", "labels": [], "entities": [{"text": "weight averaging", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.6844233721494675}]}, {"text": "However, these methods performed similarly and we thus report only the results of the weight averaging, which is computationally less demanding.", "labels": [], "entities": [{"text": "weight averaging", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.6753428131341934}]}, {"text": "In all experiments, we use the Adam optimizer () with initial learning rate 0.2, and Noam learning rate decay scheme () with \u03b2 1 = 0.9, \u03b2 2 = 0.98 and = 10 \u22129 and 4,000 warm-up steps.", "labels": [], "entities": [{"text": "Noam learning rate decay", "start_pos": 85, "end_pos": 109, "type": "METRIC", "confidence": 0.6184180602431297}]}], "tableCaptions": [{"text": " Table 1: Multi30k statistics on training and validation  data -total number of tokens, average number of to- kens per sentence, and lengths of the shortest and the  longest sentence.", "labels": [], "entities": []}, {"text": " Table 3: Results on the 2016 test set in terms of BLEU score and METEOR score. We compare our results with  the last year's best system (Caglayan et al., 2017) which used model ensembling instead of weight averaging.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.988339900970459}, {"text": "METEOR score", "start_pos": 66, "end_pos": 78, "type": "METRIC", "confidence": 0.9848807454109192}]}]}