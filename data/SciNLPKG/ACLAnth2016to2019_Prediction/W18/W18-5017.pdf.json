{"title": [{"text": "Unsupervised Counselor Dialogue Clustering for Positive Emotion Elicitation in Neural Dialogue System", "labels": [], "entities": [{"text": "Positive Emotion Elicitation", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.736987312634786}]}], "abstractContent": [{"text": "Positive emotion elicitation seeks to improve user's emotional state through dialogue system interaction, where a chat-based scenario is layered with an implicit goal to address user's emotional needs.", "labels": [], "entities": []}, {"text": "Standard neural dialogue system approaches still fall short in this situation as they tend to generate only short, generic responses.", "labels": [], "entities": []}, {"text": "Learning from expert actions is critical, as these potentially differ from standard dialogue acts.", "labels": [], "entities": []}, {"text": "In this paper , we propose using a hierarchical neu-ral network for response generation that is conditioned on 1) expert's action, 2) dialogue context, and 3) user emotion, encoded from user input.", "labels": [], "entities": [{"text": "response generation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7848083674907684}]}, {"text": "We construct a corpus of interactions between a counselor and 30 participants following a negative emotional exposure to learn expert actions and responses in a positive emotion elici-tation scenario.", "labels": [], "entities": []}, {"text": "Instead of relying on the expensive, labor intensive, and often ambiguous human annotations, we unsuper-visedly cluster the expert's responses and use the resulting labels to train the network.", "labels": [], "entities": []}, {"text": "Our experiments and evaluation show that the proposed approach yields lower perplexity and generates a larger variety of responses.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We present human judges with a dialogue triple and ask them to rate the response in terms of three criteria: 1) naturalness, which evaluates whether the response is intelligible, logically follows the dialogue context, and resembles real human response, 2) emotional impact, to measure whether the response elicits a positive emotional impact or promotes an emotionally positive conversation, and 3) engagement, to evaluate whether the proposed response shows involvement in the dialogue and promotes longer conversation by inviting more response.", "labels": [], "entities": []}, {"text": "We evaluate Emo-HRED and the best performing MC-HRED utilizing K-Means clustering labels.", "labels": [], "entities": [{"text": "Emo-HRED", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.8687636256217957}]}, {"text": "We evaluate 100 triples from the full test set, where each is judged by 20 human evaluators.", "labels": [], "entities": []}, {"text": "Each triple is presented in A-B-A format, the first two dialogue turns are held fixed according to the test set, and the last turn is the response generated by the evaluated model.", "labels": [], "entities": []}, {"text": "Evaluators are asked to judge the responses by stating their agreement to three statements: 1) A gives a natural response, 2) A's response elicits a positive emotional impact in B, and 3) A's response in engaging.", "labels": [], "entities": []}, {"text": "The agreement is given using a Likert scale, ranging from 1 (strongly disagree) to 5 (strongly agree).", "labels": [], "entities": [{"text": "agreement", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9326976537704468}, {"text": "Likert scale", "start_pos": 31, "end_pos": 43, "type": "METRIC", "confidence": 0.9607641100883484}]}, {"text": "summarizes the subjective evaluation result.: Human subjective rvaluation result.", "labels": [], "entities": []}, {"text": "We observe slight improvement on MC-HRED in the emotional impact and a more notable one in the engagement metric.", "labels": [], "entities": []}, {"text": "On average, the responses generated by MC-HRED are 2.53 words longer compared to that of Emo-HRED.", "labels": [], "entities": []}, {"text": "From the ratings, we also found that engagement is moderately correlated with response length, with an average Pearson r of 0.41.", "labels": [], "entities": [{"text": "engagement", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9629712104797363}, {"text": "Pearson r", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9834136366844177}]}, {"text": "This signals that MC-HRED is able to produce longer sentences which results in higher engagement, while still maintaining naturalness and emotional impact.", "labels": [], "entities": []}, {"text": "Dialogue samples comparing the systems responses are included in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. We com- pute the average test triple length (59.6 tokens),  and group the test triples into two: those with be- low average length as \"short\" (294 triples), and  those above as \"long\" (186). Average perplexi- ties are shown for the entire test set (all), the short  group, and the long group, separately.", "labels": [], "entities": [{"text": "be- low average length", "start_pos": 118, "end_pos": 140, "type": "METRIC", "confidence": 0.6876601636409759}, {"text": "perplexi- ties", "start_pos": 209, "end_pos": 223, "type": "METRIC", "confidence": 0.7992155154546102}]}, {"text": " Table 1: Model Perplexity of different architec- tures.", "labels": [], "entities": []}]}