{"title": [{"text": "Anaphora Resolution for Improving Spatial Relation Extraction from Text", "labels": [], "entities": [{"text": "Anaphora Resolution", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7438343167304993}, {"text": "Improving Spatial Relation Extraction from Text", "start_pos": 24, "end_pos": 71, "type": "TASK", "confidence": 0.9204357067743937}]}], "abstractContent": [{"text": "Spatial relation extraction from generic text is a challenging problem due to the ambiguity of the prepositions spatial meaning as well as the nesting structure of the spatial descriptions.", "labels": [], "entities": [{"text": "Spatial relation extraction from generic text", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8955885469913483}]}, {"text": "In this work, we highlight the difficulties that the anaphora can make in the extraction of spatial relations.", "labels": [], "entities": []}, {"text": "We use external multi-modal (here visual) resources to find the most probable candidates for resolving the anaphoras that refer to the landmarks of the spatial relations.", "labels": [], "entities": []}, {"text": "We then use global inference to decide jointly on resolving the anaphora and extraction of the spatial relations.", "labels": [], "entities": []}, {"text": "Our preliminary results show that resolving anaphora improves the state-of-the-art results on spatial relation extraction.", "labels": [], "entities": [{"text": "spatial relation extraction", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.6248250305652618}]}], "introductionContent": [{"text": "Spatial relation extraction is the task of determining the relations that can exist among the spatial roles extracted from the text.", "labels": [], "entities": [{"text": "Spatial relation extraction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8916281660397848}]}, {"text": "In the recent years, significant progress has been made in spatial language understanding (i.e. mapping natural language text to a formal spatial meaning representation) (.", "labels": [], "entities": [{"text": "spatial language understanding", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.6446588834126791}]}, {"text": "As a basic example consider the sentence, \"A car is parked in front of a house.\"", "labels": [], "entities": []}, {"text": "In this sentence car is a trajector, house is a landmark and in front of is a spatial indicator.", "labels": [], "entities": []}, {"text": "Spatial indicators indicate the existence of spatial information in a sentence.", "labels": [], "entities": []}, {"text": "Trajector is an entity whose location is described and landmark is a reference object for describing the location of a trajector.", "labels": [], "entities": []}, {"text": "Extraction of the spatial relations with a good accuracy is still challenging (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9985178112983704}]}, {"text": "Particularly, our investigation on the errors of the previous models shows that when in a sentence the landmark is expressed as a pronoun like (\"it\", \"them\", \"him\",..), the extraction of spatial relations becomes more difficult.", "labels": [], "entities": []}, {"text": "For example, in the sentence, \"A narrow, rising street with colourful houses on both sides, among them a greenhouse with balconies and a white car parked in front of it, and a blue-and-white church on the right\", some of the spatial relations for this sentence will contain a landmark which is a pronoun such as R 1 \u2190[a green house] tr , lm . This issue is related to the well-known anaphora resolution problem which is also problematic for our goal of spatial relation extraction.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 383, "end_pos": 402, "type": "TASK", "confidence": 0.7034244239330292}, {"text": "spatial relation extraction", "start_pos": 453, "end_pos": 480, "type": "TASK", "confidence": 0.706893781820933}]}, {"text": "Anaphora Resolution which mostly appears as pronoun resolution, is the linguistic phenomenon by which the given pronoun is interpreted with the help of earlier or later items in the discourse ().", "labels": [], "entities": [{"text": "Anaphora Resolution", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7059227675199509}, {"text": "pronoun resolution", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7668120861053467}]}, {"text": "The pronoun word/phrase is referred as anaphor whereas the word/phrase to which it is referring is called antecedent, as both anaphor and antecedent are referring to the same object in the real world, they are termed coreferential (.", "labels": [], "entities": []}, {"text": "It might be possible that for some anaphor, the antecedent is not mentioned in the same sentence, for example, consider a sentence, \"there area couple of trees in front of it\", here \"it\" is referring to some object which is not mentioned in the sentence, however, the referring object might have been mentioned in another sentence of the document.", "labels": [], "entities": []}, {"text": "Anaphora Resolution generally is recognized as a difficult problem in Natural Language Processing (.", "labels": [], "entities": [{"text": "Anaphora Resolution", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8649091720581055}]}, {"text": "The main research questions that we aim to address in this paper are, 1) whether the external knowledge from multimodal resources can help anaphora resolution in text.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.8225400149822235}]}, {"text": "2) whether the anaphora resolution can help in the spatial relation extraction from text (especially the relations in the form of triplet -Trajector, Spatial Indicator, Landmark).", "labels": [], "entities": [{"text": "spatial relation extraction", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7426286141077677}]}, {"text": "To answer these questions, we incorpo-: Image Textual Description: \"A narrow, rising street with colourful houses on both sides, among them a greenhouse with balconies and a white car parked in front of it, and a blue-and-white church on the right\" rated anaphora resolution for the pronouns in the sentence and proposed a global machine learning model to exploit the resolved pronouns.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 255, "end_pos": 274, "type": "TASK", "confidence": 0.7054682672023773}]}, {"text": "In the first step, we find the list of possible landmarks that can replace a pronoun in a relation (under consideration) with a specific candidate trajector and candidate spatial indicator.", "labels": [], "entities": []}, {"text": "We used Visual Genome () (an external) dataset for this purpose.", "labels": [], "entities": []}, {"text": "Visual genome dataset provides us a list of possible landmarks which can be used to resolve the anaphora by filtering them based on their similarity with the candidate landmarks that appear in the sentence.", "labels": [], "entities": [{"text": "Visual genome dataset", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.8558127085367838}]}, {"text": "This information is used in the global inference model for joint prediction.", "labels": [], "entities": [{"text": "joint prediction", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.84160515666008}]}, {"text": "We improve the spatial relation extraction from text by incorporating anaphora resolution to recognize landmarks in spatial relations which distinguishes our work from other works on anaphora resolution.", "labels": [], "entities": [{"text": "spatial relation extraction from text", "start_pos": 15, "end_pos": 52, "type": "TASK", "confidence": 0.7875141382217408}, {"text": "anaphora resolution", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7535380721092224}, {"text": "anaphora resolution", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.8120382726192474}]}, {"text": "The contribution of this paper includes a) exploiting external visual relation datasets to inject external knowledge into our models b) forming a joint model that imposes the consistency between the decisions made by separate relation classifiers that decide on a candidate spatial relation with pronoun landmark and candidate spatial relations with that pronoun replaced by candidate noun resolvants.", "labels": [], "entities": []}, {"text": "c) obtaining state-of-the-art results on spatial information extraction by exploiting the anaphora resolution.", "labels": [], "entities": [{"text": "spatial information extraction", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.6947855949401855}]}, {"text": "This paper shows our preliminary efforts in the sense that we have not applied the existing work on anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8179910480976105}]}, {"text": "We do not aim at improving the current techniques in that area but only show that such resolutions using visual resources can help spatial relation extraction.", "labels": [], "entities": [{"text": "spatial relation extraction", "start_pos": 131, "end_pos": 158, "type": "TASK", "confidence": 0.615059624115626}]}, {"text": "The rest of this paper is organized as follows, first we describe the problem setting in Section 2; our proposed model for this problem is described in Section 3.", "labels": [], "entities": []}, {"text": "The dataset used in tests, and evaluation results, are presented in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we briefly point to the related work in this area.", "labels": [], "entities": []}, {"text": "Finally, Section 6 summarizes the conclusions and outlines directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "CLEF 2017 mSpRL dataset: Our model is evaluated on this dataset which is a subset of IAPR TC-12 1 Benchmark and annotated specifically for the SpRL task.", "labels": [], "entities": [{"text": "CLEF 2017 mSpRL dataset", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.9300654083490372}, {"text": "IAPR TC-12 1 Benchmark", "start_pos": 85, "end_pos": 107, "type": "DATASET", "confidence": 0.8719906657934189}]}, {"text": "The training set contains 761 and whereas test set contains 939 spatial relations respectively ().", "labels": [], "entities": []}, {"text": "The total number of spatial relations containing pronoun landmark in train and testis 44 and 129 respectively.", "labels": [], "entities": []}, {"text": "Visual Genome dataset (VG): Visual Genome dataset has seven main components (, one of it is 'relationships' which contains the relationships between pairs of objects in the images.", "labels": [], "entities": [{"text": "Visual Genome dataset (VG)", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.7346756060918173}, {"text": "Visual Genome dataset", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.5859326124191284}]}, {"text": "Each relation has two arguments, the first one is referred as subject whereas the latter one is referred as object.", "labels": [], "entities": []}, {"text": "These relationships can be actions, spatial, prepositions, verbs, comparative or prepositional phrases.", "labels": [], "entities": []}, {"text": "Visual genome dataset contains 108077 images whereas its relationships part contains 2316104 relation instances.", "labels": [], "entities": [{"text": "Visual genome dataset", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.9276919960975647}]}, {"text": "This dataset is used to obtain the possible landmarks that can occur in a relationship with a given subject.", "labels": [], "entities": []}, {"text": "In this section, we experimentally show the effectiveness of our proposed model in improving the spatial role/relation extraction.", "labels": [], "entities": [{"text": "spatial role/relation extraction", "start_pos": 97, "end_pos": 129, "type": "TASK", "confidence": 0.6188419163227081}]}, {"text": "We use Saul ( to implement the models and solve the global inference of Section 3.5.", "labels": [], "entities": []}, {"text": "The code is publicly available here 2 . We compare our approach with the state-of-theart ().", "labels": [], "entities": []}, {"text": "However, in the mentioned paper, the authors use visual data from the accompanying images to improve the models.", "labels": [], "entities": []}, {"text": "In   this paper, we use their best model (referred here as M 0 -Baseline and M 0 + C -Baseline plus constraints) which is trained on text only and we ignore the visual information which is aligned with the text.", "labels": [], "entities": []}, {"text": "The experimental results in show that our baseline model (A-Replacement) is significantly better as compared to the state-of-theart baseline model (M0).", "labels": [], "entities": [{"text": "A-Replacement", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.9842367172241211}]}, {"text": "This shows that replacing the pronoun landmark candidates with our proposed model probable landmark has positive impact on extraction of spatial roles (as shown in) and relations.", "labels": [], "entities": []}, {"text": "The improvement in the results is because the spatial roles predication is improved, which gives a more confidence to the model to classify the triplets as spatial relations which leads to more positive predictions and higher recall of the relations.", "labels": [], "entities": [{"text": "recall", "start_pos": 226, "end_pos": 232, "type": "METRIC", "confidence": 0.9978296160697937}]}, {"text": "Furthermore, our second model (A-Inference) in which we train an additional new-relationclassifier by generating additional examples and perform joint inference further improves the results over the state-of-the-art model with constraints (M0+C).", "labels": [], "entities": []}, {"text": "The experimental results in show that adding constraints to our second model (A+Inference) significantly improves the classification of spatial roles (i.e. trajectors and landmarks), although the spatial indicators is slightly improved.", "labels": [], "entities": [{"text": "A+Inference", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.885500172773997}, {"text": "classification of spatial roles", "start_pos": 118, "end_pos": 149, "type": "TASK", "confidence": 0.818711519241333}]}, {"text": "Also these constraints help improving the coarse-grained spatial relations as shown in table 6, although it doesn't have any impact on distance category because the number of examples in test set is very small (i.e. three instances only).", "labels": [], "entities": [{"text": "distance category", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.7356344759464264}]}, {"text": "Our results improve the state-of-the-art models for spatial relation extraction.", "labels": [], "entities": [{"text": "spatial relation extraction", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6362225711345673}]}, {"text": "Both proposed models significantly improves the extraction of spatial roles and relations (when compared with independent learning and with constrained models).", "labels": [], "entities": [{"text": "extraction of spatial roles and relations", "start_pos": 48, "end_pos": 89, "type": "TASK", "confidence": 0.7833903928597769}]}, {"text": "However, the results of some of the categories in the fine-grained relations drops which are not reported here.", "labels": [], "entities": []}, {"text": "These results are at the preliminary stage and we further analyze our models.", "labels": [], "entities": []}, {"text": "Particularly, we will use existing anaphora resolution models to see how those could help and provide a more reasonable baseline.", "labels": [], "entities": []}, {"text": "This baseline will help us to evaluate the advantage of the external visual knowledge more clearly.", "labels": [], "entities": []}, {"text": "It will be interesting to investigate what caused this drop in fine-grained relation types.", "labels": [], "entities": []}, {"text": "In addition to such further analysis, this work can be extended into two possible directions, 1) incorporate cross-sentence anaphora resolution for landmark candidates, and 2) incorporate co-reference resolution in general for all spatial relations.", "labels": [], "entities": [{"text": "cross-sentence anaphora resolution", "start_pos": 109, "end_pos": 143, "type": "TASK", "confidence": 0.6137202680110931}]}], "tableCaptions": [{"text": " Table 2: Spatial Roles -Comparison of A-Replacement with M0", "labels": [], "entities": []}, {"text": " Table 3: Spatial Roles -Comparison of A-Inference with M0+C", "labels": [], "entities": []}, {"text": " Table 4: Model Comparison -Spatial Relation Extraction", "labels": [], "entities": [{"text": "Spatial Relation Extraction", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.8759384950002035}]}, {"text": " Table 5: Coarse-grained Spatial Relations -Comparison of A-Replacement with M0.", "labels": [], "entities": []}, {"text": " Table 6: Coarse-grained Spatial Relations -Comparison of A-Inference with M0+C.", "labels": [], "entities": []}]}