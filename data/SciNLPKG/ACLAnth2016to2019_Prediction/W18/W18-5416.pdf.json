{"title": [{"text": "Interpreting Neural Networks with Nearest Neighbors", "labels": [], "entities": []}], "abstractContent": [{"text": "Local model interpretation methods explain individual predictions by assigning an importance value to each input feature.", "labels": [], "entities": [{"text": "Local model interpretation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.632244219382604}]}, {"text": "This value is often determined by measuring the change in confidence when a feature is removed.", "labels": [], "entities": []}, {"text": "However , the confidence of neural networks is not a robust measure of model uncertainty.", "labels": [], "entities": []}, {"text": "This issue makes reliably judging the importance of the input features difficult.", "labels": [], "entities": []}, {"text": "We address this by changing the test-time behavior of neural networks using Deep k-Nearest Neighbors.", "labels": [], "entities": []}, {"text": "Without harming text classification accuracy, this algorithm provides a more robust uncertainty metric which we use to generate feature importance values.", "labels": [], "entities": [{"text": "text classification", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7815139889717102}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.8561090230941772}]}, {"text": "The resulting interpretations better align with human perception than base-line methods.", "labels": [], "entities": []}, {"text": "Finally, we use our interpretation method to analyze model predictions on dataset annotation artifacts.", "labels": [], "entities": []}], "introductionContent": [{"text": "The growing use of neural networks insensitive domains such as medicine, finance, and security raises concerns about human trust in these machine learning systems.", "labels": [], "entities": []}, {"text": "A central question is testtime interpretability: how can humans understand the reasoning behind model predictions?", "labels": [], "entities": [{"text": "testtime interpretability", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6802679002285004}]}, {"text": "A common way to interpret neural network predictions is to identify the most important input features.", "labels": [], "entities": [{"text": "interpret neural network predictions", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.7529640048742294}]}, {"text": "For instance, a visual saliency map that highlights important pixels in an image) or words in a sentence ( ).", "labels": [], "entities": []}, {"text": "Given a model's test prediction, the importance of each input feature is the change in model confidence when that feature is removed.", "labels": [], "entities": []}, {"text": "However, neural network confidence is not a proper measure of model uncertainty (.", "labels": [], "entities": []}, {"text": "This issue is emphasized when models make highly confident predictions on inputs that * Equal contribution are completely void of information, for example, images of pure noise ( or meaningless text snippets.", "labels": [], "entities": []}, {"text": "Consequently, a model's confidence may not properly reflect whether discriminative input features are present.", "labels": [], "entities": []}, {"text": "This issue makes it difficult to reliably judge the importance of each input feature using common confidence-based interpretation methods.", "labels": [], "entities": []}, {"text": "To address this, we apply Deep k-Nearest Neighbors (DKNN)) to neural models for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8389001786708832}]}, {"text": "Concretely, predictions are no longer made with a softmax classifier, but using the labels of the training examples whose representations are most similar to the test example.", "labels": [], "entities": []}, {"text": "This provides an alternative metric for model uncertainty, conformity, which measures how much support a test prediction has by comparing its hidden representations to the training data.", "labels": [], "entities": [{"text": "conformity", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9645459651947021}]}, {"text": "This representationbased uncertainty measurement can be used in combination with existing interpretation methods, such as leave-one-out ( , to better identify important input features.", "labels": [], "entities": []}, {"text": "We combine DKNN with CNN and LSTM models on six NLP text classification tasks, including sentiment analysis and textual entailment, with no loss in classification accuracy (Section 4).", "labels": [], "entities": [{"text": "NLP text classification", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.5887755751609802}, {"text": "sentiment analysis", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.9526064395904541}, {"text": "textual entailment", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.6960026621818542}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.915415346622467}]}, {"text": "We compare interpretations generated using DKNN conformity to baseline interpretation methods, finding DKNN interpretations rarely assign importance to extraneous words that do not align with human perception (Section 5).", "labels": [], "entities": []}, {"text": "Finally, we generate interpretations using DKNN conformity fora dataset with known artifacts (SNLI), helping to indicate whether a model has learned superficial patterns.", "labels": [], "entities": []}, {"text": "We open source the code for DKNN and our results.", "labels": [], "entities": [{"text": "DKNN", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.891351044178009}]}], "datasetContent": [{"text": "We consider six common text classification tasks: binary sentiment analysis using Stanford Senti- Nearest Neighbor Search For accurate interpretations, we trade efficiency for accuracy and replace locally sensitive hashing () used by Papernot and McDaniel (2018) with a k-d tree.", "labels": [], "entities": [{"text": "text classification", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7536187469959259}, {"text": "binary sentiment analysis", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.7907591859499613}, {"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9988783001899719}]}, {"text": "We use k = 75 nearest neighbors at each layer.", "labels": [], "entities": []}, {"text": "The empirical results are robust to the choice of k.", "labels": [], "entities": []}, {"text": "Through DKNN, we get anew uncertainty measurement, conformity, that measures how a test example's representation is positioned relative to the training data representations.", "labels": [], "entities": [{"text": "DKNN", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.830010712146759}, {"text": "conformity", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9534385800361633}]}, {"text": "In this section, we use conformity leave-one-out to interpret a model trained on SNLI.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.6677359938621521}]}, {"text": "This dataset is known to contain annotation artifacts and we demonstrate that our interpretation method can help identify when models exploit these dataset biases.", "labels": [], "entities": []}, {"text": "Recent studies) identified annotation artifacts in the SNLI dataset.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.7428329735994339}]}, {"text": "These works identified that superficial patterns exist in the input which strongly correlate with certain labels, making it possible for models to \"game\" the task: obtain high accuracy without true understanding.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9956105351448059}]}, {"text": "For instance, the hypothesis of an entailment example is often a more general paraphrase of the premise, using words such as \"outside\" instead of \"playing soccer in a park\".", "labels": [], "entities": []}, {"text": "Contradiction examples often contain negation words or non-action verbs like \"sleeping\".", "labels": [], "entities": []}, {"text": "Models trained solely on the hypothesis can learn these patterns to achieve an accuracy considerably higher than the majority baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9992489218711853}]}, {"text": "These studies indicate that the SNLI task can be gamed.", "labels": [], "entities": [{"text": "SNLI task", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.8671807646751404}]}, {"text": "We look to confirm that some artifacts are indeed exploited by normally trained models that use full input pairs.", "labels": [], "entities": []}, {"text": "We create saliency maps for examples in the validation set using conformity leave-one-out.", "labels": [], "entities": []}, {"text": "shows samples and more can be found on the supplementary website.", "labels": [], "entities": []}, {"text": "We use the blue highlights to indicate words which positively support the model's predicted class, and the color red to indicate words that support a different class.", "labels": [], "entities": []}, {"text": "The first example is a randomly sampled baseline, showing how the words \"swims\" and \"pool\" support the model's prediction of contradiction.", "labels": [], "entities": []}, {"text": "The other examples are selected be-cause they contain terms identified as artifacts.", "labels": [], "entities": []}, {"text": "In the second example, conformity leave-one-out assigns extremely high word importance to \"sleeping\", disregarding other words necessary to predict Contradiction (i.e., the Neutral class is still possible if \"pets\" is replaced with \"people\").", "labels": [], "entities": [{"text": "Contradiction", "start_pos": 148, "end_pos": 161, "type": "METRIC", "confidence": 0.9280348420143127}]}, {"text": "In the final two hypothesis, the interpretation method diagnoses the model failure, assigning high importance to \"wearing\", rather than focusing positively on the shirt color.", "labels": [], "entities": []}, {"text": "To explore this further, we compute the average importance rank using conformity and confidence leave-one-out for the top five artifacts in each SNLI class identified by.", "labels": [], "entities": [{"text": "conformity", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9748695492744446}, {"text": "SNLI", "start_pos": 145, "end_pos": 149, "type": "TASK", "confidence": 0.8639482259750366}]}, {"text": "Table 4 compares the average rank assigned by the two methods, sorting the words by Pointwise Mutual Information as provided by.", "labels": [], "entities": []}, {"text": "The word \"nobody\" particularly stands out: it is the most important input word every time it appears in a contradiction example.", "labels": [], "entities": []}, {"text": "For most of the artifacts, conformity leave-oneout assigns them a high importance, often ranking the artifacts as the most important input word.", "labels": [], "entities": []}, {"text": "Confidence leave-one-out correlates less strongly with the known artifacts, frequently assigning importance values as low as fifth or sixth most important.", "labels": [], "entities": []}, {"text": "Given the high correlation between conformity leave-one-out and the manually identified artifacts, this interpretation method may serve as a technique to identify undesirable biases a model may have learned.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Replacing a neural network's softmax classifier with DKNN maintains classification accuracy  on standard text classification tasks.", "labels": [], "entities": [{"text": "classification", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.9430539011955261}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9426843523979187}, {"text": "text classification tasks", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.7873931924502054}]}, {"text": " Table 4: The top SNLI artifacts identified by Guru- rangan et al. (2018) are shown on the left. For each  word, we compute the average importance rank  over the validation set using either Conformity or  Confidence leave-one-out. A score of 1.0 indicates  that a word is always ranked as the most important  word in the input. Conformity leave-one-out as- signs stronger importance to artifacts, suggesting  it better diagnoses model biases.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.9415101408958435}]}]}