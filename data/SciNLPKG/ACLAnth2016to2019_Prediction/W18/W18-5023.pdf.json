{"title": [{"text": "Discourse Coherence in the Wild: A Dataset, Evaluation and Methods", "labels": [], "entities": []}], "abstractContent": [{"text": "To date there has been very little work on assessing discourse coherence methods on real-world data.", "labels": [], "entities": []}, {"text": "To address this, we present anew corpus of real-world texts (GCDC) as well as the first large-scale evaluation of leading discourse coherence algorithms.", "labels": [], "entities": []}, {"text": "We show that neural models , including two that we introduce here (SENTAVG and PARSEQ), tend to perform best.", "labels": [], "entities": [{"text": "SENTAVG", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.971325159072876}, {"text": "PARSEQ", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.985109806060791}]}, {"text": "We analyze these performance differences and discuss patterns we observed in low coherence texts in four domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse coherence is an important aspect of text quality.", "labels": [], "entities": []}, {"text": "It encompasses how sentences are connected as well as how the entire document is organized to convey information to the reader.", "labels": [], "entities": []}, {"text": "Developing discourse coherence models to distinguish coherent writing from incoherent writing is useful to a range of applications.", "labels": [], "entities": []}, {"text": "An automated coherence scoring model could provide writing feedback, e.g. identifying a missing transition between topics or highlighting a poorly organized paragraph.", "labels": [], "entities": []}, {"text": "Such a model could also improve the quality of natural language generation systems.", "labels": [], "entities": []}, {"text": "One approach to modeling coherence is to model the distribution of entities over sentences.", "labels": [], "entities": []}, {"text": "The entity grid (), based on Centering Theory (, was the first of these models.", "labels": [], "entities": []}, {"text": "Extensions to the entity grid include additional features), a graph representation (, and neural convolutions.", "labels": [], "entities": []}, {"text": "Other approaches have used lexical cohesion (Morris and Hirst, * Research performed while at Grammarly.", "labels": [], "entities": [{"text": "Grammarly", "start_pos": 93, "end_pos": 102, "type": "DATASET", "confidence": 0.9172834157943726}]}, {"text": "1991;), discourse relations (, and syntactic features.", "labels": [], "entities": []}, {"text": "Neural networks have also been successfully applied to coherence.", "labels": [], "entities": []}, {"text": "However, until now, these approaches have not been benchmarked on a common dataset.", "labels": [], "entities": []}, {"text": "Past work has focused on the discourse coherence of well-formed texts in domains like newswire () via tasks like sentence ordering that use artificially constructed data.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7620974481105804}]}, {"text": "It was unknown how well the best methods would fare on real-world data that most people generate.", "labels": [], "entities": []}, {"text": "In this work, we seek to address the above deficiencies via four main contributions.", "labels": [], "entities": []}, {"text": "First, we present anew corpus, the Grammarly Corpus of Discourse Coherence (GCDC), for real-world discourse coherence.", "labels": [], "entities": [{"text": "Grammarly Corpus of Discourse Coherence (GCDC)", "start_pos": 35, "end_pos": 81, "type": "TASK", "confidence": 0.4899388812482357}]}, {"text": "The corpus contains texts the average person might write, e.g. emails and online reviews, each with a coherence rating from expert annotators (see examples in and supplementary material).", "labels": [], "entities": []}, {"text": "Second, we introduce two simple yet effective neural network models to score coherence.", "labels": [], "entities": []}, {"text": "Third, we perform the first large-scale benchmarking of 7 leading coherence algorithms.", "labels": [], "entities": []}, {"text": "We show that prior models, which performed at a very high level on well-formed and artificially generated data, have markedly lower performance in these new domains.", "labels": [], "entities": []}, {"text": "Finally, the data, annotation guidelines, and code have all been made public.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the models on multiple coherence prediction tasks.", "labels": [], "entities": [{"text": "coherence prediction", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7380029261112213}]}, {"text": "The best model parameters, reported in the supplementary material, are the result of 10-fold cross-validation over the training data.", "labels": [], "entities": []}, {"text": "For all neural models (EGRIDCONV, EGRID-CONV +coref, CLIQUE, SENTAVG, and PARSEQ), the reported results are the mean of 10 runs with different random seeds, as suggested by.", "labels": [], "entities": [{"text": "SENTAVG", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9841283559799194}, {"text": "PARSEQ", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9885507225990295}]}, {"text": "We indicate ( \u2020) when the best neural model result is significantly better (p < 0.05) than the best non-neural result.", "labels": [], "entities": []}, {"text": "We use the one-sample Wilcoxon signed rank test and adjusted the pvalues to account for the false discovery rate.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Distribution of coherence classes as a per- centage of the training data.", "labels": [], "entities": []}, {"text": " Table 3: Type and token counts in each domain.", "labels": [], "entities": []}, {"text": " Table 4: Interannotator agreement (mean and stan- dard deviation) on all domains.", "labels": [], "entities": [{"text": "mean and stan- dard deviation)", "start_pos": 36, "end_pos": 66, "type": "METRIC", "confidence": 0.8035684738840375}]}, {"text": " Table 5: Three-way classification results on test.", "labels": [], "entities": []}, {"text": " Table 6: Score prediction results on test.", "labels": [], "entities": [{"text": "Score prediction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7758850455284119}]}, {"text": " Table 7: Sentence ordering results on test data.", "labels": [], "entities": [{"text": "Sentence ordering", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9490318596363068}]}, {"text": " Table 8: Minority class predictions, F 0.5 score on  test data.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9833266337712606}]}, {"text": " Table 9: Cross-domain accuracy of PARSEQ on  three-way classification test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9958926439285278}, {"text": "PARSEQ", "start_pos": 35, "end_pos": 41, "type": "TASK", "confidence": 0.4363919496536255}]}, {"text": " Table 10: Classification accuracy of PARSEQ  when trained on data from all four domains.", "labels": [], "entities": [{"text": "Classification", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.8528407216072083}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.976175844669342}, {"text": "PARSEQ", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.3457515239715576}]}]}