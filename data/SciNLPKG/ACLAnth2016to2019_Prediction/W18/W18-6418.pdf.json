{"title": [], "abstractContent": [{"text": "In the current work, we present a description of the system submitted to WMT 2018 News Translation Shared task.", "labels": [], "entities": [{"text": "WMT 2018 News Translation Shared task", "start_pos": 73, "end_pos": 110, "type": "TASK", "confidence": 0.685857872168223}]}, {"text": "The system was created to translate news text from Finnish to En-glish.", "labels": [], "entities": []}, {"text": "The system used a Character Based Neural Machine Translation model to accomplish the given task.", "labels": [], "entities": [{"text": "Character Based Neural Machine Translation", "start_pos": 18, "end_pos": 60, "type": "TASK", "confidence": 0.597186279296875}]}, {"text": "The current paper documents the preprocessing steps, the description of the submitted system and the results produced using the same.", "labels": [], "entities": []}, {"text": "Our system garnered a BLEU score of 12.9.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9800586104393005}]}], "introductionContent": [{"text": "Machine Translation (MT) is automated translation of one natural language to another using computer software.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8812801003456116}]}, {"text": "Translation is a tough task, not only for computers, but humans as well as it incorporates a thorough understanding of the syntax and semantics of both languages.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9595946073532104}]}, {"text": "For any MT system to return good translations, it needs good quality and sufficient amount of parallel corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9835548996925354}]}, {"text": "In the modern context, MT systems can be categorized into Statistical Machine Translation (SMT) and Neural Machine Translation (NMT).", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9903337359428406}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.7656196306149164}, {"text": "Neural Machine Translation (NMT)", "start_pos": 100, "end_pos": 132, "type": "TASK", "confidence": 0.8211412926514944}]}, {"text": "SMT has had its share in making MT very popular among the masses.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8294827342033386}, {"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9902133345603943}]}, {"text": "It includes creating statistical models, whose input parameters are derived from the analysis of bilingual text corpora, created by professional translators.", "labels": [], "entities": []}, {"text": "The state-of-art for SMT is Moses Toolkit 1 , created by, incorporates subcomponents like Language Model generation, Word Alignment and Phrase Table generation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9946486353874207}, {"text": "Language Model generation", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.638676385084788}, {"text": "Word Alignment", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.7038044780492783}, {"text": "Phrase Table generation", "start_pos": 136, "end_pos": 159, "type": "TASK", "confidence": 0.6949190497398376}]}, {"text": "Various works have been done in SMT and it has shown good results for many language pairs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9932793974876404}]}, {"text": "1 http://www.statmt.org/moses/ On the other hand NMT ( ), though relatively new, has shown considerable improvements in the translation results when compared to SMT.", "labels": [], "entities": [{"text": "translation", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.9505285024642944}, {"text": "SMT", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9759445190429688}]}, {"text": "This includes better fluency of the output and better handling of the Out-of-Vocabulary problem.", "labels": [], "entities": []}, {"text": "Unlike SMT, it doesn't depend on alignment and phrasal unit translations).", "labels": [], "entities": [{"text": "SMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9891101121902466}]}, {"text": "On the contrary, it uses an EncoderDecoder approach incorporating Recurrent Neural Cells ( ).", "labels": [], "entities": []}, {"text": "As a result, when given sufficient amount of training data, it gives much more accurate results when compared to SMT ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.974631130695343}]}, {"text": "Further, NMT can be of two types, namely Word Level NMT and Character Level NMT.", "labels": [], "entities": []}, {"text": "Word Level NMT, though very successful, suffers from a few disadvantages.", "labels": [], "entities": [{"text": "Word Level NMT", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8205128113428751}]}, {"text": "It are unable to model rare words (.", "labels": [], "entities": []}, {"text": "Also, since it does not learn the morphological structure of a language it suffers when accommodating morphologically rich languages (.", "labels": [], "entities": []}, {"text": "We can address this issue, by training the models with huge parallel corpus, but, this in turn, produces very complex and resource consuming models that aren't feasible enough.", "labels": [], "entities": []}, {"text": "To combat this, we plan to use Character level NMT, so that it can learn the morphological aspects of a language and construct a word, character by character, and hence tackle the rare word occurrence problem to some extent.", "labels": [], "entities": []}, {"text": "In the current work, we participated in the WMT 2018 News Translation Shared Task 2 that focused on translating news text, for European language pairs.", "labels": [], "entities": [{"text": "WMT 2018 News Translation Shared Task", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.804259330034256}, {"text": "translating news text", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.8562872012456259}]}, {"text": "The Character Based NMT system discussed in this paper was designed to accommodate Finnish to English translations.", "labels": [], "entities": []}, {"text": "The orga-nizers provided the required parallel corpora, consisting of 3,255,303 sentence pairs, for training the translation model.", "labels": [], "entities": [{"text": "translation", "start_pos": 113, "end_pos": 124, "type": "TASK", "confidence": 0.9639078974723816}]}, {"text": "The statistics of the parallel corpus is depicted in Our model was trained on a Tesla K40 GPU, and the training took around 10 days to complete.", "labels": [], "entities": []}, {"text": "Statistics of the Finnish-English parallel corpus provided by the organizers.", "labels": [], "entities": [{"text": "Finnish-English parallel corpus", "start_pos": 18, "end_pos": 49, "type": "DATASET", "confidence": 0.5875444610913595}]}, {"text": "\"Fi\" and \"En\" depict Finnish and English, respectively.", "labels": [], "entities": []}, {"text": "\"char\" means character and \"vocab\" means vocabulary of unique tokens.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}