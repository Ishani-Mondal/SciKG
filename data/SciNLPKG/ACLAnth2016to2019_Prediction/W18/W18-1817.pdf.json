{"title": [{"text": "OpenNMT: Neural Machine Translation Toolkit", "labels": [], "entities": [{"text": "Neural Machine Translation Toolkit", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.6773792132735252}]}], "abstractContent": [{"text": "OpenNMT is an open-source toolkit for neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.8246884644031525}]}, {"text": "The system prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements.", "labels": [], "entities": []}, {"text": "The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.922248899936676}]}, {"text": "OpenNMT has been used in several production MT systems, modified for numerous research papers, and is implemented across several deep learning frameworks.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9621806144714355}]}], "introductionContent": [{"text": "Neural machine translation (NMT) is anew methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7691584775845209}, {"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.782765805721283}, {"text": "statistical machine translation (SMT)", "start_pos": 188, "end_pos": 225, "type": "TASK", "confidence": 0.8137485881646475}]}, {"text": "Originally developed using pure sequence-to-sequence models) and improved upon using attention-based variants (), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.811072438955307}, {"text": "parsing", "start_pos": 264, "end_pos": 271, "type": "TASK", "confidence": 0.7424623370170593}, {"text": "summarization", "start_pos": 277, "end_pos": 290, "type": "TASK", "confidence": 0.9293457269668579}]}, {"text": "As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7549881339073181}]}, {"text": "Just as the SMT community benefited greatly from toolkits like Moses () for phrase-based SMT and CDec () for syntax-based SMT, NMT toolkits can provide a foundation to build upon.", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9800674319267273}, {"text": "phrase-based SMT", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.5424816459417343}, {"text": "SMT", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.8234797716140747}]}, {"text": "A toolkit should aim to provide a shared framework for developing and comparing open-source systems, while at the same time being efficient and accurate enough to be used in production contexts.", "labels": [], "entities": []}, {"text": "With these goals in mind, in this work we present an open-source toolkit for developing neural machine translation systems, known as OpenNMT (http://opennmt.net).", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.7144974867502848}]}, {"text": "Since its launch in December 2016, OpenNMT has become a collection of implementations targeting both academia and industry.", "labels": [], "entities": []}, {"text": "The system is designed to be simple to use and easy to extend, while maintaining efficiency and state-of-the-art accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9982536435127258}]}, {"text": "In addition to providing code for the core translation tasks, OpenNMT was designed with two aims: (a) prioritize training and test efficiency, (b) maintain model modularity and readability hence research extensibility.", "labels": [], "entities": []}, {"text": "During this time, many other stellar open-source NMT implementations have also been  In the ongoing development of OpenNMT, we aim to build upon the strengths of those systems, while supporting a framework with high-accuracy translation, multiple options and clear documentation.", "labels": [], "entities": []}, {"text": "This engineering report describes how the system targets our design goals.", "labels": [], "entities": []}, {"text": "We begin by briefly surveying the background for NMT, and then describing the high-level implementation details.", "labels": [], "entities": [{"text": "NMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.6257249116897583}]}, {"text": "We end by showing benchmarks of the system in terms of accuracy, speed, and memory usage for several translation and natural language generation tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9994117021560669}, {"text": "speed", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9591893553733826}, {"text": "translation and natural language generation", "start_pos": 101, "end_pos": 144, "type": "TASK", "confidence": 0.7748271703720093}]}], "datasetContent": [{"text": "OpenNMT achieves competitive results against other systems, e.g. in the recent WMT 2017 translation task, it won third place in English-German translation with a single model as shown in.", "labels": [], "entities": [{"text": "WMT 2017 translation task", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.673241376876831}]}, {"text": "The system is also competitive in speed as shown in.", "labels": [], "entities": [{"text": "speed", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9972746968269348}]}, {"text": "Here we compare training and test speed to the publicly available Nematus system 1 on English-to-German (EN\u2192DE) using the WMT2015 2 dataset.", "labels": [], "entities": [{"text": "WMT2015 2 dataset", "start_pos": 122, "end_pos": 139, "type": "DATASET", "confidence": 0.9275452693303426}]}, {"text": "We have found that OpenNMT's default setting is useful for experiments, but not optimal for large-scale NMT.", "labels": [], "entities": []}, {"text": "This has been a cause of poor reported performance in other default comparisons by and.", "labels": [], "entities": []}, {"text": "We trained models with our best effort to conform to their settings and report our results in, which shows comparable performance with other systems.", "labels": [], "entities": []}, {"text": "We suspect that the reported poor performance is due to the fact that our default setting discards sequences of length greater than 50, which is too short for BPE.", "labels": [], "entities": [{"text": "BPE", "start_pos": 159, "end_pos": 162, "type": "DATASET", "confidence": 0.505864679813385}]}, {"text": "Moreover, while the reported poor performance was obtained by training with ADAM, we find that training with (the default) SGD with learning rate decay is generally better.", "labels": [], "entities": [{"text": "learning rate decay", "start_pos": 132, "end_pos": 151, "type": "METRIC", "confidence": 0.8848925828933716}]}, {"text": "We also compare OpenNMT with the GNMT () model in. have established anew state-of-the-art with the Transformer model.", "labels": [], "entities": []}, {"text": "We have also implemented this in our framework, and compare it with Tensor2Tensor (T2T) in.", "labels": [], "entities": []}, {"text": "(These experiments are run on a modified version of WMT 2017, namely News Comm v11 instead of v12, and no Rapid 2016.)", "labels": [], "entities": [{"text": "WMT 2017", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9706327021121979}, {"text": "News Comm v11", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.8968307773272196}]}, {"text": "Additionally we have found interest from the community in using OpenNMT for language geneation tasks like sentence document summarization and dialogue response generation, among others.", "labels": [], "entities": [{"text": "sentence document summarization", "start_pos": 106, "end_pos": 137, "type": "TASK", "confidence": 0.6421956022580465}, {"text": "dialogue response generation", "start_pos": 142, "end_pos": 170, "type": "TASK", "confidence": 0.7625282406806946}]}, {"text": "Using OpenNMT, we were able to replicate the sentence summarization results of, reaching a ROUGE-1 score of 35.51 on the Gigaword data.", "labels": [], "entities": [{"text": "sentence summarization", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.6368511319160461}, {"text": "ROUGE-1", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9984984397888184}, {"text": "Gigaword data", "start_pos": 121, "end_pos": 134, "type": "DATASET", "confidence": 0.967582643032074}]}, {"text": "We have also trained a model on 14 million sentences of the OpenSubtitles data set based on the work, achieving comparable perplexity.", "labels": [], "entities": [{"text": "OpenSubtitles data set", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.9588218132654825}]}, {"text": "Many other models are at http://opennmt.net/Models-py and http://opennmt.net/Models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top 3 on English-German newstest2017 WMT17.", "labels": [], "entities": [{"text": "English-German newstest2017 WMT17", "start_pos": 19, "end_pos": 52, "type": "DATASET", "confidence": 0.6694775025049845}]}, {"text": " Table 2: Performance results for EN\u2192DE on", "labels": [], "entities": [{"text": "DE", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.6580420136451721}]}, {"text": " Table 4: Comparison with GNMT on EN\u2192DE.", "labels": [], "entities": []}]}