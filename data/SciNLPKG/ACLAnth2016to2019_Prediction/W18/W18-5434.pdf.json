{"title": [], "abstractContent": [{"text": "PatternAttribution is a recent method, introduced in the vision domain, that explains classifications of deep neural networks.", "labels": [], "entities": [{"text": "classifications of deep neural networks", "start_pos": 86, "end_pos": 125, "type": "TASK", "confidence": 0.7839179992675781}]}, {"text": "We demonstrate that it also generates meaningful interpretations in the language domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last decade, deep neural classifiers achieved state-of-the-art results in many domains, among others in vision and language.", "labels": [], "entities": []}, {"text": "Due to the complexity of a deep neural model, however, it is difficult to explain its decisions.", "labels": [], "entities": []}, {"text": "Understanding its decision process potentially allows to improve the model and may reveal new knowledge about the input.", "labels": [], "entities": []}, {"text": "Recently, claimed that \"popular explanation approaches for neural networks (...) do not provide the correct explanation, even fora simple linear model.\"", "labels": [], "entities": []}, {"text": "They show that in a linear model, the weights serve to cancel noise in the input data and thus the weights show how to extract the signal but not what the signal is.", "labels": [], "entities": []}, {"text": "This is why explanation methods need to move beyond the weights, the authors explain, and they propose the methods \"PatternNet\" and \"PatternAttribution\" that learn explanations from data.", "labels": [], "entities": []}, {"text": "We test their approach in the language domain and point to room for improvement in the new framework.", "labels": [], "entities": []}, {"text": "assume that the data x passed to a linear model w T x = y is composed of signal (s) and noise (d, from distraction) x = s+d.", "labels": [], "entities": []}, {"text": "Furthermore, they also assume that there is a linear relation between signal and target ya s = s where a sis a so called signal base vector, which is in fact the \"pattern\" that PatternNet finds for us.", "labels": [], "entities": []}, {"text": "As mentioned in the introduction, the authors show that in the model above, w serves to cancel the noise such that", "labels": [], "entities": []}], "datasetContent": [{"text": "To test PatternAttribution in the NLP domain, we trained a CNN text classifier) on a subset of the Amazon review polarity data set (.", "labels": [], "entities": [{"text": "Amazon review polarity data set", "start_pos": 99, "end_pos": 130, "type": "DATASET", "confidence": 0.8701804041862488}]}, {"text": "We used 150 bigram filters, dropout regularization and a dense FC projection with 128 neurons.", "labels": [], "entities": [{"text": "dropout regularization", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.6924516707658768}]}, {"text": "Our classifier achieves an F 1 score of 0.875 on a fixed test split.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.992492139339447}]}, {"text": "We then used PatternAttribution to retrieve neuron-wise signal contributions in the input vector space.", "labels": [], "entities": []}, {"text": "To align these contributions with plain text, we summed up the contribution scores over the word vector dimensions for each word and used the accumulated scores to scale RGB values for word highlights in the plain text space.", "labels": [], "entities": []}, {"text": "Positive scores are highlighted in red, negative scores in blue.", "labels": [], "entities": []}, {"text": "This approach is inspired by.", "labels": [], "entities": []}, {"text": "Example contributions are shown in.", "labels": [], "entities": []}], "tableCaptions": []}