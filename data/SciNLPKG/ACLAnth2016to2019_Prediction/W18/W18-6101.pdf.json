{"title": [{"text": "Inducing a lexicon of sociolinguistic variables from code-mixed text", "labels": [], "entities": []}], "abstractContent": [{"text": "Sociolinguistics is often concerned with how variants of a linguistic item (e.g., nothing vs. nothin') are used by different groups or in different situations.", "labels": [], "entities": []}, {"text": "We introduce the task of inducing lexical variables from code-mixed text: that is, identifying equivalence pairs such as (football, fitba) along with their linguistic code (football\u2192British, fitba\u2192Scottish).", "labels": [], "entities": []}, {"text": "We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of English dialects , using tweets as the code-mixed text.", "labels": [], "entities": []}, {"text": "Our system achieves precision of over 70% for two of these three datasets, and produces useful results even without extensive parameter tuning.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9995007514953613}]}, {"text": "Our success in adapting this framework from gender to language variety suggests that it could be used to discover other types of analogous pairs as well.", "labels": [], "entities": []}], "introductionContent": [{"text": "Large social media corpora are increasingly used to study variation in informal written language).", "labels": [], "entities": []}, {"text": "An outstanding methodological challenge in this area is the bottomup discovery of sociolinguistic variables: linguistic items with identifiable variants that are correlated with social or contextual traits such as class, register, or dialect.", "labels": [], "entities": []}, {"text": "For example, the choice of the term rabbit versus bunny might correlate with audience or style, while fitba is a characteristically Scottish variant of the more general British football.", "labels": [], "entities": []}, {"text": "To date, most large-scale social media studies have studied the usage of individual variant forms.", "labels": [], "entities": []}, {"text": "Studying how a variable alternates between its variants controls better for 'Topic Bias', but identifying the relevant variables/variants may not be straightforward.", "labels": [], "entities": []}, {"text": "For example, used a datadriven method to identify distinctively Scottish terms, and then manually paired them with Standard English equivalents, a labour intensive process that requires good familiarity with both language varieties.", "labels": [], "entities": []}, {"text": "Our aim is to facilitate the process of curating sociolinguistic variables by providing researchers with a ranked list of candidate variant pairs, which they only have to accept or reject.", "labels": [], "entities": []}, {"text": "This task, which we term lexical variable induction, can be viewed as a type of bilingual lexicon induction (.", "labels": [], "entities": [{"text": "lexical variable induction", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.7122511863708496}]}, {"text": "However, while most work in that area assumes that monolingual corpora are available and labeled according to which language they belong to, in our setting there is a single corpus containing code-mixed text, and we must identify both translation equivalents (football, fitba) as well as linguistic code (football\u2192British, fitba\u2192Scottish).", "labels": [], "entities": []}, {"text": "To illustrate, here are some excerpts of tweets from the Scottish dataset analysed by with Standard English glosses in italics: 1.", "labels": [], "entities": [{"text": "Scottish dataset analysed", "start_pos": 57, "end_pos": 82, "type": "DATASET", "confidence": 0.9658658305803934}]}, {"text": "need to come hame fae the football need to come home from the football 2.", "labels": [], "entities": []}, {"text": "miss the fitba miss the football 3.", "labels": [], "entities": []}, {"text": "awwww man a wanty go tae the fitbaw awwww man I want to go to the football The lexical variable induction task is challenging: we cannot simply classify documents containing fitba as Scottish, since the football variant may also occur in otherwise distinctively Scottish texts, as in (1).", "labels": [], "entities": []}, {"text": "Moreover, if we start by knowing only a few variables, we would like away to learn what other likely variables might be.", "labels": [], "entities": []}, {"text": "Had we not known the (football, fitba) variable, we might not detect that (2) was distinctively Scottish.", "labels": [], "entities": []}, {"text": "Our proposed system can make identifying variants quicker and also suggest variant pairs a researcher might not have otherwise considered, such as (football, fitbaw) which could be learned from tweets like (3).", "labels": [], "entities": []}, {"text": "Our task can also be viewed as the converse of the one addressed by, who proposed a method to identify geographical regions associated with different linguistic codes, using pre-defined lexical variables.", "labels": [], "entities": []}, {"text": "Also complementary is the work of, who identified words which have the same form but different semantics across different linguistic codes; here, we seek to identify words which have the same semantics but different forms.", "labels": [], "entities": []}, {"text": "We frame our task as a ranking problem, aiming to generate a list where the best-ranked pairs consist of words that belong to different linguistic codes, but are otherwise semantically and syntactically equivalent.", "labels": [], "entities": []}, {"text": "Our approach is inspired by the work of Schmidt (2015) and, who sought to identify pairs of words that exhibit gender bias in their distributional statistics, but are otherwise semantically equivalent.", "labels": [], "entities": []}, {"text": "Their methods differ in the details but use a similar framework: they start with one or more seed pairs such as {(he, she), (man, woman)} and use these to extract a 'gender' component of the embedding space, which is then used to find and rank additional pairs.", "labels": [], "entities": []}, {"text": "Here, we replace the gendered seed pairs with pairs of sociolinguistic variants corresponding to the same variable, such as {(from, fae), (football, fitba)}.", "labels": [], "entities": []}, {"text": "In experiments on three different datasets of mixed English dialects, we demonstrate useful results over a range of hyperparameter settings, with precision@100 of over 70% in some cases using as few as five seed pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9990805387496948}]}, {"text": "These results indicate that the embedding space contains structured information not only about gendered usage, but also about other social aspects of language, and that this information can potentially be used as part of a sociolinguistic researcher's toolbox.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our methods on three pairs of language varieties: British English vs Scots/Scottish English; British English vs General American English; and General American English vs African American Vernacular English (AAVE).", "labels": [], "entities": [{"text": "General American English vs African American Vernacular English (AAVE)", "start_pos": 150, "end_pos": 220, "type": "TASK", "confidence": 0.6427418318661776}]}, {"text": "Within each data set, individual tweets may contain words from one or both codes of interest, and the only words with a known linguistic code (or which are known to have a corresponding word in the other code) are those in the seed pairs.", "labels": [], "entities": []}, {"text": "BrEng/Scottish For our first test case, we combined the two datasets collected by, consisting of complete tweet histories from Aug-Oct 2014 by users who had posted at least one tweet in the preceding year geotagged to a location in Scotland, or that contained a hashtag relating to the 2014 Scottish Independence referendum.", "labels": [], "entities": [{"text": "BrEng/Scottish", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.7840624451637268}, {"text": "Scottish Independence referendum", "start_pos": 291, "end_pos": 323, "type": "DATASET", "confidence": 0.7850343386332194}]}, {"text": "The corpus contains 9.4M tweets.", "labels": [], "entities": []}, {"text": "BrEng/GenAm For our next test case we recreated the entire process of collecting data and seed variables from scratch.", "labels": [], "entities": [{"text": "BrEng", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6222038865089417}, {"text": "GenAm", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.6129394769668579}]}, {"text": "We extracted 8.3M tweets geotagged to locations in the USA from a three-year archive of the public 1% sample of Twitter.", "labels": [], "entities": []}, {"text": "All tweets were classified as English by langid.py, none are retweets, none contain URLs or embedded media, and none are by users with more than 1000 friends or followers.", "labels": [], "entities": []}, {"text": "We combined this data with a similarly constructed corpus of 1.7M tweets geotagged to the UK and posted between 1 Sep 2013 and 30 Sep 2014.", "labels": [], "entities": []}, {"text": "To create the seed pairs, we followed Shoemark et al.", "labels": [], "entities": []}, {"text": "(2017b) and used the Sparse Additive Generative Model of Text (SAGE)) to identify the terms that were most distinctive to UK or US tweets.", "labels": [], "entities": []}, {"text": "However, most of these terms turned out to represent specific dialects within each country, rather than the standard BrEng or GenAm dialects (we discuss this issue further below).", "labels": [], "entities": [{"text": "BrEng", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.6612473726272583}, {"text": "GenAm dialects", "start_pos": 126, "end_pos": 140, "type": "DATASET", "confidence": 0.8206543028354645}]}, {"text": "We therefore manually searched through the UK terms to identify those that are standard BrEng and diframeters for each language pair: m = 20, n = 20k for BrEng/Scottish; m = 5, n = 5k for GenAm/AAVE; and m = 10, n = 5k for BrEng/GenAm.", "labels": [], "entities": [{"text": "BrEng", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.6754891872406006}, {"text": "BrEng/Scottish", "start_pos": 154, "end_pos": 168, "type": "DATASET", "confidence": 0.6483192245165507}, {"text": "GenAm/AAVE", "start_pos": 188, "end_pos": 198, "type": "DATASET", "confidence": 0.7781920830408732}, {"text": "BrEng/GenAm", "start_pos": 223, "end_pos": 234, "type": "DATASET", "confidence": 0.7079755663871765}]}, {"text": "fer from GenAm by spelling only, and paired each one with its GenAm spelling variant, e.g. (color, colour), (apologize, apologise), (pajamas, pyjamas).", "labels": [], "entities": [{"text": "GenAm", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.9427912831306458}]}, {"text": "This process involved looking through thousands of words to identify only 27 pairs (listed in the Supplement), which is a strong motivator for our proposed method to more efficiently increase the number of pairs.", "labels": [], "entities": []}, {"text": "GenAm/AAVE While creating the previous dataset, we noticed that many of the terms identified by SAGE as distinctively American were actually from AAVE.", "labels": [], "entities": [{"text": "GenAm/AAVE", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7710356513659159}]}, {"text": "To create our GenAm/AAVE seed pairs, we manually cross-referenced the most distinctively 'American' terms with the AAVE phonological processes described by.", "labels": [], "entities": [{"text": "GenAm/AAVE seed", "start_pos": 14, "end_pos": 29, "type": "DATASET", "confidence": 0.7971497774124146}]}, {"text": "We then selected terms that reflected these processes, paired with their GenAm equivalents, e.g. (about, bou), (brother, brudda).", "labels": [], "entities": []}, {"text": "The full list of 19 open-class and 20 closed-class seed pairs is included in the Supplement.", "labels": [], "entities": [{"text": "Supplement", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.652507483959198}]}, {"text": "We evaluate our systems using Precision@K, the percentage of the top K ranked word pairs judged to be valid sociolinguistic variables.", "labels": [], "entities": [{"text": "Precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9088276028633118}]}, {"text": "We discard any seed pairs from the output before computing precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9980681538581848}]}, {"text": "Since we have no gold standard translation dictionaries for our domains of interest, each of the top-K pairs was manually classified as either valid or invalid by the first author.", "labels": [], "entities": []}, {"text": "For a pair to be judged as valid, (a) each member must be strongly associated with one or the other language variety, (b) they must be referentially, functionally, and syntactically equivalent, and (c) the two words must be ordered correctly according to their language varieties, e.g. if the seeds were (BrEng, GenAm) pairs, then the BrEng words should also come first in the top-K output pairs.", "labels": [], "entities": [{"text": "BrEng", "start_pos": 335, "end_pos": 340, "type": "METRIC", "confidence": 0.9425167441368103}]}, {"text": "Evaluation judgements were based on the author's knowledge of the language varieties in question; for unfamiliar terms, tweets containing the terms were sampled and manually inspected, and cross-referenced with urbandictionary.com and/or existing sociolinguistic literature.", "labels": [], "entities": []}, {"text": "Our strict criteria exclude pairs like (dogs, dug) which differ in their inflection, or (quid, dollar) whose referents are distinct but are equivalent across cultures.", "labels": [], "entities": []}, {"text": "In many cases it was difficult to judge whether or not a pair should be accepted, such as when not all senses of the words were interchangable, e.g. BrEng/GenAm (folk, folks) works for the 'people' sense of folk, but not the adjectival sense: (folk music, *folks music).", "labels": [], "entities": [{"text": "BrEng", "start_pos": 149, "end_pos": 154, "type": "METRIC", "confidence": 0.989943265914917}]}, {"text": "The BrEng/GenAm dataset also yielded many pairs of words that exhibit different frequencies of usage in the two countries, but where both words are part of both dialects, such as (massive, huge), (vile, disgusting), and (horrendous, awful).", "labels": [], "entities": [{"text": "BrEng/GenAm dataset", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7632377445697784}]}, {"text": "We generally marked these as incorrect, although the line between these pairs and clear-cut lexical alternations is fuzzy.", "labels": [], "entities": []}, {"text": "For some applications, it maybe desirable to retrieve pairs like these, in which case the precision scores we report here are very conservative.", "labels": [], "entities": [{"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9993041753768921}]}], "tableCaptions": []}