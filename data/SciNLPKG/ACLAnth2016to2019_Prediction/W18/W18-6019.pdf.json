{"title": [{"text": "Transition-based Parsing with Lighter Feed-Forward Networks", "labels": [], "entities": [{"text": "Transition-based Parsing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6015948951244354}]}], "abstractContent": [{"text": "We explore whether it is possible to build lighter parsers, that are statistically equivalent to their corresponding standard version, fora wide set of languages showing different structures and morphologies.", "labels": [], "entities": []}, {"text": "As testbed, we use the Universal Dependencies and transition-based dependency parsers trained on feed-forward networks.", "labels": [], "entities": []}, {"text": "For these, most existing research assumes de facto standard embedded features and relies on pre-computation tricks to obtain speed-ups.", "labels": [], "entities": []}, {"text": "We explore how these features and their size can be reduced and whether this translates into speed-ups with a negligible impact on accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9980158805847168}]}, {"text": "The experiments show that granddaughter features can be removed for the majority of treebanks without a significant (negative or positive) LAS difference.", "labels": [], "entities": [{"text": "LAS", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9817513823509216}]}, {"text": "They also show how the size of the embeddings can be notably reduced.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transition-based models have achieved significant improvements in the last decade.", "labels": [], "entities": []}, {"text": "Some of them already achieve a level of agreement similar to that of experts on English newswire texts, although this does not generalize to other configurations (e.g. lower-resource languages).", "labels": [], "entities": [{"text": "agreement", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9696328639984131}]}, {"text": "These higher levels of accuracy often come at higher computational costs and lower bandwidths, which can be a disadvantage for scenarios where speed is more relevant than accuracy ( . Furthermore, running neural models on small devices for tasks such as part-of-speech tagging or word segmentation has become a matter of study (, showing that small feed-forward networks are suitable for these challenges.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9968035221099854}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9960072636604309}, {"text": "part-of-speech tagging", "start_pos": 254, "end_pos": 276, "type": "TASK", "confidence": 0.7190299332141876}, {"text": "word segmentation", "start_pos": 280, "end_pos": 297, "type": "TASK", "confidence": 0.7004310786724091}]}, {"text": "However, for parsers that are trained using neural networks, little exploration has been done beyond the application of pre-computation tricks, initially intended for fast neural machine translation (, at a cost of affordable but larger memory.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.7974595725536346}]}, {"text": "Contribution We explore efficient and light dependency parsers for languages with a variety of structures and morphologies.", "labels": [], "entities": []}, {"text": "We rely on neural feed-forward dependency parsers, since their architecture offers a competitive accuracy vs bandwidth ratio and they are also the inspiration for more complex parsers, which also rely on embedded features but previously processed by bidirectional.", "labels": [], "entities": [{"text": "neural feed-forward dependency parsers", "start_pos": 11, "end_pos": 49, "type": "TASK", "confidence": 0.7050572037696838}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9967142343521118}]}, {"text": "In particular, we study if the de facto standard embedded features and their sizes can be reduced without having a significant impact on their accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9958431124687195}]}, {"text": "Building these models is of help in downstream applications of natural language processing, such as those running on small devices and also of interest for syntactic parsing itself, as it makes it possible to explore how the same configuration affects different languages.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.8038171827793121}]}, {"text": "This study is made on the Universal Dependencies v2.1, a testbed that allows us to compare a variety of languages annotated following common guidelines.", "labels": [], "entities": [{"text": "Universal Dependencies v2.1", "start_pos": 26, "end_pos": 53, "type": "DATASET", "confidence": 0.8024005889892578}]}, {"text": "This also makes it possible to extract a robust and fair comparative analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "We followed the training configuration proposed by.", "labels": [], "entities": []}, {"text": "All models where trained using mini-batches (size=10) and stochastic gradient descent (SGD) with exponential decay (lr = 0.02, decay computed as lr \u00d7 e \u22120.2\u00d7epoch ).", "labels": [], "entities": []}, {"text": "Dropout was set to 50%.", "labels": [], "entities": [{"text": "Dropout", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8150283098220825}]}, {"text": "With our implementation dropout was observed to work better than regularization with less effort in terms of tuning.", "labels": [], "entities": [{"text": "regularization", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.9437383413314819}]}, {"text": "We used internal embeddings, initialized according to a Glorot uniform, which are learned together with the oracle during the training phase.", "labels": [], "entities": []}, {"text": "In the experiments we use no external embeddings, following the same criteria as.", "labels": [], "entities": []}, {"text": "The aim was to evaluate all parsers under a homogeneous configuration, and high-quality external embeddings maybe difficult to obtain for some languages.", "labels": [], "entities": []}, {"text": "The experiments explore two paths: (1) is it possible to reduce the number of features without a significant loss in terms of accuracy? and (2) is it possible to reduce the size of the embeddings representing those features, also without causing significant loss in terms of accuracy?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.997502863407135}, {"text": "accuracy", "start_pos": 275, "end_pos": 283, "type": "METRIC", "confidence": 0.9967350363731384}]}, {"text": "To evaluate this, we used as baseline the following configuration.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance for the (1) de facto standard, (2) NO-GD/D and (3) NO-GD set of features, when used to  train oracles with the ARC-STANDARD and SWAP algorithms. Red cells indicate a significant loss (--) with respect  to the baseline, the yellow ones a non-significant gain(+)/loss (-) and the green ones a significant gain (++).", "labels": [], "entities": [{"text": "NO-GD/D", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.7009321649869283}, {"text": "ARC-STANDARD", "start_pos": 133, "end_pos": 145, "type": "DATASET", "confidence": 0.8768013119697571}]}, {"text": " Table 3: ARC-STANDARD baseline configuration versus different runs with the NO-GD feature set and embedding  size reduction from 10% to 50%. See Table 2 for color scheme definition.", "labels": [], "entities": [{"text": "NO-GD feature set", "start_pos": 77, "end_pos": 94, "type": "DATASET", "confidence": 0.8422749042510986}]}, {"text": " Table 4: SWAP baseline configuration versus different runs with the NO-GD feature set and embedding size  reduction by a factor of 0.1 and 0.5. The average LAS/speed for the baseline is 75.29/2.8, for the NO-GD feature  set with embedding reduction by a factor of 0.1 is 75.27/3.6, and with embedding reduction by a factor of 0.5  75.02/4.0. See", "labels": [], "entities": [{"text": "NO-GD feature set", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.9400372505187988}, {"text": "LAS/speed", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9548926949501038}, {"text": "NO-GD feature  set", "start_pos": 206, "end_pos": 224, "type": "DATASET", "confidence": 0.9248510797818502}]}]}