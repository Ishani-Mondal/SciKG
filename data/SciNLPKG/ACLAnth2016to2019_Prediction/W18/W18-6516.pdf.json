{"title": [{"text": "SpatialVOC2K: A Multilingual Dataset of Images with Annotations and Features for Spatial Relations between Objects", "labels": [], "entities": []}], "abstractContent": [{"text": "We present SpatialVOC2K, the first multilingual image dataset with spatial relation annotations and object features for image-to-text generation, built using 2,026 images from the PASCAL VOC2008 dataset.", "labels": [], "entities": [{"text": "image-to-text generation", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.7770853638648987}, {"text": "PASCAL VOC2008 dataset", "start_pos": 180, "end_pos": 202, "type": "DATASET", "confidence": 0.8075171510378519}]}, {"text": "The dataset incorporates (i) the labelled object bounding boxes from VOC2008, (ii) geometrical, language and depth features for each object, and (iii) for each pair of objects in both orders, (a) the single best preposition and (b) the set of possible prepositions in the given language that describe the spatial relationship between the two objects.", "labels": [], "entities": [{"text": "VOC2008", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9649075269699097}]}, {"text": "Compared to previous versions of the dataset, we have roughly doubled the size for French, and completely reannotated as well as increased the size of the English portion, providing single best prepositions for English for the first time.", "labels": [], "entities": []}, {"text": "Furthermore, we have added explicit 3D depth features for objects.", "labels": [], "entities": []}, {"text": "We are releasing our dataset for free reuse, along with evaluation tools to enable comparative evaluation.", "labels": [], "entities": [{"text": "comparative evaluation", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.7718921899795532}]}], "introductionContent": [{"text": "Research in image labelling, description and understanding has along tradition, but has recently seen explosive growth.", "labels": [], "entities": [{"text": "image labelling", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7406409084796906}, {"text": "description and understanding", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.6492810944716135}]}, {"text": "Work in this area is most commonly motivated in terms of accessibility and data management, and has a range of different specific application tasks.", "labels": [], "entities": [{"text": "data management", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.7536720633506775}]}, {"text": "One current research focus is detection of relations between objects, in particular for image description generation, and the research presented here contributes to this line of work with anew dataset, SpatialVOC2K, 1 in which object pairs in images have been annotated with spatial relations encoded as sets of prepositions, specifically for image-to-text generation.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 88, "end_pos": 116, "type": "TASK", "confidence": 0.7610083123048147}, {"text": "image-to-text generation", "start_pos": 343, "end_pos": 367, "type": "TASK", "confidence": 0.7564822733402252}]}, {"text": "We start below with the source datasets from which we obtained the images, bounding boxes, and candidate prepositions (Section 2), followed by an overview of directory structure and file schemas (Section 3), and a summary of the annotation process (Section 4) and spatially relevant features (Section 5).", "labels": [], "entities": []}, {"text": "We describe the two evaluation tools supplied with the dataset (Section 6), and finish with a survey of other datasets with object relation annotations (Section 7).", "labels": [], "entities": []}], "datasetContent": [{"text": "SpatialVOC2K includes two evaluation tools which we have used in all previous work involving similar data.", "labels": [], "entities": []}, {"text": "The two tools, systemAccuracy and relationPrecision implement the following two methods, respectively.", "labels": [], "entities": []}, {"text": "System-level Accuracy: There are four different variants of system-level Accuracy, denoted Acc(n), n \u2208 {1, 2, 3, 4}.", "labels": [], "entities": [{"text": "Acc", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9924305081367493}]}, {"text": "Each variant returns Accuracy rates for the top n outputs returned by systems, in the sense that a system output is considered correct if at least one of the reference prepositions (the human-selected prepositions from the dataset annotations) can be found in the top n prepositions returned by the system (for n = 1 this yields standard Accuracy).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9993031024932861}, {"text": "Accuracy", "start_pos": 338, "end_pos": 346, "type": "METRIC", "confidence": 0.9444915056228638}]}, {"text": "Weighted Average Per-preposition Precision: This measure, denoted Acc P , computes the weighted mean of individual per-preposition precision scores.", "labels": [], "entities": [{"text": "Acc P", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.9873231053352356}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.8419387340545654}]}, {"text": "The individual per-preposition precision fora given system and a given preposition p is the proportion of times that p is among the corresponding human-selected prepositions out of all the times that p is returned as the top-ranked preposition by the system.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.5069944262504578}]}, {"text": "Types of relationships that have been annotated include actions (e.g. person kicks ball), other verbal relations (person wears shirt), spatial relations (person on horse), and comparative relations (one car bigger than another).", "labels": [], "entities": []}, {"text": "In this section, we provide a brief overview of available datasets with relation annotations, in terms of their stated purpose (application task), the types of relations included, the range of spatial prepositions included, as well assize and other properties of the dataset. has a summary of the datasets.", "labels": [], "entities": []}, {"text": "Visual Phrases () was the first image dataset with object relation annotations, and used the concept of a visual phrase (VP) which is defined as a bounding box that surrounds two objects in an image.", "labels": [], "entities": []}, {"text": "Out of 17 different types of VPs annotated in the data set, 13 comprise 2 objects, and 4 comprise one object.", "labels": [], "entities": []}, {"text": "However, there are 120 predicates per object category.", "labels": [], "entities": []}, {"text": "Visual and Linguistic Treebank) contains 341 images that are annotated with regions (362 in total) and visual dependency representations, which unfold to a total of 5,748 spatial relations (from a set of 8) and are aligned to the dependency parse of the image description.", "labels": [], "entities": []}, {"text": "This setup allows for the prediction of actions as well as spatial relations (using a set of 8 manual created rules).", "labels": [], "entities": [{"text": "prediction of actions", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.855804443359375}]}, {"text": "Scene Graphs () is a dataset of 5,000 human-generated scene graphs grounded to images; scene graphs describe objects and their relationships.", "labels": [], "entities": []}, {"text": "ViSen () associates sets of (object 1, preposition, object 2) triples with images, where the triples have been extracted from parses of the image descriptions in) and Flickr30k ().", "labels": [], "entities": [{"text": "ViSen", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9375692009925842}, {"text": "Flickr30k", "start_pos": 167, "end_pos": 176, "type": "DATASET", "confidence": 0.7309557199478149}]}, {"text": "Prepositions covered include all those extracted from the image descriptions including non-spatial ones.", "labels": [], "entities": [{"text": "Prepositions", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9564846158027649}]}, {"text": "By far not all descriptions contain prepositions so not all images have spatial relation annotations; the task addressed is preposition prediction, not spatial relation prediction.", "labels": [], "entities": [{"text": "preposition prediction", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.7849245667457581}, {"text": "spatial relation prediction", "start_pos": 152, "end_pos": 179, "type": "TASK", "confidence": 0.6747250556945801}]}], "tableCaptions": [{"text": " Table 2: Overview of related datasets. For explanation of relation categories see in text.", "labels": [], "entities": []}]}