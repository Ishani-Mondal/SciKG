{"title": [{"text": "From Chinese Word Segmentation to Extraction of Constructions: Two Sides of the Same Algorithmic Coin", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 5, "end_pos": 30, "type": "TASK", "confidence": 0.5984652042388916}, {"text": "Extraction of Constructions", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.8373914758364359}]}], "abstractContent": [{"text": "This paper presents the results of two experiments carried out within the framework of computational construction grammar.", "labels": [], "entities": [{"text": "computational construction grammar", "start_pos": 87, "end_pos": 121, "type": "TASK", "confidence": 0.6678598423798879}]}, {"text": "Starting from the constructionist point of view that there are just constructions in language, including lexical ones, we tested the validity of a clustering algorithm that was primarily designed for MWE extraction, the cpr-score (Colson, 2017), on Chinese word segmentation.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 200, "end_pos": 214, "type": "TASK", "confidence": 0.9771097004413605}, {"text": "Chinese word segmentation", "start_pos": 249, "end_pos": 274, "type": "TASK", "confidence": 0.6005162398020426}]}, {"text": "Our results indicate a striking recall rate of 75 percent without any special adaptation to Chinese or to the lexicon, which confirms that there is some similarity between extracting MWEs and CWS.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9898384213447571}, {"text": "extracting MWEs", "start_pos": 172, "end_pos": 187, "type": "TASK", "confidence": 0.7308627068996429}]}, {"text": "Our second experiment also suggests that the same methodology might be used for extracting more schematic or abstract constructions, thereby providing evidence for the statistical foundation of construction grammar.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many respects, constructionist approaches have led to anew paradigm in the description of language structure.", "labels": [], "entities": []}, {"text": "Building on Langacker's cognitive grammar, the different versions of construction grammar (CxG) converge on the notion of constructions, defined as Saussurean signs, i.e. \"conventional, learned form-function pairings at varying levels of complexity and abstraction\".", "labels": [], "entities": []}, {"text": "A construction maybe a word in the traditional sense (e.g. book), abound morpheme, an idiom (spill the beans, take the rough with the smooth), a partially filled idiom (take X into account), but also an abstract construction such as the ditransitive construction or the passive.", "labels": [], "entities": []}, {"text": "As the famous quotation goes, \"It's constructions all the way down\", i.e. language structure is made of nothing else than constructions, at various degrees of abstraction and schematicity.", "labels": [], "entities": []}, {"text": "Schematic slots are the positions in the constructions allowing for several choices (e.g. X in take X into account), whereas specific (or substantive) slots are fixed (e.g. into and account in the same construction).", "labels": [], "entities": []}, {"text": "The continuum between lexicon and syntax plays a key role in CxG: there is no strict borderline between grammar on the one hand and the lexicon on the other, and this cline has been called the constructicon.", "labels": [], "entities": []}, {"text": "Thus, the constructicon includes all types of constructions, be they of a more syntactic, morphological, phonological, phraseological, pragmatic or lexical nature.", "labels": [], "entities": []}, {"text": "As a general theory of language, CxG has far-reaching consequences for corpus and computational linguistics.", "labels": [], "entities": []}, {"text": "In particular, it sheds anew light on multiword expressions (MWEs), in the general sense of all word combinations displaying lexical, syntactic, semantic, pragmatic and/or statistical idiosyncrasies.", "labels": [], "entities": [{"text": "multiword expressions (MWEs)", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6932617902755738}]}, {"text": "Indeed, all constructions are per definition partly idiosyncratic and in that sense partly idiomatic: \"What may license referring to some constructions as idioms and not others is merely a reflection of the fact that effects of idiomatic variation are best observable in partially schematic complex constructions -however, this does not make them fundamentally different in nature from other constructions.\"", "labels": [], "entities": []}, {"text": "It is worth noting that constructions are seen as a complex network, ranging from abstract to specific, from simple to complex and from schematic to idiomatic constructions.", "labels": [], "entities": []}, {"text": "Crucially, this network of constructions is thought to be of a probabilistic nature).", "labels": [], "entities": []}], "datasetContent": [{"text": "As we have seen in section 1, construction grammar claims that there is a cline from syntax to lexicon, and that the structure of language therefore consists of a complex network of interrelated constructions.", "labels": [], "entities": [{"text": "construction grammar", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.9313190877437592}]}, {"text": "If this theoretical claim is correct, algorithms that are designed to extract MWEs should also be able to extract lexical constructions, provided that the corpus is adapted to that purpose.", "labels": [], "entities": []}, {"text": "For European languages, it will for instance be necessary to start from morphemes instead of (conventional) words.", "labels": [], "entities": []}, {"text": "In the case of Chinese, construction grammar predicts that looking for larger elements of meaning against the backdrop of a network of constructions will bring segmentation (CWS) and MWE recognition very close to each other.", "labels": [], "entities": [{"text": "construction grammar", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.9274598062038422}, {"text": "MWE recognition", "start_pos": 183, "end_pos": 198, "type": "TASK", "confidence": 0.9235292673110962}]}, {"text": "In this paper we report the first results of an innovative experiment designed to test this general hypothesis.", "labels": [], "entities": []}, {"text": "Recall Precision F measure Seg-cpr 0.749 0.658 0.700 Stanford-segmenter 0.882 0.843 0.862: Results of CWS by means of Seg-cpr and Stanford-segmenter.", "labels": [], "entities": [{"text": "Precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9813181757926941}, {"text": "F measure Seg-cpr 0.749 0.658 0.700 Stanford-segmenter 0.882 0.843 0.862", "start_pos": 17, "end_pos": 89, "type": "METRIC", "confidence": 0.8333393394947052}]}, {"text": "As shown in, the results obtained by our experimental segmenter based on the cpr-score are obviously less good than those of the Stanford segmenter, but this hardly comes as a surprise, as the cprscore was not designed for CWS in the first place.", "labels": [], "entities": [{"text": "CWS", "start_pos": 223, "end_pos": 226, "type": "TASK", "confidence": 0.9252648949623108}]}, {"text": "We have also stressed in section 3.1. that our methodology, contrary to state-of-the-art segmenters such as the Stanford segmenter, does not rely on segmented corpora or on dictionaries, but only on statistical attraction as measured by the cpr-score.", "labels": [], "entities": []}, {"text": "Contrary to most segmenters, it is not a mirror of how language users tend to segment the language, but of how the language itself contains statistically significant elements of meaning.", "labels": [], "entities": []}, {"text": "It is besides quite striking that the recall rate obtained by Seg-cpr (0.749) comes very close to the average rate of segmentation agreement among native speakers of Chinese (0.75, as mentioned in section 1).", "labels": [], "entities": [{"text": "recall rate", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.99003466963768}]}, {"text": "Contrary to manually segmented corpora, or to segmenters based on dictionary learning or segmentation pattern learning, our results are objectively measured by the algorithm on an unsegmented reference corpus.", "labels": [], "entities": []}, {"text": "For this reason alone, a recall of 0.749 computed from the gold standard established by Chinese native speakers is quite high.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9979245662689209}]}, {"text": "A fine-tuned analysis makes these results even more intriguing.", "labels": [], "entities": []}, {"text": "In 5 to 10 percent of the cases, wrong segmentation by Seg-cpr was simply due to the fact that the n-gram was not used a single time on the reference corpus of about 250-300 million words.", "labels": [], "entities": []}, {"text": "As the cpr-score, on which the tool is based, requires a frequency of at least 3 occurrences, the absence of an n-gram / word from the reference corpus inevitably leads to wrong segmentation, but this is to be blamed on the corpus size, not on the algorithm.", "labels": [], "entities": []}, {"text": "Taking a closer look at cases of obviously wrong segmentation by Seg-cpr raises other intriguing questions.", "labels": [], "entities": []}, {"text": "One has to do with discontinuous sequences, a central issue in construction grammar as well.", "labels": [], "entities": [{"text": "construction grammar", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.9184491038322449}]}, {"text": "As a matter of fact, another 5 to 10 percent of the instances of wrong segmentation by Seg-cpr is due to discontiuous statistical association.", "labels": [], "entities": []}, {"text": "Let us take the example of a Chinese fivegram from the MSR dataset, considered as one word by the gold standard, \u4e2a\u4eba\u8ba1\u7b97\u673a (g\u00e8r\u00e9n j\u00ecsu\u00e0nj\u012b, personal computer).", "labels": [], "entities": [{"text": "MSR dataset", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9505930244922638}]}, {"text": "shows the cpr-score and the frequency of the different levels of grams in our reference corpus.: cpr-score and frequency of the component grams of \u4e2a\u4eba\u8ba1\u7b97\u673a (g\u00e8r\u00e9n j\u00ecsu\u00e0nj\u012b).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: cpr-score and frequency of the component grams of \u4e2a\u4eba\u8ba1\u7b97\u673a (g\u00e8r\u00e9n j\u00ecsu\u00e0nj\u012b).", "labels": [], "entities": [{"text": "frequency", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.974675714969635}]}, {"text": " Table 3: cpr-score and frequency of a schematic construction and a derived MWE. 9", "labels": [], "entities": []}, {"text": " Table 4: cpr-score and frequency for the Ditransitive and All-cleft construction", "labels": [], "entities": []}]}