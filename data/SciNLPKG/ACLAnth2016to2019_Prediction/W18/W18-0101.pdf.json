{"title": [{"text": "Coreference and Focus in Reading Times", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents evidence of a linguistic focus effect on coreference resolution in broad-coverage human sentence processing.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.9811118245124817}, {"text": "broad-coverage human sentence processing", "start_pos": 87, "end_pos": 127, "type": "TASK", "confidence": 0.6051687076687813}]}, {"text": "While previous work has explored the role of prominence in coref-erence resolution (Almor, 1999; Foraker and McElree, 2007), these studies use constructed stimuli with specific syntactic patterns (e.g. cleft constructions) which could have idiosyncratic frequency confounds.", "labels": [], "entities": [{"text": "coref-erence resolution", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.8001796305179596}]}, {"text": "This paper explores the generalizability of this effect on coreference resolution in a broad-coverage analysis.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.9762523770332336}]}, {"text": "In particular, the current work proposes several new estima-tors of prominence appropriate for broad-coverage sentence processing and evaluates them as predictors of reading behavior in the Natural Stories corpus (Futrell, Gibson, Tily, Vishnevetsky, Piantadosi, and Fedorenko, in prep), a collection of \"constructed-natural\" narratives read by a large number of subjects.", "labels": [], "entities": [{"text": "broad-coverage sentence processing", "start_pos": 95, "end_pos": 129, "type": "TASK", "confidence": 0.6300113797187805}]}, {"text": "Results show a strong facilitation effect for one of these predictors on exploratory data and confirm that it generalizes to held-out data.", "labels": [], "entities": []}, {"text": "These results provide broad-coverage support for the hypothesis that coreference resolution is easier when the target entity is focused by discourse properties, resulting in faster reading times.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.975845605134964}]}], "introductionContent": [{"text": "Coreference resolution has often been assumed to incur processing costs due to some form of memory retrieval or search through accessible antecedents, similar to the binding problem for syntactic dependency attachment.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.933272123336792}, {"text": "syntactic dependency attachment", "start_pos": 186, "end_pos": 217, "type": "TASK", "confidence": 0.7246026396751404}]}, {"text": "This search has been shown to be facilitated by linguistic focus (or prominence or salience) arising from syntactic, pragmatic, semantic, lexical, information structural and other factors.", "labels": [], "entities": []}, {"text": "Previous work has investigated the role of linguistic focus in coreference resolution using constructed stimuli.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.9611624777317047}]}, {"text": "However, as discussed in, effects found using constructed stimuli often fail to generalize to broad-coverage sentence processing.", "labels": [], "entities": []}, {"text": "It is possible that results obtained using constructed stimuli are due in part to (1) information-theoretic factors that such studies rarely control for (e.g. surprisal), (2) limited syntactic coverage, and/or (3) properties of the stimuli themselves that are atypical of naturalistic sentence processing (e.g. overrepresentation of rare constructions, odd semantics, or lack of context).", "labels": [], "entities": []}, {"text": "While previous work has operationalized prominence or linguistic focus using cleft constructions, such constructions are very rare) and therefore cannot be relied upon to predict online processing in the broad-coverage setting.", "labels": [], "entities": []}, {"text": "The current work addresses these concerns by deploying novel broad-coverage implementations of focus as predictors of reading times in a large corpus of naturalistic self-paced reading (SPR) by many subjects (Futrell, Gibson, Tily, Vishnevetsky,.", "labels": [], "entities": []}, {"text": "Following, the current work evaluates these predictors against a baseline including both n-gram and probabilistic context-free grammar (PCFG) estimates of incremental surprisal.", "labels": [], "entities": []}, {"text": "Using this procedure, results show a significant facilitatory effect of predictors relating to linguistic focus on reading time latencies, supporting the hypothesis that focus effects for coreference observed using constructed stimuli do indeed gener-1 alize to broad-coverage sentence processing.", "labels": [], "entities": [{"text": "broad-coverage sentence processing", "start_pos": 262, "end_pos": 296, "type": "TASK", "confidence": 0.5801330010096232}]}], "datasetContent": [{"text": "Each main effect predictor is evaluated on the exploratory data via likelihood ratio test (LRT) of two fitted linear mixed effects (LME) models, one including the main effect as a fixed effect and one excluding it.", "labels": [], "entities": [{"text": "main effect predictor", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.6179631352424622}, {"text": "likelihood ratio test (LRT)", "start_pos": 68, "end_pos": 95, "type": "METRIC", "confidence": 0.919069766998291}]}, {"text": "Both models also contain a set of baseline fixed effects: word length, 5-gram forward surprisal, incremental PCFG surprisal, and story position.", "labels": [], "entities": []}, {"text": "All models include all baseline fixed effects.", "labels": [], "entities": []}, {"text": "Models also include by-subject random slopes for the main effect and every baseline effect, with the exception of syntactic surprisal, whose by-subject random slopes were removed as the weakest predictor in order to overcome lack of convergence.", "labels": [], "entities": []}, {"text": "Experiments evaluate each main effect overall instances of coreference, as the smaller pronounonly subset did not converge reliably.", "labels": [], "entities": [{"text": "coreference", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9692754745483398}]}, {"text": "Delays in the time course of processing effects can be modeled by spillover, where the effect of an independent variable is predicted to be observed n words later.", "labels": [], "entities": []}, {"text": "Using standard linear regression on the exploratory dataset, we found the best-fit spillover position of the baseline predictors to be zero (in situ) with the exception of PCFG surprisal, which is optimally: Effect sizes for main and baseline predictors on confirmatory partition of data.", "labels": [], "entities": []}, {"text": "The main effect, spilled over MentionCount, is highly significant (p = 7.05e \u2212 5).", "labels": [], "entities": [{"text": "MentionCount", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9674865007400513}]}, {"text": "Negative effect direction indicates a speed-up in reading times.", "labels": [], "entities": []}, {"text": "SD shows \u03b2-effect in milliseconds per unit of standard deviation.", "labels": [], "entities": [{"text": "\u03b2-effect", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9900701642036438}]}, {"text": "Predictor Units are the effect size in milliseconds, rescaled to the original predictors' units.", "labels": [], "entities": [{"text": "Predictor Units", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9775665998458862}]}, {"text": "Model includes observations from spilled over anaphors, totaling 59,632 observations.", "labels": [], "entities": []}, {"text": "Word Length is measured in characters, Surprisal is measured in bits, and Story Position is the proportion of sentences completed, scaled between 0 and 1.", "labels": [], "entities": [{"text": "Surprisal", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9917495250701904}, {"text": "Story", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9119271636009216}]}, {"text": "spilled over by 1 position.", "labels": [], "entities": []}, {"text": "In addition to optimizing the baseline predictors, we consider both in situ and spillover-1 variants of each of our main effects.", "labels": [], "entities": []}, {"text": "3 The reading time measures are transformed following to match assumptions of normality by the likelihood ratio test.", "labels": [], "entities": []}, {"text": "These experiments use a coefficient of \u03bb = -0.63.", "labels": [], "entities": []}, {"text": "All predictors are also centered and z-transformed prior to regression.", "labels": [], "entities": []}, {"text": "The reason for choosing a single optimal spillover position for each variable rather than considering multiple spillover positions simultaneously (as in, for example) is that our data are too sparse to support such highly parameterized models given that we are controlling for heterogeneity in the population via by-subject random slopes for each independent variable.", "labels": [], "entities": []}, {"text": "Since there are 181 subjects in the dataset, each additional independent variable (including each additional modeled spillover position fora given independent variable) contributes 181 additional slopes to estimate.", "labels": [], "entities": []}, {"text": "The effect estimates given in are presented in milliseconds for expository purposes.", "labels": [], "entities": []}, {"text": "However, this is in fact a back-transformation of \u03b2 into milliseconds using the equation \u03b2-ms = (\u03bb\u00af y + \u03bb\u03b2 + 1) 1/\u03bb \u2212 (\u03bb\u00af y + 1) 1/\u03bb , where \u00af y is the mean of the transformed reading times (1.55 in our data).", "labels": [], "entities": []}, {"text": "Because introduces non-linearity, \u03b2-ms is only valid at the back-transformed mean, holding all other effects at their means.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example predictor values. MentionCount is the number of previous mentions to the same  referent. The two other predictors measure distance in words or referents, respectively, back to the  antecedent. Words sharing a subscript value are coreferential.", "labels": [], "entities": [{"text": "MentionCount", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.9198246598243713}]}]}