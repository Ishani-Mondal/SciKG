{"title": [{"text": "Co-Attention Based Neural Network for Source-Dependent Essay Scoring", "labels": [], "entities": [{"text": "Source-Dependent Essay Scoring", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.6678751806418101}]}], "abstractContent": [{"text": "This paper presents an investigation of using a co-attention based neural network for source-dependent essay scoring.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 103, "end_pos": 116, "type": "TASK", "confidence": 0.6892852485179901}]}, {"text": "We use a co-attention mechanism to help the model learn the importance of each part of the essay more accurately.", "labels": [], "entities": []}, {"text": "Also, this paper shows that the co-attention based neural network model provides reliable score prediction of source-dependent responses.", "labels": [], "entities": []}, {"text": "We evaluate our model on two source-dependent response corpora.", "labels": [], "entities": []}, {"text": "Results show that our model outperforms the baseline on both corpora.", "labels": [], "entities": []}, {"text": "We also show that the attention of the model is similar to the expert opinions with examples.", "labels": [], "entities": []}], "introductionContent": [{"text": "Manually grading students' essays is labor intensive.", "labels": [], "entities": []}, {"text": "Therefore, many automated essay scoring (AES) methods have been developed to support grading essays at scale.", "labels": [], "entities": [{"text": "essay scoring (AES)", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.6987167060375213}]}, {"text": "However, in different grading tasks, the information required by an AES system is different.", "labels": [], "entities": []}, {"text": "For example, if a system needs to assign a holistic score to the essay, the system needs to take all information into account.", "labels": [], "entities": []}, {"text": "In contrast, if a system needs to assign a score for one specific aspect of the essay (e.g. use of evidence), the system needs to ignore some information.", "labels": [], "entities": []}, {"text": "Also, if an essay is a source-dependent essay, the system needs to exploit knowledge of the source article.", "labels": [], "entities": []}, {"text": "This paper focuses on source-dependent essay assessment.", "labels": [], "entities": [{"text": "source-dependent essay assessment", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.5803610384464264}]}, {"text": "In this task, students read a source article before writing the essay, and assessment involves recognizing and analyzing references to the article in the essay.", "labels": [], "entities": []}, {"text": "We propose anew type of co-attention based neural network model tailored to source-dependent grading, then use two source-dependent essay corpora to evaluate our model.", "labels": [], "entities": []}, {"text": "Our first corpus contains the four sourcedependent essay sets in the Automated Student Assessment Prize (ASAP) corpus 1 . The ASAP grading task is to assign a holistic score to each essay.", "labels": [], "entities": [{"text": "Automated Student Assessment Prize (ASAP) corpus", "start_pos": 69, "end_pos": 117, "type": "DATASET", "confidence": 0.6625441238284111}]}, {"text": "The second corpus uses the Response to Text Assessment (RTA) () to assess students' analytic writing skills.", "labels": [], "entities": [{"text": "Response to Text Assessment (RTA)", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.5033514073916844}]}, {"text": "Instead of evaluating holistic writing skills, the RTA was designed to evaluate students' writing skills along five dimensions: Analysis, Evidence, Organization, Style, and MUGS (Mechanics, Usage, Grammar, and Spelling).", "labels": [], "entities": [{"text": "RTA", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9091257452964783}, {"text": "MUGS", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.950508177280426}]}, {"text": "Our grading task for this corpus is to assign an Evidence score to each essay, by evaluating students' ability to find and use evidence from a source article to support their claims.", "labels": [], "entities": [{"text": "Evidence score", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.9395113885402679}]}, {"text": "The main contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "First, we introduce a co-attention based neural network model that is fully automated and does not need any expert effort to encode knowledge of a source article.", "labels": [], "entities": []}, {"text": "Second, our co-attention based neural network model extends prior work by designing the model to take a source article into account during grading.", "labels": [], "entities": []}, {"text": "Third, we apply our model to the subset of source-dependent responses tasks in the ASAP corpus and show that the model outperforms a previous neural network model developed for the full corpus.", "labels": [], "entities": [{"text": "ASAP corpus", "start_pos": 83, "end_pos": 94, "type": "DATASET", "confidence": 0.7387388050556183}]}, {"text": "Fourth, we show that our model also performs well on the RTA task and again significantly outperforms our baseline neural net model.", "labels": [], "entities": [{"text": "RTA task", "start_pos": 57, "end_pos": 65, "type": "TASK", "confidence": 0.8701152205467224}]}, {"text": "Last, we use examples to show that our model can assign reasonable attention scores to different sentences in the essay.", "labels": [], "entities": []}, {"text": "In the following sections, we first present related research.", "labels": [], "entities": []}, {"text": "Then we describe our tasks by introducing the ASAP corpus and the RTA corpus.", "labels": [], "entities": [{"text": "ASAP corpus", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.7754564881324768}, {"text": "RTA corpus", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.7206481546163559}]}, {"text": "Next, we explain the structure of our co-attention based neural network model.", "labels": [], "entities": []}, {"text": "Finally, we discuss the results of our experiments and future plans.", "labels": [], "entities": []}, {"text": "Previous research in AES needed feature engineering.", "labels": [], "entities": [{"text": "AES", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.7000637054443359}]}, {"text": "In very early work, developed an AES tool named Project Essay Grade (PEG) by only using linguistic surface features.", "labels": [], "entities": []}, {"text": "A more recent well-known AES system is E-Rater (, which employs many more natural language processing (NLP) technologies.", "labels": [], "entities": []}, {"text": "Later, released E-Rater V2, where they created anew set of features to represent linguistic characteristic related to organization and development, lexical complexity, promptspecific vocabulary usage, etc.", "labels": [], "entities": []}, {"text": "Similarly to, this system used regression equations for assessment of student essays.", "labels": [], "entities": []}, {"text": "One limitation of all of the above models is that all need handcrafted features for training the model.", "labels": [], "entities": []}, {"text": "In contrast, our model uses a neural network for the AES task and thus does not require feature engineering.", "labels": [], "entities": [{"text": "AES task", "start_pos": 53, "end_pos": 61, "type": "TASK", "confidence": 0.5369313210248947}]}, {"text": "Recently, neural network models have been introduced into AES, making the development of handcrafted features unnecessary or at least optional. and presented AES models that used Long Short Term Memory (LSTM) networks.", "labels": [], "entities": []}, {"text": "Differently, used a Convolutional Neural Network (CNN) model for essay scoring by applying two CNN layers on both the word level and then sentence level.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.7667478919029236}]}, {"text": "Later, presented another work that uses attention pooling to replace the mean overtime pooling after the convolutional layer in both word level and sentence levels.", "labels": [], "entities": []}, {"text": "However, none of these neural network grading models consider the source article if it exists.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a neural network model that takes the source article into account by using a co-attention mechanism instead of the self-attention mechanism of prior work.", "labels": [], "entities": []}, {"text": "Our work not only focuses on essay assessment using a holistic score, but also evaluates a particular dimension of argument-oriented writing skills, namely use of Evidence.", "labels": [], "entities": [{"text": "essay assessment", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7901250123977661}]}, {"text": "analyze only the content of essays by detecting off-topic essays.", "labels": [], "entities": []}, {"text": "used argumentation mining techniques to evaluate if students use enough evidence to support their positions.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 5, "end_pos": 25, "type": "TASK", "confidence": 0.9285984635353088}]}, {"text": "However, these two prior studies are not suitable for our task because they did not measure the use of content or evidence from a source article.", "labels": [], "entities": []}, {"text": "With respect to source-based dimensional essay analysis, developed a set of rubric-based features that compared a student's essay and a source article in terms of number of related words or paraphrases.", "labels": [], "entities": [{"text": "source-based dimensional essay analysis", "start_pos": 16, "end_pos": 55, "type": "TASK", "confidence": 0.6128185391426086}]}, {"text": "improved their model by introducing word embedding into the feature extraction process to extract relationships previously missed due to lexical errors or use of different vocabulary.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7633825540542603}]}, {"text": "However, in both of these studies, human effort was still necessary for pre-processing the source article, for example, by having experts manually create a list of important words and phrases in the article which the system would compare with features extracted from the student's essay.", "labels": [], "entities": []}, {"text": "In contrast, our work does not need any human effort to analyze the source article before essay grading.", "labels": [], "entities": []}, {"text": "Although investigated extracting example lists by using LDA () model, the data-driven model missed an example when there was no essay mentioning the example.", "labels": [], "entities": []}, {"text": "predicted which parts of the source material were important and that students needed to use in their essays.", "labels": [], "entities": []}, {"text": "The essay score is required to obtain the content importance for their work, but our work does not need to know the essay score while identifying the content importance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We configure experiments to test three hypotheses: H1: the model we proposed (denoted by CO-ATTN) will outperform or at least perform equally well as the baseline (denoted by SELF-ATTN) presented by on four ASAP essay corpora in the holistic score prediction task.", "labels": [], "entities": [{"text": "SELF-ATTN", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.960519552230835}, {"text": "holistic score prediction task", "start_pos": 233, "end_pos": 263, "type": "TASK", "confidence": 0.6717547997832298}]}, {"text": "H2: the model we proposed will outperform or at least perform equally well as the baseline on two RTA corpora in the Evidence score prediction task.", "labels": [], "entities": [{"text": "Evidence score prediction task", "start_pos": 117, "end_pos": 147, "type": "TASK", "confidence": 0.7519566565752029}]}, {"text": "H3: the model we proposed will outperform or at least perform equally well as the non-neural network baselines on both corpora.", "labels": [], "entities": []}, {"text": "We use NLTK () for text preprocessing.", "labels": [], "entities": [{"text": "text preprocessing", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7509135603904724}]}, {"text": "The vocabulary size of the data is limited to 4000, and all scores are scaled to the range, following and.", "labels": [], "entities": []}, {"text": "In particular, the 4000 most frequent words are preserved, with all other words treated as unknowns.", "labels": [], "entities": []}, {"text": "The assessment scores will be converted back to their original range during evaluation.", "labels": [], "entities": []}, {"text": "We use Quadratic Weighted Kappa (QWK) to evaluate our model.", "labels": [], "entities": [{"text": "Quadratic Weighted Kappa (QWK)", "start_pos": 7, "end_pos": 37, "type": "METRIC", "confidence": 0.8895598550637563}]}, {"text": "QWK is not only the official criteria of ASAP corpus, but also adopted as evaluation metric in;;;; for both ASAP and RTA corpora.", "labels": [], "entities": [{"text": "QWK", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5262965559959412}, {"text": "ASAP corpus", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.8146343231201172}]}, {"text": "We use 5-fold cross-validation because both RTA and ASAP corpora have no released labeled test data.", "labels": [], "entities": []}, {"text": "We split all corpora into 5 folds.", "labels": [], "entities": []}, {"text": "For the ASAP corpus, the partition is the same as the setting presented by.", "labels": [], "entities": [{"text": "ASAP corpus", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.8149701356887817}]}, {"text": "For the RTA corpus, since there is no prior work to split the corpus, we separate it into 5 folds randomly.", "labels": [], "entities": [{"text": "RTA corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.7850176393985748}]}, {"text": "In each fold, 60% of the data are used for training, 20% of the data are the development set, and 20% of the data are used for testing.", "labels": [], "entities": []}, {"text": "To select the best model, we trained each model on 100 epochs and evaluated on the development set after each epoch.", "labels": [], "entities": []}, {"text": "The best model is the model with the best QWK on the development set.", "labels": [], "entities": []}, {"text": "This is done five times, once for each partition in the cross-validation.", "labels": [], "entities": []}, {"text": "Then the average QWK score from these five evaluations on the test set is reported.", "labels": [], "entities": [{"text": "QWK", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9796844720840454}]}, {"text": "Paired t-tests are used for significance tests with p < 0.05.", "labels": [], "entities": []}, {"text": "shows all hyperparameters for training.", "labels": [], "entities": []}, {"text": "The code of SELF-ATTN are provided by, they used Keras (Chollet et al., 2015) 1.1.1 and Theano (Theano Development Team, 2016) 0.8.2 as the backend.", "labels": [], "entities": [{"text": "SELF-ATTN", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.5981355309486389}, {"text": "Keras (Chollet et al., 2015) 1.1.1", "start_pos": 49, "end_pos": 83, "type": "DATASET", "confidence": 0.8393564687834846}, {"text": "Theano (Theano Development Team, 2016) 0.8.2", "start_pos": 88, "end_pos": 132, "type": "DATASET", "confidence": 0.904479894373152}]}, {"text": "Because we are using Keras 2.1.3 and TensorFlow ( 1.4.0 as the backend, we ran all experiments with our frameworks.", "labels": [], "entities": []}, {"text": "Therefore, the numbers of SELF-ATTN have small differences to the numbers reported by the baseline model.", "labels": [], "entities": [{"text": "SELF-ATTN", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.6746513843536377}]}, {"text": "For non-neural network baselines, we introduce the SVR and BLRR baselines presented by for the ASAP corpus, and SG baseline presented by for the RTA corpus.", "labels": [], "entities": [{"text": "BLRR", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9522264003753662}, {"text": "ASAP corpus", "start_pos": 95, "end_pos": 106, "type": "DATASET", "confidence": 0.8152735233306885}, {"text": "RTA corpus", "start_pos": 145, "end_pos": 155, "type": "DATASET", "confidence": 0.7818675935268402}]}, {"text": "SVR and BLRR models use Enhanced AI Scoring Engine (EASE) 2 to extract four types of features, such as length, part of speech, prompt, and the bag of words.", "labels": [], "entities": [{"text": "BLRR", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.475614458322525}]}, {"text": "Then they use SVR and BLRR as the classifiers, respectively.", "labels": [], "entities": [{"text": "BLRR", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9912329316139221}]}, {"text": "We do not perform 2 https://github.com/edx/ease any significance test on both SVR and BLRR because we do not have detailed experiment data.", "labels": [], "entities": [{"text": "BLRR", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.6243186593055725}]}, {"text": "Therefore, we only report the result presented in.", "labels": [], "entities": []}, {"text": "SG model extracts evidence features based on hand-crafted topic and example lists, and uses random forest tree as the classifier.", "labels": [], "entities": []}, {"text": "We follow the same data partition.", "labels": [], "entities": []}, {"text": "However, we only use the training set for training and the testing set for testing while ignoring the development set so that we can perform the same paired t-tests in the experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The holistic score distribution of ASAP.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 45, "end_pos": 49, "type": "TASK", "confidence": 0.5502457618713379}]}, {"text": " Table 2: The Evidence score distribution of RTA.", "labels": [], "entities": [{"text": "Evidence score distribution", "start_pos": 14, "end_pos": 41, "type": "METRIC", "confidence": 0.9413219690322876}, {"text": "RTA", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.703055202960968}]}, {"text": " Table 3: Hyper-parameters of training.", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9263728260993958}]}, {"text": " Table 4: The performance (QWK) of the baselines and our model.  *  indicates that the model QWK is significantly  better than the SELF-ATTN (p < 0.05).  \u2020 indicates that the model QWK is significantly better than the SG  (p < 0.05). The best results in each row are in bold.", "labels": [], "entities": [{"text": "SELF-ATTN", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.8359343409538269}]}]}