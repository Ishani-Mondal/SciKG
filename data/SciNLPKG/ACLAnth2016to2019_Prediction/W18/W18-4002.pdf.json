{"title": [{"text": "Word-Embedding based Content Features for Automated Oral Proficiency Scoring", "labels": [], "entities": [{"text": "Automated Oral Proficiency Scoring", "start_pos": 42, "end_pos": 76, "type": "TASK", "confidence": 0.6808541044592857}]}], "abstractContent": [{"text": "In this study, we develop content features for an automated scoring system of non-native En-glish speakers' spontaneous speech.", "labels": [], "entities": []}, {"text": "The features calculate the lexical similarity between the question text and the ASR word hypothesis of the spoken response, based on traditional word vector models or word embeddings.", "labels": [], "entities": [{"text": "ASR word", "start_pos": 80, "end_pos": 88, "type": "TASK", "confidence": 0.774254322052002}]}, {"text": "The proposed features do not require any sample training responses for each question, and this is a strong advantage since collecting question-specific data is an expensive task, and sometimes even impossible due to concerns about question exposure.", "labels": [], "entities": []}, {"text": "We explore the impact of these new features on the automated scoring of two different question types: (a) providing opinions on familiar topics and (b) answering a question about a stimulus material.", "labels": [], "entities": []}, {"text": "The proposed features showed statistically significant correlations with the oral proficiency scores, and the combination of new features with the speech-driven features achieved a small but significant further improvement for the latter question type.", "labels": [], "entities": []}, {"text": "Further analyses suggested that the new features were effective in assigning more accurate scores for responses with serious content issues.", "labels": [], "entities": []}], "introductionContent": [{"text": "This study aims to develop new features to score the content of non-native speakers' spontaneous speech as apart of an automated oral proficiency scoring system.", "labels": [], "entities": []}, {"text": "The system provides holistic proficiency scores using audio files and their transcriptions generated by an automated speech recognition (ASR) system.", "labels": [], "entities": [{"text": "automated speech recognition (ASR)", "start_pos": 107, "end_pos": 141, "type": "TASK", "confidence": 0.7729460398356119}]}, {"text": "Previously, studies in automated speech scoring have mainly focused on assessment of fluency (, pronunciation, and intonation and rhythm.", "labels": [], "entities": [{"text": "automated speech scoring", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6440075933933258}]}, {"text": "More recently, researchers started exploring assessment of grammar () and vocabulary().", "labels": [], "entities": [{"text": "assessment of grammar", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.8354176680246989}]}, {"text": "To date, limited studies have explored approaches to evaluating the content of spoken responses.", "labels": [], "entities": []}, {"text": "explored content features based on the lexical similarity between the response and a set of sample responses for each question.", "labels": [], "entities": []}, {"text": "A content-scoring component based on word vectors was also part of the automated scoring engine described by.", "labels": [], "entities": []}, {"text": "In both of these studies, content features were developed to supplement other features measuring various aspects of speaking proficiency.", "labels": [], "entities": []}, {"text": "Neither study reported the relative contributions of content and speech features to the system performance.", "labels": [], "entities": []}, {"text": "considered a content-scoring engine based on many sparse features such as unigrams and bigrams and trained on a large corpus of existing responses.", "labels": [], "entities": []}, {"text": "They showed that this approach achieved performance comparable to that based on fluency and pronunciation, but there was only little improvement from combining the two sets of features.", "labels": [], "entities": []}, {"text": "Approaches like those above require a sizable amount of response data for each question, and collecting question-specific data is an expensive and difficult task.", "labels": [], "entities": []}, {"text": "Furthermore, for high-stakes assessment this can be impossible due to concerns about question exposure.", "labels": [], "entities": []}, {"text": "A content feature that does not require any test takers' responses for new questions has a strong advantage when scoring a large scale operational assessment.", "labels": [], "entities": []}, {"text": "To address this issue,) developed a system for scoring off-topic essays without the need for question-specific responses; the system was based on similarity features between the question text and the test response.", "labels": [], "entities": []}, {"text": "The performance of this system was lower than the benchmark system trained on question-specific responses, but it achieved a substantial improvement over a majority-based baseline.", "labels": [], "entities": []}, {"text": "further improved this system by expanding question texts to include synonyms, inflected forms, and distributionally similar words to the question content.", "labels": [], "entities": []}, {"text": "The performance of showed a substantial improvement for questions consisting of only a small amount of text.", "labels": [], "entities": []}, {"text": "More recently,  developed a set of content features based on both the questions and listening and reading materials for automated speech scoring and reported significant correlations between these content features and the proficiency scores.", "labels": [], "entities": [{"text": "automated speech scoring", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.5934426883856455}]}, {"text": "Various approaches based on deep-neural networks (DNN) and word-embeddings trained on large corpora have showed promising performance in various NLP tasks, such as document similarity detection (.", "labels": [], "entities": [{"text": "document similarity detection", "start_pos": 164, "end_pos": 193, "type": "TASK", "confidence": 0.8238600095113119}]}, {"text": "In contrast to traditional similarity features, which are limited to a reliance on exact word matching, these new approaches have the advantage of capturing topically relevant words that are not identical. and applied this approach to the task of off-topic detection in spoken responses and essays, respectively, and achieved substantial improvements over systems that only use word-matching.", "labels": [], "entities": [{"text": "off-topic detection", "start_pos": 247, "end_pos": 266, "type": "TASK", "confidence": 0.7843249440193176}]}, {"text": "In this study, we combine the approach suggested by  and with more recent advances in word embeddings and develop anew set of low-resource content features: these features are trained using the prompt text expanded with word-embeddings without relying on any pre-existing responses to a given question.", "labels": [], "entities": []}, {"text": "We conducted the following research: \u2022 Using the prompt texts included in each question, we developed two sets of content features: features based on the traditional content vector analysis (CVA) approach and features based on word embeddings.", "labels": [], "entities": []}, {"text": "\u2022 We trained automated scoring models using traditional speech-driven features and new content features and compared the performance of the models.", "labels": [], "entities": []}, {"text": "\u2022 We investigated the impact of question types on the performance of the content features and the automated scoring models.", "labels": [], "entities": []}, {"text": "We provided an in-depth discussion about what aspects of the content can be assessed by these new content features.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first generated word hypotheses for each response in using an ASR system.", "labels": [], "entities": [{"text": "ASR", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.8651056885719299}]}, {"text": "A gender independent acoustic model (AM) was trained on 800 hours of spoken responses extracted from the same English proficiency test using the Kaldi toolkit (Povey et al., 2011).", "labels": [], "entities": []}, {"text": "The AM training dataset consisted of 52,200 spoken responses from 8,700 speakers.", "labels": [], "entities": [{"text": "AM training dataset", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8862198193868002}]}, {"text": "It was based on a 5-layer DNN with p-norm nonlinearity using layer-wise supervised backpropagation training.", "labels": [], "entities": []}, {"text": "The language model (LM) was a trigram model trained using the same dataset used for AM training.", "labels": [], "entities": [{"text": "AM training", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.9413532316684723}]}, {"text": "This ASR system achieved a Word Error Rate of 23% on 600 held-out responses.", "labels": [], "entities": [{"text": "ASR", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.9528883695602417}, {"text": "Word Error Rate", "start_pos": 27, "end_pos": 42, "type": "METRIC", "confidence": 0.7708839575449625}]}, {"text": "Detailed information about the ASR system is provided in ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9761865139007568}]}, {"text": "Next, we normalized both the prompt texts and the ASR-based transcriptions of the responses; all words were tokenized, and stop words and disfluencies were removed from the texts.", "labels": [], "entities": [{"text": "ASR-based transcriptions of the responses", "start_pos": 50, "end_pos": 91, "type": "TASK", "confidence": 0.7843262076377868}]}, {"text": "The length of the original and the processed texts after removing stop words and disfluencies are summarized in.", "labels": [], "entities": []}, {"text": "The average lengths of the Independent prompts and the Integrated prompts were 41.0 and 341.6 words respectively; thus the Integrated prompts were approximately 8 times longer than Independent prompts.", "labels": [], "entities": []}, {"text": "After removing stop words and fillers, the texts were approximately 2/3 of the original texts.", "labels": [], "entities": []}, {"text": "The responses contained an average of 97 words for the Independent responses and 129 words for Integrated responses, but there were large variations across different responses.", "labels": [], "entities": []}, {"text": "After the normalization process, the length of the responses was 40% of the original responses on average.", "labels": [], "entities": [{"text": "length", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9860775470733643}]}, {"text": "The responses were shortened in larger proportion than the prompts because the responses contained disfluencies such as 'uh', 'um', which were removed.", "labels": [], "entities": []}, {"text": "From these normalized transcriptions, we created four content features as described in Section 3.", "labels": [], "entities": []}, {"text": "In addition, 35 speech-driven features were generated using the original wave files and the same ASR word hypotheses.", "labels": [], "entities": [{"text": "ASR word hypotheses", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.8014481067657471}]}, {"text": "In Experiment 1, we found that new content features could reduce the automated score errors for the lowest score point.", "labels": [], "entities": [{"text": "automated score errors", "start_pos": 69, "end_pos": 91, "type": "METRIC", "confidence": 0.6893928448359171}]}, {"text": "Based on this observation, we hypothesized that the new features could identify responses with substantial content issues and assign more accurate scores than the model based only on the speech-driven features for these responses.", "labels": [], "entities": [{"text": "accurate scores", "start_pos": 138, "end_pos": 153, "type": "METRIC", "confidence": 0.9454579055309296}]}, {"text": "In order to examine this hypothesis, we artificially created a dataset with content issues by pairing responses with mismatched prompts for feature calculation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of questions, and responses for each partition", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.952991247177124}]}, {"text": " Table 2: Descriptive analysis of the number of words in prompt texts and responses", "labels": [], "entities": []}, {"text": " Table 3: Pearson Correlation Coefficients between features considered in this study and human scores  for Independent and Integrated questions.", "labels": [], "entities": [{"text": "Pearson Correlation Coefficients", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.8330227533976237}]}, {"text": " Table 4: Correlations, weighted kappas and root mean squared error (RMSE) between the automated  scores and human scores", "labels": [], "entities": [{"text": "root mean squared error (RMSE)", "start_pos": 44, "end_pos": 74, "type": "METRIC", "confidence": 0.892670214176178}]}, {"text": " Table 5: RMSE between the human scores and automated scores generated by the 'Speech' model,  RMSE between the human scores and automated scores generated by the 'Combination' model, and the  difference in RMSE between the two models.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9033969640731812}]}, {"text": " Table 6: Comparison of the automated scores for responses with content abnormality", "labels": [], "entities": []}]}