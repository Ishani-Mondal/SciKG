{"title": [{"text": "Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring?", "labels": [], "entities": [{"text": "Sentence Planning", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7168662399053574}]}], "abstractContent": [{"text": "Responses in task-oriented dialogue systems often realize multiple propositions whose ultimate form depends on the use of sentence planning and discourse struc-turing operations.", "labels": [], "entities": []}, {"text": "For example a recommendation may consist of an explicitly evaluative utterance e.g. Chanpen Thai is the best option, along with content related by the justification discourse relation, e.g. It has great food and service , that combines multiple propositions into a single phrase.", "labels": [], "entities": []}, {"text": "While neural generation methods integrate sentence planning and surface realization in one end-to-end learning framework, previous work has not shown that neural generators can: (1) perform common sentence planning and discourse structuring operations; (2) make decisions as to whether to realize content in a single sentence or over multiple sentences; (3) generalize sentence planning and discourse relation operations beyond what was seen in training.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7701876759529114}, {"text": "sentence planning and discourse structuring", "start_pos": 197, "end_pos": 240, "type": "TASK", "confidence": 0.6699398636817933}, {"text": "generalize sentence planning", "start_pos": 358, "end_pos": 386, "type": "TASK", "confidence": 0.6461723248163859}]}, {"text": "We systematically create large training corpora that exhibit particular sentence planning operations and then test neural models to see what they learn.", "labels": [], "entities": []}, {"text": "We compare models without explicit latent variables for sentence planning with ones that provide explicit supervision during training.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7913600206375122}]}, {"text": "We show that only the models with additional supervision can reproduce sentence planning and discourse operations and generalize to situations unseen in training.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7101129442453384}]}], "introductionContent": [{"text": "Neural natural language generation (NNLG) promises to simplify the process of producing high quality responses for conversational agents by relying on the neural architecture to automatically learn how to map an input meaning representation (MR) from the dialogue manager to an output utterance).", "labels": [], "entities": [{"text": "Neural natural language generation (NNLG", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7152065336704254}]}, {"text": "For example, shows sample training data for an NNLG with a MR fora restaurant named ZIZZI, along with three reference realizations, that should allow the NNLG to learn to realize the MR as either 1, 3, or 5 sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments focus on sentence planning operations for: (1) sentence scoping, as in, where we experiment with controlling the number of sentences in the generated output; (2) distributive aggregation, as in Example 6 in, an aggregation operation that can compactly express a description when two attributes share the same value; and (3) discourse contrast, as in Example 8 in.", "labels": [], "entities": [{"text": "sentence scoping", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.7112859189510345}, {"text": "discourse contrast", "start_pos": 340, "end_pos": 358, "type": "TASK", "confidence": 0.7461252808570862}]}, {"text": "shows the general architecture, implemented in Tensorflow, based on TGen, an open-source sequence-to-sequence (seq2seq) neural generation framework ().", "labels": [], "entities": []}, {"text": "The model uses seq2seq generation with attention () with a sequence of LSTMs) for encoding and decoding, along with beamsearch and an n-best reranker.", "labels": [], "entities": []}, {"text": "The input to the sequence to sequence model is a sequence of tokens x t , t \u2208 {0, . .", "labels": [], "entities": []}, {"text": ", n} that represent the dialogue act and associated arguments.", "labels": [], "entities": []}, {"text": "Each xi is associated with an embedding vector w i of some fixed length.", "labels": [], "entities": []}, {"text": "Thus for each MR, TGen takes as input the dialogue acts represent- We also note that the evaluation of an attribute may come from the attribute itself, e.g. \"kid friendly\", or from its adjective, e.g. \"excellent service\".", "labels": [], "entities": []}, {"text": "2 https://github.com/UFAL-DSG/tgen ing system actions (recommend and inform acts) and the attributes and their values (for example, an attribute might be price range, and its value might be moderate), as shown in.", "labels": [], "entities": []}, {"text": "The MRs (and resultant embeddings) are sorted internally by dialogue act tag and attribute name.", "labels": [], "entities": []}, {"text": "For every MR in training, we have a matching reference text, which we delexicalize in pre-processing, then re-lexicalize in the generated outputs.", "labels": [], "entities": []}, {"text": "The encoder reads all the input vectors and encodes the sequence into a vector h n . At each time step t, it computes the hidden layer ht from the input wt and hidden vector at the previous time step h t\u22121 , following: All experiments use a standard LSTM decoder.", "labels": [], "entities": []}, {"text": "We describe the specific model variations for each experiment below.", "labels": [], "entities": []}, {"text": "One challenge is that NNLG models are highly sensitive to the distribution of phenomena in training data, and our previous work has shown that the outputs of NNLG models exhibit less stylistic variation than their training data ().", "labels": [], "entities": []}, {"text": "Moreover, even large corpora, such as the 50K E2E Generation Challenge corpus, may not contain particular stylistic variations.", "labels": [], "entities": [{"text": "50K E2E Generation Challenge corpus", "start_pos": 42, "end_pos": 77, "type": "DATASET", "confidence": 0.9165703654289246}]}, {"text": "For example, out of 50K crowdsourced examples in the E2E corpus, there are 1,956 examples of contrast with the operator \"but\".", "labels": [], "entities": [{"text": "E2E corpus", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.9510415196418762}]}, {"text": "There is only 1 instance of distributive aggregation because attribute values are rarely lexicalized identically in E2E.", "labels": [], "entities": []}, {"text": "To ensure that the training data contains enough examples of particular phenomena, our experiments combine crowdsourced E2E data 3 with automatically generated data from PERSON-AGE ().", "labels": [], "entities": [{"text": "E2E data", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.7311068922281265}]}, {"text": "This allows us to systematically create training data that exhibits particular sentence planning operations, or combinations of them.", "labels": [], "entities": []}, {"text": "The E2E dataset consists of pairs of reference utterances and their meaning representations (MRs), where each utterance contains up to 8 unique attributes, and each MR has multiple references.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9574331045150757}]}, {"text": "We populate PERSONAGE with the syntax/meaning mappings that it needs to produce output for the E2E meaning representations, and then automatically produce a very large (204,955 utterance/MR pairs) systematically varied sentence planning corpus.", "labels": [], "entities": []}, {"text": "It is well known that evaluation metrics used for translation such as BLEU are not well suited to evaluating generation outputs (;: they penalize stylistic variation, and don't account for the fact that different dialogue responses can be equally good, and can vary due to contextual factors).", "labels": [], "entities": [{"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9668049812316895}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9623268246650696}]}, {"text": "We also note that previous work on sentence planning has always assumed that sentence planning operations improve the quality of the output (), while our primary focus here is to determine whether an NNLG can be trained to perform such operations while maintaining semantic fidelity.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7924550771713257}]}, {"text": "Moreover, due to the large size of our controlled training sets, we observe few problems with output quality and fluency.", "labels": [], "entities": []}, {"text": "Thus we leave an evaluation of fluency and naturalness to future work, and focus hereon evaluating the multiple targets of semantic accuracy and sentence planning accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.933009147644043}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.8595277070999146}]}, {"text": "Because the MR is clearly defined, we define scripts (information extraction patterns) to measure the occurrence of the MR attributes and their values in the outputs.", "labels": [], "entities": []}, {"text": "We then compute Slot Error Rate (SER) using a variant of word error rate: where S is the number of substitutions, Dis the number of deletions, I is the number of insertions, H is the number of hallucinations and N is the number of slots in the input MR.", "labels": [], "entities": [{"text": "Slot Error Rate (SER)", "start_pos": 16, "end_pos": 37, "type": "METRIC", "confidence": 0.8204180399576823}, {"text": "word error rate", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.621500293413798}]}, {"text": "We also define scripts for evaluating the accuracy of the sentence planner's operations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9976851940155029}, {"text": "sentence planner's operations", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.7445499747991562}]}, {"text": "We check whether: (1) the output has the right number of sentences; (2) attributes with equal values are realized using distributive aggregation, and (3) discourse contrast is used when semantically appropriate.", "labels": [], "entities": []}, {"text": "Descriptions of each experiment and the results are in Section 3, Section 4, and Section 5.", "labels": [], "entities": []}, {"text": "To test whether it is possible to control basic sentence scoping with an NNLG, we experiment first with controlling the number of sentences in the generated output, as measured using the period operator.", "labels": [], "entities": [{"text": "NNLG", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9053901433944702}]}, {"text": "We experiment with two different models: \u2022 No Supervision: no additional information in the MR (only attributes and their values) \u2022 Period Count Supervision: has an additional supervision token, PERIOD, specifying the number of periods (i.e. the number of sentences) to be used in the output realization.", "labels": [], "entities": [{"text": "PERIOD", "start_pos": 195, "end_pos": 201, "type": "METRIC", "confidence": 0.9897454977035522}]}, {"text": "For sentence scoping, we construct a training set of 64,442 output/MR pairs and a test set of 398 output/MR pairs where the reference utterances for the outputs are generated from PERSONAGE.", "labels": [], "entities": [{"text": "sentence scoping", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7356067299842834}]}, {"text": "We start with the default TGen parameters and monitor the losses on Tensorboard on a subset of 3,000 validation instances from the 64,000 training set.", "labels": [], "entities": []}, {"text": "The best settings use a batch size of 20, with a minimum of 5 epochs and a maximum of 20 (with early-stopping based on validation loss).", "labels": [], "entities": []}, {"text": "We generate outputs on the test set of 398 MRs.", "labels": [], "entities": []}, {"text": "shows the accuracy of both models in terms of the counts of the output utterances that realize the MR attributes in the specified number of sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996365308761597}]}, {"text": "In the case of NOSUP, we compare the number of sentences in the generated output to those in the corresponding test reference, and for PERIODCOUNT, we compare the number of sentences in the generated output to the number of sentences we explicitly encode in the MR.", "labels": [], "entities": [{"text": "NOSUP", "start_pos": 15, "end_pos": 20, "type": "TASK", "confidence": 0.496224582195282}, {"text": "PERIODCOUNT", "start_pos": 135, "end_pos": 146, "type": "METRIC", "confidence": 0.8711803555488586}]}, {"text": "The table shows that the NO-SUP setting fails to output the correct number of sentences inmost cases (only a 22% accuracy), but the PERIODCOUNT setting makes only 2 mistakes (almost perfect accuracy), demonstrating almost perfect control of the number of output sentences with the single-token supervision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9970176219940186}, {"text": "PERIODCOUNT", "start_pos": 132, "end_pos": 143, "type": "METRIC", "confidence": 0.9172183275222778}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9975758194923401}]}, {"text": "We also show correlation levels with the gold-standard references (all correlations significant at p \u2264 0.01).", "labels": [], "entities": []}, {"text": "This yields 196 MR and reference pairs.", "labels": [], "entities": [{"text": "MR", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.8210670351982117}]}, {"text": "This experiment results in an 84% accuracy (with correlation of 0.802 with the test refs, p \u2264 0.01).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9995655417442322}]}, {"text": "When analyzing the mistakes, we observe that all of the scoping mistakes the model makes (31 in total) are the case of PERIOD=N-1.", "labels": [], "entities": [{"text": "PERIOD", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9923672080039978}]}, {"text": "These cases correspond to the right frontier of where there were fewer training instances.", "labels": [], "entities": []}, {"text": "Thus while the period supervision improves the model, it still fails on cases where there were few instances in training.", "labels": [], "entities": []}, {"text": "We performed an additional sentence scoping experiment where we specified a target sentence complexity instead of a target number of sentences, since this may more intuitively correspond to a notion of reading level or sentence complexity, where the assumption is that longer sentences are more complex).", "labels": [], "entities": []}, {"text": "We used the same training and test data, but labeled each reference as either high, medium or low complexity.", "labels": [], "entities": []}, {"text": "The number of attributes in the MR does not include the name attribute, since that is the subject of the review.", "labels": [], "entities": []}, {"text": "A reference was labeled high when there are > 2 attributes per sentence, medium when the number of attributes per sentence is > 1.5 and \u2264 2 and low when there are \u2264 1.5 attributes per sentence.", "labels": [], "entities": []}, {"text": "Aggregation describes a set of sentence planning operations that combine multiple attributes into and illustrated in Row 6 of.", "labels": [], "entities": []}, {"text": "In an SNLG setting, the generator achieves this type of aggregation by operating on syntactic trees).", "labels": [], "entities": []}, {"text": "In an NNLG setting, we hope the model will induce the syntactic structure and the mathematical operation underlying it, automatically, without explicit training supervision.", "labels": [], "entities": []}, {"text": "To prepare the training data, we limit the values for PRICE and RATING attributes to LOW, AVERAGE, and HIGH.", "labels": [], "entities": [{"text": "PRICE", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.7320404052734375}, {"text": "LOW", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9987502098083496}, {"text": "AVERAGE", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9984455704689026}, {"text": "HIGH", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9951181411743164}]}, {"text": "We reserve the combination {PRICE=HIGH, RATING=HIGH} for test, leaving two combinations of values where distribution is possible ({PRICE=LOW, RATING=LOW} and {PRICE=AVERAGE, RATING=AVERAGE}).", "labels": [], "entities": [{"text": "HIGH", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.6553642749786377}, {"text": "AVERAGE", "start_pos": 165, "end_pos": 172, "type": "METRIC", "confidence": 0.8994534015655518}, {"text": "AVERAGE", "start_pos": 181, "end_pos": 188, "type": "METRIC", "confidence": 0.8847524523735046}]}, {"text": "We then use all three values in MRs where the price and rating are not the same {PRICE=LOW, RAT-ING=HIGH}.", "labels": [], "entities": [{"text": "MRs", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.8752468228340149}, {"text": "PRICE", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9081282019615173}, {"text": "LOW", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.7788551449775696}, {"text": "RAT-ING", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9500903487205505}, {"text": "HIGH", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9009217619895935}]}, {"text": "This ensures that the model does seethe value HIGH in training, but never in a setting where distribution is possible.", "labels": [], "entities": [{"text": "HIGH", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9755370616912842}]}, {"text": "We always distribute when possible, so every MR where the values are the same uses distribution.", "labels": [], "entities": []}, {"text": "All other opportunities for aggregation, in the same sentence or in other training sentences, use the other aggregation operations defined in PERSONAGE as specified in, with equal probability.", "labels": [], "entities": []}, {"text": "As above, we start with the default TGen parameters and monitor the losses on Tensorboard on subset of 3,000 validation instances from the 63,000 training set.", "labels": [], "entities": []}, {"text": "The best settings use a batch size of 20, with a minimum of 5 epochs and a maximum of 20 epochs with early-stopping.", "labels": [], "entities": []}, {"text": "shows the accuracy of each model overall on all 4 values, as well as the accuracy specifically on HIGH, the only distribution value unseen in train.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996209144592285}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.999578058719635}, {"text": "HIGH", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9788689017295837}]}, {"text": "Model NO-SUP has a low overall accuracy, and is completely unable to generalize to HIGH, which is unseen in training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9991707801818848}, {"text": "HIGH", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.6882285475730896}]}, {"text": "It is frequently able to use the HIGH value, but is notable to distribute (generating output like high cost and cost).", "labels": [], "entities": []}, {"text": "Model BINARY is by far the best performing model, with an almost perfect accuracy (it is able to distribute over LOW and AVERAGE perfectly), but makes some mistakes when trying to distribute over HIGH; specifically, while it is always able to distribute, it may use an incorrect value (LOW or AVERAGE).", "labels": [], "entities": [{"text": "BINARY", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9884042739868164}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9993377327919006}, {"text": "AVERAGE", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9754946231842041}, {"text": "AVERAGE", "start_pos": 293, "end_pos": 300, "type": "METRIC", "confidence": 0.8842177391052246}]}, {"text": "Whenever BINARY correctly distributes over HIGH, it interestingly always selects attribute RATING before COST, realizing the output as high rating and price.", "labels": [], "entities": [{"text": "BINARY", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9494644999504089}, {"text": "HIGH", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.7737202644348145}, {"text": "RATING", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.8556287884712219}, {"text": "COST", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.5613847970962524}]}, {"text": "Also, BINARY is consistent even when it incorrectly uses the value LOW instead of HIGH: it always selects the attribute price before rating.", "labels": [], "entities": [{"text": "BINARY", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9955319166183472}, {"text": "LOW", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9860739707946777}, {"text": "HIGH", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9923413991928101}]}, {"text": "Xname is a low customer rated coffee shop offering xcuisine food in the xlocation.", "labels": [], "entities": [{"text": "Xname", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8488298654556274}]}, {"text": "Yes, it is child friendly, but the price range is more than $30. 3,500 contrast 3,540 contrast 11K  Persuasive settings such as recommending restaurants, hotels or travel options often have a critical discourse structure (Scott and de.", "labels": [], "entities": []}, {"text": "For example a recommendation may consist of an explicitly evaluative utterance e.g. Chanpen Thai is the best option, along with content related by the justify discourse relation, e.g. It has great food and service, as in.", "labels": [], "entities": []}, {"text": "Our experiments focus on DISCOURSE-CONTRAST.", "labels": [], "entities": [{"text": "DISCOURSE-CONTRAST", "start_pos": 25, "end_pos": 43, "type": "METRIC", "confidence": 0.40408292412757874}]}, {"text": "We developed a script to find contrastive sentences in the 40K E2E training set by searching for any instance of a contrast cue word, such as but, although, and even if.", "labels": [], "entities": [{"text": "40K E2E training set", "start_pos": 59, "end_pos": 79, "type": "DATASET", "confidence": 0.9183185398578644}]}, {"text": "While this data size is comparable to the 3-4K instances used in prior work (, we anticipated that it might not be enough data to properly test whether an NNLG can learn to produce discourse contrast.", "labels": [], "entities": []}, {"text": "We were also interested in testing whether synthetic data would improve the ability of the NNLG to produce contrastive utterances while maintaining semantic fidelity.", "labels": [], "entities": []}, {"text": "Thus we used PERSONAGE with its native database of New York City restaurants (NYC) to generate an additional 3,500 examples of one form of contrast using only the discourse marker but, which are most similar to the examples in the E2E data.", "labels": [], "entities": [{"text": "E2E data", "start_pos": 231, "end_pos": 239, "type": "DATASET", "confidence": 0.9471063613891602}]}, {"text": "illustrates both PERSONAGE and E2E contrast examples.", "labels": [], "entities": [{"text": "PERSONAGE", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.8239458799362183}]}, {"text": "While PERSONAGE also contains JUSTIFICATIONS, which could possibly confuse the NNLG, it offers many more attributes that can be contrasted and thus more unique instances of contrast.", "labels": [], "entities": [{"text": "JUSTIFICATIONS", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.9140074849128723}, {"text": "NNLG", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.9218037724494934}]}, {"text": "We create 4 training datasets with contrast data in order to systematically test the effect of the combined training set.", "labels": [], "entities": []}, {"text": "provides an overview of the training sets, with their rationales below.", "labels": [], "entities": []}, {"text": "This dataset consists of all instances of contrast in the E2E training data, i.e. 3,540 E2E references.", "labels": [], "entities": [{"text": "E2E training data", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.8793887694676717}]}, {"text": "We created a training set of 7k references by supplementing the E2E contrastive references with an equal number of PERSONAGE references.", "labels": [], "entities": [{"text": "PERSONAGE", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9730124473571777}]}, {"text": "Since 7K is smaller than desirable for training an NNLG, we created several additional training sets with the aim of helping the model learn to correctly realize domain semantics while still being able to produce contrastive utterances.", "labels": [], "entities": []}, {"text": "We thus added an additional 4K crowdsourced E2E data that was not contrastive to our training data, fora total of 11,065.", "labels": [], "entities": []}, {"text": "We created an additional larger training set by adding more E2E data, again to test the effect of increasing the size of the training set on realization of domain semantics, without a significant decrease in our ability to produce contrastive utterances.", "labels": [], "entities": []}, {"text": "We added an additional 14K E2E references, fora total of 21,065.", "labels": [], "entities": []}, {"text": "We perform two experiments with the 21K training set.", "labels": [], "entities": [{"text": "21K training set", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.7117330531279246}]}, {"text": "First we trained on the MR and reference exactly as we had done for the 7K and 11K training sets.", "labels": [], "entities": [{"text": "MR", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.8780936598777771}, {"text": "reference", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9710004925727844}]}, {"text": "The second experiment added a contrast token during training time with values of either 1 (contrast) or 0 (no contrast) to test if that would achieve better control of contrast.", "labels": [], "entities": []}, {"text": "To have a potential for contrast there must bean attribute with a positive value and another attribute with a negative value in the same MR.", "labels": [], "entities": []}, {"text": "We constructed 3 different test sets, two for E2E and one for NYC.", "labels": [], "entities": [{"text": "E2E", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9465163350105286}, {"text": "NYC", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9255142211914062}]}, {"text": "We created a delexicalized version of the test set used in the E2E generation challenge.", "labels": [], "entities": [{"text": "E2E generation challenge", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.6956510742505392}]}, {"text": "This resulted in a test of 82 MRs of which only 25 could support contrast (E2E TEST).", "labels": [], "entities": [{"text": "MRs", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.8268083333969116}, {"text": "contrast", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9878437519073486}, {"text": "E2E TEST)", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.7544120152791342}]}, {"text": "In order to allow fora better test of contrast, we constructed an additional test set of 500 E2E MRs all of which could support contrast (E2E CONTRAST TEST).", "labels": [], "entities": [{"text": "E2E MRs", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.5845268815755844}, {"text": "CONTRAST TEST)", "start_pos": 142, "end_pos": 156, "type": "METRIC", "confidence": 0.7236376802126566}]}, {"text": "For the NYC test, which provides many opportunities for contrast, we created a dataset of 785 MRs that were different than those seen in training (NYC TEST).", "labels": [], "entities": [{"text": "NYC test", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.9560807645320892}, {"text": "MRs", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.8739089965820312}, {"text": "NYC TEST)", "start_pos": 147, "end_pos": 156, "type": "DATASET", "confidence": 0.7417423526446024}]}, {"text": "At test time, in the 21K contrast token experiment, we utilize the contrast token as we did in training., and NYC test set in. shows the results for testing on the original E2E test set, where we only have 25 instances with the possibility for contrast.", "labels": [], "entities": [{"text": "NYC test set", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.937536895275116}, {"text": "E2E test set", "start_pos": 173, "end_pos": 185, "type": "DATASET", "confidence": 0.9030742049217224}]}, {"text": "Overall, the table shows large performance improvements with the CONTRAST token supervision for 21K for both slot errors and correct contrast.", "labels": [], "entities": [{"text": "CONTRAST token supervision", "start_pos": 65, "end_pos": 91, "type": "DATASET", "confidence": 0.7250209252039591}]}, {"text": "On the E2E test set, the the 3K E2E training set gives a slot error rate of .38 and only 15% correct contrast.", "labels": [], "entities": [{"text": "E2E test set", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9656271934509277}, {"text": "3K E2E training set", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.7908562421798706}, {"text": "slot error rate", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.9485785365104675}, {"text": "correct contrast", "start_pos": 93, "end_pos": 109, "type": "METRIC", "confidence": 0.8510225117206573}]}, {"text": "The 7K training set, supplemented with additional generated contrast examples gets a correct contrast of .41 but a much higher slot error rate.", "labels": [], "entities": [{"text": "7K training set", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7714465856552124}, {"text": "correct contrast", "start_pos": 85, "end_pos": 101, "type": "METRIC", "confidence": 0.8350853323936462}, {"text": "slot error rate", "start_pos": 127, "end_pos": 142, "type": "METRIC", "confidence": 0.940162718296051}]}, {"text": "Interestinglyx, the 11K dataset is much better than the 3K for contrast correct, suggesting a positive effect for the automatically generated contrast examples along with more E2E training data.", "labels": [], "entities": []}, {"text": "The 21K set without the contrast token does not attempt contrast since the frequency of contrast data is low, but with the CONTRAST token, it attempts contrast every time it is possible (25/25 instances).", "labels": [], "entities": []}, {"text": "In with only contrast data, we see similar trends, with the lowest slot error rate (.16) and highest correct contrast (.75) ratios for the experiment with token supervision on 21K.", "labels": [], "entities": [{"text": "slot error rate", "start_pos": 67, "end_pos": 82, "type": "METRIC", "confidence": 0.9426828225453695}, {"text": "correct contrast (.75) ratios", "start_pos": 101, "end_pos": 130, "type": "METRIC", "confidence": 0.9604821999867758}]}, {"text": "Again, we see much better performance from the 11K set than the 3K and 7K in terms of slot error and correct contrast, indicating that more training data (even if that data does not contain contrast) helps the model.", "labels": [], "entities": [{"text": "slot error", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9039095640182495}, {"text": "correct contrast", "start_pos": 101, "end_pos": 117, "type": "METRIC", "confidence": 0.8521310985088348}]}, {"text": "As before, we see very low contrast attempts with 21K without supervision, with a huge increase in the number of contrast attempts when using token supervision (422/500).", "labels": [], "entities": []}, {"text": "also shows large performance improvements from the use of the CONTRAST token supervision for the NYC test set, again with improvements for the 21K CONTRAST in both slot error rate and incorrect contrast.", "labels": [], "entities": [{"text": "NYC test set", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9365749557813009}, {"text": "slot error rate", "start_pos": 164, "end_pos": 179, "type": "METRIC", "confidence": 0.8874900937080383}]}, {"text": "Interestingly, while we get the highest correct contrast ratio of .85 with 21K CONTRAST, we actually see fewer contrast attempts, showing that the most explicitly supervised model is becoming more selective when deciding when to do contrast.", "labels": [], "entities": [{"text": "correct contrast ratio", "start_pos": 40, "end_pos": 62, "type": "METRIC", "confidence": 0.8235231836636862}, {"text": "CONTRAST", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.984369158744812}]}, {"text": "When training on the 7K dataset, the neural model always produces a contrastive utterance for the NYC MRs (all the NYC data is contrastive).", "labels": [], "entities": [{"text": "7K dataset", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.9709962904453278}, {"text": "NYC MRs", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.8079343736171722}]}, {"text": "Although it never sees any NYC non-contrastive MRs, the additional E2E training data allows it to improve its ability to decide when to contrast (Row 21K as well as improving the slot error rate in the final experiment.", "labels": [], "entities": [{"text": "slot error rate", "start_pos": 179, "end_pos": 194, "type": "METRIC", "confidence": 0.9272774259249369}]}], "tableCaptions": [{"text": " Table 4: Distribution of Training Data", "labels": [], "entities": [{"text": "Distribution of Training", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8626025517781576}]}, {"text": " Table 5: Sentence Scoping Results", "labels": [], "entities": [{"text": "Sentence Scoping", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9836478531360626}]}, {"text": " Table 7: Distributive Aggregation Results", "labels": [], "entities": [{"text": "Distributive Aggregation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9414753615856171}]}, {"text": " Table 10: Slot Error Rates and Contrast for E2E", "labels": [], "entities": [{"text": "Slot Error", "start_pos": 11, "end_pos": 21, "type": "TASK", "confidence": 0.7489496171474457}, {"text": "Contrast", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9546833634376526}, {"text": "E2E", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.7256237864494324}]}, {"text": " Table 11: Slot Error Rates and Contrast for E2E,  Contrast Only", "labels": [], "entities": [{"text": "Contrast", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9809565544128418}]}, {"text": " Table 12: Slot Error Rates and Contrast for NYC", "labels": [], "entities": [{"text": "Slot Error", "start_pos": 11, "end_pos": 21, "type": "TASK", "confidence": 0.862643301486969}]}]}