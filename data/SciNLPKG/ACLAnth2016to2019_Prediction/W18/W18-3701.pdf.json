{"title": [{"text": "Generating Questions for Reading Comprehension using Coherence Relations", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a technique for generating complex reading comprehension questions from a discourse that are more useful than factual ones derived from assertions.", "labels": [], "entities": []}, {"text": "Our system produces a set of general-level questions using coherence relations.", "labels": [], "entities": []}, {"text": "These evaluate comprehension abilities like comprehensive analysis of the text and its structure, correct identification of the author's intent, thorough evaluation of stated arguments; and deduction of the high-level semantic relations that hold between text spans.", "labels": [], "entities": []}, {"text": "Experiments performed on the RST-DT corpus allow us to conclude that our system possesses a strong aptitude for generating intricate questions.", "labels": [], "entities": [{"text": "RST-DT corpus", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.7872911691665649}]}, {"text": "These questions are capable of effectively assessing student interpretation of text.", "labels": [], "entities": [{"text": "student interpretation of text", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.7624569460749626}]}], "introductionContent": [{"text": "The argument fora strong correlation between question difficulty and student perception comes from Bloom's taxonomy ().", "labels": [], "entities": []}, {"text": "It is a framework that attempts to categorize question difficulty in accordance with educational goals.", "labels": [], "entities": []}, {"text": "The framework has undergone several revisions overtime and currently has six levels of perception in the cognitive domain: Remembering, Understanding, Applying, Analyzing, Evaluating and Creating ().", "labels": [], "entities": [{"text": "Remembering", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.9526993036270142}]}, {"text": "The goal of a Question Generation (QG) system should be to generate meaningful questions that cater to the higher levels of this hierarchy and are therefore adept at gauging comprehension skills.", "labels": [], "entities": [{"text": "Question Generation (QG)", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.8233180463314056}]}, {"text": "The scope of several QG tasks has been severely restricted to restructuring declarative sentences into specific level questions.", "labels": [], "entities": []}, {"text": "For example, consider the given text and the questions that follow.", "labels": [], "entities": []}, {"text": "Input: The project under construction will raise Las Vegas' supply of rooms by 20%.", "labels": [], "entities": [{"text": "Input", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9595116376876831}]}, {"text": "Clark county will have 18000 new jobs.", "labels": [], "entities": [{"text": "Clark county", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.894825279712677}]}, {"text": "Question 1: What will raise Las Vegas' supply of rooms by 20%?", "labels": [], "entities": []}, {"text": "Question 2: Why will Clark County have 18000 new jobs?", "labels": [], "entities": []}, {"text": "From the perspective of Bloom's Taxonomy, questions like Question 1 cater to the 'Remembering' level of the hierarchy and are not apt for evaluation purposes.", "labels": [], "entities": [{"text": "Remembering", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.7235644459724426}]}, {"text": "Alternatively, questions like Question 2 would be associated with the 'Analyzing' level as these would require the student to draw a connection between the events, 'increase in room supply in Las Vegas' and 'creation of 18000 new jobs in Clark County'.", "labels": [], "entities": []}, {"text": "Further, such questions would be more relevant in the context of an entire document or paragraph; and serve as better reading comprehension questions.", "labels": [], "entities": []}, {"text": "This paper describes a generic framework for generating comprehension questions from short edited texts using coherence relations.", "labels": [], "entities": []}, {"text": "It is organized as follows: Section 2 elaborates on previously designed QG systems and outlines their limitations.", "labels": [], "entities": []}, {"text": "We also discuss Rhetorical Structure Theory (RST), which lays the linguistic foundations for discourse parsing.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.8240360816319784}, {"text": "discourse parsing", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7151752859354019}]}, {"text": "In Section 3, we explain our model and describe the syntactic transformations and templates applied to text spans for performing QG.", "labels": [], "entities": []}, {"text": "In Section 4, we discuss experiments performed on the annotated RST-DT corpus and measure the quality of questions generated by the system.", "labels": [], "entities": [{"text": "RST-DT corpus", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.7853330969810486}]}, {"text": "Proposed evaluation criteria address both the grammaticality and complexity of generated questions.", "labels": [], "entities": []}, {"text": "We have also compared our system with a baseline to show that our system is able to generate complex questions.", "labels": [], "entities": []}, {"text": "Finally, in Section 5, we provide our conclusions and suggest potential avenues for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the quality of generated questions, we used a set of criteria that are defined below.", "labels": [], "entities": []}, {"text": "We considered and designed metrics that measure both the correctness and difficulty of the question.", "labels": [], "entities": [{"text": "difficulty", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.8862313628196716}]}, {"text": "All the metrics use a two-point scale: a score of 1 indicates the question successfully passed the metric, a score of 0 indicates otherwise.", "labels": [], "entities": []}, {"text": "\u2022 Grammatic correctness of questions: This metric checks whether the question generated is only syntactically correct.", "labels": [], "entities": [{"text": "Grammatic correctness of questions", "start_pos": 2, "end_pos": 36, "type": "TASK", "confidence": 0.7432898506522179}]}, {"text": "We do not take into account the semantics of the question.", "labels": [], "entities": []}, {"text": "\u2022 Semantic correctness of questions: We account for the meaning of the generated question and whether it makes sense to the reader.", "labels": [], "entities": []}, {"text": "It is assumed if a question is grammatically incorrect, it is also semantically incorrect.", "labels": [], "entities": []}, {"text": "\u2022 Superfluous use of language: Since we are not focusing on shortening sentences or removing redundant data from the text, generated questions may contain information not required by the student to arrive at the answer.", "labels": [], "entities": []}, {"text": "Such questions should be refined to make them shorter and sound more fluent or natural.", "labels": [], "entities": []}, {"text": "\u2022 Question appropriateness: This metric judges whether the question is posed correctly i.e. we check if the question is not ambivalent and makes complete sense to the reader.", "labels": [], "entities": []}, {"text": "\u2022 Nature of coherence relation: Coherence relations are classified into two categories: explicit (the relations that are made apparent through using discourse connectives) and implicit (the relations that require a deep understanding of the text).", "labels": [], "entities": []}, {"text": "Questions generated through explicit coherence relations are easier to attempt as compared to the ones generated via implicit coherence relations.", "labels": [], "entities": []}, {"text": "We assign a score of 1 to a question generated from an implicit coherence relation and 0 to that generated from an explicit relation.", "labels": [], "entities": []}, {"text": "\u2022 Nature of question: We check for the nature of generated question: If both the answer and question are derived from the same sentence, we assign a score of 0, otherwise the score will be 1.", "labels": [], "entities": []}, {"text": "\u2022 Number of inference steps (): To evaluate this metric, we consider three semantic concepts: paraphrase detection, entity co-reference resolution and event co-reference resolution.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.8077326416969299}, {"text": "entity co-reference resolution", "start_pos": 116, "end_pos": 146, "type": "TASK", "confidence": 0.6058509349822998}, {"text": "event co-reference resolution", "start_pos": 151, "end_pos": 180, "type": "TASK", "confidence": 0.6645306944847107}]}, {"text": "We consider a score for each concept: 1 if the concept is required and 0 if not.", "labels": [], "entities": []}, {"text": "We take the arithmetic mean of these scores to get the average number of inference steps fora question.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Statistics for Generated Questions", "labels": [], "entities": []}, {"text": " Table 6: Average score for the evaluation criteria. Here R1: Explanation, R2: Background, R3: Solu- tionhood, R4: Cause, R5: Result, R6: Condition, R7: Evaluation. The average scores for each criterion  are indicated in the last column.", "labels": [], "entities": [{"text": "Explanation", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.942215621471405}, {"text": "Result", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9530661702156067}]}]}