{"title": [{"text": "Deep contextualized word representations for detecting sarcasm and irony", "labels": [], "entities": [{"text": "detecting sarcasm and irony", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.8766688108444214}]}], "abstractContent": [{"text": "Predicting context-dependent and non-literal utterances like sarcastic and ironic expressions still remains a challenging task in NLP, as it goes beyond linguistic patterns, encompassing commonsense and shared knowledge as crucial components.", "labels": [], "entities": [{"text": "Predicting context-dependent and non-literal utterances like sarcastic and ironic expressions", "start_pos": 0, "end_pos": 93, "type": "TASK", "confidence": 0.7320482015609742}]}, {"text": "To capture complex morpho-syntactic features that can usually serve as indicators for irony or sarcasm across dynamic contexts, we propose a model that uses character-level vector representations of words, based on ELMo.", "labels": [], "entities": []}, {"text": "We test our model on 7 different datasets derived from 3 different data sources, providing state-of-the-art performance in 6 of them, and otherwise offering competitive results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sarcastic and ironic expressions are prevalent in social media and, due to the tendency to invert polarity, play an important role in the context of opinion mining, emotion recognition and sentiment analysis).", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 149, "end_pos": 163, "type": "TASK", "confidence": 0.7744867205619812}, {"text": "emotion recognition", "start_pos": 165, "end_pos": 184, "type": "TASK", "confidence": 0.7205940335988998}, {"text": "sentiment analysis", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.9134934544563293}]}, {"text": "Sarcasm and irony are two closely related linguistic phenomena, with the concept of meaning the opposite of what is literally expressed at its core.", "labels": [], "entities": []}, {"text": "There is no consensus in academic research on the formal definition, both terms are non-static, depending on different factors such as context, domain and even region in some cases.", "labels": [], "entities": []}, {"text": "In light of the general complexity of natural language, this presents a range of challenges, from the initial dataset design and annotation to computational methods and evaluation.", "labels": [], "entities": []}, {"text": "The difficulties lie in capturing linguistic nuances, context-dependencies and latent meaning, due to richness of dynamic variants and figurative use of language (.", "labels": [], "entities": []}, {"text": "The automatic detection of sarcastic expressions often relies on the contrast between positive and negative sentiment (.", "labels": [], "entities": [{"text": "automatic detection of sarcastic expressions", "start_pos": 4, "end_pos": 48, "type": "TASK", "confidence": 0.8078235268592835}]}, {"text": "This incongruence can be found on a lexical level with sentiment-bearing words, as in \"I love being ignored\".", "labels": [], "entities": []}, {"text": "In more complex linguistic settings an action or a situation can be perceived as negative, without revealing any affect-related lexical elements.", "labels": [], "entities": []}, {"text": "The intention of the speaker as well as common knowledge or shared experience can be key aspects, as in \"I love waking up at 5 am\", which can be sarcastic, but not necessarily.", "labels": [], "entities": []}, {"text": "Similarly, verbal irony is referred to as saying the opposite of what is meant and based on sentiment contrast, whereas situational irony is seen as describing circumstances with unexpected consequences.", "labels": [], "entities": [{"text": "verbal irony", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.73863884806633}]}, {"text": "Empirical studies have shown that there are specific linguistic cues and combinations of such that can serve as indicators for sarcastic and ironic expressions.", "labels": [], "entities": []}, {"text": "Lexical and morpho-syntactic cues include exclamations and interjections, typographic markers such as all caps, quotation marks and emoticons, intensifiers and hyperboles).", "labels": [], "entities": []}, {"text": "In the case of Twitter, the usage of emojis and hashtags has also proven to help automatic irony detection.", "labels": [], "entities": [{"text": "automatic irony detection", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.6648690501848856}]}, {"text": "We propose a purely character-based architecture which tackles these challenges by allowing us to use a learned representation that models features derived from morpho-syntactic cues.", "labels": [], "entities": []}, {"text": "To do so, we use deep contextualized word representations, which have recently been used to achieve the state of the art on six NLP tasks, including sentiment analysis (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.9665871858596802}]}, {"text": "We test our proposed architecture on 7 different irony/sarcasm datasets derived from 3 different data sources, providing state-of-the-art performance in 6 of them and otherwise offering competitive results, showing the effectiveness of our proposal.", "labels": [], "entities": []}, {"text": "We make our code available at https://github.com/ epochx/elmo4irony.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.7526519894599915}, {"text": "sarcasm or irony", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.6284132897853851}]}, {"text": "Below we describe each dataset, please see below fora summary.", "labels": [], "entities": []}, {"text": "Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets (Van Hee et al., 2018).", "labels": [], "entities": [{"text": "SemEval 2018 Task 3", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.6001347303390503}]}, {"text": "The dataset was manually annotated using binary labels.", "labels": [], "entities": []}, {"text": "We also use the dataset by, which is manually annotated for sarcasm.", "labels": [], "entities": []}, {"text": "Finally, we use the dataset by, who collected a user self-annotated corpus of tweets with the #sar-casm hashtag.", "labels": [], "entities": []}, {"text": "Reddit and the political subset, SARC 2.0 pol.", "labels": [], "entities": [{"text": "Reddit", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9370850324630737}, {"text": "SARC 2.0 pol", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.7962575356165568}]}, {"text": "Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC).", "labels": [], "entities": [{"text": "Internet Argument Corpus (IAC)", "start_pos": 119, "end_pos": 149, "type": "DATASET", "confidence": 0.7501007169485092}]}, {"text": "Compared to other datasets in our selection, these differ mainly in text length and structure complexity ().", "labels": [], "entities": []}, {"text": "In, we see a notable difference in terms of size among the Twitter datasets.", "labels": [], "entities": [{"text": "Twitter datasets", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.8725942671298981}]}, {"text": "Given this circumstance, and in light of the findings by Van Hee et al.", "labels": [], "entities": []}, {"text": "(2018), we are interested in studying how the addition of external soft-annotated data impacts on the performance.", "labels": [], "entities": []}, {"text": "Thus, in addition to the datasets introduced before, we use two corpora for augmentation purposes.", "labels": [], "entities": []}, {"text": "The first dataset was collected using the Twitter API, targeting tweets with the hashtags #sarcasm or #irony, resulting on a total of 180,000 and 45,000 tweets respectively.", "labels": [], "entities": []}, {"text": "On the other hand, to obtain non-sarcastic and nonironic tweets, we relied on the SemEval 2018 Task 1 dataset.", "labels": [], "entities": [{"text": "SemEval 2018 Task 1 dataset", "start_pos": 82, "end_pos": 109, "type": "DATASET", "confidence": 0.7318240165710449}]}, {"text": "To augment each dataset with our external data, we first filter out tweets that are not in English using language guessing systems.", "labels": [], "entities": []}, {"text": "We later extract all the hashtags in each target dataset and proceed to augment only using those external tweets that contain any of these hashtags.", "labels": [], "entities": []}, {"text": "This allows us to, for each class, add a total of 36,835 tweets for the Pt\u00e1\u010dek corpus, 8,095 for the Riloff corpus and 26,168 for the SemEval-2018 corpus.", "labels": [], "entities": [{"text": "Pt\u00e1\u010dek corpus", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.943458765745163}, {"text": "Riloff corpus", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.8652011454105377}, {"text": "SemEval-2018 corpus", "start_pos": 134, "end_pos": 153, "type": "DATASET", "confidence": 0.8291367888450623}]}, {"text": "In terms of pre-processing, as in our case the preservation of morphological structures is crucial, the amount of normalization is minimal.", "labels": [], "entities": []}, {"text": "Concretely, we forgo stemming or lemmatizing, punctuation removal and lowercasing.", "labels": [], "entities": [{"text": "stemming", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.9647932052612305}, {"text": "punctuation removal", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6608821600675583}]}, {"text": "We limit ourselves to replacing user mentions and URLs with one generic token respectively.", "labels": [], "entities": []}, {"text": "In the case of the SemEval-2018 dataset, an additional step was to remove the hashtags #sarcasm, #irony and #not, as they are the artifacts used for creating the dataset.", "labels": [], "entities": [{"text": "SemEval-2018 dataset", "start_pos": 19, "end_pos": 39, "type": "DATASET", "confidence": 0.8177299797534943}]}, {"text": "For tokenizing, we use a variation of the Twokenizer () to better deal with emojis.", "labels": [], "entities": [{"text": "Twokenizer", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.82560133934021}]}, {"text": "Our models are trained using Adam with a learning rate of 0.001 and a decay rate of 0.5 when there is no improvement on the accuracy on the validation set, which we use to select the best models.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.9509866535663605}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9987785220146179}]}, {"text": "We also experimented using a slanted triangular learning rate scheme, which was shown by to deliver excellent results on several tasks, but in practice we did not obtain significant differences.", "labels": [], "entities": []}, {"text": "We experimented with batch sizes of 16, 32 and 64, and dropouts ranging from 0.1 to 0.5.", "labels": [], "entities": []}, {"text": "The size of the LSTM hidden layer was fixed to 1,024, based on our preliminary experiments.", "labels": [], "entities": [{"text": "LSTM hidden layer", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.6372783780097961}]}, {"text": "We do not train the ELMo embeddings, but allow their dropouts to be active during training.", "labels": [], "entities": []}, {"text": "For each dataset, the top row denotes our baseline and the second row shows our best comparable model.", "labels": [], "entities": []}, {"text": "Rows with FULL models denote our best single model trained with all the development available data, without any other preprocessing other than mentioned in the previous section.", "labels": [], "entities": [{"text": "FULL", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.993024468421936}]}, {"text": "In the case of the Twitter datasets, rows indicated as AUG refer to our the models trained using the augmented version of the corresponding datasets.", "labels": [], "entities": [{"text": "Twitter datasets", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.7890283763408661}, {"text": "AUG", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9959404468536377}]}], "tableCaptions": [{"text": " Table 1: Benchmark datasets: Tweets, Reddit posts and online debates for sarcasm and irony detection.", "labels": [], "entities": [{"text": "Benchmark datasets", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.7609492838382721}, {"text": "sarcasm and irony detection", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.7228893712162971}]}, {"text": " Table 2: Summary of our obtained results.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9291685223579407}]}]}