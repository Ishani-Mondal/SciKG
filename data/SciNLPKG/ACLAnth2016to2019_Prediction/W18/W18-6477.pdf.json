{"title": [{"text": "An Unsupervised System for Parallel Corpus Filtering", "labels": [], "entities": [{"text": "Parallel Corpus Filtering", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.6938966115315756}]}], "abstractContent": [{"text": "In this paper we describe LMU Munich's submission for the WMT 2018 Parallel Corpus Filtering shared task which addresses the problem of cleaning noisy parallel corpora.", "labels": [], "entities": [{"text": "WMT 2018 Parallel Corpus Filtering shared task", "start_pos": 58, "end_pos": 104, "type": "TASK", "confidence": 0.6378441112382072}]}, {"text": "The task of mining and cleaning parallel sentences is important for improving the quality of machine translation systems, especially for low-resource languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7227344810962677}]}, {"text": "We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal.", "labels": [], "entities": []}, {"text": "After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing mono-lingual similarity as well.", "labels": [], "entities": []}, {"text": "Our unsupervised system achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems , while not requiring any parallel training data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9994791150093079}]}], "introductionContent": [{"text": "Machine translation is important for eliminating language barriers in everyday life.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7834815680980682}]}, {"text": "To train systems which can produce good quality translations large parallel corpora are needed.", "labels": [], "entities": []}, {"text": "Mining parallel sentences from various sources in order to train better performing MT systems is essential, especially for low resource languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.9819281101226807}]}, {"text": "Previous efforts 1 showed that it is possible to crawl parallel data from the web, but also showed that additional steps are necessary to filter noisy sentence pairs.", "labels": [], "entities": []}, {"text": "In this paper we introduce our approach to filter noisy parallel corpora without the need of any initial bilingual signal to train the filtering system.", "labels": [], "entities": []}, {"text": "We participate in the WMT 2018 Parallel Corpus Filtering shared task with our system which tackles the problem of selecting the best quality 1 https://paracrawl.eu sentence pairs for training both statistical and neural MT systems (.", "labels": [], "entities": [{"text": "WMT 2018 Parallel Corpus Filtering shared task", "start_pos": 22, "end_pos": 68, "type": "TASK", "confidence": 0.7370102575847081}]}, {"text": "A lot of previous work has studied the problem of parallel data cleaning.Esp\u00ec a- proposed BiTextor which filters data based on sentence alignment scores and URL information.", "labels": [], "entities": [{"text": "parallel data cleaning.Esp\u00ec", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.7003987431526184}]}, {"text": "Similarly, word alignments and language modeling were used in) to select sentence pairs that are useful for training an MT system.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.7789836525917053}, {"text": "language modeling", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7035537660121918}, {"text": "MT", "start_pos": 120, "end_pos": 122, "type": "TASK", "confidence": 0.9837234020233154}]}, {"text": "proposed Zipporah, a logistic regression based model that uses bag-of-words translation features to measure fluency and adequacy in order to score sentence pairs.", "labels": [], "entities": []}, {"text": "Another line of work is to select data based on the target domain.", "labels": [], "entities": []}, {"text": "A static sentence-selection method was used for domain adaptation based on the internal sentence embedding of NMT () while van der used domain-based cross-entropy as a criterion to gradually fine-tune the NMT training in a dynamic manner.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7977626919746399}]}, {"text": "In contrast with previous work, we do not rely on any bilingual supervision, making our approach applicable to language pairs which lack initial parallel resources.", "labels": [], "entities": []}, {"text": "Similarly to the work of, where word embeddings were used to mine monolingual sentence pairs for text simplification, we use a word level metric to compute sentence pair similarity in a computationally efficient way.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7264578491449356}]}, {"text": "Our approach consists of three steps.", "labels": [], "entities": []}, {"text": "Due to the noisiness of the input data we use a pre-filtering step which detects sentences which are not useful.", "labels": [], "entities": []}, {"text": "We developed a simple rule-based method which looks for sentence pairs which for example came from the wrong languages or have significantly different lengths.", "labels": [], "entities": []}, {"text": "As a second step, we calculate sentence pair similarities using bilingual word embeddings and orthographic information.", "labels": [], "entities": []}, {"text": "In the third step, we perform post-ranking where we counterweight source language sentences which are less fluent or redundant using language modeling and monolingual document similarity respectively.", "labels": [], "entities": []}, {"text": "Our system is fully unsupervised, i.e., we do not use any parallel data for the training of our methods.", "labels": [], "entities": []}, {"text": "We show results on the official test sets of the shared task which includes six datasets from different sources.", "labels": [], "entities": []}, {"text": "Although, our method is fully unsupervised it achieves good performance on the extrinsic task of training MT systems on the filtered parallel data, scoring only 2.17 BLEU points behind the best systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.9508835077285767}, {"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9992510676383972}]}], "datasetContent": [{"text": "The goal of the shared task is, given a noisy parallel corpus, to filter candidate sentence pairs that are most useful for training MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.9109266400337219}]}, {"text": "Candidate pairs have to be scored based on the predicted quality of the corresponding candidate where the scores do not have a special meaning except that higher values indicate better quality.", "labels": [], "entities": []}, {"text": "To produce the actual training data for the MT systems the scored corpus is sampled using an official tool, released by the organizers, which samples sentences with a probability proportional to their scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9803393483161926}]}, {"text": "To evaluate systems two setups were performed: (i) sampling 10M tokens and (ii) 100M tokens from the scored corpus using the released sampler tool.", "labels": [], "entities": []}, {"text": "The quality of the resulting subsets is determined by the quality of a German-English SMT ( and an NMT (JunczysDowmunt et al., 2018) system trained on this data and using BLEU to measure translation quality.", "labels": [], "entities": [{"text": "German-English SMT", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.5698789060115814}, {"text": "NMT (JunczysDowmunt et al., 2018)", "start_pos": 99, "end_pos": 132, "type": "DATASET", "confidence": 0.8663766235113144}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9927971959114075}]}, {"text": "We will refer to these setups as SMT 10M, SMT 100M, NMT 10M and NMT 100M.", "labels": [], "entities": [{"text": "SMT 10M", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.7122671008110046}]}, {"text": "As development set newstest 2017 was used, while newstest  2018, iwslt2017, Acquis, EMEA, Global Voices and KDE were the undisclosed test sets (Koehn et al., 2018).", "labels": [], "entities": [{"text": "Acquis", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.8635553121566772}, {"text": "EMEA", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.8923941850662231}, {"text": "KDE", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.7743169665336609}]}], "tableCaptions": [{"text": " Table 1: BLEU scores of our setups on the different datasets. We underline best results for each setup and dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994327425956726}]}, {"text": " Table 2: Best systems of participants on the four setups av-", "labels": [], "entities": []}]}