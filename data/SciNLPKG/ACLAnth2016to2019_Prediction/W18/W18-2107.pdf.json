{"title": [{"text": "Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically-motivated Test Suite", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8205563724040985}]}], "abstractContent": [{"text": "We present an alternative method of evaluating Quality Estimation systems, which is based on a linguistically-motivated Test Suite.", "labels": [], "entities": [{"text": "Quality Estimation", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.6280983537435532}]}, {"text": "We create a test-set consisting of 14 linguistic error categories and we gather for each of them a set of samples with both correct and erroneous translations.", "labels": [], "entities": []}, {"text": "Then, we measure the performance of 5 Quality Estimation systems by checking their ability to distinguish between the correct and the erroneous translations.", "labels": [], "entities": []}, {"text": "The detailed results are much more informative about the ability of each system.", "labels": [], "entities": []}, {"text": "The fact that different Quality Estimation systems perform differently at various phenomena confirms the usefulness of the Test Suite.", "labels": [], "entities": []}], "introductionContent": [{"text": "The evaluation of empirical Natural Language Processing (NLP) systems is a necessary task during research for new methods and ideas.", "labels": [], "entities": []}, {"text": "The evaluation task is the last one to come after the development process and aims to indicate the overall performance of the newly built system and compare it against previous versions or other systems.", "labels": [], "entities": []}, {"text": "Additionally, it also allows for conclusions related to the decisions taken for the development parameters and provides hints for improvement.", "labels": [], "entities": []}, {"text": "Defining evaluation methods that satisfy the original development requirements is an ongoing field of research.", "labels": [], "entities": []}, {"text": "Automatic evaluation in sub-fields of Machine Translation (MT) has been mostly performed on given textual hypothesis sets, where the performance of the system is measured against goldstandard reference sets with one or more metrics.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8583127617835998}]}, {"text": "Despite the extensive research on various automatic metrics and scoring methods, little attention has been paid to the actual content of the test-sets and how these can be adequate for judging the output from a linguistic perspective.", "labels": [], "entities": []}, {"text": "The text of most test-sets so far has been drawn from various random sources and the only characteristic that is controlled and reported is the generic domain of the text.", "labels": [], "entities": []}, {"text": "In this paper we make an effort to demonstrate the value of using a linguistically-motivated controlled test-set (also known as a Test Suite) for evaluation instead of generic test-sets.", "labels": [], "entities": []}, {"text": "We will focus on the sub-field of sentence-level Quality Estimation (QE) on MT and see how the evaluation of QE on a Test Suite can provide useful information concerning particular linguistic phenomena.", "labels": [], "entities": [{"text": "sentence-level Quality Estimation (QE)", "start_pos": 34, "end_pos": 72, "type": "TASK", "confidence": 0.7047705401976904}, {"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9127219915390015}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: QE accuracy (%) per error category", "labels": [], "entities": [{"text": "QE", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9948581457138062}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.904220461845398}]}, {"text": " Table 3: QE accuracy (%) on error types related to verb tenses", "labels": [], "entities": [{"text": "QE", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9902454614639282}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.8463120460510254}]}, {"text": " Table 4: QE accuracy (%) on error types related to verb types", "labels": [], "entities": [{"text": "QE", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9863101243972778}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.8859290480613708}]}]}