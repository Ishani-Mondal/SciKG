{"title": [{"text": "Evaluating Syntactic Properties of Seq2seq Output with a Broad Coverage HPSG: A Case Study on Machine Translation", "labels": [], "entities": [{"text": "HPSG", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.535586416721344}, {"text": "Machine Translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7952224612236023}]}], "abstractContent": [{"text": "Sequence to sequence (seq2seq) models are often employed in settings where the target output is natural language.", "labels": [], "entities": []}, {"text": "However, the syntactic properties of the language generated from these models are not well understood.", "labels": [], "entities": []}, {"text": "We explore whether such output belongs to a formal and realistic grammar, by employing the English Resource Grammar (ERG), abroad coverage, linguistically precise HPSG-based grammar of English.", "labels": [], "entities": []}, {"text": "From a French to English parallel corpus, we analyze the parseability and grammatical constructions occurring in output from a seq2seq translation model.", "labels": [], "entities": []}, {"text": "Over 93% of the model translations are parseable, suggesting that it learns to generate conforming to a grammar.", "labels": [], "entities": []}, {"text": "The model has trouble learning the distribution of rarer syntactic rules, and we pinpoint several constructions that differentiate translations between the references and our model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence to sequence models (seq2seq;) have found use cases in tasks such as machine translation (, dialogue agents (, and summarization (, where the target output is natural language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8314439654350281}, {"text": "summarization", "start_pos": 123, "end_pos": 136, "type": "TASK", "confidence": 0.9882646799087524}]}, {"text": "However, the decoder side in these models is usually parameterized by gated variants of recurrent neural networks, and are general models of sequential data not explicitly designed to generate conforming to the grammar of natural language.", "labels": [], "entities": []}, {"text": "The syntactic properties of seq2seq output is our central interest.", "labels": [], "entities": []}, {"text": "We focus on machine translation as a case study, and situate our work among those of artificial language learning, where we train our translation model exclusively on sentence pairs where the target-side output is in our grammar, and test our models by evaluating the output with respect to a grammar.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7652464509010315}]}, {"text": "We attempt to understand seq2seq output with the English Resource Grammar), abroad coverage, linguistically precise HPSG-based grammar of English, and explore the advantages and potential of using such an approach.", "labels": [], "entities": [{"text": "English Resource Grammar", "start_pos": 49, "end_pos": 73, "type": "DATASET", "confidence": 0.8602427045504252}]}, {"text": "This approach has three appealing properties in evaluating seq2seq output.", "labels": [], "entities": []}, {"text": "First, the language of the ERG is a departure from studies on unrealistic artificial languages with regular or context-free grammars, which give exact analyses on grammars that bear little relation to human language).", "labels": [], "entities": []}, {"text": "In fact, about 85% of the sentences found in Wikipedia are parseable by the ERG.", "labels": [], "entities": [{"text": "ERG", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.9028697609901428}]}, {"text": "Second, our methodology directly evaluates sequences the model outputs in practice with greedy or beam search, in contrast to methods rescoring pre-generated contrastive pairs to test implicit model knowledge (.", "labels": [], "entities": []}, {"text": "Third, the linguistically precise nature of the ERG gives us detailed analyses of the linguistic constructions exhibited by reference translations and parseable seq2seq translations for comparison.", "labels": [], "entities": []}, {"text": "shows an example from our analysis.", "labels": [], "entities": []}, {"text": "Each testing example records the reference derivation, the model translation, and the derivation of that translation, if applicable.", "labels": [], "entities": []}, {"text": "The derivations richly annotate the rule types and the linguistic constructions present in the translations.", "labels": [], "entities": []}, {"text": "Our analysis in \u00a74.1 presents results on parseability by the ERG and summarizes its relation to surface level statistics using Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 127, "end_pos": 146, "type": "METRIC", "confidence": 0.795435756444931}]}, {"text": "In \u00a74.2 we manually annotate a small sample of NMT output without ERG derivations for grammaticality.", "labels": [], "entities": []}, {"text": "We find that 60% of exhaustively unparseable NMT translations are ungrammatical by humans.", "labels": [], "entities": []}, {"text": "We also identify that 18.3% of the ungrammatical sentences could be corrected by fixing agreement attachment errors.", "labels": [], "entities": []}, {"text": "We conduct a discriminatory analysis in \u00a74.4 on reference and NMT rule usage to guide a qualitative analysis on our NMT output.", "labels": [], "entities": []}, {"text": "In analyzing specific samples, we find a general trend that our NMT model prefers to translate literally.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section details our setup of a French to English (FR \u2192 EN) neural machine translation system which we now refer to as NMT.", "labels": [], "entities": [{"text": "French to English (FR \u2192 EN) neural machine translation", "start_pos": 36, "end_pos": 90, "type": "TASK", "confidence": 0.5489622300321405}]}, {"text": "Our goal was to test a baseline system for comparable results to machine translation and seq2seq models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.6812082380056381}]}, {"text": "From 2M French to English sentence pairs in the Europarl v7 parallel corpora (), we subset 1.6M where the English/reference sentence was parseable by the ERG.", "labels": [], "entities": [{"text": "Europarl v7 parallel corpora", "start_pos": 48, "end_pos": 76, "type": "DATASET", "confidence": 0.9020519256591797}, {"text": "ERG", "start_pos": 154, "end_pos": 157, "type": "DATASET", "confidence": 0.9493359923362732}]}, {"text": "For these 1.6M sentence pairs, we record the best tree of the English sentence as determined by the maximum entropy model included in the ERG.", "labels": [], "entities": [{"text": "ERG", "start_pos": 138, "end_pos": 141, "type": "DATASET", "confidence": 0.8943036794662476}]}, {"text": "All sentence pairs we now consider have at least one English translation within our grammar, and we make no constraint on French.", "labels": [], "entities": []}, {"text": "About 1.4M pairs were used for training, 5K for validation, and the remaining 200K reserved for analysis.", "labels": [], "entities": []}, {"text": "On the source-side: The distribution of root node conditions for the reference and NMT translations on the 200K analysis sentence pairs.", "labels": [], "entities": []}, {"text": "Root node conditions are taken from the recorded best derivation.", "labels": [], "entities": []}, {"text": "The best derivation is chosen by the maximum entropy model included in the ERG.", "labels": [], "entities": [{"text": "ERG", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.9220942854881287}]}, {"text": "French sentences, simple rare word handling was applied, where all tokens with a frequency rank over 40K were replaced with an \"UNK\" token.", "labels": [], "entities": [{"text": "word handling", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.7740056812763214}]}, {"text": "However, when handling rare words in the targetside English sentences, \"UNK\" will significantly degrade ERG parsing performance on model output.", "labels": [], "entities": [{"text": "ERG parsing", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.8996914029121399}]}, {"text": "We replace our output tokens based on the lexical entries recognized by the ERG in our best parses (as in's NMT output).", "labels": [], "entities": [{"text": "NMT output", "start_pos": 108, "end_pos": 118, "type": "DATASET", "confidence": 0.8379662930965424}]}, {"text": "This form of rare word handling is similar to the 10K PTB dataset), but with more detailed part-of-speech and regular expression conditioned \"UNK\" tokens.", "labels": [], "entities": [{"text": "rare word handling", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.6046914855639139}, {"text": "10K PTB dataset", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.8392266631126404}]}, {"text": "After preprocessing, we had a source vocabulary size of 40000, and a target vocabulary size of 36292.", "labels": [], "entities": []}, {"text": "Our translation model is a word-level neural machine translation system with an attention mechanism (?).", "labels": [], "entities": [{"text": "word-level neural machine translation", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.603169396519661}]}, {"text": "We used an encoder and decoder with 512 dimensions and 2 layers each, and word embeddings of size 1024.", "labels": [], "entities": []}, {"text": "Dropout rates of 0.3 on the source, target, and hidden layers were applied.", "labels": [], "entities": []}, {"text": "A dropout of 0.4 was applied to the word embedding, which was tied for both input and output.", "labels": [], "entities": []}, {"text": "The model was trained for about 20 hours with early stopping on validation perplexity with patience 10 on a single Nvidia GPU Titan X (Maxwell).", "labels": [], "entities": [{"text": "patience", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9744485020637512}, {"text": "Nvidia GPU Titan X", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.8891199976205826}]}, {"text": "We used the NEMATUS () implementation, a highly ranked system in WMT16.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.9217867851257324}]}, {"text": "After training convergence on the 1M sentence pairs, the saved model is used for translation on the 200K sentences pairs left for analysis.", "labels": [], "entities": [{"text": "translation", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.9737827777862549}]}, {"text": "A beam size of 5 is used to search for the best translation under our NMT model.", "labels": [], "entities": []}, {"text": "We parse these translations with the ERG and record the best tree under the maximum entropy model.", "labels": [], "entities": []}, {"text": "We have parallel data of the French sentence, the human/reference English translation, the NMT English translation, the parse of the reference trans- Mean LP log Pm(So) |So|", "labels": [], "entities": [{"text": "NMT English translation", "start_pos": 91, "end_pos": 114, "type": "DATASET", "confidence": 0.9125282367070516}, {"text": "trans- Mean LP log Pm", "start_pos": 143, "end_pos": 164, "type": "METRIC", "confidence": 0.7185717523097992}]}], "tableCaptions": [{"text": " Table 1: The distribution of root node conditions for  the reference and NMT translations on the 200K anal- ysis sentence pairs. Root node conditions are taken  from the recorded best derivation. The best derivation  is chosen by the maximum entropy model included in  the ERG.", "labels": [], "entities": [{"text": "ERG", "start_pos": 274, "end_pos": 277, "type": "DATASET", "confidence": 0.9142704606056213}]}]}