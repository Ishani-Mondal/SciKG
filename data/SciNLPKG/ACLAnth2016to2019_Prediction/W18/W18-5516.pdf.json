{"title": [{"text": "UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification", "labels": [], "entities": [{"text": "UKP-Athene", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9657741189002991}, {"text": "Multi-Sentence Textual Entailment", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.5787163178126017}, {"text": "Claim Verification", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.6648138165473938}]}], "abstractContent": [{"text": "The Fact Extraction and VERification (FEVER) shared task was launched to support the development of systems able to verify claims by extracting supporting or refuting facts from raw text.", "labels": [], "entities": [{"text": "Fact Extraction and VERification (FEVER) shared task", "start_pos": 4, "end_pos": 56, "type": "TASK", "confidence": 0.6332123378912607}]}, {"text": "The shared task organizers provide a large-scale dataset for the consecutive steps involved in claim verification, in particular, document retrieval, fact extraction, and claim classification.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.7630580067634583}, {"text": "fact extraction", "start_pos": 150, "end_pos": 165, "type": "TASK", "confidence": 0.7656022012233734}, {"text": "claim classification", "start_pos": 171, "end_pos": 191, "type": "TASK", "confidence": 0.7485540807247162}]}, {"text": "In this paper, we present our claim verification pipeline approach, which, according to the preliminary results, scored third in the shared task, out of 23 competing systems.", "labels": [], "entities": [{"text": "claim verification pipeline", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.7907843987147013}]}, {"text": "For the document retrieval, we implemented anew entity linking approach.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.6833899319171906}, {"text": "entity linking", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7438381612300873}]}, {"text": "In order to be able to rank candidate facts and classify a claim on the basis of several selected facts, we introduce two extensions to the Enhanced LSTM (ESIM).", "labels": [], "entities": []}], "introductionContent": [{"text": "In the past years, the amount of false or misleading content on the Internet has significantly increased.", "labels": [], "entities": []}, {"text": "As a result, information evaluation in terms of fact-checking has become increasingly important as it allows to verify controversial claims stated on the web.", "labels": [], "entities": [{"text": "information evaluation", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.8071436882019043}]}, {"text": "However, due to the large number of fake news and hyperpartisan articles published online everyday, manual fact-checking is no longer feasible.", "labels": [], "entities": []}, {"text": "Thus, researchers as well as corporations are exploring different techniques to automate the fact-checking process . In order to advance research in this direction, the Fact Extraction and VERification (FEVER) shared task 2 was launched.", "labels": [], "entities": [{"text": "Fact Extraction and VERification (FEVER) shared task", "start_pos": 169, "end_pos": 221, "type": "TASK", "confidence": 0.6548507346047295}]}, {"text": "The organizers of the FEVER shared task constructed a large-scale dataset) based on Wikipedia.", "labels": [], "entities": [{"text": "FEVER shared task", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.46291182438532513}]}, {"text": "This dataset contains 185,445 claims, each of which comes with several evidence sets.", "labels": [], "entities": []}, {"text": "An evidence set consists of facts, i.e. sentences from Wikipedia articles that jointly support or contradict the claim.", "labels": [], "entities": []}, {"text": "On the basis of (any one of) its evidence sets, each claim is labeled as Supported, Refuted, or NotEnoughInfo if no decision about the veracity of the claim can be made.", "labels": [], "entities": [{"text": "Refuted", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9341648817062378}]}, {"text": "Supported by the structure of the dataset, the FEVER shared task encompasses three sub-tasks that need to be solved.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9765366315841675}]}, {"text": "Document retrieval: Given a claim, find (English) Wikipedia articles containing information about this claim.", "labels": [], "entities": [{"text": "Document retrieval", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.889675110578537}]}, {"text": "Sentence selection: From the retrieved articles, extract facts in the form of sentences that are relevant for the verification of the claim.", "labels": [], "entities": [{"text": "Sentence selection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8897315561771393}]}, {"text": "Recognizing textual entailment: On the basis of the collected sentences (facts), predict one of three labels for the claim: Supported, Refuted, or NotEnoughInfo.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the competing systems, an evaluation metric was devised by the FEVER organizers: a claim is considered as correctly verified if, in addition to predicting the correct label, a correct evidence set was retrieved.", "labels": [], "entities": [{"text": "FEVER organizers", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.8514826893806458}]}, {"text": "In this paper, we describe the pipeline system that we developed to address the FEVER task.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.8482840657234192}]}, {"text": "For document retrieval, we implemented an entity linking approach based on constituency parsing and handcrafted rules.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.767685055732727}, {"text": "constituency parsing", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.7509830296039581}]}, {"text": "For sentence selection, we developed a sentence ranking model based on the Enhanced Sequential Inference Model (ESIM).", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8550713956356049}]}, {"text": "We furthermore extended the ESIM for recognizing textual entailment between multiple input sentences and the claim using an attention mechanism.", "labels": [], "entities": [{"text": "ESIM", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.8609293699264526}, {"text": "recognizing textual entailment between multiple input sentences", "start_pos": 37, "end_pos": 100, "type": "TASK", "confidence": 0.8424566984176636}]}, {"text": "According to the preliminary results of the FEVER shared task, our systems came third out of 23 competing teams.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.8495127558708191}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of the retrieval systems using dif- ferent numbers of MediaWiki search results", "labels": [], "entities": []}, {"text": " Table 2: Performance of the textual entailment model  using different numbers of sentences", "labels": [], "entities": []}]}