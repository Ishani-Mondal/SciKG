{"title": [{"text": "Language Identification in Code-Mixed Data using Multichannel Neural Networks and Context Capture", "labels": [], "entities": [{"text": "Language Identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6919559836387634}, {"text": "Context Capture", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7812861800193787}]}], "abstractContent": [{"text": "An accurate language identification tool is an absolute necessity for building complex NLP systems to be used on code-mixed data.", "labels": [], "entities": [{"text": "language identification", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.7284812778234482}]}, {"text": "Lot of work has been recently done on the same, but there's still room for improvement.", "labels": [], "entities": []}, {"text": "Inspired from the recent advancements in neural network architectures for computer vision tasks, we have implemented multichannel neural networks combining CNN and LSTM for word level language identification of code-mixed data.", "labels": [], "entities": [{"text": "word level language identification of code-mixed data", "start_pos": 173, "end_pos": 226, "type": "TASK", "confidence": 0.7393400498798915}]}, {"text": "Combining this with a Bi-LSTM-CRF context capture module, accuracies of 93.28% and 93.32% is achieved on our two testing sets.", "labels": [], "entities": [{"text": "Bi-LSTM-CRF context capture", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.5399209211270014}, {"text": "accuracies", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9977236390113831}]}], "introductionContent": [{"text": "With the rise of social media, the amount of mineable data is rising rapidly.", "labels": [], "entities": []}, {"text": "Countries where bilingualism is popular, we see users often switchback and forth between two languages while typing, a phenomenon known as code-mixing or codeswitching.", "labels": [], "entities": []}, {"text": "For analyzing such data, language tagging acts as a preliminary step and its accuracy and performance can impact the system results to a great extent.", "labels": [], "entities": [{"text": "language tagging", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7459016740322113}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9987181425094604}]}, {"text": "Though a lot of work has been done recently targeting this task, the problem of language tagging in code-mixed scenario is still far from being solved.", "labels": [], "entities": [{"text": "language tagging", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7476405799388885}]}, {"text": "Code-mixing scenarios where one of the languages have been typed in its transliterated from possesses even more challenges, especially due to inconsistent phonetic typing.", "labels": [], "entities": []}, {"text": "On such type of data, context capture is extremely hard as well.", "labels": [], "entities": [{"text": "context capture", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.8743818700313568}]}, {"text": "Proper context capture can help in solving problems like ambiguity, that is word forms which are common to both the languages, but for which, the correct tag can be easily understood by knowing the context.", "labels": [], "entities": [{"text": "context capture", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.7386715412139893}]}, {"text": "An additional issue is alack of available code-mixed data.", "labels": [], "entities": []}, {"text": "Since most of the tasks require supervised models, the bottleneck of data crisis affects the performance quite a lot, mostly due to the problem of over-fitting.", "labels": [], "entities": []}, {"text": "In this article, we present a novel architecture, which captures information at both word level and context level to output the final tag.", "labels": [], "entities": []}, {"text": "For word level, we have used a multichannel neural network (MNN) inspired from the recent works of computer vision.", "labels": [], "entities": [{"text": "word level", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.8115991950035095}]}, {"text": "Such networks have also shown promising results in NLP tasks like sentence classification).", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7857309877872467}]}, {"text": "For context capture, we used Bi-LSTM-CRF.", "labels": [], "entities": [{"text": "context capture", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8850670456886292}, {"text": "Bi-LSTM-CRF", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.7807066440582275}]}, {"text": "The context module was tested more rigorously as in quite a few of the previous work, this information has been sidelined or ignored.", "labels": [], "entities": []}, {"text": "We have experimented on BengaliEnglish (Bn-En) and Hindi-English (Hi-En) codemixed data.", "labels": [], "entities": []}, {"text": "Hindi and Bengali are the two most popular languages in India.", "labels": [], "entities": []}, {"text": "Since none of them have Roman as their native script, both are written in their phonetically transliterated from when code-mixed with English.", "labels": [], "entities": []}], "datasetContent": [{"text": "For comparison purposes, we decided to use the character encoding architecture described in we can see that the jump inaccuracy from baseline to the word model is quite significant (4.55%).", "labels": [], "entities": [{"text": "character encoding", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7215099930763245}]}, {"text": "From word to context model, though not much, but still an improvement is seen (0.41%).", "labels": [], "entities": []}, {"text": "In, again a similar pattern can be seen, i.e. a significant improvement (4.37%) from baseline to word model.", "labels": [], "entities": []}, {"text": "Using the context model, accuracy increases by 0.67%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995793700218201}]}, {"text": "In both the Tables, we see that precision has been much higher than recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9996820688247681}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9987548589706421}]}], "tableCaptions": [{"text": " Table 2: Evaluation on Bn.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation on Hi.", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.7905789017677307}, {"text": "Hi.", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.7541841566562653}]}, {"text": " Table 5: Confusion matrices for Hi.", "labels": [], "entities": [{"text": "Hi", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9539482593536377}]}]}