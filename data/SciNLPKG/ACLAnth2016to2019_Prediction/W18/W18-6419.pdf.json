{"title": [{"text": "NICT's Neural and Statistical Machine Translation Systems for the WMT18 News Translation Task", "labels": [], "entities": [{"text": "NICT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9363231658935547}, {"text": "Neural and Statistical Machine Translation", "start_pos": 7, "end_pos": 49, "type": "TASK", "confidence": 0.5604647696018219}, {"text": "WMT18 News Translation", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.8227297067642212}]}], "abstractContent": [{"text": "This paper presents the NICT's participation to the WMT18 shared news translation task.", "labels": [], "entities": [{"text": "NICT", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.8450707793235779}, {"text": "WMT18 shared news translation task", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.8148282766342163}]}, {"text": "We participated in the eight translation directions of four language pairs: Estonian-English, Finnish-English, Turkish-English and Chinese-English.", "labels": [], "entities": []}, {"text": "For each translation direction , we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems.", "labels": [], "entities": [{"text": "translation direction", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.9203298985958099}, {"text": "SMT) and neural (NMT) machine translation", "start_pos": 75, "end_pos": 116, "type": "TASK", "confidence": 0.6649635831514994}]}, {"text": "Our NMT systems were trained with the transformer architecture using the provided parallel data enlarged with a large quantity of back-translated monolingual data that we generated with anew incremental training framework.", "labels": [], "entities": []}, {"text": "Our primary submissions to the task are the result of a simple combination of our SMT and NMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9297743439674377}]}, {"text": "Our systems are ranked first for the Estonian-English and Finnish-English language pairs (constraint) according to BLEU-cased.", "labels": [], "entities": [{"text": "BLEU-cased", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9973734617233276}]}], "introductionContent": [{"text": "This paper describes the neural (NMT) and statistical machine translation systems (SMT) built for the participation of the National Institute of Information and Communications Technology (NICT) to the WMT18 shared News Translation Task (.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6211031476656595}, {"text": "WMT18 shared News Translation Task", "start_pos": 201, "end_pos": 235, "type": "TASK", "confidence": 0.8165668368339538}]}, {"text": "We participated in four language pairs (eight translation directions): Estonian-English (Et-En), Finnish-English (Fi-En), Turkish-English (Tr-En), and ChineseEnglish (Zh-En).", "labels": [], "entities": []}, {"text": "We chose these language pairs since they appear to be among the most challenging: involving distant languages and with less training data, for Finnish, Estonian, and Turkish, provided by the organizers than for Russian, German, and Czech.", "labels": [], "entities": []}, {"text": "All our systems are constrained, i.e., we used only the parallel and monolingual data provided by the organizers to train and tune them.", "labels": [], "entities": []}, {"text": "For all the translation directions, we trained NMT and SMT systems, and combined them through n-best list reranking using different informative features as proposed by.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9700522422790527}]}, {"text": "This simple combination method, associated to the exploitation of large back-translated monolingual data, performed among the best MT systems at WMT18.", "labels": [], "entities": [{"text": "MT", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.981717050075531}, {"text": "WMT18", "start_pos": 145, "end_pos": 150, "type": "DATASET", "confidence": 0.949785053730011}]}, {"text": "Especially for the competitive Et-En and Fi-En translation tasks, for which our submissions are ranked first according to the BLEU-cased metric (henceforth BLEU).", "labels": [], "entities": [{"text": "Et-En and Fi-En translation tasks", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.5402679443359375}, {"text": "BLEU-cased metric", "start_pos": 126, "end_pos": 143, "type": "METRIC", "confidence": 0.9732799828052521}, {"text": "BLEU", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9276474118232727}]}, {"text": "Our systems for Et-En, Fi-En, and Tr-En were trained using the exactly same procedures, without any specific linguistic treatments.", "labels": [], "entities": [{"text": "Tr-En", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.955162525177002}]}, {"text": "On the other hand, for Zh-En, we used a specific tokenizer and used slightly different training parameters due to the much larger quantity of training data.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce the data preprocessing.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the details of our NMT and SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9431290030479431}]}, {"text": "The back-translation of monolingual data using our new incremental training framework for NMT is described in Section 4.", "labels": [], "entities": [{"text": "NMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.7940576076507568}]}, {"text": "Then, the combination of NMT and SMT is described in Section 5.", "labels": [], "entities": [{"text": "NMT", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.6217092871665955}, {"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9627146124839783}]}, {"text": "Empirical results produced with our systems are showed and analyzed in Section 6, and Section 7 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 5: Set of features used by our reranking systems. The column \"Feature\" refers to the same feature name used  in Marie and Fujita", "labels": [], "entities": [{"text": "Fujita", "start_pos": 129, "end_pos": 135, "type": "DATASET", "confidence": 0.7066525816917419}]}, {"text": " Table 6: Detokenized BLEU-cased scores for our MT systems on the Newstest2018 test set. \"NMT-reranked\"  denotes the reranking of the Moses's 100-best hypotheses using all our NMT models (left-to-right and right-to- left, for both translation directions, trained with back-translated data) as features. \"backtr\" denotes the use or not  of back-translated monolingual data. \"Moses + Marian\" denotes our combination of best NMT (#5) and SMT  (#1) systems described in Section 5.", "labels": [], "entities": [{"text": "BLEU-cased", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9811080098152161}, {"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9713144898414612}, {"text": "Newstest2018 test set", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.9910714824994405}, {"text": "SMT", "start_pos": 435, "end_pos": 438, "type": "TASK", "confidence": 0.922468364238739}]}]}