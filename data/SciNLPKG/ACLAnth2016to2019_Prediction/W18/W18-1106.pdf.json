{"title": [{"text": "The Social and the Neural Network: How to Make Natural Language Processing about People again", "labels": [], "entities": [{"text": "Make Natural Language Processing about People", "start_pos": 42, "end_pos": 87, "type": "TASK", "confidence": 0.7264248728752136}]}], "abstractContent": [{"text": "Over the years, natural language processing has increasingly focused on tasks that can be solved by statistical models, but ignored the social aspects of language.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6612337529659271}]}, {"text": "These limitations are in large part due to historically available data and the limitations of the models, but have narrowed our focus and biased the tools demographically.", "labels": [], "entities": []}, {"text": "However, with the increased availability of data sets including socio-demographic information and more expressive (neural) models, we have the opportunity to address both issues.", "labels": [], "entities": []}, {"text": "I argue that this combination can broaden the focus of NLP to solve a whole new range of tasks, enable us to generate novel linguistic insights, and provide fairer tools for everyone .", "labels": [], "entities": []}], "introductionContent": [{"text": "Up until the 1970s, economic theory assumed that people make economic decisions with their own best interest in mind, and based on the full available information.", "labels": [], "entities": []}, {"text": "This was a useful assumption, which allowed researchers to model people, firms, and markets as statistical linear models of the form y = w T x, to test existing theories and to generate new insights.", "labels": [], "entities": []}, {"text": "The seminal work by Tversky and, however, showed that this assumption was wrong: they demonstrated experimentally that again and again, people would make economic decisions that were not in their best interest, even with the full available knowledge, but instead relied on biases and heuristics.", "labels": [], "entities": []}, {"text": "This did not mean that the linear models were useless they were useful abstractions.", "labels": [], "entities": []}, {"text": "It did show, however, that there was more to the subject, and that it was fundamentally about people.", "labels": [], "entities": []}, {"text": "Incorporating people's behavior opened up economics to new insights, and even established a completely new field, behavioral economics.", "labels": [], "entities": []}, {"text": "Up until the 1990s, NLP was largely based on applying heuristics based on linguistic theory.", "labels": [], "entities": []}, {"text": "However, in the 1990s, the field underwent a \"statistical revolution\": It turned out that statistical linear models of the form y = w T x were more robust, accurate, and reliable in extracting linguistic information from text than linguistic heuristics were.", "labels": [], "entities": []}, {"text": "This was a useful insight, which enabled us to solve a number of tasks.", "labels": [], "entities": []}, {"text": "However, as a consequence, the field focused more and more on tasks that could be solved with these models, and moved away from tasks that could not.", "labels": [], "entities": []}, {"text": "While this approach enabled a number of breakthroughs, it also increasingly narrowed the focus of the field, in what could be called \"streetlamp science\": much like the person searching for their keys under the light of the streetlamp (rather than where they lost them), NLP has continued to search for tasks that could be solved by the statistical models we have, rather than the ones that could help us understand the underpinnings of language.", "labels": [], "entities": []}, {"text": "This shift to the streetlamp and away from the social aspects of language has had two practical consequences: it ignored a whole host of applications that are more difficult to model, and it biased our tools.", "labels": [], "entities": []}, {"text": "Language is about much more than information: language is used by people to communicate with other people, to establish social order, to convince, entertain, and achieve a whole host of other communicative goals, but also to signal membership in asocial group.", "labels": [], "entities": []}, {"text": "The latter is most obvious in teenagers, who become linguistically creative to distinguish themselves from their parents.", "labels": [], "entities": []}, {"text": "For most other groups, the process is much less obvious and often subconscious, but all people use language to mark their membership in a variety of demographic groups: these groups range from gender to region, social class, ethnicity, and occupation.", "labels": [], "entities": []}, {"text": "This property of language has been used in NLP to predict those demographic labels from text in authorattribute prediction tasks,b, inter alia).", "labels": [], "entities": [{"text": "authorattribute prediction tasks,b", "start_pos": 96, "end_pos": 130, "type": "TASK", "confidence": 0.7644517521063486}]}, {"text": "However, demographics also affect NLP beyond their use as prediction target.", "labels": [], "entities": []}, {"text": "Demographic bias in the training data can severely distort the performance of our tools, while accounting for demographic factors can actually improve performance in a variety of tasks (.", "labels": [], "entities": []}, {"text": "In order to move forward as afield, we will have to follow two strands of research: 1) we need to identify the specific demographic factors that do have an influence on NLP models (on bias and performance), and 2) based on this knowledge, we need to develop models that account for demographics to improve performance while preventing bias.", "labels": [], "entities": []}, {"text": "In this position paper, I argue that the recent abundance of demographically rich data sets and complex neural architectures allows us to breakout of streetlamp science and to explore those two strands of demographically-based research.", "labels": [], "entities": []}, {"text": "This shift will enable a host of new applications that make socio-demographic aspects an integral part of language.", "labels": [], "entities": []}, {"text": "I highlight several neural network architectures and procedures that show promise to achieve these goals, and provide some experimental results in applying them.", "labels": [], "entities": []}], "datasetContent": [{"text": "I preprocess the data to remove stop words and function words, replace numbers with 0s, lowercase all words and lemmatize them.", "labels": [], "entities": []}, {"text": "I also concatenate collocations with an underscore to form a single item.", "labels": [], "entities": []}, {"text": "This reduces the amount of noise in the data.", "labels": [], "entities": []}, {"text": "As labels, I use the seven age decades, as well as the two genders present in the data.", "labels": [], "entities": []}, {"text": "Overall, this results in slightly over 2M instances.", "labels": [], "entities": []}, {"text": "I run the model for 100 iterations, following the settings described in (, with the embedding dimensions to 300, window size to 15, minimum frequency to 10, negative samples to 5, downsampling to 0.00001.", "labels": [], "entities": []}, {"text": "Comparing words to each other The effect of the modeling process is that semantically similar words get closer in embedding space.", "labels": [], "entities": []}, {"text": "The 10 nearest neighbors when querying for the word great are well, fantastic, amazing, really good, good, really, lot, perfect, especially, and love (see fora graphical depiction).", "labels": [], "entities": [{"text": "love", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9793597459793091}]}, {"text": "This is not new or surprising, but I will show further results building on this in subsequent sections.", "labels": [], "entities": []}, {"text": "Comparing words to labels We can use each demographic label vector and find the closest words around them.", "labels": [], "entities": []}, {"text": "This gives us descriptors of the labels.", "labels": [], "entities": []}, {"text": "Using the example word from the previous paragraph, great, but adding and subtracting demographic label representations in the calculation, we can compute to see which words women and men, respectively, use with or for great.", "labels": [], "entities": []}, {"text": "The first calculation give us fab, fabulous, lovely, love, wonderful, really pleased, fantastic, brilliant, amazing, and thrill for women and guy, decent, good, topnotch, couple, new, well, gear, get good, and awesome for men.", "labels": [], "entities": [{"text": "thrill", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9733240604400635}]}, {"text": "Such knowledge is interesting with respect to sociodemographic studies, but can have practical applications: have shown how gender can be obfuscated online by replacing particularly \"male\" or \"female\" words with a neutral or even opposite counterpart.", "labels": [], "entities": []}, {"text": "The approach shown here based on vector arithmetics is a possible simple alternative.", "labels": [], "entities": []}, {"text": "Comparing labels to labels Comparing labels to each other is again very similar to the situation we have seen above for words.", "labels": [], "entities": []}, {"text": "In the present study, this comparison is less interesting (though we can for example see which age groups are more or less similar to each other, see).", "labels": [], "entities": []}, {"text": "However, we will exploit this attribute in the next section (2.3), were we explicitly compare labels to each other.", "labels": [], "entities": []}, {"text": "Clustering Clustering the word representations with k-means gives us a number of centroids in the embedding space, which we excellent service order arrive day, first class service would recommend -PRON-friend, guitar, good service fast delivery excellent product, reliable service prompt SERVICE: excellent service prompt delivery good price, refuse, tell, apparently, akinika MISC: srv, hendrix, marvin, bankcard, irrational TRAVEL: hotel, airport, flight, -PRON-flight, -PRON-trip", "labels": [], "entities": [{"text": "SERVICE", "start_pos": 288, "end_pos": 295, "type": "METRIC", "confidence": 0.984775960445404}, {"text": "MISC", "start_pos": 377, "end_pos": 381, "type": "METRIC", "confidence": 0.9126296639442444}, {"text": "TRAVEL", "start_pos": 426, "end_pos": 432, "type": "METRIC", "confidence": 0.9613906145095825}]}], "tableCaptions": []}