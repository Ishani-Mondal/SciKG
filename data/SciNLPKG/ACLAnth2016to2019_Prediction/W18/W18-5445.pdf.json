{"title": [{"text": "Portable, layer-wise task performance monitoring for NLP models", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "There is a long-standing interest in understanding the internal behavior of neural networks.", "labels": [], "entities": []}, {"text": "Deep neural architectures for natural language processing (NLP) are often accompanied by explanations for their effectiveness, from general observations (e.g. RNNs can represent unbounded dependencies in a sequence) to specific arguments about linguistic phenomena (early layers encode lexical information, deeper layers syntactic).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.8283777435620626}]}, {"text": "The recent ascendancy of DNNs is fueling efforts in the NLP community to explore these claims (.", "labels": [], "entities": []}, {"text": "Previous work has tended to focus on easily-accessible representations like word or sentence embeddings, with deeper structure requiring more ad hoc methods to extract and examine.", "labels": [], "entities": []}, {"text": "In this work, we introduce Vivisect, a toolkit that aims at a general solution for broad and fine-grained monitoring in the major DNN frameworks, with minimal change to research patterns.", "labels": [], "entities": []}, {"text": "Vivisect is general enough to serve as a less-polished version of the widely-used TensorBoard tool, but has several priorities that set it apart: \u2022 Minimal invasiveness (e.g. no SummaryOps) \u2022 Low resource use (only keep final metrics) \u2022 Uniform support for major DNN frameworks \u2022 Monitor performance on auxiliary tasks The first three points are largely ergonomic, though we hope that feature parity between the major DNN research frameworks will yield answers to previously-daunting questions, such as why seemingly-identical implementations of a deep architecture perform differently.", "labels": [], "entities": []}, {"text": "The fourth point is the most important: when made aware of task labels from various linguistic modalities, Vivisect will train lightweight linear classifiers and clusterers using each of the model's internal representations as features.", "labels": [], "entities": []}, {"text": "A lightweight web server aggregates and plots these scores as a function of other variables (e.g. training epoch) to give insight into what linguistic information is captured by different parts of the model, and how they evolve overtime.", "labels": [], "entities": []}, {"text": "Vivisect has evolved out of a focus on neural machine translation models, but is designed with generalization as a fundamental principle.", "labels": [], "entities": [{"text": "neural machine translation models", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.778453528881073}]}, {"text": "Therefore, it includes mechanisms for deciding what and when calculations are made, and on which parts of a model.", "labels": [], "entities": []}, {"text": "There are simple APIs for registering additional metrics and entire DNN frameworks.", "labels": [], "entities": []}, {"text": "Vivisect is provided as a code repository and optional prebuilt Docker image.", "labels": [], "entities": [{"text": "Vivisect", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8421438336372375}]}], "datasetContent": [], "tableCaptions": []}