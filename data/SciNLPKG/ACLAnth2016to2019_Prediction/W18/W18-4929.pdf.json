{"title": [{"text": "Mumpitz at PARSEME Shared Task 2018: A Bidirectional LSTM for the Identification of Verbal Multiword Expressions", "labels": [], "entities": [{"text": "Mumpitz at PARSEME Shared Task 2018", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.768824060757955}, {"text": "Identification of Verbal Multiword Expressions", "start_pos": 66, "end_pos": 112, "type": "TASK", "confidence": 0.8692893266677857}]}], "abstractContent": [{"text": "In this paper, we describe Mumpitz, the system we submitted to the PARSEME Shared Task on automatic identification of verbal multiword expressions (VMWEs).", "labels": [], "entities": [{"text": "Mumpitz", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.6913503408432007}, {"text": "PARSEME Shared Task on automatic identification of verbal multiword expressions (VMWEs)", "start_pos": 67, "end_pos": 154, "type": "TASK", "confidence": 0.6854871603158804}]}, {"text": "Mumpitz consists of a Bidirectional Recurrent Neural Network (BRNN) with Long Short-Term Memory (LSTM) units and a heuristic that leverages the dependency information provided in the PARSEME corpus data to differentiate VMWEs in a sentence.", "labels": [], "entities": [{"text": "Mumpitz", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9674358367919922}, {"text": "Long Short-Term Memory (LSTM)", "start_pos": 73, "end_pos": 102, "type": "METRIC", "confidence": 0.8243840386470159}, {"text": "PARSEME corpus data", "start_pos": 183, "end_pos": 202, "type": "DATASET", "confidence": 0.9027803738911947}]}, {"text": "We submitted results for seven languages in the closed track of the task and for one language in the open track.", "labels": [], "entities": []}, {"text": "For the open track we used the same system, but with pretrained instead of randomly initialized word embeddings to improve the system performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiword Expressions (MWEs) are linguistic expressions that consist of multiple lexemes and exhibit some form of idiomaticity.", "labels": [], "entities": [{"text": "Multiword Expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8169910371303558}]}, {"text": "The term idiomaticity denotes the fact that the properties of the whole cannot (fully) be predicted by the properties of the components.", "labels": [], "entities": []}, {"text": "Idiomaticity can surface on the lexical, syntactic, semantic, pragmatic and/or statistical level ( and it is a reason why MWEs manifest a well-known challenge for natural language processing (NLP) tasks as parsing and machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 218, "end_pos": 237, "type": "TASK", "confidence": 0.7383188307285309}]}, {"text": "The PARSEME shared task (ST) focuses only on a subset of MWEs, namely verbal multiword expressions (VMWEs), since their attributes like discontinuity (i.e. non-adjacency of the VMWEs' components) and semantic or syntactic ambiguity, make them especially challenging for automatic processing.", "labels": [], "entities": [{"text": "PARSEME shared task (ST)", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7637908061345419}]}, {"text": "The goal of the ST is the identification of VMWEs in running text.", "labels": [], "entities": []}, {"text": "Besides extraction, which is the process of finding new MWE types, identification of MWE tokens in running text is the most important task with regard to MWE processing.", "labels": [], "entities": [{"text": "identification of MWE tokens in running text", "start_pos": 67, "end_pos": 111, "type": "TASK", "confidence": 0.7924029741968427}, {"text": "MWE processing", "start_pos": 154, "end_pos": 168, "type": "TASK", "confidence": 0.9607436954975128}]}, {"text": "In the context of the ST VMWEs are categorized into types like verb-particle constructions (VPC), light verb constructions (LVC), idioms (VID), inherently reflexive verbs (IRV) or multi-verb constructions (MVC).", "labels": [], "entities": [{"text": "ST VMWEs", "start_pos": 22, "end_pos": 30, "type": "TASK", "confidence": 0.5819514840841293}]}, {"text": "The ST is multilingual, i.e. the PARSEME organizers provide annotated corpora in 20 different languages which are split in train, dev and test sets.", "labels": [], "entities": [{"text": "PARSEME organizers", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.8684690892696381}]}, {"text": "Competing teams can choose to only incorporate the features provided in these data sets or use external features such as MWE lexicons, word embeddings, etc.", "labels": [], "entities": []}, {"text": "Systems solely based on preexisting features compete in the closed track, systems that also use external data in the open track of the ST.", "labels": [], "entities": []}, {"text": "Our system, Mumpitz, is a bidirectional recurrent neural network (BRNN) with long short-term memory (LSTM) units coupled with a heuristic that distinguishes different VMWEs in a sentence.", "labels": [], "entities": [{"text": "Mumpitz", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.9875412583351135}]}, {"text": "We submitted results for 7 of the 20 languages to the closed track and for one language to the open track.", "labels": [], "entities": []}, {"text": "In the next section we will provide an overview over last year's entries to the ST, before we will describe Mumpitz in detail.", "labels": [], "entities": [{"text": "last year's entries to the ST", "start_pos": 53, "end_pos": 82, "type": "DATASET", "confidence": 0.6664446336882455}, {"text": "Mumpitz", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9715922474861145}]}, {"text": "Subsequently, we will present the results and discuss the performance of the system as well as its possibilities for improvement.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used Keras with TensorFlow as back-end to build our network.", "labels": [], "entities": []}, {"text": "Keras is a deep learning API written in Python that makes it very convenient to write complex neural network architectures in just a few lines of code.", "labels": [], "entities": []}, {"text": "The back-end, TensorFlow, is an open source software library developed by Google Brain () that is optimized for large-scale machine learning applications.", "labels": [], "entities": []}, {"text": "This is achieved by defining the models as computational graphs which then can be split to distribute them across different servers.", "labels": [], "entities": []}, {"text": "The model we employed for the identification task is a BRNN with 100 LSTM units.", "labels": [], "entities": [{"text": "identification task", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.9158002734184265}, {"text": "BRNN", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.606869101524353}]}, {"text": "The features that were fed as input to the BRNN were mapped to embeddings and concatenated.", "labels": [], "entities": [{"text": "BRNN", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.954724133014679}]}, {"text": "Of all the features that were provided in the train and test files, combinations of the following were tested as input for the neural network: word form (WF), lemma (L), universal POS (UP), language specific POS (LP), head of the current word (H) and the dependency relation to the head (D).", "labels": [], "entities": []}, {"text": "The combination of features that achieved the highest F1 score on the German dev set consisted of L, LP and D and it was subsequently used for the predictions on the test sets for all the languages.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9896430969238281}, {"text": "German dev set", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.9675851662953695}]}, {"text": "Of course, it would have made more sense to choose the best performing set of features for every language or language family (as did), but we chose this course of action because we did not have the time to do so and we were mostly interested in the German data set.", "labels": [], "entities": [{"text": "German data set", "start_pos": 249, "end_pos": 264, "type": "DATASET", "confidence": 0.9336051742235819}]}, {"text": "One of the reasons for this focus is the fact that two of the authors did some annotation work on the German corpus of this year's edition of the ST and we were eager to see how well a deep learning algorithm would do at this challenging data set.", "labels": [], "entities": [{"text": "German corpus of this year's edition of the ST", "start_pos": 102, "end_pos": 148, "type": "DATASET", "confidence": 0.945218312740326}]}, {"text": "To form the input for the network the lemmas were mapped to randomly initialized embeddings of dimensionality 50, while the embeddings for LP and D both had 20 dimensions.", "labels": [], "entities": []}, {"text": "Thus, the concatenated feature vectors were of size 90 and every sentence was represented by a certain number of those feature vectors.", "labels": [], "entities": []}, {"text": "Because Keras requires all input sequences to have the same length, this number had to be fixed.", "labels": [], "entities": []}, {"text": "This was achieved by padding all the feature sequences to the length of the longest sentence (which, for example, was 100 words long in the German corpus) with an arbitrary value of L, LP or D.", "labels": [], "entities": [{"text": "German corpus", "start_pos": 140, "end_pos": 153, "type": "DATASET", "confidence": 0.8199609518051147}, {"text": "LP", "start_pos": 185, "end_pos": 187, "type": "METRIC", "confidence": 0.9483621120452881}]}, {"text": "Hence, every sentence was represented by a matrix of shape maximum-sentence-length \u00d7 90 (e.g. 100 \u00d7 90 for German).", "labels": [], "entities": []}, {"text": "To avoid overfitting, we applied a Dropout of 0.1 to the LSTM layer.", "labels": [], "entities": [{"text": "Dropout", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.92476487159729}]}, {"text": "Dropout is a regularization method that randomly deletes a specific number of nodes (in our case 10% of the nodes) during the training process.", "labels": [], "entities": []}, {"text": "This basically has the effect that the network is inhibited from \"memorizing\" the training data too well which in turn prevents overfitting.", "labels": [], "entities": []}, {"text": "We experimented with different dropout rates and found that the performance of the model decreased when using larger rates, so we maintained a rate of 0.1 for all the experiments.", "labels": [], "entities": []}, {"text": "The last part of the model is a softmax layer that returns the probability distribution over the tags.", "labels": [], "entities": []}, {"text": "The focus was exclusively on the identification of VMWEs.", "labels": [], "entities": [{"text": "identification of VMWEs", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8319199283917745}]}, {"text": "The classifier did only decide if a certain word was part of a VMWE or not, that is, the classifier did not consider any of the VMWE type labels.", "labels": [], "entities": []}, {"text": "One problem that arises by applying this strategy is that the different VMWEs in an sentence are not distinguished, because every word that belongs to a VMWE has the same label, even if they do not belong to the same expression.", "labels": [], "entities": []}, {"text": "Differentiating the words tagged as VMWEs in an sentence was the purpose of the heuristic mentioned above which worked in different steps: first, it identified all the words in a sentence that were labeled as VMWEs and had the universal POS-tag \"VERB\" and enumerated them.", "labels": [], "entities": [{"text": "POS-tag \"VERB", "start_pos": 237, "end_pos": 250, "type": "METRIC", "confidence": 0.732911745707194}]}, {"text": "In the next step, every word that was a dependent of an enumerated verb received the same number as this verb.", "labels": [], "entities": []}, {"text": "Finally, every verb not labeled as a VMWE by the classifier that had a dependent labeled as such, also got the VMWE label.", "labels": [], "entities": []}, {"text": "To summarize, here is how the classification proceeded: Every sentence was fed into the BRNN one word at a time, i.e. one feature vector at a time.", "labels": [], "entities": [{"text": "BRNN", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8917421698570251}]}, {"text": "At every time step the hidden states of the LSTM units 2 were passed onto the softmax layer to compute a prediction for the current word that included the information from all the time steps before and all the time steps after.", "labels": [], "entities": []}, {"text": "For example, if a sequence of 100 words (feature vectors) was fed to the network, the output was a 100 \u00d7 2 matrix, i.e. the probabilities for every word if it belonged to a VMWE or not.", "labels": [], "entities": []}, {"text": "The training was not conducted with a fixed set of epochs, but the loss of across validation set (10% of the train set) was monitored and the training was stopped when it did not decrease anymore for two consecutive epochs.", "labels": [], "entities": []}, {"text": "The convergence was relatively fast, usually it took between 4 and 7 epochs to train.", "labels": [], "entities": [{"text": "convergence", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9597994685173035}]}, {"text": "The loss was computed with binary cross-entropy while the optimization algorithm used was RMSProp.", "labels": [], "entities": [{"text": "RMSProp", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.784001886844635}]}, {"text": "After the network made its predictions the heuristic differentiated the annotated VMWEs as described above.", "labels": [], "entities": []}, {"text": "The model for the open track is essentially the same as for the closed track.", "labels": [], "entities": []}, {"text": "The only difference is that we didn't use randomly initialized embeddings, but pretrained word embeddings.", "labels": [], "entities": []}, {"text": "We used the word2vec implementation (more precisely the skip-gram model) of the python package gensim to create the word embeddings.", "labels": [], "entities": []}, {"text": "The word vectors were trained on the DECOW16 corpus) which is a German web corpus that comes in two variants: one with 20 billion tokens and full documents and one with 11 billion tokens and shuffled sentences.", "labels": [], "entities": [{"text": "DECOW16 corpus", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.9550478160381317}]}, {"text": "The latter was used for the training.", "labels": [], "entities": []}, {"text": "The word embeddings have dimensionality 100 and represent lemmas, not the word surface forms.", "labels": [], "entities": []}, {"text": "Besides the embeddings we trained ourselves, we also tested pretrained embeddings that were trained on Wikipedia with fastText (, an extension of word2vec.", "labels": [], "entities": []}, {"text": "After evaluation on the dev set we opted for the selftrained embeddings, as they performed considerably better then those pretrained with fastText.", "labels": [], "entities": []}, {"text": "And since the training of embeddings on a very large corpus like DECOW16 4 is somewhat time-consuming, we only managed to submit results for the German dataset in time for the open track.", "labels": [], "entities": [{"text": "DECOW16 4", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.9012911915779114}, {"text": "German dataset", "start_pos": 145, "end_pos": 159, "type": "DATASET", "confidence": 0.9520218372344971}]}, {"text": "Ahead of the training process every lemma in the vocabulary was mapped to its respective word2vec embedding to form an embeddings matrix which was then used instead of the random instantiations.", "labels": [], "entities": []}, {"text": "For some words such an embedding did not exist (because it did not appear often enough in DECOW16), so in these cases the words were mapped to zero vectors.", "labels": [], "entities": [{"text": "DECOW16", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9251595735549927}]}, {"text": "Like the random instantiations the pretrained word embeddings were concatenated with the (still randomly initialized) embeddings for LP and D which resulted in feature vectors of 140 dimensions to represent the words in the sentences.", "labels": [], "entities": []}, {"text": "Everything else stayed exactly the same as for the setup of the closed track.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Number of tokens (DE)", "labels": [], "entities": [{"text": "DE", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.6930662393569946}]}]}