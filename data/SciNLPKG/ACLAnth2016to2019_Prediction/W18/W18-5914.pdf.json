{"title": [{"text": "Drug-use Identification from Tweets with Word and Character N-grams", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our systems in social media mining for health applications (SMM4H) shared task.", "labels": [], "entities": [{"text": "SMM4H) shared task", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.8656964004039764}]}, {"text": "We participated in all four tracks of the shared task using linear models with a combination of character and word n-gram features.", "labels": [], "entities": []}, {"text": "We did not use any external data or domain specific information.", "labels": [], "entities": []}, {"text": "The resulting systems achieved above-average scores among other participating systems, with F 1-scores of 91.22, 46.8, 42.4, and 85.53 on tasks 1, 2, 3, and 4 respectively.", "labels": [], "entities": [{"text": "F 1-scores", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.992061197757721}]}], "introductionContent": [{"text": "The increasing use of social media platforms worldwide offers an interesting application of natural language processing tools for monitoring public health and health-related events on the social media.", "labels": [], "entities": []}, {"text": "The social media mining for health applications (SMM4H) shared task hosts four tasks aiming to identify mentions of different aspects medication use on Twitter.", "labels": [], "entities": [{"text": "social media mining for health applications (SMM4H) shared task", "start_pos": 4, "end_pos": 67, "type": "TASK", "confidence": 0.6674705662510612}]}, {"text": "Briefly, the tasks and their descriptions are: Task 1: Automatic detection of posts mentioning drug names.", "labels": [], "entities": [{"text": "Automatic detection of posts mentioning drug names", "start_pos": 55, "end_pos": 105, "type": "TASK", "confidence": 0.8577339180878231}]}, {"text": "Task 2: Automatic classification of posts describing medication intake.", "labels": [], "entities": [{"text": "Automatic classification of posts describing medication intake", "start_pos": 8, "end_pos": 70, "type": "TASK", "confidence": 0.8057811089924404}]}, {"text": "Task 3: Automatic classification of adverse drug reaction mentioning posts.", "labels": [], "entities": [{"text": "Automatic classification of adverse drug reaction mentioning posts", "start_pos": 8, "end_pos": 74, "type": "TASK", "confidence": 0.8082545883953571}]}, {"text": "Task 4: Automatic detection of posts mentioning vaccination behavior.", "labels": [], "entities": [{"text": "Automatic detection of posts mentioning vaccination behavior", "start_pos": 8, "end_pos": 68, "type": "TASK", "confidence": 0.8906293937138149}]}, {"text": "All tasks, except Task 2 are binary classification tasks.", "labels": [], "entities": []}, {"text": "Task 2 requires three-way classification, including an uncertain class indicating posts mentioning possible medication intake.", "labels": [], "entities": []}, {"text": "For all tasks, we used linear SVM classifiers with character and word bag-of-n-gram features.", "labels": [], "entities": []}, {"text": "We also experimented with other methods, including deep learning methods with gated RNNs for building document representations.", "labels": [], "entities": []}, {"text": "However, SVM models achieved best results on the development data.", "labels": [], "entities": []}, {"text": "As a result, we only submitted results using linear SVMs, and we will only describe and discuss results of these model in this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the same general model for all tasks: linear SVM classifiers with character and word bagof-n-gram features.", "labels": [], "entities": []}, {"text": "Tokenization was done using a simple regular expression tokenizer that splits the text into consecutive alphanumeric and nonspace, non-alphanumeric tokens.", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9590950608253479}]}, {"text": "For each text to be classified, we extracted both character and word n-grams of order one up to a certain upper limit (specified below).", "labels": [], "entities": []}, {"text": "All features are combined in a flat manner as a single text-feature matrix.", "labels": [], "entities": []}, {"text": "We experimented with two feature weighting methods: tf-idf (Jurafsky and Martin, 2009, p.805) and BM25 ().", "labels": [], "entities": [{"text": "BM25", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.7914630174636841}]}, {"text": "The weighted features are then used for training an SVM classifier.", "labels": [], "entities": []}, {"text": "We used one-vs-rest multi-class strategy when training the SVM classifier for task 2.", "labels": [], "entities": []}, {"text": "All models were implemented in Python, using scikit-learn machine learning library).", "labels": [], "entities": []}, {"text": "The models are similar to the models we used in a few other text classification tasks (C \u00b8 \u00a8 oltekin and Rama, 2018; Rama and C \u00b8 \u00a8 oltekin, 2017; C \u00b8 \u00a8 oltekin and Rama, 2017), where the models are explained in detail.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.8187070886294047}]}, {"text": "We tuned the models for each task separately, changing the maximum order of character and word n-gram features, case normalization, and SVM margin parameter 'C'.", "labels": [], "entities": [{"text": "case normalization", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.6984686255455017}, {"text": "SVM margin parameter 'C'", "start_pos": 136, "end_pos": 160, "type": "METRIC", "confidence": 0.7968699683745702}]}, {"text": "The parameter ranges explored during tuning was 0-12 for maximum character n-gram order, 0-7 for maximum word n-gram order, and 0.1-2.0 with steps of 0.1 for 'C'.", "labels": [], "entities": []}, {"text": "We used 5-fold cross validation during tuning, using random search through the space of hy-: F1-scores of tf-idf and BM25 weighted models on the development set and the official test set.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.910540759563446}, {"text": "official test set", "start_pos": 169, "end_pos": 186, "type": "DATASET", "confidence": 0.7555853327115377}]}, {"text": "The F1-scores for task 2 are micro-averaged.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9986513257026672}]}, {"text": "The two set of scores for Task 4 reflect the difference between the full labeled-data set (including additional 1211 training instances) in comparison to the original training set.", "labels": [], "entities": []}, {"text": "Approximately 1000 random hyperparameter settings were tried for each model.", "labels": [], "entities": []}, {"text": "The models with the best parameter settings were retrained using the complete training data for producing the final predictions.", "labels": [], "entities": []}, {"text": "The source of the texts for all tasks is Twitter.", "labels": [], "entities": []}, {"text": "At the time we downloaded them, some tweets were not available, resulting in training set sizes of 9182, 15 723, 16 888, and 5759 for tasks 1, 2, 3 and 4 respectively.", "labels": [], "entities": []}, {"text": "Some of these numbers are substantially lower than that of intended number of training samples of 10 000, 17 000, 25 000, and 8180 respectively.", "labels": [], "entities": []}, {"text": "For task 4, we also used an additional 1211 tweets, initially planned as the test set for this task.", "labels": [], "entities": []}, {"text": "The test sets contained (approximately) 5000 tweets for tasks 1, 2 and 3, and a considerably smaller number (161) for task 4.", "labels": [], "entities": []}, {"text": "All training sets showed some degree of class imbalance.", "labels": [], "entities": []}, {"text": "The imbalance was particularly strong for tasks 3 and 4, where over 70 % and 90 % of the instances belonged to the negative class, respectively.", "labels": [], "entities": []}, {"text": "Further information on the data sets can be found in. presents F 1 -scores of the models on each task.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9790421277284622}]}, {"text": "In general, we do not observe substantial differences between the term weighting schemes, but for some tasks the gap between training and development set scores is rather large.", "labels": [], "entities": []}, {"text": "We do not know the system rankings at the time of writing, but only know that the results above are above the mean of the best-scores from all participating teams.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1-scores of tf-idf and BM25 weighted mod- els on the development set and the official test set. The  F1-scores for task 2 are micro-averaged. The two set  of scores for Task 4 reflect the difference between the  full labeled-data set (including additional 1211 training  instances) in comparison to the original training set.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9981977343559265}, {"text": "BM25 weighted mod- els", "start_pos": 34, "end_pos": 56, "type": "METRIC", "confidence": 0.8479300737380981}, {"text": "F1-scores", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9607648253440857}]}]}