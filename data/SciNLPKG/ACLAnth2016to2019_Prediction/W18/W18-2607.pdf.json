{"title": [{"text": "A Systematic Classification of Knowledge, Reasoning, and Context within the ARC Dataset", "labels": [], "entities": [{"text": "ARC Dataset", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.9292734265327454}]}], "abstractContent": [{"text": "The recent work of Clark et al.", "labels": [], "entities": []}, {"text": "(2018) introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into an Easy Set and a Challenge Set.", "labels": [], "entities": [{"text": "AI2 Reasoning Challenge (ARC)", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.7052440494298935}, {"text": "ARC dataset", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.8071699142456055}]}, {"text": "That paper includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them; however, it does not include clear definitions of these types, nor does it offer information about the quality of the labels.", "labels": [], "entities": []}, {"text": "We propose a comprehensive set of definitions of knowledge and reasoning types necessary for answering the questions in the ARC dataset.", "labels": [], "entities": [{"text": "ARC dataset", "start_pos": 124, "end_pos": 135, "type": "DATASET", "confidence": 0.897191196680069}]}, {"text": "Using ten annotators and a sophisticated annotation interface, we analyze the distribution of labels across the Challenge Set and statistics related to them.", "labels": [], "entities": []}, {"text": "Additionally , we demonstrate that although naive information retrieval methods return sentences that are irrelevant to answering the query, sufficient supporting text is often present in the (ARC) corpus.", "labels": [], "entities": []}, {"text": "Evaluating with human-selected relevant sentences improves the performance of a neu-ral machine comprehension model by 42 points.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recent work of  introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset.", "labels": [], "entities": [{"text": "AI2 Reasoning Challenge (ARC)", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.7276469171047211}, {"text": "ARC dataset", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.8926704525947571}]}, {"text": "This dataset contains science questions from standardized tests that are separated into an Easy Set and a Challenge Set.", "labels": [], "entities": []}, {"text": "The Challenge Set is comprised of questions that are answered incorrectly by two solvers based on http://data.allenai.org/arc/ Pointwise Mutual Information (PMI) Information Retrieval (IR).", "labels": [], "entities": [{"text": "Pointwise Mutual Information (PMI) Information Retrieval (IR)", "start_pos": 127, "end_pos": 188, "type": "TASK", "confidence": 0.6849383982745084}]}, {"text": "In addition to this division, a survey of the various types of knowledge as well as the types of reasoning that are required to answer various questions in the ARC dataset was presented.", "labels": [], "entities": [{"text": "ARC dataset", "start_pos": 160, "end_pos": 171, "type": "DATASET", "confidence": 0.836501270532608}]}, {"text": "This survey was based on an analysis of 100 questions chosen at random from the Challenge Set.", "labels": [], "entities": []}, {"text": "However, very little detail is provided about the questions chosen, the annotations provided, or the methodology used.", "labels": [], "entities": []}, {"text": "These questions surround the very core of the paper, since the main contribution is a dataset that contains complex questions.", "labels": [], "entities": []}, {"text": "Additionally, while their manual analysis suggests that 95% of the questions can be answered using the ARC corpus,  note that the IR system (Elasticsearch 2 ) serves as a severe bottleneck.", "labels": [], "entities": [{"text": "ARC corpus", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.840559333562851}]}, {"text": "Our annotation process supports this observation, but we also find that simple reformulations to the query can greatly increase the quality of the retrieved sentences.", "labels": [], "entities": []}, {"text": "Contributions: In this work, in order to overcome some of the limitations of  described above, we present a detailed annotation process for the ARC dataset.", "labels": [], "entities": [{"text": "ARC dataset", "start_pos": 144, "end_pos": 155, "type": "DATASET", "confidence": 0.9097347259521484}]}, {"text": "Specifically, we (a) introduce a novel labeling interface that allows a distributed set of annotators to label the knowledge and reasoning types; and (b) improve upon the knowledge and reasoning type categories provided previously, in order to make the annotation more intuitive and accurate.", "labels": [], "entities": []}, {"text": "Following an annotation round involving over ten people at two institutions, we measure and report statistics such as inter-rater agreement, and the distribution of knowledge and reasoning type labels in the dataset.", "labels": [], "entities": []}, {"text": "We then (c) clarify the role of knowledge and reasoning within the ARC dataset with a comprehensive set of annotations for both the questions and returned results, demonstrating the efficacy of query refine-ment to improve existing QA systems.", "labels": [], "entities": [{"text": "ARC dataset", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.9415949583053589}]}, {"text": "Our annotators were also asked to mark whether individual retrieved sentences were relevant to answering a given question.", "labels": [], "entities": []}, {"text": "Our labeling interface logs the reformulated queries issued by each annotator, as well as their relevance annotations.", "labels": [], "entities": []}, {"text": "To quantitatively demonstrate the effectiveness of the relevant sentences, we (d) evaluate a subset of questions and the relevant retrieval results with a pre-trained DrQA model (, and find that the performance of the system increases by 42 points.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 167, "end_pos": 171, "type": "DATASET", "confidence": 0.9198698997497559}]}], "datasetContent": [{"text": "In previous work , the standardized test questions under consideration are split into various categories based on the kinds of knowledge and reasoning that are needed to answer those questions.", "labels": [], "entities": []}, {"text": "The idea of classifying questions by these two types is central to the notion of standardized testing, which endeavors to test students on various kinds of knowledge, as well as various problem types and solution techniques.", "labels": [], "entities": []}, {"text": "In accordance with this, Clark et al. provide preliminary definitions for knowledge and reasoning categories that can be employed by a QA system to solve a given question.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Pairwise inter-rater agreement along with the mean  and Fleiss' \u03ba for survey responses.", "labels": [], "entities": [{"text": "Fleiss' \u03ba", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9858455955982208}]}, {"text": " Table 4: Pairwise inter-rater agreement along with the mean  and Fleiss' \u03ba for survey responses.", "labels": [], "entities": [{"text": "Fleiss' \u03ba", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9857039153575897}]}, {"text": " Table 5: Accuracy on our subset of ARC Challenge Set questions, partitioned based on the first label from the Kemeny ordering  of the reasoning and knowledge type annotations, respectively; 1 /k partial credit is given when the correct answer is in the set  of k selected answers. The number of questions assigned each primary label is indicated by (#).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9882078766822815}, {"text": "ARC Challenge Set questions", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.8657047599554062}]}]}