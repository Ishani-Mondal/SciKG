{"title": [{"text": "On Learning Better Word Embeddings from Chinese Clinical Records: Study on Combining In-Domain and Out-Domain Data", "labels": [], "entities": [{"text": "Chinese Clinical Records", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.5956367750962576}]}], "abstractContent": [{"text": "High quality word embeddings are of great significance to advance applications of bi-omedical natural language processing.", "labels": [], "entities": [{"text": "bi-omedical natural language processing", "start_pos": 82, "end_pos": 121, "type": "TASK", "confidence": 0.6706462204456329}]}, {"text": "In recent years, a surge of interest on how to learn good embeddings and evaluate embedding quality based on English medical text has become increasing evident, however a limited number of studies based on Chinese medical text, particularly Chinese clinical records, were performed.", "labels": [], "entities": []}, {"text": "Herein, we proposed a novel approach of improving the quality of learned embeddings using out-domain data as a supplementary in the case of limited Chinese clinical records.", "labels": [], "entities": []}, {"text": "Moreover, the embedding quality evaluation method was conducted based on Medical Conceptual Similarity Property.", "labels": [], "entities": []}, {"text": "The experimental results revealed that selecting good training samples was necessary, and collecting right amount of out-domain data and trading off between the quality of em-beddings and the training time consumption were essential factors for better em-beddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings, or embeddings for short, have been widely used in various natural language processing tasks, such as language modeling (, syntactic parsing ( and part-ofspeech tagging.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7080731838941574}, {"text": "syntactic parsing", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.715765506029129}, {"text": "part-ofspeech tagging", "start_pos": 163, "end_pos": 184, "type": "TASK", "confidence": 0.7360822856426239}]}, {"text": "Owing to the advantage of embeddings in boosting performance, a surge of interest in applying embeddings has become increasingly evident with numerous encouraging results in the field of biomedical applications, e.g. disease prediction (, clinical events prediction (), medical concept disambiguation (, and biomedical information retrieval.", "labels": [], "entities": [{"text": "disease prediction", "start_pos": 217, "end_pos": 235, "type": "TASK", "confidence": 0.731235608458519}, {"text": "clinical events prediction", "start_pos": 239, "end_pos": 265, "type": "TASK", "confidence": 0.623911440372467}, {"text": "medical concept disambiguation", "start_pos": 270, "end_pos": 300, "type": "TASK", "confidence": 0.6488032639026642}, {"text": "biomedical information retrieval", "start_pos": 308, "end_pos": 340, "type": "TASK", "confidence": 0.6527305046717325}]}, {"text": "Learning embeddings from English medical texts, as a hot topic in recent years, has been extensively studied due to the efforts of open datasets, such as UMLS of NLM, medical journal abstracts from PubMed (, and some released clinical data.", "labels": [], "entities": [{"text": "UMLS of NLM", "start_pos": 154, "end_pos": 165, "type": "DATASET", "confidence": 0.8616430759429932}]}, {"text": "These datasets have been widely used as gold standards by the biomedical natural language processing domain for learning embeddings).", "labels": [], "entities": []}, {"text": "However, the development of learning embeddings from Chinese medical texts has fallen far behind, especially from Chinese clinical records.", "labels": [], "entities": []}, {"text": "Due to the privacy concerns, Chinese clinical records that can be used are generally limited.", "labels": [], "entities": []}, {"text": "Learning better embeddings based on neural network architectures, for instance the widely used skipgram model), usually needs a large number of training data.", "labels": [], "entities": []}, {"text": "As a result, the learned embeddings from Chinese clinical records are not good enough.", "labels": [], "entities": []}, {"text": "Moreover, to the best of our knowledge, there is a limited number of studies focusing on learning embeddings from Chinese clinical records, not to mention the embedding evaluation.", "labels": [], "entities": []}, {"text": "Many methods have been developed to learn embeddings from English medical texts, however, Chinese medical texts, especially clinical records, have their particular language features.", "labels": [], "entities": []}, {"text": "Therefore, adaptions to the approaches of learning embeddings from English medical texts are urgently needed for learning embeddings from Chinese clinical records.", "labels": [], "entities": []}, {"text": "In this paper, we focused on learning embeddings from Chinese clinical records, and our major contributions were as follows: \u2022 We proposed an in-domain and out-domain data combination method for learning better embeddings from Chinese clinical records by the skip-gram model under the situation that we only have limited Chinese clinical records.", "labels": [], "entities": []}, {"text": "\u2022 Referring to the evaluation method for medical concept embeddings proposed in () which is based on medical conceptual similarity property, we proposed a method for distantly evaluating the learned embeddings from Chinese clinical records using an additional standard medical terminology dataset.", "labels": [], "entities": []}, {"text": "\u2022 We found that selecting good training samples is necessary.", "labels": [], "entities": []}, {"text": "Collecting right amount of out-domain data, trading off between the quality of embeddings and the training time consumption are essential factors for better embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Chinese clinical records were segmented into words by the latest version of Stanford CoreNLP tool 1 with default settings, and adjacent words appearing in our prepared standard medical word dataset would not be segmented (.", "labels": [], "entities": [{"text": "Stanford CoreNLP tool 1", "start_pos": 76, "end_pos": 99, "type": "DATASET", "confidence": 0.8571017980575562}]}, {"text": "Out-domain data went through a similar process but without the second process.", "labels": [], "entities": []}, {"text": "We assume that in out-domain data there is no medical words.", "labels": [], "entities": []}, {"text": "We directly applied skip-gram model implemented by DeepLearning4J 2 to learn embeddings.", "labels": [], "entities": []}, {"text": "Hierarchical SoftMax is used in training process, and context window size and embedding dimensionality are set to 5 and 200 respectively (Choi et al, 2016b).", "labels": [], "entities": []}, {"text": "We used an intrinsic evaluation method, named Chinese Medical Concept Similarity Measure (CMCSM), to distantly measure quality of learned embeddings.", "labels": [], "entities": [{"text": "Chinese Medical Concept Similarity Measure (CMCSM", "start_pos": 46, "end_pos": 95, "type": "TASK", "confidence": 0.7166258777890887}]}, {"text": "CMCSM is defined below: where ; is the number of groups of the medical words in the same level of a prepared medical word dataset <, % = \u2208 < is one group of the medical words, and 4 ? and 4 @ are the ?th and @th terms in % = . 234 5 , 4 \" 7 is any commonly used embedding similarity measure (.", "labels": [], "entities": [{"text": "CMCSM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8984591960906982}]}, {"text": "In this paper, we used the cosine measure.", "labels": [], "entities": []}, {"text": "Firstly, we applied skip-gram model to learn embeddings from CCRD and the learned embeddings were evaluated by CMCSM.", "labels": [], "entities": [{"text": "CCRD", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9650479555130005}, {"text": "CMCSM", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.9714940190315247}]}, {"text": "We sampled 5 subdatasets from CCRD in order to assess effect of different size of datasets on quality of the learned embeddings.", "labels": [], "entities": [{"text": "CCRD", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9583063125610352}]}, {"text": "The sizes of the sampled datasets were 80%, 60%, 40%, 20% and 10% of instances in the original CCRD.", "labels": [], "entities": [{"text": "CCRD", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.8947644233703613}]}, {"text": "The sampling process was a recursive sampling without replacement.", "labels": [], "entities": []}, {"text": "It implied that more data means more stable learning results of embeddings.", "labels": [], "entities": []}, {"text": "Moreover, we ran the above process 10 times to further assess the stability of the results.", "labels": [], "entities": []}, {"text": "The results were used as the baseline, and they were shown in.", "labels": [], "entities": []}, {"text": "We found in that the more Chinese clinical records were used for learning embeddings, the smaller variance of CMCSM tended to be achieved.", "labels": [], "entities": []}, {"text": "Moreover, an interesting result was that the use of all Chinese clinical records did not nec-  We filtered the terminologies which do not appear in CCRD.", "labels": [], "entities": [{"text": "CCRD", "start_pos": 148, "end_pos": 152, "type": "DATASET", "confidence": 0.9308300614356995}]}, {"text": "URL: http://www.wpro.who.int/publications/who_i-strm_file.pdf?ua=1.", "labels": [], "entities": []}, {"text": "Size, indicating through combining ODD into CCRD, the qualities of the learned embeddings in different conditions were improved dramatically.", "labels": [], "entities": []}, {"text": "More ODD data is combined into CCRD, better embeddings would be learned.", "labels": [], "entities": []}, {"text": "In the best case (combining the \"Time 2-60%\" dataset with the \"ODD-ALL\" dataset), CMCSM increased by 3.8 times.", "labels": [], "entities": [{"text": "Time 2-60%\" dataset", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.8463481664657593}, {"text": "ODD-ALL\" dataset", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.6609621644020081}]}, {"text": "Notably, the highest quality of the learned embeddings in each row of was not always achieved when all data in ODD was used.", "labels": [], "entities": []}, {"text": "This result was consistent with the result mentioned earlier, indicating that we should collect as much training data as possible and also need to pay attention to reasonably choosing training samples.", "labels": [], "entities": []}, {"text": "In addition, the results showed that when the amount of ODD was 1000 times of the basis size of CCRD, optimal embeddings would be achieved.", "labels": [], "entities": [{"text": "ODD", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.7671189904212952}]}, {"text": "Moreover, the results suggested that, in practice, the trade-off between quality of embeddings and training time consumption should be considered.", "labels": [], "entities": []}, {"text": "displayed that with increasing the amount of the combined ODD, the growth rate of CMCSM of learned embeddings from basis size of CCRD decreased sharply.", "labels": [], "entities": []}, {"text": "Furthermore, when the amount of the combined ODD was more than 50 times of the basis size, the growth rate was almost converged.", "labels": [], "entities": []}, {"text": "While, as we know, more data were used for learning embeddings by skip-gram model, much more time would be consumed.", "labels": [], "entities": []}, {"text": "We should consider whether it is worthwhile to spend a lot of training time in exchange for very little quality improvement.", "labels": [], "entities": []}, {"text": "Moreover, little quality improvement sometimes may not improve performance of downstream biomedical applications.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Detailed Information of the Experi- mental Datasets.", "labels": [], "entities": [{"text": "Experi- mental Datasets", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.5234833583235741}]}, {"text": " Table 2: CMCSM Results of the Embeddings Learned from CCRD by the Skip-Gram Model.", "labels": [], "entities": [{"text": "CMCSM", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.8309704065322876}, {"text": "CCRD", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9264296889305115}]}, {"text": " Table 3: CMCSM Results of the Embeddings Learned from the Combinations of CCRD and ODD  by the Skip-Gram Model. \"Tn-X%\" means that \"the dataset is the X% data of CCRD which is used  for learning the highest quality of embeddings in", "labels": [], "entities": []}]}