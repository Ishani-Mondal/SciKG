{"title": [{"text": "An Empirical Study of Machine Translation for the Shared Task of WMT18", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7401953637599945}, {"text": "WMT18", "start_pos": 65, "end_pos": 70, "type": "TASK", "confidence": 0.3499908447265625}]}], "abstractContent": [{"text": "This paper describes the Global Tone Communication Co., Ltd.'s submission of the WMT18 shared news translation task.", "labels": [], "entities": [{"text": "Global Tone Communication Co.", "start_pos": 25, "end_pos": 54, "type": "DATASET", "confidence": 0.7502359300851822}, {"text": "WMT18 shared news translation task", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.6985753178596497}]}, {"text": "We participated in the English-to-Chinese direction and get the best BLEU (43.8) scores among all the participants.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9995290040969849}]}, {"text": "The submitted system focus on data clearing and techniques to build a competitive model for this task.", "labels": [], "entities": [{"text": "data clearing", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.8484936058521271}]}, {"text": "Unlike other participants, the submitted system are mainly relied on the data filtering to obtain the best BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.9710044264793396}]}, {"text": "We do data filtering not only for provided sentences but also for the back translated sentences.", "labels": [], "entities": []}, {"text": "The techniques we apply for data filtering include filtering by rules, language models and translation models.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.8263446986675262}]}, {"text": "We also conduct several experiments to validate the effectiveness of training techniques.", "labels": [], "entities": []}, {"text": "According to our experiments, the Annealing Adam optimizing function and ensemble decoding are the most effective techniques for the model training.", "labels": [], "entities": []}], "introductionContent": [{"text": "We participated in the WMT shared news translation task and focus on the English-to-Chinese direction.", "labels": [], "entities": [{"text": "WMT shared news translation task", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.7097899317741394}]}, {"text": "Our neural machine translation system is developed as transformer () architecture and the toolkit we used is Marian.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.7566934625307719}]}, {"text": "Since BLEU () is the main ranking index for all submitted systems, we apply BLEU as the evaluation matrix for our translation system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9972696900367737}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9980019927024841}]}, {"text": "We aim to verify whether the techniques we applied in the Encoder Decoder architecture of recurrent neural network(RNN) and attention mechanism () are also positive for transformer architecture () and the effectiveness of the data filtering.", "labels": [], "entities": []}, {"text": "For data preprocessing, the basic methods include Chinese word segmentation, tokenization, byte pair encoding(BPE) ().", "labels": [], "entities": [{"text": "data preprocessing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6958156526088715}, {"text": "Chinese word segmentation", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.5598254203796387}]}, {"text": "Besides, human rules and translation model are also involved for cleaning parallel data, as well as using language model for cleaning monolingual data.", "labels": [], "entities": []}, {"text": "As to the techniques on model training, Annealing Adam (, back-translation () and rightto-left reranking) which have proven to be effective in the Encoder Decoder model with RNN layer and attention mechanism are applied to verify whether these techniques in transformer architecture are also effective.", "labels": [], "entities": []}, {"text": "When comparing our baseline model, we show the increase in 5.57 BLEU scores of English to Chinese direction for news.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9959938526153564}]}, {"text": "And comparing the best score in last year, transformer architecture is more powerful than RNN with attention mechanism with 3.65 BLEU score improvement.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9996644258499146}]}, {"text": "However, not all the techniques we applied to RNN with attention mechanism are equally effective against transformer architecture, especially reranking by right-to-left model.", "labels": [], "entities": []}, {"text": "This paper is arranged as follows.", "labels": [], "entities": []}, {"text": "We firstly describe the task and provided data information, then introduce the method of data filtering, including rules, language model and translation model.", "labels": [], "entities": []}, {"text": "After that, we describe the techniques on transformer architecture and show the conducted experiments in detail, including data preprocessing, postprocessing and model architecture.", "labels": [], "entities": [{"text": "transformer architecture", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.8625454902648926}]}, {"text": "At last, we analyse the results of experiments and draw the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the all experiments we conducted and illustrate how we get the evaluation step by step.", "labels": [], "entities": []}, {"text": "We describe all the experiment setups for this task in detail.", "labels": [], "entities": []}, {"text": "The transformer baseline is trained with only parallel data, including CWMT corpus, UN Parallel Corpus V1.0 and News Commentary v13, after data preprocessing.", "labels": [], "entities": [{"text": "CWMT corpus", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.9526486396789551}, {"text": "UN Parallel Corpus V1.0", "start_pos": 84, "end_pos": 107, "type": "DATASET", "confidence": 0.8629166483879089}]}, {"text": "We trained the baseline system not only in English to Chinese direction, but also in Chinese to English direction in order to translate the filtered monolingual data and do   the parallel data filtering.", "labels": [], "entities": [{"text": "parallel data filtering", "start_pos": 179, "end_pos": 202, "type": "TASK", "confidence": 0.6827077269554138}]}, {"text": "During the training procedure the number of BPE merge operation is set to 40,000 for both English and Chinese.", "labels": [], "entities": [{"text": "BPE merge", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.594974011182785}]}, {"text": "The hyperparameter of our baseline model configuration is shown in Table2 and the training parameter is in.", "labels": [], "entities": []}, {"text": "After the baseline, we filter parallel data through rules and translation model.", "labels": [], "entities": []}, {"text": "The relative punctuation frequency threshold and absolute punctuation frequency threshold we mentioned in section 3 is 5 and 15 respectively.", "labels": [], "entities": [{"text": "relative punctuation frequency threshold", "start_pos": 4, "end_pos": 44, "type": "METRIC", "confidence": 0.7107961103320122}, {"text": "absolute punctuation frequency threshold", "start_pos": 49, "end_pos": 89, "type": "METRIC", "confidence": 0.8906170576810837}]}, {"text": "We construct the synthetic data with back translation baseline model from Chinese to English.", "labels": [], "entities": []}, {"text": "The synthetic data is firstly filtered by Chinese language model and then filtered by English language model.", "labels": [], "entities": []}, {"text": "shows the detail information about the data filtering.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7429333329200745}]}, {"text": "In general, we trained 3 models to explore the effect of data filtering, which are: 1.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.7234573364257812}]}, {"text": "baseline model with provided parallel sentences; 2.", "labels": [], "entities": []}, {"text": "baseline model with parallel sentences filtered by rules and translation model; 3.", "labels": [], "entities": []}, {"text": "baseline model with sentences mixed parallel sentences filtered by rules and translation model and synthetic sentences filtered by language model.", "labels": [], "entities": []}, {"text": "Beside the baseline models, we trained four groups of translation model with fully filtered parallel data and synthetic data.", "labels": [], "entities": []}, {"text": "Each model in the four groups is trained with different random seed and also apply Annealing Adam which get better performance compared with Adam.", "labels": [], "entities": []}, {"text": "Therefore, we got 8 different translation models with the filtered data.", "labels": [], "entities": []}, {"text": "We applied the greedy ensemble strategy to combine the 8 models and finally obtain the best translation performance on the development set with 3 models.", "labels": [], "entities": []}, {"text": "Another, the right-to-left model in target side is also trained to rerank n-best translation of three best translation performance models.", "labels": [], "entities": []}, {"text": "shows the BLEU score we evaluated on development set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981260895729065}]}, {"text": "For data filtering, we observed that the methods improve the quality of sentences and get a better BLEU score.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.749032735824585}, {"text": "BLEU score", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9826614260673523}]}, {"text": "The methods can solve some problems of corpus quality.", "labels": [], "entities": []}, {"text": "For model training techniques, back-translation is still the most effective method of improvement on 3.83-3.93 BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9784693717956543}]}, {"text": "Annealing Adam has an improvement of BLEU score ranging from 0.04 to 0.36.", "labels": [], "entities": [{"text": "Annealing Adam", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9831554293632507}, {"text": "BLEU score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9622672200202942}]}, {"text": "The evaluation table shows that the higher BLEU score we get from the neural machine translation model, the smaller improvement can we get from Annealing Adam.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9993478655815125}, {"text": "neural machine translation", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.6925687789916992}, {"text": "Annealing Adam", "start_pos": 144, "end_pos": 158, "type": "DATASET", "confidence": 0.8337141275405884}]}, {"text": "When ensemble decoding, the greedy ensemble decoding strategy get the improvement on 0.56 BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9988232254981995}]}, {"text": "However, when trying to decode our models ensemble with rightto-left reranking it did not improve the BLEU score as we expected.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9831995964050293}]}], "tableCaptions": [{"text": " Table 2: The main model configuration. FFN means  feed forward network.", "labels": [], "entities": [{"text": "FFN", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.541999340057373}]}, {"text": " Table 3: The training and decoding parameter.", "labels": [], "entities": []}, {"text": " Table 5: The BLEU score in character level for devel- opment set of English-to-Chinese direction. SS means  synthetic sentences, TM means translation model, LM  means language model and PS means parallel sen- tences. The greedy ensemble decoding means decod- ing the 8 models and finally obtain the best translation  performance on development set with 3 models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9990418553352356}]}]}