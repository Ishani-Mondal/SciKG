{"title": [{"text": "Morphological Word Embeddings for Arabic Neural Machine Translation in Low-Resource Settings", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.6562179426352183}]}], "abstractContent": [{"text": "Neural machine translation has achieved impressive results in the last few years, but its success has been limited to settings with large amounts of parallel data.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8717711766560873}]}, {"text": "One way to improve NMT for lower-resource settings is to initialize a word-based NMT model with pretrained word embeddings.", "labels": [], "entities": []}, {"text": "However, rare words still suffer from lower quality word embeddings when trained with standard word-level objectives.", "labels": [], "entities": []}, {"text": "We introduce word embeddings that utilize morphological resources, and compare to purely unsupervised alternatives.", "labels": [], "entities": []}, {"text": "We work with Arabic, a morphologically rich language with available linguistic resources, and perform Ar-to-En MT experiments on a small corpus of TED subtitles.", "labels": [], "entities": [{"text": "Ar-to-En MT", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.5457595735788345}]}, {"text": "We find that word em-beddings utilizing subword information consistently outperform standard word embed-dings on a word similarity task and as initial-ization of the source word embeddings in a low-resource NMT system.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.7451107700665792}]}], "introductionContent": [{"text": "Neural machine translation () has recently become the dominant approach to machine translation.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7986270189285278}, {"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7806923687458038}]}, {"text": "However, the standard encoder-decoder models with attention have been shown to perform poorly in low-resource settings, a problem which can be alleviated by initialization of parameters from an NMT system trained on higher-resource languages ().", "labels": [], "entities": []}, {"text": "An alternative way to initialize parameters in a low-resource NMT setup is to use pretrained monolingual word embeddings, which are quick to train and readily available for many languages.", "labels": [], "entities": []}, {"text": "There is a large body of work on word embeddings.", "labels": [], "entities": []}, {"text": "Popular approaches include word2vec () and GloVe (.", "labels": [], "entities": []}, {"text": "These have been shown to perform well in word similarity tasks and a variety of downstream tasks.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.8526177803675333}]}, {"text": "However, they have been primarily evaluated on English.", "labels": [], "entities": []}, {"text": "The learned representations for rare words are of low quality due to sparsity.", "labels": [], "entities": []}, {"text": "For morphologically rich languages, we may want word embeddings that also consider morphological information, to reduce sparsity in word embedding training.", "labels": [], "entities": []}, {"text": "Previous work on morphological word embeddings has shown improvements on word similarity tasks, but has not been evaluated on downstream NMT tasks.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.7765278617540995}]}, {"text": "Our contribution is two-fold: 1.", "labels": [], "entities": []}, {"text": "We adapt word2vec to utilize lemmas from a morphological analyzer, and show improvements on a word similarity task over a stateof-the-art unsupervised approach to incorporating morphological information based on character n-grams ().", "labels": [], "entities": []}, {"text": "2. We experiment with Arabic-to-English NMT on the TED Talks corpus.", "labels": [], "entities": [{"text": "TED Talks corpus", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.8871935407320658}]}, {"text": "Our results demonstrate that incorporating some form of morphological word embeddings into NMT improves BLEU scores and outperforms the conventional approaches of using standard word embeddings, random initialization, or bytepair encoding (BPE).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9979368448257446}]}], "datasetContent": [{"text": "We compare three types of embeddings: \u2022 word2vec: standard skip-gram word embeddings that only use word information.", "labels": [], "entities": []}, {"text": "\u2022 fastText: skip-gram embeddings that are sums of vectors representing character ngrams, implicitly incorporating some form of morphological information.", "labels": [], "entities": []}, {"text": "\u2022 morph: the modified skip-gram word embeddings described in Section 3.1, which rely on a morphological analyzer and lemma embeddings.", "labels": [], "entities": []}, {"text": "The word embeddings inserted into the NMT system are always of dimension 300, and in word similarity experiments, we experiment with dimensions of different sizes.", "labels": [], "entities": []}, {"text": "All word embeddings are trained with negative sampling (5 samples), with a window size of 5, a 10 \u22124 rejection threshold for subsampling, and 5 iterations.", "labels": [], "entities": []}, {"text": "Additional fastText parameters are left at the default.", "labels": [], "entities": []}, {"text": "We use OpenNMT-py () for all NMT experiments, with a max sentence size of 80.", "labels": [], "entities": []}, {"text": "We use word-level prediction accuracy for model selection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.7694534659385681}, {"text": "model selection", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7360685467720032}]}, {"text": "For the BPE baseline, the number of BPE merge operations is 30,000.", "labels": [], "entities": [{"text": "BPE baseline", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.9317334294319153}, {"text": "BPE merge", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.5971251726150513}]}, {"text": "The hidden layer size is 1024, trained with batch size 80, with Adadelta (Zeiler, 2012) and a dropout rate of 0.2 for 20 epochs with a learning rate of 1.0.", "labels": [], "entities": [{"text": "Adadelta", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.8025215864181519}]}, {"text": "When initializing the encoder with word embeddings, we experiment both with locking the word We use Buckwalter transliteration).: Spearman coefficient for Arabic word similarity dataset built off of WS353.", "labels": [], "entities": [{"text": "Spearman coefficient", "start_pos": 130, "end_pos": 150, "type": "METRIC", "confidence": 0.9374261796474457}, {"text": "WS353", "start_pos": 199, "end_pos": 204, "type": "DATASET", "confidence": 0.9677857160568237}]}, {"text": "We list the dimension of the word embedding, and in the case of morph, we list the dimensions of the word part and the lemma part.", "labels": [], "entities": []}, {"text": "In the morph system, lemma refers to using just the lemma part of the vector to compare similarity, word refers to using just the word part, and word+lemma refers to using the whole vector.", "labels": [], "entities": []}, {"text": "embeddings throughout training (\"fixed\") and allowing backpropagation through the word embeddings (\"unfixed\").", "labels": [], "entities": []}, {"text": "At test time, words not seen in the MT training data are also initialized with word embeddings, if they were seen in the word embedding training data.", "labels": [], "entities": [{"text": "MT training data", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.8449124097824097}]}, {"text": "Words unseen by either corpus are mapped to the embedding of an <unk> token.", "labels": [], "entities": []}, {"text": "The bitext we use for NMT is a collection of TED subtitles obtained from WIT 3 (Cettolo et al., 2012).", "labels": [], "entities": [{"text": "NMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.6549172401428223}, {"text": "TED subtitles obtained from WIT 3", "start_pos": 45, "end_pos": 78, "type": "DATASET", "confidence": 0.652502050002416}]}, {"text": "4 This is a collection of monologue speeches from TED talks, covering a wide range of topics such technology, design, and social science.", "labels": [], "entities": []}, {"text": "We downloaded the latest XML files (version 2016-04-08) for Arabic and performed subtitle extraction and sentence merging using the WIT 3 scripts.", "labels": [], "entities": [{"text": "subtitle extraction", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7861031591892242}, {"text": "sentence merging", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.7168934494256973}, {"text": "WIT 3 scripts", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.9096807837486267}]}, {"text": "The data is then randomly split at the granularity of talks, with 1939 talks for training, 30 talks for development, and 30 talks for testing.", "labels": [], "entities": []}, {"text": "The corresponding sentence/token statistics are shown in.", "labels": [], "entities": []}, {"text": "In this data, 9% of word types and 3% of tokens in the test data were not seen in train.", "labels": [], "entities": []}, {"text": "The monolingual corpus we use for word embeddings is cleaned and tokenized Arabic Wikipedia, consisting of about 80 million tokens, with a vocabulary of around 350k words.", "labels": [], "entities": []}, {"text": "The word embeddings are trained on both the monolingual corpus and the source side of the TED training data.", "labels": [], "entities": [{"text": "TED training data", "start_pos": 90, "end_pos": 107, "type": "DATASET", "confidence": 0.6764121353626251}]}, {"text": "The number of lemma types in the monolingual corpus is 672k, and in TED training data is 42k.", "labels": [], "entities": [{"text": "TED training data", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.677258312702179}]}], "tableCaptions": [{"text": " Table 1: Spearman coefficient for Arabic word similarity dataset built off of WS353. We list the  dimension of the word embedding, and in the case of morph, we list the dimensions of the word part and  the lemma part. In the morph system, lemma refers to using just the lemma part of the vector to compare  similarity, word refers to using just the word part, and word+lemma refers to using the whole vector.", "labels": [], "entities": [{"text": "WS353", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.958303689956665}]}, {"text": " Table 3: Corpus-level BLEU on the test set, averaged over 3 runs, with individual runs. \u2206 is the difference  in BLEU between the model vs. random initialization with words as units.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.8835512399673462}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9984702467918396}]}, {"text": " Table 4: BLEU on test sentences that have rare morphological variants. \u2206 is the difference in BLEU  between the model vs. random initialization with words as units.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984353184700012}, {"text": "\u2206", "start_pos": 72, "end_pos": 73, "type": "METRIC", "confidence": 0.9602997303009033}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9991657733917236}]}]}