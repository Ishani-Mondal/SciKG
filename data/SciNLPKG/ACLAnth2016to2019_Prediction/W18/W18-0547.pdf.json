{"title": [{"text": "Neural sequence modelling for learner error prediction", "labels": [], "entities": [{"text": "Neural sequence modelling", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7264469067255656}, {"text": "learner error prediction", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6046854257583618}]}], "abstractContent": [{"text": "This paper describes our use of two recurrent neural network sequence models: sequence labelling and sequence-to-sequence models, for the prediction of future learner errors in our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM).", "labels": [], "entities": [{"text": "Duolingo Shared Task on Second Language Acquisition Modeling (SLAM)", "start_pos": 204, "end_pos": 271, "type": "TASK", "confidence": 0.6398852277885784}]}, {"text": "We show that these two models capture complementary information as combining them improves performance.", "labels": [], "entities": []}, {"text": "Furthermore, the same network architecture and group of features can be used directly to build competitive prediction models in all three language tracks, demonstrating that our approach generalises well across languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most recent work on second language acquisition (SLA) has focused on intermediate-to-advanced learners in assessment settings driven by a series of shared tasks (.", "labels": [], "entities": [{"text": "second language acquisition (SLA)", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.8097733010848364}]}, {"text": "The 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM) ( targets early stage learners and aims to provide personalised learning instructions.", "labels": [], "entities": [{"text": "Duolingo Shared Task on Second Language Acquisition Modeling (SLAM)", "start_pos": 9, "end_pos": 76, "type": "TASK", "confidence": 0.6456755833192305}]}, {"text": "Participating teams are provided with transcripts from exercises submitted by learners over their first 30 days of learning on Duolingo, 1 which are annotated for token (word) level errors.", "labels": [], "entities": [{"text": "Duolingo, 1", "start_pos": 127, "end_pos": 138, "type": "DATASET", "confidence": 0.9591618180274963}]}, {"text": "The task is to predict what errors each learner will make in the future based on their learning history.", "labels": [], "entities": []}, {"text": "There are three language tracks in this shared task: \u2022 en es: native Spanish speakers learning English; \u2022 es en: native English speakers learning Spanish; 1 https://www.duolingo.com \u2022 fr en: native English speakers learning French.", "labels": [], "entities": []}, {"text": "Teams can either focus on a particular language track, or explore generalised models and features across all three languages.", "labels": [], "entities": []}, {"text": "Inspired by the success of neural sequence models in grammatical error detection and correction (, we propose two recurrent neural network sequence models for this problem: sequence labelling and sequence-to-sequence modelling.", "labels": [], "entities": [{"text": "grammatical error detection and correction", "start_pos": 53, "end_pos": 95, "type": "TASK", "confidence": 0.7357329726219177}]}, {"text": "We demonstrate the utility of these two models for the future learner error prediction task.", "labels": [], "entities": [{"text": "learner error prediction task", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.7213663086295128}]}, {"text": "We also provide evidence of performance gains by using an ensemble of these two models, suggesting that they are complementary to each other.", "labels": [], "entities": []}, {"text": "For model development, we focus on the English track only and language-specific features are introduced and studied.", "labels": [], "entities": []}, {"text": "When it comes to official evaluation, two new prediction systems, one for the es en track and another for the fr en track, are built using the same network architecture and the same (hyper-)parameter setting, without tuning for new datasets or languages.", "labels": [], "entities": []}, {"text": "Competitive results on all three language tracks show that our approach generalises well and might be used as a generic solution across different languages.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows: Section 2 describes our approach and two neural sequence models in detail, Section 3 discusses the feature types that we exploit in our models, Section 4 reports our experiments and results on the development set for the en es track, Section 5 presents our official results on the test sets for all three language tracks.", "labels": [], "entities": []}, {"text": "Finally, Section 6 provides conclusions and ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The shared task dataset comprises answers submitted by more than 6,000 Duolingo users over the course of their first 30 days.", "labels": [], "entities": []}, {"text": "Token-level binary labels are provided: Matched tokens are given the label '0'; and missing or misspelt tokens (ignoring capitalisation, punctuation and accents) are given the label '1' to indicate an error.", "labels": [], "entities": []}, {"text": "Only correct references and label sequences are provided, not original learners' responses.", "labels": [], "entities": []}, {"text": "Therefore, in our experiments, we map correct reference to its label sequence.", "labels": [], "entities": []}, {"text": "The dataset is partitioned sequentially into training, development and test sets, which all contain the same group of learners.", "labels": [], "entities": []}, {"text": "The training set contains the first 80% of the sessions for each learner, followed by the next 10% for development and the final 10% for testing.", "labels": [], "entities": []}, {"text": "Each learner's test items are subsequent to their development items, which in turn are all subsequent to their training items.", "labels": [], "entities": []}, {"text": "During development, we focus on learners of English.", "labels": [], "entities": []}, {"text": "The training set provided for the en es track contains approximately 2,622,958 tokens (however, only 13% are labelled with '1') in about 824,012 sentences.", "labels": [], "entities": []}, {"text": "The development set includes additional 387,374 tokens in 115,770 sentences.", "labels": [], "entities": []}, {"text": "All the data has been pre-processed using the Google SyntaxNet dependency parser 8 by the shared task organisers.", "labels": [], "entities": []}, {"text": "System performance is evaluated in terms of area under the ROC curve (AUROC) and F1 (with a threshold of 0.5).", "labels": [], "entities": [{"text": "ROC curve (AUROC)", "start_pos": 59, "end_pos": 76, "type": "METRIC", "confidence": 0.8514567971229553}, {"text": "F1", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9991462230682373}]}, {"text": "Our submissions to the shared task are the results of our best systems.", "labels": [], "entities": []}, {"text": "As each participating team is allowed to submit up to 10 runs, we first run our best sequence labelling, sequence-to-sequence and combined systems from the previous section on the en es test set.", "labels": [], "entities": [{"text": "en es test set", "start_pos": 180, "end_pos": 194, "type": "DATASET", "confidence": 0.7916430085897446}]}, {"text": "After   the es en and fr en tracks using the same network architecture and the same group of features as for en es.", "labels": [], "entities": []}, {"text": "No tuning of (hyper-)parameters is performed for new datasets or languages.", "labels": [], "entities": []}, {"text": "The official results of our submissions for all three language tracks are reported in.", "labels": [], "entities": []}, {"text": "Results on the en es test set are similar to those on the en es development set (see) -no significant drop is observed.", "labels": [], "entities": [{"text": "en es test set", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.6708890572190285}, {"text": "en es development set", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.5537717267870903}]}, {"text": "The combined model produces the best overall performance, and the seqlabel model outperforms the seq2seq model.", "labels": [], "entities": []}, {"text": "In the fr en track, the combined model again yields the highest AUROC and F1 scores, followed by the seq2seq model and the seqlabel model.", "labels": [], "entities": [{"text": "AUROC", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.8958454132080078}, {"text": "F1", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9765776991844177}]}, {"text": "Our es en seq2seq model had not finished training by the shared task submission deadline, therefore, we only submit the es en seqlabel model.", "labels": [], "entities": []}, {"text": "Based on the results for the other two language tracks, we expect our es en results might be further improved by combining a seqlabel model and a seq2seq model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of our sequence labelling models on the en es development set. The results of our best model are  marked in bold.", "labels": [], "entities": [{"text": "en es development set", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.7996428459882736}]}, {"text": " Table 2: Results of our sequence-to-sequence models on the en es development set. The results of our best model  are marked in bold.", "labels": [], "entities": [{"text": "en es development set", "start_pos": 60, "end_pos": 81, "type": "DATASET", "confidence": 0.7882353961467743}]}, {"text": " Table 3: Results of our best models on the en es de- velopment set. The best results are marked in bold.", "labels": [], "entities": [{"text": "en es de- velopment set", "start_pos": 44, "end_pos": 67, "type": "DATASET", "confidence": 0.5145724068085352}]}, {"text": " Table 4: Official results of our submitted systems on the test sets for all three tracks: seqlabel is our best se- quence labelling model, seq2seq is our best sequence-to-sequence model, and combined is the combination of  these two models. For comparison, we also include the baseline results provided by the shared task organisers  and the results from the top-performing systems.", "labels": [], "entities": []}]}