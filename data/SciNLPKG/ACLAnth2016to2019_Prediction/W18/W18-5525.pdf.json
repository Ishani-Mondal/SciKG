{"title": [{"text": "Team SWEEPer: Joint Sentence Extraction and Fact Checking with Pointer Networks", "labels": [], "entities": [{"text": "Joint Sentence Extraction", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.6283276081085205}, {"text": "Fact Checking", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.6403344869613647}]}], "abstractContent": [{"text": "Many tasks such as question answering and reading comprehension rely on information extracted from unreliable sources.", "labels": [], "entities": [{"text": "question answering", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.9127129316329956}, {"text": "reading comprehension", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.9175908267498016}]}, {"text": "These systems would thus benefit from knowing whether a statement from an unreliable source is correct.", "labels": [], "entities": []}, {"text": "We present experiments on the FEVER (Fact Extraction and VERification) task, a shared task that involves selecting sentences from Wikipedia and predicting whether a claim is supported by those sentences, refuted , or there is not enough information.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9983865022659302}, {"text": "Fact Extraction and VERification) task", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.6035979290803274}]}, {"text": "Fact checking is a task that benefits from not only asserting or disputing the veracity of a claim but also finding evidence for that position.", "labels": [], "entities": [{"text": "Fact checking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.954204797744751}]}, {"text": "As these tasks are dependent on each other, an ideal model would consider the veracity of the claim when finding evidence and also find only the evidence that is relevant.", "labels": [], "entities": []}, {"text": "We thus jointly model sentence extraction and verification on the FEVER shared task.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8121103644371033}, {"text": "FEVER", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.9625823497772217}]}, {"text": "Among all participants , we ranked 5th on the blind test set (prior to any additional human evaluation of the evidence).", "labels": [], "entities": []}], "introductionContent": [{"text": "Verifying claims using textual sources is a difficult problem, requiring natural language inference as well as information retrieval if the sources are not provided.", "labels": [], "entities": [{"text": "Verifying claims using textual sources", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8713116765022277}]}, {"text": "The FEVER task () provides a large annotated resource for extraction of sentences from Wikipedia and verification of the extracted evidence against a claim.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9899997115135193}]}, {"text": "A system that extracts and verifies statements in this framework must consist of three components: 1) Retrieving Wikipedia articles.", "labels": [], "entities": [{"text": "Retrieving Wikipedia articles", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.8432538112004598}]}, {"text": "2) Identifying the sentences from Wikipedia that support or refute the claim.", "labels": [], "entities": [{"text": "Identifying the sentences", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.8500282963116964}]}, {"text": "3) Predicting supporting, refuting, or not enough info.", "labels": [], "entities": [{"text": "Predicting", "start_pos": 3, "end_pos": 13, "type": "METRIC", "confidence": 0.953208327293396}]}, {"text": "We combine these components into two stages: 1) identifying relevant documents (Wikipedia ar- * Work completed while at Amazon AI Lab ticles) fora claim and 2) jointly extracting sentences from the top-ranked articles and predicting a relation for whether the claim is supported, refuted, or if there is not enough information in Wikipedia.", "labels": [], "entities": []}, {"text": "We first identify relevant documents by ranking Wikipedia articles according to a model using lexical and syntactic features.", "labels": [], "entities": []}, {"text": "Then, we derive contextual sentence representations for the claim paired with each evidence sentence in the extracted documents.", "labels": [], "entities": []}, {"text": "We use the ESIM module () to create embeddings for each claim/evidence pair and use a pointer network () to recurrently extract only relevant evidence sentences while predicting the relation using the entire set of evidence sentences.", "labels": [], "entities": []}, {"text": "Finally, given these components, which are pre-trained using multi-task learning (, we tune the parameters of the entailment component given the extracted sentences.", "labels": [], "entities": []}, {"text": "Our experiments reveal that jointly training the model provides a significant boost in performance, suggesting that the contextual representations learn information about which sentences are most important for relation prediction as well as information about the type of relationship the evidence has to the claim.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 210, "end_pos": 229, "type": "TASK", "confidence": 0.9557441174983978}]}], "datasetContent": [{"text": "We use Pytorch for our experiments.", "labels": [], "entities": [{"text": "Pytorch", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.947594404220581}]}, {"text": "For the multi-task learning and tuning, we use the Adagrad optimizer with learning rates of 0.01 and 0.001 and gradients clipped to 5 and 2, respectively.", "labels": [], "entities": []}, {"text": "For both experiments, we used a batch size of 16.", "labels": [], "entities": []}, {"text": "We used the paper development set for early stopping.", "labels": [], "entities": [{"text": "paper development set", "start_pos": 12, "end_pos": 33, "type": "DATASET", "confidence": 0.800508956114451}]}, {"text": "We initialized the word embeddings with 300-dimensional GloVe vectors and fixed them during training, using a 200-dimensional projection.", "labels": [], "entities": []}, {"text": "Out-of-vocabulary words were initialized randomly during both training and evaluation.", "labels": [], "entities": []}, {"text": "The evidence as given has no meaningful order but we use the support/refute sequence as provided in the dataset as it may contain annotator bias in terms of importance.", "labels": [], "entities": []}, {"text": "The second dimension of all other parameter matrices was 200.", "labels": [], "entities": []}, {"text": "For the pointer network, we used abeam size of 5 and q = 3 hops.", "labels": [], "entities": []}, {"text": "\u03bb was set to 1.", "labels": [], "entities": []}], "tableCaptions": []}