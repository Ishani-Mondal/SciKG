{"title": [{"text": "Er ... well, it matters, right? On the role of data representations in spoken language dependency parsing", "labels": [], "entities": [{"text": "spoken language dependency parsing", "start_pos": 71, "end_pos": 105, "type": "TASK", "confidence": 0.6837990432977676}]}], "abstractContent": [{"text": "Despite the significant improvement of data-driven dependency parsing systems in recent years, they still achieve a considerably lower performance in parsing spoken language data in comparison to written data.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.6815306395292282}, {"text": "parsing spoken language", "start_pos": 150, "end_pos": 173, "type": "TASK", "confidence": 0.9035077095031738}]}, {"text": "On the example of Spoken Slovenian Treebank, the first spoken data treebank using the UD annotation scheme, we investigate which speech-specific phenomena undermine parsing performance , through a series of training data and treebank modification experiments using two distinct state-of-the-art parsing systems.", "labels": [], "entities": [{"text": "Spoken Slovenian Treebank", "start_pos": 18, "end_pos": 43, "type": "DATASET", "confidence": 0.5675407648086548}]}, {"text": "Our results show that utterance segmentation is the most prominent cause of low parsing performance , both in parsing raw and pre-segmented transcriptions.", "labels": [], "entities": [{"text": "utterance segmentation", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.9093229174613953}, {"text": "parsing raw", "start_pos": 110, "end_pos": 121, "type": "TASK", "confidence": 0.8976924419403076}]}, {"text": "In addition to shorter utterances , both parsers perform better on normalized transcriptions including basic markers of prosody and excluding disfluencies, discourse markers and fillers.", "labels": [], "entities": []}, {"text": "On the other hand, the effects of written training data addition and speech-specific dependency representations largely depend on the parsing system selected.", "labels": [], "entities": [{"text": "written training data addition", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.5940197333693504}]}], "introductionContent": [{"text": "With an exponential growth of spoken language data available online on the one hand and the rapid development of systems and techniques for language understanding on the other, spoken language research is gaining increasing prominence.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.7618052661418915}]}, {"text": "Many syntactically annotated spoken language corpora have been developed in the recent years to benefit the data-driven parsing systems for speech (), including two spoken language treebanks adopting the Universal Dependencies (UD) annotation scheme, aimed at cross-linguistically consistent dependency treebank annotation.", "labels": [], "entities": []}, {"text": "However, in the recent CoNLL 2017 shared task on multilingual parsing from raw text to UD, the results achieved on the Spoken Slovenian Treebank ( -the only spoken treebank among the 81 participating treebanks -were substantially lower than on other treebanks.", "labels": [], "entities": [{"text": "multilingual parsing from raw text", "start_pos": 49, "end_pos": 83, "type": "TASK", "confidence": 0.7913369715213776}, {"text": "Spoken Slovenian Treebank", "start_pos": 119, "end_pos": 144, "type": "DATASET", "confidence": 0.7394684553146362}]}, {"text": "This includes the written Slovenian treebank (, with a best labeled attachment score difference of more than 30 percentage points between the two treebanks by all of the 33 participating systems.", "labels": [], "entities": [{"text": "Slovenian treebank", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.9372348785400391}, {"text": "labeled attachment score difference", "start_pos": 60, "end_pos": 95, "type": "METRIC", "confidence": 0.7621625810861588}]}, {"text": "Given this significant gap in parsing performance between the two modalities, spoken and written language, this paper aims to investigate which speech-specific phenomena influence the poor parsing performance for speech, and to what extent.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.971316933631897}]}, {"text": "Specifically, we focus on questions related to data representation in all aspects of the dependency parsing pipeline, by introducing different types of modifications to spoken language transcripts and speech-specific dependency annotations, as well as to the type of data used for spoken language modelling.", "labels": [], "entities": [{"text": "dependency parsing pipeline", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.8765019575754801}, {"text": "spoken language modelling", "start_pos": 281, "end_pos": 306, "type": "TASK", "confidence": 0.6459965209166209}]}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 addresses the related research on spoken language parsing and Section 3 presents the structure and annotation of the Spoken Slovenian Treebank on which all the experiments were conducted.", "labels": [], "entities": [{"text": "spoken language parsing", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.6655967136224111}, {"text": "Spoken Slovenian Treebank", "start_pos": 127, "end_pos": 152, "type": "DATASET", "confidence": 0.6014285782972971}]}, {"text": "Section 4 presents the parsing systems used in the experiments (4.1) and the series of SST data modifications to narrow the performance gap between written and spoken treebanks for these systems, involving the training data (4.3.1), speech transcriptions (4.3.2) and UD dependency annotations.", "labels": [], "entities": [{"text": "SST", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9530077576637268}]}, {"text": "Results are presented in Section 5, while conclusions and some directions for further work are addressed in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To enable system-independent generalizations, two parsing systems were selected, UDPipe 1.2 ( and Stanford (, covering the two most common parsing approaches, transition-based and graph-based parsing, respectively.", "labels": [], "entities": []}, {"text": "UDPipe 1.2 is a trainable pipeline for sentence segmentation, tokenization, POS tagging, lemmatization and dependency parsing.", "labels": [], "entities": [{"text": "UDPipe 1.2", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8854592740535736}, {"text": "sentence segmentation", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.756665050983429}, {"text": "tokenization", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.9587357640266418}, {"text": "POS tagging", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.8731370270252228}, {"text": "dependency parsing", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7996698617935181}]}, {"text": "It represents an improved version of the UDPipe 1.1 (used as a baseline system in the CONLL-2017 Shared Task () and finished as the 8th best system out of 33 systems participating in the task.", "labels": [], "entities": [{"text": "UDPipe 1.1", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.8531078398227692}, {"text": "CONLL-2017 Shared Task", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.8728641470273336}]}, {"text": "A single-layer bidirectional GRU network together with a case insensitive dictionary and a set of automatically generated suffix rules are used for sentence segmentation and tokenization.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.7587311565876007}]}, {"text": "The part of speech tagging module consists of a guesser, which generates several universal part of speech (XPOS), language-specific part of speech (UPOS), and morphological feature list (FEATS) tag triplets for each word according to its last four characters.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7300971150398254}]}, {"text": "These are given as an input to an averaged perceptron tagger ( to perform the final disambiguation on the generated tags.", "labels": [], "entities": []}, {"text": "Transition-based dependency parser is based on a shallow neural network with one hidden layer and without any recurrent connections, making it one of the fastest parsers in the CONLL-2017 Shared Task.", "labels": [], "entities": [{"text": "Transition-based dependency parser", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6142662366231283}, {"text": "CONLL-2017 Shared Task", "start_pos": 177, "end_pos": 199, "type": "DATASET", "confidence": 0.8010436693827311}]}, {"text": "We used the default parameter configuration often training iterations and a hidden layer of size 200 for training all the models.", "labels": [], "entities": []}, {"text": "Stanford parser is a neural graph-based parser) capable of leveraging word and character based information in order to produce part of speech tags and labeled dependency parses from segmented and tokenized sequences of words.", "labels": [], "entities": [{"text": "labeled dependency parses from segmented and tokenized sequences of words", "start_pos": 151, "end_pos": 224, "type": "TASK", "confidence": 0.7807004272937774}]}, {"text": "Its architecture is based on a deep biaffine neural dependency parser presented by, which uses a multilayer bidirectional LSTM network to produce vector representations for each word.", "labels": [], "entities": []}, {"text": "These representations are used as an input to a stack of biaffine classifiers capable of producing the most probable UD tree for every sentence and the most probable part of speech tag for every word.", "labels": [], "entities": []}, {"text": "The system was ranked first according to all five relevant criteria in the CONLL-2017 Shared Task.", "labels": [], "entities": [{"text": "CONLL-2017 Shared Task", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.8611291448275248}]}, {"text": "Same hyperparameter configuration was used as reported in) with every model trained for 30,000 training steps.", "labels": [], "entities": []}, {"text": "For the parameters values that were not explicitly mentioned in, default values were used.", "labels": [], "entities": []}, {"text": "For both parsers, no additional fine-tuning was performed for any specific data set, in order to minimize the influence of training procedure on the parser's performance for different data preprocessing techniques, especially given that no development data has been released for the small SST treebank.", "labels": [], "entities": [{"text": "SST treebank", "start_pos": 289, "end_pos": 301, "type": "DATASET", "confidence": 0.8040609359741211}]}, {"text": "For evaluation, we used the official CoNLL-ST-2017 evaluation script ( to calculate the standard labeled attachments score (LAS), i.e. the percentage of nodes with correctly assigned reference to parent node, including the label (type) of relation.", "labels": [], "entities": [{"text": "CoNLL-ST-2017 evaluation script", "start_pos": 37, "end_pos": 68, "type": "DATASET", "confidence": 0.8540914456049601}, {"text": "labeled attachments score (LAS)", "start_pos": 97, "end_pos": 128, "type": "METRIC", "confidence": 0.7955687890450159}]}, {"text": "For baseline experiments involving parsing of raw transcriptions (see Section 4.2), for which the number of nodes in gold-standard annotation and in the system output might vary, the F 1 LAS score, marking the harmonic mean of precision an recall LAS scores, was used instead.", "labels": [], "entities": [{"text": "parsing of raw transcriptions", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.8685755431652069}, {"text": "F 1 LAS score", "start_pos": 183, "end_pos": 196, "type": "METRIC", "confidence": 0.9413034170866013}, {"text": "precision", "start_pos": 227, "end_pos": 236, "type": "METRIC", "confidence": 0.884154736995697}, {"text": "recall LAS scores", "start_pos": 240, "end_pos": 257, "type": "METRIC", "confidence": 0.901438037554423}]}], "tableCaptions": [{"text": " Table 1: UDPipe and Stanford sentence segmentation (Sents), part-of-speech tagging (UPOS), unlabelled (UAS)  and labelled attachment (LAS) F 1 scores on the spoken SST and written SSJ Slovenian UD treebanks for parsing  raw text, and for parsing texts with gold-standard tokenization, segmentation and tagging information.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.8267755508422852}, {"text": "Stanford sentence segmentation", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.6998395323753357}, {"text": "part-of-speech tagging (UPOS)", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.7323112845420837}, {"text": "unlabelled (UAS)  and labelled attachment (LAS) F 1", "start_pos": 92, "end_pos": 143, "type": "METRIC", "confidence": 0.7489054650068283}, {"text": "SSJ Slovenian UD treebanks", "start_pos": 181, "end_pos": 207, "type": "DATASET", "confidence": 0.5847237631678581}, {"text": "parsing  raw text", "start_pos": 212, "end_pos": 229, "type": "TASK", "confidence": 0.8797504901885986}]}, {"text": " Table 2: LAS on the Spoken Slovenian Treebank  (sst) for different types of training data, transcrip- tion and annotation modifications. Improvements of  the baseline are marked in bold.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9949636459350586}, {"text": "Slovenian Treebank  (sst)", "start_pos": 28, "end_pos": 53, "type": "DATASET", "confidence": 0.8899916291236878}]}]}