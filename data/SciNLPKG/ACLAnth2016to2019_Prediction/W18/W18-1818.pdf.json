{"title": [{"text": "XNMT: The eXtensible Neural Machine Translation Toolkit", "labels": [], "entities": [{"text": "Neural Machine Translation Toolkit", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.712655819952488}]}], "abstractContent": [{"text": "This paper describes XNMT, the eXtensible Neural Machine Translation toolkit.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.599755048751831}]}, {"text": "XNMT distinguishes itself from other open-source NMT toolkits by its focus on modular code design, with the purpose of enabling fast iteration in research and replicable, reliable results.", "labels": [], "entities": []}, {"text": "In this paper we describe the design of XNMT and its experiment configuration system, and demonstrate its utility on the tasks of machine translation, speech recognition, and multi-tasked machine transla-tion/parsing.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8312214910984039}, {"text": "speech recognition", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.743528813123703}, {"text": "machine transla-tion/parsing", "start_pos": 188, "end_pos": 216, "type": "TASK", "confidence": 0.5399652644991875}]}, {"text": "XNMT is available open-source at https://github.com/neulab/xnmt.", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to the effectiveness and relative ease of implementation, there is now a proliferation of toolkits for neural machine translation, as many as 51 according to the tally by nmt-list.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 107, "end_pos": 133, "type": "TASK", "confidence": 0.6805130441983541}]}, {"text": "The common requirements for such toolkits are speed, memory efficiency, and translation accuracy, which are essential for the use of such systems in practical translation settings.", "labels": [], "entities": [{"text": "speed", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9799096584320068}, {"text": "translation", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9506579041481018}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9444624781608582}]}, {"text": "Many open source toolkits do an excellent job at this to the point where they can be used in production systems (e.g. OpenNMT is used by Systran ().", "labels": [], "entities": []}, {"text": "This paper describes XNMT, the eXtensible Neural Machine Translation toolkit, a toolkit that optimizes not for efficiency, but instead for ease of use in practical research settings.", "labels": [], "entities": [{"text": "Neural Machine Translation toolkit", "start_pos": 42, "end_pos": 76, "type": "TASK", "confidence": 0.6921949163079262}]}, {"text": "In other words, instead of only optimizing time for training or inference, XNMT aims to reduce the time it takes fora researcher to turn their idea into a practical experimental setting, test with a large number of parameters, and produce valid and trustable research results.", "labels": [], "entities": []}, {"text": "Of course, this necessitates a certain level of training efficiency and accuracy, but XNMT also takes into account a number of considerations, such as those below: \u2022 XNMT places a heavy focus on modular code design, making it easy to swap in and out different parts of the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9992019534111023}]}, {"text": "Ideally, implementing research prototypes with XNMT involves only few changes to existing code.", "labels": [], "entities": []}, {"text": "\u2022 XNMT is implemented in Python, the de facto standard in the research community.", "labels": [], "entities": []}, {"text": "\u2022 XNMT uses DyNet ( as its deep learning framework.", "labels": [], "entities": []}, {"text": "DyNet uses dynamic computation graphs, which makes it possible to write code in a very natural way, and benefit from additional flexibility to implement complex networks with dynamic structure, as are often beneficial in natural language processing.", "labels": [], "entities": [{"text": "DyNet", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8728324770927429}]}, {"text": "Further benefits include transparent handling of batching operations, or even removing explicit batch handling and relying on autobatching for speed-up instead.", "labels": [], "entities": []}, {"text": "\u2022 XNMT of course contains standard NMT models, but also includes functionality for optimization using reinforcement learning ( or minimum risk training), flexible multi-task learning, encoders for speech (, and training and testing of retrieval-based models ().", "labels": [], "entities": []}, {"text": "In the remainder of the paper, we provide some concrete examples of the design principles behind XNMT, and a few examples of how it can be used to implement standard models.", "labels": [], "entities": []}], "datasetContent": [{"text": "As demonstrates, XNMT supports the basic functionality for experiments described in \u00a72.1.", "labels": [], "entities": []}, {"text": "In the example, the model specifies the input data structure to be plain text (PlainTextReader), word embedding method to be a standard lookuptable based embedding (SimpleWordEmbedder), encoder to be a bidirectional LSTM (BiLSTMSeqTransducer), attender to be a multi-layer perceptron based attention method (MlpAttender), and the decoder to use a LSTM with a MLP-based softmax (MlpSoftmaxDecoder).", "labels": [], "entities": []}, {"text": "Similarly, in the training: and evaluate: subsections, the training and evaluation parameters are set as well.", "labels": [], "entities": []}, {"text": "XNMT also provides a number of conveniences to support efficient experimentation: Named experiments and overwriting: Experiments are given a name such as mini exp.", "labels": [], "entities": []}, {"text": "{EXP} strings in the configuration file are automatically overwritten by this experiment name, distinguishing between log or model files from different experiments.", "labels": [], "entities": [{"text": "EXP", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.8635119795799255}]}, {"text": "Multiple experiments and sharing of parameters: Multiple experiments can be specified in a single YAML file by defining multiple top-level elements of the YAML file.", "labels": [], "entities": [{"text": "YAML file", "start_pos": 155, "end_pos": 164, "type": "DATASET", "confidence": 0.9062922894954681}]}, {"text": "These multiple experiments can share settings through YAML anchors, where one experiment can inherit the settings from another, only overwriting the relevant settings that needs to be changed.", "labels": [], "entities": []}, {"text": "Saving configurations: For reproducibility, XNMT dumps the whole experiment specification when saving a model.", "labels": [], "entities": []}, {"text": "Thus, experiments can be re-run by simply opening the configuration file associated with any model.", "labels": [], "entities": []}, {"text": "Re-starting training: A common requirement is loading a previously trained model, be it for fine-tuning on different data, tuning decoding parameters, or testing on different data.", "labels": [], "entities": []}, {"text": "XNMT allows this by re-loading the dumped configuration file, overwriting a subset of the settings such as file paths, decoding parameters, or training parameters, and re-running the experiment.", "labels": [], "entities": []}, {"text": "Random parameter search: Often we would like to tune parameter values by trying several different configurations.", "labels": [], "entities": [{"text": "Random parameter search", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6177159349123637}]}, {"text": "Currently XNMT makes it possible to do so by defining a set of parameters to evaluate and then searching over them using random search.", "labels": [], "entities": []}, {"text": "In the future, we may support other strategies such as Bayesian optimization or enumeration.", "labels": [], "entities": [{"text": "Bayesian optimization", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.6853601038455963}]}], "tableCaptions": [{"text": " Table 1: Speech recognition results (WER in %) compared to a similar pyramidal LSTM model  (Zhang et al., 2017) and a highly engineered hybrid HMM system (Rousseau et al., 2014).", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7429527044296265}, {"text": "WER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9974305033683777}]}]}