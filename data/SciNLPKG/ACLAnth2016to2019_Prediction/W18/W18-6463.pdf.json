{"title": [{"text": "Sheffield Submissions for the WMT18 Quality Estimation Shared Task", "labels": [], "entities": [{"text": "WMT18 Quality Estimation Shared", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.7155049964785576}]}], "abstractContent": [{"text": "In this paper we present the University of Sheffield submissions for the WMT18 Quality Estimation shared task.", "labels": [], "entities": [{"text": "WMT18 Quality Estimation shared task", "start_pos": 73, "end_pos": 109, "type": "TASK", "confidence": 0.6556677341461181}]}, {"text": "We discuss our submissions to all four sub-tasks, where ours is the only team to participate in all language pairs and variations (37 combinations).", "labels": [], "entities": []}, {"text": "Our systems show competitive results and outper-form the baseline in nearly all cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality Estimation (QE) predicts the quality of Machine Translation (MT) when automatic evaluation or human assessment is not possible (typically at system run-time).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.8560447216033935}]}, {"text": "QE is mainly addressed as a supervised Machine Learning problem with QE models trained using labelled data.", "labels": [], "entities": []}, {"text": "These labels differ for different tasks, for example, binary labels for fine-grained predictions (e.g. OK/BAD for words or phrases) and continuous measurements of quality for coarse-grained levels (e.g. HTER) for sentences).", "labels": [], "entities": [{"text": "OK/BAD", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.5109746356805166}]}, {"text": "For this year's shared task, post-edited (PE) and manually annotated data were provided.", "labels": [], "entities": []}, {"text": "They cover four levels of predictions: sentence-level (task 1), word-level (task 2), phrase-level (task 3) and document-level (task 4), over five language pairs: English into German, Latvian, Czech and French, as well as German-English.", "labels": [], "entities": []}, {"text": "For the first time, these data contain translations produced by neural MT (NMT) systems.", "labels": [], "entities": []}, {"text": "Such translations are known to be more fluent but less adequate.", "labels": [], "entities": []}, {"text": "For tasks 2 and 3, this year's edition introduces anew task variant of predicting missing words in the translations.", "labels": [], "entities": [{"text": "predicting missing words in the translations", "start_pos": 71, "end_pos": 115, "type": "TASK", "confidence": 0.8472962379455566}]}, {"text": "Thus two additional prediction types are required: (i) binary labels for gaps in the translation to indicate whether one or more tokens are missing from a certain position, and (ii) binary labels for words in source sentences to indicate which of these words lead to incorrect words in the translations.", "labels": [], "entities": []}, {"text": "We participated with two different systems, both available in the DeepQuest 1 toolkit (: \u2022 SHEF-PT: an in-house re-implementation of the POSTECH system (, and \u2022 SHEF-bRNN: a bidirectional recurrent neural network (bRNN) system.", "labels": [], "entities": []}, {"text": "We participated in all sub-tasks and submitted a total of 74 predictions (37 per system).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Evaluation of our systems for task 1 on the test set. We show scores of Pearson's r correlation, MAE and  Spearman's \u03c1 correlation.", "labels": [], "entities": [{"text": "Pearson's r correlation", "start_pos": 82, "end_pos": 105, "type": "METRIC", "confidence": 0.9571409374475479}, {"text": "MAE", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9946936964988708}, {"text": "Spearman's \u03c1 correlation", "start_pos": 116, "end_pos": 140, "type": "METRIC", "confidence": 0.6451020166277885}]}, {"text": " Table 2: Evaluation of our systems for task 2 on the test set. We show scores of F1-MULT, F1 for the OK class  and F1 for the BAD class.", "labels": [], "entities": [{"text": "F1-MULT", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9992092847824097}, {"text": "F1", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9971365928649902}, {"text": "F1", "start_pos": 116, "end_pos": 118, "type": "METRIC", "confidence": 0.9948068261146545}]}, {"text": " Table 4: Evaluation of our systems for task 3a on the test set. We show scores of F1-MULT, F1 for the OK class  and F1 for the BAD class.", "labels": [], "entities": [{"text": "F1-MULT", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9991439580917358}, {"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9963996410369873}, {"text": "F1", "start_pos": 117, "end_pos": 119, "type": "METRIC", "confidence": 0.9940940737724304}]}, {"text": " Table 5: Evaluation of our systems for task 3b on  the test set. We show scores of F1-MULT, F1 for  the OK class, F1 for the BAD class and F1 for the  BAD word order class.", "labels": [], "entities": [{"text": "F1-MULT", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9990830421447754}, {"text": "F1", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9900498390197754}, {"text": "F1", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.9834282994270325}, {"text": "F1", "start_pos": 140, "end_pos": 142, "type": "METRIC", "confidence": 0.9931665062904358}, {"text": "BAD word order class", "start_pos": 152, "end_pos": 172, "type": "TASK", "confidence": 0.536051794886589}]}, {"text": " Table 6: Evaluation of our systems for task 4 on the  test set. We show scores of Pearson's r correlation and  MAE.", "labels": [], "entities": [{"text": "Pearson's r correlation", "start_pos": 83, "end_pos": 106, "type": "METRIC", "confidence": 0.8788762986660004}, {"text": "MAE", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9948521256446838}]}]}