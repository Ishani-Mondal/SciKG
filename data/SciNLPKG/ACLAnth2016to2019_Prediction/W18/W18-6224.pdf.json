{"title": [{"text": "UTFPR at IEST 2018: Exploring Character-to-Word Composition for Emotion Analysis", "labels": [], "entities": [{"text": "UTFPR at IEST 2018", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.746857538819313}, {"text": "Emotion Analysis", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.8192046582698822}]}], "abstractContent": [{"text": "We introduce the UTFPR system for the Implicit Emotions Shared Task of 2018: A compositional character-to-word recurrent neural network that does not exploit heavy and/or hard-to-obtain resources.", "labels": [], "entities": [{"text": "Implicit Emotions Shared Task", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.6647840812802315}]}, {"text": "We find that our approach can outperform multiple baselines, and offers an elegant and effective solution to the problem of ortho-graphic variance in tweets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Emotion analysis has become one of the most prominent tasks in Natural Language Processing (NLP) in recent years.", "labels": [], "entities": [{"text": "Emotion analysis", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9402376711368561}, {"text": "Natural Language Processing (NLP)", "start_pos": 63, "end_pos": 96, "type": "TASK", "confidence": 0.7625439564387003}]}, {"text": "It can be framed as either a regression task, where one wants to gauge the degree of some emotion, such as how optimistic a certain opinion is with respect to a given matter, or a classification task, where one wants to decide on which type of emotion is being conveyed, such as happiness, fear, anger, etc.", "labels": [], "entities": []}, {"text": "The task is particularly interesting for industry applications, since it can potentially allow for institutions to automatically assess the public opinion on things like initiatives, products, etc.", "labels": [], "entities": []}, {"text": "The task can also be applied in many interesting natural language domains.", "labels": [], "entities": []}, {"text": "For instance, one can try to determine whether a certain restaurant review posted in a major news outlet favors the establishment, or whether a certain tweet about a celebrity or institution conveys support or disdain.", "labels": [], "entities": []}, {"text": "The type of target domain can greatly influence how an emotion analysis system is structured.", "labels": [], "entities": [{"text": "emotion analysis", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.735442042350769}]}, {"text": "If the targets are formally written and well revised articles from newspapers and magazines, then one can expect to find only orthographically correct words and appropriately structured sentences in the input.", "labels": [], "entities": []}, {"text": "The authors of tweets, on the other hand, often use many distinct orthographic variants of the same word (ex: you, u, youu), tend to have less regard for form, and use non-textual symbols to express meaning, such as emojis.", "labels": [], "entities": []}, {"text": "Also, articles tend to be much longer than tweets, which have a size limit of just a few hundred characters.", "labels": [], "entities": []}, {"text": "Systems for the later type of domain must address a lot of challenges that systems for the former do not have to, which can compel them to be much more complex.", "labels": [], "entities": []}, {"text": "The SeerNet system (), one of the best performing systems of the SemEval 2018 shared task on affect in tweets (, is a great example of that.", "labels": [], "entities": [{"text": "SemEval 2018 shared task on affect in tweets", "start_pos": 65, "end_pos": 109, "type": "TASK", "confidence": 0.723128542304039}]}, {"text": "In this shared task, participants were asked to create both regression (for emotion intensity) and classification (for emotion decision) systems for emotion analysis in English, Arabic, and Spanish.", "labels": [], "entities": [{"text": "emotion analysis", "start_pos": 149, "end_pos": 165, "type": "TASK", "confidence": 0.7155623435974121}]}, {"text": "In order to overcome the challenge of analyzing tweets, the SeerNet system resorts to a wide range of specialized resources, such as special tokenizers and embedding models for tweets, emoji analyzers, and even off-the-shelf systems trained on large amounts of curated data.", "labels": [], "entities": [{"text": "emoji analyzers", "start_pos": 185, "end_pos": 200, "type": "TASK", "confidence": 0.7292371094226837}]}, {"text": "Though undoubtedly effective, the SeerNet has a very complex architecture that would be difficult to replicate, specially for under-resourced languages.", "labels": [], "entities": []}, {"text": "In an effort to offer a simpler solution to emotion analysis in tweets, we present the UTFPR system submitted to Implicit Emotions Shared Task (IEST) of 2018 (.", "labels": [], "entities": [{"text": "emotion analysis", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7170024812221527}, {"text": "Implicit Emotions Shared Task (IEST) of", "start_pos": 113, "end_pos": 152, "type": "TASK", "confidence": 0.6383401639759541}]}, {"text": "Ours is a character-to-word recurrent neural network architecture that offers an elegant solution to orthographic variance within tweets, and does not rely on any resources other than the input provided by the shared task organizers.", "labels": [], "entities": []}, {"text": "We describe our approach in what follows.", "labels": [], "entities": []}], "datasetContent": [{"text": "Before conceiving the final version of the UTFPR system, we conducted a preliminary experiment with baseline classification models in order to test some model design options, and hence guide the creation of the UTFPR approach.", "labels": [], "entities": []}, {"text": "More specifically, we assessed two design options with respect to input: \u2022 Structure: Since each instance contains an omitted target emotion word, we tested whether it is more productive to address the entire tweet as a bag of words, or to individually model the words to the left and right of the target.", "labels": [], "entities": []}, {"text": "\u2022 Enhancement: We also tested whether or not it is helpful to complement the training set with data gathered in unsupervised fashion.", "labels": [], "entities": []}, {"text": "For our experiments, we configure the UTFPR system as follows: \u2022 Character embedding size: 25 \u2022 RNN layer type: Gated Recurrent Units (GRU) \u2022 RNN layer depth: 2 \u2022 RNN layer size: 50 \u2022 Dropout proportion: 50% \u2022 Loss function: Cross-entropy \u2022 Framework used: PyTorch 2 As mentioned in section 2, we submited the UTFPR system to the IEST 2018 shared task.", "labels": [], "entities": [{"text": "IEST 2018 shared task", "start_pos": 330, "end_pos": 351, "type": "DATASET", "confidence": 0.8435326218605042}]}, {"text": "We trained the UTFPR system over the entire training set provided by the organizers, and validated it over the trial set.", "labels": [], "entities": []}, {"text": "Our final submission was the model resulting from iteration with the lowest cross-entropy error on the trial set.", "labels": [], "entities": []}, {"text": "In order to offer some points of comparison and highlight the importance of some design decisions made when creating UTFPR, we trained two other variants of UTFPR: \u2022 UTFPR-C: A version of UTFPR without the character-to-word layers.", "labels": [], "entities": []}, {"text": "Instead, it uses as input word embeddings extracted from the word embeddings model described in section 3.", "labels": [], "entities": []}, {"text": "\u2022 UTFPR-CD: A version of UTFPR-C trained without dropout.", "labels": [], "entities": [{"text": "UTFPR-CD", "start_pos": 2, "end_pos": 10, "type": "DATASET", "confidence": 0.9139460325241089}]}, {"text": "Due to the limited amount of computing resources available to us, we were notable to train anymore variants of UTFPR.", "labels": [], "entities": []}, {"text": "We also include in our performance comparison all the baseline models described in section 3, the baselines provided by the IEST 2018 organizers, and the 5 systems with the highest macro F-scores in the shared task.", "labels": [], "entities": [{"text": "IEST 2018 organizers", "start_pos": 124, "end_pos": 144, "type": "DATASET", "confidence": 0.9044573704401652}, {"text": "F-scores", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.8344888091087341}]}, {"text": "showcases the micro and macro Precision, Recall, and F-scores of our UTFPR variants, as well the IEST 2018 baselines and top 5 systems.", "labels": [], "entities": [{"text": "Precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9842823147773743}, {"text": "Recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9766070246696472}, {"text": "F-scores", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.995487630367279}, {"text": "IEST 2018 baselines", "start_pos": 97, "end_pos": 116, "type": "DATASET", "confidence": 0.8646982510884603}]}, {"text": "Although our approach did not manage to reach the top of the leaderboards, the results do highlight the impact of some design decisions made when creating the final UTFPR system.", "labels": [], "entities": []}, {"text": "As it can be noticed, incorporating dropout and adding a character-toword encoder to our model slightly increases its performance.", "labels": [], "entities": []}, {"text": "While the complete UTFPR system 179", "labels": [], "entities": [{"text": "UTFPR", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.5673287510871887}]}], "tableCaptions": [{"text": " Table 1: Preliminary experiment results. Each cell represents the macro F-score obtained by a given model.", "labels": [], "entities": [{"text": "F-score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.8683949112892151}]}, {"text": " Table 3: Macro F-scores obtained by our baseline models on the official IEST 2018 test set.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9469245672225952}, {"text": "IEST 2018 test set", "start_pos": 73, "end_pos": 91, "type": "DATASET", "confidence": 0.9357054084539413}]}, {"text": " Table 4: Confusion matrix of the UTFPR system. Lines represent reference labels and columns represent predictions. The last  column and line feature the occurrence proportion of each emotion in the reference and predicted label set, respectively.", "labels": [], "entities": []}, {"text": " Table 5: Official micro and macro scores obtained by the UTFPR systems on the jammed test set. Bold-case scores showcase  the highest scores obtained by the UTFPR systems.", "labels": [], "entities": [{"text": "UTFPR", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.8629283905029297}, {"text": "jammed test set", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.7742111881573995}, {"text": "UTFPR", "start_pos": 158, "end_pos": 163, "type": "DATASET", "confidence": 0.8951647877693176}]}]}