{"title": [{"text": "RICOH at SemEval-2016 Task 1: IR-based Semantic Textual Similarity Estimation", "labels": [], "entities": [{"text": "IR-based Semantic Textual Similarity Estimation", "start_pos": 30, "end_pos": 77, "type": "TASK", "confidence": 0.8382331252098083}]}], "abstractContent": [{"text": "This paper describes our IR (Information Retrieval) based method for SemEval 2016 task 1, Semantic Textual Similarity (STS).", "labels": [], "entities": [{"text": "IR (Information Retrieval)", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.6196672022342682}, {"text": "SemEval 2016 task 1", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8899862468242645}, {"text": "Semantic Textual Similarity (STS)", "start_pos": 90, "end_pos": 123, "type": "TASK", "confidence": 0.7400083541870117}]}, {"text": "The main feature of our approach is to extend a conventional IR-based scheme by incorporating word alignment information.", "labels": [], "entities": [{"text": "word alignment information", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.8041438261667887}]}, {"text": "This enables us to develop a more fine-grained similarity measurement.", "labels": [], "entities": []}, {"text": "In the evaluation results, we have seen that the proposed method improves upon a conventional IR-based method on average.", "labels": [], "entities": []}, {"text": "In addition, one of our submissions achieved the best performance for the \"poste-diting\" data set.", "labels": [], "entities": [{"text": "poste-diting\" data set", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.7590853199362755}]}], "introductionContent": [{"text": "Given two sentences, Semantic Textual Similarity (STS) measures their degree of semantic equivalence (.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.6375987678766251}]}, {"text": "This fundamental functionality can be used for many applications such as text search, classification and clustering.", "labels": [], "entities": [{"text": "text search", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.8321252167224884}]}, {"text": "This paper describes our monolingual (English) STS system which participated in SemEval 2016 task 1.", "labels": [], "entities": [{"text": "SemEval 2016 task 1", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.9072444289922714}]}, {"text": "Our objective is to improve a conventional IR (Information Retrieval) based method for the STS task.", "labels": [], "entities": [{"text": "IR (Information Retrieval)", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.6194970667362213}, {"text": "STS task", "start_pos": 91, "end_pos": 99, "type": "TASK", "confidence": 0.9288471043109894}]}, {"text": "In general, an IR-based method estimates semantic similarity between given sentences using similarity between document search results which are obtained using each sentence as a search query.", "labels": [], "entities": []}, {"text": "This scheme allows us to utilize a large document database for handling diverse semantic phenomena.", "labels": [], "entities": []}, {"text": "However, in our preliminary experiments using past SemEval test data, a conventional IR-based method was not so effective.", "labels": [], "entities": [{"text": "SemEval test data", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.7958006064097086}]}, {"text": "In failure analysis of the method, we found the following: \u2022 It is not sufficient only to measure the commonality among sentences.", "labels": [], "entities": []}, {"text": "\u2022 It is necessary to measure the importance of the identified commonality in each sentence.", "labels": [], "entities": []}, {"text": "Based on these findings, we propose anew IR-based method for STS.", "labels": [], "entities": [{"text": "STS", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.987551748752594}]}, {"text": "In this method, word alignment techniques are applied to refine the assessment of commonality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.801624983549118}]}, {"text": "The importance of the commonality in each sentence is measured using IR techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "We submitted three variants of our system to the shared task evaluation.", "labels": [], "entities": []}, {"text": "The three systems differed in how they used Wikipedia redirect relationships (explained in section 4.4) to aid in word alignment for each word win input sentences.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.7038014084100723}]}, {"text": "RUN-b : Attempts to match w with the targets of the redirect relationships.", "labels": [], "entities": [{"text": "RUN-b", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9782424569129944}]}, {"text": "If no matches are found, tries to match w with the titles of the redirection sources.", "labels": [], "entities": []}, {"text": "RUN-s : Attempts to match w with only the titles of the redirection sources.", "labels": [], "entities": [{"text": "RUN-s", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9055384993553162}]}, {"text": "RUN-n : Does not use Wikipedia redirection relationships.", "labels": [], "entities": [{"text": "RUN-n", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.4865543246269226}]}, {"text": "The evaluation results (Pearson correlation with the gold standard data) of our three submitted runs are shown in.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 24, "end_pos": 43, "type": "METRIC", "confidence": 0.9601488411426544}, {"text": "gold standard data", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.7452123562494913}]}, {"text": "As a baseline, we also include the performance of our system when configured to operate as a conventional IR-based method.", "labels": [], "entities": []}, {"text": "Specifically, the baseline system uses only the features F3, F4 and F5 (see section 4.9).", "labels": [], "entities": []}, {"text": "As with Run-n, the baseline does not use the Wikipedia redirect relationships.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. As a baseline, we also include the  performance of our system when configured to oper- ate as a conventional IR-based method. Specifically,  the baseline system uses only the features F3, F4 and  F5 (see section 4.9). As with Run-n, the baseline  does not use the Wikipedia redirect relationships.", "labels": [], "entities": []}, {"text": " Table 1: Evaluation Results on SemEval 2016 Task 1", "labels": [], "entities": [{"text": "SemEval 2016 Task", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.8445143699645996}]}]}