{"title": [{"text": "Sense Embedding Learning for Word Sense Induction", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.6450415154298147}]}], "abstractContent": [{"text": "Conventional word sense induction (WSI) methods usually represent each instance with discrete linguistic features or co-occurrence features, and train a model for each polysemous word individually.", "labels": [], "entities": [{"text": "word sense induction (WSI)", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.8325987160205841}]}, {"text": "In this work, we propose to learn sense embeddings for the WSI task.", "labels": [], "entities": [{"text": "WSI task", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.8936152756214142}]}, {"text": "In the training stage, our method induces several sense centroids (embedding) for each polysemous word.", "labels": [], "entities": []}, {"text": "In the testing stage, our method represents each instance as a contextual vector, and induces its sense by finding the nearest sense centroid in the embedding space.", "labels": [], "entities": []}, {"text": "The advantages of our method are (1) distributed sense vectors are taken as the knowledge representations which are trained discrimi-natively, and usually have better performance than traditional count-based distri-butional models, and (2) a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutli-task learning framework.", "labels": [], "entities": []}, {"text": "Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods.", "labels": [], "entities": [{"text": "SemEval-2010 WSI dataset", "start_pos": 13, "end_pos": 37, "type": "DATASET", "confidence": 0.7800620396931967}]}, {"text": "We further verify the two advantages by comparing with carefully designed baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense induction (WSI) is the task of automatically finding sense clusters for polysemous words.", "labels": [], "entities": [{"text": "Word sense induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8236549297968546}]}, {"text": "In contrast, word sense disambiguation (WSD) assumes there exists an already-known sense inventory, and the sense of a word type is disambiguated according to the sense inventory.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.8059680213530859}]}, {"text": "Therefore, clustering methods are generally applied in WSI tasks, while classification methods are utilized in WSD tasks.", "labels": [], "entities": [{"text": "WSI tasks", "start_pos": 55, "end_pos": 64, "type": "TASK", "confidence": 0.8639914393424988}, {"text": "WSD tasks", "start_pos": 111, "end_pos": 120, "type": "TASK", "confidence": 0.7071478366851807}]}, {"text": "WSI has been successfully applied to many NLP tasks such as machine translation), information retrieval ( and novel sense detection (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.827770471572876}, {"text": "information retrieval", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.8872492015361786}, {"text": "novel sense detection", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.6301383078098297}]}, {"text": "However, existing methods usually represent each instance with discrete hand-crafted features; Van de Cruys and), which are designed manually and require linguistic knowledge.", "labels": [], "entities": []}, {"text": "Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words.", "labels": [], "entities": []}, {"text": "There is a great advance in recent distributed semantics, such as word embedding) and sense embedding).", "labels": [], "entities": []}, {"text": "Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks.", "labels": [], "entities": []}, {"text": "In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task.", "labels": [], "entities": [{"text": "WSI task", "start_pos": 36, "end_pos": 44, "type": "TASK", "confidence": 0.8005906939506531}, {"text": "WSI task", "start_pos": 189, "end_pos": 197, "type": "TASK", "confidence": 0.8792808353900909}]}, {"text": "Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage.", "labels": [], "entities": []}, {"text": "In testing stage, our method represents each instance as a contextual vector, and induces its sense by finding the nearest sense centroid in the embedding space.", "labels": [], "entities": []}, {"text": "Comparing with existing methods, our method has two advantages: (1) distributed sense embeddings are taken as the knowledge representations which are trained discriminatively, and usually have better performance than traditional count-based dis-tributional models (), and (2) a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutlitask learning framework.", "labels": [], "entities": []}, {"text": "Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods.", "labels": [], "entities": [{"text": "SemEval-2010 WSI dataset", "start_pos": 13, "end_pos": 37, "type": "DATASET", "confidence": 0.7800620396931967}]}], "datasetContent": [{"text": "We evaluate our methods on the test set of the SemEval-2010 WSI task ( . It contains 8,915 instances for 100 target words (50 nouns and 50 verbs) which mostly come from news domain.", "labels": [], "entities": [{"text": "SemEval-2010 WSI task", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.486234317223231}]}, {"text": "We choose the April 2010 snapshot of Wikipedia ( as our training set, as it is freely available and domain general.", "labels": [], "entities": [{"text": "April 2010 snapshot of Wikipedia", "start_pos": 14, "end_pos": 46, "type": "DATASET", "confidence": 0.7329073309898376}]}, {"text": "It contains around 2 million documents and 990 million tokens.", "labels": [], "entities": []}, {"text": "We train and test our models and the baselines according to the above data setting, and compare with reported performance on the same test set from previous papers.", "labels": [], "entities": []}, {"text": "For our sense embedding method, we build two systems: SE-WSI-fix which adopts Multi-Sense Skip-gram (MSSG) model) and assigns 3 senses for each word type, and SE-WSI-CRP (Li and Jurafsky, 2015) which dynamically decides the number of senses using a Chinese restaurant process.", "labels": [], "entities": [{"text": "SE-WSI-fix", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.889022171497345}]}, {"text": "For SE-WSI-fix, we learn sense embeddings for the top 6K frequent words in the training set.", "labels": [], "entities": [{"text": "SE-WSI-fix", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.6255380511283875}]}, {"text": "For SE-WSI-CRP, we first learn word embeddings with word2vec 1 , then use them as pre-trained vectors to learn sense embeddings.", "labels": [], "entities": []}, {"text": "All training is under default parameter settings, and all word and sense embeddings are fixed at 300 dimensions.", "labels": [], "entities": []}, {"text": "For fair comparison, we create SE-WSI-fix-cmp by training the MSSG model on the training data of the SemEval-2010 WSI task with the same setting of SE-WSI-fix.", "labels": [], "entities": [{"text": "SemEval-2010 WSI task", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.5212680796782175}]}, {"text": "We also design baselines to verify the two advantages of our sense embedding methods.", "labels": [], "entities": []}, {"text": "One (CRP-PPMI) uses the same CRP algorithm as SE-WSI-CRP, but with Positive PMI vectors as pretrained vectors.", "labels": [], "entities": []}, {"text": "The other (WE-Kmeans) uses the vectors learned by SE-WSI-fix, but separately clusters all the context vectors into 3 groups for each target word with kmeans.", "labels": [], "entities": []}, {"text": "We compute a context vector by averaging the vectors of all selected words in the context 2 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Result on SemEval-2010 WSI task. 80-20 SR is the supervised recall of 80-20 split supervised  evaluation. FS is the F-Score of 80-20 split supervised evaluation. #CI is the average number of clusters  (senses)", "labels": [], "entities": [{"text": "SemEval-2010 WSI task", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.4870640238126119}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.8898396492004395}, {"text": "FS", "start_pos": 116, "end_pos": 118, "type": "METRIC", "confidence": 0.9964841604232788}, {"text": "F-Score", "start_pos": 126, "end_pos": 133, "type": "METRIC", "confidence": 0.9851137399673462}]}]}