{"title": [{"text": "UH-PRHLT at SemEval-2016 Task 3: Combining Lexical and Semantic-based Features for Community Question Answering", "labels": [], "entities": [{"text": "UH-PRHLT", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9538905620574951}, {"text": "Community Question Answering", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.6074030299981436}]}], "abstractContent": [{"text": "In this work we describe the system built for the three English subtasks of the Se-mEval 2016 Task 3 by the Department of Computer Science of the University of Houston (UH) and the Pattern Recognition and Human Language Technology (PRHLT) research center-Universitat Polit\u00e8cnica deVa\u00ec encia: UH-PRHLT.", "labels": [], "entities": [{"text": "Pattern Recognition and Human Language Technology (PRHLT)", "start_pos": 181, "end_pos": 238, "type": "TASK", "confidence": 0.7896287375026279}]}, {"text": "Our system represents instances by using both lexical and semantic-based similarity measures between text pairs.", "labels": [], "entities": []}, {"text": "Our semantic features include the use of distributed representations of words, knowledge graphs generated with the BabelNet multilingual semantic network, and the FrameNet lexical database.", "labels": [], "entities": [{"text": "FrameNet lexical database", "start_pos": 163, "end_pos": 188, "type": "DATASET", "confidence": 0.764182428518931}]}, {"text": "Experimental results outper-form the random and Google search engine baselines in the three English sub-tasks.", "labels": [], "entities": []}, {"text": "Our approach obtained the highest results of subtask B compared to the other task participants.", "labels": [], "entities": []}], "introductionContent": [{"text": "The key role that the Internet plays today for our society benefited the dawn of thousands of new Web social activities.", "labels": [], "entities": []}, {"text": "Among those, forums emerged with special relevance following the paradigm of the Community Question Answering (CQA).", "labels": [], "entities": [{"text": "Community Question Answering (CQA)", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.7485469281673431}]}, {"text": "These type of social networks allow people to post a question to other users of that community.", "labels": [], "entities": []}, {"text": "The usage is simple, without much restrictions, and infrequently moderated.", "labels": [], "entities": []}, {"text": "The popularity of CQA is a strong indicator that users receive some good and valuable answers.", "labels": [], "entities": []}, {"text": "However, there are several issues related to that type of community.", "labels": [], "entities": []}, {"text": "First is the large amount of answers received that makes it difficult and time-consuming for users to search and distinguish the good ones.", "labels": [], "entities": []}, {"text": "This is exacerbated with the amount of noise that these questions contain.", "labels": [], "entities": []}, {"text": "It is not uncommon to have wrong or misguiding answers that produce more unrelated answers, discussions and sub-threads.", "labels": [], "entities": []}, {"text": "Finally, there is a lot of redundancy, questions maybe repeated or closely related to previously asked questions.", "labels": [], "entities": []}, {"text": "Details of the SemEval 2016 Task 3 on CQA can be found in the overview paper.", "labels": [], "entities": [{"text": "SemEval 2016 Task 3", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8393106311559677}]}, {"text": "In this work we evaluate the three English-related Task 3 subtasks on CQA.", "labels": [], "entities": [{"text": "CQA", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.9572802782058716}]}, {"text": "We first represent each instance to rank -question versus (vs.) comments, question vs. related questions, or question vs. comments of related questions -with a set of similarities computed at two different levels: lexical and semantic.", "labels": [], "entities": []}, {"text": "This representation allows us to estimate the relatedness between text pairs in terms of what is explicitly stated and what it means.", "labels": [], "entities": []}, {"text": "Our lexical similarities employ representations such as word and character n-grams, and bag-of-words (BOW).", "labels": [], "entities": [{"text": "BOW", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.8480691313743591}]}, {"text": "The semantic similarities include the use of distributed word bidirectional alignments, distributed representations of text, knowledge graphs, and frames from the FrameNet lexical database (.", "labels": [], "entities": [{"text": "FrameNet lexical database", "start_pos": 163, "end_pos": 188, "type": "DATASET", "confidence": 0.8976808587710062}]}, {"text": "This type of dual representations have been successfully employed for question answering by the highest performing system in the previous edition of this Se-mEval task).", "labels": [], "entities": [{"text": "question answering", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.9067606627941132}]}, {"text": "Other Natural Language Processing (NLP) tasks such as cross-language document retrieval and categorization also benefited from similar representations.", "labels": [], "entities": [{"text": "cross-language document retrieval", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.6801579693953196}]}, {"text": "In this task, if the question or comment includes multiple text fields, e.g. body and subject, similarities are estimated using all possible combinations (see Section 3.2).", "labels": [], "entities": []}, {"text": "Finally, the ranking of instances is performed using a state-of-the-art machine-learned ranking algorithm: SVM rank .", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents the evaluation of the SemEval 2016 Task 3 on CQA.", "labels": [], "entities": [{"text": "SemEval 2016 Task 3", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8606162071228027}, {"text": "CQA", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8565158843994141}]}, {"text": "Details about this task, the datasets, and the three subtasks can be found in the task overview (.", "labels": [], "entities": []}, {"text": "Note that for our system we did not use data from SemEval 2015 CQA as we did not observe gains in performance.", "labels": [], "entities": [{"text": "SemEval 2015 CQA", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.7963738838831583}]}, {"text": "We compared the results of our approach with those provided by the random baseline and the Google search engine when ranking the questions and comments.", "labels": [], "entities": []}, {"text": "The official measure of the task is the Mean Average Precision (MAP), but we included also two alternative ranking measures: Average Recall (AvgRec) and Mean Reciprocal Rank (MRR).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 40, "end_pos": 68, "type": "METRIC", "confidence": 0.9681718746821085}, {"text": "Average Recall (AvgRec)", "start_pos": 125, "end_pos": 148, "type": "METRIC", "confidence": 0.9620429873466492}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 153, "end_pos": 179, "type": "METRIC", "confidence": 0.9422635634740194}]}, {"text": "In addition, we included four classification measures: Accuracy (acc.), Precision (P), Recall (R), and F1-measure (F1).", "labels": [], "entities": [{"text": "Accuracy (acc.)", "start_pos": 55, "end_pos": 70, "type": "METRIC", "confidence": 0.7994027882814407}, {"text": "Precision (P)", "start_pos": 72, "end_pos": 85, "type": "METRIC", "confidence": 0.9407714903354645}, {"text": "Recall (R)", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.949482187628746}, {"text": "F1-measure (F1)", "start_pos": 103, "end_pos": 118, "type": "METRIC", "confidence": 0.8789792060852051}]}], "tableCaptions": [{"text": " Table 1: Results of Subtask A: English Question-Comment Similarity. (a) Baselines; (b) proposed approach.", "labels": [], "entities": [{"text": "English Question-Comment Similarity", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.5943286120891571}]}, {"text": " Table 2: Results of Subtask B: English Question-Question Similarity. (a) Baselines; (b) proposed approach.", "labels": [], "entities": [{"text": "English Question-Question Similarity", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.5655765732129415}]}, {"text": " Table 3: Results of Subtask C: English Question-External Comment Similarity. (a) Baselines; (b) proposed  approach.", "labels": [], "entities": [{"text": "English Question-External Comment Similarity", "start_pos": 32, "end_pos": 76, "type": "TASK", "confidence": 0.5997066646814346}]}]}