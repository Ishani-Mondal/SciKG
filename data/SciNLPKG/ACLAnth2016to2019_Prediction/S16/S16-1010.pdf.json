{"title": [{"text": "TwiSE at SemEval-2016 Task 4: Twitter Sentiment Classification", "labels": [], "entities": [{"text": "SemEval-2016 Task 4", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7870736320813497}, {"text": "Twitter Sentiment Classification", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.7248514493306478}]}], "abstractContent": [{"text": "This paper describes the participation of the team \"TwiSE\" in the SemEval 2016 challenge.", "labels": [], "entities": [{"text": "SemEval 2016 challenge", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.8280652562777201}]}, {"text": "Specifically, we participated in Task 4, namely \"Sentiment Analysis in Twitter\" for which we implemented sentiment classification systems for subtasks A, B, C and D.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter\"", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.8874695897102356}, {"text": "sentiment classification", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.8449129462242126}]}, {"text": "Our approach consists of two steps.", "labels": [], "entities": []}, {"text": "In the first step, we generate and validate diverse feature sets for twitter sentiment evaluation, inspired by the work of participants of previous editions of such challenges.", "labels": [], "entities": [{"text": "twitter sentiment evaluation", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.817000945409139}]}, {"text": "In the second step, we focus on the optimization of the evaluation measures of the different subtasks.", "labels": [], "entities": []}, {"text": "To this end, we examine different learning strategies by validating them on the data provided by the task organisers.", "labels": [], "entities": []}, {"text": "For our final submissions we used an ensemble learning approach (stacked generalization) for Subtask A and single linear models for the rest of the subtasks.", "labels": [], "entities": []}, {"text": "In the official leaderboard we were ranked 9/35, 8/19, 1/11 and 2/14 for subtasks A, B, C and D respectively.", "labels": [], "entities": []}, {"text": "The code can be found at https://github.com/ balikasg/SemEval2016-Twitter_ Sentiment_Evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "During the last decade, short-text communication forms, such as Twitter microblogging, have been widely adopted and have become ubiquitous.", "labels": [], "entities": []}, {"text": "Using such forms of communication, users share a variety of information.", "labels": [], "entities": []}, {"text": "However, information concerning one's sentiment on the world around her has attracted a lot of research interest (.", "labels": [], "entities": []}, {"text": "Working with such short, informal text spans poses a number of different challenges to the Natural Language Processing (NLP) and Machine Learning (ML) communities.", "labels": [], "entities": []}, {"text": "Those challenges arise from the vocabulary used (slang, abbreviations, emojis) (, the short size, and the complex linguistic phenomena such as sarcasm) that often occur.", "labels": [], "entities": []}, {"text": "We present, here, our participation in Task 4 of, namely Sentiment Analysis in Twitter.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.963373988866806}]}, {"text": "Task 4 comprised five different subtasks: Subtask A: Message Polarity Classification, Subtask B: Tweet classification according to a two-point scale, Subtask C: Tweet classification according to a five-point scale, Subtask D: Tweet quantification according to a two-point scale, and Subtask E: Tweet quantification according to a fivepoint scale.", "labels": [], "entities": [{"text": "Message Polarity Classification", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.6534857153892517}]}, {"text": "We participated in the first four subtasks under the team name \"TwiSE\" (Twitter Sentiment Evaluation).", "labels": [], "entities": [{"text": "TwiSE", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.8707818984985352}]}, {"text": "Our work consists of two steps: the preprocessing and feature extraction step, where we implemented and tested different feature sets proposed by participants of the previous editions of SemEval challenges (, and the learning step, where we investigated and optimized the performance of different learning strategies for the SemEval subtasks.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.6945257931947708}]}, {"text": "For Subtask A we submitted the output of a stacked generalization ensemble learning approach using the probabilistic outputs of a set of linear models as base models, whereas for the rest of the subtasks we submitted the outputs of single models, such as Support Vector Machines and Logistic Regression.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows: in Section 2 we describe the feature extraction and the feature transformations we used, in Section 3 we present the learning strategies we employed, in Section 4 we present apart of the in-house validation we performed to assess the models' performance, and finally, we conclude in Section 5 with remarks on our future work.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7129032015800476}]}], "datasetContent": [{"text": "Before reporting the scores we obtained, we elaborate on our validation strategy and the steps we used when tuning our models.", "labels": [], "entities": []}, {"text": "in each of the subtasks we only used the data that were realised for the 2016 edition of the challenge.", "labels": [], "entities": []}, {"text": "Our validation had the following steps:  1.", "labels": [], "entities": [{"text": "validation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9716414213180542}]}, {"text": "Training using the released training data, 2.", "labels": [], "entities": []}, {"text": "validation on the validation data, 3.", "labels": [], "entities": [{"text": "validation data", "start_pos": 18, "end_pos": 33, "type": "DATASET", "confidence": 0.8449724614620209}]}, {"text": "validation again, in the union of the devtest and trial data (when applicable), after retraining on training and validation data.", "labels": [], "entities": []}, {"text": "For each parameter, we selected its value by averaging the optimal parameters with respect to the output of the above-listed steps (2) and (3).", "labels": [], "entities": []}, {"text": "It is to be noted, that we strictly relied on the data released as part of the 2016 edition of SemEval; we didn't use past data.", "labels": [], "entities": []}, {"text": "We now present the performance we achieved both in our local evaluation schemas and in the official results released by the challenge organisers.", "labels": [], "entities": []}, {"text": "Table 3 presents the results we obtained in the \"DevTest\" part of the challenge dataset and the scores on the test data as they were released by the organisers.", "labels": [], "entities": [{"text": "DevTest\" part of the challenge dataset", "start_pos": 49, "end_pos": 87, "type": "DATASET", "confidence": 0.6517762626920428}]}, {"text": "In the latter, we were ranked 9/35, 8/19, 1/11 and 2/14 for subtasks A, B, C and D respectively.", "labels": [], "entities": []}, {"text": "Observe, that for subtasks A and B, using just the \"devtest\" part of the data as validation mechanism results in a quite accurate performance estimation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of the data used for training and development purposes. We only relied on the SemEval 2016 datasets.", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9373679757118225}, {"text": "SemEval 2016 datasets", "start_pos": 93, "end_pos": 114, "type": "DATASET", "confidence": 0.9230578939119974}]}, {"text": " Table 3: The performance obtained on the \"devtest\" data and", "labels": [], "entities": []}]}