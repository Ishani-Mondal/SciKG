{"title": [{"text": "PotTS at SemEval-2016 Task 4: Sentiment Analysis of Twitter Using Character-level Convolutional Neural Networks", "labels": [], "entities": [{"text": "Sentiment Analysis of Twitter", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.9395385831594467}]}], "abstractContent": [{"text": "This paper presents an alternative approach to polarity and intensity classification of sentiments in microblogs.", "labels": [], "entities": [{"text": "polarity and intensity classification of sentiments in microblogs", "start_pos": 47, "end_pos": 112, "type": "TASK", "confidence": 0.7852966710925102}]}, {"text": "In contrast to previous works, which either relied on carefully designed hand-crafted feature sets or automatically derived neural embeddings for words, our method harnesses character embeddings as its main input units.", "labels": [], "entities": []}, {"text": "We obtain task-specific vector representations of characters by training a deep multi-layer convolutional neural network on the labeled dataset provided to the participants of the SemEval-2016 Shared Task 4 (Sentiment Analysis in Twitter; Nakov et al., 2016b) and subsequently evaluate our classifiers on subtasks B (two-way polarity classification) and C (joint five-way prediction of polarity and intensity) of this competition.", "labels": [], "entities": []}, {"text": "Our first system, which uses three manifold convolution sets followed by four non-linear layers, ranks 16 in the former track; while our second network, which consists of a single convolutional filter set followed by a highway layer and three non-linearities with linear mappings in-between, attains the 10-th place on subtask C.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis (SA) -a field of knowledge which deals with the analysis of people's opinions, sentiments, evaluations, appraisals, attitudes, and emotions towards particular entities mentioned in discourse ( -is commonly considered to be one of the most challenging, competitive, but at The source code of our implementation is freely available online at https://github.com/WladimirSidorenko/ SemEval-2016/ the same time utmost necessary areas of research in modern computational linguistics.", "labels": [], "entities": [{"text": "Sentiment analysis (SA) -a field of knowledge which deals with the analysis of people's opinions, sentiments, evaluations, appraisals, attitudes, and emotions towards particular entities mentioned in discourse", "start_pos": 0, "end_pos": 209, "type": "Description", "confidence": 0.7500339398781458}]}, {"text": "Unfortunately, despite numerous notable advances in recent years (e.g.,), many of the challenges in the opinion mining field, such as domain adaptation or analysis of noisy texts, still pose considerable difficulties to researchers.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.7326558381319046}, {"text": "domain adaptation", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.7162833511829376}]}, {"text": "In this respect, rapidly evaluating and comparing different approaches to solving these problems in a controlled environment -like the one provided for the SemEval task () -is of crucial importance for finding the best possible way of mastering them.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 156, "end_pos": 168, "type": "TASK", "confidence": 0.8327714502811432}]}, {"text": "We also pursue this goal in the present paper by investigating whether one of the newest machine learning trends -the use of deep neural networks (DNNs) with small receptive fields -would be a viable solution for improving state-of-the-art results in sentiment analysis of Twitter.", "labels": [], "entities": [{"text": "sentiment analysis of Twitter", "start_pos": 251, "end_pos": 280, "type": "TASK", "confidence": 0.9233105480670929}]}, {"text": "After a brief summary of related work in Section 2, we present the architectures of our networks and describe the training procedure we used for them in Section 3.", "labels": [], "entities": []}, {"text": "Since we applied two different DNN topologies to subtasks B and C, we make a crosscomparison of both systems and evaluate the role of the preprocessing steps in the next-to-last section.", "labels": [], "entities": []}, {"text": "Finally, in Section 5, we draw conclusions from our experiments and make further suggestions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "To train our final model, we used both training and development data provided by the organizers, setting aside 15 percent of the samples drawn in each epoch for evaluation and using the remaining 85 percent for optimizing the networks' weights.", "labels": [], "entities": []}, {"text": "We obtained the final classifier by choosing the network state that produced the best task-specific score on the set-aside part of the corpus during the training.", "labels": [], "entities": []}, {"text": "For this purpose, in each training iteration, we estimated the macroaveraged recall \u03c1 P N on the evaluation set for subtask B: and computed the macroaveraged mean absolute error measure M AE M (cf.) to select a model for track C : The resulting models were then used in both classification and quantification subtasks of the SemEval competition, i.e., we used the adversarial network with the maximum \u03c1 P N score observed during the training to generate the output for tracks B and D and applied the highway classifier with the minimum achieved M AE M rate to get predictions for subtasks C and E.", "labels": [], "entities": [{"text": "recall \u03c1 P N", "start_pos": 77, "end_pos": 89, "type": "METRIC", "confidence": 0.9458740502595901}, {"text": "macroaveraged mean absolute error measure M AE M", "start_pos": 144, "end_pos": 192, "type": "METRIC", "confidence": 0.7728792317211628}, {"text": "AE M rate", "start_pos": 547, "end_pos": 556, "type": "METRIC", "confidence": 0.8411273757616679}]}, {"text": "The scores of the final evaluation on the official test set are shown in.", "labels": [], "entities": [{"text": "official test set", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.7783753474553426}]}, {"text": "Since many of our parameter and design choices were made empirically by analyzing systems' errors at each development step, we decided to recheck whether these decisions were still optimal for the final configuration.", "labels": [], "entities": []}, {"text": "To that end, we re-evaluated the effects of the preprocessing steps by temporarily switching off lower-casing and stop word filtering, and also estimated the impact of the network structure by applying the model architecture used for subtask B to the five-way prediction task, and vice versa using the highway network for the binary classification.", "labels": [], "entities": []}, {"text": "The output layers, costs, and regularization functions of these two approaches were also swapped in these experiments when applied to different objectives.", "labels": [], "entities": []}, {"text": "Because re-running the complete training from scratch was relatively expensive (taking eight to ten hours on our machine), we reduced the number of training epochs by a factor of five, but tested each configuration thrice in order to overcome the random factors in the He initialization.", "labels": [], "entities": [{"text": "He initialization", "start_pos": 269, "end_pos": 286, "type": "TASK", "confidence": 0.6331122517585754}]}, {"text": "The arithmetic mean and standard deviation (with N = 2) of these three outcomes for each setting are also provided in the table.", "labels": [], "entities": [{"text": "arithmetic mean", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9628036320209503}, {"text": "standard deviation", "start_pos": 24, "end_pos": 42, "type": "METRIC", "confidence": 0.9591713547706604}]}, {"text": "As can be seen from the results, running fewer training epochs does not notably harm the final prediction quality for the binary task.", "labels": [], "entities": []}, {"text": "On the contrary, it might even lead to some improvements for the adversarial network.", "labels": [], "entities": []}, {"text": "We explain this effect by the fact that the model selected during the shorter training had a lower score on the evaluation set than the network state chosen during 50 epochs.", "labels": [], "entities": []}, {"text": "Nevertheless, despite its worse evaluation results, this first configuration was more able to fit the test data than the second system, which apparently overfitted the setaside part of the corpus.", "labels": [], "entities": []}, {"text": "Furthermore, we also can observe a mixed effect of the normalization on the two tasks: while keeping stop words and preserving character case deteriorates the results for the binary classification, abandoning any preprocessing steps turns out to be a more favorable solution when doing five-way prediction.", "labels": [], "entities": []}, {"text": "The reasons for such different behavior are presumably twofold: a) the character case by itself might serve as a good indicator of sentiment intensity but be rather irrelevant to expressing its polarity, and b) the number of training instances might have become scarce as the number of possible gold classes in the corpus increased.", "labels": [], "entities": [{"text": "sentiment intensity", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.8061390221118927}]}, {"text": "Finally, one also can see that the highway network performs slightly better on both subtasks (two-and five-way) than its adversarial counterpart when used with shorter training.", "labels": [], "entities": []}, {"text": "In this case, we assume that the swapping of the regularization and cost functions has hidden the distinctions of the two networks at their initial layers, since, in our earlier experiments, we did observe better results for the two-way classification with the adversarial structure.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of the adversarial and highway networks with", "labels": [], "entities": []}, {"text": " Table 2: Top-10 character n-grams from the training data and", "labels": [], "entities": []}]}