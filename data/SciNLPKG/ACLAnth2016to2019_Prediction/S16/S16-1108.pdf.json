{"title": [{"text": "Identifying Semantic Similarity Using Levenshtein Ratio", "labels": [], "entities": [{"text": "Identifying Semantic Similarity", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8881471355756124}]}], "abstractContent": [{"text": "In this paper we describe the JUNITMZ 1 system that was developed for participation in Se-mEval 2016 Task 1: Semantic Textual Similarity.", "labels": [], "entities": [{"text": "Se-mEval 2016 Task 1: Semantic Textual Similarity", "start_pos": 87, "end_pos": 136, "type": "TASK", "confidence": 0.6349649950861931}]}, {"text": "Methods for measuring the textual similarity are useful to abroad range of applications including: text mining, information retrieval , dialogue systems, machine translation and text summarization.", "labels": [], "entities": [{"text": "text mining", "start_pos": 99, "end_pos": 110, "type": "TASK", "confidence": 0.8013539910316467}, {"text": "information retrieval", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.8076748847961426}, {"text": "machine translation", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.8206573724746704}, {"text": "text summarization", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.7752809226512909}]}, {"text": "However, many systems developed specifically for STS are complex , making them hard to incorporate as a module within a larger applied system.", "labels": [], "entities": [{"text": "STS", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9598407745361328}]}, {"text": "In this paper, we present an STS system based on three simple and robust similarity features that can be easily incorporated into more complex applied systems.", "labels": [], "entities": []}, {"text": "The shared task results show that on most of the shared tasks evaluation sets, these signals achieve a strong (>0.70) level of correlation with human judgements.", "labels": [], "entities": []}, {"text": "Our system's three features are: unigram overlap count, length normalized edit distance and the score computed by the METEOR machine translation metric.", "labels": [], "entities": [{"text": "unigram overlap count", "start_pos": 33, "end_pos": 54, "type": "METRIC", "confidence": 0.9045813878377279}, {"text": "length normalized edit distance", "start_pos": 56, "end_pos": 87, "type": "METRIC", "confidence": 0.8814194947481155}, {"text": "METEOR machine translation", "start_pos": 118, "end_pos": 144, "type": "TASK", "confidence": 0.5345124701658884}]}, {"text": "Features are combined to produces a similarity prediction using both a feedforward and recurrent neural network.", "labels": [], "entities": [{"text": "similarity prediction", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.752790242433548}]}], "introductionContent": [{"text": "Semantic similarity plays important role in many natural language processing (NLP) applications.", "labels": [], "entities": [{"text": "Semantic similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7959858179092407}]}, {"text": "The semantic textual similarity (STS) shared task has been held annually since 2012 in order to assess different approaches to computing textual similarity across a variety of different domains.", "labels": [], "entities": [{"text": "semantic textual similarity (STS) shared task", "start_pos": 4, "end_pos": 49, "type": "TASK", "confidence": 0.7754158712923527}]}, {"text": "Research systems developed specifically for the STS task have resulted in a progression of systems that achieve increasing levels of performance but that are often also increasingly more complex.", "labels": [], "entities": [{"text": "STS task", "start_pos": 48, "end_pos": 56, "type": "TASK", "confidence": 0.9277766942977905}]}, {"text": "Complex approaches maybe difficult if not impossible to incorporate as a component of larger applied NLP systems.", "labels": [], "entities": []}, {"text": "The system described in this paper explores an alternative approach based on three simple and robust textual similarity features.", "labels": [], "entities": []}, {"text": "Our features are simple enough that they can be easily incorporated into larger applied systems that could benefit from textual similarity scores.", "labels": [], "entities": []}, {"text": "The first feature simply counts the number of words common to the pair of sentences being assessed.", "labels": [], "entities": []}, {"text": "The second provides the length normalized edit distance to transform one sentence into another.", "labels": [], "entities": [{"text": "length normalized edit distance", "start_pos": 24, "end_pos": 55, "type": "METRIC", "confidence": 0.8521728962659836}]}, {"text": "The final feature scores the two sentences using the METEOR machine translation metric.", "labels": [], "entities": [{"text": "METEOR machine translation", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.7280586063861847}]}, {"text": "The latter allows the reuse of the linguistic analysis modules developed within the machine translation community to assess translation quality.", "labels": [], "entities": [{"text": "machine translation community", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.7766812940438589}]}, {"text": "METEOR's implementation of these modules is lightweight and efficient, making it not overly cumbersome to incorporate features based on ME-TEOR into larger applied systems.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.900233805179596}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides an overview of our system architecture and Section 3 describes our feature set.", "labels": [], "entities": []}, {"text": "Section 4 reviews the neural network models we use to predict the STS scores.", "labels": [], "entities": [{"text": "STS scores", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.7148675620555878}]}, {"text": "Section 5 describes the evaluation data followed by our results on the evaluation data in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The 2016 STS shared task includes sentence pairs from a number of different data sources organized into five evaluation sets: News Headlines, Plagiarism, Postediting, Q&A Answer-Answer and Q&A Question-Question.", "labels": [], "entities": []}, {"text": "The sentence pairs are assigned similarity scores by multiple crowdsourced annotators on a scale ranging from 0 to 5 with the scores having the following interpretations: (5) complete equivalence, (4) equivalent but differing in minor details, (3) roughly equivalent but differing in important details (2) not equivalent but sharing some details (1) not equivalent but on the same topic   scores are aggregated to assign a final gold standard similarity score to each pair.", "labels": [], "entities": []}, {"text": "provides example sentence pairs with their corresponding gold standard similarity scores.", "labels": [], "entities": []}, {"text": "Systems are assessed on each data set based on the Pearson correlation between the scores they produce and the gold standard.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 51, "end_pos": 70, "type": "METRIC", "confidence": 0.9764723479747772}]}, {"text": "The detailed statistics of the STS-2016 Test datasets are given in.", "labels": [], "entities": [{"text": "STS-2016 Test datasets", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.8785910606384277}]}, {"text": "For the training process we used all gold standard training and test data of year 2012 to 2015 resulting in 12500 sentence pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Examples of sentence pairs with their gold scores (on", "labels": [], "entities": []}, {"text": " Table 4: Statistics of STS-2016 Test Data", "labels": [], "entities": [{"text": "STS-2016 Test Data", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.8988848924636841}]}, {"text": " Table 5: Training data used for the STS-2016 datasets", "labels": [], "entities": [{"text": "STS-2016 datasets", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.863819420337677}]}, {"text": " Table 6: Top and median scores of SemEval-2016", "labels": [], "entities": [{"text": "Top and median scores", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.7244111597537994}, {"text": "SemEval-2016", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.9415279030799866}]}, {"text": " Table 7: System performance on SemEval STS-2016 data.", "labels": [], "entities": [{"text": "SemEval STS-2016 data", "start_pos": 32, "end_pos": 53, "type": "DATASET", "confidence": 0.7282321949799856}]}, {"text": " Table 6.  While our models are less accurate in their predic- tions than other systems, we note that our submis- sion is based on simple and robust features that al- low it to be more easily integrated into complex  downstream applications. With our feature set, we  still achieve a strong (>0.70) correlation with hu- man judgements on 3 of the 5 shared task evaluation  sets. However, our system struggles on both of the  Q&A data sets, questionquestion and answeranswer,  suggesting additional signals may be necessary in  order to correctly handle pairs from this domain.", "labels": [], "entities": []}]}