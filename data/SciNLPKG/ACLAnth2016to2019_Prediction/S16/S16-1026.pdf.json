{"title": [{"text": "INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for Sentiment Classification and Quantification", "labels": [], "entities": [{"text": "SemEval-2016 Task 4", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.6438573201497396}, {"text": "Sentiment Classification and Quantification", "start_pos": 68, "end_pos": 111, "type": "TASK", "confidence": 0.8580353707075119}]}], "abstractContent": [{"text": "This paper describes our deep learning-based approach to sentiment analysis in Twitter as part of SemEval-2016 Task 4.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.9488610029220581}, {"text": "SemEval-2016 Task 4", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7584705352783203}]}, {"text": "We use a convolutional neural network to determine sentiment and participate in all subtasks, i.e. two-point, three-point, and five-point scale sentiment classification and two-point and five-point scale sentiment quantification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 144, "end_pos": 168, "type": "TASK", "confidence": 0.7249837815761566}, {"text": "sentiment quantification", "start_pos": 204, "end_pos": 228, "type": "TASK", "confidence": 0.6902808099985123}]}, {"text": "We achieve competitive results for two-point scale sentiment classification and quantifica-tion, ranking fifth and a close fourth (third and second by alternative metrics) respectively despite using only pre-trained embed-dings that contain no sentiment information.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.8773886561393738}]}, {"text": "We achieve good performance on three-point scale sentiment classification, ranking eighth out of 35, while performing poorly on five-point scale sentiment classification and quan-tification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.8207472562789917}, {"text": "sentiment classification", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.8055212795734406}]}, {"text": "An error analysis reveals that this is due to low expressiveness of the model to capture negative sentiment as well as an inability to take into account ordinal information.", "labels": [], "entities": []}, {"text": "We propose improvements in order to address these and other issues.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social media allows hundreds of millions of people to interact and engage with each other, while expressing their thoughts about the things that move them.", "labels": [], "entities": []}, {"text": "Sentiment analysis allows us to gain insights about opinions towards persons, objects, and events in the public eye and is used nowadays to gauge public opinion towards companies or products, to analyze customer satisfaction, and to detect trends.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9471453130245209}]}, {"text": "Its immediacy allowed Twitter to become an important platform for expressing opinions and public discourse, while the accessibility of large quantities of data in turn made it the focal point of social media sentiment analysis research.", "labels": [], "entities": [{"text": "social media sentiment analysis", "start_pos": 195, "end_pos": 226, "type": "TASK", "confidence": 0.7084106802940369}]}, {"text": "Recently, deep learning-based approaches have demonstrated remarkable results for text classification and sentiment analysis) and have performed well for phrase-level and messagelevel sentiment classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7816358506679535}, {"text": "sentiment analysis", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.929343193769455}, {"text": "messagelevel sentiment classification", "start_pos": 171, "end_pos": 208, "type": "TASK", "confidence": 0.7570895155270895}]}, {"text": "Past SemEval competitions in Twitter sentiment analysis () have contributed to shape research in this field. is no exception, as it introduces both quantification and five-point-scale classification tasks, neither of which have been tackled with deep learning-based approaches before.", "labels": [], "entities": [{"text": "Twitter sentiment analysis", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.6454180081685384}]}, {"text": "We apply our deep learning-based model for sentiment analysis to all subtasks of SemEval-2016 Task 4: three-point scale message polarity classification (subtask A), two-point and five-point scale topic sentiment classification (subtasks B and C respectively), and two-point and five-point scale topic sentiment quantification (subtasks D and E respectively).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.9653967022895813}, {"text": "message polarity classification", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.7591019670168558}, {"text": "topic sentiment classification", "start_pos": 196, "end_pos": 226, "type": "TASK", "confidence": 0.7768469850222269}, {"text": "topic sentiment quantification", "start_pos": 295, "end_pos": 325, "type": "TASK", "confidence": 0.7464916110038757}]}, {"text": "Our model achieves excellent results for subtasks B and D, ranks competitively for subtask A, while performing poorly for subtasks C and E.", "labels": [], "entities": []}, {"text": "We perform an error analysis of our model to obtain a better understanding of strengths and weaknesses of a deep learning-based approach particularly for these new tasks and subsequently propose improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "For every subtask, the organizers provide a training, development, and development test set for training and tuning.", "labels": [], "entities": []}, {"text": "We use the concatentation of the training and development test set for each subtask for training and use the development set for validation.", "labels": [], "entities": []}, {"text": "Additionally, the organizers make training and development data from SemEval-2013 and trial data from 2016 available that can be used for training and tuning for subtask A and subtasks B, C, D, and E respectively.", "labels": [], "entities": []}, {"text": "We experiment with adding these datasets to the respective subtask.", "labels": [], "entities": []}, {"text": "Interestingly, adding them slightly increases loss on the validation set, while providing a significant performance boost on past development test sets, which we view as a proxy for performance on the 2016 test set.", "labels": [], "entities": [{"text": "2016 test set", "start_pos": 201, "end_pos": 214, "type": "DATASET", "confidence": 0.7772179841995239}]}, {"text": "For this reason, we include these datasets for training of all our models.", "labels": [], "entities": []}, {"text": "We notably do not select the model that achieves the lowest loss on the validation set, but choose the one that maximizes the F P N 1 score, i.e. the arithmetic mean of the F 1 of positive and negative tweets, which has historically been used to evaluate the SemEval message polarity classification subtask.", "labels": [], "entities": [{"text": "F P N 1 score", "start_pos": 126, "end_pos": 139, "type": "METRIC", "confidence": 0.9499858975410461}, {"text": "arithmetic mean of the F 1", "start_pos": 150, "end_pos": 176, "type": "METRIC", "confidence": 0.7326169560352961}, {"text": "SemEval message polarity classification", "start_pos": 259, "end_pos": 298, "type": "TASK", "confidence": 0.8476264327764511}]}, {"text": "We observe that the lowest loss does not necessarily lead to the lowest F P N 1 , as it does not include F 1 of neutral tweets.", "labels": [], "entities": [{"text": "F P N 1", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.976578876376152}, {"text": "F 1", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.980752021074295}]}, {"text": "We report results of our model in (subtask A), (subtask B), (subtask C), (subtask D), and Table 7 (subtask E).", "labels": [], "entities": []}, {"text": "For some subtasks, the organizers make available alternative metrics.", "labels": [], "entities": []}, {"text": "We observe that the choice of the scoring metric influences results considerably, with our system always placing higher if ranked by one of the alternative metrics.", "labels": [], "entities": []}, {"text": "Subtask A. We obtain competitive performance on subtask A in.", "labels": [], "entities": []}, {"text": "Analysis of results on the progress test sets in reveals that our system achieves competitive F 1 scores for positive and neutral tweets, but only low F 1 scores for negative tweets due to low recall.", "labels": [], "entities": [{"text": "progress test sets", "start_pos": 27, "end_pos": 45, "type": "DATASET", "confidence": 0.7446383337179819}, {"text": "F 1 scores", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9830877582232157}, {"text": "F 1 scores", "start_pos": 151, "end_pos": 161, "type": "METRIC", "confidence": 0.9810242056846619}, {"text": "recall", "start_pos": 193, "end_pos": 199, "type": "METRIC", "confidence": 0.9960616230964661}]}, {"text": "This is mirrored in, where we rank higher for accuracy than for recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.999194324016571}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9980169534683228}]}, {"text": "The scoring metric for subtask A, F P OS 1 accentuates F 1 for positive and negative tweets, thereby ignoring our good performance on neutral tweets and leading to only mediocre ranks on the progress test sets for our system.", "labels": [], "entities": [{"text": "F 1", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9048371315002441}]}, {"text": "We achieve a competitive fifth rank for subtask B by the official recall metric in Table 3.", "labels": [], "entities": [{"text": "B", "start_pos": 48, "end_pos": 49, "type": "METRIC", "confidence": 0.7977465987205505}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9979410767555237}]}, {"text": "However, ranked by F 1 (as in subtask A), we place third -and second if ranked by accuracy.", "labels": [], "entities": [{"text": "F 1", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9868971705436707}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9984906911849976}]}, {"text": "Similarly, for subtask D, we rank fourth (with a differential of 0.001 to the second rank) by KLD, but second and first if ranked by AE and RAE respectively.", "labels": [], "entities": [{"text": "KLD", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9757261872291565}, {"text": "AE", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.9973734617233276}, {"text": "RAE", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9710589051246643}]}, {"text": "Jointly, these results demonstrate that classifi-   cation performance is a good indicator for quantification without using anymore sophisticated quantification methods.", "labels": [], "entities": []}, {"text": "These results are inline with past research showcasing that even a conceptually simple neural network-based approach can achieve excellent results given enough training data per class.", "labels": [], "entities": []}, {"text": "These results also highlight that embeddings trained using distant supervision, which should be particularly helpful for this task as they are fine-tuned using the same classes, i.e. positive and negative, are not necessary given enough data.", "labels": [], "entities": []}, {"text": "We achieve mediocre results for subtask C in, only ranking sixth -however, placing third by the alternative metric.", "labels": [], "entities": []}, {"text": "Similarly, we only achieve an unsatisfactory eighth rank for subtask E in.", "labels": [], "entities": [{"text": "eighth rank", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.9309452474117279}]}, {"text": "An error analysis for subtask C in reveals that the model is able to differentiate between neutral, positive, and very positive tweets with good accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9963023662567139}]}, {"text": "However, similarly to results in subtask A, we find that it lacks expressiveness for negative sentiment and completely fails to capture very negative tweets due to their low number in the training data.", "labels": [], "entities": []}, {"text": "Additionally, it is unable to take into account sentiment order to reduce error for very positive and very negative tweets.: Our score and rank for subtask E compared to the best team's score.", "labels": [], "entities": [{"text": "error", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9762502312660217}]}], "tableCaptions": [{"text": " Table 1: Our score and rank for subtask A for each metric com-", "labels": [], "entities": []}, {"text": " Table 3: Our score and rank for subtask B for each metric com-", "labels": [], "entities": []}, {"text": " Table 4: Our score and rank for subtask D for each metric com-", "labels": [], "entities": []}, {"text": " Table 5: Our score and rank for subtask C for each metric com-", "labels": [], "entities": []}, {"text": " Table 6: Macro-averaged mean absolute error (M AE M ) of our", "labels": [], "entities": [{"text": "Macro-averaged mean absolute error (M AE M )", "start_pos": 10, "end_pos": 54, "type": "METRIC", "confidence": 0.8526010149055057}]}, {"text": " Table 7: Our score and rank for subtask E compared to the best", "labels": [], "entities": []}]}