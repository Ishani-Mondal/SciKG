{"title": [{"text": "UMD-TTIC-UW at SemEval-2016 Task 1: Attention-Based Multi-Perspective Convolutional Neural Networks for Textual Similarity Measurement", "labels": [], "entities": [{"text": "Textual Similarity Measurement", "start_pos": 104, "end_pos": 134, "type": "TASK", "confidence": 0.664367139339447}]}], "abstractContent": [{"text": "We describe an attention-based convolutional neural network for the English semantic tex-tual similarity (STS) task in the SemEval-2016 competition (Agirre et al., 2016).", "labels": [], "entities": [{"text": "English semantic tex-tual similarity (STS) task", "start_pos": 68, "end_pos": 115, "type": "TASK", "confidence": 0.7038941979408264}]}, {"text": "We develop an attention-based input interaction layer and incorporate it into our multi-perspective convolutional neural network (He et al., 2015), using the PARAGRAM-PHRASE word embeddings (Wieting et al., 2016) trained on paraphrase pairs.", "labels": [], "entities": [{"text": "PARAGRAM-PHRASE", "start_pos": 158, "end_pos": 173, "type": "METRIC", "confidence": 0.8966543674468994}]}, {"text": "Without using any sparse features, our final model outperforms the winning entry in STS2015 when evaluated on the STS2015 data.", "labels": [], "entities": [{"text": "STS2015", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.8307539820671082}, {"text": "STS2015 data", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.9360698461532593}]}], "introductionContent": [{"text": "Measuring the semantic textual similarity (STS) of two pieces of text remains a fundamental problem in language research.", "labels": [], "entities": [{"text": "semantic textual similarity (STS) of two pieces of text", "start_pos": 14, "end_pos": 69, "type": "TASK", "confidence": 0.7273771383545615}]}, {"text": "It lies at the core of many language processing tasks, including paraphrase detection (), question answering, and query ranking.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.8258711397647858}, {"text": "question answering", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.9101711809635162}, {"text": "query ranking", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.7814977765083313}]}, {"text": "The STS problem can be formalized as: given a query sentence S 1 and a comparison sentence S 2 , the task is to compute their semantic similarity in terms of a similarity score sim(S 1 , S 2 ).", "labels": [], "entities": [{"text": "STS", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9651175737380981}]}, {"text": "The SemEval Semantic Textual Similarity tasks () area popular evaluation venue for the STS problem.", "labels": [], "entities": [{"text": "SemEval Semantic Textual Similarity tasks", "start_pos": 4, "end_pos": 45, "type": "TASK", "confidence": 0.8209450602531433}, {"text": "STS problem", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.9250412881374359}]}, {"text": "Over the years the competitions have made more than 15, 000 human annotated sentence pairs publicly available, and have evaluated over 300 system runs.", "labels": [], "entities": []}, {"text": "Traditional approaches are based on hand-crafted feature engineering ().", "labels": [], "entities": []}, {"text": "Competitive systems in recent years are mostly based on neural networks, which can alleviate data sparseness with pretraining and distributed representations.", "labels": [], "entities": []}, {"text": "In this paper, we extend the multi-perspective convolutional neural network (MPCNN) of.", "labels": [], "entities": []}, {"text": "Most previous neural network models, including the MPCNN, treat input sentences separately, and largely ignore context-sensitive interactions between the input sentences.", "labels": [], "entities": [{"text": "MPCNN", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.9127461314201355}]}, {"text": "We address this problem by utilizing an attention mechanism) to develop an attention-based input interaction layer (Sec. 3).", "labels": [], "entities": []}, {"text": "It converts the two independent input sentences into an inter-related sentence pair, which can help the model identify important input words for improved similarity measurement.", "labels": [], "entities": []}, {"text": "We also use the strongly-performing PARAGRAM-PHRASE word embeddings () (Sec.", "labels": [], "entities": [{"text": "PARAGRAM-PHRASE", "start_pos": 36, "end_pos": 51, "type": "METRIC", "confidence": 0.8334222435951233}]}, {"text": "4) trained on phrase pairs from the Paraphrase Database ().", "labels": [], "entities": [{"text": "Paraphrase Database", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.7201772034168243}]}, {"text": "These components comprise our submission to the SemEval-2016 STS competition (shown in): an attention-based multi-perspective convolutional neural network augmented with PARAGRAM-PHRASE word embeddings.", "labels": [], "entities": [{"text": "SemEval-2016 STS competition", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.7282767295837402}]}, {"text": "We provide details of each component in the following sections.", "labels": [], "entities": []}, {"text": "Unlike much previous work in the SemEval competitions), we do not use sparse features, syntactic parsers, or external resources like WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.9719618558883667}]}, {"text": "1. A multi-perspective sentence model for converting a sentence into a representation.", "labels": [], "entities": []}, {"text": "A convolutional neural network captures different granularities of information in each sentence using multiple types of convolutional filters, types of pooling, and window sizes.", "labels": [], "entities": []}], "datasetContent": [{"text": "The test data of the SemEval-2016 English STS competition consists of five datasets from different domains.", "labels": [], "entities": [{"text": "SemEval-2016 English STS competition", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.8019745498895645}]}, {"text": "We tokenize all data using Stanford CoreNLP ( . Each pair has a similarity score \u2208 [0, 5] which increases with similarity.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.9299108982086182}, {"text": "similarity", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9568085074424744}]}, {"text": "We use training data from previous STS competitions (2012 to 2015).", "labels": [], "entities": []}, {"text": "We largely follow the same experimental settings as, e.g., we perform optimization with stochastic gradient descent using a fixed learning rate of 0.01.", "labels": [], "entities": []}, {"text": "We use the 300-dimensional PARAGRAM-PHRASE XXL word embeddings (d = 300).", "labels": [], "entities": [{"text": "PARAGRAM-PHRASE", "start_pos": 27, "end_pos": 42, "type": "METRIC", "confidence": 0.8005935549736023}]}, {"text": "We provide results of three runs in.", "labels": [], "entities": []}, {"text": "The three runs are from the same system, but with models of different training epochs.", "labels": [], "entities": []}, {"text": "We observe a significant drop when the attentionbased input interaction layer (Sec. 3) is removed.", "labels": [], "entities": []}, {"text": "We also find that the PARAGRAM-PHRASE word embeddings are highly beneficial, outperforming both GloVe word embeddings () and the PARAGRAM-SL999 embeddings of.", "labels": [], "entities": []}, {"text": "Our full system performs favorably compared to the winning system ( at the STS2015 SemEval competition.", "labels": [], "entities": [{"text": "STS2015 SemEval competition", "start_pos": 75, "end_pos": 102, "type": "DATASET", "confidence": 0.5837692221005758}]}], "tableCaptions": [{"text": " Table 1: Data statistics for STS2016.", "labels": [], "entities": []}, {"text": " Table 2. The three runs are from the same  system, but with models of different training epochs.", "labels": [], "entities": []}, {"text": " Table 2: Pearson's r on all five test sets. We show  our three submission runs.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.7550585667292277}]}]}