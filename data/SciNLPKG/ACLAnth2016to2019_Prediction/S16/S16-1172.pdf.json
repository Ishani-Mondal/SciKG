{"title": [{"text": "KeLP at SemEval-2016 Task 3: Learning Semantic Relations between Questions and Answers", "labels": [], "entities": [{"text": "Learning Semantic Relations between Questions and Answers", "start_pos": 29, "end_pos": 86, "type": "TASK", "confidence": 0.6532633134296962}]}], "abstractContent": [{"text": "This paper describes the KeLP system participating in the SemEval-2016 Community Question Answering (cQA) task.", "labels": [], "entities": [{"text": "SemEval-2016 Community Question Answering (cQA) task", "start_pos": 58, "end_pos": 110, "type": "TASK", "confidence": 0.8546617776155472}]}, {"text": "The challenge tasks are modeled as binary classification problems: kernel-based classifiers are trained on the SemEval datasets and their scores are used to sort the instances and produce the final ranking.", "labels": [], "entities": [{"text": "SemEval datasets", "start_pos": 111, "end_pos": 127, "type": "DATASET", "confidence": 0.7800304889678955}]}, {"text": "All classifiers and kernels have been implemented within the Kernel-based Learning Platform called KeLP.", "labels": [], "entities": []}, {"text": "Our primary submission ranked first in Sub-task A, third in Subtask B and second in Sub-task C.", "labels": [], "entities": []}, {"text": "These ranks are based on MAP, which is the referring challenge system score.", "labels": [], "entities": [{"text": "MAP", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9641892910003662}]}, {"text": "Our approach outperforms all the other systems with respect to all the other challenge metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the KeLP system participating in the SemEval-2016 cQA challenge.", "labels": [], "entities": [{"text": "SemEval-2016 cQA challenge", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.6255302826563517}]}, {"text": "In this task, participants are asked to automatically provide good answers in a cQA setting).", "labels": [], "entities": []}, {"text": "In particular, the main task is: given anew question and a large collection of question-comment threads created by a user community, rank the most useful comments on the top.", "labels": [], "entities": []}, {"text": "We participated in all English subtasks: the datasets were extracted from Qatar Living 1 , a web forum where people pose questions about multiple aspects of their daily life in Qatar.", "labels": [], "entities": [{"text": "Qatar Living 1", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.9371920824050903}]}, {"text": "Three subtasks are associated with the English challenge: Subtask A: Given a question q and its first 10 comments c 1 , . .", "labels": [], "entities": []}, {"text": ", c 10 in its question thread, re-rank these 10 comments according to their relevance with respect to the question, i.e., the good comments have to be ranked above potential or bad comments.", "labels": [], "entities": []}, {"text": "Subtask B: Given anew question o and the set of the first 10 related questions q 1 , . .", "labels": [], "entities": []}, {"text": ", q 10 (retrieved by a search engine), re-rank the related questions according to their similarity with respect too, i.e., the perfect match and relevant questions should be ranked above the irrelevant ones.", "labels": [], "entities": []}, {"text": "Subtask C: Given anew question o, and the set of the first 10 related questions, q 1 , . .", "labels": [], "entities": []}, {"text": ", q 10 , (retrieved by a search engine), each one associated with its first 10 comments, c q 1 , . .", "labels": [], "entities": []}, {"text": ", c q 10 , appearing in its thread, re-rank the 100 comments according to their relevance with respect too, i.e., the good comments are to be ranked above potential or bad comments.", "labels": [], "entities": []}, {"text": "All the above subtasks have been modeled as binary classification problems: kernel-based classifiers are trained and the classification score is used to sort the instances and produce the final ranking.", "labels": [], "entities": []}, {"text": "All classifiers and kernels have been implemented within the Kernel-based Learning Platform 2 (KeLP) (), thus determining the team's name.", "labels": [], "entities": [{"text": "Kernel-based Learning Platform 2 (KeLP)", "start_pos": 61, "end_pos": 100, "type": "DATASET", "confidence": 0.6419631668499538}]}, {"text": "The proposed solution provides three main contributions: (i) we employ the approach proposed in, which applies tree kernels directly to question and answer texts modeled as pairs of linked syntactic trees.", "labels": [], "entities": []}, {"text": "We further improve the methods using the kernels proposed in ().", "labels": [], "entities": []}, {"text": "(ii) we extended the features developed in, by adopting several features (also derived from Word Embeddings ().", "labels": [], "entities": []}, {"text": "(iii) we propose a stacking schema so that classifiers for Subtask B and C exploit the inferences obtained in the previous subtasks.", "labels": [], "entities": []}, {"text": "Our primary submission ranked first in Subtask A, third in Subtask B and second in Subtask C, demonstrating that the proposed method is very accurate and adaptable to different learning problems.", "labels": [], "entities": []}, {"text": "These ranks are based on the MAP metric.", "labels": [], "entities": []}, {"text": "However, if we consider the other metrics also adopted in the challenge (e.g., F 1 or Accuracy) our approach outperforms all the remaining systems.", "labels": [], "entities": [{"text": "F 1", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9943404495716095}, {"text": "Accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9959625601768494}]}, {"text": "In the remaining, Section 2 introduces the system, Sections 3 and 4 describe the feature and kernel modeling, while Section 5 reports official results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on Subtask A on a 10 fold CV on the training", "labels": [], "entities": [{"text": "Subtask", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9174984097480774}, {"text": "A", "start_pos": 29, "end_pos": 30, "type": "METRIC", "confidence": 0.4052519202232361}, {"text": "training", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.8142896294593811}]}, {"text": " Table 2: Results on Subtask B on a 10 fold Cross-Validation", "labels": [], "entities": [{"text": "Subtask B", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.8065811395645142}]}, {"text": " Table 3: Results on Subtask C on the official development set,", "labels": [], "entities": [{"text": "official development set", "start_pos": 38, "end_pos": 62, "type": "DATASET", "confidence": 0.7657816410064697}]}]}