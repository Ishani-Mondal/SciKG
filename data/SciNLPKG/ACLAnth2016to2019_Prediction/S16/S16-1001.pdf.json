{"title": [{"text": "SemEval-2016 Task 4: Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9526435434818268}]}], "abstractContent": [{"text": "This paper discusses the fourth year of the \"Sentiment Analysis in Twitter Task\".", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter Task", "start_pos": 45, "end_pos": 79, "type": "TASK", "confidence": 0.9207161664962769}]}, {"text": "SemEval-2016 Task 4 comprises five sub-tasks, three of which represent a significant departure from previous editions.", "labels": [], "entities": []}, {"text": "The first two subtasks are reruns from prior years and ask to predict the overall sentiment, and the sentiment towards a topic in a tweet.", "labels": [], "entities": []}, {"text": "The three new subtasks focus on two variants of the basic \"sentiment classification in Twitter\" task.", "labels": [], "entities": [{"text": "sentiment classification in Twitter\" task", "start_pos": 59, "end_pos": 100, "type": "TASK", "confidence": 0.8944516877333323}]}, {"text": "The first variant adopts a five-point scale, which confers an ordinal character to the classification task.", "labels": [], "entities": []}, {"text": "The second variant focuses on the correct estimation of the prevalence of each class of interest, a task which has been called quantification in the supervised learning literature.", "labels": [], "entities": []}, {"text": "The task continues to be very popular, attracting a total of 43 teams.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment classification is the task of detecting whether a textual item (e.g., a product review, a blog post, an editorial, etc.) expresses a POSI-TIVE or a NEGATIVE opinion in general or about a given entity, e.g., a product, a person, apolitical party, or a policy.", "labels": [], "entities": [{"text": "Sentiment classification is the task of detecting whether a textual item (e.g., a product review, a blog post, an editorial, etc.) expresses a POSI-TIVE or a NEGATIVE opinion in general or about a given entity, e.g., a product, a person, apolitical party, or a policy", "start_pos": 0, "end_pos": 267, "type": "Description", "confidence": 0.8506363358880792}]}, {"text": "Sentiment classification has become a ubiquitous enabling technology in the Twittersphere.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9570358395576477}]}, {"text": "Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others.", "labels": [], "entities": [{"text": "Classifying tweets according to sentiment", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8548545837402344}, {"text": "political science", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7964153587818146}]}, {"text": "* Fabrizio Sebastiani is currently on leave from Consiglio Nazionale delle Ricerche, Italy.", "labels": [], "entities": [{"text": "Consiglio Nazionale delle Ricerche", "start_pos": 49, "end_pos": 83, "type": "DATASET", "confidence": 0.7265597730875015}]}, {"text": "As a testament to the prominence of research on sentiment analysis in Twitter, the tweet sentiment classification (TSC) task has attracted the highest number of participants in the last three SemEval campaigns (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8983233273029327}, {"text": "tweet sentiment classification (TSC) task", "start_pos": 83, "end_pos": 124, "type": "TASK", "confidence": 0.8367833920887539}]}, {"text": "Previous editions of the SemEval task involved binary (POSITIVE vs. NEGATIVE) or single-label multi-class classification (SLMC) when a NEU-TRAL 1 class is added (POSITIVE vs. NEGATIVE vs. NEUTRAL).", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.9166377186775208}]}, {"text": "SemEval-2016 Task 4 represents a significant departure from these previous editions.", "labels": [], "entities": []}, {"text": "Although two of the subtasks (Subtasks A and B) are reincarnations of previous editions (SLMC classification for Subtask A, binary classification for Subtask B), SemEval-2016 Task 4 introduces two completely new problems, taken individually (Subtasks C and D) and in combination (Subtask E):", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the process of collection and annotation of the training, development and testing tweets for all five subtasks.", "labels": [], "entities": []}, {"text": "We dub this dataset the Tweet 2016 dataset in order to distinguish it from datasets generated in previous editions of the task.", "labels": [], "entities": [{"text": "Tweet 2016 dataset", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.922592024008433}]}, {"text": "This section discuss the evaluation measures for the five subtasks of our SemEval-2016 Task 4.", "labels": [], "entities": [{"text": "SemEval-2016 Task 4", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7130882740020752}]}, {"text": "A document describing the evaluation measures in detail (, and a scoring software implementing all the five \"official\" measures, were made available to the participants via the task website together with the training data.", "labels": [], "entities": []}, {"text": "For Subtasks B to E, the datasets are each subdivided into a number of \"topics\", and the subtask needs to be carried out independently for each topic.", "labels": [], "entities": []}, {"text": "As a result, each of the evaluation measures will be \"macroaveraged\" across the topics, i.e., we compute the measure individually for each topic, and we then average the results across the topics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics about data from the 2013-2015 editions of", "labels": [], "entities": []}, {"text": " Table 4: 2016 data statistics (Subtask A).", "labels": [], "entities": [{"text": "2016 data statistics", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.7919081747531891}]}, {"text": " Table 5: 2016 data statistics (Subtasks B and D).", "labels": [], "entities": []}, {"text": " Table 6: 2016 data statistics (Subtasks C and E).", "labels": [], "entities": [{"text": "2016 data statistics", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.7612122992674509}]}, {"text": " Table 8: Results for Subtask A \"Message Polarity Classifica- tion\" on the Tweet 2016 dataset. The systems are ordered by  their F P N", "labels": [], "entities": [{"text": "Tweet 2016 dataset", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.9402424891789755}, {"text": "F P N", "start_pos": 129, "end_pos": 134, "type": "METRIC", "confidence": 0.9459599057833353}]}, {"text": " Table 9: Historical results for Subtask A \"Message Polarity Classification\". The systems are ordered by their score on the Tweet  2016 dataset; the rankings on the individual datasets are indicated with a subscript. The meaning of \"(*)\" is as in", "labels": [], "entities": [{"text": "Message Polarity Classification", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6852879822254181}, {"text": "Tweet  2016 dataset", "start_pos": 124, "end_pos": 143, "type": "DATASET", "confidence": 0.9602499008178711}]}, {"text": " Table 10: Historical results for the best systems for Subtask A \"Message Polarity Classification\" over the years 2013-2016.", "labels": [], "entities": [{"text": "Message Polarity Classification\"", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.7449696362018585}]}, {"text": " Table 11: Results for Subtask B \"Tweet classification according  to a two-point scale\" on the Tweet 2016 dataset. The systems  are ordered by their \u03c1 P N score (higher is better). The meaning  of \"(*)\" is as in", "labels": [], "entities": [{"text": "Tweet classification", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.8170501887798309}, {"text": "Tweet 2016 dataset", "start_pos": 95, "end_pos": 113, "type": "DATASET", "confidence": 0.941639264424642}]}, {"text": " Table 13: Results for Subtask D \"Tweet quantification accord- ing to a two-point scale\" on the Tweet 2016 dataset. The sys- tems are ordered by their KLD score (lower is better). The  meaning of \"(*)\" is as in", "labels": [], "entities": [{"text": "Tweet 2016 dataset", "start_pos": 96, "end_pos": 114, "type": "DATASET", "confidence": 0.9293095668156942}, {"text": "KLD score", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.8714761137962341}]}, {"text": " Table 14: Results for Subtask E \"Tweet quantification accord- ing to a five-point scale\" on the Tweet 2016 dataset. The sys- tems are ordered by their EM D score (lower is better). The  meaning of \"(*)\" is as in", "labels": [], "entities": [{"text": "Tweet 2016 dataset", "start_pos": 97, "end_pos": 115, "type": "DATASET", "confidence": 0.9261213938395182}, {"text": "EM D score", "start_pos": 152, "end_pos": 162, "type": "METRIC", "confidence": 0.9639310240745544}]}, {"text": " Table 15: Participating teams (Column 2), their affiliation (Column 3) and nationality (Column 4), the subtasks they have par-", "labels": [], "entities": []}]}