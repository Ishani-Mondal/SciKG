{"title": [{"text": "INESC-ID at SemEval-2016 Task 4-A: Reducing the Problem of Out-of-Embedding Words", "labels": [], "entities": [{"text": "INESC-ID", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8970385789871216}]}], "abstractContent": [{"text": "We present the INESC-ID system for the 2016 edition of SemEval Twitter Sentiment Analysis shared task (subtask 4-A).", "labels": [], "entities": [{"text": "INESC-ID", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.7327147126197815}, {"text": "SemEval Twitter Sentiment Analysis shared task", "start_pos": 55, "end_pos": 101, "type": "TASK", "confidence": 0.8623576064904531}]}, {"text": "The system was based on the Non-Linear Sub-space Embedding (NLSE) model developed for last year's competition.", "labels": [], "entities": []}, {"text": "This model trains a projection of pre-trained embeddings into a small sub-space using the supervised data available.", "labels": [], "entities": []}, {"text": "Despite its simplicity, the system attained performances comparable to the best systems of last edition with no need for feature engineering.", "labels": [], "entities": []}, {"text": "One limitation of this model was the assumption that a pre-trained embedding was available for every word.", "labels": [], "entities": []}, {"text": "In this paper, we investigated different strategies to overcome this limitation by exploiting character-level embed-dings and learning representations for out-of-embedding vocabulary words.", "labels": [], "entities": []}, {"text": "The resulting approach outperforms our previous model by a relatively small margin, while still attaining strong results and a consistent good performance across all the evaluation datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Pre-trained word embeddings provide a simple means to attain semi-supervised learning in Natural Language Processing (NLP) tasks).", "labels": [], "entities": []}, {"text": "They can be trained with large amounts of unsupervised data and be fine tuned as the initial building block of a semi-supervised system.", "labels": [], "entities": []}, {"text": "However, in domains with a significant number of typos, use of slang and abbreviations, such as social media, the high number of singletons leads to a poor fine tuning of the embeddings.", "labels": [], "entities": []}, {"text": "In previous work, we addressed this by learning a projection of the embeddings into a small sub-space (.", "labels": [], "entities": []}, {"text": "This allowed us to attain representations also for Out-Of-Vocabulary (OOV) words, provided that embeddings for those words are available.", "labels": [], "entities": []}, {"text": "However, even if the embeddings are estimated from large amounts of unlabeled text, in noisy domains, such as Twitter, a significant number of words will not be seen and therefore will not have an embedding.", "labels": [], "entities": []}, {"text": "We refer to those words as the Outof-Embedding Vocabulary (OOEV).", "labels": [], "entities": [{"text": "Outof-Embedding Vocabulary (OOEV)", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.46884180307388307}]}, {"text": "In this paper, we focus on the problem of obtaining good representations for OOEV words.", "labels": [], "entities": []}, {"text": "We experimented with character to word models (C2W) and investigated different strategies for initializing and updating OOEVs from the available training data.", "labels": [], "entities": []}, {"text": "The best results were attained by using the labeled data to perform small updates to these representations in the first few epochs of training.", "labels": [], "entities": []}, {"text": "The resulting system outperforms that of the previous evaluation, although by a small margin.", "labels": [], "entities": []}, {"text": "It ranks fourth in the 2016 evaluation with a consistently high performance in all years.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Out Of Vocabulary (OOV) and Ouf Of  Embedding Vocabulary (OOEV) statistics for the  different SemEval Task4-B datasets. Embeddings  reported are the Structured Skipgram embeddings  used in the experiments.", "labels": [], "entities": [{"text": "Ouf Of  Embedding Vocabulary (OOEV)", "start_pos": 38, "end_pos": 73, "type": "METRIC", "confidence": 0.8577802096094403}, {"text": "SemEval Task4-B datasets", "start_pos": 104, "end_pos": 128, "type": "DATASET", "confidence": 0.8001342614491781}]}, {"text": " Table 2: Effect of the improvements on the NLSE  model.", "labels": [], "entities": []}, {"text": " Table 3: Comparision of strategies to address the  problem of OOEV", "labels": [], "entities": [{"text": "OOEV", "start_pos": 63, "end_pos": 67, "type": "TASK", "confidence": 0.5280387997627258}]}, {"text": " Table 4: Official test-set results for the top five sys- tems in SemEval 2016 Task 4-B. Subscript number  indicates position in general ranking.", "labels": [], "entities": [{"text": "SemEval 2016 Task 4-B", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7081693708896637}]}]}