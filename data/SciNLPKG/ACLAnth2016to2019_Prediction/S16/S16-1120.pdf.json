{"title": [{"text": "Rev at SemEval-2016 Task 2: Aligning Chunks by Lexical, Part of Speech and Semantic Equivalence", "labels": [], "entities": []}], "abstractContent": [{"text": "We present the description of our submission to SemEval-2016 Task 2, for the sub-task of aligning pre-annotated chunks between sentence pairs and providing similarity and relat-edness labels for the alignment.", "labels": [], "entities": []}, {"text": "The objective of the task is to provide interpretable semantic textual similarity assessments by adding an explanatory layer to aligned chunks.", "labels": [], "entities": [{"text": "interpretable semantic textual similarity assessments", "start_pos": 40, "end_pos": 93, "type": "TASK", "confidence": 0.6459914922714234}]}, {"text": "We analysed the provided datasets, considering lexical overlap, the part of speech tags and the synonyms of the words in the chunks, and developed a rule-based system reflecting that analysis.", "labels": [], "entities": []}, {"text": "Our system performance indicates that when sentence pairs are similar, alignment of chunks can be performed fairly well using lexical information alone without syntactic or semantic analysis.", "labels": [], "entities": []}, {"text": "The advantage of our system is that we can easily trace when chunks are aligned.", "labels": [], "entities": []}], "introductionContent": [{"text": "We developed a system for SemEval-2016 Task 2: \"Interpretable Semantic Textual Similarity\", which requires the development of a system that labels aligned chunks of a sentence pair, in terms of their similarity or relatedness.", "labels": [], "entities": [{"text": "SemEval-2016 Task 2", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8957625031471252}]}, {"text": "Our approach is based on a detailed analysis of the provided annotation guidelines for the task and the annotated training data.", "labels": [], "entities": []}, {"text": "The annotation guidelines for the task indicate how to align the chunks of two sentence pairs and how to label the similarity or relatedness types and assign scores.", "labels": [], "entities": []}, {"text": "Annotations are provided for three different training sets: headlines, images and student responses to questions.", "labels": [], "entities": []}, {"text": "Chunks consist of a word or contiguous words.", "labels": [], "entities": []}, {"text": "The training sets come in the form of sentence pairs, pre-annotated chunks with their alignment and alignment labels.", "labels": [], "entities": []}, {"text": "The number of sentence pairs for the training and test sets supplied is, respectively: headlines -726 and 375; images -750 and 375; and student answers -330 and 344.", "labels": [], "entities": []}, {"text": "The test sets come with the pre-annotated chunks but without alignments or labels.", "labels": [], "entities": []}, {"text": "Participants of the sub-task are required to align the pre-annotated chunks and provide the similarity and relatedness assessment for the alignment, which is considered as an explanatory layer that enriches measurement of similarity between the sentence pairs.", "labels": [], "entities": [{"text": "similarity", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.979729413986206}]}, {"text": "The alignment types are semantically equivalent (EQUI), opposite in meaning (OPPO), similar in meaning but specific to the chunk in the first sentence (SPE1), similar in meaning but specific to the chunk in second sentence (SPE2), similar in meaning (SIMI) and related in meaning (REL).", "labels": [], "entities": [{"text": "meaning (REL)", "start_pos": 272, "end_pos": 285, "type": "METRIC", "confidence": 0.677123099565506}]}, {"text": "Additional types, are factuality (FACT) and polarity (POL).", "labels": [], "entities": [{"text": "polarity (POL)", "start_pos": 44, "end_pos": 58, "type": "METRIC", "confidence": 0.7732803672552109}]}, {"text": "For chunks with no alignment, the label is NOALI.", "labels": [], "entities": [{"text": "NOALI", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9482316374778748}]}, {"text": "The similarity and relatedness scores are 5 for equivalent (i.e. EQUI//5), for very similar or closely related, for slightly similar or somehow related and 0 for completely unrelated (i.e. NOALI//0).", "labels": [], "entities": [{"text": "similarity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9731017351150513}, {"text": "NOALI", "start_pos": 189, "end_pos": 194, "type": "METRIC", "confidence": 0.8676356077194214}]}, {"text": "In order to demonstrate that the relationship between chunks are based on lexical selection, Abney (1991) uses context-free grammar to describe the structure of chunks, providing a definition of a chunk from a linguistic perspective, which he hypothesizes is closer to how humans parse texts.", "labels": [], "entities": []}, {"text": "His definition provided room for computational implementation.", "labels": [], "entities": []}, {"text": "Thus, for this SemEval sub-task, we attempt to understand how lexical overlap, syntactic (more precisely, Part of Speech (PoS) categories), and word synonyms can be used to align chunks, as this information is not provided.", "labels": [], "entities": []}, {"text": "Even though string, substring and approximate string matching have been well studied (, there have been fewer attempts to assess similarity based on not only lexical/character-level overlap but also incorporating other linguistic characteristics.", "labels": [], "entities": [{"text": "approximate string matching", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6302737494309744}]}, {"text": "Therefore we attempt to explore the use of this more linguistic information in chunk alignment.", "labels": [], "entities": [{"text": "chunk alignment", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.9026878178119659}]}, {"text": "We manually examined the annotation guidelines and training set supplied to understand how the chunks are aligned and how the similarity and relatedness labels maybe assigned based on this information.", "labels": [], "entities": []}, {"text": "Our objective is to develop rules for an automatic system that can effectively align and label the chunks between sentence pairs.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on Different Training Sets", "labels": [], "entities": []}]}