{"title": [{"text": "SemEval-2016 Task 3: Community Question Answering", "labels": [], "entities": [{"text": "SemEval-2016 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8822544813156128}, {"text": "Community Question Answering", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.648875872294108}]}], "abstractContent": [{"text": "This paper describes the SemEval-2016 Task 3 on Community Question Answering , which we offered in English and Ara-bic.", "labels": [], "entities": [{"text": "SemEval-2016 Task 3 on Community Question Answering", "start_pos": 25, "end_pos": 76, "type": "TASK", "confidence": 0.6412567964621952}]}, {"text": "For English, we had three sub-tasks: Question-Comment Similarity (subtask A), Question-Question Similarity (B), and Question-External Comment Similarity (C).", "labels": [], "entities": [{"text": "Question-External Comment Similarity", "start_pos": 116, "end_pos": 152, "type": "TASK", "confidence": 0.6079563697179159}]}, {"text": "For Arabic, we had another subtask: Rerank the correct answers fora new question (D).", "labels": [], "entities": [{"text": "Rerank", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.8629707098007202}]}, {"text": "Eighteen teams participated in the task, submitting a total of 95 runs (38 primary and 57 contrastive) for the four subtasks.", "labels": [], "entities": []}, {"text": "A variety of approaches and features were used by the participating systems to address the different subtasks, which are summarized in this paper.", "labels": [], "entities": []}, {"text": "The best systems achieved an official score (MAP) of 79.19, 76.70, 55.41, and 45.83 in subtasks A, B, C, and D, respectively.", "labels": [], "entities": [{"text": "official score (MAP)", "start_pos": 29, "end_pos": 49, "type": "METRIC", "confidence": 0.9541206598281861}]}, {"text": "These scores are significantly better than those for the baselines that we provided.", "labels": [], "entities": []}, {"text": "For subtask A, the best system improved over the 2015 winner by 3 points absolute in terms of Accuracy.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9993789196014404}]}], "introductionContent": [{"text": "Building on the success of SemEval-2015 Task 3 \"Answer Selection in Community Question Answering\" 1 ( , we run an extension in 2016, which covers a full task on Community Question Answering (CQA) and which is, therefore, closer to the real application needs.", "labels": [], "entities": [{"text": "Answer Selection in Community Question Answering\"", "start_pos": 48, "end_pos": 97, "type": "TASK", "confidence": 0.7556864193507603}, {"text": "Community Question Answering (CQA)", "start_pos": 161, "end_pos": 195, "type": "TASK", "confidence": 0.7720065166552862}]}, {"text": "All the information related to the task, data, participants, results and publications can be found on the SemEval-2016 Task 3 website.", "labels": [], "entities": [{"text": "SemEval-2016 Task 3 website", "start_pos": 106, "end_pos": 133, "type": "DATASET", "confidence": 0.6887027621269226}]}, {"text": "1 http://alt.qcri.org/semeval2015/task3 2 http://alt.qcri.org/semeval2016/task3 CQA forums such as Stack Overflow and Qatar Living , are gaining popularity online.", "labels": [], "entities": [{"text": "Qatar Living", "start_pos": 118, "end_pos": 130, "type": "DATASET", "confidence": 0.9467819333076477}]}, {"text": "These forums are seldom moderated, quite open, and thus they typically have little restrictions, if any, on who can post and who can answer a question.", "labels": [], "entities": []}, {"text": "On the positive side, this means that one can freely ask any question and can then expect some good, honest answers.", "labels": [], "entities": []}, {"text": "On the negative side, it takes effort to go through all possible answers and to make sense of them.", "labels": [], "entities": []}, {"text": "For example, it is not unusual fora question to have hundreds of answers, which makes it very time-consuming for the user to inspect and to winnow through them all.", "labels": [], "entities": []}, {"text": "The present task could help to automate the process of finding good answers to new questions in a community-created discussion forum, e.g., by retrieving similar questions in the forum and by identifying the posts in the comment threads of those similar questions that answer the original question well.", "labels": [], "entities": []}, {"text": "In essence, the main CQA task can be defined as follows: \"given (i) anew question and (ii) a large collection of question-comment threads created by a user community, rank the comments that are most useful for answering the new question\".", "labels": [], "entities": []}, {"text": "The test question is new with respect to the collection, but it is expected to be related to one or several questions in the collection.", "labels": [], "entities": []}, {"text": "The best answers can come from different question-comment threads.", "labels": [], "entities": []}, {"text": "In the collection, the threads are independent of each other and the lists of comments are chronologically sorted and contain some meta information, e.g., date, user, topic, etc.", "labels": [], "entities": []}, {"text": "The comments in a particular thread are intended to answer the question initiating that thread, but since this is a resource created by a community of casual users, there is a lot of noise and irrelevant material, apart from informal language usage and lots of typos and grammatical mistakes.", "labels": [], "entities": []}, {"text": "Interestingly, the questions in the collection can be semantically related to each other, although not explicitly.", "labels": [], "entities": []}, {"text": "Our intention was not to run just another regular Question Answering task.", "labels": [], "entities": [{"text": "Question Answering task", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.8072622617085775}]}, {"text": "Similarly to the 2015 edition, we had three objectives: (i) to focus on semantic-based solutions beyond simple \"bag-ofwords\" representations and \"word matching\" techniques; (ii) to study the new natural language processing (NLP) phenomena arising in the community question answering scenario, e.g., relations between the comments in a thread, relations between different threads and question-to-question similarity; and (iii) to facilitate the participation of non IR/QA experts to our challenge.", "labels": [], "entities": [{"text": "word matching\"", "start_pos": 146, "end_pos": 160, "type": "TASK", "confidence": 0.8043644825617472}, {"text": "question answering scenario", "start_pos": 264, "end_pos": 291, "type": "TASK", "confidence": 0.7882765531539917}]}, {"text": "The third point was achieved by explicitly providing the set of potential answers-the search engine step was carried out by us-to be (re)ranked and by defining two optional subtasks apart from the main CQA task.", "labels": [], "entities": []}, {"text": "Subtask A (Question-Comment Similarity): given a question from a question-comment thread, rank the comments according to their relevance (similarity) with respect to the question; Subtask B (QuestionQuestion Similarity): given the new question, rerank all similar questions retrieved by a search engine, assuming that the answers to the similar questions should be answering the new question too.", "labels": [], "entities": [{"text": "Question-Comment Similarity)", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.7798356215159098}, {"text": "QuestionQuestion Similarity)", "start_pos": 191, "end_pos": 219, "type": "TASK", "confidence": 0.7353849212328593}]}, {"text": "Subtasks A and B should give participants enough tools to create a CQA system to solve the main task.", "labels": [], "entities": []}, {"text": "Nonetheless, one can approach CQA without necessarily solving the two tasks above.", "labels": [], "entities": []}, {"text": "Participants were free to use whatever approach they wanted, and the participation in the main task and/or the two subtasks was optional.", "labels": [], "entities": []}, {"text": "A more precise definition of all subtasks can be found in Section 3.", "labels": [], "entities": []}, {"text": "Keeping the multilinguality from 2015, we provided data for two languages: English and Arabic.", "labels": [], "entities": []}, {"text": "For English, we used real data from the communitycreated Qatar Living forum.", "labels": [], "entities": [{"text": "Qatar Living forum", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.9351953864097595}]}, {"text": "The Arabic data was collected from medical forums, with a slightly different procedure.", "labels": [], "entities": []}, {"text": "We only proposed the main ranking CQA task on this data, i.e., finding good answers fora given new question.", "labels": [], "entities": []}, {"text": "Finally, we provided training data for all languages and subtasks with human supervision.", "labels": [], "entities": []}, {"text": "All examples were manually labeled by a community of annotators in a crowdsourcing platform.", "labels": [], "entities": []}, {"text": "The datasets and the annotation procedure are described in Section 4, and some examples can be found in.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 introduces some related work.", "labels": [], "entities": []}, {"text": "Section 3 gives a more detailed definition of the task.", "labels": [], "entities": []}, {"text": "Section 4 describes the datasets and the process of their creation.", "labels": [], "entities": []}, {"text": "Section 5 explains the evaluation measures.", "labels": [], "entities": []}, {"text": "Section 6 presents the results for all subtasks and for all participating systems.", "labels": [], "entities": []}, {"text": "Section 7 summarizes the main approaches and features used by these systems.", "labels": [], "entities": []}, {"text": "Finally, Section 8 offers some further discussion and presents the main conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "As we mentioned above, the task is offered for two languages, English and Arabic.", "labels": [], "entities": []}, {"text": "Below we describe the data for each language.", "labels": [], "entities": []}, {"text": "We refer to the English data as the CQA-QL corpus; it is based on data from the Qatar Living forum.", "labels": [], "entities": [{"text": "CQA-QL corpus", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9647948443889618}, {"text": "Qatar Living forum", "start_pos": 80, "end_pos": 98, "type": "DATASET", "confidence": 0.940523107846578}]}, {"text": "The English data is organized with focus on the main task, which is subtask C, but it contains annotations for all three subtasks.", "labels": [], "entities": []}, {"text": "It consists of a list of original questions, where for each original question there are ten related questions from Qatar Living, together with the first ten comments from their threads.", "labels": [], "entities": [{"text": "Qatar Living", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.9608630537986755}]}, {"text": "The data is annotated with the relevance of each related question with respect to the original question (subtask B), as well as with the relevance of each comment with respect to the related (subtask A) and also with respect to the original question (subtask C).", "labels": [], "entities": []}, {"text": "To build the dataset, we first selected a set of questions to serve as original questions.", "labels": [], "entities": []}, {"text": "Ina real-world scenario those would be questions that were never asked before; however, here we used existing questions from Qatar Living.", "labels": [], "entities": [{"text": "Qatar Living", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.980430543422699}]}, {"text": "For the training and for the development datasets, we used questions from), while we used new Qatar Living questions for testing.", "labels": [], "entities": [{"text": "Qatar Living questions", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.969235897064209}]}, {"text": "From each original question, we generated a query, using the question's subject (after some word removal if the subject was too long).", "labels": [], "entities": []}, {"text": "Then, we executed the query in Google, limiting the search to the Qatar Living forum, and we collected up to 200 resulting question-comment threads as related questions.", "labels": [], "entities": [{"text": "Qatar Living forum", "start_pos": 66, "end_pos": 84, "type": "DATASET", "confidence": 0.9752484560012817}]}, {"text": "Afterwards, we filtered out threads with less than ten comments as well as those for which the question was more than 2,000 characters long.", "labels": [], "entities": []}, {"text": "Finally, we kept the top-10 surviving threads, keeping just the first 10 comments in each thread.", "labels": [], "entities": []}, {"text": "We formatted the results in XML with UTF-8 encoding, adding metadata for the related questions and for their comments; however, we did not provide any meta information about the original question, in order to emulate a scenario where it is anew question, never asked before in the forum.", "labels": [], "entities": []}, {"text": "In order to have a valid XML, we had to do some cleansing and normalization of the data.", "labels": [], "entities": []}, {"text": "We added an XML format definition at the beginning of the XML file and made sure it validates.", "labels": [], "entities": []}, {"text": "We provided a split of the data into three datasets: training, development, and testing.", "labels": [], "entities": []}, {"text": "A dataset file is a sequence of original questions (OrgQuestion), where each question has a subject, a body (text), and a unique question identifier (ORGQ ID).", "labels": [], "entities": [{"text": "ORGQ ID)", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9419710437456766}]}, {"text": "Each such original question is followed by ten threads, where each thread has a related question (according to the search engine results) and its first ten comments.", "labels": [], "entities": []}, {"text": "Each related question (RelQuestion) has a subject and a body (text), as well as the following attributes: \u2022 RELQ ID: question identifier; \u2022 RELQ RANKING ORDER: the rank of the related question in the list of results returned by the search engine for the original question; 7 \u2022 RELQ DATE: date of posting; \u2022 RELQ USERID: identifier of the user asking the question; \u2022 RELQ USERNAME: name of the user asking the question; \u2022 RELQ RELEVANCE2ORGQ: human assessement on the relevance this RelQuestion thread with respect to OrgQuestion.", "labels": [], "entities": [{"text": "RELQ ID", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9457494914531708}, {"text": "RELQ RANKING ORDER", "start_pos": 140, "end_pos": 158, "type": "METRIC", "confidence": 0.7265933553377787}, {"text": "RELQ DATE", "start_pos": 277, "end_pos": 286, "type": "METRIC", "confidence": 0.9031573235988617}, {"text": "RELQ", "start_pos": 307, "end_pos": 311, "type": "METRIC", "confidence": 0.9548404812812805}, {"text": "USERID", "start_pos": 312, "end_pos": 318, "type": "METRIC", "confidence": 0.5193850994110107}, {"text": "RELQ USERNAME", "start_pos": 366, "end_pos": 379, "type": "METRIC", "confidence": 0.8048945069313049}, {"text": "RELQ RELEVANCE2ORGQ", "start_pos": 421, "end_pos": 440, "type": "METRIC", "confidence": 0.8114938139915466}]}, {"text": "This label can take one of the following values: -PerfectMatch: RelQuestion matches OrgQuestion (almost) perfectly; attest time, this label is to be merged with Relevant; -Relevant: RelQuestion covers some aspects of OrgQuestion; -Irrelevant: RelQuestion covers no aspects of OrgQuestion.", "labels": [], "entities": []}, {"text": "Each comment has a body text, 9 as well as the following attributes: \u2022 RELC ID: comment identifier; \u2022 RELC USERID: identifier of the user posting the comment; \u2022 RELC USERNAME: name of the user posting the comment; \u2022 RELC RELEVANCE2ORGQ: human assessment about whether the comment is Good, Bad, or Potentially Useful with respect to the original question, OrgQuestion.", "labels": [], "entities": [{"text": "RELC", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9321935176849365}, {"text": "RELC", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9882935285568237}, {"text": "USERID", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.5016380548477173}, {"text": "RELC USERNAME", "start_pos": 161, "end_pos": 174, "type": "METRIC", "confidence": 0.7929763197898865}, {"text": "RELC RELEVANCE2ORGQ", "start_pos": 216, "end_pos": 235, "type": "METRIC", "confidence": 0.8228473365306854}]}, {"text": "This label can take one of the following values: -Good: at least one subquestion is directly answered by a portion of the comment; -PotentiallyUseful: no subquestion is directly answered, but the comment gives potentially useful information about one or more subquestions (at test time, this class will be merged with Bad); -Bad: no subquestion is answered and no useful information is provided (e.g., the answer is another question, a thanks, dialog with another user, a joke, irony, attack of other users, or is not in English, etc.).", "labels": [], "entities": []}, {"text": "\u2022 RELC RELEVANCE2RELQ: human assessment about whether the comment is Good, Bad, or PotentiallyUseful (again, the latter two are merged under Bad attest time) with respect to the related question, RelQuestion.", "labels": [], "entities": [{"text": "RELC RELEVANCE2RELQ", "start_pos": 2, "end_pos": 21, "type": "METRIC", "confidence": 0.7976963222026825}, {"text": "RelQuestion", "start_pos": 196, "end_pos": 207, "type": "METRIC", "confidence": 0.7250742316246033}]}, {"text": "We used the CrowdFlower 10 crowdsourcing platform to annotate the gold labels for the three subtasks, namely RELC RELEVANCE2RELQ for subtask A, RELQ RELEVANCE2ORGQ for subtask B, and RELC RELEVANCE2ORGQ for subtask C.", "labels": [], "entities": [{"text": "RELC RELEVANCE2RELQ", "start_pos": 109, "end_pos": 128, "type": "METRIC", "confidence": 0.8807329535484314}]}, {"text": "We collected several annotations for each decision (there were at least three human annotators per example) and we resolved the discrepancies using the default mechanisms of CrowdFlower, which take into account the general quality of annotation for each annotator (based on the hidden tests).", "labels": [], "entities": []}, {"text": "http://www.crowdflower.com, where we excluded comments for which there was a lot of disagreement about the labels between the human annotators, this time we did not eliminate any comments (but we controlled the annotation quality with hidden tests), and thus we guarantee that for each question thread, we have the first ten comments without any comment being skipped.", "labels": [], "entities": []}, {"text": "To gather gold annotation labels, we created two annotation jobs on CrowdFlower, screenshots of which are shown in.", "labels": [], "entities": [{"text": "CrowdFlower", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.9600852131843567}]}, {"text": "The first annotation job aims to collect labels for subtasks B and C.", "labels": [], "entities": []}, {"text": "We show a screenshot in.", "labels": [], "entities": []}, {"text": "An annotation example consists of an original question, a related question, and the first ten comments for that related question.", "labels": [], "entities": []}, {"text": "We asked the annotators to judge the relevance of the thread with respect to the original question (RELQ RELEVANCE2ORGQ, for subtask B), as well as the relevance of each comment with respect to the original question (RELC RELEVANCE2ORGQ, for subtask C).", "labels": [], "entities": [{"text": "RELQ RELEVANCE2ORGQ", "start_pos": 100, "end_pos": 119, "type": "METRIC", "confidence": 0.8704738020896912}]}, {"text": "Each example is judged by three annotators who must maintain 70% accuracy throughout the job, measured on a hidden set of 121 examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9989175796508789}]}, {"text": "The average inter-annotator agreement on the training, development, and testing datasets is 80%, 74%, and 87% for RELQ RELEVANCE2ORGQ, and 83%, 74%, and 88% for RELC RELEVANCE2ORGQ.", "labels": [], "entities": [{"text": "RELQ RELEVANCE2ORGQ", "start_pos": 114, "end_pos": 133, "type": "METRIC", "confidence": 0.5156291723251343}]}, {"text": "The second CrowdFlower job collects labels for subtask A; a screenshot is shown in.", "labels": [], "entities": []}, {"text": "An annotation example consists of a question-comments thread, with ten comments, and we ask annotators to judge the relevance of each comment with respect to the thread question (RELC RELEVANCE2RELQ).", "labels": [], "entities": [{"text": "RELC RELEVANCE2RELQ)", "start_pos": 179, "end_pos": 199, "type": "METRIC", "confidence": 0.8661402463912964}]}, {"text": "Again, each example is judged by three annotators who must maintain 70% accuracy throughout the job, measured on a hidden set of 150 examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9989100694656372}]}, {"text": "The average inter-annotator agreement on the training, development, and testing datasets is 82%, 89%, and 79% for RELC RELEVANCE2RELQ.", "labels": [], "entities": [{"text": "RELC", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.6536600589752197}, {"text": "RELEVANCE2RELQ", "start_pos": 119, "end_pos": 133, "type": "METRIC", "confidence": 0.4007298946380615}]}, {"text": "A fully annotated example is shown in.", "labels": [], "entities": []}, {"text": "Statistics about the datasets are shown in.", "labels": [], "entities": []}, {"text": "Note that the training data is split into two parts, where part2 is noisier than part1.", "labels": [], "entities": []}, {"text": "For part2, a different annotation setup was used, 12 which confused the annotators, and they often provided annotation for RELC RELEVANCE2ORGQ while wrongly thinking that they were actually annotating RELC RELEVANCE2RELQ.", "labels": [], "entities": []}, {"text": "Note that the development data was annotated with the same setup as training part2; however, we manually doublechecked and corrected it.", "labels": [], "entities": []}, {"text": "Instead, the training part1 and testing datasets used the less confusing, and thus higher-quality annotation setup described above.", "labels": [], "entities": []}, {"text": "Note also that in addition to the above-described canonical XML format, we further released the data in an alternative uncleansed   We released this reformatted SemEval-2015 Task 3, subtask A data as additional training data.", "labels": [], "entities": []}, {"text": "We further released a large unannotated dataset from Qatar Living with 189,941 questions and 1,894,456 comments, which is useful for unsupervised learning or for training domain-specific word embeddings.", "labels": [], "entities": [{"text": "Qatar Living", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.906337559223175}]}, {"text": "While at we used a dataset from the Fatwa website, this year we changed the domain to medical, which is largely ignored for Arabic.", "labels": [], "entities": [{"text": "Fatwa website", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.903820127248764}]}, {"text": "We will refer to the Arabic corpus as CQA-MD.", "labels": [], "entities": [{"text": "Arabic corpus", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.7153897136449814}, {"text": "CQA-MD", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.5510753393173218}]}, {"text": "We extracted data from three popular Arabic medical websites that allow visitors to post questions related to health and medical conditions, and to get answers by professional doctors.", "labels": [], "entities": []}, {"text": "We collected 1,531 question-answer (QA) pairs from WebTeb, 69,582 pairs from Al-Tibbi, and 31,714 pairs from the medical corner of Islamweb.", "labels": [], "entities": [{"text": "WebTeb", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9742560982704163}, {"text": "Islamweb", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.6785285472869873}]}, {"text": "We used the 1,531 questions from WebTeb as our original questions, and we looked to find related QA pairs from the other two websites.", "labels": [], "entities": [{"text": "WebTeb", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.9402400255203247}]}, {"text": "We collected over 100,000 QA pairs in total from the other two websites, we indexed them in Solr, and we searched them trying to find answers to the WebTeb questions.", "labels": [], "entities": []}, {"text": "for the 1,531 original questions.", "labels": [], "entities": []}, {"text": "Next, we used CrowdFlower to obtain judgments about the relevance of these QA pairs with respect to the original question using the following labels: \u2022 \"D\" (Direct): The QA pair contains a direct answer to the original question such that if the user is searching for an answer to the original question, the proposed QA pair would be satisfactory and there would be no need to search any further.", "labels": [], "entities": []}, {"text": "\u2022 \"R\" (Related): The QA pair contains an answer to the original question that covers some of the aspects raised in the original question, but this is not sufficient to answer it directly.", "labels": [], "entities": [{"text": "R\"", "start_pos": 3, "end_pos": 5, "type": "METRIC", "confidence": 0.9547136425971985}]}, {"text": "With this QA pair, it would be expected that the user will continue the search to find a direct answer or more information.", "labels": [], "entities": []}, {"text": "\u2022 \"I\" (Irrelevant): The QA pair contains an answer that is irrelevant to the original question.", "labels": [], "entities": [{"text": "I\" (Irrelevant)", "start_pos": 3, "end_pos": 18, "type": "METRIC", "confidence": 0.7623241186141968}]}, {"text": "We controlled the quality of annotation using a hidden set of 50 test questions.", "labels": [], "entities": []}, {"text": "We had three judgments per example, which we combined using the CrowdFlower mechanism.", "labels": [], "entities": []}, {"text": "The average interannotator agreement was 81%.", "labels": [], "entities": [{"text": "interannotator agreement", "start_pos": 12, "end_pos": 36, "type": "METRIC", "confidence": 0.5898468196392059}]}, {"text": "Finally, we divided the data into training, development and testing datasets, based on confidence, where the examples in the test dataset were those with the highest annotation confidence.", "labels": [], "entities": []}, {"text": "We further double-checked and manually corrected some of the annotations for the development and the testing datasets whenever necessary.", "labels": [], "entities": []}, {"text": "shows part of the XML file we generated.", "labels": [], "entities": []}, {"text": "We can see that, unlike the English data, there are no threads here, just a set of question-answer pairs; moreover, we do not provide much metadata, but we give information about the confidence of annotation (for the training and development datasets only, but not for the test dataset).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Main statistics about the English CQA-QL corpus.", "labels": [], "entities": [{"text": "English CQA-QL corpus", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.9077398379643759}]}, {"text": " Table 2: Main statistics about the CQA-MD corpus.", "labels": [], "entities": [{"text": "CQA-MD corpus", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9466809034347534}]}, {"text": " Table 3: Subtask A, English (Question-Comment Similarity): results for all submissions. The first col- umn shows the rank of the primary runs with respect to the official MAP score. The second column contains  the team's name and its submission type (primary vs. contrastive). The following columns show the results  for the primary, and then for other, unofficial evaluation measures. The subindices show the rank of the  primary runs with respect to the evaluation measure in the respective column.", "labels": [], "entities": [{"text": "Question-Comment Similarity)", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7318464517593384}]}, {"text": " Table 4: Subtask B, English (Question-Question Similarity): results for all submissions. The first column  shows the rank of the primary runs with respect to the official MAP score. The second column contains the  team's name and its submission type (primary vs. contrastive). The following columns show the results for  the primary, and then for other, unofficial evaluation measures. The subindices show the rank of the primary  runs with respect to the evaluation measure in the respective column.", "labels": [], "entities": [{"text": "Question-Question Similarity)", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.7612323760986328}, {"text": "MAP score", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.5740295499563217}]}, {"text": " Table 5: Subtask C, English (Question-External Comment Similarity): results for all submissions. The  first column shows the rank of the primary runs with respect to the official MAP score. The second column  contains the team's name and its submission type (primary vs. contrastive). The following columns show  the results for the primary, and then for other, unofficial evaluation measures. The subindices show the rank  of the primary runs with respect to the evaluation measure in the respective column.", "labels": [], "entities": [{"text": "Question-External Comment Similarity)", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.7113804146647453}]}, {"text": " Table 6: Subtask D, Arabic (Reranking the correct answers for a new question)", "labels": [], "entities": []}]}