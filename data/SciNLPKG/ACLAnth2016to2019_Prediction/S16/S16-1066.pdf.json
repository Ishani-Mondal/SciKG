{"title": [{"text": "UWB at SemEval-2016 Task 6: Stance Detection", "labels": [], "entities": [{"text": "Stance Detection", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.979863703250885}]}], "abstractContent": [{"text": "This paper describes our system participating in the SemEval 2016 task: Detecting stance in Tweets.", "labels": [], "entities": [{"text": "SemEval 2016 task", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.8936890363693237}, {"text": "Detecting stance in Tweets", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.8520622700452805}]}, {"text": "The goal was to identify whether the author of a tweet is in favor of the given target or against.", "labels": [], "entities": []}, {"text": "Our approach is based on a maximum entropy classifier, which uses surface-level, sentiment and domain-specific features.", "labels": [], "entities": []}, {"text": "We participated in both the supervised and weakly supervised subtasks and received promising results for most of the targets .", "labels": [], "entities": []}], "introductionContent": [{"text": "Stance detection has been defined as automatically detecting whether the author of apiece of text is in favor of the given target or against it.", "labels": [], "entities": [{"text": "Stance detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9536867439746857}]}, {"text": "In the third class, there are the cases, in which neither inference is likely.", "labels": [], "entities": []}, {"text": "It can be viewed as a subtask of opinion mining and it stands next to the sentiment analysis.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7826907634735107}, {"text": "sentiment analysis", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.8435591459274292}]}, {"text": "The significant difference is that in the case of sentiment analysis, systems determine whether apiece of text is positive, negative, or neutral.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9531972110271454}]}, {"text": "However, instance detection, systems are to determine the author's favorability towards a given target and the target even may not be explicitly mentioned in the text.", "labels": [], "entities": [{"text": "instance detection", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.8471521139144897}]}, {"text": "Moreover, the text may express positive opinion about an entity contained in the text, but one can also infer that the author is against the defined target (an entity or a topic).", "labels": [], "entities": []}, {"text": "This makes the task more difficult, compared to the sentiment analysis, but it can often bring complementary information.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9299994111061096}]}, {"text": "There are many applications which could benefit from the automatic stance detection, including information retrieval, textual entailment, or text summarization, in particular opinion summarization.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.8667280375957489}, {"text": "information retrieval", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.7962897419929504}, {"text": "textual entailment", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7276555597782135}, {"text": "text summarization", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.7413012683391571}, {"text": "opinion summarization", "start_pos": 175, "end_pos": 196, "type": "TASK", "confidence": 0.7369638681411743}]}, {"text": "Twitter was selected as the source of the text because of its popularity and because people express stance implicitly or explicitly there.", "labels": [], "entities": []}, {"text": "We first shortly introduce the task in Section 2 and the available dataset.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our preprocessing, the implemented approach and system's features.", "labels": [], "entities": []}, {"text": "It is followed by the setup for each analysed topic and a discussion of official results (Section 4).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 5. There were 19 participating sys- tems for subtask A and 9 for subtask B. We per-", "labels": [], "entities": []}, {"text": " Table 4: Development results per topic with features  turned on/off (K-10 fold validation).", "labels": [], "entities": []}, {"text": " Table 5: Official results of the SemEval task. There  were 19 submissions for subtask A and 9 submis- sions for subtask B.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9435529708862305}]}]}