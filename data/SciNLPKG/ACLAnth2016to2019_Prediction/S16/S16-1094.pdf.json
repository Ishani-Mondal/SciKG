{"title": [{"text": "ECNU at SemEval-2016 Task 1: Leveraging Word Embedding from Macro and Micro Views to Boost Performance for Semantic Textual Similarity", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 107, "end_pos": 134, "type": "TASK", "confidence": 0.5999710063139597}]}], "abstractContent": [{"text": "This paper presents our submissions for semantic textual similarity task in SemEval 2016.", "labels": [], "entities": [{"text": "semantic textual similarity task", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.7481654435396194}]}, {"text": "Based on several traditional features (i.e., string-based, corpus-based, machine translation similarity and alignment metrics), we leverage word embedding from macro (i.e., first get representation of sentence , then measure the similarity of sentence pair) and micro views (i.e., measure the similarity of word pairs separately) to boost performance.", "labels": [], "entities": []}, {"text": "Due to the various domains of training data and test data, we adopt three different strategies: 1) U-SEVEN: an unsupervised model, which utilizes seven straightforward metrics; 2) S1-All: using all available dataset-s; 3) S2: selecting the most similar training sets for each test set.", "labels": [], "entities": [{"text": "U-SEVEN", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9337164759635925}]}, {"text": "Results on test sets show that the unified supervised model (i.e., S1-All) achieves the best averaged performance with a mean correlation of 75.07%.", "labels": [], "entities": [{"text": "correlation", "start_pos": 126, "end_pos": 137, "type": "METRIC", "confidence": 0.49544188380241394}]}], "introductionContent": [{"text": "Estimating the degree of semantic similarity between two sentences is the building block of many Natural Language Processing (NLP) applications, such as question answering, textual entailment, text summarization etc.", "labels": [], "entities": [{"text": "question answering", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.8857680559158325}, {"text": "textual entailment", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.698461726307869}, {"text": "text summarization", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.7263977825641632}]}, {"text": "Therefore, Semantic Textual Similarity (STS) has received an increasing amount of attention in recent years, e.g., the STS tasks in Semantic Evaluation Exercises have been held from 2012 to 2016.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.8491201102733612}]}, {"text": "To identify semantic similarity of sentence pairs, most existing works adopt at least one of the following feature types: 1) string based similarity) which employs common functions to calculate similarities over string sequences extracted from original strings, e.g., lemma, stem, or n-grams sequences; 2) corpus based similarity) where distributional models such as Latent Semantic Analysis (LSA), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; 3) knowledge based method () which estimates the similarities with the aid of external resources, such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 699, "end_pos": 706, "type": "DATASET", "confidence": 0.9618275761604309}]}, {"text": "Among them, leverage different word alignment strategies to bring word-level similarity to sentence-level similarity.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.747992753982544}]}, {"text": "Traditional NLP feature engineering often treat sentence as a bag of words or term frequency, and endeavor to evaluate the similarity according to the co-occurrence of words or other replacement words.", "labels": [], "entities": []}, {"text": "For example, built a supervised model using ensemble of heterogeneous features and achieved great performance on STS Task 2014.", "labels": [], "entities": [{"text": "STS Task 2014", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.7809170881907145}]}, {"text": "However, it is difficult to evaluate semantic relatedness if all the word in both sentences is unique.", "labels": [], "entities": []}, {"text": "For example: A storm will spread snow over Shanghai; The earthquakes have shaken parts of Oklahoma.", "labels": [], "entities": []}, {"text": "These sentences have no words in common, although they convey the similar information.", "labels": [], "entities": []}, {"text": "In this work, we first borrow the aforementioned effective types of similarity measurements including string-based, corpus-based, machine translation similarity and alignment measures to capture the semantic similarity between two sentences.", "labels": [], "entities": []}, {"text": "Besides, we also present our highly interpretable and hyper-parameter free word embedding features from macro and micro views to boost the performance.", "labels": [], "entities": []}, {"text": "Then we adopt three different strategies of the usage of training data: 1) U-SEVEN: an unsupervised model, which utilizes seven straight-forward metrics (i.e., longest common sequence, alignment feature, corpus-based feature, and others are all from word embedding features); 2) S1-All: use all available datasets and train a unified regression model after deleting unnecessary features; 3) S2: select the most similar training sets for each test set, according to the source of the dataset, average sentence length, and similarity distance (i.e., word mover's distance, discussed in Section 2.3).", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes various similarity measurements used in our systems.", "labels": [], "entities": []}, {"text": "Section 3 gives the datasets and system setups.", "labels": [], "entities": []}, {"text": "Results on training set and test set will show in Section 4 and 5 respectively, and finally conclusion is given in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collect all the datasets from 2012 to 2015 as training data.", "labels": [], "entities": []}, {"text": "Each dataset consists of a number of sentence pairs and each pair has a human-assigned similarity score in the range which increases with similarity.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 87, "end_pos": 103, "type": "METRIC", "confidence": 0.8861157894134521}]}, {"text": "The datasets are collected from different but related domains.", "labels": [], "entities": []}, {"text": "We briefly describe data in, Refer Agirre et al.", "labels": [], "entities": []}, {"text": "(2015a) for details.", "labels": [], "entities": []}, {"text": "We emphasize dataset with symbol * for that this dataset appears both in training and test sets, which is very useful to our third submission S2 (see Section 3.4 for more details).", "labels": [], "entities": []}, {"text": "In order to evaluate the performance of different algorithms, we adopt the official evaluation measure, i.e, Pearson correlation coefficient for each individual test set, and a weighted sum of all correlations is used as final evaluation metric.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 109, "end_pos": 140, "type": "METRIC", "confidence": 0.9685158133506775}]}, {"text": "It weights according to the number of gold sentence pairs.", "labels": [], "entities": []}, {"text": "The weight of   a test set is equal to the rate of the gold sentences pairs in all the gold sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The statistics of all datasets for STS task. Dataset with", "labels": [], "entities": [{"text": "STS task", "start_pos": 45, "end_pos": 53, "type": "TASK", "confidence": 0.7386515438556671}]}, {"text": " Table 2: Pearson coefficient of development data using dif-", "labels": [], "entities": [{"text": "Pearson coefficient", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9546710550785065}]}, {"text": " Table 3: Results of feature selection experiments on STS 2015", "labels": [], "entities": [{"text": "feature selection", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7680851221084595}, {"text": "STS 2015", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.7497352063655853}]}, {"text": " Table 4: The results of our three runs on STS 2016 test dataset-", "labels": [], "entities": [{"text": "STS 2016 test dataset", "start_pos": 43, "end_pos": 64, "type": "DATASET", "confidence": 0.931028738617897}]}]}