{"title": [{"text": "M2L at SemEval-2016 Task 8: AMR Parsing with Neural Networks", "labels": [], "entities": [{"text": "SemEval-2016 Task 8", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7522534728050232}, {"text": "AMR Parsing with Neural Networks", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8316843152046204}]}], "abstractContent": [{"text": "This paper describes our contribution to the SemEval 2016 Workshop.", "labels": [], "entities": [{"text": "SemEval 2016 Workshop", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.8935675422350565}]}, {"text": "We participated in the Shared Task 8 on Meaning Representation parsing using a transition-based approach, which builds upon the system of Wang et al.", "labels": [], "entities": [{"text": "Shared Task 8", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.7469973564147949}, {"text": "Meaning Representation parsing", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.8041904270648956}]}, {"text": "(2015a) and Wang et al.", "labels": [], "entities": []}, {"text": "(2015b), with additions that utilize a Feedforward Neural Network classifier and an enriched feature set.", "labels": [], "entities": []}, {"text": "We observed that exploiting Neural Networks in Abstract Meaning Representation parsing is challenging and we could not benefit from it, while the feature enhancements yielded an improved performance over the baseline model.", "labels": [], "entities": [{"text": "Abstract Meaning Representation parsing", "start_pos": 47, "end_pos": 86, "type": "TASK", "confidence": 0.7097027599811554}]}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) () is a semantic formalism which represents sentence meaning in a form of a rooted directed acyclic graph.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.867039163907369}]}, {"text": "AMR graph nodes represent concepts, labelled directed edges between the nodes show the relationships between concepts.", "labels": [], "entities": []}, {"text": "The AMR formalism was created in order to explore the semantics behind natural language units for further analysis and application in various tasks.", "labels": [], "entities": []}, {"text": "At the time of writing this paper two AMR parsers are publicly available: graph-based JAMR) and transition-based CAMR (;).", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.8096417784690857}]}, {"text": "The latter has served as our baseline model, which we tried to improve by incorporating additional features defined fora wider conditioning context and a neural network (NN) classifier.", "labels": [], "entities": []}, {"text": "Inspired by the results of and, who obtained state-of-the-art results in transition-based dependency parsing using Feedforward Neural Networks (FFNN), and taking into account the transition nature of the CAMR model, we performed a series of experiments in the same direction.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 73, "end_pos": 108, "type": "TASK", "confidence": 0.6107905209064484}]}, {"text": "Neural networks have been successfully applied to many NLP fields and we were curious to examine their potential in the task of AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 128, "end_pos": 139, "type": "TASK", "confidence": 0.9728797674179077}]}, {"text": "Specifically, we investigated the possibility of constraining the averaged perceptron algorithm) predictions by those of an FFNN at the initial step of the inference process.", "labels": [], "entities": [{"text": "FFNN", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.6706841588020325}]}], "datasetContent": [{"text": "All the experiments were performed on the LDC2015E86 dataset, provided by the organizers.", "labels": [], "entities": [{"text": "LDC2015E86 dataset", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.977451890707016}]}, {"text": "In our experiments we followed the standard train/dev/test split sentences, respectively).", "labels": [], "entities": []}, {"text": "Parser performance was evaluated with the Smatch ) scoring script v2.0.2 4 (.", "labels": [], "entities": [{"text": "Parser", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6262038350105286}]}, {"text": "As expected, using SRL features resulted in better performance compared to the baseline model (roughly a 2 F 1 points gain).", "labels": [], "entities": [{"text": "SRL", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.585740864276886}]}, {"text": "Conditioning on a wider context was also beneficial -widening the context to include more configuration elements is often a good feature expansion technique ().", "labels": [], "entities": []}, {"text": "In contrast to our expectations, the NN classifier did not improve the parser performance.", "labels": [], "entities": []}, {"text": "This might be due to the higher complexity of the AMR parsing task or the peculiarities of the underlying parsing algorithm (as mentioned in Section 3, we discarded some actionspecific features due to the difficulty of their integration into the NN model).", "labels": [], "entities": [{"text": "AMR parsing task", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.8948086102803549}]}, {"text": "Further investigation on this matter is required to draw ground conclusions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Feature sets. \u03c3 head", "labels": [], "entities": []}]}