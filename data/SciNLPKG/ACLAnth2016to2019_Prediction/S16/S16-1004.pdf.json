{"title": [{"text": "SemEval-2016 Task 7: Determining Sentiment Intensity of English and Arabic Phrases", "labels": [], "entities": [{"text": "Determining Sentiment Intensity of English and Arabic Phrases", "start_pos": 21, "end_pos": 82, "type": "TASK", "confidence": 0.8919150903820992}]}], "abstractContent": [{"text": "We present a shared task on automatically determining sentiment intensity of a word or a phrase.", "labels": [], "entities": [{"text": "determining sentiment intensity of a word or a phrase", "start_pos": 42, "end_pos": 95, "type": "TASK", "confidence": 0.7941311962074704}]}, {"text": "The words and phrases are taken from three domains: general English, English Twit-ter, and Arabic Twitter.", "labels": [], "entities": []}, {"text": "The phrases include those composed of negators, modals, and degree adverbs as well as phrases formed by words with opposing polarities.", "labels": [], "entities": []}, {"text": "For each of the three domains, we assembled the datasets that include multi-word phrases and their constituent words, both manually annotated for real-valued sentiment intensity scores.", "labels": [], "entities": []}, {"text": "The three datasets were presented as the test sets for three separate tasks (each focusing on a specific domain).", "labels": [], "entities": []}, {"text": "Five teams submitted nine system outputs for the three tasks.", "labels": [], "entities": []}, {"text": "All datasets created for this shared task are freely available to the research community.", "labels": [], "entities": []}], "introductionContent": [{"text": "Words have prior associations with sentiment.", "labels": [], "entities": []}, {"text": "For example, honest and competent are associated with positive sentiment, whereas dishonest and dull are associated with negative sentiment.", "labels": [], "entities": []}, {"text": "Further, the degree of positivity (or negativity), also referred to as intensity, can vary.", "labels": [], "entities": [{"text": "intensity", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9798412322998047}]}, {"text": "For example, most people will agree that succeed is more positive (or less negative) than improve, and fail is more negative (or less positive) than setback.", "labels": [], "entities": []}, {"text": "We present a shared task where automatic systems are asked to predict a prior sentiment intensity score fora word or a phrase.", "labels": [], "entities": []}, {"text": "The words and phrases are taken from three domains: general English, English Twitter, and Arabic Twitter.", "labels": [], "entities": []}, {"text": "For each domain, a separate task with its own development and test sets was setup.", "labels": [], "entities": []}, {"text": "The phrases include those composed of negators (e.g., nothing wrong), modals (e.g., might be fun), and degree adverbs (e.g., fairly important) as well as phrases formed by words with opposing polarities (e.g., lazy sundays).", "labels": [], "entities": []}, {"text": "Lists of words and their associated sentiment are commonly referred to as sentiment lexicons.", "labels": [], "entities": []}, {"text": "They are used in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.9786711931228638}]}, {"text": "For example, a number of unsupervised classifiers rely primarily on sentiment lexicons to determine whether apiece of text is positive or negative.", "labels": [], "entities": []}, {"text": "Supervised classifiers also often use features drawn from sentiment lexicons ().", "labels": [], "entities": []}, {"text": "Sentiment lexicons are also beneficial instance detection (), literary analysis, and for detecting personality traits (.", "labels": [], "entities": [{"text": "instance detection", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.725906029343605}, {"text": "literary analysis", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.8277109265327454}]}, {"text": "Existing manually created sentiment lexicons tend to provide only lists of positive and negative words (.", "labels": [], "entities": []}, {"text": "The coarse-grained distinctions maybe less useful in downstream applications than having access to fine-grained (realvalued) sentiment association scores.", "labels": [], "entities": []}, {"text": "Most of the existing sentiment resources are available only for English.", "labels": [], "entities": []}, {"text": "Non-English resources are scarce and often based on automatic translation of the English lexicons.", "labels": [], "entities": []}, {"text": "Manually created sentiment lexicons usually include only single words.", "labels": [], "entities": []}, {"text": "Yet, the sentiment of a phrase can differ markedly from the sentiment of its constituent words.", "labels": [], "entities": []}, {"text": "Sentiment composition is the determining of sentiment of a multiword linguistic unit, such as a phrase or a sentence, from its constituents.", "labels": [], "entities": [{"text": "Sentiment composition", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9438681602478027}, {"text": "determining of sentiment of a multiword linguistic unit, such as a phrase or a sentence", "start_pos": 29, "end_pos": 116, "type": "TASK", "confidence": 0.6227957010269165}]}, {"text": "Lexicons that include sentiment associations for phrases and their constituents are useful in studying sentiment composition.", "labels": [], "entities": [{"text": "sentiment composition", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.9020211696624756}]}, {"text": "We refer to them as sentiment composition lexicons.", "labels": [], "entities": [{"text": "sentiment composition lexicons", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.8004106481870016}]}, {"text": "Automatically created lexicons often have realvalued sentiment association scores, have a high coverage, can include longer phrases, and can easily be collected fora specific domain.", "labels": [], "entities": []}, {"text": "However, due to the lack of manually annotated real-valued sentiment lexicons the quality of automatic lexicons are often assessed only extrinsically through their use in sentence-level sentiment prediction.", "labels": [], "entities": [{"text": "sentence-level sentiment prediction", "start_pos": 171, "end_pos": 206, "type": "TASK", "confidence": 0.7203975915908813}]}, {"text": "In this shared task, we intrinsically evaluate automatic methods that estimate sentiment association scores for terms in English and Arabic.", "labels": [], "entities": []}, {"text": "For this, we assembled three datasets of phrases and their constituent single words manually annotated for sentiment with realvalued scores.", "labels": [], "entities": [{"text": "sentiment", "start_pos": 107, "end_pos": 116, "type": "TASK", "confidence": 0.964544415473938}]}, {"text": "We first introduced this task as part of the SemEval-2015 Task 10 'Sentiment Analysis in Twitter' Subtask E ().", "labels": [], "entities": [{"text": "SemEval-2015 Task 10", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7681919535001119}, {"text": "Sentiment Analysis in Twitter' Subtask E", "start_pos": 67, "end_pos": 107, "type": "TASK", "confidence": 0.8131792545318604}]}, {"text": "The 2015 test set was restricted to English single words and simple two-word negated expressions commonly found in tweets.", "labels": [], "entities": [{"text": "2015 test set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7629798054695129}]}, {"text": "This year (2016), we broadened the scope of the task and included three different domains.", "labels": [], "entities": []}, {"text": "Furthermore, we shifted the focus from single words to longer, more complex phrases to explore sentiment composition.", "labels": [], "entities": [{"text": "sentiment composition", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.9262185394763947}]}, {"text": "Five teams submitted nine system outputs for the three tasks.", "labels": [], "entities": []}, {"text": "All submitted outputs correlated strongly with the gold term rankings (Kendall's rank correlation above 0.35).", "labels": [], "entities": [{"text": "Kendall's rank correlation", "start_pos": 71, "end_pos": 97, "type": "METRIC", "confidence": 0.72182996571064}]}, {"text": "The best results on all tasks were achieved with supervised methods by exploiting a variety of sentiment resources.", "labels": [], "entities": []}, {"text": "The highest rank correlation was obtained by team ECNU on the General English test set (\u03c4 = 0.7).", "labels": [], "entities": [{"text": "rank correlation", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.8494959771633148}, {"text": "ECNU", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9582585692405701}, {"text": "General English test set", "start_pos": 62, "end_pos": 86, "type": "DATASET", "confidence": 0.9025940448045731}]}, {"text": "On the other two domains, the results were lower (\u03c4 of 0.4-0.5).", "labels": [], "entities": []}, {"text": "All datasets created as part of this shared task are freely available through the task website.", "labels": [], "entities": []}, {"text": "For ease of exploration, we also created online interactive visualizations for the two English datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "The three datasets, General English Sentiment Modifiers Set, English Twitter Mixed Polarity Set, and Arabic Twitter Set, were created through manual annotation using an annotation scheme known as BestWorst Scaling (described below in Section 3.1).", "labels": [], "entities": [{"text": "English Twitter Mixed Polarity Set", "start_pos": 61, "end_pos": 95, "type": "DATASET", "confidence": 0.7022894382476806}]}, {"text": "The terms for each set (domain) were chosen as described in Sections 3.2, 3.3, and 3.4, respectively.", "labels": [], "entities": []}, {"text": "Note that the exact sources of data and the term selection procedures were not known to the participants.", "labels": [], "entities": []}, {"text": "The total number of words and phrases included in each of the datasets can be found in Table 1.", "labels": [], "entities": []}, {"text": "shows a few example entries from each set.", "labels": [], "entities": []}, {"text": "The terms for this dataset were taken from the Sentiment Composition Lexicon for Negators, Modals, and Degree Adverbs (SCL-NMA) ).", "labels": [], "entities": []}, {"text": "4 SCL-NMA includes all 1,621 positive and negative words from Osgood's seminal study on word meaning ( available in General Inquirer ().", "labels": [], "entities": [{"text": "word meaning", "start_pos": 88, "end_pos": 100, "type": "TASK", "confidence": 0.632228285074234}, {"text": "General Inquirer", "start_pos": 116, "end_pos": 132, "type": "DATASET", "confidence": 0.8976461887359619}]}, {"text": "In addition, it includes 1,586 high-frequency phrases formed by the Osgood words in combination with simple negators such as no, don't, and never, modals such as can, might, and should, or degree adverbs such as very and fairly.", "labels": [], "entities": []}, {"text": "The eligible adverbs were chosen manually from adverbs that appeared in combination with an Osgood word at least ten times in the British National Corpus (BNC) . Each phrase includes at least one modal, one negator, or one adverb; a phrase can include several modifiers (e.g., would be very happy).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 130, "end_pos": 159, "type": "DATASET", "confidence": 0.9729984104633331}]}, {"text": "Sixty-four different (single or multi-word) modifiers were used in the dataset.", "labels": [], "entities": []}, {"text": "For this shared task, we removed terms that were used in the SemEval-2015 dataset.", "labels": [], "entities": [{"text": "SemEval-2015 dataset", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.8877244293689728}]}, {"text": "The final SemEval-2016 General English Sentiment Modifiers dataset contains 2,999 terms.", "labels": [], "entities": [{"text": "SemEval-2016 General English Sentiment Modifiers dataset", "start_pos": 10, "end_pos": 66, "type": "DATASET", "confidence": 0.7111295759677887}]}, {"text": "The terms for this dataset were taken in part from the Sentiment Composition Lexicon for Opposing Polarity Phrases (SCL-OPP)).", "labels": [], "entities": []}, {"text": "7 SCL-OPP was created as follows.", "labels": [], "entities": []}, {"text": "We polled the Twitter API (from 2013 to 2015) to collect a corpus of tweets that contain emoticons: ':)' or ':('.", "labels": [], "entities": []}, {"text": "From this corpus, we selected bigrams and trigrams that had at least one positive word and at least one negative word.", "labels": [], "entities": []}, {"text": "The polarity labels (positive or negative) of the words were determined by simple look-up in existing sentiment lexicons: Hu and Liu lexicon (), NRC Emotion lexicon, MPQA lexicon (), and NRC's Twitterspecific lexicon ().", "labels": [], "entities": [{"text": "NRC Emotion lexicon", "start_pos": 145, "end_pos": 164, "type": "DATASET", "confidence": 0.8862479329109192}, {"text": "MPQA lexicon", "start_pos": 166, "end_pos": 178, "type": "DATASET", "confidence": 0.9180723428726196}, {"text": "NRC's Twitterspecific lexicon", "start_pos": 187, "end_pos": 216, "type": "DATASET", "confidence": 0.7927047461271286}]}, {"text": "8 Apart from the requirement of having at least one positive and at least one negative word, an n-gram must satisfy the following criteria: \u2022 the n-gram must have a clear meaning on its own, (for example, the n-gram should not start or end with 'or', 'and', etc.); \u2022 the n-gram should not include a named entity; \u2022 the n-gram should not include obscene language.", "labels": [], "entities": []}, {"text": "In addition, we ensured that there was a good variety of phrases-for example, even though there were a large number of bigrams of the form super w, where w is a negative adjective, only a small number of such bigrams were included.", "labels": [], "entities": []}, {"text": "Finally, we aimed to achieve a good spread in terms of degree of sentiment association (from very negative terms to very positive terms, and all the degrees of polarity in between).", "labels": [], "entities": []}, {"text": "For this, we estimated the sentiment score of each phrase using an automatic PMI-based method described in ().", "labels": [], "entities": []}, {"text": "Then, the full range of sentiment values was divided into 5 bins, and approximately the same number of terms were selected from each bin.", "labels": [], "entities": []}, {"text": "In total, 851 n-grams (bigrams and trigrams) were selected.", "labels": [], "entities": []}, {"text": "We also chose for annotation all unigrams www.saifmohammad.com/WebPages/SCL.html#OPP 8 If a word was marked with conflicting polarity in two lexicons, then that word was not considered as positive or negative.", "labels": [], "entities": [{"text": "OPP", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.8206273913383484}]}, {"text": "For example, the word defeat is marked as positive in Hu and Liu lexicon and marked as negative in MPQA; therefore, we did not select any phrases with this word.", "labels": [], "entities": [{"text": "defeat", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.8997995257377625}, {"text": "MPQA", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.9375929236412048}]}, {"text": "Fewer terms were selected from the middle bin that contained phrases with very weak association to sentiment (e.g., phrases like cancer foundation, fair game, and along nap). that appeared in the selected set of bigrams and trigrams.", "labels": [], "entities": []}, {"text": "There were 810 such unigrams.", "labels": [], "entities": []}, {"text": "When selecting the terms, we used sentiment associations obtained from both manual and automatic lexicons.", "labels": [], "entities": []}, {"text": "As a result, some unigrams had erroneous sentiment associations.", "labels": [], "entities": []}, {"text": "After manually annotating the full set of 1,661 terms (that include unigrams, bigrams, and trigrams), we found that 114 bigrams and 161 trigrams had all their comprising unigrams of the same polarity.", "labels": [], "entities": []}, {"text": "These 275 n-grams were discarded from SCL-OPP but are included in this task dataset.", "labels": [], "entities": []}, {"text": "Further, for this task we removed terms that were used in the SemEval-2015 dataset or in the General English set.", "labels": [], "entities": [{"text": "SemEval-2015 dataset", "start_pos": 62, "end_pos": 82, "type": "DATASET", "confidence": 0.9019960463047028}, {"text": "General English set", "start_pos": 93, "end_pos": 112, "type": "DATASET", "confidence": 0.7967570622762045}]}, {"text": "The final SemEval-2016 English Twitter Mixed Polarity dataset contains 1,269 terms.", "labels": [], "entities": [{"text": "SemEval-2016 English Twitter Mixed Polarity dataset", "start_pos": 10, "end_pos": 61, "type": "DATASET", "confidence": 0.7040565510590872}]}, {"text": "Mohammad et al. automatically generated three high-coverage sentiment lexicons from Arabic tweets using hashtags and emoticons: Arabic Emoticon Lexicon, Arabic Hashtag Lexicon, and Dialectal Arabic Hashtag Lexicon.", "labels": [], "entities": []}, {"text": "In addition to Modern Standard Arabic (MSA), these three lexicons comprise terms in Dialectal Arabic as well as hashtagged compound words, e.g., # (#MaritalHappiness), which do not usually appear in manually created lexicons.", "labels": [], "entities": []}, {"text": "Apart from unigrams, they also include entries for bigrams.", "labels": [], "entities": []}, {"text": "From these lexicons, we selected single words as well as bigrams representing negated expressions in the form of 'negator w', where negator is a negation trigger from a list of 16 common Arabic negation words.", "labels": [], "entities": []}, {"text": "Words used in negated expressions, but missing from the original list were also included.", "labels": [], "entities": [{"text": "negated expressions", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.8864599466323853}]}, {"text": "The selected terms satisfied the following criteria: \u2022 the terms must occur frequently in tweets; \u2022 the terms should not be highly ambiguous.", "labels": [], "entities": []}, {"text": "We also wanted the set of terms as a whole to have these properties: \u2022 the set should have a good spread in terms of degree of sentiment association (from very Team ID Affiliation ECNU ( East China Normal University, China iLab-Edinburgh ( Heriot-Watt University, UK LSIS ( Aix-Marseille University, France NileTMRG (El-Beltagy, 2016a) Nile University, Egypt UWB ( University of West Bohemia, Czech Republic negative terms to very positive terms, and all the degrees of polarity in between); \u2022 the set should include both standard and dialectal Arabic, Romanized words, misspellings, hashtags, and other categories frequently used in Twitter.", "labels": [], "entities": []}, {"text": "(We chose not to include URLs, user mentions, named entities, and obscene terms.)", "labels": [], "entities": []}, {"text": "The final SemEval-2016 Arabic Twitter dataset contains 1,366 terms.", "labels": [], "entities": [{"text": "SemEval-2016 Arabic Twitter dataset", "start_pos": 10, "end_pos": 45, "type": "DATASET", "confidence": 0.6986668705940247}]}, {"text": "Sentiment association scores are most meaningful when compared to each other; they indicate which term is more positive than the other.", "labels": [], "entities": []}, {"text": "Therefore, the automatic systems were evaluated in terms of their abilities to correctly rank the terms by the degree of sentiment association.", "labels": [], "entities": []}, {"text": "For each task, the predicted sentiment intensity scores submitted by the participated systems were evaluated by first ranking the terms according to the proposed sentiment scores and then comparing this ranked list to the gold rankings.", "labels": [], "entities": []}, {"text": "We used Kendall's rank correlation coefficient (Kendall's \u03c4 ) as the official evaluation metric to determine the similarity between the ranked lists (: where c is the number of concordant pairs, i.e., pairs of terms w i and w j for which both the gold ranked list and the predicted ranked list agree (either both lists rank w i higher than w j or both lists rank w i lower than w j ); dis the number of discordant pairs, i.e., pairs of terms w i and w j for which the gold ranked list and the predicted ranked list disagree (one list ranks w i higher than w j and the other list ranks w i lower than w j ); and n is the total number of terms.", "labels": [], "entities": [{"text": "rank correlation coefficient (Kendall's \u03c4 )", "start_pos": 18, "end_pos": 61, "type": "METRIC", "confidence": 0.7583448961377144}]}, {"text": "If any list ranks two terms w i and w j the same, this pair of terms is considered neither concordant nor discordant.", "labels": [], "entities": []}, {"text": "The values of Kendall's \u03c4 range from -1 to 1.", "labels": [], "entities": []}, {"text": "We also calculated scores for Spearman's rank correlation, as an additional (unofficial) metric.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 30, "end_pos": 57, "type": "METRIC", "confidence": 0.5906946584582329}]}], "tableCaptions": [{"text": " Table 1: The number of single-word and multi-word terms in the development and test sets.", "labels": [], "entities": []}, {"text": " Table 2: Examples of entries with real-valued sentiment scores", "labels": [], "entities": []}, {"text": " Table 6: Results for General English Sentiment Modifiers test set. The systems are ordered by their overall Kendall's \u03c4 score,", "labels": [], "entities": [{"text": "General English Sentiment Modifiers test set", "start_pos": 22, "end_pos": 66, "type": "DATASET", "confidence": 0.6963870426019033}, {"text": "Kendall's \u03c4 score", "start_pos": 109, "end_pos": 126, "type": "METRIC", "confidence": 0.6867486536502838}]}, {"text": " Table 7: Results for English Twitter Mixed Polarity test set. The systems are ordered by their overall Kendall's \u03c4 score, which", "labels": [], "entities": [{"text": "English Twitter Mixed Polarity test set", "start_pos": 22, "end_pos": 61, "type": "DATASET", "confidence": 0.8892352879047394}, {"text": "Kendall's \u03c4 score", "start_pos": 104, "end_pos": 121, "type": "METRIC", "confidence": 0.7184481173753738}]}, {"text": " Table 8: Results for Arabic Twitter test set. The systems are ordered by their overall Kendall's \u03c4 score, which was the official", "labels": [], "entities": [{"text": "Arabic Twitter test set", "start_pos": 22, "end_pos": 45, "type": "DATASET", "confidence": 0.9071797132492065}, {"text": "Kendall's \u03c4 score", "start_pos": 88, "end_pos": 105, "type": "METRIC", "confidence": 0.7780823111534119}]}]}