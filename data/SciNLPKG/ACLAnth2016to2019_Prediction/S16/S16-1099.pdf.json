{"title": [{"text": "DLS@CU at SemEval-2016 Task 1: Supervised Models of Sentence Similarity", "labels": [], "entities": [{"text": "Sentence Similarity", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7144841849803925}]}], "abstractContent": [{"text": "We describe a set of systems submitted to the SemEval-2016 English Semantic Textual Similarity (STS) task.", "labels": [], "entities": [{"text": "SemEval-2016 English Semantic Textual Similarity (STS) task", "start_pos": 46, "end_pos": 105, "type": "TASK", "confidence": 0.8850172426965501}]}, {"text": "Given two English sentences , the task is to compute the degree of their semantic similarity.", "labels": [], "entities": []}, {"text": "Each of our systems uses the SemEval 2012-2015 STS datasets to train a ridge regression model that combines different measures of similarity.", "labels": [], "entities": [{"text": "SemEval 2012-2015 STS datasets", "start_pos": 29, "end_pos": 59, "type": "DATASET", "confidence": 0.7894629836082458}]}, {"text": "Our best system demonstrates 73.6% correlation with average human annotations across five test sets.", "labels": [], "entities": [{"text": "correlation", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9600241184234619}]}], "introductionContent": [{"text": "Identification of short-text semantic similarity is an important research problem with application in a multitude of NLP tasks: question answering (), short answer grading), text summarization (), evaluation of machine translation, and soon.", "labels": [], "entities": [{"text": "Identification of short-text semantic similarity", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8951845049858094}, {"text": "question answering", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.8423773348331451}, {"text": "text summarization", "start_pos": 174, "end_pos": 192, "type": "TASK", "confidence": 0.7511020004749298}, {"text": "evaluation of machine translation", "start_pos": 197, "end_pos": 230, "type": "TASK", "confidence": 0.6464454233646393}]}, {"text": "The SemEval Semantic Textual Similarity (STS) task series () is a core platform for the task: a publicly available corpus of more than 14,000 sentence pairs have been developed over a span of four years with human annotations of similarity for each pair; and about 300 system runs have been evaluated.", "labels": [], "entities": [{"text": "SemEval Semantic Textual Similarity (STS) task", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.8403859883546829}]}, {"text": "In this article, we describe a set of systems that participated in the SemEval-2016 English Semantic Textual Similarity (STS) task.", "labels": [], "entities": [{"text": "SemEval-2016 English Semantic Textual Similarity (STS) task", "start_pos": 71, "end_pos": 130, "type": "TASK", "confidence": 0.8904795183075799}]}, {"text": "Given two English sentences, the objective is to compute their semantic similarity in the range, where the score increases with similarity (i.e., 0 indicates no similarity and 5 indicates identical meanings).", "labels": [], "entities": [{"text": "similarity", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9824013113975525}]}, {"text": "The official evaluation metric is the Pearson productmoment correlation coefficient with human annotations.", "labels": [], "entities": [{"text": "Pearson productmoment correlation coefficient", "start_pos": 38, "end_pos": 83, "type": "METRIC", "confidence": 0.7535841688513756}]}, {"text": "Our systems leverage different measures of sentence similarity and train ridge regression models that learn to combine predictions from these different sources using past SemEval data.", "labels": [], "entities": []}, {"text": "The best of our three system runs achieves 73.6% with human annotations among all submitted systems on five test sets (containing a total of 1186 test pairs).", "labels": [], "entities": []}, {"text": "Early work in sentence similarity () established the basic procedural framework under which most modern algorithms operate: computing sentence similarity as a mean of word similarities across the two input sentences.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7100854963064194}]}, {"text": "With no human annotated STS dataset available, these algorithms are unsupervised and were evaluated extrinsically on tasks like paraphrase detection and textual entailment recognition.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.819098711013794}, {"text": "textual entailment recognition", "start_pos": 153, "end_pos": 183, "type": "TASK", "confidence": 0.7701650857925415}]}, {"text": "The SemEval STS task series has made an important contribution through the large annotated dataset, enabling intrinsic evaluation of STS systems and making supervised STS systems a reality.", "labels": [], "entities": [{"text": "SemEval STS task series", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.5849359780550003}]}, {"text": "At SemEval 2012-2015, most of the topperforming STS systems used a regression algorithm to combine different measures of similarity, with the notable exception of a couple of unsupervised systems that relied primarily on alignment of related words in the two sentences).", "labels": [], "entities": []}, {"text": "Our models are based on the successful linear re-Robin Warren was awarded a Nobel Prize .: Words aligned by our aligner across two sentences taken from the MSR alignment corpus.", "labels": [], "entities": [{"text": "MSR alignment corpus", "start_pos": 156, "end_pos": 176, "type": "DATASET", "confidence": 0.6795190572738647}]}, {"text": "(We show only part of the second sentence.)", "labels": [], "entities": []}, {"text": "Besides exact word/lemma matches, it identifies and aligns semantically similar word pairs using PPDB (awarded \u2194 received in this example).", "labels": [], "entities": []}], "datasetContent": [{"text": "best performing system for each test set.", "labels": [], "entities": []}, {"text": "The last row shows the official evaluation metric that computes a weighted sum of correlations overall test sets, where the weight of a test set is proportional to the number of sentence pairs it contains.", "labels": [], "entities": []}, {"text": "Runs 1 and 3 have very similar overall performances, slightly better than that of run 2.", "labels": [], "entities": []}, {"text": "Among the different test sets, the models perform well on news headlines, plagiarism and machine translation data, but poorly on the Q&A forums data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.6854538172483444}, {"text": "Q&A forums data", "start_pos": 133, "end_pos": 148, "type": "DATASET", "confidence": 0.589957880973816}]}], "tableCaptions": [{"text": " Table 1: Test sets at SemEval STS 2016.", "labels": [], "entities": [{"text": "SemEval STS 2016", "start_pos": 23, "end_pos": 39, "type": "DATASET", "confidence": 0.6778650879859924}]}, {"text": " Table 2: Performance on STS 2016 data. Each number in rows", "labels": [], "entities": [{"text": "STS 2016 data", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8181532422701517}]}, {"text": " Table 3: Performance of each individual feature of our best", "labels": [], "entities": []}, {"text": " Table 4: Pairwise correlations between the three runs.", "labels": [], "entities": []}]}