{"title": [{"text": "Overfitting at SemEval-2016 Task 3: Detecting Semantically Similar Questions in Community Question Answering Forums with Word Embeddings", "labels": [], "entities": [{"text": "SemEval-2016 Task", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8217114210128784}, {"text": "Detecting Semantically Similar Questions in Community Question Answering Forums", "start_pos": 36, "end_pos": 115, "type": "TASK", "confidence": 0.8705086443159316}]}], "abstractContent": [{"text": "This paper presents an approach for estimating the question-question similarity of an En-glish dataset specified in Shared Task 3, sub-task B of SemEval-2016.", "labels": [], "entities": []}, {"text": "Given anew question and a set of the first 10 related questions retrieved by a search engine, participants are asked to produce a binary relevant/irrelevant judgement and rerank the related questions according to their similarity with respect to the original question.", "labels": [], "entities": []}, {"text": "Our submitted system uses a 2-layer feed-forward neural network with the averages of word embedding vectors to predict the semantic similarity score of two questions.", "labels": [], "entities": []}, {"text": "We also evaluate the results of Random Forests and Support Vector Machine in comparison to the Neural Network.", "labels": [], "entities": []}, {"text": "Results on the test dataset show that the model achieves a Mean Average Precision 69.681.", "labels": [], "entities": [{"text": "Mean Average Precision 69.681", "start_pos": 59, "end_pos": 88, "type": "METRIC", "confidence": 0.8761575073003769}]}], "introductionContent": [{"text": "Finding semantically related questions is a difficult task due to two main factors: (1) paraphrasing, which can appear at different levels, e.g., lexical, phrasal, sentential; and (2) two questions could be asking different things but look for the same solution.", "labels": [], "entities": []}, {"text": "Thus, traditional surface similarity measures such as Edit Distance, Jaccard, Dice and Overlap coefficient and their variations are notable to capture many cases of semantic relatedness.", "labels": [], "entities": [{"text": "Edit Distance", "start_pos": 54, "end_pos": 67, "type": "METRIC", "confidence": 0.8985728919506073}, {"text": "Jaccard, Dice and Overlap coefficient", "start_pos": 69, "end_pos": 106, "type": "METRIC", "confidence": 0.8552122612794241}]}, {"text": "To preserve the semantic meaning of a question, we use weighted average word embedding vectors to model questions.", "labels": [], "entities": []}, {"text": "We evaluate several algorithms that take the weighted average word embedding vectors of two questions as inputs and predict whether two questions are related or not.", "labels": [], "entities": []}, {"text": "To provide a benchmark so as to compare and develop question-question similarity measuring models in Community Question Answering forums, the Question-Question Similarity task in requires the participants to determine whether two questions are sementically related and given anew question, rerank all similar questions retrieved by a search engine.", "labels": [], "entities": [{"text": "Community Question Answering forums", "start_pos": 101, "end_pos": 136, "type": "TASK", "confidence": 0.7089658975601196}, {"text": "Question-Question Similarity task", "start_pos": 142, "end_pos": 175, "type": "TASK", "confidence": 0.7362393041451772}]}, {"text": "presents an example of a pair of semantically related questions.", "labels": [], "entities": []}, {"text": "We perform an extensive number of experiments using data from Shared Task 3 and compare three types of classifiers: Neural Network (NN), Support Vector Machine (SVM) and Random Forests (RF), as well as the impact of different weighting schemes for averaging word embedding vectors and different word embedding dimensions.", "labels": [], "entities": []}, {"text": "The results show that 2-layer Feedforward Neural Network with Idf weighting scheme and in-domain word embeddings outperforms the rest of models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we use data from the communitycreated Qatar Living Forums 1 collected for SemEval-2016 task 3, subtask B.", "labels": [], "entities": [{"text": "Qatar Living Forums 1 collected for SemEval-2016 task 3", "start_pos": 57, "end_pos": 112, "type": "DATASET", "confidence": 0.8405207528008355}]}, {"text": "There are 317 original questions and 3,169 related questions.", "labels": [], "entities": []}, {"text": "The dataset is pre-splitted into 264 original questions and 2669 related question for training, as well as 50 original questions and 500 related question for validation.", "labels": [], "entities": []}, {"text": "Each data point is a pair of questions (an original question and a related question) and a similarity label, which is either \"PerfectMatch\", \"Relevant\" or \"Irrelevant\".", "labels": [], "entities": [{"text": "Relevant", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9559980034828186}]}, {"text": "According to the task description, we need to predict a binary label where \"True\" covers \"PerfectMatch\" and \"Relevant\", and \"False\" covers \"Irrelevant\", and rerank a set of related questions according to their similarity with respect to the original question.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Performance of out top 6 systems and baseline systems on test dataset, as well as top ranking systems. The second", "labels": [], "entities": []}]}