{"title": [{"text": "SimiHawk at SemEval-2016 Task 1: A Deep Ensemble System for Semantic Textual Similarity", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.6138817568620046}]}], "abstractContent": [{"text": "This paper describes the SimiHawk system submission from UMass Lowell for the core Semantic Textual Similarity task at SemEval-2016.", "labels": [], "entities": [{"text": "SimiHawk system submission from UMass Lowell", "start_pos": 25, "end_pos": 69, "type": "DATASET", "confidence": 0.6168066014846166}, {"text": "Semantic Textual Similarity task at SemEval-2016", "start_pos": 83, "end_pos": 131, "type": "TASK", "confidence": 0.8290679057439169}]}, {"text": "We built four systems: a small feature-based system that leverages word alignment and machine translation quality evaluation metrics, two end-to-end LSTM-based systems , and an ensemble system.", "labels": [], "entities": [{"text": "word alignment and machine translation quality evaluation", "start_pos": 67, "end_pos": 124, "type": "TASK", "confidence": 0.6982900840895516}]}, {"text": "The LSTM-based systems used either a simple LSTM architecture or a Tree-LSTM structure.", "labels": [], "entities": []}, {"text": "We found that of the three base systems, the feature-based model obtained the best results, outperforming each LSTM-based model's correlation by .06.", "labels": [], "entities": []}, {"text": "Ultimately, the ensemble system was able to outperform the base systems substantially, obtaining a weighted Pear-son correlation of 0.738, and placing 7th out of 115 participating systems.", "labels": [], "entities": [{"text": "Pear-son correlation", "start_pos": 108, "end_pos": 128, "type": "METRIC", "confidence": 0.6911464631557465}]}, {"text": "We find that the ensemble system's success comes largely from its ability to form a consensus and eliminate complementary noise from its base systems' predictions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of semantic textual similarity (STS) has been developed over the past several SemEval competitions with the idea of capturing the degree of similarity in the meaning conveyed by two snippets of text (usually two sentences).", "labels": [], "entities": [{"text": "semantic textual similarity (STS)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8108595013618469}]}, {"text": "In that respect, the STS task can be seen as similar to such tasks as paraphrase detection (, recognizing textual entailment (RTE) (, and semantic relatedness).", "labels": [], "entities": [{"text": "STS task", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.8770508468151093}, {"text": "paraphrase detection", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.9344915449619293}, {"text": "recognizing textual entailment (RTE)", "start_pos": 94, "end_pos": 130, "type": "TASK", "confidence": 0.7459948460261027}]}, {"text": "The * These three authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "STS task captures different gradations of similarity, rather than a binary decision, and it is symmetric, rather than directed, as is the case with RTE (.", "labels": [], "entities": []}, {"text": "It also aims to capture a more general notion of semantic similarity that does not focus solely on the semantic relatedness derived through compositional processes.", "labels": [], "entities": []}, {"text": "In this paper, we describe the SimiHawk system submission for the core Semantic Textual Similarity (STS) task at the SemEval-2016 competition.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task at the SemEval-2016 competition", "start_pos": 71, "end_pos": 141, "type": "TASK", "confidence": 0.8280261104757135}]}, {"text": "The STS task series ( has aggregated a sizable dataset of sentence pairs annotated with numeric similarity scores.", "labels": [], "entities": [{"text": "STS task series", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7334068616231283}]}, {"text": "The presence of this dataset allows fora shift from earlier work that mostly used unsupervised learning;), to the supervised approaches that leverage the labeled data (.", "labels": [], "entities": []}, {"text": "The availability of labeled data from different genres also allows fora clearer evaluation and a better comparison across different approaches.", "labels": [], "entities": []}, {"text": "Specifically, the task of semantic similarity is defined as follows: given an input of two sentences, output areal number in the range where 0 means lowest and 5 means highest similarity.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7156014293432236}]}, {"text": "The top-performing system from last year's task) relied heavily on a hand-engineered word alignment tool () (achieving 5th place with the aligner alone), combined with dense word embeddings () to create a two-feature regression model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.7069421261548996}]}, {"text": "Other topperforming systems follow this trend of exploiting word alignment and embedding similarity across textual pairs.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.7227515131235123}]}, {"text": "Recent work has shown the effectiveness of deep learning end-to-end architectures using recurrent and recursive neural networks on tasks similar to semantic similarity, such as semantic relatedness (, natural language inference (, and textual entailment (), providing state-of-theart performance.", "labels": [], "entities": []}, {"text": "Since all of these tasks evaluate the relationship between the semantics of two input sentences, it stands to reason that systems with such architecture may perform well on the more general task of semantic similarity.", "labels": [], "entities": []}, {"text": "In our submission, we are interested in comparing an approach using hand-engineered features against the deep RNN-based architectures with Long ShortTerm Memory (LSTM) cells.", "labels": [], "entities": []}, {"text": "We therefore implemented a feature-based system over a small number of heavily engineered features, and two LSTMbased systems: one that uses a simple LSTM architecture and one that uses a Tree-LSTM architecture that mirrors the syntactic dependencies of the input in the LSTM model.", "labels": [], "entities": []}, {"text": "In the following section, we describe each system in detail.", "labels": [], "entities": []}, {"text": "We then report and analyze the outcome of this bake-off.", "labels": [], "entities": []}, {"text": "Not surprisingly, the ensemble system performs the best, obtaining a weighted Pearson correlation of 0.738.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 78, "end_pos": 97, "type": "METRIC", "confidence": 0.9167520701885223}]}, {"text": "Of the three base systems, the feature-based model obtained the best results, outperforming the LSTM-based models by .06.", "labels": [], "entities": []}], "datasetContent": [{"text": "2012, an ensemble of various MT metrics was created to attain (at the time) state-of-the-art results in Paraphrase Identification ().", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9769333004951477}, {"text": "Paraphrase Identification", "start_pos": 104, "end_pos": 129, "type": "TASK", "confidence": 0.8085505962371826}]}, {"text": "We created features for each of the following metrics to compute the similarity between sentences 1 and 2: \u2022 BLEU () \u2022 NIST  We combine these four feature classes using a fully-connected neural network with 2 layers of size 40.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.998755931854248}]}, {"text": "Similar to) (specifically equations and), the network produces a probability distribution over scores and is trained using categorical cross-entropy loss.", "labels": [], "entities": []}, {"text": "To build the network, we use the Keras library, a Python neural network library written on top of Theano ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of our systems on the 2016 gold standard. The highest score from each column is in bold.", "labels": [], "entities": [{"text": "2016 gold standard", "start_pos": 40, "end_pos": 58, "type": "DATASET", "confidence": 0.6661413908004761}]}, {"text": " Table 2. It is visible in  the table that the only feature absence that generated  any significant drop-off is alignment ratio. There- fore, we decided to include alignment ratio as the  fourth and final feature in our stacking system. For  the stacking implementation, we use the same neu-", "labels": [], "entities": [{"text": "alignment ratio", "start_pos": 112, "end_pos": 127, "type": "METRIC", "confidence": 0.985886961221695}, {"text": "alignment ratio", "start_pos": 164, "end_pos": 179, "type": "METRIC", "confidence": 0.8715484440326691}, {"text": "stacking", "start_pos": 246, "end_pos": 254, "type": "TASK", "confidence": 0.9852800369262695}]}, {"text": " Table 3: Results of our base systems on cross-validation folds", "labels": [], "entities": []}, {"text": " Table 4: Pairwise correlation scores amongst systems' predictions and gold labels (Reference) on the answer-answer domain.", "labels": [], "entities": []}]}