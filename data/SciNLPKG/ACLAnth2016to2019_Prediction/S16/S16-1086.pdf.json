{"title": [{"text": "FBK HLT-MT at SemEval-2016 Task 1: Cross-lingual Semantic Similarity Measurement Using Quality Estimation Features and Compositional Bilingual Word Embeddings", "labels": [], "entities": [{"text": "FBK HLT-MT", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7709644138813019}, {"text": "Cross-lingual Semantic Similarity Measurement", "start_pos": 35, "end_pos": 80, "type": "TASK", "confidence": 0.6624112352728844}]}], "abstractContent": [{"text": "This paper describes the system by FBK HLT-MT for cross-lingual semantic textual similarity measurement.", "labels": [], "entities": [{"text": "FBK HLT-MT", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.9095382690429688}, {"text": "cross-lingual semantic textual similarity measurement", "start_pos": 50, "end_pos": 103, "type": "TASK", "confidence": 0.7901964783668518}]}, {"text": "Our approach is based on supervised regression with an ensemble decision tree.", "labels": [], "entities": []}, {"text": "In order to assign a semantic similarity score to an input sentence pair, the model combines features collected by state-of-the-art methods in machine translation quality estimation and distance metrics between cross-lingual embeddings of the two sentences.", "labels": [], "entities": [{"text": "machine translation quality estimation", "start_pos": 143, "end_pos": 181, "type": "TASK", "confidence": 0.816151961684227}]}, {"text": "In our analysis, we compare different techniques for composing sentence vectors, several distance features and ways to produce training data.", "labels": [], "entities": []}, {"text": "The proposed system achieves a mean Pearson's correlation of 0.39533, ranking 7 th among all participants in the cross-lingual STS task organized within the SemEval 2016 evaluation campaign.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 36, "end_pos": 57, "type": "METRIC", "confidence": 0.9736861983935038}, {"text": "cross-lingual STS task", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.6233717501163483}, {"text": "SemEval 2016 evaluation", "start_pos": 157, "end_pos": 180, "type": "TASK", "confidence": 0.718994935353597}]}], "introductionContent": [{"text": "Semantic textual similarity (STS) measures the degree of equivalence between the meanings of two text sequences (.", "labels": [], "entities": [{"text": "Semantic textual similarity (STS", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5534071981906891}]}, {"text": "The similarity of the text pair can be represented as a continuous or discrete-time value ranging from irrelevance to exact semantic equivalence (.", "labels": [], "entities": []}, {"text": "STS has been one of the official shared tasks in SemEval since 2013 and has attracted the participation of many researchers from the scientific community; enabling the evaluation of several different approaches in natural language processing with a common benchmark and the production of novel annotated data sets that can be used in future research.", "labels": [], "entities": []}, {"text": "State-of-the-art monolingual STS methods make use of several approaches including word alignments and distributional semantics, which are typically employed in a machine learning scenario (.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7545946836471558}]}, {"text": "This is the first year in which SemEval has organized a cross-lingual STS (CL-STS) sub-task, for which a baseline system applicable to the problem has not been defined yet.", "labels": [], "entities": [{"text": "cross-lingual STS (CL-STS) sub-task", "start_pos": 56, "end_pos": 91, "type": "TASK", "confidence": 0.7417299350102743}]}, {"text": "Similar to the monolingual STS task, the cross-lingual task requires the interpretation of the semantic similarity of two crosslingual sentences, one in English and another one in Spanish, with a score ranging from 0 to 5.", "labels": [], "entities": [{"text": "monolingual STS task", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.6984897454579672}]}, {"text": "CL-STS measurement could be extremely useful for achieving textual entailment, paraphrase identification, word-sense disambiguation or sentiment analysis at the cross-lingual level as well as providing new means for an adequacy-oriented evaluation of machine translation outputs.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.828106015920639}, {"text": "word-sense disambiguation", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.7481782734394073}, {"text": "sentiment analysis", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7076937705278397}, {"text": "machine translation outputs", "start_pos": 251, "end_pos": 278, "type": "TASK", "confidence": 0.762753983338674}]}, {"text": "A related task in natural language processing is quality estimation.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.6296540002028147}, {"text": "quality estimation", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.6160440891981125}]}, {"text": "Quality estimation (QE) is used for automatically predicting the quality of machine translation outputs with respect to the source sentences in the original language.", "labels": [], "entities": [{"text": "Quality estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6491702377796174}]}, {"text": "One shortcoming of QE approaches is that the QE system may not capture all aspects of the semantic representations of sentences.", "labels": [], "entities": []}, {"text": "For instance, from a QE perspective, under which the number of edit operations required to fix a translation is used as a proxy of quality, a fluent translation containing an unnecessary negation would likely be labelled as a \"good\" translation.", "labels": [], "entities": []}, {"text": "Therefore, a better solution would be geared to also capture the adequacy aspects of crosslingual comparison of the sentences.", "labels": [], "entities": []}, {"text": "In order to improve the quality of the comparison, the features used in a QE system can be improved using distributional semantics.", "labels": [], "entities": []}, {"text": "Neural language models, such as CBOW or) have proved to be useful in the monolingual STS task before (.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8109580874443054}, {"text": "monolingual STS task", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.6494168241818746}]}, {"text": "Recent studies have extended these models to create bilingual word embeddings such that the embeddings are mapped to a common cross-lingual vector space by using a parallel training corpus or a dictionary ().", "labels": [], "entities": []}, {"text": "In light of these considerations, our submission to the first SemEval CL-STS task combines features derived from QE with distance features obtained by applying cross-lingual word embeddings.", "labels": [], "entities": [{"text": "SemEval CL-STS task", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8156829675038656}]}, {"text": "These features are used to feed an Extremely Randomized Trees (ET) regressor) trained to predict the similarity score of the two sentences.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the components of our CL-STS system.", "labels": [], "entities": []}, {"text": "The details of the experimental analysis carried out on different composition approaches, the characteristics of the system under the influence of each vector space feature and varying data distributions are presented in Section 3.", "labels": [], "entities": []}, {"text": "The final ranking of our system can be found in Section 4 along with the conclusions of our study in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the second phase, we compare the performance of our ET regressor in terms of three different training sets, as explained in Section 3.1. 30% of the training set is used for validation and the remaining 70% is used for training purposes.", "labels": [], "entities": []}, {"text": "The regressor then performs predictions for the same validation set using the different sets.", "labels": [], "entities": []}, {"text": "Finally, we implement an ensemble model which is an average of the predictions of the three subsystems.", "labels": [], "entities": []}, {"text": "The results of these experiments can be seen in.", "labels": [], "entities": []}, {"text": "In light of these experiments, we choose the feature sets composing of 82 and 87 features and the merged set for training purposes (See).", "labels": [], "entities": []}, {"text": "These two systems are submitted as our and [run2] to the CL-STS shared task.", "labels": [], "entities": []}, {"text": "We contribute with the ensemble average system as shown in Ta-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson's correlation of system predictions using cross-validation. The first and second entries in the table indicate  performance of the system using the two approaches separately. The second part indicates the system performance when the two  approaches are combined, revealing the effect of each distance-based feature on the performance. Each column represents a different  sentence composition method; including averaging, averaging after removing stop words and weighted averaging. Numbers in bold  are the two runs, [run1] wth 82 and [run2] wth 87 features respectively, submitted to the SemEval 2016 -CL-STS shared task.", "labels": [], "entities": [{"text": "SemEval 2016 -CL-STS shared task", "start_pos": 606, "end_pos": 638, "type": "TASK", "confidence": 0.512109304467837}]}, {"text": " Table 2. The three  data sets are evaluated during our experiments in  terms of the capability to represent the true data dis- tribution and used in the test phase with the selected  settings (Section 3.3).", "labels": [], "entities": []}, {"text": " Table 2: Sizes of the training sets: number of sentence pairs  and number of words in source and target languages", "labels": [], "entities": []}, {"text": " Table 3: Pearson's correlation of system (run2) predictions us- ing 30-70 split in the (merged) data set. Performance is shown  according to three different training data distributions. Average  indicates the performance of ensemble of the three approaches,  and is the system chosen to be submitted as [run3] to the Se- mEval 2016 -CL-STS shared task.", "labels": [], "entities": [{"text": "Se- mEval 2016 -CL-STS shared task", "start_pos": 318, "end_pos": 352, "type": "TASK", "confidence": 0.53403664752841}]}, {"text": " Table 4: Official results of SemEval 2016 -CL-STS shared task", "labels": [], "entities": [{"text": "SemEval 2016 -CL-STS shared task", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.8276372651259104}]}]}