{"title": [{"text": "Taking the best from the Crowd: Learning Question Passage Classification from Noisy Data", "labels": [], "entities": [{"text": "Learning Question Passage Classification", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.6707847341895103}]}], "abstractContent": [{"text": "In this paper, we propose methods to take into account the disagreement between crowd annotators as well as their skills for weighting instances in learning algorithms.", "labels": [], "entities": []}, {"text": "The latter can thus better deal with noise in the annotation and produce higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9975941777229309}]}, {"text": "We created two passage rerank-ing datasets: one with crowdsource platform , and the second with an expert who completely revised the crowd annotation.", "labels": [], "entities": []}, {"text": "Our experiments show that our weighting approach reduces noise improving passage reranking up to 1.47% and 1.85% on MRR and P@1, respectively.", "labels": [], "entities": [{"text": "MRR", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.49671828746795654}]}], "introductionContent": [{"text": "One of the most important steps for building accurate QA systems is the selection/reranking of answer passage (AP) candidates typically provided by a search engine.", "labels": [], "entities": [{"text": "selection/reranking of answer passage (AP) candidates", "start_pos": 72, "end_pos": 125, "type": "TASK", "confidence": 0.6275880455970764}]}, {"text": "This task requires the automatic learning of a ranking function, which pushes the correct answer passages (i.e., containing the answer to the question) higher in the list.", "labels": [], "entities": []}, {"text": "The accuracy of such function, among other, also depends on the quality of the supervision provided in the training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994427561759949}]}, {"text": "Traditionally, the latter is annotated by experts through a rather costly procedure.", "labels": [], "entities": []}, {"text": "Thus, sometimes, only noisy annotations obtained via automatic labeling mechanisms are available.", "labels": [], "entities": []}, {"text": "For example, the Text REtrieval Conference (TREC ) provides open-domain QA datasets, e.g., for factoid QA.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC )", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.7438714802265167}]}, {"text": "This data contains a set of questions, the answer keywords and a set of unannotated candidate APs.", "labels": [], "entities": []}, {"text": "The labeling of the latter can be automatically carried out by checking if a given passage contains the correct answer keyword or not.", "labels": [], "entities": []}, {"text": "However, this method is prone to http://trec.nist.gov generate passage labels, i.e., containing the answer keyword but not supporting it.", "labels": [], "entities": []}, {"text": "For instance, given the following question, Q, from TREC 2002-03 QA, associated with the answer key Denmark: Q: Where was Hans Christian Anderson born?", "labels": [], "entities": [{"text": "TREC 2002-03 QA", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.8595101038614908}]}, {"text": "the candidate passage: AP: Fairy Tales written by Hans Christian Andersen was published in 1835-1873 in Denmark.", "labels": [], "entities": [{"text": "AP: Fairy Tales written by Hans Christian Andersen was published in 1835-1873 in Denmark", "start_pos": 23, "end_pos": 111, "type": "DATASET", "confidence": 0.7755857447783152}]}, {"text": "would be wrongly labeled as a correct passage since it contains Denmark.", "labels": [], "entities": [{"text": "Denmark", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9426897764205933}]}, {"text": "Such passages can be both misleading for training and unreliable for evaluating the reranking model, thus requiring manual annotation.", "labels": [], "entities": []}, {"text": "Since the expert work is costly, we can rely on crowdsourcing platforms such as CrowdFlower 2 for labeling data, faster and at lower cost (.", "labels": [], "entities": []}, {"text": "This method has shown promising results but it still produces noisy labels.", "labels": [], "entities": []}, {"text": "Thus, a solution consists in (i) using redundant annotations from multiple annotators and (ii) resolving their disagreements with a majority voting approach (.", "labels": [], "entities": []}, {"text": "However, the consensus mechanism can still produce annotation noise, which (i) depends on crowd workers' skill and the difficulty of the given task; and (ii) can degrade the classifier accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9512543678283691}]}, {"text": "In this paper, we study methods to take into account the disagreement among the crowd annotators as well as their skills in the learning algorithms.", "labels": [], "entities": []}, {"text": "For this purpose, we design several instance weighting strategies, which help the learning algorithm to deal with the noise of the training examples, thus producing higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9967029690742493}]}, {"text": "More in detail: firstly, we define some weight factors that characterize crowd annotators' skill, namely: Prior Confidence, which indicates the previous performance of the crowd worker re-ported by the crowdsourcing platform; Task Confidence, which is determined by the total number of annotations performed by the crowd worker in the target task; and Consistency Confidence, which quantify the agreements between the annotator and the majority voting labels.", "labels": [], "entities": []}, {"text": "We used these parameters for building our weighting functions, which aim at reducing the impact of the noisy annotations in learning algorithms.", "labels": [], "entities": []}, {"text": "Secondly, we build a passage reranking dataset based on TREC 2002/2003 QA.", "labels": [], "entities": [{"text": "TREC 2002/2003 QA", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.9504164934158326}]}, {"text": "We used Crowdflowers for carrying our an intial noisy annotation and we had an expert to manually verify and corrected incorrect labels.", "labels": [], "entities": [{"text": "Crowdflowers", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.9528560042381287}]}, {"text": "This is an important QA resource that we will release to the research community.", "labels": [], "entities": []}, {"text": "Additionally, the accuracy of our models, e.g., classifiers and search engines, tested on such gold standard data establish new baselines, useful for future research in the field.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9964219331741333}]}, {"text": "Finally, we conducted comparative experiments on our QA dataset using our weighting strategies.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.7638395130634308}]}, {"text": "The results show that (i) our rerankers improve on the IR baseline, i.e., BM25, by 17.47% and 19.22% in MRR and P@1, respectively; and (ii) our weighting strategy improves the best reranker (using no-weighting model) up to 1.47% and 1.85% on MRR and P@1, respectively.", "labels": [], "entities": [{"text": "IR baseline", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.6076976358890533}, {"text": "BM25", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.7448493242263794}, {"text": "MRR", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.6502767205238342}, {"text": "MRR", "start_pos": 242, "end_pos": 245, "type": "DATASET", "confidence": 0.8771214485168457}]}], "datasetContent": [{"text": "Initially, we ran a crowdsourcing task on CrowdFlower micro-tasking platform and asked the crowd workers to assign a relevant/not relevant annotation label to the given Q/AP pairs.", "labels": [], "entities": [{"text": "CrowdFlower micro-tasking platform", "start_pos": 42, "end_pos": 76, "type": "DATASET", "confidence": 0.9109539786974589}]}, {"text": "The crowd workers had to decide whether the given AP supports the raised question or not.", "labels": [], "entities": []}, {"text": "We consider the TREC corpora described in Section 5.1 and in particular the first 20 APs retrieved by BM25 search engine for every question.", "labels": [], "entities": [{"text": "BM25 search engine", "start_pos": 102, "end_pos": 120, "type": "DATASET", "confidence": 0.9036997556686401}]}, {"text": "We collect 5 judgments for each AP.", "labels": [], "entities": []}, {"text": "Additionally, we removed the maximum quota of annotations a crowd worker can perform.", "labels": [], "entities": []}, {"text": "We demonstrated that this (i) does not affected the quality of the annotations in Section 5.1; and (ii) allows us to collect reliable statistics about the crowd annotators since they can participate extensively to our annotation project.", "labels": [], "entities": [{"text": "Section 5.1", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.9274612665176392}]}, {"text": "The intuition behind the idea is: a crowd worker is more reliable fora given task if (s)he annotates more passages.", "labels": [], "entities": []}, {"text": "Finally, we used control questions discarding the annotation of crowd annotators providing incorrect answers.", "labels": [], "entities": []}, {"text": "Overall, we crowdsourced 527 questions of the TREC 2002/2003 QA task and collected 52,700 judgments.", "labels": [], "entities": [{"text": "TREC 2002/2003 QA task", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.8006999393304189}]}, {"text": "The number of the participant workers was 108 and the minimum and maximum number of answer passages annotated by a single crowd annotator were 21 and 1,050, respectively.", "labels": [], "entities": []}, {"text": "To obtain an accurate gold standard, we asked an expert to revise the passages labeled by crowd annotators when at least one disagreement was present among the annotations.", "labels": [], "entities": []}, {"text": "This super gold standard is always and only used for testing our models (not for training).", "labels": [], "entities": []}, {"text": "In this paper, we used the questions from TREC 2002 and 2003 from the large newswire corpus, AQUAINT.", "labels": [], "entities": [{"text": "TREC 2002 and 2003 from the large newswire corpus", "start_pos": 42, "end_pos": 91, "type": "DATASET", "confidence": 0.7784794171651205}, {"text": "AQUAINT", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.6393200159072876}]}, {"text": "We created the Q/AP pairs training BM25 on AQUAINT and retrieving candidate passages for each question.", "labels": [], "entities": [{"text": "BM25", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.3115789294242859}, {"text": "AQUAINT", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8974519371986389}]}, {"text": "Before running the main crowdsourcing task, we evaluated the effect of the initial configurations of the platform on the quality of the collected annotation.", "labels": [], "entities": []}, {"text": "We conducted two pilot crowdsourcing experiments, which show that without quota limitation, the collected sets of annotations have both high level of agreement (0.769) calculated with the Kappa statistic.", "labels": [], "entities": [{"text": "agreement", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.971286952495575}]}, {"text": "We used the rich set of features described in the state-of-the-art QA system (.", "labels": [], "entities": []}, {"text": "Such features are based on the similarity between question and the passage text: N-gram overlap (e.g., word lemmas, bi-gram, part-of-speech tags and etc.), tree kernel similarity, relatedness between question category and the related named entity types extracted from the candidate answer, LDA similarity between the topic distributions of question and answer passage.", "labels": [], "entities": [{"text": "LDA similarity", "start_pos": 290, "end_pos": 304, "type": "METRIC", "confidence": 0.9691787362098694}]}, {"text": "In these experiments, we used the labels provided by crowd annotators using majority voting for training and testing our models.", "labels": [], "entities": []}, {"text": "Most interestingly, we also assign weights to the examples in SVMs with the weighting schemes below: -Labels Only (L), i.e., we set P (u) = T (u) = C(u) = 1 in Eq.", "labels": [], "entities": []}, {"text": "2. This means that the instance weight (Eq. 1) is just the sum of the labels j hp . -Prior Only (P): to study the impact of prior annotation skills, we set C(u) = T (u) = 1 in Eq.", "labels": [], "entities": []}, {"text": "2, and we only use P (u) (crowdflower trust), i.e., we do not account for the sign of annotations, j hp . -Labels & Prior (LP): the previous model but we also used the sign of the label, j hp . -Task & Consistency (TC): we set P (u) = 1 such that Eq.", "labels": [], "entities": []}, {"text": "2 takes into account both annotator skill parameters for the specific task, i.e., task and consistency confidence, but only in the current task and no sign of j hp . -L & TC (LTC): same as before but we also take into account the sign of the annotator decision.", "labels": [], "entities": [{"text": "consistency confidence", "start_pos": 91, "end_pos": 113, "type": "METRIC", "confidence": 0.960063248872757}]}, {"text": "-LPTC: all parameters are used.", "labels": [], "entities": [{"text": "LPTC", "start_pos": 1, "end_pos": 5, "type": "DATASET", "confidence": 0.9091334342956543}]}, {"text": "shows the evaluation of the different baselines and weighting schemes proposed in this paper (using the default c parameter of SVMs).", "labels": [], "entities": []}, {"text": "We note that: firstly, the accuracy of BM25 is lower than the one expressed by rerankers trained on noisy labels (-15.66% in MRR, -14.5% in MAP, -15.81 in P@1%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9998082518577576}, {"text": "BM25", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.782941997051239}, {"text": "MRR", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.5017750859260559}]}, {"text": "Secondly, although there is some improvement using crowd annotations for training 5 compared to the noisy training labels (RE), the improvement is not significant (+0.34% in MRR, +0.34% in MAP, +1.56% in P@1).", "labels": [], "entities": [{"text": "MRR", "start_pos": 174, "end_pos": 177, "type": "METRIC", "confidence": 0.6974250674247742}, {"text": "MAP", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.8181980848312378}]}, {"text": "This is due to three reasons: (i) the crowdsourcing annotation suffers from a certain level of noise as well (only 27,350 of the answer passages, i.e., 51.80%, are labeled with \"crowd fully in agreement\"), (ii) although the RE labels may generate several false positives, these are always a small percentage of the total instances as the dataset is highly unbalanced (9,535 negative vs. 1,005 positive examples); and (iii) RE do not generate many false negatives as they are precise.", "labels": [], "entities": [{"text": "RE", "start_pos": 423, "end_pos": 425, "type": "METRIC", "confidence": 0.7747374176979065}]}, {"text": "Thirdly, the table clearly shows the intuitive fact that it is always better to take into account the sign of the label given by the annotator, i.e., LP vs. Land LTC vs. TC.", "labels": [], "entities": []}, {"text": "Next, when we apply our different weighting schema, we observe that the noise introduced by the crowd annotation can be significantly reduced as the classifier improves by +1.47% in MRR, +0.54% in MAP and +1.85% in P@1, e.g., when using LTC & LPTC compared to CA, which does not provide any weight to the reranker.", "labels": [], "entities": [{"text": "MRR", "start_pos": 182, "end_pos": 185, "type": "METRIC", "confidence": 0.8793774247169495}, {"text": "MAP", "start_pos": 197, "end_pos": 200, "type": "METRIC", "confidence": 0.9576164484024048}]}, {"text": "Finally, as the trade-off parameter, c, may alone mitigate the noise problem, we compared our models with the baselines according to several value of the parameter.", "labels": [], "entities": []}, {"text": "plots the rank measures averaged over 5-folds: our weighting methods, especially LPTC (black curve), is constantly better than the baseline, CA, (blue curve) in MRR and P@1.", "labels": [], "entities": [{"text": "CA", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.968719482421875}, {"text": "MRR", "start_pos": 161, "end_pos": 164, "type": "DATASET", "confidence": 0.45353808999061584}]}], "tableCaptions": [{"text": " Table 1: Results over 5 fold cross validation. Our Weight-", "labels": [], "entities": []}]}