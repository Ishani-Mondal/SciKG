{"title": [], "abstractContent": [{"text": "Learning embeddings of words and knowledge base elements is a promising approach for open domain question answering.", "labels": [], "entities": [{"text": "open domain question answering", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.7004246115684509}]}, {"text": "Based on the remark that relations and entities are distinct object types lying in the same embedding space, we analyze the benefit of adding a regularizer favoring the embeddings of entities to be orthogonal to those of relations.", "labels": [], "entities": []}, {"text": "The main motivation comes from the observation that modifying the embeddings using prior knowledge often helps performance.", "labels": [], "entities": []}, {"text": "The experiments show that incorporating the regularizer yields better results on a challenging question answering benchmark.", "labels": [], "entities": [{"text": "question answering", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7079464942216873}]}], "introductionContent": [{"text": "Having a system which is able to answer questions based on a structured knowledge base is a challenging problem.", "labels": [], "entities": []}, {"text": "The problem has been addressed recently by researchers working on large knowledge bases such as) and Freebase (.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 101, "end_pos": 109, "type": "DATASET", "confidence": 0.9754443764686584}]}, {"text": "The creation of question answering (QA) benchmarks for these knowledge bases (KB) has a significant impact on the domain, as shown by the number of QA systems recently proposed in the literature.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.8426444351673126}]}, {"text": "We identify two types of approaches for KBcentric QA systems: parsing-based approaches and information retrieval (IR) based approaches.", "labels": [], "entities": [{"text": "KBcentric QA", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.7918280363082886}]}, {"text": "Parsing-based approaches) answer factoid questions by learning a structured representation for the sentences, called logical form.", "labels": [], "entities": []}, {"text": "This logical form is then used to query the knowledge base and retrieve the answer.", "labels": [], "entities": []}, {"text": "IR-based approaches try to identify the best possible match between the knowledge base and the question (.", "labels": [], "entities": []}, {"text": "In this work, we focus on the second approach, using embedding models, mainly because it is robust to invalid syntax and can exploit information of the answer.", "labels": [], "entities": []}, {"text": "We focus on the) dataset constructed for Reverb.", "labels": [], "entities": [{"text": "Reverb", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.8180487155914307}]}, {"text": "On Wikianswers, the underlying semantics is very simple (just one single triple).", "labels": [], "entities": []}, {"text": "However, the task remains challenging due to the large variety of lexicalizations for the same semantics.", "labels": [], "entities": []}, {"text": "We follow the approach of Bordes et .al (2014b) which learns the embeddings of words and KB elements.", "labels": [], "entities": []}, {"text": "They model the semantics of natural language sentences and KB triples as the sum of the embeddings of the associated words and KB elements respectively.", "labels": [], "entities": []}, {"text": "Despite its simplicity, this model performs surprisingly well in practice.", "labels": [], "entities": []}, {"text": "Something even more interesting () is that the system can have a good performance even without using a paraphrase corpus.", "labels": [], "entities": []}, {"text": "This makes the system very attractive in practice because in many specific domains, we might have a KB but there maybe no paraphrase corpus as in Wikianswers.", "labels": [], "entities": []}, {"text": "In our work, we push the results further when learning a QA system based only on the KB.", "labels": [], "entities": []}, {"text": "Our contribution is to introduce anew orthogonality regularizer which distinguishes entities and relations.", "labels": [], "entities": []}, {"text": "We also investigate the tradeoff captured by the orthogonality constraints.", "labels": [], "entities": []}, {"text": "With a synthetic example, we show that if entities and relations are independent, orthogonal embeddings generate better results.", "labels": [], "entities": []}, {"text": "The orthogonality constraint in the context of question answering is new, although it has been successfully used in other contexts ( ).", "labels": [], "entities": [{"text": "question answering", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8490289151668549}]}, {"text": "Like (), we use al-most no linguistic features such as POS tagging, parsing, etc.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.6913769096136093}, {"text": "parsing", "start_pos": 68, "end_pos": 75, "type": "TASK", "confidence": 0.9172677397727966}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Performance for re-ranking question answer pairs", "labels": [], "entities": [{"text": "re-ranking question answer pairs", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.7886176258325577}]}]}