{"title": [{"text": "SLS at SemEval-2016 Task 3: Neural-based Approaches for Ranking in Community Question Answering", "labels": [], "entities": [{"text": "Ranking in Community Question Answering", "start_pos": 56, "end_pos": 95, "type": "TASK", "confidence": 0.6172499001026154}]}], "abstractContent": [{"text": "Community question answering platforms need to automatically rank answers and questions with respect to a given question.", "labels": [], "entities": [{"text": "question answering", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6782177686691284}]}, {"text": "In this paper, we present the approaches for the Answer Selection and Question Retrieval tasks of SemEval-2016 (task 3).", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.9355590343475342}, {"text": "Question Retrieval tasks", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.818159262339274}]}, {"text": "We develop a bag-of-vectors approach with various vector-and text-based features, and different neu-ral network approaches including CNNs and LSTMs to capture the semantic similarity between questions and answers for ranking purpose.", "labels": [], "entities": []}, {"text": "Our evaluation demonstrates that our approaches significantly outperform the base-lines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Community Question Answering (cQA) forums are rapidly growing, resulting in an urgent need to automatically search for relevant answers among many responses provided fora given question (Answer Selection), and search for relevant questions to reuse their existing answers.", "labels": [], "entities": [{"text": "Community Question Answering (cQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7416171431541443}, {"text": "Answer Selection)", "start_pos": 187, "end_pos": 204, "type": "TASK", "confidence": 0.6856275796890259}]}, {"text": "In this paper, we aim to address the SemEval 2016 tasks () that are designed for Answer Selection (AS) and Question Retrieval (QR).", "labels": [], "entities": [{"text": "SemEval 2016 tasks", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7020144065221151}, {"text": "Answer Selection (AS)", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.8976193428039551}, {"text": "Question Retrieval (QR)", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.8386224985122681}]}, {"text": "These tasks are briefly described as follows: A Question-Comment Similarity: given a question and its first 10 comments in the question thread, rerank these 10 comments according to their relevance with respect to the question.", "labels": [], "entities": []}, {"text": "Task B is considered as QR and the others as AS problems.", "labels": [], "entities": [{"text": "AS", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.8306678533554077}]}, {"text": "The first three tasks are evaluated on an English dataset and the fourth on an Arabic dataset.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.8184385597705841}, {"text": "Arabic dataset", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.685014396905899}]}, {"text": "Several factors make all these tasks more challenging.", "labels": [], "entities": []}, {"text": "First, cQA forums contain open-domain and non-factoid questions and answers, resulting in high variance Q&A quality.", "labels": [], "entities": []}, {"text": "A second factor is that the Q&A are long and their length may vary from several words to several hundred words.", "labels": [], "entities": [{"text": "length", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9561213254928589}]}, {"text": "The third factor concerns the relatively close relation between some annotation labels; the comments in the tasks A and C are labeled as Relevant, Potential and Irrelevant, and the Relevant comments need to be ranked above the Potential and Irrelevant comments.", "labels": [], "entities": []}, {"text": "From a natural language processing perspective, it is difficult to define a clear distinction between the relevant and potential labels.", "labels": [], "entities": []}, {"text": "To address these tasks, we first present a bag-ofvectors (BOV) approach in which various vectorand text-based features are designed and passed through a linear SVM classifier to compute the degree of relatedness between the Q&As.", "labels": [], "entities": []}, {"text": "Then, we present different NN-based approaches including CNNs and LSTMs to compute the representations of the Q&As.", "labels": [], "entities": []}, {"text": "We evaluate our models on the cQA corpus provided by SemEval.", "labels": [], "entities": [{"text": "cQA corpus provided by SemEval", "start_pos": 30, "end_pos": 60, "type": "DATASET", "confidence": 0.9253052830696106}]}, {"text": "The results demonstrate that our approaches outperform the baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approaches on all the cQA tasks.", "labels": [], "entities": []}, {"text": "We use the cQA datasets provided by SemEval 2016.", "labels": [], "entities": [{"text": "cQA datasets provided by SemEval 2016", "start_pos": 11, "end_pos": 48, "type": "DATASET", "confidence": 0.8292302787303925}]}, {"text": "The English data were collected from the Qatar Living forum. and the Arabic data were collected from medical forums.", "labels": [], "entities": [{"text": "Qatar Living forum.", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.9587685068448385}]}, {"text": "Baselines For a baseline, we use the Information Retrieval (IR) ranking score that is computed as follows: given a q, the top 100 threads retrieved by Google from the Qatar Living forum are considered and the order of each thread is used as its IR ranking score.", "labels": [], "entities": [{"text": "Qatar Living forum", "start_pos": 167, "end_pos": 185, "type": "DATASET", "confidence": 0.8950836062431335}, {"text": "IR ranking score", "start_pos": 245, "end_pos": 261, "type": "METRIC", "confidence": 0.7864181399345398}]}, {"text": "As another baseline, we use a system that randomly ranks a given list of Q or A.", "labels": [], "entities": []}, {"text": "Question-comment similarity The results for this task are shown in(a).", "labels": [], "entities": []}, {"text": "The first two rows are the IR and random baseline results, and the next two rows are the best two performances among all SemEval submissions for this task.", "labels": [], "entities": [{"text": "IR", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.6307371258735657}, {"text": "SemEval submissions", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.900335967540741}]}, {"text": "The other three rows are the results of our approaches and respectively submitted to SemEval as primary, con-   trastive1 and contrastive2 results.", "labels": [], "entities": []}, {"text": "As shown in the table, our results are significantly higher than the baselines and comparable with the best results overall performance metrics, and there is no significant difference between the results of our approaches for this task.", "labels": [], "entities": []}, {"text": "We use various combinations of our BOV, LSTM and CNN approaches, then select the best ones with respect to the development set.", "labels": [], "entities": [{"text": "BOV", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.6384715437889099}]}, {"text": "The combination of the approaches is computed by 1/(R 1 + R 2 + ...", "labels": [], "entities": []}, {"text": "+ R i ) where R i is the ranking of the i th approach.", "labels": [], "entities": []}, {"text": "In this task, the BOV includes all the features except for the NMF features, and we employ the order of the answers in their threads as augmented features for our NN-based approaches.", "labels": [], "entities": [{"text": "BOV", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.8478012084960938}]}, {"text": "The structure of the threads (e.g., answer order) can help to extract relevant answers).(b) shows the results for this task.", "labels": [], "entities": []}, {"text": "The first two rows are the results for IR and random baselines, and the next two are the best two performances of SemEval.", "labels": [], "entities": [{"text": "IR", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9033686518669128}]}, {"text": "The other three results are related to our approaches and respectively submitted to SemEval as primary, contrastive1 and contrastive2 results.", "labels": [], "entities": []}, {"text": "The table shows that our results are significantly better than the baselines.", "labels": [], "entities": []}, {"text": "While there is no significant difference between our contrastive1 and contrastive2 results with the best result, these results are higher than the second best SemEval result on MAP, and the highest result is obtained with our primary result on accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 244, "end_pos": 252, "type": "METRIC", "confidence": 0.9984782338142395}]}, {"text": "With respect to our results, the combination of BOV, LSTM and RCNN achieves the highest result on MAP and the combination of BOV and RCNN is the best on F1.", "labels": [], "entities": [{"text": "BOV", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9897769689559937}, {"text": "MAP", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.4980594515800476}, {"text": "BOV", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9585551023483276}, {"text": "F1", "start_pos": 153, "end_pos": 155, "type": "DATASET", "confidence": 0.7731436491012573}]}], "tableCaptions": [{"text": " Table 1: Hyper-parameters of the LSTM model. The bold value", "labels": [], "entities": []}, {"text": " Table 2: The hyper-parameters of RCNN model.", "labels": [], "entities": [{"text": "RCNN", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.8791948556900024}]}, {"text": " Table 3: The hyper-parameters of CNN model.", "labels": [], "entities": [{"text": "CNN model", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.8961964547634125}]}, {"text": " Table 4: Statistics of the dataset through the tasks.", "labels": [], "entities": []}, {"text": " Table 5: Results on test data for answer selection and question retrieval tasks", "labels": [], "entities": [{"text": "answer selection", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.8736579716205597}, {"text": "question retrieval", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7836288511753082}]}]}