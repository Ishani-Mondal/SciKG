{"title": [{"text": "DTSim at SemEval-2016 Task 2: Interpreting Similarity of Texts Based on Automated Chunking, Chunk Alignment and Semantic Relation Prediction", "labels": [], "entities": [{"text": "Semantic Relation Prediction", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.7780581414699554}]}], "abstractContent": [{"text": "In this paper we describe our system (DTSim) submitted at SemEval-2016 Task 2: Inter-pretable Semantic Textual Similarity (iSTS).", "labels": [], "entities": [{"text": "SemEval-2016 Task 2: Inter-pretable Semantic Textual Similarity (iSTS)", "start_pos": 58, "end_pos": 128, "type": "TASK", "confidence": 0.6847325590523806}]}, {"text": "We participated in both gold chunks category (texts chunked by human experts and provided by the task organizers) and system chunks category (participants had to automatically chunk the input texts).", "labels": [], "entities": []}, {"text": "We developed a Conditional Random Fields based chunker and applied rules blended with semantic similarity methods in order to predict chunk alignments , alignment types and similarity scores.", "labels": [], "entities": []}, {"text": "Our system obtained F1 score up to 0.648 in predicting the chunk alignment types and scores together and was one of the top performing systems overall.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9830461144447327}]}], "introductionContent": [{"text": "Measuring the semantic similarity of texts is to quantify the degree of semantic similarity between a given pair of texts, such as two words or two sentences (.", "labels": [], "entities": []}, {"text": "For example, a similarity score of 0 means that the texts are not similar at all and 5 means that they have same meaning.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 15, "end_pos": 31, "type": "METRIC", "confidence": 0.9610994458198547}]}, {"text": "While useful, such quantitative or even qualitative assessments are hard to interpret because they do not provide details, i.e. they do not explain or justify why the similarity score was assigned high or low.", "labels": [], "entities": []}, {"text": "One way to provide an explanatory layer to text similarity assessment methods is to align chunks between texts and assigning semantic relation to each alignment.", "labels": [], "entities": [{"text": "text similarity assessment", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.7865157922108968}]}, {"text": "To this end, and produced datasets * * These authors contributed equally to this work where corresponding words (or multiword expressions) were aligned and in the latter case their semantic relations were explicitly labeled In interpretable Semantic Textual Similarity (iSTS) tasks, participants had first to identify the chunks in each sentence (sys chunks) or use the chunks given by the task organizers (gold chunks), and then, align chunks across the two sentences indicating the semantic relation and similarity score of each alignment.", "labels": [], "entities": [{"text": "interpretable Semantic Textual Similarity (iSTS) tasks", "start_pos": 227, "end_pos": 281, "type": "TASK", "confidence": 0.7303139492869377}]}, {"text": "The chunk alignment types were EQUI (semantically equivalent), OPPO (opposite in meaning), SPE (one chunk is more specific than other), SIMI (similar meanings, but not EQUI, OPPO, SPE), REL (related meanings, but not SIMI, EQUI, OPPO, SPE), and NOALI (has no corresponding chunk in the other sentence).", "labels": [], "entities": [{"text": "REL", "start_pos": 186, "end_pos": 189, "type": "METRIC", "confidence": 0.9863394498825073}, {"text": "NOALI", "start_pos": 245, "end_pos": 250, "type": "METRIC", "confidence": 0.965117335319519}]}, {"text": "The relatedness/similarity scores were assigned in the range of 0 to 5.", "labels": [], "entities": [{"text": "similarity", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.8566541075706482}]}, {"text": "A pilot on iSTS task was organized in 2015 (.", "labels": [], "entities": []}, {"text": "In 2016, the iSTS allowed many-to-many chunk alignment while in the pilot task of 2015 they only allowed one-to-one or no alignment.", "labels": [], "entities": []}, {"text": "Also, anew dataset consisting of student answers given to a tutoring system was added in 2016.", "labels": [], "entities": []}, {"text": "For further details about the task, please see.", "labels": [], "entities": []}, {"text": "We participated in both categories: system chunks and gold chunks.", "labels": [], "entities": []}, {"text": "Our system first preprocesses texts, creates chunks (in sys chunks category) using our own chunking tool, and performs alignments and labels them with semantic relations and similarity scores.", "labels": [], "entities": []}, {"text": "In this paper, we describe our system DTSim 1 and the submitted three different runs in the 1 Available for download from http://semanticsimilarity.org 809 shared task.", "labels": [], "entities": []}, {"text": "Our system was one of the top performing systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "Chunker We evaluated the chunking accuracy of the CRF chunker by comparing its output against the gold chunks of iSTS 2015 data: the training and test data sets each consist of 375 pairs of Images annotation data and 378 pairs of Headlines texts.", "labels": [], "entities": [{"text": "Chunker", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9354024529457092}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9062239527702332}, {"text": "iSTS 2015 data", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.9627788464228312}, {"text": "Headlines texts", "start_pos": 230, "end_pos": 245, "type": "DATASET", "confidence": 0.920007586479187}]}, {"text": "This chunker yielded the highest average accuracies on both the training and test datasets compared to other chunkers which are described next.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9796603322029114}]}, {"text": "The accuracies on the training dataset were 86.20% and 68.34% at chunk level and sentence level respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9868482947349548}]}, {"text": "For the test dataset, the accuracies were 86.81% and 69% at chunk and sentence level, respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.995121419429779}]}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "We also chunked the input texts using the Open-NLP chunking tool (O-NLP).", "labels": [], "entities": []}, {"text": "The average (of Images and Headlines data) accuracies were 53.04% at chunk level and a modest 9.27% at sentence level for the training dataset.", "labels": [], "entities": [{"text": "Images and Headlines data)", "start_pos": 16, "end_pos": 42, "type": "DATASET", "confidence": 0.7169212162494659}, {"text": "accuracies", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.7531237006187439}]}, {"text": "It yielded similar results on test data.", "labels": [], "entities": []}, {"text": "We extended Open-NLP chunker (EO-NLP) using the rules described before.", "labels": [], "entities": []}, {"text": "The results were improved and resulted in 84.% chunk level 810 and 66.02% sentence level average accuracies on the training dataset, respectively.", "labels": [], "entities": [{"text": "chunk level 810", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.8954803148905436}, {"text": "sentence level average accuracies", "start_pos": 74, "end_pos": 107, "type": "METRIC", "confidence": 0.8097462505102158}]}, {"text": "The accuracy on the test data was comparable at 85.13% chunk level and 66.15% at sentence level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996856451034546}]}, {"text": "However, the results of our CRF based chunker were superior in all cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of chunking accuracies of the various", "labels": [], "entities": []}, {"text": " Table 2: F1 scores on test data with gold chunks. A, T and S", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9995222091674805}, {"text": "A", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9478340148925781}]}, {"text": " Table 3: Results on test data with system chunks.", "labels": [], "entities": []}]}