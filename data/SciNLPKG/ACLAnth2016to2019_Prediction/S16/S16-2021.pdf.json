{"title": [{"text": "Detecting Stance in Tweets And Analyzing its Interaction with Sentiment", "labels": [], "entities": [{"text": "Detecting Stance in Tweets", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9311052560806274}, {"text": "Sentiment", "start_pos": 62, "end_pos": 71, "type": "TASK", "confidence": 0.7790257334709167}]}], "abstractContent": [{"text": "One may express favor (or disfavor) towards a target by using positive or negative language.", "labels": [], "entities": []}, {"text": "Here for the first time we present a dataset of tweets annotated for whether the tweeter is in favor of or against pre-chosen targets, as well as for sentiment.", "labels": [], "entities": []}, {"text": "These targets mayor may not be referred to in the tweets, and they mayor may not be the target of opinion in the tweets.", "labels": [], "entities": []}, {"text": "We develop a simple stance detection system that outper-forms all 19 teams that participated in a recent shared task competition on the same dataset (SemEval-2016 Task #6).", "labels": [], "entities": [{"text": "stance detection", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.9230737090110779}]}, {"text": "Additionally , access to both stance and sentiment annotations allows us to conduct several experiments to tease out their interactions.", "labels": [], "entities": []}, {"text": "We show that while sentiment features are useful for stance classification, they alone are not sufficient.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.9421728849411011}]}, {"text": "We also show the impacts of various features on detecting stance and sentiment, respectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Stance detection is the task of automatically determining from text whether the author of the text is in favor of, against, or neutral towards a proposition or target.", "labels": [], "entities": [{"text": "Stance detection is the task of automatically determining from text whether the author of the text is in favor of, against, or neutral towards a proposition or target", "start_pos": 0, "end_pos": 166, "type": "Description", "confidence": 0.7510074605544408}]}, {"text": "The target maybe a person, an organization, a government policy, a movement, a product, etc.", "labels": [], "entities": []}, {"text": "For example, one can infer from Barack Obama's speeches that he is in favor of stricter gun laws in the US.", "labels": [], "entities": []}, {"text": "Similarly, people often express stance towards various target entities through posts on online forums, blogs, Twitter, Youtube, Instagram, etc.", "labels": [], "entities": []}, {"text": "Automatically detecting stance has widespread applications in information retrieval, text summarization, and textual entailment.", "labels": [], "entities": [{"text": "Automatically detecting stance", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8110659122467041}, {"text": "information retrieval", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.8157524466514587}, {"text": "text summarization", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.7935324311256409}, {"text": "textual entailment", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.797721266746521}]}, {"text": "Over the last decade, there has been active research in modeling stance.", "labels": [], "entities": [{"text": "modeling stance", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.8523822724819183}]}, {"text": "However, most work focuses on congressional debates () or debates in online forums.", "labels": [], "entities": []}, {"text": "Here we explore the task of detecting stance in Twitter-a popular microblogging platform where people often express stance implicitly or explicitly.", "labels": [], "entities": []}, {"text": "The task we explore is formulated as follows: given a tweet text and a target entity (person, organization, movement, policy, etc.), automatic natural language systems must determine whether the tweeter is in favor of the given target, against the given target, or whether neither inference is likely.", "labels": [], "entities": []}, {"text": "For example, consider the target-tweet pair: Target: legalization of abortion (1) Tweet: The pregnant are more than walking incubators, and have rights!", "labels": [], "entities": []}, {"text": "Humans can deduce from the tweet that the tweeter is likely in favor of the target.", "labels": [], "entities": []}, {"text": "1 Note that lack of evidence for 'favor' or 'against', does not imply that the tweeter is neutral towards the target.", "labels": [], "entities": []}, {"text": "It may just mean that we cannot deduce stance from the tweet.", "labels": [], "entities": []}, {"text": "In fact, this is a common phenomenon.", "labels": [], "entities": []}, {"text": "On the other hand, the number of tweets from which we can infer neutral stance is expected to be small.", "labels": [], "entities": []}, {"text": "An example is shown below: Target: Hillary Clinton (2) Tweet: Hillary Clinton has some strengths and some weaknesses.", "labels": [], "entities": []}, {"text": "Stance detection is related to, but different from, sentiment analysis.", "labels": [], "entities": [{"text": "Stance detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9623213112354279}, {"text": "sentiment analysis", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9419254064559937}]}, {"text": "Sentiment analysis tasks are formulated as determining whether apiece of text is positive, negative, or neutral, or determining from text the speaker's opinion and the target of the opinion (the entity towards which opinion is expressed).", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9386067688465118}]}, {"text": "However, instance detection, systems are to determine favorability towards a given (prechosen) target of interest.", "labels": [], "entities": [{"text": "instance detection", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.8514506220817566}]}, {"text": "The target of interest may not be explicitly mentioned in the text and it may not be the target of opinion in the text.", "labels": [], "entities": []}, {"text": "For example, consider the target-tweet pair below: Target: Donald Trump (3) Tweet: Jeb Bush is the only sane candidate in this republican lineup.", "labels": [], "entities": []}, {"text": "The target of opinion in the tweet is Jeb Bush, but the given target of interest is Donald Trump.", "labels": [], "entities": []}, {"text": "Nonetheless, we can infer that the tweeter is likely to be unfavorable towards Donald Trump.", "labels": [], "entities": []}, {"text": "Also note that, instance detection, the target can be expressed in different ways which impacts whether the instance is labeled 'favor' or 'against'.", "labels": [], "entities": [{"text": "instance detection", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7579802572727203}]}, {"text": "For example, the target in example 1 could have been phrased as 'pro-life movement', in which case the correct label for that instance is 'against'.", "labels": [], "entities": []}, {"text": "Also, the same stance (favor or against) towards a given target can be deduced from positive tweets and negative tweets.", "labels": [], "entities": []}, {"text": "This interaction between sentiment and stance has not been adequately addressed in past work, and an important reason for this is the lack of a dataset annotated for both stance and sentiment.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: (1) We create the first tweets dataset labeled for stance, target of opinion, and sentiment.", "labels": [], "entities": []}, {"text": "More than 4,000 tweets are annotated for whether one can deduce favorable or unfavorable stance towards one of five targets 'Atheism', 'Climate Change is a Real Concern', 'Feminist Movement', 'Hillary Clinton', and 'Legalization of Abortion'.", "labels": [], "entities": [{"text": "Legalization of Abortion", "start_pos": 216, "end_pos": 240, "type": "TASK", "confidence": 0.8667790293693542}]}, {"text": "Each of these tweets is also annotated for whether the target of opinion expressed in the tweet is the same as the given target of interest.", "labels": [], "entities": []}, {"text": "Finally, each tweet is annotated for whether it conveys positive, negative, or neutral sentiment.", "labels": [], "entities": []}, {"text": "(2) Partitions of this stance-annotated data were used as training and test sets in the SemEval-2016 shared task competition 'Task #6: Detecting Stance in Tweets' ().", "labels": [], "entities": [{"text": "SemEval-2016 shared task competition", "start_pos": 88, "end_pos": 124, "type": "TASK", "confidence": 0.8022356778383255}, {"text": "Detecting Stance in Tweets'", "start_pos": 135, "end_pos": 162, "type": "TASK", "confidence": 0.9040958285331726}]}, {"text": "Participants were provided with 2,914 training instances labeled for stance for the five targets.", "labels": [], "entities": []}, {"text": "The test data included 1,249 instances.", "labels": [], "entities": []}, {"text": "The task received submissions from 19 teams.", "labels": [], "entities": []}, {"text": "The best performing system obtained an overall average Fscore of 67.82.", "labels": [], "entities": [{"text": "average", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9395921230316162}, {"text": "Fscore", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9554753303527832}]}, {"text": "Their approach employed two recurrent neural network (RNN) classifiers: the first was trained to predict task-relevant hashtags on a very large unlabeled Twitter corpus.", "labels": [], "entities": []}, {"text": "This network was used to initialize a second RNN classifier, which was trained with the provided training data.", "labels": [], "entities": []}, {"text": "(3) We propose a stance detection system that is much simpler than the SemEval-2016 Task #6 winning system (described above), and yet obtains an even better F-score of 70.32 on the shared task's test set.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.9496713280677795}, {"text": "SemEval-2016 Task #6 winning", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.5250116288661957}, {"text": "F-score", "start_pos": 157, "end_pos": 164, "type": "METRIC", "confidence": 0.9986822009086609}]}, {"text": "We use a linear-kernel SVM classifier that relies on features drawn from the training instances-such as word and character ngrams-as well as those obtained using external resources-such as sentiment features from lexicons and word-embedding features from additional unlabeled data.", "labels": [], "entities": []}, {"text": "(4) We conduct experiments to better understand the interaction between stance and sentiment and the factors influencing their interaction.", "labels": [], "entities": []}, {"text": "We use the gold labels to determine the extent to which stance can be determined simply from sentiment.", "labels": [], "entities": []}, {"text": "We apply the stance detection system (mentioned above in), as a common text classification framework, to determine both stance and sentiment.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.8797033727169037}]}, {"text": "Results show that while sentiment features are substantially useful for sentiment classification, they are not as effective for stance classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.9768508076667786}, {"text": "stance classification", "start_pos": 128, "end_pos": 149, "type": "TASK", "confidence": 0.9418376088142395}]}, {"text": "Word embeddings improve the performance of both stance and sentiment classifiers.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.675288736820221}]}, {"text": "Further, even though both stance and sentiment detection are framed as three-way classification tasks on a common dataset where the majority class baselines are similar, automatic systems perform markedly better when detecting sentiment than when detecting stance towards a given target.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8649022281169891}]}, {"text": "Finally, we show that stance detection towards the target of interest is particularly challenging when the tweeter expresses opinion about an entity other than the target of interest.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.908545047044754}]}, {"text": "In fact, the text classification system performs close to majority baseline for such instances.", "labels": [], "entities": [{"text": "text classification", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7577723562717438}]}, {"text": "All of the stance data, including annotations for target of opinion and sentiment, are made freely available through the shared task website and the homepage for this Stance Project.: Examples of stance-indicative and stance-ambiguous hashtags that were manually identified.", "labels": [], "entities": [{"text": "this Stance Project.", "start_pos": 162, "end_pos": 182, "type": "DATASET", "confidence": 0.782433827718099}]}], "datasetContent": [{"text": "The stance annotations we use are described in detail in.", "labels": [], "entities": []}, {"text": "We summarize below how we compiled a set of tweets and targets for stance annotation, the questionnaire and crowdsourcing setup used for stance annotation, and an analysis of the stance annotations.", "labels": [], "entities": [{"text": "stance annotation", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.8909964859485626}, {"text": "stance annotation", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.9241528511047363}]}, {"text": "We first identified a list of target entities that were commonly known in the United States and also topics of debate: 'Atheism', 'Climate Change is a Real Concern\", 'Feminist Movement', 'Hillary Clinton', and 'Legalization of Abortion'.", "labels": [], "entities": [{"text": "Legalization of Abortion", "start_pos": 211, "end_pos": 235, "type": "TASK", "confidence": 0.9094573060671488}]}, {"text": "Next, we compiled a small list of hashtags, which we will call query hashtags, that people use when tweeting about the targets.", "labels": [], "entities": []}, {"text": "We split these hashtags into three categories: (1) favor hashtags: expected to occur in tweets expressing favorable stance towards the target (for example, #Hillary4President), (2) against hashtags: expected to occur in tweets expressing opposition to the target (for example, #HillNo), and (3) stanceambiguous hashtags: expected to occur in tweets about the target, but are not explicitly indicative of stance (for example, #Hillary2016).", "labels": [], "entities": []}, {"text": "lists examples of hashtags used for each of the targets.", "labels": [], "entities": []}, {"text": "Next, we polled the Twitter API to collect close to 2 million tweets containing these hashtags (query hashtags).", "labels": [], "entities": []}, {"text": "We discarded retweets and tweets with URLs.", "labels": [], "entities": []}, {"text": "We kept only those tweets where the query hashtags appeared at the end.", "labels": [], "entities": []}, {"text": "This reduced the number of tweets to about 1.7 million.", "labels": [], "entities": []}, {"text": "We removed the query hashtags from the tweets to exclude obvious cues for the classification task.", "labels": [], "entities": [{"text": "classification task", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8921958208084106}]}, {"text": "Since we only select tweets that have the query hashtag at the end, removing them from the tweet often still results in text that is understandable and grammatical.", "labels": [], "entities": []}, {"text": "Note that the presence of a stance-indicative hashtag is not a guarantee that the tweet will have the same stance.", "labels": [], "entities": []}, {"text": "Further, removal of query hash-3 A tweet that has a seemingly favorable hashtag may in tags may result in a tweet that no longer expresses the same stance as with the query hashtag.", "labels": [], "entities": []}, {"text": "Thus we manually annotate the tweet-target pairs after the pre-processing described above.", "labels": [], "entities": []}, {"text": "For each target, we sampled an equal number of tweets pertaining to the favor hashtags, the against hashtags, and the stance-ambiguous hashtags.", "labels": [], "entities": []}, {"text": "This helps in obtaining a sufficient number of tweets pertaining to each of the stance categories.", "labels": [], "entities": []}, {"text": "Note that removing the query hashtag can sometimes result in tweets that do not explicitly mention the target.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Distribution of instances in the Stance Train and Test sets for Question 1 (Stance).", "labels": [], "entities": [{"text": "Stance Train and Test sets", "start_pos": 43, "end_pos": 69, "type": "DATASET", "confidence": 0.8996592283248901}]}, {"text": " Table 3: Distribution of instances in the Stance  dataset for Question 2 (Target of Opinion).", "labels": [], "entities": [{"text": "Stance  dataset", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.8705862164497375}]}, {"text": " Table 4: Distribution of target of opinion across  stance labels.", "labels": [], "entities": [{"text": "Distribution of target of opinion", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.855299997329712}]}, {"text": " Table 5: Distribution of sentiment in the Stance Train and Test sets.", "labels": [], "entities": [{"text": "Distribution of sentiment", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.9163733919461569}, {"text": "Stance Train and Test sets", "start_pos": 43, "end_pos": 69, "type": "DATASET", "confidence": 0.9333216667175293}]}, {"text": " Table 6: Stance Classification: Results obtained  by automatic systems.", "labels": [], "entities": [{"text": "Stance Classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9288731217384338}]}, {"text": " Table 7: Stance Classification: F-scores obtained for each of the targets (the columns) when one or more  of the feature groups are added. Highest scores in each column is shown in bold.", "labels": [], "entities": [{"text": "Stance Classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7508343160152435}, {"text": "F-scores", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9981186985969543}]}, {"text": " Table 8: Stance Classification: F-scores obtained  for tweets with opinion towards the target and  tweets with opinion towards another entity.", "labels": [], "entities": [{"text": "Stance Classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6198136210441589}, {"text": "F-scores", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9981786012649536}]}, {"text": " Table 9: Sentiment Classification: Results ob- tained by automatic systems.", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9664161205291748}]}]}