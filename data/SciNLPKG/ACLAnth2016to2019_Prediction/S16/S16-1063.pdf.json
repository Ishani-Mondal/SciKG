{"title": [{"text": "USFD at SemEval-2016 Task 6: Any-Target Stance Detection on Twitter with Autoencoders", "labels": [], "entities": [{"text": "USFD at SemEval-2016 Task 6", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.7383605003356933}, {"text": "Any-Target Stance Detection", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.7197551329930624}]}], "abstractContent": [{"text": "This paper describes the University of Sheffield's submission to the SemEval 2016 Twitter Stance Detection weakly supervised task (SemEval 2016 Task 6, Subtask B).", "labels": [], "entities": [{"text": "SemEval 2016 Twitter Stance Detection weakly supervised task", "start_pos": 69, "end_pos": 129, "type": "TASK", "confidence": 0.790664441883564}]}, {"text": "In stance detection, the goal is to classify the stance of a tweet towards a target as \"favor\", \"against\", or \"none\".", "labels": [], "entities": [{"text": "stance detection", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.9028005599975586}]}, {"text": "In Subtask B, the targets in the test data are different from the targets in the training data, thus rendering the task more challenging but also more realistic.", "labels": [], "entities": []}, {"text": "To address the lack of target-specific training data, we use a large set of unlabelled tweets containing all targets and train a bag-of-words autoencoder to learn how to produce feature representations of tweets.", "labels": [], "entities": []}, {"text": "These feature representations are then used to train a logistic regression clas-sifier on labelled tweets, with additional features such as an indicator of whether the target is contained in the tweet.", "labels": [], "entities": []}, {"text": "Our submitted run on the test data achieved an F1 of 0.3270.", "labels": [], "entities": [{"text": "F1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9994468092918396}]}], "introductionContent": [{"text": "Stance detection is the task of assigning stance labels to apiece of text with respect to a topic, i.e. whether apiece of text is in favour of \"abortion\", neutral, or against.", "labels": [], "entities": [{"text": "Stance detection is the task of assigning stance labels to apiece of text with respect to a topic, i.e. whether apiece of text is in favour of \"abortion\", neutral, or", "start_pos": 0, "end_pos": 166, "type": "Description", "confidence": 0.7557875718389239}]}, {"text": "Previous work considered targetspecific stance predictors in debates ( or news.", "labels": [], "entities": [{"text": "targetspecific stance predictors", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.5844163497289022}]}, {"text": "The variety of topics discussed on Twitter calls for developing methods that can generalise to any target, including targets not seen in the training data, which is the focus of Subtask B in Task 6 of.", "labels": [], "entities": []}, {"text": "A further challenge is that the targets are not always mentioned in the tweets, which distinguishes this task from target-dependent sentiment analysis (, and open-domain target-dependent sentiment analysis ().", "labels": [], "entities": [{"text": "target-dependent sentiment analysis", "start_pos": 115, "end_pos": 150, "type": "TASK", "confidence": 0.6785682539145151}, {"text": "open-domain target-dependent sentiment analysis", "start_pos": 158, "end_pos": 205, "type": "TASK", "confidence": 0.6765839755535126}]}, {"text": "The SemEval Stance Detection task is further related to that of textual entailment, i.e. we judge if a hypothesis (tweet in our task) entails, contradicts or is neutral towards a textual premise (target in our task).", "labels": [], "entities": [{"text": "SemEval Stance Detection", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9418044487635294}]}, {"text": "However, the premises in typical RTE datasets offer a richer context than the stance detection targets, i.e. they are full sentences instead of topic labels such as \"atheism\".", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.8783665299415588}, {"text": "stance detection", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.8350895643234253}]}, {"text": "Simple baselines such as textual overlap can achieve an F1 of >0., whereas for stance detection such baselines would not perform well, as the target is only mentioned in about half the tweets.", "labels": [], "entities": [{"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9989508390426636}, {"text": "stance detection", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.9198685586452484}]}, {"text": "In our approach we learn a 3-way logistic regression classifier to perform stance detection.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.9176945388317108}]}, {"text": "Apart from the standard bag-of-words features commonly used in sentiment analysis, we also use features from a trained bag-of-words autoencoder similar to the one used by.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.957470178604126}]}, {"text": "In our experiments we show that the bag-of-words autoencoder trained on a large amount of unlabelled tweets about the targets can help generalise to unseen targets better; on our development set it achieves an 8% increase over our best baseline.", "labels": [], "entities": [{"text": "generalise to unseen targets", "start_pos": 135, "end_pos": 163, "type": "TASK", "confidence": 0.88330939412117}]}, {"text": "Further, tweets which contain the target are easier to classify correctly than tweets which do not contain the target.", "labels": [], "entities": []}, {"text": "Such information can be useful for stance detection and we experiment with different ways of integrating it, finding that including a binary feature \"targetContainedIn-Tweet\" outperforms including features extracted by applying the autoencoder to the target.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.9508700966835022}]}], "datasetContent": [{"text": "Our development setup is to train on all labelled tweets for the targets \"Climate Change is a Real Concern\", \"Feminist Movement\" and \"Legalization of Abortion\", then evaluate on \"Hillary Clinton\" tweets.", "labels": [], "entities": [{"text": "Legalization of Abortion\"", "start_pos": 134, "end_pos": 159, "type": "TASK", "confidence": 0.8642412573099136}]}, {"text": "The motivation for this is that Hillary Clinton is the most semantically related target to the Task B test target Donald Trump, since both entities are persons and politicians.", "labels": [], "entities": []}, {"text": "For final submission we tuned all settings with this setup, then retrained on all data and applied the model to the test data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Stance Detection results, reported on the development", "labels": [], "entities": [{"text": "Stance Detection", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.5864979326725006}]}, {"text": " Table 2: Stance Detection results, reported on official test. The", "labels": [], "entities": [{"text": "Stance Detection", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6612744629383087}]}, {"text": " Table 3: Stance Detection results, reported on dev, split by", "labels": [], "entities": [{"text": "Stance Detection", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7146521508693695}]}, {"text": " Table 4: Stance Detection results, reported on test, split by", "labels": [], "entities": [{"text": "Stance Detection", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7120252251625061}]}]}