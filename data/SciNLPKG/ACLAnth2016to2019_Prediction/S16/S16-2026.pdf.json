{"title": [{"text": "Approximating Givenness in Content Assessment through Distributional Semantics", "labels": [], "entities": [{"text": "Approximating Givenness in Content Assessment", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.5702240526676178}]}], "abstractContent": [{"text": "Givenness (Schwarzschild, 1999) is one of the central notions in the formal pragmatic literature discussing the organization of discourse.", "labels": [], "entities": []}, {"text": "In this paper, we explore where distributional semantics can help address the gap between the linguistic insights into the formal pragmatic notion of Givenness and its implementation in computational linguistics.", "labels": [], "entities": []}, {"text": "As experimental testbed, we focus on short answer assessment, in which the goal is to assess whether a student response correctly answers the provided reading comprehension question or not.", "labels": [], "entities": [{"text": "short answer assessment", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.6433031558990479}]}, {"text": "Current approaches only implement a very basic, surface-based perspective on Givenness: A word of the answer that appears as such in the question counts as GIVEN.", "labels": [], "entities": [{"text": "GIVEN", "start_pos": 156, "end_pos": 161, "type": "METRIC", "confidence": 0.9805224537849426}]}, {"text": "We show that an approach approximating Givenness using distributional semantics to check whether a word in a sentence is similar enough to a word in the context to count as GIVEN is more successful quantitatively and supports interesting qualitative insights into the data and the limitations of a basic distributional semantic approach identifying Givenness at the lexical level.", "labels": [], "entities": [{"text": "GIVEN", "start_pos": 173, "end_pos": 178, "type": "METRIC", "confidence": 0.6832263469696045}]}], "introductionContent": [{"text": "Givenness is one of the central notions in the formal pragmatic literature discussing the organization of discourse.", "labels": [], "entities": []}, {"text": "The distinction between given and new material in an utterance dates back at least to where given is defined as \"anaphorically recoverable\" and the notion is used to predict patterns of prosodic prominence.", "labels": [], "entities": []}, {"text": "proposes to define Givenness in terms of the entailment of the existential f-closure between previously mentioned material and the GIVEN expression, hereby also capturing the occurrence of synonyms and hyponyms as given.", "labels": [], "entities": [{"text": "GIVEN expression", "start_pos": 131, "end_pos": 147, "type": "DATASET", "confidence": 0.8313724398612976}]}, {"text": "On the theoretical linguistic side, a foundational question is whether an approach to Information Structure should be grounded in terms of a GivenNew or a Focus-Background dichotomy, or whether the two are best seen as complementing each other.", "labels": [], "entities": []}, {"text": "Computational linguistic research on short answer assessment points in the direction of both perspectives providing performance gains).", "labels": [], "entities": []}, {"text": "On the empirical side, the characteristic problem of obtaining high inter-annotator agreement in focus annotation ( can be overcome through an incremental annotation process making reference to questions as part of an explicit task context.", "labels": [], "entities": []}, {"text": "In short answer assessment approaches determining whether a student response correctly answers a provided reading comprehension question, the practical value of excluding material that is mentioned in the question from evaluating the content of the answer has been clearly established).", "labels": [], "entities": [{"text": "short answer assessment", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.6344916125138601}]}, {"text": "Yet these computational linguistic approaches only implement a very basic, completely surface-based perspective on Givenness: A word of the answer that appears as such in the question counts as GIVEN.", "labels": [], "entities": [{"text": "GIVEN", "start_pos": 194, "end_pos": 199, "type": "METRIC", "confidence": 0.9685072302818298}]}, {"text": "Such a surface-based approach to Givenness fails to capture that the semantic notion of Givenness i) maybe transported by semantically similar words, ii) entailment rather than identity is at stake, and iii) so-called bridging cases seem to involve semantically related rather than semantically similar words.", "labels": [], "entities": []}, {"text": "Computational linguistic approaches to classifying Givenness () have concentrated on the information status of noun phrases, without taking into account other syntactic elements.", "labels": [], "entities": []}, {"text": "Furthermore, they do not explicitly make use of similarity and relatedness between lexical units as we propose in this paper.", "labels": [], "entities": []}, {"text": "Our approach thus explores anew avenue in computationally determining Givenness.", "labels": [], "entities": []}, {"text": "Theoretical linguistic proposals spelling out Givenness are based on formal semantic formalisms and notions such as logical entailment, type shifting, and existential f-closure, which do not readily lend themselves to extending the computational linguistic approaches.", "labels": [], "entities": [{"text": "type shifting", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.756921648979187}]}, {"text": "As already alluded to by the choice of words \"semantically similar\" and \"semantically related\" above, in this paper we want to explore whether distributional semantics can help address the gap between the linguistic insights into Givenness and the computational linguistic realizations.", "labels": [], "entities": []}, {"text": "In place of surface-based Givenness checks, as a first step in this direction we developed an approach integrating distributional semantics to check whether a word in a sentence is similar enough to a word in the context to count as In section 2, we provide the background on Schwarzschild's notion of Givenness and conceptually explore what a distributional semantic perspective may offer.", "labels": [], "entities": []}, {"text": "Section 3 then introduces the application domain of content assessment as our experimental sandbox and the CoMiC system (Meurers et al., 2011) we extended.", "labels": [], "entities": [{"text": "content assessment", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7122067213058472}, {"text": "Meurers et al., 2011)", "start_pos": 121, "end_pos": 142, "type": "DATASET", "confidence": 0.8667014340559641}]}, {"text": "The distributional model for German used in extending the baseline system is builtin section 4.", "labels": [], "entities": []}, {"text": "In section 5 we then turn to the experiments we conducted using the system extended with the distributional Givenness component and provide quantitative results.", "labels": [], "entities": []}, {"text": "Section 6 then presents the qualitative perspective, discussing examples to probe into the connection between the theoretical linguistic notion of Givenness and its distributional semantic approximation, and where it fails.", "labels": [], "entities": []}, {"text": "Finally, section 7 concludes with a summary of the approach and its contribution.", "labels": [], "entities": []}], "datasetContent": [{"text": "Now that we have a baseline content assessment system (section 3) and a distributional model for German (section 4) in place, we have all the components to quantitatively and qualitatively evaluate the idea to model Givenness through semantic similarity measures.", "labels": [], "entities": []}, {"text": "To do so, we simply replaced the surface-based Givenness filter of the baseline CoMiC system with a distributional-semantics based Givenness filter based on the model described in the previous section.", "labels": [], "entities": []}, {"text": "For this we must make concrete, how exactly distributional-semantic distances are used to determine the words in an answer counting as GIVEN.", "labels": [], "entities": [{"text": "GIVEN", "start_pos": 135, "end_pos": 140, "type": "METRIC", "confidence": 0.9253939390182495}]}, {"text": "The parameters to be estimated relate to two different ways one can determine semantic relatedness using word vectors for two words w 1 and w 2 : I.", "labels": [], "entities": []}, {"text": "Calculate cosine similarity of w 1 and w 2 and require it to beat least equal to a threshold t.", "labels": [], "entities": [{"text": "similarity", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.5055758953094482}]}, {"text": "Calculate n nearest words tow 1 and check whether w 2 is among them.", "labels": [], "entities": []}, {"text": "For the first method, one needs to estimate the threshold t, while for the second method one needs to determine how many neighbors to calculate (n).", "labels": [], "entities": []}, {"text": "For the threshold parameter t, we experimented with values from 0.1 to 0.9 in increments of 0.1.", "labels": [], "entities": []}, {"text": "For the number of nearest neighbors n, we used a space from 2 to 20 with increments of 2.", "labels": [], "entities": []}, {"text": "To cleanly separate our test data from the data used for training and parameter estimation, we randomly sampled approximately 20% of the CREG-5K data set and set it aside as the final test set.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7137709259986877}, {"text": "CREG-5K data set", "start_pos": 137, "end_pos": 153, "type": "DATASET", "confidence": 0.9536339044570923}]}, {"text": "The remaining 80% was used as training set.", "labels": [], "entities": []}, {"text": "All parameter estimation was done before running the final system on the test set and using only the training data.", "labels": [], "entities": []}, {"text": "shows the results in terms of classification accuracy for 10-fold cross-validation on the training data.", "labels": [], "entities": [{"text": "classification", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.8557512760162354}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9645591378211975}]}, {"text": "The table includes the performance of the system without a Givenness filter as well as with the basic surface-based approach.: Content Assessment results on training set sub-corpora of CREG-5K corresponding to the universities where they were collected, KU and OSU.", "labels": [], "entities": [{"text": "KU", "start_pos": 254, "end_pos": 256, "type": "DATASET", "confidence": 0.8994306921958923}, {"text": "OSU", "start_pos": 261, "end_pos": 264, "type": "DATASET", "confidence": 0.8607150316238403}]}, {"text": "First, the results confirm that an alignment-based content assessment system such as CoMiC greatly benefits from a Givenness filter, as demonstrated by the big gap in performance between the noGivenness and surface-Givenness conditions.", "labels": [], "entities": []}, {"text": "Second, both the threshold method and the nearestwords method outperform the surface baseline, if only by a small margin.", "labels": [], "entities": []}, {"text": "Turning to the actual testing, we wanted to find out whether the improvements found for the distributional-semantic Givenness filters carryover to the untouched test set.", "labels": [], "entities": []}, {"text": "We trained the classifier on the full training set and used the best parameters from the training set.", "labels": [], "entities": []}, {"text": "The results thus obtained are summarized in  We can see that results on the test set are generally lower, but the general picture for the test set is the same as what we found for the 10-fold CV on the training data: Surface-based-Givenness easily outperforms the system not employing a Givenness filter, and at least one of the systems employing a distributional semantic Givenness filter (marginally) outperforms the surface-based method.", "labels": [], "entities": []}, {"text": "Interestingly, the two data sets seem to differ in terms of which relatedness method works best for recognizing Givenness: while the threshold method works better for OSU, the n-nearest-words method is the optimal choice for the KU data set.", "labels": [], "entities": [{"text": "KU data set", "start_pos": 229, "end_pos": 240, "type": "DATASET", "confidence": 0.8296401898066202}]}, {"text": "This maybe due to the fact that the OSU data set is generally more diverse in terms of lexical variation and thus presents more opportunities for false positives, i.e., words that are somewhat related but should not be counted as given.", "labels": [], "entities": [{"text": "OSU data set", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9526177247365316}]}, {"text": "Such cases are better filtered out using a global threshold.", "labels": [], "entities": []}, {"text": "The KU data set, on the other hand, contains less variation and hence profits from the more local n-nearest-words method, which always returns a list of candidates for any known word in the vocabulary, no matter whether the candidates are globally very similar or not.", "labels": [], "entities": [{"text": "KU data set", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9048308332761129}]}], "tableCaptions": [{"text": " Table 1: Content Assessment results on training set", "labels": [], "entities": []}, {"text": " Table 2: Content Assessment results on test set", "labels": [], "entities": []}]}