{"title": [{"text": "SteM at SemEval-2016 Task 4: Applying Active Learning to Improve Sentiment Classification", "labels": [], "entities": [{"text": "SemEval-2016 Task 4", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.6451084812482198}, {"text": "Sentiment Classification", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.8131392896175385}]}], "abstractContent": [{"text": "This paper describes our approach to the Se-mEval 2016 task 4, \"Sentiment Analysis in Twitter\", where we participated in subtask A.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter\"", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.8889016747474671}]}, {"text": "Our system relies on AlchemyAPI and Senti-WordNet to create 43 features based on which we select a feature subset as final representation.", "labels": [], "entities": []}, {"text": "Active Learning then filters out noisy tweets from the provided training set, leaving a smaller set of only 900 tweets which we use for training a Multinomial Naive Bayes classi-fier to predict the labels of the test set with an F1 score of 0.478.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.9863425195217133}]}], "introductionContent": [{"text": "Gaining an overview of opinions on recent events or trends is an appealing feature of Twitter.", "labels": [], "entities": [{"text": "Gaining an overview of opinions on recent events or trends", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.7927026152610779}]}, {"text": "For example, receiving real-time feedback from the public about a politician's speech provides insights to media for the latest polls, analysts and interested individuals including the politician herself.", "labels": [], "entities": []}, {"text": "However, detecting tweet sentiment still poses a challenge due to the frequent use of informal language, acronyms, neologisms which constantly change, and the shortness of tweets, which are limited to 140 characters.", "labels": [], "entities": [{"text": "detecting tweet sentiment", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.8905442555745443}]}, {"text": "SemEval's subtask 4A () deals with the sentiment classification of single tweets into one of the classes \"positive\", \"neutral\" or \"negative\".", "labels": [], "entities": [{"text": "sentiment classification of single tweets", "start_pos": 39, "end_pos": 80, "type": "TASK", "confidence": 0.904908299446106}]}, {"text": "Concretely a training set of 5481 tweets and a development set comprising 1799 tweets was given, and the sentiment of 32009 tweets in a test set had to be predicted and was evaluated using F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9978195428848267}]}, {"text": "The distribution of labels for the given datasets is depicted in.", "labels": [], "entities": []}, {"text": "Tweets with positive polarity outnumber the other two classes and the least number of instances is available for negative tweets.", "labels": [], "entities": []}, {"text": "Initially, the organizers provided 6000 tweets for the training set and 2000 tweets for the development set, but only the tweet IDs were released to abide by the Twitter terms of agreement.", "labels": [], "entities": []}, {"text": "By the time we downloaded the data, around 10% of the tweets (519 in the training set, 201 in the test set) were not available anymore.", "labels": [], "entities": []}, {"text": "For further details about the labeling process and the datasets see (.", "labels": [], "entities": [{"text": "labeling process", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.8926960825920105}]}, {"text": "Lexicon-based approaches have done very well in this competition over the past years, i.e, the winners of the years) relied heavily on them.", "labels": [], "entities": []}, {"text": "Our goal is to explore alternatives and to complement lexicon-based strategies.", "labels": [], "entities": []}, {"text": "The SteM system performs preprocessing, including canonicalization of tweets, and based on that we extract 43 features as our representation, some features are based on AlchemyAPI).", "labels": [], "entities": []}, {"text": "We choose 28 of the features as final representation and learn three classifiers based on different subsets of these 28 features.", "labels": [], "entities": []}, {"text": "We refer to the latter as feature subspaces or subspaces hereafter.", "labels": [], "entities": []}, {"text": "However, independently of the subspace we use, we face the fact that tweet datasets inherently contain noise and all subspaces will -to a greater or lesser extend-be affected by this noise.", "labels": [], "entities": []}, {"text": "To alleviate this problem, we propose to concentrate on only few of the labeled tweets, those likely to be most discriminative.", "labels": [], "entities": []}, {"text": "To this purpose, we use Active Learning (AL), as explained in Section 6.", "labels": [], "entities": []}, {"text": "For AL, we setup a \"budget\", translating: Distribution of sentiment labels in the datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate our approach on the development set as no labels for the test set are available.", "labels": [], "entities": []}, {"text": "As AlchemyAPI is a full-fledged system, we use the 8 features extracted from it (Features 25\u221232 in) as a baseline in our experiments to compare it with SteM using a) the full training set and b) the reduced tweets after performing confidencebased UC as explained in the previous section.", "labels": [], "entities": []}, {"text": "We then reapply the learning procedure described in Section 5 to obtain our F1-scores.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9964776635169983}]}, {"text": "The results are depicted in.", "labels": [], "entities": []}, {"text": "Initially, our system achieves an F1-score of 0.454 using all training instances.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9997639060020447}]}, {"text": "After selecting the 900 most informative tweets using confidence-based UC from the previous section, the score increases to 0.473.", "labels": [], "entities": []}, {"text": "We observe a similar trend for our baseline and note that it is outperformed by SteM, although the margin shrinks when reducing the number of tweets in the training set.", "labels": [], "entities": [{"text": "SteM", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.7051562666893005}]}, {"text": "When analyzing the corresponding confusion matrix of SteM   with 900 tweets in, it becomes obvious that it fails to distinguish neutral sentiment from the remaining ones properly.", "labels": [], "entities": [{"text": "SteM", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.8546224236488342}]}], "tableCaptions": [{"text": " Table 1: Distribution of sentiment labels in the datasets.", "labels": [], "entities": [{"text": "Distribution of sentiment labels", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8728552460670471}]}, {"text": " Table 4: Comparing F1-scores on development set using SteM", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9772645235061646}, {"text": "SteM", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8989076614379883}]}, {"text": " Table 5: Confusion matrix of SteM with 900 training instances.", "labels": [], "entities": []}]}