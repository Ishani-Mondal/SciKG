{"title": [{"text": "UofR at SemEval-2016 Task 8: Learning Synchronous Hyperedge Replacement Grammar for AMR Parsing", "labels": [], "entities": [{"text": "Learning Synchronous Hyperedge Replacement Grammar", "start_pos": 29, "end_pos": 79, "type": "TASK", "confidence": 0.6180532932281494}, {"text": "AMR Parsing", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.9057860374450684}]}], "abstractContent": [{"text": "In this paper, we apply a synchronous-graph-grammar-based approach to SemEval-2016 Task 8, Meaning Representation Parsing.", "labels": [], "entities": [{"text": "SemEval-2016 Task 8", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8277268012364706}, {"text": "Meaning Representation Parsing", "start_pos": 91, "end_pos": 121, "type": "TASK", "confidence": 0.8927586476008097}]}, {"text": "In particular, we learn Synchronous Hyperedge Replacement Grammar (SHRG) rules from aligned pairs of sentences and AMR graphs.", "labels": [], "entities": [{"text": "Synchronous Hyperedge Replacement Grammar (SHRG)", "start_pos": 24, "end_pos": 72, "type": "TASK", "confidence": 0.7788503382887159}]}, {"text": "Then we use Earley algorithm with cube-pruning for AMR parsing given new sentences and the learned SHRG.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9182969033718109}]}, {"text": "Experiments on the evaluation dataset demonstrate that competitive results can be achieved using a SHRG-based approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) () is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8844736814498901}]}, {"text": "shows an example of the edge-labeled representation of an AMR graph, where the edges are labeled while the nodes are not.", "labels": [], "entities": []}, {"text": "AMR utilizes PropBank frames, non-core semantic roles, coreference, named entity annotations and other semantic phenomena to represent the semantic structure of a sentence and abstracts away its syntax form.", "labels": [], "entities": []}, {"text": "These properties render AMR representation useful in applications like question answering and semantics-based machine translation.", "labels": [], "entities": [{"text": "AMR representation", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9707637429237366}, {"text": "question answering", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.9084612727165222}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.6857640445232391}]}, {"text": "SemEval-2016 Task 8 is the task of recovering this type of semantic formalism for plain text.", "labels": [], "entities": []}, {"text": "A large corpus of annotated English/AMR pairs is provided to learn this mapping.", "labels": [], "entities": []}, {"text": "Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs ().", "labels": [], "entities": [{"text": "Hyperedge replacement grammar (HRG)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8749165534973145}]}, {"text": "Its synchronous counterpart, SHRG, can be used for transforming a graph from/to another structured representation such as a string or tree structure.", "labels": [], "entities": []}, {"text": "Therefore, an SHRG-based approach can be used for AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9631073772907257}]}, {"text": "Previous approaches usually first map the components of the sentence to components of the graph.", "labels": [], "entities": []}, {"text": "Then different supervised algorithms are used to assemble these graph components to generate a complete AMR graph (.", "labels": [], "entities": []}, {"text": "Previously, we have developed a system that learns SHRG rules from sentence/AMR graph pairs (, with automatic alignments extracted from JAMR ().", "labels": [], "entities": [{"text": "SHRG rules from sentence/AMR graph pairs", "start_pos": 51, "end_pos": 91, "type": "TASK", "confidence": 0.748773604631424}, {"text": "JAMR", "start_pos": 136, "end_pos": 140, "type": "DATASET", "confidence": 0.8288787603378296}]}, {"text": "During the decoding procedure, we also use the concept identification results from.", "labels": [], "entities": []}, {"text": "The system is evaluated on the newswire section of LDC2013E117, which has around 4000 sentence-AMR pairs as training data.", "labels": [], "entities": [{"text": "newswire section of LDC2013E117", "start_pos": 31, "end_pos": 62, "type": "DATASET", "confidence": 0.926326796412468}]}, {"text": "In this paper, we extend this system by using the alignments from Ulf Hermjakob's automatic aligner and building a perceptron-based concept identifier where the boundary information of the mapped frag-", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our parser on the LDC2015E86 dataset, which includes 16833 training, 1368 dev, and 1371 test sentences.", "labels": [], "entities": [{"text": "LDC2015E86 dataset", "start_pos": 30, "end_pos": 48, "type": "DATASET", "confidence": 0.9733683466911316}]}, {"text": "During the sampling procedure, all the cut variables in the derivation forest are initialized as 1 and an incoming hyperedge is sampled uniformly for each node.", "labels": [], "entities": []}, {"text": "We run the sampler for 160 iterations and combine the grammar dumped every 10th iteration.", "labels": [], "entities": []}, {"text": "The performance of our SHRG-based parser is evaluated using Smatch v2.0., which evaluates the precision, recall, and F 1 of the concepts and relations all together.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9994916915893555}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9990280866622925}, {"text": "F 1", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9897181391716003}]}, {"text": "shows the results on the dev and test set.", "labels": [], "entities": []}, {"text": "We also report smatch score on the shared task evaluation data, which includes 1053 sentences.", "labels": [], "entities": []}, {"text": "The smatch score on the evaluation data is 0.50.", "labels": [], "entities": []}, {"text": "This score is much lower than the performance on the dev and test data.", "labels": [], "entities": []}, {"text": "The reason might be that the evaluation data is much harder and includes more noise, which can breakdown the structure of the learned grammar.", "labels": [], "entities": []}, {"text": "The results show that SHRG-based parsing can be a viable approach for AMR parsing.", "labels": [], "entities": [{"text": "SHRG-based parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.6809189319610596}, {"text": "AMR parsing", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.9653671979904175}]}, {"text": "Currently our system only uses a few simple features and all the weights are tuned by hand.", "labels": [], "entities": []}, {"text": "The performance could be improved by using more complicated features and tuning their weights automatically.", "labels": [], "entities": []}, {"text": "It would also be helpful to use external resources such as the com-mon organizational roles, relational roles and the verbalization lists and use fallback techniques to deal with unknown words.", "labels": [], "entities": []}], "tableCaptions": []}