{"title": [{"text": "Natural Solution to FraCaS Entailment Problems", "labels": [], "entities": [{"text": "FraCaS Entailment", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7653293013572693}]}], "abstractContent": [{"text": "Reasoning over several premises is not a common feature of RTE systems as it usually requires deep semantic analysis.", "labels": [], "entities": [{"text": "Reasoning over several premises", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8245568126440048}]}, {"text": "On the other hand, FraCaS is a collection of entailment problems consisting of multiple premises and covering semantically challenging phenomena.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.9261441230773926}]}, {"text": "We employ the tableau theorem prover for natural language to solve the FraCaS problems in a natural way.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.8895798325538635}]}, {"text": "The expressiveness of a type theory, the transparency of natural logic and the schematic nature of tableau inference rules make it easy to model challenging semantic phenomena.", "labels": [], "entities": []}, {"text": "The efficiency of theorem proving also becomes challenging when reasoning over several premises.", "labels": [], "entities": [{"text": "theorem proving", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.9493790566921234}]}, {"text": "After adapting to the dataset, the prover demonstrates state-of-the-art competence over certain sections of FraCaS.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.9658406376838684}]}], "introductionContent": [{"text": "Understanding and automatically processing the natural language semantics is a central task for computational linguistics and its related fields.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.7367088496685028}]}, {"text": "At the same time, inference tasks are regarded as the best way of testing an NLP systems's semantic capacity (.", "labels": [], "entities": []}, {"text": "Following this view, recognizing textual entailment (RTE) challenges () were regularly held which evaluate the RTE systems based on the RTE dataset.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.7569216887156168}, {"text": "RTE dataset", "start_pos": 136, "end_pos": 147, "type": "DATASET", "confidence": 0.9230943322181702}]}, {"text": "The RTE data represents a set of texthypotheses pairs that are human annotated on the inference relations: entailment, contradiction and neutral.", "labels": [], "entities": [{"text": "RTE data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8256945312023163}]}, {"text": "Hence it attempts to evaluate the systems on human reasoning.", "labels": [], "entities": []}, {"text": "In general, the RTE datasets are created semi-automatically and are often motivated by the scenarios found in the applications like question answering, relation extraction, information retrieval and summarization).", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 16, "end_pos": 28, "type": "DATASET", "confidence": 0.8703922927379608}, {"text": "question answering", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.8785905539989471}, {"text": "relation extraction", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.887778639793396}, {"text": "information retrieval", "start_pos": 173, "end_pos": 194, "type": "TASK", "confidence": 0.8198418319225311}, {"text": "summarization", "start_pos": 199, "end_pos": 212, "type": "TASK", "confidence": 0.9797412157058716}]}, {"text": "On the other hand, the semanticists are busy designing theories that account for the valid logical relations over natural language sentences.", "labels": [], "entities": []}, {"text": "These theories usually model reasoning that depends on certain semantic phenomena, e.g., Booleans, quantifiers, events, attitudes, intensionality, monotonicity, etc.", "labels": [], "entities": []}, {"text": "These types of reasoning are weak points of RTE systems as the above mentioned semantic phenomena are underrepresented in the RTE datasets.", "labels": [], "entities": [{"text": "RTE", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9493122100830078}, {"text": "RTE datasets", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.8190740346908569}]}, {"text": "In order to test and train the weak points of an RTE system, we choose the FraCaS dataset).", "labels": [], "entities": [{"text": "FraCaS dataset", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.982616513967514}]}, {"text": "The set contains complex entailment problems covering various challenging semantic phenomena which are still not fully mastered by RTE systems.", "labels": [], "entities": []}, {"text": "Moreover, unlike the standard RTE datasets, FraCaS also allows multipremised problems.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.8091434836387634}, {"text": "FraCaS", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.9207208752632141}]}, {"text": "To account for these complex entailment problems, we employ the theorem prover for higher-order logic, which represents the version of formal logic motivated by natural logic.", "labels": [], "entities": []}, {"text": "Though such expressive logics usually come with the inefficient decision procedures, the prover maintains efficiency by using the inference rules that are specially tailored for the reasoning in natural language.", "labels": [], "entities": []}, {"text": "We introduce new rules for the prover in light of the FraCaS problems and test the rules against the relevant portion of the set.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.9431754350662231}]}, {"text": "The test results are compared to the current stateof-the-art on the dataset.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "We start with introducing a tableau system for natural logic.", "labels": [], "entities": []}, {"text": "Section 3 explores the FraCaS dataset in more details.", "labels": [], "entities": [{"text": "FraCaS dataset", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.9788522124290466}]}, {"text": "In Section 4, we describe the process of adapting the theorem prover to FraCaS, i.e. how specific semantic phenomena are modeled with the help of tableau rules.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.9489641785621643}]}, {"text": "Several premises with monotone quantifiers in- \u2264\u00d7 \u2286 \u2264\u00d7 \u2264\u00d7 Figure 1: A closed tableau proves that every prover halts quickly entails most tableau provers terminate.", "labels": [], "entities": []}, {"text": "Each branch growth is marked with the corresponding rule application.", "labels": [], "entities": []}, {"text": "crease the search space for proofs.", "labels": [], "entities": []}, {"text": "In Section 5, we present several rules that contribute to shorter proofs.", "labels": [], "entities": []}, {"text": "In the evaluation part (Section 6), we analyze the results of the prover on the relevant FraCaS sections and compare them with the related RTE systems.", "labels": [], "entities": [{"text": "FraCaS sections", "start_pos": 89, "end_pos": 104, "type": "DATASET", "confidence": 0.9569701552391052}]}, {"text": "We end with possible directions of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The FraCaS test suite) is a set of 346 test problems.", "labels": [], "entities": [{"text": "FraCaS test suite", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9562491178512573}]}, {"text": "It was prepared by the FraCaS consortium as an initial benchmark for semantic competence of NLP systems.", "labels": [], "entities": [{"text": "FraCaS consortium", "start_pos": 23, "end_pos": 40, "type": "DATASET", "confidence": 0.9559340476989746}]}, {"text": "Each FraCaS problem is a pair of premises and a yes-nounknown question that is annotated with a gold judgment: yes (entailment), no (contradiction), or unknown (neutral).", "labels": [], "entities": []}, {"text": "The problems mainly consist of short sentences and resemble the problems found in introductory logic books.", "labels": [], "entities": []}, {"text": "To convert the test suite into the style of RTE dataset, MacCartney and Manning (2007) translated the questions into declarative sentences.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.7431460171937943}]}, {"text": "The judgments were copied from the original test suite with slight modifications.", "labels": [], "entities": []}, {"text": "Several problems drawn from the obtained FraCaS dataset are presented in.", "labels": [], "entities": [{"text": "FraCaS dataset", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.9770336449146271}]}, {"text": "Unlike other RTE datasets, the FraCaS problems contain multiple premises (45% of the total problems) and are structured in sections according to the semantic phenomena they concern.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.8195236921310425}]}, {"text": "The sections cover generalized quantifiers (GQs), plurals, anaphora, ellipsis, adjectives, comparatives, temporal reference, verbs and attitudes.", "labels": [], "entities": []}, {"text": "Due to the challenging problems it contains, the FraCaS dataset can be seen as one of the most complex RTE data from the semantics perspective.", "labels": [], "entities": [{"text": "FraCaS dataset", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.9806550443172455}]}, {"text": "Unfortunately, due to its small size the dataset is not representative enough for system evaluation purposes.", "labels": [], "entities": []}, {"text": "The above mentioned facts perhaps are the main reasons why the FraCaS data is less favored for developing and assessing the semantic competence of RTE systems.", "labels": [], "entities": [{"text": "FraCaS data", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.9516682624816895}]}, {"text": "Nevertheless, several RTE systems) were trained and evaluated on (the parts of) the dataset.", "labels": [], "entities": []}, {"text": "Usually the goal of these evaluations is to show that specific theories/frameworks and the corresponding RTE systems are able to model deep semantic reasoning over the phenomena found in FraCaS.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 187, "end_pos": 193, "type": "DATASET", "confidence": 0.942973792552948}]}, {"text": "Our aim is also the same in the rest of the sections.", "labels": [], "entities": []}, {"text": "After adapting the prover to the FraCaS sections for GQs, plurals, adjectives and attitudes, we evaluate it on the relevant sections and analyze the performance.", "labels": [], "entities": [{"text": "FraCaS sections", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.934087723493576}]}, {"text": "Obtained results are compared to related RTE systems.", "labels": [], "entities": [{"text": "Obtained", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9865913987159729}]}, {"text": "We run two version of the prover, ccLangPro and easyLangPro, that employ CCG derivations produced by C&C and EasyCCG respectively.", "labels": [], "entities": [{"text": "ccLangPro", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.8806657195091248}, {"text": "easyLangPro", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.8501424193382263}, {"text": "EasyCCG", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.8921791911125183}]}, {"text": "In order to abstract from the parser errors to some extent, the answers from both provers are aggregated in LangPro: a proof is found iff one of the parser-specific provers finds a proof.", "labels": [], "entities": []}, {"text": "The evaluation results of the three versions of LangPro on the relevant FraCaS sections are presented in along with the confusion matrix for LangPro.", "labels": [], "entities": [{"text": "LangPro", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9130614995956421}, {"text": "FraCaS sections", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9608265161514282}, {"text": "LangPro", "start_pos": 141, "end_pos": 148, "type": "DATASET", "confidence": 0.9598420858383179}]}, {"text": "The results show that LangPro performs slightly better with C&C compared to EasyCCG.", "labels": [], "entities": [{"text": "EasyCCG", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9331305027008057}]}, {"text": "This is due to LLFgen which is mostly tuned on the C&C derivations.", "labels": [], "entities": [{"text": "LLFgen", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.7976878881454468}, {"text": "C&C derivations", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.7948872596025467}]}, {"text": "Despite this bias, easyLangPro proves 8 problems that were not proved by ccLangPro.", "labels": [], "entities": [{"text": "ccLangPro", "start_pos": 73, "end_pos": 82, "type": "DATASET", "confidence": 0.9749480485916138}]}, {"text": "In case of half of these problems, C&C failed to return derivations for some of the sentences while in another half of the problems the errors in C&C derivations were crucial, e.g., in the conclusion of Fr-44 committee members was not analyzed as a constituent.", "labels": [], "entities": [{"text": "Fr-44 committee members", "start_pos": 203, "end_pos": 226, "type": "DATASET", "confidence": 0.8990966478983561}]}, {"text": "On the other hand, ccLangPro proves 10 problems unsolved by easyLangPro, e.g., Fr-6 was not proved because EasyCCG analyzes really as a modifier of are in the conclusion, or even more unfortunate, the morphological analyzer of EasyCCG cannot get the lemma of clients correctly in Fr-99 and as a result the prover cannot relate clients to client.", "labels": [], "entities": [{"text": "Fr-6", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.8723082542419434}]}, {"text": "The precision of LangPro is high due to its sound inference rules.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995390176773071}]}, {"text": "Fr-109 in was the only case when entailment and contradiction were confused: plurals are not modeled as strictly more than one.", "labels": [], "entities": [{"text": "Fr-109", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9425424337387085}]}, {"text": "The false proves are mostly due to alack of knowledge about adjectives.", "labels": [], "entities": []}, {"text": "LangPro does not know a default comparison class for clever, e.g., clever person\u2192clever but clever politician \u2192clever).", "labels": [], "entities": [{"text": "LangPro", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9462849497795105}]}, {"text": "Fr-215 was proved as entailment because we have not modeled intensionality of adjectives.", "labels": [], "entities": [{"text": "Fr-215", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8711427450180054}]}, {"text": "Since EasyCCG was barely used during adaptation (except changing most of NP modifiers into noun modifiers), it analyzed at most in Fr-64 as a sentential modifier which was not modeled as downward monotone in the signature.", "labels": [], "entities": [{"text": "Fr-64", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.9588403105735779}]}, {"text": "Hence, by default, it was considered as upward monotone leading to the proof for entailment.", "labels": [], "entities": []}, {"text": "There are several reasons behind the problems that were not proved by the prover.", "labels": [], "entities": []}, {"text": "Several problems for adjectives were not proved as they con- We also check the FraCaS sections how representative they are for higher-order GQs (HOGQs).", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.9382476806640625}]}, {"text": "After replacing all occurrences of most, several, many, sand the with the indefinite a in LLFs, LangPro -HOGQ (without the HOGQs) achieves an overall accuracy of 81% over FrSec-1,2,5,9.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9993076324462891}, {"text": "FrSec-1,2,5,9", "start_pos": 171, "end_pos": 184, "type": "DATASET", "confidence": 0.9472855925559998}]}, {"text": "Compared to LangPro only 6 problems, including Fr-56, 99, were misclassified while Fr-26, 100 were solved.", "labels": [], "entities": [{"text": "LangPro", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.9516905546188354}, {"text": "Fr-56", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.8625661730766296}, {"text": "misclassified", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.9767499566078186}, {"text": "Fr-26", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.9260766506195068}]}, {"text": "This shows that the dataset is not representative enough for HOGQs.", "labels": [], "entities": [{"text": "HOGQs", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.7615812420845032}]}, {"text": "In, the current results are compared to the RTE systems that have been tested on the single or multi-premised FraCaS problems.", "labels": [], "entities": []}, {"text": "11 According to the table, the current work shows that the natural tableau system and LangPro are successful in deep reasoning over multiple premises.", "labels": [], "entities": []}, {"text": "The natural logic approach in and models monotonicity reasoning with the exclusion relation in terms of the string edit operations over phrases.", "labels": [], "entities": []}, {"text": "Since the approach heavily hinges on a sequence of edits that relates a premise to a conclusion, it cannot process multi-premised problems properly. and both base on first-order logic representations.", "labels": [], "entities": []}, {"text": "While Lewis and Steedman (2013) employs distributional relation clustering to model the semantics of content words, extends first-order logic with several higher-order terms (e.g., for most, believe, manage) and augments first-order inference of Coq with additional inference rules for the higher-order terms.  and build an inference engine that reasons over abstract denotations, formulas of relational algebra or a sort of description logic, obtained from Dependency-based Compositional Semantic trees (Liang et al., 2011).", "labels": [], "entities": []}, {"text": "Our system and approach differ from the above mentioned ones in its unique combination of expressiveness of highorder logic, naturalness of logical forms (making them easily obtainable) and flexibility of a semantic tableau method.", "labels": [], "entities": []}, {"text": "All these allow to model surface and deep semantic reasoning successfully in a single system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Measures of ccLangPro (ccLP), easy- LangPro (eLP) and LangPro (LP) on FraCaS sec- tions 1, 2, 5, 9 and the confusion matrix for LP.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of RTE systems tested on FraCaS: NL07 (MacCartney and Manning, 2007), NL08  (MacCartney and Manning, 2008), LS (Lewis and Steedman, 2013) with Parser and Gold syntax, NLI  (Angeli and Manning, 2014), T14a (Tian et al., 2014), T14b (Dong et al., 2014) and M15 (Mineshima et  al., 2015). BL is a majority (yes) baseline. Results for non-applicable sections are strikeout.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.950193464756012}, {"text": "BL", "start_pos": 307, "end_pos": 309, "type": "METRIC", "confidence": 0.9984340071678162}]}]}