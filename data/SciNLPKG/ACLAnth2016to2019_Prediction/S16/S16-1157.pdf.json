{"title": [{"text": "TALN at SemEval-2016 Task 11: Modelling Complex Words by Contextual, Lexical and Semantic Features", "labels": [], "entities": [{"text": "TALN", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5647735595703125}, {"text": "Modelling Complex Words", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.8562583923339844}]}], "abstractContent": [{"text": "This paper presents the participation of the TALN team in the Complex Word Identification Task of SemEval-2016 (Task 11).", "labels": [], "entities": [{"text": "Complex Word Identification Task", "start_pos": 62, "end_pos": 94, "type": "TASK", "confidence": 0.6828665435314178}]}, {"text": "The purpose of the task was to determine if a word in a given sentence can be judged as complex or not by a certain target audience.", "labels": [], "entities": []}, {"text": "To experiment with word complexity identification approaches, Task organizers provided a training set of 2,237 words judged as complex or not by 20 human evaluators, together with the sentence in which each word occurs.", "labels": [], "entities": [{"text": "word complexity identification", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.6796272099018097}]}, {"text": "In our contribution we modelled each word to evaluate as a numeric vector populated with a set of lexical, semantic and contextual features that may help assess the complexity of a word.", "labels": [], "entities": []}, {"text": "We trained a Random Forest clas-sifier to automatically decide if each word is complex or not.", "labels": [], "entities": [{"text": "Random Forest clas-sifier", "start_pos": 13, "end_pos": 38, "type": "DATASET", "confidence": 0.8970712622006735}]}, {"text": "We submitted two runs in which we respectively considered unweighted and weighted instances of complex words to train our classifier, where the weight of each instance is proportional to the number of eval-uators that judged the word as complex.", "labels": [], "entities": []}, {"text": "Our system scored as the third best performing one.", "labels": [], "entities": []}], "introductionContent": [{"text": "Approaches to automatically identify if a target audience will perceive a certain word as complex or not constitute a core component in several languagerelated areas of research, including Lexical Simplification ( and Readability Assessment).", "labels": [], "entities": [{"text": "Lexical Simplification", "start_pos": 189, "end_pos": 211, "type": "TASK", "confidence": 0.9318016469478607}]}, {"text": "The Complex Word Identification Task of SemEval-2016 proposes a shared framework for evaluating complex word identification systems.", "labels": [], "entities": [{"text": "Complex Word Identification Task", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.6656505540013313}, {"text": "word identification", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7472778856754303}]}, {"text": "Task participants were provided with a set of sentences where, for each sentence, one or more words have been rated as complex or not by 20 human evaluators.", "labels": [], "entities": []}, {"text": "An example sentence from this dataset is: If the growth rate is known , the maximum lichen size will give a minimum age for when this rock was deposited.", "labels": [], "entities": []}, {"text": "In this sentence, the words 'lichen' and 'deposited' were classified as complex by at least one out of the 20 evaluators, unlike e.g. 'growth', which did not received this label by any of them.", "labels": [], "entities": []}, {"text": "In our participation in Task 11, we cast the identification of complex words as a binary classification problem in which each word is evaluated as complex or not, given the sentence in which it occurs.", "labels": [], "entities": [{"text": "identification of complex words", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.8227182030677795}]}, {"text": "We modelled each word by a set of lexical, semantic and contextual features and evaluated distinct binary classification algorithms.", "labels": [], "entities": []}, {"text": "Our approach to Task 11 obtained good performance: our team ranked as the second best performing one and one of the two systems we proposed scored as the third best performing system according to the G-score official evaluation metric (harmonic mean between Accuracy and Recall).", "labels": [], "entities": [{"text": "G-score official evaluation metric (harmonic mean between", "start_pos": 200, "end_pos": 257, "type": "METRIC", "confidence": 0.7984979301691055}, {"text": "Accuracy", "start_pos": 258, "end_pos": 266, "type": "METRIC", "confidence": 0.5852231979370117}, {"text": "Recall", "start_pos": 271, "end_pos": 277, "type": "METRIC", "confidence": 0.6962500214576721}]}, {"text": "In Section 2 we provide an overview of relevant research related to Complex Word Identification.", "labels": [], "entities": [{"text": "Complex Word Identification", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.7369552055994669}]}, {"text": "Section 3 and 4 respectively introduce the Task 11 dataset and present the text analysis tools and resources we exploited to characterize complex words.", "labels": [], "entities": []}, {"text": "In Section 5 we describe the word features we used to build our complex word classifier.", "labels": [], "entities": []}, {"text": "In Section 6 we present and discuss the performance of our Task 11 system.", "labels": [], "entities": []}, {"text": "Finally, in Section 7 we formulate our conclusions and outline future venues of research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to identify the best approach to classify words as complex or not, we compared four classifiers by training on both the Simple and Weighted datasets.", "labels": [], "entities": []}, {"text": "We evaluated the classification performance by means of a 10-fold cross-validation.", "labels": [], "entities": [{"text": "classification", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9564237594604492}]}, {"text": "Results are summarized in shows that the best performance in terms of F-Score was achieved by the Random Forest classifier for both approaches (Simple and Weighted).", "labels": [], "entities": [{"text": "F-Score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9957953691482544}]}, {"text": "As a consequence, the two systems we submitted to Task 11 relied on a Random Forest model respectively trained on Simple (unweighted, run 1) and Weighted (run 2) instances.", "labels": [], "entities": []}, {"text": "Our run based on Weighted instances performed quite well, ranking as the third best system in Task 11 with a G-Score of 0.772, where the G-Score of the best performing system is 0.774.", "labels": [], "entities": [{"text": "G-Score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9933558702468872}, {"text": "G-Score", "start_pos": 137, "end_pos": 144, "type": "METRIC", "confidence": 0.9767667651176453}]}, {"text": "With respect to F-Score our best performing run was the one based on Simple instances that ranked as sixth.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9845771193504333}]}, {"text": "In we show the top 10 features in our feature set in terms of information gain.", "labels": [], "entities": []}, {"text": "We can see that the frequencies of the word to evaluate in the two corpora we considered (English Wikipedia and British National Corpus) constitute the two most informative features.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 90, "end_pos": 107, "type": "DATASET", "confidence": 0.9597756862640381}, {"text": "British National Corpus", "start_pos": 112, "end_pos": 135, "type": "DATASET", "confidence": 0.9225197633107504}]}, {"text": "Five of the top 10 features are computed by relying on WordNet, without performing Word Sense Disambiguation: among them we can find the number of synsets (senses) of the word to evaluate (WNSynsetN), the average length of the glosses of these synsets (WNGloss) and the average depth of these synsets in the hypernym tree (WNDepth).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.928871214389801}, {"text": "Word Sense Disambiguation", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.6653210620085398}]}, {"text": "Other useful indicators of word complexity are the presence of the word in the simple words list of Dale & Chall and the set of lexicalizations (of the synsets associated to the word) characterized by a frequency in the British National Corpus lower than the frequency of the word to evaluate (LexicLowerFreqALL and LexicLowerSumFreqALL).", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 220, "end_pos": 243, "type": "DATASET", "confidence": 0.9516385396321615}]}, {"text": "In we show the performance of the four classification algorithms we considered by training them on the whole training dataset (with Simple or Weighted instances) and testing them on the testing dataset.", "labels": [], "entities": []}, {"text": "The best performance in terms of both FScore and G-Score are achieved by the two Random Forest classifiers that were trained respectively on Simple (unweighted) and Weighted instances.", "labels": [], "entities": [{"text": "FScore", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9859359264373779}, {"text": "G-Score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9665573239326477}]}, {"text": "In general, when we train the classifiers on Weighted instances in place of Simple ones, on the one hand both recall and accuracy improve, thus resulting in a higher G-Score, on the other hand the precision decreases, thus resulting in a lower F-Score.: Comparison of the performance of four complex word binary classifiers.", "labels": [], "entities": [{"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.999458372592926}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9993268251419067}, {"text": "G-Score", "start_pos": 166, "end_pos": 173, "type": "METRIC", "confidence": 0.9911662936210632}, {"text": "precision", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9992510676383972}, {"text": "F-Score.", "start_pos": 244, "end_pos": 252, "type": "METRIC", "confidence": 0.996837854385376}]}, {"text": "Each classifier is trained on the whole training dataset and tested on the annotated testing dataset.", "labels": [], "entities": []}, {"text": "The asterisk symbol (*) points out the best performing classifier by G-Score while the plus symbol (+) the best performing classifier by F-Score.", "labels": [], "entities": [{"text": "G-Score", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9389609098434448}, {"text": "F-Score", "start_pos": 137, "end_pos": 144, "type": "METRIC", "confidence": 0.9559530019760132}]}], "tableCaptions": [{"text": " Table 1: Comparison of the performance of four complex word", "labels": [], "entities": []}, {"text": " Table 2: Top 10 features with respect to information gain.", "labels": [], "entities": [{"text": "information gain", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.8452746570110321}]}, {"text": " Table 3: Comparison of the performance of four complex word", "labels": [], "entities": []}]}