{"title": [{"text": "FBK-HLT-NLP at SemEval-2016 Task 2: A Multitask, Deep Learning Approach for Interpretable Semantic Textual Similarity", "labels": [], "entities": [{"text": "FBK-HLT-NLP", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9468632340431213}]}], "abstractContent": [{"text": "We present the system developed at FBK for the SemEval 2016 Shared Task 2 \"Inter-pretable Semantic Textual Similarity\" as well as the results of the submitted runs.", "labels": [], "entities": [{"text": "FBK", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.9785934686660767}, {"text": "SemEval 2016 Shared Task 2", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.8320719003677368}]}, {"text": "We use a single neural network classification model for predicting the alignment at chunk level, the relation type of the alignment and the similarity scores.", "labels": [], "entities": []}, {"text": "Our best run was ranked as first in one the subtracks (i.e. raw input data, Student Answers), among 12 runs submitted, and the approach proved to be very robust across the different datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Semantic Textual Similarity (STS) task measures the degree of equivalence between the meaning of two texts, usually sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8015092270714896}]}, {"text": "In the Interpretable STS (iSTS) () the similarity is calculated at chunk level, and systems are asked to provide the type of the relationship between two chunks, as an interpretation of the similarity.", "labels": [], "entities": []}, {"text": "Given an input pair of sentences, participant systems were asked to: (i) identify the chunks in each sentence; (ii) align chunks across the two sentences; (iii) indicate the relation between the aligned chunks and (iv) specify the similarity score of each alignment.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 231, "end_pos": 247, "type": "METRIC", "confidence": 0.9603792130947113}]}, {"text": "The iSTS task has already been the object of an evaluation campaign in 2015, as a subtask of the SemEval-2015 Task 2: Semantic Textual Similarity ().", "labels": [], "entities": [{"text": "SemEval-2015 Task 2: Semantic Textual Similarity", "start_pos": 97, "end_pos": 145, "type": "TASK", "confidence": 0.6378502675465175}]}, {"text": "More in general, shared tasks for the identification and measurement of STS were organized in 2012 () and).", "labels": [], "entities": [{"text": "identification and measurement of STS", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.8016939520835876}]}, {"text": "Data provided to participants include three datasets: image captions (Images), pairs of sentences from news headlines, and a question-answer dataset collected and annotated during the evaluation of the BEETLE II tutorial dialogue system (Student Answers) (.", "labels": [], "entities": [{"text": "BEETLE II tutorial dialogue system", "start_pos": 202, "end_pos": 236, "type": "DATASET", "confidence": 0.6967352271080017}]}, {"text": "For each dataset, two subtracks were released: the first with raw input data (SYS), the second with data split in gold standard chunks (GS).", "labels": [], "entities": []}, {"text": "Given these input data, participants were required to identify the chunks in each sentence (for the first subtrack only), align chunks across the two sentences, specify the semantic relation of the alignment -selecting one of the following: EQUI for equivalent, OPPO for opposite, SPE1 and SPE2 if chunk in sentence1 is more specific than chunk in sentence2 and vice versa, SIMI for similar meanings, REL for chunks that have related meanings, and NOALI for chunk has no corresponding chunk in the other sentence)-, and provide a similarity score for each alignment, from 5 (maximum similarity/relatedness) to 0 (no relation at all).", "labels": [], "entities": [{"text": "EQUI", "start_pos": 241, "end_pos": 245, "type": "METRIC", "confidence": 0.982860803604126}, {"text": "OPPO", "start_pos": 262, "end_pos": 266, "type": "METRIC", "confidence": 0.9867892861366272}, {"text": "SIMI", "start_pos": 374, "end_pos": 378, "type": "METRIC", "confidence": 0.9178188443183899}, {"text": "REL", "start_pos": 401, "end_pos": 404, "type": "METRIC", "confidence": 0.9903016686439514}, {"text": "NOALI", "start_pos": 448, "end_pos": 453, "type": "METRIC", "confidence": 0.9953733086585999}, {"text": "similarity score", "start_pos": 530, "end_pos": 546, "type": "METRIC", "confidence": 0.9525029361248016}]}, {"text": "In addition, an optional tag for alignments showing factuality (FACT) or polarity (POL) phenomena, can be specified.", "labels": [], "entities": []}, {"text": "The evaluation is based on, which uses the F1 of precision and recall of token alignments.", "labels": [], "entities": [{"text": "F1", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.999054491519928}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.994754433631897}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.998323380947113}]}, {"text": "We participate in the iSTS shared task with a system that combines different features -including word embedding and chunk similarity -using a Multilayer Perceptrons (MLP).", "labels": [], "entities": []}, {"text": "Our main contribution was focused on the optimization of a Neural Network setting (i.e. topology, activation function, multi-task training) for the iSTS task.", "labels": [], "entities": []}, {"text": "We show that even with a relatively small and unbalanced training dataset, a neural network classifier can be built that achieves results very close to the best system.", "labels": [], "entities": []}, {"text": "Particularly, our system makes use of a single model for the different training sets of the task, proving to be very robust to domain differences.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the system we built; Section 3 reports the results we obtained and an evaluation of our system.", "labels": [], "entities": []}, {"text": "Finally, Section 4 provides some conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "\u2022 F1 on alignment classification (F); \u2022 F1 on alignment classification plus relation alignment type (+T); \u2022 F1 on alignment classification plus STS score (+S); \u2022 F1 on alignment classification plus relation alignment type and STS score (+TS); \u2022 Ranked position over the runs submitted: i.e. 13 runs for Images and Headlines SYS, 12 for Student Answer SYS, 20 for Images and Headlines GS and 19 for Student Answer GS (RANK) shows that for all the six subtracks run1 and run3 register better results.", "labels": [], "entities": [{"text": "F1", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.9947682619094849}, {"text": "F1", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9910945296287537}, {"text": "F1", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.996528685092926}, {"text": "STS score (+S)", "start_pos": 144, "end_pos": 158, "type": "METRIC", "confidence": 0.9197034239768982}, {"text": "F1", "start_pos": 162, "end_pos": 164, "type": "METRIC", "confidence": 0.9971107244491577}, {"text": "STS score (+TS)", "start_pos": 226, "end_pos": 241, "type": "METRIC", "confidence": 0.9331299781799316}, {"text": "Student Answer GS (RANK)", "start_pos": 398, "end_pos": 422, "type": "DATASET", "confidence": 0.49742011229197186}]}, {"text": "In particular, for what concerns GS subtasks (with already chunked sentences), run2 is ranked at least two positions lower with respect to the other two runs.", "labels": [], "entities": []}, {"text": "Since the difference between run2 and the other runs lays on the data used for training, these results seem to  suggest that the system takes advantage of a bigger training set with different domain data.", "labels": [], "entities": []}, {"text": "Instead, the size of the mini-batch (that is the difference between run1 and run3) does not seem to have a clear influence on the system performance, since in some cases run1 is higher ranked while in other cases run3 is higher ranked.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for the Baseline, Our System three runs and the Best System for the two subtracks split in the three datasets.", "labels": [], "entities": []}, {"text": " Table 3: Mean of the F+TS results in the two subtracks for the", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9975178241729736}, {"text": "F+TS", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.8825274705886841}]}]}