{"title": [{"text": "CLIP@UMD at SemEval-2016 Task 8: Parser for Abstract Meaning Representation using Learning to Search", "labels": [], "entities": [{"text": "SemEval-2016 Task 8", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8009990255037943}, {"text": "Abstract Meaning Representation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6546086271603903}]}], "abstractContent": [{"text": "In this paper we describe our approach to the Abstract Meaning Representation (AMR) parsing shared task as part of SemEval 2016.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR) parsing shared task", "start_pos": 46, "end_pos": 103, "type": "TASK", "confidence": 0.8360781139797635}]}, {"text": "We develop a novel technique to parse En-glish sentences into AMR using Learning to Search.", "labels": [], "entities": [{"text": "parse En-glish sentences into AMR", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.6735341608524322}]}, {"text": "We decompose the AMR parsing task into three subtasks-that of predicting the concepts , the relations, and the root.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.966460645198822}]}, {"text": "Each of these subtasks are treated as a sequence of predictions.", "labels": [], "entities": []}, {"text": "Using Learning to Search, we add past predictions as features for future predictions, and define a combined loss over the entire AMR structure.", "labels": [], "entities": [{"text": "AMR structure", "start_pos": 129, "end_pos": 142, "type": "DATASET", "confidence": 0.7394879758358002}]}], "introductionContent": [{"text": "This paper describes our submission to the Abstract Meaning Representation (AMR) Parsing Shared Task at SemEval 2016.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR) Parsing Shared Task at SemEval 2016", "start_pos": 43, "end_pos": 116, "type": "TASK", "confidence": 0.8337682137886683}]}, {"text": "The goal of the task is to generate AMRs automatically for English sentences.", "labels": [], "entities": []}, {"text": "We develop a novel technique for AMR parsing that uses Learning to Search (L2S)).", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9805456697940826}]}, {"text": "L2S is a family of approaches that solves structured prediction problems.", "labels": [], "entities": []}, {"text": "These algorithms have proven to be highly effective for problems in NLP like part-of-speech tagging, named entity recognition), coreference resolution (, and dependency parsing (.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7089201956987381}, {"text": "named entity recognition", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.6321293512980143}, {"text": "coreference resolution", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.9671517312526703}, {"text": "dependency parsing", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.8388775885105133}]}, {"text": "Briefly, L2S attempts to do structured prediction by (1) decomposing the production of the structured output in terms of an explicit search space (states, actions, etc.); and (2) learning hypotheses * The first two authors contributed equally to this work. that control a policy that takes actions in this search space.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.7069898247718811}]}, {"text": "AMR (, in turn, is a structured semantic representation which is a rooted, directed, acyclic graph.", "labels": [], "entities": [{"text": "AMR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6899934411048889}]}, {"text": "The nodes of this graph represent concepts in the given sentence and the edges represent relations between these concepts.", "labels": [], "entities": []}, {"text": "As such, the task of predicting AMRs can be naturally placed in the L2S framework.", "labels": [], "entities": [{"text": "predicting AMRs", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.8701891899108887}]}, {"text": "This allows us to model the learning of concepts and relations in a unified setting which aims to minimize the loss over the entire predicted structure.", "labels": [], "entities": []}, {"text": "In the next section, we briefly review DAGGER and explain its various components with respect to our AMR parsing task.", "labels": [], "entities": [{"text": "DAGGER", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.49313876032829285}, {"text": "AMR parsing task", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.9057170152664185}]}, {"text": "Section 3 describes our main algorithm along with the strategies we use to deal with the large search space of the search problem.", "labels": [], "entities": []}, {"text": "We then describe our experiments and results (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "The primary corpus for this shared task is the AMR Annotation Release 1.0 (LDC2015E86).", "labels": [], "entities": [{"text": "AMR Annotation Release 1.0 (LDC2015E86)", "start_pos": 47, "end_pos": 86, "type": "DATASET", "confidence": 0.851528525352478}]}, {"text": "This corpus consists of datasets from varied domains such as online discussion forums, blogs, and newswire, with about 19,000 sentence-AMR pairs.", "labels": [], "entities": []}, {"text": "All datasets have a pre-specified training, dev and test split.", "labels": [], "entities": []}, {"text": "The first was trained on all available training data.", "labels": [], "entities": []}, {"text": "The two other systems were trained using data from a single domain.", "labels": [], "entities": []}, {"text": "Specifically, we chose BOLT DF English and Proxy reports since these are the two largest training datasets individually.", "labels": [], "entities": [{"text": "BOLT DF English", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.6682054499785105}, {"text": "Proxy reports", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.8519142270088196}]}, {"text": "The system trained on the Proxy reports dataset performed the best when evaluated on the dev set.", "labels": [], "entities": [{"text": "Proxy reports dataset", "start_pos": 26, "end_pos": 47, "type": "DATASET", "confidence": 0.9373431007067362}]}, {"text": "Hence we used this system as our primary system for the task.", "labels": [], "entities": []}, {"text": "Additionally, we use DAGGER as implemented in the Vowpal Wabbit machine learning library ().", "labels": [], "entities": [{"text": "DAGGER", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.928196132183075}, {"text": "Vowpal Wabbit machine learning library", "start_pos": 50, "end_pos": 88, "type": "DATASET", "confidence": 0.9374250650405884}]}, {"text": "The evaluation of predicted AMRs is done using Smatch , which compares two AMRs using precision, recall and F 1 . Our system obtained a Smatch F 1 score of 0.46 with a P recision of 0.51 and a Recall of 0.43 on the test set in the Shared Task (We made a tokenization error during the actual semeval submission and so reported an F 1 score of 0.44 instead).", "labels": [], "entities": [{"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.999271810054779}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9987142086029053}, {"text": "F 1", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9931902289390564}, {"text": "Smatch F 1 score", "start_pos": 136, "end_pos": 152, "type": "METRIC", "confidence": 0.7718126997351646}, {"text": "P recision", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.9691168665885925}, {"text": "Recall", "start_pos": 193, "end_pos": 199, "type": "METRIC", "confidence": 0.9982706308364868}, {"text": "F 1 score", "start_pos": 329, "end_pos": 338, "type": "METRIC", "confidence": 0.9786980152130127}]}, {"text": "The mean F 1 score of all systems submitted to the shared task was 0.55 and the standard deviation was 0.06.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.988812267780304}, {"text": "standard deviation", "start_pos": 80, "end_pos": 98, "type": "METRIC", "confidence": 0.9190615117549896}]}], "tableCaptions": [{"text": " Table 4: Dataset statistics. All figures represent  number of sentences.", "labels": [], "entities": []}]}