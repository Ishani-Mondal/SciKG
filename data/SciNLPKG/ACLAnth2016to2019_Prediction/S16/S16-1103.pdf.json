{"title": [{"text": "MayoNLP at SemEval-2016 Task 1: Semantic Textual Similarity based on Lexical Semantic Net and Deep Learning Semantic Model", "labels": [], "entities": [{"text": "MayoNLP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7759202718734741}, {"text": "Semantic Textual Similarity", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.7429228027661642}]}], "abstractContent": [{"text": "Given two sentences, participating systems assign a semantic similarity score in the range of 0-5.", "labels": [], "entities": [{"text": "semantic similarity score", "start_pos": 52, "end_pos": 77, "type": "METRIC", "confidence": 0.6136344869931539}]}, {"text": "We applied two different techniques for the task: one is based on lexical semantic net (corresponding to run 1) and the other is based on deep learning semantic model (corresponding to run 2).", "labels": [], "entities": []}, {"text": "We also combined these two runs linearly (corresponding to run 3).", "labels": [], "entities": []}, {"text": "Our results indicate that the two techniques perform comparably while the combination outperforms the individual ones on four out of five datasets, namely answer-answer, headlines, plagiarism, and question-question, and on the overall weighted mean of STS 2016 and 2015 datasets.", "labels": [], "entities": [{"text": "STS 2016 and 2015 datasets", "start_pos": 252, "end_pos": 278, "type": "DATASET", "confidence": 0.8513995766639709}]}], "introductionContent": [{"text": "There is a growing need for an effective method to compute semantic similarity between two text snippets.", "labels": [], "entities": []}, {"text": "Many natural language processing (NLP) applications can benefit from effective semantic textual similarity (STS) techniques such as paraphrase recognition), textual summarization, automatic machine translation evaluation), tweets search (, and student answer assessment).", "labels": [], "entities": [{"text": "paraphrase recognition", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.7521383166313171}, {"text": "textual summarization", "start_pos": 157, "end_pos": 178, "type": "TASK", "confidence": 0.5861958563327789}, {"text": "machine translation evaluation", "start_pos": 190, "end_pos": 220, "type": "TASK", "confidence": 0.7730306684970856}]}, {"text": "The SemEval STS task series () have provided a vital platform for this task by making available a huge collection of sentence pairs with manual annotation for each sentence pair.", "labels": [], "entities": [{"text": "SemEval STS task", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7278063694636027}]}, {"text": "The objective of the task is to compute semantic similarity between a pair of sentences in the range, and where 0 indicates no similarity and 5 indicates complete similarity.", "labels": [], "entities": []}, {"text": "The approaches for computing semantic similarity between text snippets can be broadly classified into three approaches: information retrieval vector space model (e.g.), text alignment on the basis of semantic equivalence (e.g.) and machine learning models on the basis of lexical, syntactic and semantic features (e.g..", "labels": [], "entities": [{"text": "computing semantic similarity between text snippets", "start_pos": 19, "end_pos": 70, "type": "TASK", "confidence": 0.766835610071818}, {"text": "information retrieval vector space", "start_pos": 120, "end_pos": 154, "type": "TASK", "confidence": 0.826792299747467}, {"text": "text alignment", "start_pos": 169, "end_pos": 183, "type": "TASK", "confidence": 0.7900221049785614}]}, {"text": "In this paper, we describe two techniques for the SemEval 2016 English STS task: the first one adopts the text alignment technique on the basis of semantic equivalence by leveraging a semantic net and corpus statistics; and the second technique is a machine learning models where we utilize a deep learning semantic model.", "labels": [], "entities": [{"text": "SemEval 2016 English STS task", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.8679136633872986}, {"text": "text alignment", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7521985173225403}]}, {"text": "In the following, we present our techniques in detail and provide the evaluation results of our systems on STS 2016 and 2015 datasets.", "labels": [], "entities": [{"text": "STS 2016 and 2015 datasets", "start_pos": 107, "end_pos": 133, "type": "DATASET", "confidence": 0.8984572291374207}]}, {"text": "Conclusions and future directions are also provided.", "labels": [], "entities": []}], "datasetContent": [{"text": "We submitted 3 runs at SemEval 2016 English STS task based on the Section 2.", "labels": [], "entities": [{"text": "SemEval 2016 English STS task", "start_pos": 23, "end_pos": 52, "type": "DATASET", "confidence": 0.6781230330467224}]}, {"text": "shows the performance of each official run.", "labels": [], "entities": []}, {"text": "This is reported as the Pearson correlation between the scores produced by our systems and the human annotations.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 24, "end_pos": 43, "type": "METRIC", "confidence": 0.9770189523696899}]}, {"text": "The last row shows the value of weighted mean that is the sum of all datasets correlation scores.", "labels": [], "entities": [{"text": "correlation", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.9688665270805359}]}, {"text": "The weight assigned to a dataset is proportional to its number of pairs.", "labels": [], "entities": []}, {"text": "Our run 1 performed better than run 2 on the following datasets: answer-answer and postediting while run 2 performed better than run 1 on the following datasets: headlines, plagiarism and question-question.", "labels": [], "entities": []}, {"text": "Our run 3 that is a linear combination of run 1 and run 2 is the best performing system in terms of weighted mean.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Official Results of evaluation on STS 2016 data", "labels": [], "entities": [{"text": "STS 2016 data", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.7815007368723551}]}, {"text": " Table 2: Results of evaluation on STS 2015 data", "labels": [], "entities": [{"text": "STS 2015 data", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.7912006378173828}]}]}