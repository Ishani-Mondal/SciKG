{"title": [{"text": "USFD at SemEval-2016 Task 1: Putting different State-of-the-Arts into a Box", "labels": [], "entities": [{"text": "USFD at SemEval-2016 Task", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.8810894936323166}]}], "abstractContent": [{"text": "In this paper we describe our participation in the STS Core subtask which is the determination of the monolingual semantic similarity between pair of sentences.", "labels": [], "entities": [{"text": "STS Core subtask", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.8906403183937073}]}, {"text": "In our participation we adapted state-of-the-art approaches from related work applied on previous STS Core subtasks and run them on the 2016 data.", "labels": [], "entities": [{"text": "STS Core subtasks", "start_pos": 98, "end_pos": 115, "type": "DATASET", "confidence": 0.9498914480209351}, {"text": "2016 data", "start_pos": 136, "end_pos": 145, "type": "DATASET", "confidence": 0.7268328368663788}]}, {"text": "We investigated the performance of single methods but also the combination of them.", "labels": [], "entities": []}, {"text": "Our results show that Convolutional Neu-ral Networks (CNN) are superior to both the Monolingual Word Alignment and the Word2Vec approaches.", "labels": [], "entities": [{"text": "Monolingual Word Alignment", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.5193885266780853}]}, {"text": "The combination of all the three methods performs slightly better than using CNN only.", "labels": [], "entities": []}, {"text": "Our results also show that the performance of our systems varies between the datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) is a metric which aims to determine the likeness between two short textual entities.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7718111226956049}]}, {"text": "Therefore, STS is widely used in many research areas such as Natural Language Processing, fora large amount of tasks like Information Retrieval (IR), Natural Language Understanding (NLU) or even Machine Translation (MT) evaluation, for which STS allows capturing more information than traditional metrics based on n-grams match like BLEU ().", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 122, "end_pos": 148, "type": "TASK", "confidence": 0.8240306138992309}, {"text": "Machine Translation (MT) evaluation", "start_pos": 195, "end_pos": 230, "type": "TASK", "confidence": 0.8849331537882487}, {"text": "BLEU", "start_pos": 333, "end_pos": 337, "type": "METRIC", "confidence": 0.9924349188804626}]}, {"text": "Part of the SemEval campaign, STS competition benefits from a growing interest over the year (.", "labels": [], "entities": []}, {"text": "In 2016, participants had the possibility to compete in two tasks: (i) STS split into 'STS Core' and 'Cross-lingual STS' which are respectively an English monolingual and English/Spanish bilingual subtasks; (ii) iSTS which focuses on the interpretable aspect of STS assessment.", "labels": [], "entities": [{"text": "STS assessment", "start_pos": 262, "end_pos": 276, "type": "TASK", "confidence": 0.8507372736930847}]}, {"text": "Introduced for the first time in 2015, iSTS became a standalone task this year.", "labels": [], "entities": []}, {"text": "This paper describes our participation in the STS Core subtask and is organised as follows: we first describe in Section 2 methods we have adapted to tackle the task.", "labels": [], "entities": [{"text": "STS Core subtask", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9257981975873312}]}, {"text": "Next, we present and discuss our results (Section 3).", "labels": [], "entities": []}, {"text": "Finally, Section 4 will conclude this system description paper with some remarks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Pearson correlation between the prediction and gold standard data.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.884992241859436}, {"text": "gold standard data", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.5906189978122711}]}]}