{"title": [{"text": "SemEval-2016 Task 9: Chinese Semantic Dependency Parsing", "labels": [], "entities": [{"text": "SemEval-2016 Task 9", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8686331113179525}, {"text": "Chinese Semantic Dependency Parsing", "start_pos": 21, "end_pos": 56, "type": "TASK", "confidence": 0.5575455352663994}]}], "abstractContent": [{"text": "This paper describes the SemEval-2016 Shared Task 9: Chinese semantic Dependency Parsing.", "labels": [], "entities": [{"text": "SemEval-2016 Shared Task 9: Chinese semantic Dependency Parsing", "start_pos": 25, "end_pos": 88, "type": "TASK", "confidence": 0.7036845717165205}]}, {"text": "We extend the traditional tree-structured representation of Chinese sentence to directed acyclic graphs that can capture richer latent semantics, and the goal of this task is to identify such semantic structures from a corpus of Chinese sentences.", "labels": [], "entities": []}, {"text": "We provide two distinguished corpora in the NEWS domain with 10,068 sentences and the TEXTBOOKS domain with 14,793 sentences respectively.", "labels": [], "entities": [{"text": "NEWS domain", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.952045202255249}, {"text": "TEXTBOOKS", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.7821431159973145}]}, {"text": "We will first introduce the motivation for this task, and then present the task in detail including data preparation, data format , task evaluation and soon.", "labels": [], "entities": [{"text": "data preparation", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.686696246266365}]}, {"text": "At last, we briefly describe the submitted systems and analyze these results.", "labels": [], "entities": []}], "introductionContent": [{"text": "This task is a rerun of the task 5 at SemEval 2012 (), named Chinese semantic dependency parsing (SDP).", "labels": [], "entities": [{"text": "SemEval 2012", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.6581708341836929}, {"text": "Chinese semantic dependency parsing (SDP)", "start_pos": 61, "end_pos": 102, "type": "TASK", "confidence": 0.7470437628882272}]}, {"text": "In the previous task, we aimed at investigating \"deep\" semantic relations within sentences through tree-structured dependencies.", "labels": [], "entities": []}, {"text": "As traditionally defined, syntactic dependency parsing results are connected trees defined overall words of a sentence and language-specific grammatical functions.", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.7332664330800375}]}, {"text": "On the contrary, in semantic dependency parsing, each head-dependent arc instead bears a semantic relation, rather than grammatical relation.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6720403830210367}]}, {"text": "In this way, semantic dependency parsing results can be used to answer questions directly, like who did what to whom when and where.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.7195548415184021}]}, {"text": "However, according to the meaning-text linguistic theory (Mel'\u02c7 cuk and\u017dolkovskijand\u02c7and\u017dolkovskij, 1965), a theoretical framework for the description of natural languages, it is said that trees are not sufficient to express the complete meaning of sentences in some cases, which has been proven undoubted in our practice of corpus annotation.", "labels": [], "entities": []}, {"text": "This time, not only do we refine the easy-to-understand meaning representation in Chinese in order to decrease ambiguity or fuzzy boundary of semantic relations on the basis of Chinese linguistic knowledge, we extend the dependency structure to directed acyclic graphs that conform to the characteristics of Chinese, because Chinese is an parataxis language with flexible word orders, and rich latent information is hidden in facial words.", "labels": [], "entities": []}, {"text": "illustrates an example of semantic dependency graph.", "labels": [], "entities": []}, {"text": "Here, \"\u5979 (she)\" is the argument of \"\u8138\u8272 (face)\" and at the same time it is an argument of \"\u75c5 (disease)\".", "labels": [], "entities": []}, {"text": "Researchers in dependency parsing community realized dependency parsing restricted in a tree structure is still too shallow, so they explored semantic information beyond tree structure in task 8 at and task 18 at SemEval 2015 (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8068955540657043}, {"text": "dependency parsing", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7753889858722687}]}, {"text": "They provided data in similar structure with what we are going to provide, but in distinct semantic representation systems.", "labels": [], "entities": []}, {"text": "Once again we propose this task to promote research that will lead to deeper understanding of Chinese sentences, and we believe that freely available and well annotated corpora which can be used as common testbed is necessary to promote research in data-driven statistical dependency parsing.", "labels": [], "entities": [{"text": "statistical dependency parsing", "start_pos": 261, "end_pos": 291, "type": "TASK", "confidence": 0.6769219835599264}]}], "datasetContent": [{"text": "During the phase of evaluation, each system should propose parsing results on the previously unseen testing data.", "labels": [], "entities": []}, {"text": "Similar with training phase, testing files containing sentences in two domains will be released separately.", "labels": [], "entities": []}, {"text": "The final rankings will refer to the average results of the two testing files (taking training data size into consideration).", "labels": [], "entities": []}, {"text": "We compare predicted dependencies (predicate-roleargument triples, and some of them contain roots of the whole sentences) with our human-annotated ones, which are regarded as gold dependencies.", "labels": [], "entities": []}, {"text": "Our evaluate measures are on two granularity, dependency arc and the complete sentence.", "labels": [], "entities": []}, {"text": "Labeled and unlabeled precision and recall with respect to predicted dependencies will be used as evaluation measures.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9965997338294983}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9988142251968384}]}, {"text": "Since non-local dependencies (following, we call these dependency arcs making dependency trees collapsed non-local ones) discovery is extremely difficult, we will evaluate non-local dependencies separately.", "labels": [], "entities": []}, {"text": "For sentences level, we will use labeled and unlabeled exact match to measure sentence parsing accuracy.", "labels": [], "entities": [{"text": "sentence parsing", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.6938885003328323}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.867244303226471}]}, {"text": "Following Task 8 at SemEval 2014, below and in other taskrelated contexts, we abbreviate these metrics as: \u2022 Labeled precision (LP), recall (LR), F1 (LR) and recall for non-local dependencies (NLR); \u2022 Unlabeled precision (UP), recall (UR), F1 (UF) and recall for non-local dependencies (NUR); \u2022 Labeled and unlabeled exact match (LM, UM).", "labels": [], "entities": [{"text": "Labeled precision (LP)", "start_pos": 109, "end_pos": 131, "type": "METRIC", "confidence": 0.7854585707187652}, {"text": "recall (LR)", "start_pos": 133, "end_pos": 144, "type": "METRIC", "confidence": 0.9276182949542999}, {"text": "F1 (LR)", "start_pos": 146, "end_pos": 153, "type": "METRIC", "confidence": 0.9311698228120804}, {"text": "recall", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.9730609059333801}, {"text": "Unlabeled precision (UP)", "start_pos": 201, "end_pos": 225, "type": "METRIC", "confidence": 0.7869355976581573}, {"text": "recall (UR)", "start_pos": 227, "end_pos": 238, "type": "METRIC", "confidence": 0.912571594119072}, {"text": "F1 (UF)", "start_pos": 240, "end_pos": 247, "type": "METRIC", "confidence": 0.919308602809906}, {"text": "recall", "start_pos": 252, "end_pos": 258, "type": "METRIC", "confidence": 0.9823434352874756}]}, {"text": "When ranking systems participating in this task, we mainly refer to the average F1 (LF) on the two testing sets.", "labels": [], "entities": [{"text": "F1 (LF)", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9504646509885788}]}], "tableCaptions": [{"text": " Table 2: Statics of the corpus.", "labels": [], "entities": []}, {"text": " Table 4: Results of the submitted systems.", "labels": [], "entities": []}]}