{"title": [{"text": "SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation", "labels": [], "entities": [{"text": "Cross-Lingual Evaluation", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.707928255200386}]}], "abstractContent": [{"text": "Semantic Textual Similarity (STS) seeks to measure the degree of semantic equivalence between two snippets of text.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8071900407473246}]}, {"text": "Similarity is expressed on an ordinal scale that spans from semantic equivalence to complete unrelated-ness.", "labels": [], "entities": []}, {"text": "Intermediate values capture specifically defined levels of partial similarity.", "labels": [], "entities": []}, {"text": "While prior evaluations constrained themselves to just monolingual snippets of text, the 2016 shared task includes a pilot subtask on computing semantic similarity on cross-lingual text snippets.", "labels": [], "entities": []}, {"text": "This year's traditional mono-lingual subtask involves the evaluation of En-glish text snippets from the following four domains: Plagiarism Detection, Post-Edited Machine Translations, Question-Answering and News Article Headlines.", "labels": [], "entities": [{"text": "Plagiarism Detection", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.6487753540277481}, {"text": "Post-Edited Machine Translations", "start_pos": 150, "end_pos": 182, "type": "TASK", "confidence": 0.5979020595550537}, {"text": "News Article Headlines", "start_pos": 207, "end_pos": 229, "type": "TASK", "confidence": 0.6004340350627899}]}, {"text": "From the question-answering domain, we include both question-question and answer-answer pairs.", "labels": [], "entities": []}, {"text": "The cross-lingual subtask provides paired Spanish-English text snippets drawn from the same sources as the English data as well as independently sampled news data.", "labels": [], "entities": []}, {"text": "The English sub-task attracted 43 participating teams producing 119 system submissions, while the cross-lingual Spanish-English pilot subtask attracted 10 teams resulting in 26 systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) assesses the degree to which the underlying semantics of two segments of text are equivalent to each other.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7997625569502512}]}, {"text": "This assessment is performed using an ordinal scale that The authors of this paper are listed in alphabetic order.", "labels": [], "entities": []}, {"text": "ranges from complete semantic equivalence to complete semantic dissimilarity.", "labels": [], "entities": []}, {"text": "The intermediate levels capture specifically defined degrees of partial similarity, such as topicality or rough equivalence, but with differing details.", "labels": [], "entities": []}, {"text": "The snippets being scored are approximately one sentence in length, with their assessment being performed outside of any contextualizing text.", "labels": [], "entities": []}, {"text": "While STS has previously just involved judging text snippets that are written in the same language, this year's evaluation includes a pilot subtask on the evaluation of cross-lingual sentence pairs.", "labels": [], "entities": []}, {"text": "The systems and techniques explored as apart of STS have abroad range of applications including Machine Translation (MT), Summarization, Generation and Question Answering (QA).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.845881724357605}, {"text": "Summarization, Generation", "start_pos": 122, "end_pos": 147, "type": "TASK", "confidence": 0.7632717490196228}, {"text": "Question Answering (QA)", "start_pos": 152, "end_pos": 175, "type": "TASK", "confidence": 0.7937635540962219}]}, {"text": "STS allows for the independent evaluation of methods for computing semantic similarity drawn from a diverse set of domains that would otherwise be only studied within a particular subfield of computational linguistics.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9385630488395691}]}, {"text": "Existing methods from a subfield that are found to perform well in a more general setting as well as novel techniques created specifically for STS may improve any natural language processing or language understanding application where knowing the similarity in meaning between two pieces of text is relevant to the behavior of the system.", "labels": [], "entities": [{"text": "natural language processing or language understanding", "start_pos": 163, "end_pos": 216, "type": "TASK", "confidence": 0.6338665584723154}]}, {"text": "La mujer est\u00e1 tocando el viol\u00edn.", "labels": [], "entities": []}, {"text": "The young lady enjoys listening to the guitar.", "labels": [], "entities": []}, {"text": "0 The two sentences are completely dissimilar.", "labels": [], "entities": []}, {"text": "John went horseback riding at dawn with a whole group of friends.", "labels": [], "entities": [{"text": "horseback riding", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8129718601703644}]}, {"text": "Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.", "labels": [], "entities": []}, {"text": "Al amanecer, Juan se fue a montar a caballo con un grupo de amigos.", "labels": [], "entities": []}, {"text": "Sunrise at dawn is a magnificent view to take in if you wake up early enough for it. rization of both paraphrase detection and textual entailment to a finer grained similarity scale.", "labels": [], "entities": [{"text": "rization", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.7239252328872681}, {"text": "paraphrase detection", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.8659060299396515}]}, {"text": "The additional degrees of similarity introduced by STS are directly relevant to many applications where intermediate levels of similarity are significant.", "labels": [], "entities": []}, {"text": "For example, when evaluating machine translation system output, it is desirable to give credit for partial semantic equivalence to human reference translations.", "labels": [], "entities": [{"text": "machine translation system output", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.8082707226276398}]}, {"text": "Similarly, a summarization system may prefer short segments of text with a rough meaning equivalence to longer segments with perfect semantic coverage.", "labels": [], "entities": [{"text": "summarization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9752534031867981}]}, {"text": "STS is related to research into machine translation evaluation metrics.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9264354705810547}, {"text": "machine translation evaluation metrics", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.9073561728000641}]}, {"text": "This subfield of machine translation investigates methods for replicating human judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7966107130050659}, {"text": "replicating human judgements", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.8749354283014933}]}, {"text": "STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME-TEOR, TER).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9983606934547424}, {"text": "MEANT", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.9427100419998169}, {"text": "TER", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9661344289779663}]}, {"text": "The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation.", "labels": [], "entities": [{"text": "machine translation quality estimation", "start_pos": 89, "end_pos": 127, "type": "TASK", "confidence": 0.8624593764543533}]}, {"text": "The STS shared task has been held annually since 2012, providing avenue for the evaluation of state-of-the-art algorithms and models.", "labels": [], "entities": []}, {"text": "During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet),),), web discussion forums, and Q&A data sets).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 172, "end_pos": 179, "type": "DATASET", "confidence": 0.9392916560173035}]}, {"text": "This year's evaluation adds new data sets drawn from plagiarism detection and post-edited machine translations.", "labels": [], "entities": []}, {"text": "We also introduce an evaluation set on Q&A forum question-question similarity and revisit news headlines and Q&A answer-answer similarity.", "labels": [], "entities": [{"text": "Q&A forum question-question similarity", "start_pos": 39, "end_pos": 77, "type": "TASK", "confidence": 0.6877456456422806}, {"text": "Q&A answer-answer similarity", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.5921898186206818}]}, {"text": "The 2016 task includes both a traditional monolingual subtask with English data and a pilot cross-lingual subtask that pairs together Spanish and English texts.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports the evaluation results for the 2016 STS English and cross-lingual SpanishEnglish subtasks.", "labels": [], "entities": [{"text": "STS English and cross-lingual SpanishEnglish subtasks", "start_pos": 57, "end_pos": 110, "type": "DATASET", "confidence": 0.7003240784009298}]}, {"text": "On each test set, systems are evaluated based on their Pearson correlation with the gold standard STS labels.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 55, "end_pos": 74, "type": "METRIC", "confidence": 0.9665279388427734}]}, {"text": "The overall score for each system is computed as the average of the correlation values on the individual evaluation sets, weighted by the number of data points in each evaluation set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: STS 2016: English Rankings. Late or corrected systems are marked with a * symbol.", "labels": [], "entities": [{"text": "English Rankings", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.8133094608783722}]}, {"text": " Table 5: STS 2016: English Rankings (continued). As before, late or corrected systems are marked with a * symbol. The baseline", "labels": [], "entities": [{"text": "English Rankings", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.9114257097244263}]}, {"text": " Table 6: Best and median scores by evaluation set for the En-", "labels": [], "entities": []}]}