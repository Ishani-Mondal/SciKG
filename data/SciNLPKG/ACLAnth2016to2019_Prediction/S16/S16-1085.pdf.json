{"title": [{"text": "SemEval 2016 Task 11: Complex Word Identification", "labels": [], "entities": [{"text": "SemEval 2016 Task 11", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8486392498016357}, {"text": "Complex Word Identification", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6613568266232809}]}], "abstractContent": [{"text": "We report the findings of the Complex Word Identification task of SemEval 2016.", "labels": [], "entities": [{"text": "Complex Word Identification task", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.7139012590050697}]}, {"text": "To create a dataset, we conduct a user study with 400 non-native English speakers, and find that complex words tend to be rarer, less ambiguous and shorter.", "labels": [], "entities": []}, {"text": "A total of 42 systems were submitted from 21 distinct teams, and nine baselines were provided.", "labels": [], "entities": []}, {"text": "The results highlight the effectiveness of Decision Trees and Ensemble methods for the task, but ultimately reveal that word frequencies remain the most reliable predictor of word complexity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Complex Word Identification (CWI) is the task of deciding which words should be simplified in a given text.", "labels": [], "entities": [{"text": "Complex Word Identification (CWI)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7637993494669596}]}, {"text": "It is commonly connected with the task of Lexical Simplification (LS), which has as goal to replace complex words and expressions with simpler alternatives.", "labels": [], "entities": [{"text": "Lexical Simplification (LS)", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.8921387672424317}]}, {"text": "In the usual LS pipeline, which was first introduced by, CWI is the first step.", "labels": [], "entities": []}, {"text": "An effective CWI strategy can prevent LS approaches from replacing simple words, and hence prevent them from making grammatical and/or semantic errors.", "labels": [], "entities": []}, {"text": "Early LS approaches) do not include CWI.", "labels": [], "entities": []}, {"text": "As shown in, ignoring this step can considerably decrease the quality of the output produced by a simplifier.", "labels": [], "entities": []}, {"text": "CWI has been gaining popularity in recent research.", "labels": [], "entities": [{"text": "CWI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8034613132476807}]}, {"text": "The LS approach in () employs an implicit CWI strategy in which a target word is only deemed complex if the LS model can find a candidate substitution which is simpler.", "labels": [], "entities": []}, {"text": "Their results, however, show that the approach is unable to find simplifications for one third of the complex words in the dataset.) presents the CW corpus: the first dataset for CWI.", "labels": [], "entities": [{"text": "CW corpus", "start_pos": 146, "end_pos": 155, "type": "DATASET", "confidence": 0.7436783015727997}]}, {"text": "Although a relevant contribution, this dataset contains only 731 instances extracted automatically from the Simple English Wikipedia edits, which raises concerns about its reliability and applicability.", "labels": [], "entities": [{"text": "Simple English Wikipedia edits", "start_pos": 108, "end_pos": 138, "type": "DATASET", "confidence": 0.8172630816698074}]}, {"text": "The results obtained by Shardlow (2013a) highlight some of the issues of the dataset.", "labels": [], "entities": []}, {"text": "They use the CW corpus to compare the performance of three solutions to CWI: a Threshold-Based approach, a Support Vector Machine (SVM), and a \"Simplify Everything\" approach.", "labels": [], "entities": [{"text": "CW corpus", "start_pos": 13, "end_pos": 22, "type": "DATASET", "confidence": 0.6968043297529221}]}, {"text": "In their experiments, the \"Simplify Everything\" approach achieves higher Accuracy, Recall and F-scores than all other systems, suggesting that simplifying all words in a sentence is the most effective approach for CWI.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9988479614257812}, {"text": "Recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9599081873893738}, {"text": "F-scores", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9899405837059021}]}, {"text": "These results are clearly counter intuitive and conflicting with the conclusions drawn in.", "labels": [], "entities": []}, {"text": "In this paper we describe the first edition of the Complex Word Identification task, organized at SemEval 2016.", "labels": [], "entities": [{"text": "Complex Word Identification task", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.7134402394294739}]}, {"text": "This is an initiative that aims to provide reliable resources and new insights for CWI, as well as to establish the state of the art performance in CWI for English texts, and bring more visibility to the area of Text Simplification.", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 212, "end_pos": 231, "type": "TASK", "confidence": 0.7276367843151093}]}], "datasetContent": [{"text": "We have created two training datasets for the task: joint and decomposed.", "labels": [], "entities": []}, {"text": "Both contain all instances which were annotated by 20 non-native speakers.", "labels": [], "entities": []}, {"text": "The joint dataset contains a single label for each instance, which is 1 if at least one of the 20 annotators has deemed it complex, and 0 otherwise.", "labels": [], "entities": []}, {"text": "Differently, the decomposed dataset contains one label for each of the 20 annotators, which is 1 if they have judged it to be complex, and 0 otherwise.", "labels": [], "entities": []}, {"text": "Along with the labels, the dataset instances also include the sentence, target word and its position.", "labels": [], "entities": []}, {"text": "Participants were allowed to use any additional external resources to build their models.", "labels": [], "entities": []}, {"text": "A participant could, for an example, use other (not necessarily publicly available) datasets to complement the one provided.", "labels": [], "entities": []}, {"text": "The test set is composed by all the instances annotated by only one non-native speaker.", "labels": [], "entities": []}, {"text": "While the training sets contain the data pertaining to the same 2,237 instances, the test set contains 88,221 instances.", "labels": [], "entities": []}, {"text": "Using this setup, we are able to replicate a realistic scenario in Text Simplification, where the needs of many readers must be predicted based on the needs of a sample of the reader population.", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.6866632997989655}]}, {"text": "shows some examples of instances from our joint training set.", "labels": [], "entities": []}, {"text": "To assess the systems' performance, we choose to complement the typical F-score, which is the harmonic mean between Precision and Recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9964706897735596}, {"text": "Precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9453074336051941}, {"text": "Recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.7772791981697083}]}, {"text": "Even though F-score is arguably the most frequently used evaluation metric to compare the performance of classifiers, we feel that, as far as the relationship between Complex Word Identification and Lexical 1 http://ogden.basic-english.org/words.html Simplification are concerned, it does not accurately capture the effectiveness of a solution for the task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9831803441047668}]}, {"text": "To motivate our decision, we must first outline the characteristics of a great lexical simplifier.", "labels": [], "entities": []}, {"text": "In order to be both effective and reliable, it must accomplish two things simultaneously: 1.", "labels": [], "entities": []}, {"text": "Not to make any replacements that compromise the sentences' grammaticality and/or meaning.", "labels": [], "entities": []}, {"text": "2. To make a text as simple as possible.", "labels": [], "entities": []}, {"text": "In order to help a simplifier achieve these goals, a complex word identifier must consequently: 1.", "labels": [], "entities": []}, {"text": "Avoid labeling complex words as simple, and hence impede them from being simplified.", "labels": [], "entities": []}, {"text": "2. Avoid labeling simple words as complex, and hence allow for unnecessary, possibly erroneous simplifications.", "labels": [], "entities": []}, {"text": "3. To capture as many complex words as possible, and hence maximise the simplicity of a sentence.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9933462142944336}]}, {"text": "Now that we have outlined what the ideal identifier must do, we can translate these objectives into typical evaluation expressions used in the context of classification problems.", "labels": [], "entities": []}, {"text": "In this context, \"positive\" and \"negative\" decisions refer to labeling words as complex and simple, respectively.", "labels": [], "entities": []}, {"text": "While objectives number one and two state that the identifier must minimise the number of false negatives and false positives, item three states that it must maximise the number of true positives.", "labels": [], "entities": []}, {"text": "One way to measure the proficiency of a classifier in achieving these goals is through Accuracy and Recall, respectively.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9983627200126648}, {"text": "Recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9872137308120728}]}, {"text": "In order to balance these two metrics, we have conceived the G-score, which measures the harmonic mean between Accuracy and Recall.", "labels": [], "entities": [{"text": "G-score", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9536291360855103}, {"text": "Accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9501200318336487}, {"text": "Recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.8282041549682617}]}, {"text": "For completion, we also report the systems' ranking according to F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9952921867370605}]}], "tableCaptions": [{"text": " Table 1: Results of dataset analysis", "labels": [], "entities": []}, {"text": " Table 2: Mean and standard deviation for word features", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987202882766724}]}, {"text": " Table 4: Final system ranks and scores. Baselines are in boldface.", "labels": [], "entities": []}]}