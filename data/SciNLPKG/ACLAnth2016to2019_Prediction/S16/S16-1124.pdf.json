{"title": [{"text": "UWB at SemEval-2016 Task 2: Interpretable Semantic Textual Similarity with Distributional Semantics for Chunks", "labels": [], "entities": [{"text": "UWB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7791968584060669}]}], "abstractContent": [{"text": "We introduce a system focused on solving SemEval 2016 Task 2-Interpretable Semantic Textual Similarity.", "labels": [], "entities": [{"text": "SemEval 2016 Task 2-Interpretable Semantic Textual Similarity", "start_pos": 41, "end_pos": 102, "type": "TASK", "confidence": 0.8423884510993958}]}, {"text": "The system explores machine learning and rule-based approaches to the task.", "labels": [], "entities": []}, {"text": "We focus on machine learning and experiment with a wide variety of machine learning algorithms as well as with several types of features.", "labels": [], "entities": []}, {"text": "The core of our system consists in exploiting distributional semantics to compare similarity of sentence chunks.", "labels": [], "entities": []}, {"text": "The system won the competition in 2016 in the \"Gold standard chunk scenario\".", "labels": [], "entities": []}, {"text": "We have not participated in the \"System chunk scenario\".", "labels": [], "entities": [{"text": "System chunk", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.6842578053474426}]}], "introductionContent": [{"text": "The goal of the Interpretable Semantic Textual Similarity task is to go deeper with the assessment of semantic textual similarity of sentence pairs.", "labels": [], "entities": [{"text": "Interpretable Semantic Textual Similarity", "start_pos": 16, "end_pos": 57, "type": "TASK", "confidence": 0.626716747879982}]}, {"text": "It is requested to add an explanatory layer that offers a deeper insight into the sentence similarities.", "labels": [], "entities": []}, {"text": "The sentences are split into chunks and the first goal is to find corresponding chunks (with respect to their meanings) among the compared sentences.", "labels": [], "entities": []}, {"text": "When the corresponding chunks are known, the chunks are annotated with their similarity scores and their relation types (e.g. equivalent, more specific, etc).", "labels": [], "entities": []}, {"text": "The task follows a pilot task from the preceding).", "labels": [], "entities": []}, {"text": "The best performing systems adopted various approaches, () relied on handcrafter rules, () employed a classifier for relation types and they associated each relation with a precomputed similarity score and () extended their word alignment algorithm for the task.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 224, "end_pos": 238, "type": "TASK", "confidence": 0.7584210634231567}]}], "datasetContent": [{"text": "Machine learning approach We employ the following classifiers and classification frameworks: \u2022 Alignment binary classification -Voted perceptron (Weka).", "labels": [], "entities": [{"text": "Alignment binary classification", "start_pos": 95, "end_pos": 126, "type": "TASK", "confidence": 0.7721059521039327}]}, {"text": "\u2022 Score classification -Maximum entropy (Brainy).", "labels": [], "entities": [{"text": "Score classification", "start_pos": 2, "end_pos": 22, "type": "TASK", "confidence": 0.7072065770626068}]}, {"text": "\u2022 Type classification -Support vector machines (Brainy).", "labels": [], "entities": [{"text": "Type classification", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.8943321406841278}]}, {"text": "These classifiers perform best on the evaluation datasets.", "labels": [], "entities": []}, {"text": "We achieved the best results for estimating chunk similarity with Word2Vec and the modified lexical semantic vectors -see Section 2.2.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.967325747013092}]}, {"text": "We experimented with reduced feature set (word overlap, word positions difference, POS tags difference, semantic similarity, global semantic similarity, paraphrase database) -run 1 and with all featuresrun 3.", "labels": [], "entities": []}, {"text": "The run 1 contains the optimal combination of features.", "labels": [], "entities": []}, {"text": "Since this combination is established on evaluation datasets it does not need to be optimal for the test datasets.", "labels": [], "entities": []}, {"text": "To increase our chances in the completion, we also run the system with all features -run 3.", "labels": [], "entities": []}, {"text": "We use the provided annotated evaluation dataset (Images, Headlines, Answer students) for training the models.", "labels": [], "entities": []}, {"text": "We train three models, each for one dataset.", "labels": [], "entities": []}, {"text": "For development, we use the 10-fold crossvalidation.", "labels": [], "entities": []}, {"text": "For final test runs, we train the three models on evaluation datasets and run the system on the corresponding test datasets (e.g. Images evaluation dataset based model is used to annotate Images test data).", "labels": [], "entities": []}, {"text": "We do not neither modify the original datasets nor annotate any additional data.", "labels": [], "entities": []}, {"text": "Rule-based approach There are little options in this approach.", "labels": [], "entities": []}, {"text": "Again, we have achieved the best results for estimating chunk similarity with Word2Vec and the modified lexical semantic vectors -see Section 2.2.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9654211401939392}]}, {"text": "We set the threshold for the similarity score to 2.5.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 29, "end_pos": 45, "type": "METRIC", "confidence": 0.986314445734024}]}, {"text": "All lower values are set to 0.", "labels": [], "entities": []}, {"text": "This is the run 2.", "labels": [], "entities": []}, {"text": "Individual setting for different dataset We restrained from setting individual configurations for different datasets.", "labels": [], "entities": []}, {"text": "The setup is completely identical for all datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Official system evaluation.", "labels": [], "entities": []}]}