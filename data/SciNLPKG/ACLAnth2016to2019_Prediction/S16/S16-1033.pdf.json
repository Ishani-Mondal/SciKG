{"title": [{"text": "UniPI at SemEval-2016 Task 4: Convolutional Neural Networks for Sen- timent Classification", "labels": [], "entities": [{"text": "Sen- timent Classification", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.6786669343709946}]}], "abstractContent": [{"text": "The paper describes our submission to the task on Sentiment Analysis on Twitter at SemEval 2016.", "labels": [], "entities": [{"text": "Sentiment Analysis on Twitter at SemEval 2016", "start_pos": 50, "end_pos": 95, "type": "TASK", "confidence": 0.8116418719291687}]}, {"text": "The approach is based on a Deep Learning architecture using convolu-tional neural networks.", "labels": [], "entities": []}, {"text": "The approach used only word embeddings as features.", "labels": [], "entities": []}, {"text": "The submission used embeddings created from a corpus of news articles.", "labels": [], "entities": []}, {"text": "We report on further experiments using embeddings built fora corpus of tweets as well as sentiment specific word em-beddings obtained by distant supervision.", "labels": [], "entities": []}], "introductionContent": [{"text": "Up until recently, the typical approaches to sentiment analysis of tweets were based on classifiers trained using several hand-crafted features, in particular lexicons of words with an assigned polarity value.", "labels": [], "entities": [{"text": "sentiment analysis of tweets", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.9178119003772736}]}, {"text": "At the SemEval 2015 task 10 on Sentiment Analysis of Twitter (), most systems relied on features derived from sentiment lexicons.", "labels": [], "entities": [{"text": "SemEval 2015 task 10", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.8871012479066849}, {"text": "Sentiment Analysis of Twitter", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.8686568737030029}]}, {"text": "Other important features included bag-ofwords features, hash-tags, handling of negation, word shape and punctuation features, elongated words, etc.", "labels": [], "entities": [{"text": "handling of negation", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.6556485493977865}]}, {"text": "Moreover, tweet pre-processing and normalization were an important part of the processing pipeline.", "labels": [], "entities": []}, {"text": "Quite significantly, the top scoring system in subtask A: Phrase-Level Polarity ( was instead based on the use of a convolutional neural network, which used word embeddings as its only features.", "labels": [], "entities": []}, {"text": "Word embeddings were created by unsupervised learning from a collection of 50 million tweets, using the SkipGram model by.", "labels": [], "entities": []}, {"text": "The tweets used for training were collected by querying the Twitter API on the presence of a set of emoticons representing positive or negative sentiment.", "labels": [], "entities": []}, {"text": "The winning team achieved an F1 of 84.79 on the Twitter2015 test set.", "labels": [], "entities": [{"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9995900988578796}, {"text": "Twitter2015 test set", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.985540529092153}]}, {"text": "The team participated with a similar approach also to subtask B: Message-Level Polarity, achieving the second best score with an F1 of 64.59.", "labels": [], "entities": [{"text": "F1", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.9991913437843323}]}, {"text": "The fourth F1 score of 64.17 was achieved also by a system exploiting word embeddings by INESC-ID.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9863959550857544}, {"text": "INESC-ID", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.9639939665794373}]}, {"text": "The top scoring system instead consisted in an ensemble combining four Twitter sentiment classification approaches that participated in previous editions, with an F1 of 64.84.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 71, "end_pos": 103, "type": "TASK", "confidence": 0.549891879161199}, {"text": "F1", "start_pos": 163, "end_pos": 165, "type": "METRIC", "confidence": 0.9988455772399902}]}, {"text": "We decided to explore a similar approach for tackling SemEval 2016 task 4 on Sentiment Analysis in).", "labels": [], "entities": [{"text": "tackling SemEval 2016 task 4 on Sentiment Analysis", "start_pos": 45, "end_pos": 95, "type": "TASK", "confidence": 0.7592731714248657}]}], "datasetContent": [{"text": "The settings for the experiment were the following: word embeddings of 300 dimensions from a Google News corpus 2 , filters of size 7,7,7 each with a 100 feature maps, dropout rate 0.5, MLP hidden units 100, batch size 50, adadelta decay 0.95, convolutional layer activation relu, training epochs 25.", "labels": [], "entities": [{"text": "Google News corpus", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.7394649883111318}]}, {"text": "For choosing the setting of our single submission, we took into account the suggestion from the experiments carried out by.", "labels": [], "entities": []}, {"text": "The experiments were run on a linux server with an nVIDIA Tesla K40 accelerated GPU.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Detailed scores of UniPI official submission.", "labels": [], "entities": [{"text": "Detailed", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.852242648601532}, {"text": "UniPI official submission", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.7640199065208435}]}, {"text": " Table 3. Detailed scores of top submission for Task 4,  subtask A.", "labels": [], "entities": [{"text": "Detailed", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8888494372367859}]}, {"text": " Table 4. Unofficial run using the full training set and embed- dings from a corpus of 35 million tweets.", "labels": [], "entities": []}, {"text": " Table 1. Official results of our submission compared to the top one, and an unofficial run.", "labels": [], "entities": []}, {"text": " Table 5. Unofficial run using embeddings from tweets and fil- ters of sizes 3,5,7,7.", "labels": [], "entities": []}, {"text": " Table 6. Run using SWE and filters of size 3,5,7,7.", "labels": [], "entities": []}, {"text": " Table 7. Test performed using the code by Moschit- ti&Severyn for SemEval 2015.", "labels": [], "entities": [{"text": "SemEval 2015", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.5508831143379211}]}]}