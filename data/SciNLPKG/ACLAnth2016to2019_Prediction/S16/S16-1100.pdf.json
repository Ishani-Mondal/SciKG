{"title": [{"text": "DCU-SEManiacs at SemEval-2016 Task 1: Synthetic Paragram Embeddings for Semantic Textual Similarity", "labels": [], "entities": []}], "abstractContent": [{"text": "We experiment with learning word representations designed to be combined into sentence-level semantic representations, using an objective function which does not directly make use of the supervised scores provided with the training data, instead opting fora simpler objective which encourages similar phrases to be close together in the embedding space.", "labels": [], "entities": []}, {"text": "This simple objective lets us start with high-quality embeddings trained using the Paraphrase Database (PPDB) (Wieting et al., 2015; Ganitkevitch et al., 2013), and then tune these embeddings using the official STS task training data, as well as synthetic paraphrases for each test dataset, obtained by pivoting through machine translation.", "labels": [], "entities": [{"text": "STS task training data", "start_pos": 211, "end_pos": 233, "type": "DATASET", "confidence": 0.6213204860687256}]}, {"text": "Our submissions include runs which only compare the similarity of phrases in the embedding space, directly using the similarity score to produce predictions, as well as a run which uses vector similarity in addition to a suite of features we investigated for our 2015 Semeval submission.", "labels": [], "entities": []}, {"text": "For the crosslingual task, we simply translate the Spanish sentences to English, and use the same system we designed for the monolingual task.", "labels": [], "entities": []}], "introductionContent": [{"text": "We describe the work carried out by the DCUSEManiacs team on the Semantic Textual Similarity (STS) task at.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 65, "end_pos": 103, "type": "TASK", "confidence": 0.8265919940812247}]}, {"text": "The main ideas we investigate in our systems are: 1.", "labels": [], "entities": []}, {"text": "Using a margin-based objective function to train high-quality sentence embeddings without using supervised scores 2.", "labels": [], "entities": []}, {"text": "Creating new synthetic training data using machine translation to generate artificial paraphrases 3.", "labels": [], "entities": []}, {"text": "Using ensemble models to combine features generated by our embedding networks with features obtained from other sources", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Using the 2015 STS data as development data, a comparison of our 2015 model with raw results from paragram vectors", "labels": [], "entities": [{"text": "2015 STS data", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.6328779856363932}]}, {"text": " Table 2: Monolingual STS results by run, with median scores and best scores for reference. \"fusion\" indicates the ensemble", "labels": [], "entities": [{"text": "Monolingual STS", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7280563712120056}]}, {"text": " Table 3: Crosslingual STS results by run, with best scores for reference. Our best performing systems are bolded.", "labels": [], "entities": [{"text": "Crosslingual STS", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8131286203861237}]}, {"text": " Table 4: Ensemble model top 10 features in decreasing order", "labels": [], "entities": []}, {"text": " Table 5: Total % unknown for each 2016 dataset", "labels": [], "entities": [{"text": "Total", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9632874727249146}]}]}