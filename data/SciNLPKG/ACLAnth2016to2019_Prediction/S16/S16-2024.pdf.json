{"title": [{"text": "Random Positive-Only Projections: PPMI-Enabled Incremental Semantic Space Construction", "labels": [], "entities": [{"text": "PPMI-Enabled Incremental Semantic Space Construction", "start_pos": 34, "end_pos": 86, "type": "TASK", "confidence": 0.6789348006248475}]}], "abstractContent": [{"text": "We introduce positive-only projection (PoP), anew algorithm for constructing semantic spaces and word embeddings.", "labels": [], "entities": []}, {"text": "The PoP method employs random projections.", "labels": [], "entities": []}, {"text": "Hence, it is highly scalable and com-putationally efficient.", "labels": [], "entities": []}, {"text": "In contrast to previous methods that use random projection matrices R with the expected value of 0 (i.e., E(R) = 0), the proposed method uses R with E(R) > 0.", "labels": [], "entities": []}, {"text": "We use Kendall's \u03c4 b correlation to compute vector similarities in the resulting non-Gaussian spaces.", "labels": [], "entities": []}, {"text": "Most importantly, since E(R) > 0, weighting methods such as positive pointwise mutual information (PPMI) can be applied to PoP-constructed spaces after their construction for efficiently transferring PoP embeddings onto spaces that are discriminative for semantic similarity assessments.", "labels": [], "entities": [{"text": "semantic similarity assessments", "start_pos": 255, "end_pos": 286, "type": "TASK", "confidence": 0.7252118786176046}]}, {"text": "Our PoP-constructed models, combined with PPMI, achieve an average score of 0.75 in the MEN relatedness test, which is comparable to results obtained by state-of-the-art algorithms.", "labels": [], "entities": [{"text": "MEN relatedness", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.4894982576370239}]}], "introductionContent": [{"text": "The development of data-driven methods of natural language processing starts with an educated guess, a distributional hypothesis: We assume that some properties of linguistic entities can be modelled by 'some statistical' observations in language data.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6953261494636536}]}, {"text": "In the second step, this statistical information (which is determined by the hypothesis) is collected and represented in a mathematical framework.", "labels": [], "entities": []}, {"text": "In the third step, tools provided by the chosen mathematical framework are used to implement a similarity-based logic to identify linguistic structures, and/or to verify the proposed hypothesis.", "labels": [], "entities": []}, {"text": "Harris's distributional hypothesis) is a well-known example of step one that states that meanings of words correlate with the environment in which the words appear.", "labels": [], "entities": []}, {"text": "Vector space models and \u03b7-normed-based similarity measures are notable examples of steps two and three, respectively (i.e., word space models or word embeddings).", "labels": [], "entities": []}, {"text": "However, as pointed out for instance by, the count-based models resulting from the steps two and three are not discriminative enough to achieve satisfactory results; instead, predictive models are required.", "labels": [], "entities": []}, {"text": "To this end, an additional transformation step is often added.", "labels": [], "entities": []}, {"text": "describe this extra step as a combination of weighting and dimensionality reduction.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.6931630820035934}]}, {"text": "This transformation from count-based to predictive models can be implemented simply via a collection of rules of thumb (such as frequency threshold to filter out highly frequent and/or rare context elements), and/or it can involve more sophisticated mathematical transformations, such as converting raw counts to probabilities and using matrix factorization techniques.", "labels": [], "entities": []}, {"text": "Likewise, by exploiting the large amounts of computational power available nowadays, this transformation can be achieved via neural word embedding techniques (.", "labels": [], "entities": []}, {"text": "To a large extent, the need for such transformations arises from the heavy-tailed distributions that we often find in statistical natural language models (such as the Zipfian distribution of words in contexts when building word spaces).", "labels": [], "entities": []}, {"text": "Consequently, count-based models are sparse and highdimensional and therefore both computationally expensive to manipulate (due of the high dimensionality of models) and nondiscriminatory (due to the combination of the high-dimensionality of the models and the sparseness of observations-see).", "labels": [], "entities": []}, {"text": "On the one hand, although neural networks are often the top performers for addressing this problem, their usage is costly: they need to be trained, which is often very time-consuming, and their performance can vary from one task to another depending on their objective function.", "labels": [], "entities": []}, {"text": "On the other hand, although methods based on random projections efficiently address the problem of reducing the dimensionality of vectors-such as random indexing (RI) (), reflective random indexing (RRI),), ISA ( and random Manhattan indexing (RMI))-in effect they retain distances between entities in the original space.", "labels": [], "entities": [{"text": "random Manhattan indexing (RMI))-", "start_pos": 217, "end_pos": 250, "type": "TASK", "confidence": 0.6823285420735677}]}, {"text": "Moreover, since these methods use asymptotic Gaussian or Cauchy random projection matrices R with E(R) = 0, their resulting vectors cannot be adjusted and transformed using weighting techniques such as PPMI.", "labels": [], "entities": []}, {"text": "Consequently, these methods often do not outperform neural embeddings and combinations of PPMI weighting of count-based models followed by matrix factorization-such as the truncation of weighted vectors using singular value decomposition (SVD).", "labels": [], "entities": []}, {"text": "To overcome these problems, we propose anew method called positive-only projection (PoP).", "labels": [], "entities": []}, {"text": "PoP is an incremental semantic space construction method which employs random projections.", "labels": [], "entities": [{"text": "semantic space construction", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.7413123448689779}]}, {"text": "Hence, building models using PoP does not require training but simply generating random vectors.", "labels": [], "entities": []}, {"text": "However, in contrast to RI (and previous methods), the PoP-constructed spaces can undergo weighting transformations such as PPMI, after their construction and at a reduced dimensionality.", "labels": [], "entities": []}, {"text": "This is due to the fact that PoP uses random vectors that contain only positive integer values.", "labels": [], "entities": []}, {"text": "Because the PoP method employs random projections, models can be built incrementally and efficiently.", "labels": [], "entities": []}, {"text": "Since the vectors in PoP-constructed models are small (i.e., with a dimensionality of a few hundred), applying weighting methods such 2 That is, the well known curse of dimensionality problem.", "labels": [], "entities": []}, {"text": "3  state that it took Ronan Collobert two months to train a set of embeddings from a Wikipedia dump.", "labels": [], "entities": [{"text": "Ronan Collobert", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.8712000548839569}]}, {"text": "Even using GPU-accelerated computing, the required computation and training time for inducing neural word embeddings is high.", "labels": [], "entities": []}, {"text": "Ibid, see results reported in supplemental materials.", "labels": [], "entities": []}, {"text": "For \u03b7-normed space that they are designed for, i.e., \u03b7 = 2 for RI, RRI, and ISA and \u03b7 = 1 for RMI.", "labels": [], "entities": []}, {"text": "as PPMI to these models is incredibly faster than applying them to classical count-based models.", "labels": [], "entities": []}, {"text": "Combined with a suitable weighting method such as PPMI, the PoP algorithm yields competitive results concerning accuracy in semantic similarity assessments, compared for instance to neural net-based approaches and combinations of countbased models with weighting and matrix factorization.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9978113770484924}, {"text": "semantic similarity assessments", "start_pos": 124, "end_pos": 155, "type": "TASK", "confidence": 0.7379838625590006}]}, {"text": "These results, however, are achieved without the need for heavy computations.", "labels": [], "entities": []}, {"text": "Thus, instead of hours, models can be builtin a matter of a few seconds or minutes.", "labels": [], "entities": []}, {"text": "Note that even without weighting transformation, PoP-constructed models display a better performance than RI on tasks of semantic similarity assessments.", "labels": [], "entities": [{"text": "semantic similarity assessments", "start_pos": 121, "end_pos": 152, "type": "TASK", "confidence": 0.6978022952874502}]}, {"text": "We describe the PoP method in \u00a7 2.", "labels": [], "entities": []}, {"text": "In order to evaluate our models, in \u00a7 3, we report the performance of PoP in the MEN relatedness test.", "labels": [], "entities": [{"text": "MEN relatedness", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.6205377876758575}]}, {"text": "Finally, \u00a7 4 concludes with a discussion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}