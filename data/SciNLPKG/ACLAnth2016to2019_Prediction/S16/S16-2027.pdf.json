{"title": [{"text": "Learning Embeddings to lexicalise RDF Properties", "labels": [], "entities": []}], "abstractContent": [{"text": "A difficult task when generating text from knowledge bases (KB) consists in finding appropriate lexicalisations for KB symbols.", "labels": [], "entities": []}, {"text": "We present an approach for lexicalis-ing knowledge base relations and apply it to DBPedia data.", "labels": [], "entities": [{"text": "DBPedia data", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.9420396089553833}]}, {"text": "Our model learns low-dimensional embeddings of words and RDF resources and uses these representations to score RDF properties against candidate lexicalisations.", "labels": [], "entities": []}, {"text": "Training our model using (i) pairs of RDF triples and automatically generated verbalisations of these triples and (ii) pairs of paraphrases extracted from various resources, yields competitive results on DBPedia data.", "labels": [], "entities": [{"text": "DBPedia data", "start_pos": 204, "end_pos": 216, "type": "DATASET", "confidence": 0.9692940711975098}]}], "introductionContent": [{"text": "In recent years, work on the Semantic Web has led to the publication of large scale datasets in the socalled Linked Data framework such as for instance DBPedia or Yago.", "labels": [], "entities": [{"text": "Yago", "start_pos": 163, "end_pos": 167, "type": "DATASET", "confidence": 0.6650020480155945}]}, {"text": "However, as shown in), the basic standards (e.g., RDF, OWL) established by the Semantic Web community for representing data and ontologies are difficult for human beings to use and understand.", "labels": [], "entities": []}, {"text": "With the development of the semantic web and the rapid increase of Linked Data, there is consequently a growing need in the semantic web community for technologies that give humans easy access to the machine-oriented Web of data.", "labels": [], "entities": []}, {"text": "Because it maps data to text, Natural Language Generation (NLG) provides a natural means for presenting this data in an organized, coherent and accessible way.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.7976729273796082}]}, {"text": "It can be used to display the content of linked data or of knowledge bases to lay users; to generate explanations, descriptions and summaries from DBPedia or from knowledge bases; to guide the user in formulating knowledge base queries; and to provide ways for cultural heritage institutions such as museums and libraries to present information about their holdings in multiple textual forms.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 147, "end_pos": 154, "type": "DATASET", "confidence": 0.8264994025230408}]}, {"text": "In this paper, we focus on an important subtask of generation from RDF data namely lexicalisation of RDF properties.", "labels": [], "entities": []}, {"text": "Given a property, our goal is to map this property to a set of possible lexicalisations.", "labels": [], "entities": []}, {"text": "For instance, given the property HASWONPRIZE, our goal is to automatically infer lexicalisations such as was honored with and received.", "labels": [], "entities": [{"text": "HASWONPRIZE", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9211657643318176}]}, {"text": "Our approach is based on learning lowdimensional vector embeddings of words and of KB triples so that representations of triples and their corresponding lexicalisations end up being similar in the embedding space.", "labels": [], "entities": []}, {"text": "Using these embeddings, we can then assess the similarity between a property and a set of candidate lexicalisations by simply applying the dot product to their vector embeddings.", "labels": [], "entities": []}, {"text": "One difficulty when lexicalising RDF properties is that, while in some cases, there is a direct and simple relation between the name of a property and its verbalisation (e.g., BIRTHDATE / \"was born on\"), in other cases, the relation is either indirect (e.g., / \"finishes at\") or opaque (e.g., / \"is the commander of\").", "labels": [], "entities": [{"text": "BIRTHDATE", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.996129035949707}]}, {"text": "To account for these two possibilities, we therefore explore two main ways of creating candidate lexicalisations based on either lexical-or on extensional-relatedness.", "labels": [], "entities": []}, {"text": "Given some input property p, lexically-related candidate lexicalisations for pare phrases containing synonyms or derivationally related words of the tokens making up the name of the input property.", "labels": [], "entities": []}, {"text": "In contrast, extensionally-related candidate lexicalisations are phrases containing named entities which are in its extension.", "labels": [], "entities": []}, {"text": "For instance, given the property CREW1UP, if the pair of entities (STS-130, GEORGE D.", "labels": [], "entities": [{"text": "CREW1UP", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.7535392045974731}]}, {"text": "ZAMK) is in its extension (i.e., there exists an RDF triple of the form STS-130, CREW1UP, GEORGE D. ZAMK ), all sentences mentioning STS-130, GEORGE D.", "labels": [], "entities": [{"text": "CREW1UP", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.6649736762046814}]}, {"text": "ZAMK or both will be retrieved and exploited to build the set of candidate lexicalisations for CREW1UP.", "labels": [], "entities": [{"text": "CREW1UP", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9138307571411133}]}, {"text": "shows some example L-and E-candidate lexicalisations phrases.", "labels": [], "entities": []}, {"text": "In summary, the key contribution made in this paper is a novel method for lexicalising RDF properties which differs from previous work in two ways.", "labels": [], "entities": []}, {"text": "First, while lexical and extensional relatedness have been used before for lexicalising RDF properties (, ours is the first lexicalisation approach which jointly considers both sources of information.", "labels": [], "entities": []}, {"text": "Second, while previous approaches have used discrete representations and similarity metrics based on Wordnet, our method exploits continuous representations of both words and KB symbols that are learned and optimised for the lexicalisation task.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9837889671325684}]}], "datasetContent": [{"text": "DBPedia 4 is a crowd-sourced knowledge based extracted from Wikipedia and available on the Web in RDF format.", "labels": [], "entities": []}, {"text": "Available as Linked Dataon the web, the DBPedia knowledge base defines Linked Data URIs for millions of concepts.", "labels": [], "entities": [{"text": "DBPedia knowledge base", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.9564077655474345}]}, {"text": "It has become a de facto central hub of the web of data and is heavily used by systems that employ structured data for applications such as web-based information retrieval or search engines.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 150, "end_pos": 171, "type": "TASK", "confidence": 0.753678947687149}]}, {"text": "Like many other large knowledge bases (e.g., Freebase or Yago) available on the web, DBPedia lacks lexical information stating how DBPedia properties should be lexicalised.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.8595573306083679}]}, {"text": "We apply our lexicalisation model to DBPedia object properties.", "labels": [], "entities": []}, {"text": "We construct candidate lexicalisation sets in the following way.", "labels": [], "entities": []}, {"text": "Candidate Lexicalisations As mentioned in Section 1, we consider two main ways of building sets of candidate lexicalisations fora given property p.", "labels": [], "entities": []}, {"text": "E-LEX p : Let WKP p be the set of sentences extracted from Wikipedia which contain at least one mention of two entities that are related in DBPedia by the property p.", "labels": [], "entities": []}, {"text": "WKP p was built using the pre-processing tools 5 of the MATOLL framework (.", "labels": [], "entities": [{"text": "WKP p", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8960390985012054}, {"text": "MATOLL framework", "start_pos": 56, "end_pos": 72, "type": "DATASET", "confidence": 0.8587366938591003}]}, {"text": "Then E-LEX p is the corpus of candidate lexicalisations extracted from WKP p using Reverb.", "labels": [], "entities": [{"text": "Reverb", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.8569359183311462}]}, {"text": "L-LEX p : Given WKP the corpus of Wikipedia sentences, L-LEX p is the corpus of relation mentions extracted from WKP using Reverb and filtered to contain only mentions which include words that are lexically related to the tokens making up the property name.", "labels": [], "entities": [{"text": "Reverb", "start_pos": 123, "end_pos": 129, "type": "DATASET", "confidence": 0.8480562567710876}]}, {"text": "Lexically related words include all synonyms and all derivationally related words listed in Wordnet fora given token.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9790412187576294}]}, {"text": "We compare the output of our lexicalisation method with the following resources and approaches.: Set of DBPedia object properties used in the evaluation.", "labels": [], "entities": []}, {"text": "DBlexipedia e : a lexicon 6 automatically inferred from Wikipedia using the method described in () (c.f. section 2).", "labels": [], "entities": [{"text": "DBlexipedia e", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9110619425773621}]}, {"text": "Lexical entries are inferred using either the extension of the properties (by retrieving sentences containing entities that are related by a DBPedia property and generalising over the dependency paths that connect them.) or synonyms of the words contained in the property name.", "labels": [], "entities": []}, {"text": "PATTY: a lexicon automatically inferred from web data using relation extraction and clustering (c.f.", "labels": [], "entities": [{"text": "PATTY", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7519487142562866}, {"text": "relation extraction", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7115223854780197}]}, {"text": "QUELO: a lexicon automatically derived using the method described in) (c.f. section 2).", "labels": [], "entities": [{"text": "QUELO", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5296905636787415}]}, {"text": "Lexical entries are derived by first, tokenizing and pos tagging property names and second, mapping the resulting pos-tagged sequences to pre-defined mention patterns.", "labels": [], "entities": [{"text": "tokenizing and pos tagging property names", "start_pos": 38, "end_pos": 79, "type": "TASK", "confidence": 0.7121185759703318}]}, {"text": "For the quantitative evaluation, we use the lexicon developed manually for DBPedia properties by) as a gold standard . We test on a held-out set of 30 properties 8 chosen from DBPedia and which were present in the gold standard lexicon, in the other systems we compare with and in the available E-Lex p corpus.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 176, "end_pos": 183, "type": "DATASET", "confidence": 0.9675945043563843}, {"text": "E-Lex p corpus", "start_pos": 295, "end_pos": 309, "type": "DATASET", "confidence": 0.8650481899579366}]}, {"text": "lists the set of properties.", "labels": [], "entities": []}, {"text": "We compute precision (Correct/Found), recall (Correct/GOLD) and F1 measure of each of the above resources.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9989326596260071}, {"text": "recall (Correct/GOLD)", "start_pos": 38, "end_pos": 59, "type": "METRIC", "confidence": 0.6959188729524612}, {"text": "F1 measure", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9810653924942017}]}, {"text": "Recall is the proportion of (property, lexicalisation) pairs present in GOLD which are present in the resource being evaluated, precision the proportion in a resource which is also present in GOLD and F1 is the harmonic mean of precision and recall 9 . In our setup though, precision (and therefore F1) values are artificially decreased because the reference lexicon is small (2.4 lexicalisations in average per property) and often fails to include all possible lexicalisations.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9850906133651733}, {"text": "GOLD", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.7199745774269104}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9990423321723938}, {"text": "GOLD", "start_pos": 192, "end_pos": 196, "type": "DATASET", "confidence": 0.8102067708969116}, {"text": "F1", "start_pos": 201, "end_pos": 203, "type": "METRIC", "confidence": 0.9925119876861572}, {"text": "precision", "start_pos": 228, "end_pos": 237, "type": "METRIC", "confidence": 0.9987271428108215}, {"text": "recall", "start_pos": 242, "end_pos": 248, "type": "METRIC", "confidence": 0.9976987242698669}, {"text": "precision", "start_pos": 274, "end_pos": 283, "type": "METRIC", "confidence": 0.9987819790840149}, {"text": "F1", "start_pos": 299, "end_pos": 301, "type": "METRIC", "confidence": 0.9752501249313354}]}, {"text": "The number of correct lexicalisations can therefore be under-estimated while the number of found lexicalisations is usually larger than the number of gold lexicalisations and therefore much larger than the number of correct (= GOLD \u2229 Found) lexicalisations.", "labels": [], "entities": []}, {"text": "We report results using different sets of lexicalisation candidates (L-LEX, E-LEX, their union and their intersection) and different thresholds or methods for selecting the final set of lexicalisations.", "labels": [], "entities": []}, {"text": "These include: retrieving the n-best lexicalisations (k=10) versus using an adaptive threshold which varies depending on the size of the set of candidate lexicalisations and on the distributions of its ranking scores.", "labels": [], "entities": []}, {"text": "We tried taking all lexicalisations over the median (median), over the mid-range ((min+max)/2) or in the third quartile (Q3).", "labels": [], "entities": []}, {"text": "We also tested an alternative ranking technique where the score of each lexicalisation is the product of its similarity score (dot product of the embedding vectors representing the property and the lexicalisation) with the frequency of this particular lexicalisation in the set of candidate lexicalisations . We rerank the lexicalisations using these new scores and consider only the lexicalisations in the third quartile of the distribution (FreqQ3).", "labels": [], "entities": [{"text": "FreqQ3", "start_pos": 443, "end_pos": 449, "type": "DATASET", "confidence": 0.8085952997207642}]}, {"text": "Further if this results in having either less than 7 or more than 25 lexicalisations, we ignore the Q3 constraint and take the 7 and 25 best respectively (FreqQ3Limit).", "labels": [], "entities": [{"text": "FreqQ3Limit", "start_pos": 155, "end_pos": 166, "type": "DATASET", "confidence": 0.5553673505783081}]}, {"text": "To determine whether a given property lexicalisation is correct, i.e. present in the GOLD, we use \"soft\" comparison rather than strict string matching.", "labels": [], "entities": [{"text": "GOLD", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.8186935782432556}]}, {"text": "This consists in checking whether the stemmed gold lexicalisation is contained in a given candidate lexicalisation.", "labels": [], "entities": []}, {"text": "For instance, the candidate \"main occupation of\" and gold \"occupation of\" are considered as a match.", "labels": [], "entities": []}, {"text": "In the set of candidate lexicalisations, the same lexicalisation may occur with minor variations.", "labels": [], "entities": []}, {"text": "We compute the frequency of a given lexicalisation by removing adjectives and adverbs and counting the number of repeated occurrences after removing these.", "labels": [], "entities": []}], "tableCaptions": []}