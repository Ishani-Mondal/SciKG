{"title": [{"text": "MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection", "labels": [], "entities": [{"text": "SemEval-2016 Task 6", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.5620192885398865}, {"text": "Transfer Learning", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.865117609500885}, {"text": "Stance Detection", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.8573934137821198}]}], "abstractContent": [{"text": "We describe MITRE's submission to the SemEval-2016 Task 6, Detecting Stance in Tweets.", "labels": [], "entities": [{"text": "SemEval-2016 Task 6", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7379103700319926}, {"text": "Detecting Stance in Tweets", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.9057134836912155}]}, {"text": "This effort achieved the top score in Task A on supervised stance detection, producing an average F1 score of 67.8 when assessing whether a tweet author was in favor or against a topic.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.8332843482494354}, {"text": "F1 score", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9864110350608826}]}, {"text": "We employed a recurrent neu-ral network initialized with features learned via distant supervision on two large unlabeled datasets.", "labels": [], "entities": []}, {"text": "We trained embeddings of words and phrases with the word2vec skip-gram method, then used those features to learn sentence representations via a hashtag prediction auxiliary task.", "labels": [], "entities": [{"text": "hashtag prediction auxiliary task", "start_pos": 144, "end_pos": 177, "type": "TASK", "confidence": 0.8033926635980606}]}, {"text": "These sentence vectors were then fine-tuned for stance detection on several hundred labeled examples.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.9668464362621307}]}, {"text": "The result was a high performing system that used transfer learning to maximize the value of the available training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes a system for performing automatic stance detection in social media messages.", "labels": [], "entities": [{"text": "stance detection in social media messages", "start_pos": 55, "end_pos": 96, "type": "TASK", "confidence": 0.9125080207983652}]}, {"text": "Our approach employs a recurrent neural network which was initialized from pre-trained features learned in successive attempts to encode world knowledge via weak external supervision.", "labels": [], "entities": []}, {"text": "Stance detection is the task of determining whether the author of a text is in favor or against a given topic, while rejecting texts in which neither inference is likely.", "labels": [], "entities": [{"text": "Stance detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9428279101848602}]}, {"text": "This task is distinct from sentiment analysis in that an in favor or against stance can be measured independently of an author's emotional state.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9504988789558411}]}, {"text": "In stance detection we attempt to measure how an author's opinion is expressed in spontaneous, unstructured messages rather than the explicit prompts of formal opinion polls.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.945623904466629}]}, {"text": "Declarations of stance are often couched in figurative language that can be difficult for machines to unravel.", "labels": [], "entities": [{"text": "Declarations of stance", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6216710011164347}]}, {"text": "Consider the texts We don't inherit the earth from our parents we borrow it from our children and Last time I checked, Al Gore is a politician, not a scientist.", "labels": [], "entities": []}, {"text": "To the human observer messages like these contain an interpretable stance relevant to the topic of climate change.", "labels": [], "entities": []}, {"text": "But to understand rhetorical devices like sarcasm, irony, analogy, and metaphor, a reader often uses personal experience to infer broader context.", "labels": [], "entities": []}, {"text": "For machines, matters are additionally complicated by use of informal vocabulary, grammar, and spelling.", "labels": [], "entities": []}, {"text": "Furthermore, training data is often expensive or difficult to collect in bulk.", "labels": [], "entities": []}, {"text": "These challenges motivated our efforts to seek transfer learning of broad world knowledge through feature pre-training using large unlabeled datasets.", "labels": [], "entities": [{"text": "transfer learning of broad world knowledge", "start_pos": 47, "end_pos": 89, "type": "TASK", "confidence": 0.8620854318141937}]}], "datasetContent": [{"text": "Detecting Stance in Tweets, Subtask A: Supervised Frameworks (Mohammad et al., 2016) was a shared task organized within SemEval-2016.", "labels": [], "entities": [{"text": "Detecting Stance in Tweets", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9043577462434769}]}, {"text": "The task organizers provided training data in the form of 2,814 tweets covering five topics, with 395 to 664 tweets per topic.", "labels": [], "entities": []}, {"text": "The organizers used crowdsourcing to manually annotate these tweets for stance.", "labels": [], "entities": [{"text": "stance", "start_pos": 72, "end_pos": 78, "type": "TASK", "confidence": 0.9714061617851257}]}, {"text": "Class balance varied between topics, with some topics showing significant skew (e.g. Climate Change is a Real Concern with 4% AGAINST and 54% FAVOR) while others were more balanced (e.g. Feminist Movement with 49% AGAINST and 32% FAVOR).", "labels": [], "entities": [{"text": "AGAINST", "start_pos": 126, "end_pos": 133, "type": "METRIC", "confidence": 0.9901323318481445}, {"text": "FAVOR", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.9900642037391663}, {"text": "AGAINST", "start_pos": 214, "end_pos": 221, "type": "METRIC", "confidence": 0.8067315816879272}, {"text": "FAVOR", "start_pos": 230, "end_pos": 235, "type": "METRIC", "confidence": 0.9747784733772278}]}, {"text": "Approximately 74% of the provided tweets were judged to be either in favor or against, while the remainder contained neither inference.", "labels": [], "entities": []}, {"text": "An additional 1249 tweets with held-out labels were used as evaluation data.", "labels": [], "entities": []}, {"text": "Systems were evaluated using the macro-average of F1-score(FAVOR) and F1-score(AGAINST) across all topics.", "labels": [], "entities": [{"text": "F1-score(FAVOR)", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.8699132800102234}, {"text": "F1-score(AGAINST)", "start_pos": 70, "end_pos": 87, "type": "METRIC", "confidence": 0.858577162027359}]}, {"text": "The system described in section 4 was designed to detect stances pertaining to a single topic.", "labels": [], "entities": []}, {"text": "As such we trained five distinct classifiers, one for each of the five topics under consideration in the evaluation.", "labels": [], "entities": []}, {"text": "The embedding and recurrent layers of each classifier were initialized with the weights obtained from the pre-training process described above.", "labels": [], "entities": []}, {"text": "The remainder of the weights were randomly initialized and the network was trained with stochastic gradient descent using a learning rate of 0.015 and momentum of 0.9.", "labels": [], "entities": [{"text": "momentum", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.971177875995636}]}, {"text": "These networks were trained using a categorical cross-entropy loss function, with costs for each example weighted according to the prevalence of the class in the training data.", "labels": [], "entities": []}, {"text": "This placed higher weight on rare classes.", "labels": [], "entities": []}, {"text": "The recurrent networks were implemented using the Keras framework.", "labels": [], "entities": [{"text": "Keras framework", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.9037588834762573}]}, {"text": "The training data for each topic was shuffled and split into five chunks for cross-validation.", "labels": [], "entities": []}, {"text": "The training process fora single topic's classifier therefore resulted in five distinct neural networks, each learning from 80% of the training data.", "labels": [], "entities": []}, {"text": "These training set sizes ranged from 316 to 532 tweets.", "labels": [], "entities": []}, {"text": "Each network was trained for 50 epochs, with early stopping to select the model with the best validation loss.", "labels": [], "entities": []}, {"text": "Predictions from these five trained networks were used to select a single class via majority vote at decode time.", "labels": [], "entities": []}, {"text": "Variants of this approach were considered as well.", "labels": [], "entities": []}, {"text": "One variant used an identical framework with a recurrent layer initialized instead from a RNN trained on 6.5 million tweets containing the top 10,000 most frequent hashtags (as opposed to 197 topic-relevant hashtags).", "labels": [], "entities": []}, {"text": "We also omitted the RNN pre-training altogether and randomly initialized the recurrent layer.", "labels": [], "entities": []}, {"text": "These variants were not found to improve performance.", "labels": [], "entities": []}], "tableCaptions": []}