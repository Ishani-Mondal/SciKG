{"title": [{"text": "When Hyperparameters Help: Beneficial Parameter Combinations in Distributional Semantic Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional semantic models can predict many linguistic phenomena, including word similarity, lexical ambiguity, and semantic priming, or even to pass TOEFL synonymy and analogy tests (Landauer and Dumais, 1997; Griffiths et al., 2007; Turney and Pantel, 2010).", "labels": [], "entities": [{"text": "word similarity", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.7023325264453888}]}, {"text": "But what does it take to create a competitive distributional model?", "labels": [], "entities": []}, {"text": "(2015) argue that the key to success lies in hyperparameter tuning rather than in the model's architecture.", "labels": [], "entities": []}, {"text": "More hyperparameters trivially lead to potential performance gains, but what do they actually do to improve the mod-els?", "labels": [], "entities": []}, {"text": "Are individual hyperparameters' contributions independent of each other?", "labels": [], "entities": []}, {"text": "Or are only specific parameter combinations beneficial?", "labels": [], "entities": []}, {"text": "To answer these questions, we perform a quantitative and qualitative evaluation of major hyperparameters as identified in previous research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina rigorous evaluation, (  showed that neural word embeddings such as skipgram have an edge over traditional count-based models.", "labels": [], "entities": []}, {"text": "However, as argued by, the difference is not as big as it appears, since skip-gram is implicitly factorizing a word-context matrix whose cells are the pointwise mutual information (PMI) of word context pairs shifted by a global constant.", "labels": [], "entities": []}, {"text": "further suggest that the performance advantage of neural network based models is largely due to hyperparameter optimization, and that the optimization of count based models can result in similar performance gains.", "labels": [], "entities": []}, {"text": "In this paper we take this claim as the starting point.", "labels": [], "entities": []}, {"text": "We experiment with three hyperparameters that have the greatest effect on model performance according to: subsampling, shifted PMI and context distribution smoothing.", "labels": [], "entities": [{"text": "context distribution smoothing", "start_pos": 135, "end_pos": 165, "type": "TASK", "confidence": 0.6312760710716248}]}, {"text": "To get a more detailed picture, we use a greater range of hyperparameter values than in previous work, comparing all hyperparameter value combinations, and perform a qualitative analysis of their effect.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three data sets were used to evaluate the models.", "labels": [], "entities": []}, {"text": "The MEN data set contains 3000 word pairs rated by human similarity judgements.", "labels": [], "entities": [{"text": "MEN data set", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.948661208152771}]}, {"text": "report an accuracy of 78% on this data-set using an approach that combines visual and textual features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996294975280762}]}, {"text": "The WordSim data set is a collection of word pairs associated with human judgements of similarity or relatedness.", "labels": [], "entities": [{"text": "WordSim data set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9677994251251221}]}, {"text": "The similarity set contains 203 items (WS sim) and the relatedness set contains 252 items (WS rel).", "labels": [], "entities": [{"text": "WS rel)", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.599738746881485}]}, {"text": "Agirre et al. achieved an accuracy of 77% on this data set using a context window approach ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9993711113929749}]}, {"text": "The TOEFL data set includes 80 multiplechoice synonym questions).", "labels": [], "entities": [{"text": "TOEFL data set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9313022295633951}]}, {"text": "For this data set, corpus-based approaches have reached an accuracy of 92.50% (Rapp, 2003).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9993484616279602}]}], "tableCaptions": [{"text": " Table 1: Context Distribution Smoothing", "labels": [], "entities": [{"text": "Context Distribution Smoothing", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.9453863104184469}]}, {"text": " Table 3: CDS and Shifted PPMI", "labels": [], "entities": [{"text": "PPMI", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.4962506592273712}]}, {"text": " Table 4: CDS and SPPMI with subsampling", "labels": [], "entities": []}, {"text": " Table 5: Top 10 neighbours of doughnut. Semantically re- lated neighbors are given in bold.", "labels": [], "entities": [{"text": "doughnut", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.910084068775177}]}, {"text": " Table 6: Sample of words with zero positive dimensions after  SPPMI with CDS", "labels": [], "entities": [{"text": "Sample", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9521520137786865}]}, {"text": " Table 7: Performance of count vs. predict models as a func- tion of corpus size", "labels": [], "entities": []}]}