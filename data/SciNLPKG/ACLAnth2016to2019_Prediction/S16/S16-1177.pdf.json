{"title": [{"text": "DynamicPower at SemEval-2016 Task 8: Processing syntactic parse trees with a Dynamic Semantics core", "labels": [], "entities": [{"text": "SemEval-2016 Task 8", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7556602160135905}, {"text": "Processing syntactic parse trees", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.7038942947983742}]}], "abstractContent": [{"text": "This is a system description paper fora submission to Task 8 of SemEval-2016: Meaning Representation Parsing.", "labels": [], "entities": [{"text": "Meaning Representation Parsing", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.8920554320017496}]}, {"text": "No use was made of the training data provided by the task.", "labels": [], "entities": []}, {"text": "Instead existing components were combined to form a pipeline able to take raw sentences as input and output meaning representations.", "labels": [], "entities": []}, {"text": "Components area part-of-speech tagger and parser trained on the Penn Parsed Corpus of Modern British English to produce syntactic parse trees, a semantic role labeller and a named entity recogniser to supplement obtained parse trees with word sense, functional and named entity information, followed by an adapted Tarskian satisfaction relation fora Dynamic Semantics that is used to transform a syntactic parse into a predicate logic based meaning representation, followed by conversion to pen-man/AMR notation required for the task appraisal .", "labels": [], "entities": [{"text": "Penn Parsed Corpus of Modern British English", "start_pos": 64, "end_pos": 108, "type": "DATASET", "confidence": 0.9656083583831787}]}], "introductionContent": [{"text": "This is a system description paper fora submission to Task 8 of SemEval-2016: Meaning Representation Parsing.", "labels": [], "entities": [{"text": "Meaning Representation Parsing", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.8920554320017496}]}, {"text": "Syntactic structures are first obtained by parsing raw language input, from which meaning representations are derived by printing off information accumulated with an adapted Tarskian satisfaction relation fora Dynamic Semantics.", "labels": [], "entities": []}, {"text": "This is akin to compositional approaches of formal semantics that view the task of reaching a semantic value as being rooted in first obtaining a syntactic parse.", "labels": [], "entities": []}, {"text": "Key advantages are modularity and domain independence.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 sketches the method used to obtain a syntactic parse.", "labels": [], "entities": [{"text": "syntactic parse", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7743239402770996}]}, {"text": "Section 3 covers reaching a semantic representation.", "labels": [], "entities": []}, {"text": "Section 4 outlines conversion to penman/AMR notation.", "labels": [], "entities": [{"text": "penman/AMR notation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.5158173367381096}]}, {"text": "Section 5 reports experiment results.", "labels": [], "entities": []}, {"text": "Section 6 is a conclusion.", "labels": [], "entities": []}, {"text": "An appendix details how to run the available implementation.", "labels": [], "entities": []}], "datasetContent": [{"text": "The method of this paper is evaluated on the shared task evaluation data, which includes 1053 sentences.", "labels": [], "entities": []}, {"text": "The smatch score on the evaluation data is 0.47.", "labels": [], "entities": []}, {"text": "Table 1 also reports smatch score on the LDC2015E86 dataset, which includes 1371 test sentences.", "labels": [], "entities": [{"text": "LDC2015E86 dataset", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.9726200699806213}]}, {"text": "The scores are calculated with Smatch v2.0., which evaluates the precision, recall, and F1 of the concepts and relations all together.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9996379613876343}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9991098046302795}, {"text": "F1", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9996767044067383}]}, {"text": "The score for Task 8 is higher than the performance on the LDC2015E86 test data.", "labels": [], "entities": [{"text": "LDC2015E86 test data", "start_pos": 59, "end_pos": 79, "type": "DATASET", "confidence": 0.9702669382095337}]}, {"text": "Reasons for the difference include parser performance being better on the evaluate data, and there being fewer non-compositional aspects of representation in the evaluate data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing results on the evaluation data of SemEval-", "labels": [], "entities": []}]}