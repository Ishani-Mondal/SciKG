{"title": [{"text": "ICL00 at SemEval-2016 Task 3: Translation-Based Method for CQA System", "labels": [], "entities": [{"text": "ICL00", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5958402752876282}, {"text": "Translation-Based", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.9471925497055054}]}], "abstractContent": [{"text": "We participate in the English subtask B and C at SemEval-2016 Task 3 \"Community Question Answering\".", "labels": [], "entities": [{"text": "Community Question Answering", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.5498909652233124}]}, {"text": "This paper is concerned with the description of our participating system.", "labels": [], "entities": []}, {"text": "We propose a ranking model that combines a translation model with the cosine similarity-based method.", "labels": [], "entities": []}, {"text": "Compared to the traditional bag of words method, the proposed model is more effective because the relationships between words can be explicitly modeled through word-to-word translation probabilities.", "labels": [], "entities": []}, {"text": "Experiments conducted on the official test data demonstrate that our proposed ranking method obtains promising results.", "labels": [], "entities": [{"text": "official test data", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.7861734827359518}]}], "introductionContent": [{"text": "The) Community Question Answering (CQA) covers a full task on CQA and which is, therefore, closer to areal application.", "labels": [], "entities": [{"text": "Community Question Answering (CQA)", "start_pos": 5, "end_pos": 39, "type": "TASK", "confidence": 0.7481422871351242}]}, {"text": "To facilitate the participation of non IR/QA scholar to the task, the search engine step is already carried out which means that the task organizers explicitly provides the set of potential answers to be reranked.", "labels": [], "entities": []}, {"text": "That is to say, given anew question (aka original question) and the set of the first 10 related questions (retrieved by a search engine) in subtask B, our system will focus on reranking the related questions according to their similarity with the original question.", "labels": [], "entities": []}, {"text": "Similar to subtask B, in subtask C that is the main English subtask, Given anew question and the set of the first 10 related questions, each associated with its first 10 comments appearing in its thread, we will rerank the 100 comments (10 questions * 10 comments) according to their relevance with respect to the original question.", "labels": [], "entities": []}, {"text": "Note that the subtask B will give us enough tools to solve the main subtask.", "labels": [], "entities": []}, {"text": "And therefore, our paper will give an overall description of the system based on subtask B.", "labels": [], "entities": []}, {"text": "In section 2, we will briefly discuss an important difference between the subtask C and subtask B in the course of reranking.", "labels": [], "entities": []}, {"text": "However, the major challenge for subtask B, as for most of CQA systems, is the word mismatch between the original question and the related question.", "labels": [], "entities": []}, {"text": "For example, \"Where I can buy good oil for massage?\" and \"Is there anyplace I can find scented massage oils in Qatar?\" are two very similar questions, but they only have a few words in common.", "labels": [], "entities": []}, {"text": "To solve the word mismatch problem, we focus on translation-based approaches in this paper.", "labels": [], "entities": [{"text": "word mismatch", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.7208015322685242}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the methods used in the ranking model clearly.", "labels": [], "entities": []}, {"text": "Section 3 presents the translation probabilities Estimation.", "labels": [], "entities": [{"text": "translation probabilities Estimation", "start_pos": 23, "end_pos": 59, "type": "TASK", "confidence": 0.6848276654879252}]}, {"text": "We will talk about an overview of the system in section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents the experimental results.", "labels": [], "entities": []}, {"text": "In Section 6, we conclude with ideas for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, experiments are conducted on DEV  The official dataset contains TRAIN, DEV and TEST.", "labels": [], "entities": [{"text": "DEV  The official dataset", "start_pos": 46, "end_pos": 71, "type": "DATASET", "confidence": 0.8279549330472946}, {"text": "TRAIN", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9909635782241821}, {"text": "DEV", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.8147329092025757}, {"text": "TEST", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9470726847648621}]}, {"text": "The development dataset is intended to be used as a development-time evaluation dataset as we develop our systems.", "labels": [], "entities": []}, {"text": "However, when submitting prediction results, we will add the development dataset to training data as well.", "labels": [], "entities": []}, {"text": "The total available data of the TRAIN part is made up of 267 original questions and 2669 related questions for Subtask B, plus 26690 related comments for Subtask C.", "labels": [], "entities": []}, {"text": "DEV dataset which were manually double-checked and are very reliable consist of 50 original questions, 500 related questions, and 5,000 comments.", "labels": [], "entities": [{"text": "DEV dataset", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.987873762845993}]}, {"text": "As far as the TEST is concerned, the task organizers provide participants with 70 original questions, so, there would be 700 predictions for subtask B and 7,000 predictions for subtask C.", "labels": [], "entities": [{"text": "TEST", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.5937084555625916}]}, {"text": "The official scorer will provide a number of evaluation measures to assess the quality of the output of a system, but the official evaluation measure towards which all systems will be evaluated and ranked is Mean Average Precision (MAP).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 208, "end_pos": 236, "type": "METRIC", "confidence": 0.9711108903090159}]}], "tableCaptions": []}