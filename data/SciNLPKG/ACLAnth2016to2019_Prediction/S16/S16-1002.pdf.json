{"title": [{"text": "SemEval-2016 Task 5: Aspect Based Sentiment Analysis", "labels": [], "entities": [{"text": "SemEval-2016 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8732210993766785}, {"text": "Aspect Based Sentiment Analysis", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.7717536687850952}]}], "abstractContent": [{"text": "This paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of the respective tasks of 2014 and 2015.", "labels": [], "entities": [{"text": "SemEval 2016 shared task", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.8796514868736267}, {"text": "Aspect Based Sentiment Analysis (ABSA)", "start_pos": 53, "end_pos": 91, "type": "TASK", "confidence": 0.7269267908164433}]}, {"text": "In its third year, the task provided 19 training and 20 testing datasets for 8 languages and 7 domains, as well as a common evaluation procedure.", "labels": [], "entities": []}, {"text": "From these datasets, 25 were for sentence-level and 14 for text-level ABSA; the latter was introduced for the first time as a subtask in SemEval.", "labels": [], "entities": [{"text": "text-level ABSA", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.4295218586921692}]}, {"text": "The task attracted 245 submissions from 29 teams.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many consumers use the Web to share their experiences about products, services or travel destinations.", "labels": [], "entities": []}, {"text": "Online opinionated texts (e.g., reviews, tweets) are important for consumer decision making) and constitute a source of valuable customer feedback that can help companies to measure satisfaction and improve their products or services.", "labels": [], "entities": [{"text": "consumer decision making", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6709074378013611}]}, {"text": "In this setting, Aspect Based Sentiment Analysis (ABSA) -i.e., mining opinions from text about specific entities and their aspects (Liu, 2012) -can provide valuable insights to both consumers and businesses.", "labels": [], "entities": [{"text": "Aspect Based Sentiment Analysis (ABSA)", "start_pos": 17, "end_pos": 55, "type": "TASK", "confidence": 0.7425666792052132}]}, {"text": "An ABSA * *Corresponding author: mpontiki@ilsp.gr. method can analyze large amounts of unstructured texts and extract (coarse-or fine-grained) information not included in the user ratings that are available in some review sites (e.g.,).", "labels": [], "entities": []}, {"text": "Sentiment Analysis (SA) touches every aspect (e.g., entity recognition, coreference resolution, negation handling) of Natural Language Processing (Liu, 2012) and as mention \"it requires a deep understanding of the explicit and implicit, regular and irregular, and syntactic and semantic language rules\".", "labels": [], "entities": [{"text": "Sentiment Analysis (SA", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8784961700439453}, {"text": "entity recognition", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.791562408208847}, {"text": "coreference resolution", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.8808125853538513}, {"text": "negation handling", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.8662197589874268}]}, {"text": "Within the last few years several SA-related shared tasks have been organized in the context of workshops and conferences focus-ing on somewhat different research problems (.", "labels": [], "entities": [{"text": "SA-related shared tasks", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8642471234003702}]}, {"text": "Such competitions provide training datasets and the opportunity for direct comparison of different approaches on common test sets.", "labels": [], "entities": []}, {"text": "Currently, most of the available SA-related datasets, whether released in the context of shared tasks or not, are monolingual and usually focus on English texts.", "labels": [], "entities": []}, {"text": "Multilingual datasets) provide additional benefits enabling the development and testing of crosslingual methods.", "labels": [], "entities": []}, {"text": "Following this direction, this year the SemEval ABSA task provided datasets in a variety of languages.", "labels": [], "entities": [{"text": "SemEval ABSA task", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.9196943442026774}]}, {"text": "ABSA was introduced as a shared task for the first time in the context of SemEval in 2014; SemEval-2014 Task 4 1 (SE-ABSA14) provided datasets of English reviews annotated at the sentence level with aspect terms (e.g., \"mouse\", \"pizza\") and their polarity for the laptop and restaurant domains, as well as coarser aspect categories (e.g., \"food\") and their polarity only for restaurants ().", "labels": [], "entities": []}, {"text": "SemEval-2015 Task 12 2 (SE-ABSA15) built upon SE-ABSA14 and consolidated its subtasks into a unified framework in which all the identified constituents of the expressed opinions (i.e., aspects, opinion target expressions and sentiment polarities) meet a set of guidelines and are linked to each other within sentence-level tuples (.", "labels": [], "entities": []}, {"text": "These tuples are important since they indicate the part of text within which a specific opinion is expressed.", "labels": [], "entities": []}, {"text": "However, a user might also be interested in the overall rating of the text towards a particular aspect.", "labels": [], "entities": []}, {"text": "Such ratings can be used to estimate the mean sentiment per aspect from multiple reviews ().", "labels": [], "entities": []}, {"text": "Therefore, in addition to sentence-level annotations, SE-ABSA16 3 accommodated also text-level ABSA annotations and provided the respective training and testing data.", "labels": [], "entities": []}, {"text": "Fur-1 http://alt.qcri.org/semeval2014/task4/ 2 http://alt.qcri.org/semeval2015/task12/ 3 http://alt.qcri.org/semeval2016/task5/ thermore, the SE-ABSA15 annotation framework was extended to new domains and applied to languages other than English (Arabic, Chinese, Dutch, French, Russian, Spanish, and Turkish).", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: the task set-up is described in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 provides information about the datasets and the annotation process, while Section 4 presents the evaluation measures and the baselines.", "labels": [], "entities": []}, {"text": "General information about participation in the task is provided in Section 5.", "labels": [], "entities": []}, {"text": "The evaluation scores of the participating systems are presented and discussed in Section 6.", "labels": [], "entities": []}, {"text": "The paper concludes with an overall assessment of the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Similarly to SE-ABSA14 and SE-ABSA15, the datasets 8 of SE-ABSA16 were provided in an XML format and they are available under specific license terms through META-SHARE 9 , a repository devoted to the sharing and dissemination of language resources (Piperidis, 2012).", "labels": [], "entities": []}, {"text": "The evaluation ran in two phases.", "labels": [], "entities": []}, {"text": "In the first phase (Phase A), the participants were asked to return separately the aspect categories (Slot1), the OTEs (Slot2), and the {Slot1, Slot2} tuples for SB1.", "labels": [], "entities": []}, {"text": "For SB2 the respective text-level categories had to be identified.", "labels": [], "entities": [{"text": "SB2", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9334246516227722}]}, {"text": "In the second phase (Phase B), the gold annotations for the test sets of Phase A were provided and participants had to return the respective sentiment polarity values (Slot3).", "labels": [], "entities": [{"text": "Slot3)", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.8669313490390778}]}, {"text": "Similarly to SE-ABSA15, F-1 scores were calculated for Slot1, Slot2 and {Slot1, Slot2} tuples, by comparing the annotations that a system returned to the gold annotations (using micro-averaging).", "labels": [], "entities": [{"text": "F-1 scores", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9724097847938538}]}, {"text": "For Slot1 evaluation, duplicate occurrences of categories were ignored in both SB1 and SB2.", "labels": [], "entities": [{"text": "Slot1 evaluation", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8223935067653656}]}, {"text": "For Slot2, the calculation for each sentence considered only distinct targets and discarded \"null\" targets, since they do not correspond to explicit mentions.", "labels": [], "entities": []}, {"text": "To evaluate sentiment polarity classification (Slot3) in Phase B, we calculated the accuracy of each system, defined as the number of correctly predicted polarity labels of the (gold) aspect categories, divided by the total number of the gold aspect categories.", "labels": [], "entities": [{"text": "sentiment polarity classification (Slot3)", "start_pos": 12, "end_pos": 53, "type": "TASK", "confidence": 0.7450928787390391}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9995298385620117}]}, {"text": "Furthermore, we implemented and provided baselines for all slots of SB1 and SB2.", "labels": [], "entities": [{"text": "SB2", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.6933221220970154}]}, {"text": "In particular, the SE-ABSA15 baselines that were implemented for the English language (, were adapted for the other languages by using appropriate stopword lists and tokenization functions.", "labels": [], "entities": []}, {"text": "The baselines are briefly discussed below: SB1-Slot1: For category (E#A) extraction, a Support Vector Machine (SVM) with a linear kernel is trained.", "labels": [], "entities": [{"text": "category (E#A) extraction", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.7030484846660069}]}, {"text": "In particular, n unigram features are extracted from the respective sentence of each tuple that is encountered in the training data.", "labels": [], "entities": []}, {"text": "The category value (e.g., \"service#general\") of the tuple is used as the correct label of the feature vector.", "labels": [], "entities": []}, {"text": "Similarly, for each test sentence s, a feature vector is built and the trained SVM is used to predict the probabilities of assigning each possible category to s (e.g., {\"service#general\", 0.2}, {\"restaurant#general\", 0.4}.", "labels": [], "entities": []}, {"text": "Then, a threshold 10 t is used to decide which of the categories will be assigned to s.", "labels": [], "entities": []}, {"text": "As features, we use the 1,000 most frequent unigrams of the training data excluding stopwords.", "labels": [], "entities": []}, {"text": "SB1-Slot2: The baseline uses the training reviews to create for each category c (e.g., \"service#general\") a list of OTEs (e.g., \"service#general\" \u2192 {\"staff\", \"waiter\"}).", "labels": [], "entities": [{"text": "SB1-Slot2", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8186879754066467}]}, {"text": "These are extracted from the (training) opinion tuples whose category value is c . Then, given a test sentence sand an assigned category c, the baseline finds in s the first occurrence of each OTE of c's list.", "labels": [], "entities": []}, {"text": "The OTE slot is filled with the first of the target occurrences found in s.", "labels": [], "entities": [{"text": "OTE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5567483901977539}]}, {"text": "If no target occurrences are found, the slot is assigned the value \"null\".", "labels": [], "entities": []}, {"text": "SB1-Slot3: For polarity prediction we trained a SVM classifier with a linear kernel.", "labels": [], "entities": [{"text": "polarity prediction", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7264958322048187}]}, {"text": "Again, as in Slot1, n unigram features are extracted from the respective sentence of each tuple of the training data.", "labels": [], "entities": [{"text": "Slot1", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.7286241054534912}]}, {"text": "In addition, an integer-valued feature 12 that indicates the category of the tuple is used.", "labels": [], "entities": []}, {"text": "The correct label for the extracted training feature vector is the corresponding polarity value (e.g., \"positive\").", "labels": [], "entities": []}, {"text": "Then, for each tuple {category, OTE} of a test sentence s, a feature vector is built and classified using the trained SVM.", "labels": [], "entities": []}, {"text": "SB2-Slot1: The sentence-level tuples returned by the SB1 baseline are copied to the text level and duplicates are removed.", "labels": [], "entities": [{"text": "SB2-Slot1", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8386255502700806}]}, {"text": "The evaluation results are presented in Tables 3 (SB1: rest-en), 4 (SB1: rest-es, fr, ru, du, tu & hote-ar), 6 (SB1: lapt, came, phns), and 7 (SB2) . Each participating team was allowed to submit up to two runs per slot and domain in each phase; one constrained (C), where only the provided training data could be used, and one unconstrained (U), where other resources (e.g., publicly available  lexica) and additional data of any kind could be used for training.", "labels": [], "entities": []}, {"text": "In the latter case, the teams had to report the resources used.", "labels": [], "entities": []}, {"text": "Delayed submissions (i.e., runs submitted after the deadline and the release of the gold annotations) are marked with \"*\".", "labels": [], "entities": []}, {"text": "As revealed by the results, in both SB1 and SB2 the majority of the systems surpassed the baseline by a small or large margin and, as expected, the unconstrained systems achieved better results than the constrained ones.", "labels": [], "entities": [{"text": "SB1", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.6966103911399841}]}, {"text": "In SB1, the teams with the highest scores for Slot1 and Slot2 achieved similar F-1 scores (see) inmost cases (e.g., en/rest, es/rest, du/rest, fr/rest), which shows that the two slots have a similar level of difficulty.", "labels": [], "entities": [{"text": "Slot1", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.8342808485031128}, {"text": "Slot2", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.792976438999176}, {"text": "F-1 scores", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9848524034023285}]}, {"text": "However, as expected, the {Slot1, Slot2} scores were significantly lower since the linking of the target expressions to the corresponding aspects is also required.", "labels": [], "entities": []}, {"text": "The highest scores in SB1 for all slots (Slot1, Slot2, {Slot1, Slot2}, Slot3) were achieved in the en/rest; this is probably due to the high participation and to the lower complexity of the rest annotation schema compared to the other domains.", "labels": [], "entities": []}, {"text": "If we compare the results for SB1 and SB2, we notice that the SB2 scores for Slot1 are significantly higher (e.g., en/lapt, en/rest, es/rest) even though the respective annotations are for the same (or almost the same) set of texts.", "labels": [], "entities": []}, {"text": "This is due to the fact that it is easier to identify whether a whole text discusses an aspect c than finding all the sentences in the text discussing c . On the other hand, for Slot3, the SB2 scores are lower (e.g., en/rest, es/rest, ru/rest, en/lapt) than the respective SB1 scores.", "labels": [], "entities": []}, {"text": "This is mainly because an aspect maybe discussed at different points in a text and often with different sentiment.", "labels": [], "entities": []}, {"text": "In such cases a system has to identify the dominant sentiment, which usually is not trivial.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets provided for SE-ABSA16.", "labels": [], "entities": [{"text": "SE-ABSA16", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.8081154227256775}]}, {"text": " Table 5: Number of participating teams and submitted runs per  language.", "labels": [], "entities": []}]}