{"title": [{"text": "MSejrKu at SemEval-2016 Task 14: Taxonomy Enrichment by Evidence Ranking", "labels": [], "entities": [{"text": "Taxonomy Enrichment", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8760100305080414}]}], "abstractContent": [{"text": "Automatic enrichment of semantic tax-onomies with novel data is a relatively unexplored task with potential benefits in abroad array of natural language processing problems.", "labels": [], "entities": []}, {"text": "Task 14 of SemEval 2016 poses the challenge of designing systems for this task.", "labels": [], "entities": [{"text": "SemEval 2016", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.8131129443645477}]}, {"text": "In this paper, we describe and evaluate several machine learning systems constructed for our participation in the competition.", "labels": [], "entities": []}, {"text": "We demonstrate an f1-score of 0.680 for our submitted systems-a small improvement over the 0.679 produced by the hard baseline.", "labels": [], "entities": [{"text": "f1-score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9730858206748962}]}], "introductionContent": [{"text": "This article describes our systems submitted for SemEval2016, task 14 on taxonomy enrichment.", "labels": [], "entities": [{"text": "SemEval2016", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9244685173034668}, {"text": "taxonomy enrichment", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8863523900508881}]}, {"text": "The two submitted runs are based on Gaussian-kernel SVMs, and fall respectively under the constrained and the unconstrained condition of the shared task, namely using only WordNet and the training data, or incorporating outside sources.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 172, "end_pos": 179, "type": "DATASET", "confidence": 0.9672530889511108}]}, {"text": "We chose our submitted models by evaluation on the trial data in lieu of a development set.", "labels": [], "entities": []}, {"text": "Our runs perform better than the very hard baseline with a small 2% margin, while the best-performing heuristic outperforms the baseline by a 5% margin.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we explain the strategies we attempted.", "labels": [], "entities": []}, {"text": "In all cases, we discount the merge action, focusing only on attaching.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the datasets, showing the composition of nouns and verbs, merge-action and attach-actions, and the mean", "labels": [], "entities": []}, {"text": " Table 2: Performance reported as the f1 of Wu & Palmer-score and recall, along with Lemma Matches. Results are shown for the", "labels": [], "entities": [{"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.999667763710022}, {"text": "Lemma", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.9822090864181519}]}]}