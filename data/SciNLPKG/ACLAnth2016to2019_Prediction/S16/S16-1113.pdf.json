{"title": [{"text": "UNBNLP at SemEval-2016 Task 1: Semantic Textual Similarity: A Unified Framework for Semantic Processing and Evaluation", "labels": [], "entities": [{"text": "UNBNLP", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.829532265663147}, {"text": "Semantic Textual Similarity", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.7567274570465088}, {"text": "Semantic Processing and Evaluation", "start_pos": 84, "end_pos": 118, "type": "TASK", "confidence": 0.687425285577774}]}], "abstractContent": [{"text": "In this paper we consider several approaches to predicting semantic textual similarity using word embeddings, as well as methods for forming embeddings for larger units of text.", "labels": [], "entities": [{"text": "predicting semantic textual similarity", "start_pos": 48, "end_pos": 86, "type": "TASK", "confidence": 0.8493609577417374}]}, {"text": "We compare these methods to several base-lines, and find that none of them outperform the baselines.", "labels": [], "entities": []}, {"text": "We then consider both a supervised and unsupervised approach to combining these methods which achieve modest improvements over the baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings () have recently led to improvements in a wide range of tasks in natural language processing.", "labels": [], "entities": []}, {"text": "A number of approaches to forming embeddings for sentences, paragraphs, and documents have also recently been proposed (e.g.,.", "labels": [], "entities": []}, {"text": "These methods seem particularly well suited to the task of predicting semantic textual similarity (STS), and indeed have been shown to work very well on similar tasks (.", "labels": [], "entities": [{"text": "predicting semantic textual similarity (STS)", "start_pos": 59, "end_pos": 103, "type": "TASK", "confidence": 0.8895032235554287}]}, {"text": "This paper describes the system of UNBNLP at SemEval-2016 Task 1.", "labels": [], "entities": [{"text": "UNBNLP at SemEval-2016 Task 1", "start_pos": 35, "end_pos": 64, "type": "DATASET", "confidence": 0.715814870595932}]}, {"text": "We first implement several baseline approaches to STS based on cosine similarity of count-based vectors representing sentences, with a variety of approaches to term weighting.", "labels": [], "entities": [{"text": "STS", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.985734224319458}]}, {"text": "We then consider approaches drawing off of word2vec (, paragraph vectors (, and skip-thoughts (.", "labels": [], "entities": []}, {"text": "We find that none of these approaches improve over any of our baselines.", "labels": [], "entities": []}, {"text": "We then consider combining information from these individual methods to measuring STS.", "labels": [], "entities": [{"text": "STS", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9952991008758545}]}, {"text": "We consider an unsupervised approach based on the average of the predicted similarities fora number of these individual approaches.", "labels": [], "entities": []}, {"text": "We further consider a supervised approach in which we train ridge regression with features corresponding to the similarities from these individual methods.", "labels": [], "entities": []}, {"text": "Each of these methods for combining information achieves modest improvements over the baselines.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Pearson correlation for each method, on each dataset, as well as the weighted average correlation over all datasets (\"All\").", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8814522922039032}, {"text": "weighted average correlation", "start_pos": 79, "end_pos": 107, "type": "METRIC", "confidence": 0.703700602054596}]}]}