{"title": [{"text": "UCL+Sheffield at SemEval-2016 Task 8: Imitation learning for AMR parsing with an \u03b1-bound", "labels": [], "entities": [{"text": "UCL+Sheffield", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9545579155286154}, {"text": "Imitation learning", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9226740002632141}, {"text": "AMR parsing", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.9698696434497833}]}], "abstractContent": [{"text": "We develop a novel transition-based parsing algorithm for the abstract meaning representation parsing task using exact imitation learning , in which the parser learns a statistical model by imitating the actions of an expert on the training data.", "labels": [], "entities": [{"text": "abstract meaning representation parsing task", "start_pos": 62, "end_pos": 106, "type": "TASK", "confidence": 0.7688607335090637}]}, {"text": "We then use the imitation learning algorithm DAGGER to improve the performance, and apply an \u03b1-bound as a simple noise reduction technique.", "labels": [], "entities": [{"text": "DAGGER", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.8105544447898865}]}, {"text": "Our performance on the test set was 60% in F-score, and the performance gains on the development set due to DAGGER was up to 1.1 points of F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9986067414283752}, {"text": "DAGGER", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.8112071752548218}, {"text": "F-score", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.9908924102783203}]}, {"text": "The \u03b1-bound improved performance by up to 1.8 points.", "labels": [], "entities": []}], "introductionContent": [{"text": "In abstract meaning representation parsing (, the goal is to parse natural language in a domain-independent graph-based meaning representation (AMR).", "labels": [], "entities": [{"text": "abstract meaning representation parsing", "start_pos": 3, "end_pos": 42, "type": "TASK", "confidence": 0.6891408190131187}, {"text": "parse natural language", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.8697649240493774}]}, {"text": "In the first AMR parsing work, split the task into two sub-tasks; concept identification and graph creation.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9549336731433868}, {"text": "concept identification", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.7497824132442474}, {"text": "graph creation", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.7046216726303101}]}, {"text": "The sub-tasks are learned independently, and exact inference is used to find highest-scoring maximum spanning connected acyclic graph that contains all the concepts identified in the first stage.", "labels": [], "entities": []}, {"text": "Later work by adopted a different strategy based on the similarity between the dependency parse of a sentence and the semantic AMR graph.", "labels": [], "entities": []}, {"text": "They start from the dependency parse and learn a transition-based parser that converts it into an AMR graph.", "labels": [], "entities": []}, {"text": "To learn the parser, define an algorithm that for each instance in the training data infers the action sequence that convert the input dependency tree into the corresponding AMR graph and train a classifier to predict the actions to betaken during testing.", "labels": [], "entities": []}, {"text": "This strategy is also referred to as exact imitation learning, while the algorithm that infers the action sequence in the training instances is commonly referred to as the expert policy.", "labels": [], "entities": [{"text": "exact imitation learning", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.6966857512791952}]}, {"text": "In our submission to SemEval Task 8 on AMR parsing, we follow the transition-based paradigm of with modifications to the parsing algorithm, and also use the DAGGER imitation learning algorithm) to generalise better to unseen data.", "labels": [], "entities": [{"text": "SemEval Task 8", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.8806673487027487}, {"text": "AMR parsing", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.8484925329685211}]}, {"text": "The central idea of DAGGER is that the distribution of states encountered by the expert policy during training may not be a good approximation to those seen in testing by the trained policy.", "labels": [], "entities": []}, {"text": "Previous work by used SEARN, a similar imitation learning algorithm, on the AMR problem, with an algorithm that constructs the AMR graph directly from the sentence tokens.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.7806258797645569}, {"text": "AMR problem", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.8020617663860321}]}, {"text": "Imitation learning has also been used successfully in other semantic parsing tasks.", "labels": [], "entities": [{"text": "Imitation learning", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9620485007762909}, {"text": "semantic parsing tasks", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.81233149766922}]}, {"text": "In imitation learning approaches such as DAG-GER the previous actions become features for classification learning.", "labels": [], "entities": [{"text": "classification learning", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.9683840870857239}]}, {"text": "However the partial graphs in AMR parsing are rather complex to represent in this way, and combined with the finite amount of training data different actions can be chosen by the expert even though the feature representations for them can be very similar.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9468279778957367}]}, {"text": "These decisions appear as noisy outliers in classification learning.", "labels": [], "entities": [{"text": "classification learning", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.9675715863704681}]}, {"text": "To control noise we experiment with the \u03b1-bound discussed by, which excludes a training example from future training once it has been misclas-Action Name Param.", "labels": [], "entities": []}, {"text": "Pre-conditions Outcome of action NextEdge l r \u03b2 non-empty Set label of edge (\u03c3 0 , \u03b2 0 ) to l r . Pop \u03b2 0 . NextNode l c \u03b2 empty Set concept of node \u03c3 0 to l c . Pop \u03c3 0 , and initialise \u03b2.", "labels": [], "entities": []}, {"text": "Swap \u03b2 non-empty Make \u03b2 0 parent of \u03c3 0 (reverse edge) and its sub-graph.", "labels": [], "entities": []}, {"text": "Pop \u03b2 0 and insert \u03b2 0 as \u03c3 1 . ReplaceHead \u03b2 non-empty Pop \u03c3 0 and delete it from the graph.", "labels": [], "entities": []}, {"text": "Parents of \u03c3 0 become parents of \u03b2 0 . Other children of \u03c3 0 become children of \u03b2 0 . Insert \u03b2 0 at the head of \u03c3 and re-initialise \u03b2.", "labels": [], "entities": []}, {"text": "Reattach \u03ba \u03b2 non-empty Pop \u03b2 0 and delete edge (\u03c3 0 , \u03b2 0 ).", "labels": [], "entities": []}, {"text": "Attach \u03b2 0 as a child of \u03ba.", "labels": [], "entities": []}, {"text": "If \u03ba has already been popped from \u03c3 then re-insert it as \u03c3", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Features used by context. \u03c30P is the parent of \u03c30, \u03c30P P the parent of \u03c30P , and \u03c30C a child of \u03c30.", "labels": [], "entities": []}, {"text": " Table 3: F-Score results on validation set (\u03b1 = 1).", "labels": [], "entities": [{"text": "F-Score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9985731840133667}]}, {"text": " Table 4: Alpha-bound results with DAGGER, Inc and Red", "labels": [], "entities": [{"text": "DAGGER, Inc", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.821183999379476}]}]}