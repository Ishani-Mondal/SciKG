{"title": [{"text": "VRep at SemEval-2016 Task 1 and Task 2: A System for Interpretable Semantic Similarity", "labels": [], "entities": [{"text": "VRep", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8129186034202576}, {"text": "Similarity", "start_pos": 76, "end_pos": 86, "type": "TASK", "confidence": 0.5640019774436951}]}], "abstractContent": [{"text": "VRep is a system designed for SemEval 2016 Task 1-Semantic Textual Similarity (STS) and Task 2-Interpretable Semantic Textual Similarity (iSTS).", "labels": [], "entities": [{"text": "VRep", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9072286486625671}, {"text": "SemEval 2016 Task 1-Semantic Textual Similarity (STS)", "start_pos": 30, "end_pos": 83, "type": "TASK", "confidence": 0.8739409844080607}, {"text": "Semantic Textual Similarity (iSTS)", "start_pos": 109, "end_pos": 143, "type": "TASK", "confidence": 0.7247155904769897}]}, {"text": "STS quantifies the semantic equivalence between two snippets of text, and iSTS provides a reason why those snippets of text are similar.", "labels": [], "entities": []}, {"text": "VRep makes extensive use of WordNet for both STS, where the Vector relatedness measure is used, and for iSTS, where features are extracted to create a learned rule-based classifier.", "labels": [], "entities": [{"text": "VRep", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9363437294960022}, {"text": "WordNet", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9544881582260132}]}, {"text": "This paper outlines the VRep algorithm, provides results from the 2016 SemEval competition, and analyzes the performance contributions of the system components.", "labels": [], "entities": []}], "introductionContent": [{"text": "VRep competed in SemEval 2016 Task 1 -Semantic Textual Similarity (STS) and Task 2 -Semantic Interpretable Textual Similarity (iSTS).", "labels": [], "entities": [{"text": "VRep", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9483755230903625}, {"text": "SemEval 2016 Task 1 -Semantic Textual Similarity (STS)", "start_pos": 17, "end_pos": 71, "type": "TASK", "confidence": 0.7932986183599993}, {"text": "Semantic Interpretable Textual Similarity (iSTS)", "start_pos": 84, "end_pos": 132, "type": "TASK", "confidence": 0.7119773754051754}]}, {"text": "Both of these tasks compute STS between two fragments of text.", "labels": [], "entities": []}, {"text": "Task 2 expands upon Task 1 by requiring a reason for their similarity.", "labels": [], "entities": []}, {"text": "VRep uses an STS measure based on the Vector relatedness measure), and a reasoning system based on JRIP, an implementation of the iREP algorithm.", "labels": [], "entities": [{"text": "VRep", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.952180027961731}, {"text": "STS measure", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.937011182308197}]}, {"text": "For Task 1, we are provided with paired sentences, and for each pair of sentences VRep assigns a number indicating their STS.", "labels": [], "entities": [{"text": "STS", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9972201585769653}]}, {"text": "The number ranges from 0 to 5, 0 indicating no similarity and 5 indicating equivalence.", "labels": [], "entities": [{"text": "similarity", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.8885271549224854}]}, {"text": "For Task 2, we are provided with paired sentences and align the chunks of one sentence to the most similar chunks in the other sentence.", "labels": [], "entities": []}, {"text": "Next, a reason and score are computed for that alignment.", "labels": [], "entities": []}, {"text": "A chunk is a fragment of text that conveys a single meaning such as in the following example for which chunks are bracketed.", "labels": [], "entities": []}, {"text": "[ Alignment reasons are selected from a small list of possible labels created by the event organizers (): 1.", "labels": [], "entities": []}, {"text": "Equivalent (EQUI) -the two chunks convey an equivalent meaning (\"hot water\", \"scalding water\") 2.", "labels": [], "entities": [{"text": "Equivalent (EQUI)", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.8792326748371124}]}, {"text": "Opposite (OPPO) -the two chunks convey an opposite meaning (\"hot water\", \"cold water\") 3.", "labels": [], "entities": [{"text": "Opposite (OPPO)", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.795748233795166}]}, {"text": "More General (SPE1) -this chunk conveys a more general meaning than the other chunk (\"hot water\", \"water\") 4.", "labels": [], "entities": []}, {"text": "More Specific (SPE2) -this chunk conveys a more specific meaning than the other chunk \"water\", \"hot water\") 5.", "labels": [], "entities": []}, {"text": "Similar (SIMI) -the two chunks convey a similar meaning (\"sip water\", \"gulp water\") 6.", "labels": [], "entities": []}, {"text": "Related (REL) -the two chunks are somehow related (\"boil water\", \"ocean water\") 7.", "labels": [], "entities": [{"text": "Related (REL)", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9148194193840027}]}, {"text": "No Alignment (NOALI) -there are no chunks in the other sentence that are semantically similar to this chunk As in Task 1 the scores range from 0 to 5, 0 indicating no similarity and 5 indicating equivalence.", "labels": [], "entities": [{"text": "Alignment (NOALI)", "start_pos": 3, "end_pos": 20, "type": "METRIC", "confidence": 0.5649021491408348}]}, {"text": "VRep makes extensive use of WordNet) to compute STS and assign a label in iSTS.", "labels": [], "entities": [{"text": "VRep", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9612724184989929}, {"text": "WordNet", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.934771716594696}]}, {"text": "Vrep is written in Perl and is freely available for download 1 .", "labels": [], "entities": [{"text": "Vrep", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9198049306869507}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. More de- tails on the data sets and evaluation metrics are de- scribed in the competition summary", "labels": [], "entities": []}, {"text": " Table 1: Results of VRep on SemEval 2016 Task 1 Test Data", "labels": [], "entities": [{"text": "VRep on SemEval 2016 Task 1 Test Data", "start_pos": 21, "end_pos": 58, "type": "DATASET", "confidence": 0.7483812421560287}]}, {"text": " Table 2: Results of VRep on SemEval 2016 Task 2 Test Data", "labels": [], "entities": [{"text": "VRep on SemEval 2016 Task 2 Test Data", "start_pos": 21, "end_pos": 58, "type": "DATASET", "confidence": 0.7594743072986603}]}, {"text": " Table 3: The effects of additional components to the core VRep", "labels": [], "entities": [{"text": "VRep", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.6187207698822021}]}, {"text": " Table 4: The effects of components to the core VRep system", "labels": [], "entities": [{"text": "VRep", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.8614851832389832}]}, {"text": " Table 5: The performance of different classifiers on alignment", "labels": [], "entities": []}]}