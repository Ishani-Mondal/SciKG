{"title": [{"text": "Sensible at SemEval-2016 Task 11: Neural Nonsense Mangled in Ensemble Mess", "labels": [], "entities": [{"text": "Sensible at SemEval-2016 Task 11", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.761403226852417}, {"text": "Neural Nonsense Mangled in Ensemble Mess", "start_pos": 34, "end_pos": 74, "type": "TASK", "confidence": 0.7559805413087209}]}], "abstractContent": [{"text": "This paper describes our submission to the Complex Word Identification (CWI) task in SemEval-2016.", "labels": [], "entities": [{"text": "Complex Word Identification (CWI) task in SemEval-2016", "start_pos": 43, "end_pos": 97, "type": "TASK", "confidence": 0.7755383849143982}]}, {"text": "We test an experimental approach to blindly use neural nets to solve the CWI task that we know little/nothing about.", "labels": [], "entities": []}, {"text": "By structuring the input as a series of sequences and the output as a binary that indicates 1 to denote complex words and 0 otherwise , we introduce a novel approach to complex word identification using Recurrent Neu-ral Nets (RNN).", "labels": [], "entities": [{"text": "complex word identification", "start_pos": 169, "end_pos": 196, "type": "TASK", "confidence": 0.653435230255127}]}, {"text": "We also show that it is possible to simply ensemble several RNN clas-sifiers when we are unsure of the optimal hyper-parameters or the best performing models using eXtreme gradient boosted trees clas-sifiers.", "labels": [], "entities": []}, {"text": "Our systems submitted to the CWI task achieved the highest accuracy and F-score among the systems that uses neural networks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9995679259300232}, {"text": "F-score", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9991881251335144}]}], "introductionContent": [{"text": "The Deep Learning Tsunami has hit the Natural Language Processing (NLP) and Computational Linguistics field.", "labels": [], "entities": []}, {"text": "Deep neural nets has shown to be the ultimate hammer in various NLP shared tasks, systems trained on neural nets often emerge as the top systems and/or beat state-of-theart performance In the concluding remarks of the Google's Deep Learning course on Udacity 1 , Vincent Vanhoucke 1 https://www.udacity.com/course/deep-learning-ud730 said, \"What's really cool about those [neural net application] examples is that you don't have to know much about the problem you're trying to solve\".", "labels": [], "entities": []}, {"text": "Armed with basic knowledge of deep learning and neural nets and almost zero familiarity of the problem, we attempt to treat the Complex Word Identification (CWI) task as a binary classification task using Long Short-Term Memory (LSTM) Recurrent Neural Nets (RNN) with Gated Recurrent Units (GRU).", "labels": [], "entities": [{"text": "Complex Word Identification (CWI) task", "start_pos": 128, "end_pos": 166, "type": "TASK", "confidence": 0.7687216571399144}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of Best (Upper) and Neural Network Systems (Lower) in the SemEval-2016 CWI Task", "labels": [], "entities": [{"text": "SemEval-2016 CWI Task", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.683281421661377}]}]}