{"title": [{"text": "NTNUSentEval at SemEval-2016 Task 4: * Combining General Classifiers for Fast Twitter Sentiment Analysis", "labels": [], "entities": [{"text": "NTNUSentEval", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8379590511322021}, {"text": "Fast Twitter Sentiment Analysis", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.6404433399438858}]}], "abstractContent": [{"text": "The paper describes experiments on sentiment classification of microblog messages using an architecture allowing general machine learning classifiers to be combined either sequentially to form a multi-step classifier, or in parallel , creating an ensemble classifier.", "labels": [], "entities": [{"text": "sentiment classification of microblog messages", "start_pos": 35, "end_pos": 81, "type": "TASK", "confidence": 0.9280059576034546}]}, {"text": "The system achieved very competitive results in the shared task on sentiment analysis in Twitter, in particular on non-Twitter social media data, that is, input it was not specifically tailored to.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8967283666133881}]}], "introductionContent": [{"text": "As a growing platform for people to express themselves on a global scale, Twitter has become exceedingly attractive as an information source.", "labels": [], "entities": []}, {"text": "In addition to text, a tweet comes with metadata such as the sender's location and language, and hashtags, making it possible to quickly gather vast amounts of data regarding a specific product, person or event.", "labels": [], "entities": []}, {"text": "With a working Twitter Sentiment Analysis system, companies could get a feel of what consumers think of their products, or politicians could estimate their popularity amongst Twitter users in specific regions.", "labels": [], "entities": []}, {"text": "However, tweets and other informal texts on social media are quite different from texts elsewhere.", "labels": [], "entities": []}, {"text": "They are short in length and contain a lot of abbreviations, misspellings, Internet slang, and creative syntax.", "labels": [], "entities": []}, {"text": "Although the relative occurrence of nonstandard English syntax is fairly constant among many types of social media (, * Thanks to Mikael Brevik, J\u00f8rgen Faret, Johan Reitan and \u00d8yvind Selmer for their work on two previous NTNU systems.", "labels": [], "entities": []}, {"text": "analysing such texts using traditional language processing systems can be problematic, primarily since the main common denominator of social media text is not that it is informal, but that it describes language in rapid change, so that resources targeted directly at social media language quickly become outdated.", "labels": [], "entities": []}, {"text": "Twitter Sentiment Analysis (TSA) has been a rapibly growing research area in recent years, and atypical approach to TSA has been identified, using a supervised machine learning strategy, consisting of three main steps: preprocessing, feature extraction and classifier training.", "labels": [], "entities": [{"text": "Twitter Sentiment Analysis (TSA)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7732971509297689}, {"text": "feature extraction", "start_pos": 234, "end_pos": 252, "type": "TASK", "confidence": 0.760612964630127}]}, {"text": "Preprocessing is used in order to remove noise and standardize the tweet format, for example, by replacing or removing URLs.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9741593599319458}]}, {"text": "Desired features of the tweets are then extracted, such as sentiment scores using specific sentiment lexica or the occurrence of different emoticons.", "labels": [], "entities": []}, {"text": "Finally, a classifier is trained on the extracted features.", "labels": [], "entities": []}, {"text": "Since the machine learning algorithms used commonly are supervised, sentiment-annotated data is a prerequisite for training -and the growth of the TSA research field can largely be attributed to the International Workshop on Semantic Evaluation (SemEval) having run shared tasks on this theme since, annually producing new annotated data.", "labels": [], "entities": [{"text": "TSA research field", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.845970074335734}, {"text": "International Workshop on Semantic Evaluation (SemEval)", "start_pos": 199, "end_pos": 254, "type": "TASK", "confidence": 0.6203941181302071}]}, {"text": "The SemEval-2016 version (Task 4) of the TSA task and the data sets are described by.", "labels": [], "entities": [{"text": "TSA task", "start_pos": 41, "end_pos": 49, "type": "TASK", "confidence": 0.6945847868919373}]}, {"text": "Here we will specifically address Subtask A, which is a 3-way sentiment polarity classification problem, attributing the labels 'positive', 'negative' or 'neutral' to tweets.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.687708447376887}]}, {"text": "The rest of the paper is laid out as follows: Section 2 describes a general architecture for building: Overview of the core system architecture Twitter sentiment classifiers, drawing on the experiences of developing two previous TSA systems (.", "labels": [], "entities": [{"text": "Twitter sentiment classifiers", "start_pos": 144, "end_pos": 173, "type": "TASK", "confidence": 0.664202610651652}]}, {"text": "Section 3 reports the application of such a system ('NTNUSentEval') to the SemEval data sets, while Section 4 points to ways that the results could be improved.", "labels": [], "entities": [{"text": "NTNUSentEval", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.8477624654769897}, {"text": "SemEval data sets", "start_pos": 75, "end_pos": 92, "type": "DATASET", "confidence": 0.8588782946268717}]}], "datasetContent": [{"text": "The NTNUSentEval TSA system was trained on the Twitter training set (8,748 tweets), using the optimal parameters identified through grid search, and tested on the SemEval Twitter test sets from 2013 and 2014.", "labels": [], "entities": [{"text": "NTNUSentEval TSA", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8474051356315613}, {"text": "Twitter training set", "start_pos": 47, "end_pos": 67, "type": "DATASET", "confidence": 0.766519288221995}, {"text": "SemEval Twitter test sets", "start_pos": 163, "end_pos": 188, "type": "DATASET", "confidence": 0.8169626295566559}]}, {"text": "The complete results on these test sets are shown in below, while give the results on all test sets, including the unknown 2016 tweet set, in terms of the official evaluation metric, F PN 1 , which is the average of the F1-scores on the negative and the positive tweets.", "labels": [], "entities": [{"text": "F PN 1", "start_pos": 183, "end_pos": 189, "type": "METRIC", "confidence": 0.9620676636695862}, {"text": "F1-scores", "start_pos": 220, "end_pos": 229, "type": "METRIC", "confidence": 0.9964247345924377}]}, {"text": "Notably, our system performed extremely well on the out-of-domain test sets (i.e., the non-Twitter data), being the best of all 34 participating systems on the 2013-SMS set (with a 0.641 F PN 1 score, compared to a 0.190 F PN 1 baseline), the 3rd on the 2014-Live-journal set (F PN 1 = 0.719, with a 0.272 baseline), and overall tied for first on the out-of-domain data, supporting our claim that the approach taken in itself is quite general.", "labels": [], "entities": [{"text": "F PN 1 score", "start_pos": 187, "end_pos": 199, "type": "METRIC", "confidence": 0.9479917734861374}, {"text": "F PN 1 baseline", "start_pos": 221, "end_pos": 236, "type": "METRIC", "confidence": 0.8302087932825089}, {"text": "F PN 1", "start_pos": 277, "end_pos": 283, "type": "METRIC", "confidence": 0.9503800670305887}]}, {"text": "However, the lack of domain fine-tuning of the system showed in comparison to the best systems on Twitter data, with the NTNUSentEval system consistently placing 11-13 on the different test sets, including 11th on the 2016 set (F PN 1 = 0.583, with baseline 0.255).", "labels": [], "entities": [{"text": "NTNUSentEval", "start_pos": 121, "end_pos": 133, "type": "DATASET", "confidence": 0.9086819887161255}, {"text": "F PN 1", "start_pos": 228, "end_pos": 234, "type": "METRIC", "confidence": 0.9493791063626608}]}, {"text": "Two instances of the BaseClassifier can be chained sequentially creating a 2-step classifier.", "labels": [], "entities": []}, {"text": "Such a classifier was tested on the 2013 and 2014 test sets, as shown in.", "labels": [], "entities": [{"text": "2013 and 2014 test sets", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.7436379015445709}]}, {"text": "The 2-step classifier performs worse than the 1-step classifier on the 2013 set, while their performances on the 2014 set are comparable, so based on these results it is not clear that 1-step classification is better than 2-step.", "labels": [], "entities": []}, {"text": "The GATE TwitIE part-of-speech tagger uses an underlying model when tagging tweets.", "labels": [], "entities": [{"text": "GATE TwitIE part-of-speech tagger", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7715395092964172}]}, {"text": "In addition to the standard best performing model, another highspeed model trading 2.5% token accuracy for half: Sentiment classifier performance the tagging speed is available, and the results from testing BaseClassifier using the high-speed tagger model are also shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.8129061460494995}]}, {"text": "Although a slight reduction in performance can be observed compared to using the best tagger model, the high-speed model significantly reduced the total execution time, from 107 to 80 seconds on the 2013-test set and from 53 to 41 seconds on the 2014-test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Preprocessing used by feature extractors", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.954167366027832}, {"text": "feature extractors", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.6998610645532608}]}, {"text": " Table 2: Optimal parameter settings", "labels": [], "entities": []}, {"text": " Table 3: Feature ablation study results (F 1 -scores)", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9523820281028748}]}, {"text": " Table 4: Sentiment classifier performance", "labels": [], "entities": [{"text": "Sentiment classifier", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9604089558124542}]}]}