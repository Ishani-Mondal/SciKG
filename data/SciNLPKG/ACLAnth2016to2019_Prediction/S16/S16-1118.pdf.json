{"title": [{"text": "DalGTM at SemEval-2016 Task 1: Importance-Aware Compositional Approach to Short Text Similarity", "labels": [], "entities": [{"text": "DalGTM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.915213406085968}, {"text": "Importance-Aware Compositional Approach", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.7010622620582581}, {"text": "Short Text Similarity", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.5670084059238434}]}], "abstractContent": [{"text": "This paper describes our system submission to the SemEval 2016 English Semantic Textual Similarity (STS) shared task.", "labels": [], "entities": [{"text": "SemEval 2016 English Semantic Textual Similarity (STS) shared task", "start_pos": 50, "end_pos": 116, "type": "TASK", "confidence": 0.9071155407211997}]}, {"text": "The proposed system is based on the compositional text similarity model, which aggregates pairwise word similarities for computing the semantic similarity between texts.", "labels": [], "entities": []}, {"text": "In addition, our system combines word importance and word similarity to build an importance-similarity matrix.", "labels": [], "entities": []}, {"text": "Three different word similarity measures are used in our three submitted runs.", "labels": [], "entities": []}, {"text": "The evaluation results show that taking into account context dependent word importance information improves performance.", "labels": [], "entities": []}, {"text": "However, the performance of the system varies drastically between different evaluation subsets.", "labels": [], "entities": []}, {"text": "The best of our submitted runs achieves rank 60th with weighted mean Pearson correlation to human judgements of 0.6892.", "labels": [], "entities": [{"text": "weighted mean Pearson correlation", "start_pos": 55, "end_pos": 88, "type": "METRIC", "confidence": 0.7304042279720306}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) measures the degree of equivalence in the underlying semantics of paired natural language texts.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7799161871274313}]}, {"text": "It is an extensively researched problem with applications widely used in many research areas including natural language processing, information retrieval, and text mining.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.63343213001887}, {"text": "information retrieval", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.8481765687465668}, {"text": "text mining", "start_pos": 159, "end_pos": 170, "type": "TASK", "confidence": 0.8618082702159882}]}, {"text": "The STS task has been held annually since 2012 () to encourage research into understanding sentence-level semantics.", "labels": [], "entities": [{"text": "STS task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.7912689745426178}]}, {"text": "Systems for this task compute semantic similarity scores for paired text snippets.", "labels": [], "entities": []}, {"text": "Performance is evaluated by: The general procedure of the compositional text similarity measures.", "labels": [], "entities": []}, {"text": "They take three general steps: tokenize the input text, compute pairwise word similarities between all words, and aggregate the resulting scores to a sentence level textual similarity score.", "labels": [], "entities": []}, {"text": "{w11, w12, . .", "labels": [], "entities": []}, {"text": ", and {w21, w22, . .", "labels": [], "entities": []}, {"text": ", are the tokenized words from Text 1 and Text 2, respectively.", "labels": [], "entities": []}, {"text": "Each node in the middle represents a vector of pairwise similarity values computed by one word from Text 1 and all distinct words from Text 2.", "labels": [], "entities": []}, {"text": "the Pearson correlation between the system scores and human judgements.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9198722541332245}]}, {"text": "This paper describes our system submission to the SemEval 2016 STS shared task ().", "labels": [], "entities": [{"text": "SemEval 2016 STS shared task", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.8289577841758728}]}, {"text": "The proposed system is based on the compositional text similarity model, which have been broadly researched in the literature by.", "labels": [], "entities": []}, {"text": "The compositional text similarity model makes use of word-level It first computes the word importance value w using Eq.", "labels": [], "entities": [{"text": "compositional text similarity", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.6137853463490804}]}, {"text": "3 and adapted into every entry sim in the importance-similarity matrix using similarity values as the building blocks to compute sentence-level semantics.", "labels": [], "entities": []}, {"text": "Computing textual similarity using this approach proceeds as follows: tokenize the input text, compute pairwise word similarities between all words, and aggregate the resulting scores to a sentence level textual similarity score.", "labels": [], "entities": []}, {"text": "State-of-the-art word similarity measures can be used in this model to provide context independent word relatedness.", "labels": [], "entities": []}, {"text": "However, words an be more or less important depending on the contexts in which they appear.", "labels": [], "entities": []}, {"text": "We extend traditional compositional models with an importance term for each word.", "labels": [], "entities": []}, {"text": "Our three submitted runs use this extended model in combination with three different word similarity measures: Google Trigram Method (), Skipgrams word embedding (, and GloVe word embedding ().", "labels": [], "entities": []}, {"text": "The evaluation results show that including matching importance information improves the performance of compositional models on most of the evaluation sets for STS 2015 and 2016.", "labels": [], "entities": [{"text": "STS 2015", "start_pos": 159, "end_pos": 167, "type": "TASK", "confidence": 0.663907378911972}]}, {"text": "However, the relative performance of our systems varies dramatically when comparing against other systems submitted to the shared task.", "labels": [], "entities": []}, {"text": "The best of our submitted runs achieves rank 60th with weighted mean Pearson correlation of 0.6892 with human judgements.", "labels": [], "entities": [{"text": "weighted mean Pearson correlation", "start_pos": 55, "end_pos": 88, "type": "METRIC", "confidence": 0.7340784966945648}]}, {"text": "The rest of the paper is organized as follows: Section 2 describes the details of the submitted systems.", "labels": [], "entities": []}, {"text": "Section 3 shows the experimental results for our three runs using evaluation data from SemEval 2015 and 2016.", "labels": [], "entities": []}, {"text": "Section 4 summarizes our observations and concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our three system submissions using the STS 2015 and 2016 evaluation datasets.", "labels": [], "entities": [{"text": "STS 2015 and 2016 evaluation datasets", "start_pos": 52, "end_pos": 89, "type": "DATASET", "confidence": 0.8822093705336252}]}, {"text": "The SemEval 2015 and 2016 datasets contain test sentence pairs distributed across nine domains.", "labels": [], "entities": [{"text": "SemEval 2015 and 2016 datasets", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.80765700340271}]}, {"text": "Each pair was assigned a similarity scores in the range by multiple human annotators.", "labels": [], "entities": [{"text": "similarity", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9690360426902771}]}, {"text": "The performance of our three system submissions is shown in and 3.", "labels": [], "entities": []}, {"text": "Recall that our three systems only differ in the method they use for assessing lexical similarity: Google Trigram Method (GTM), Word2vec (W2V), and GloVe.", "labels": [], "entities": []}, {"text": "Systems that make use of matching importance are tagged with +IAC.", "labels": [], "entities": [{"text": "IAC", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9855403304100037}]}, {"text": "Otherwise, the system directly uses pairwise similarity values to compute the aggregate similarity score using Eq.", "labels": [], "entities": [{"text": "aggregate similarity score", "start_pos": 78, "end_pos": 104, "type": "METRIC", "confidence": 0.6848817865053812}]}, {"text": "6. Note that systems with the proposed matching importance approach perform consistently better than the original compositional model inmost of the domain subsets.", "labels": [], "entities": []}, {"text": "This shows that adding an importance feature can effectively improve the performance of the compositional model.", "labels": [], "entities": []}, {"text": "However, comparing against the average system performance in each domain, the performance of our submitted systems vary dramatically in their relative performance to systems submitted by other participating teams.", "labels": [], "entities": []}, {"text": "For example, our systems perform well on the postediting dataset and dramatically worse, even relative to other systems, on the question-question data.", "labels": [], "entities": []}, {"text": "This suggests that the proposed system may have an implicit domain specific bias.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A brief description of SemEval 2015 and 2016 datasets.", "labels": [], "entities": [{"text": "SemEval 2015 and 2016 datasets", "start_pos": 33, "end_pos": 63, "type": "DATASET", "confidence": 0.8189626932144165}]}, {"text": " Table 2: Evaluation result for SemEval 2015 STS dataset. Both the compositional model and the proposed model (systems with", "labels": [], "entities": [{"text": "SemEval 2015 STS dataset", "start_pos": 32, "end_pos": 56, "type": "DATASET", "confidence": 0.6786898821592331}]}, {"text": " Table 3: Evaluation results for SemEval 2016 STS dataset. It shows the same characteristics of the proposed model in both SemEval", "labels": [], "entities": [{"text": "SemEval 2016 STS dataset", "start_pos": 33, "end_pos": 57, "type": "DATASET", "confidence": 0.6895157843828201}, {"text": "SemEval", "start_pos": 123, "end_pos": 130, "type": "TASK", "confidence": 0.5596274733543396}]}]}