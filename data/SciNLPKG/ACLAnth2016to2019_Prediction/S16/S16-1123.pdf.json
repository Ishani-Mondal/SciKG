{"title": [{"text": "VENSESEVAL at Semeval-2016 Task 2: iSTS -with a full-fledged rule- based approach", "labels": [], "entities": []}], "abstractContent": [{"text": "In our paper we present our rule-based system for semantic processing.", "labels": [], "entities": [{"text": "semantic processing", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.9018504917621613}]}, {"text": "In particular we show examples and solutions that maybe challenge our approach.", "labels": [], "entities": []}, {"text": "We then discuss problems and shortcomings of Task 2-iSTS.", "labels": [], "entities": []}, {"text": "We comment on the existence of a tension between the inherent need to on the one side, to make the task as much as possible \"semantically feasible\".", "labels": [], "entities": []}, {"text": "Whereas the detailed presentation and some notes in the guidelines refer to inferential processes, paraphrases and the use of commonsense knowledge of the world for the interpretation to work.", "labels": [], "entities": []}, {"text": "We then present results and some conclusions.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this presentation we will focus on Task 2, Interpretable Semantic Textual Similarity.", "labels": [], "entities": [{"text": "Interpretable Semantic Textual Similarity", "start_pos": 46, "end_pos": 87, "type": "TASK", "confidence": 0.5828628614544868}]}, {"text": "We will comment on the way in which the task has been defined and what is eventually made available to participants to the task.", "labels": [], "entities": []}, {"text": "In the section below we will present our system created for RTERecognizing Textual Entailment -challenges, and how it has been reorganized to suit the new current task.", "labels": [], "entities": [{"text": "RTERecognizing Textual Entailment", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.9087056517601013}]}, {"text": "We discuss examples that suitor challenge our approach.", "labels": [], "entities": []}, {"text": "We then comment in detail problems and shortcoming related to issues related to the annotations and the choice of semantic relations.", "labels": [], "entities": []}, {"text": "Ina final section we will present our results and a discussion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results compared to Baseline for  IMAGES SYS -Rank 12 over 13", "labels": [], "entities": [{"text": "IMAGES SYS -Rank", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.597294919192791}]}, {"text": " Table 2: Results compared to Baseline for  IMAGES GS -Rank 10 over 20", "labels": [], "entities": [{"text": "IMAGES GS -Rank", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.5711332410573959}]}, {"text": " Table 3: Results compared to Baseline for  HEADLINES SYS -Rank 12 over 13", "labels": [], "entities": [{"text": "HEADLINES SYS", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.4685472548007965}]}, {"text": " Table 4: Results compared to Baseline for  HEADLINES GS -Rank 13 over 20", "labels": [], "entities": [{"text": "HEADLINES GS -Rank", "start_pos": 44, "end_pos": 62, "type": "METRIC", "confidence": 0.7451570108532906}]}]}