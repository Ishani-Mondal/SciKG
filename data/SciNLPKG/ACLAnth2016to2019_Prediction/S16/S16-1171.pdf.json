{"title": [{"text": "Inspire at SemEval-2016 Task 2: Interpretable Semantic Textual Similarity Alignment based on Answer Set Programming", "labels": [], "entities": [{"text": "SemEval-2016 Task", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.8508874773979187}, {"text": "Interpretable Semantic Textual Similarity Alignment", "start_pos": 32, "end_pos": 83, "type": "TASK", "confidence": 0.6292955994606018}]}], "abstractContent": [{"text": "In this paper we present our system developed for the SemEval 2016 Task 2-Interpretable Semantic Textual Similarity along with the results obtained for our submitted runs.", "labels": [], "entities": [{"text": "SemEval 2016 Task 2-Interpretable Semantic Textual Similarity", "start_pos": 54, "end_pos": 115, "type": "TASK", "confidence": 0.9005133509635925}]}, {"text": "Our system participated in the subtasks predicting chunk similarity alignments for gold chunks as well as for predicted chunks.", "labels": [], "entities": [{"text": "predicting chunk similarity alignments", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.8414461612701416}]}, {"text": "The Inspire system extends the basic ideas from last years participant NeRoSim, however we realize the rules in logic programming and obtain the result with an Answer Set Solver.", "labels": [], "entities": [{"text": "NeRoSim", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9334039688110352}]}, {"text": "To prepare the input for the logic program, we use the PunktTokenizer, Word2Vec, and WordNet APIs of NLTK, and the POS-and NER-taggers from Stanford CoreNLP.", "labels": [], "entities": [{"text": "PunktTokenizer", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.9702305197715759}, {"text": "Word2Vec", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9210016131401062}, {"text": "Stanford CoreNLP", "start_pos": 140, "end_pos": 156, "type": "DATASET", "confidence": 0.7781642973423004}]}, {"text": "For chunking we use a joint POS-tagger and dependency parser and based on that determine chunks with an Answer Set Program.", "labels": [], "entities": [{"text": "chunking", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.965543270111084}]}, {"text": "Our system ranked third place overall and first place in the Headlines gold chunk subtask.", "labels": [], "entities": [{"text": "Headlines gold chunk subtask", "start_pos": 61, "end_pos": 89, "type": "DATASET", "confidence": 0.9785314500331879}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS), refers to the degree of semantic equivalence between a pair of texts.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7854873935381571}]}, {"text": "This helps in explaining how some texts are related or unrelated.", "labels": [], "entities": []}, {"text": "In Interpretable STS (iSTS) systems, further explanation is provided as to why the two texts are related or unrelated.", "labels": [], "entities": [{"text": "Interpretable STS (iSTS)", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.7738051772117615}]}, {"text": "Finding these detailed explanations helps in gathering a meaningful representation of their similarities.", "labels": [], "entities": []}, {"text": "The competition at SemEval 2016 was run on three different datasets: Headlines, Images and Student-Answers.", "labels": [], "entities": []}, {"text": "Each dataset included two files containing pairs of sentences and two files containing pairs of gold-chunked sentences.", "labels": [], "entities": []}, {"text": "Either the gold chunks provided by the organizers or chunks obtained from the given texts would be used as input to the system.", "labels": [], "entities": []}, {"text": "The expected outputs of the system are m:n chunk-chunk alignments, corresponding similarity scores between 0 (unrelated) and 5(equivalent), and a label indicating the type of semantic relation.", "labels": [], "entities": []}, {"text": "Possible semantic relations are EQUI (semantically equal), OPPO (opposite), SPE1/SPE2 (chunk 1/2 is more specific than chunk 2/1), SIMI (similar but none of the relations above), REL (related but none of the relations above), and NOALI (not aligned, e.g., punctuation).", "labels": [], "entities": [{"text": "OPPO", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.7767049074172974}, {"text": "REL", "start_pos": 179, "end_pos": 182, "type": "METRIC", "confidence": 0.9815442562103271}, {"text": "NOALI", "start_pos": 230, "end_pos": 235, "type": "METRIC", "confidence": 0.9527513384819031}]}, {"text": "All relations shall be considered in the given context.", "labels": [], "entities": []}, {"text": "For details seethe companion paper ().", "labels": [], "entities": []}, {"text": "Similarity alignment in the Inspire system is based on ideas of previous year's NeRoSim () entry, however we reimplemented the system and realize the rule engine in Answer Set Programming (ASP)) which gives us flexibility for reordering rules or applying them in parallel.", "labels": [], "entities": [{"text": "NeRoSim", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.8576645851135254}]}, {"text": "To account for differences in datasets we detect the type of input with a Naive Bayes classifier and use different parameter sets for each of the datasets.", "labels": [], "entities": []}, {"text": "Chunking in the Inspire system is based on a joint POS-tagger and dependency parser) and an ASP program that determines chunk boundaries.", "labels": [], "entities": []}, {"text": "In Section 2 we give preliminaries of ASP, Section 3 describes how we represented semantic alignment in ASP, in Section 5 we explain parameter sets used for different datasets, Section 4 briefly outlines our chunking approach, and we give evaluation results in Section 6.", "labels": [], "entities": [{"text": "ASP", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.932277500629425}]}], "datasetContent": [{"text": "Our system does not require training, so we tested and optimized it on the training data for Headlines (H), Images (I), and Student-Answers (S) datasets.", "labels": [], "entities": []}, {"text": "As a criteria for accuracy the competition used the F1 score based on alignments (A), alignments and alignment type (AT), alignments and alignment score (AS), and full consideration of alignment, type, and score (ATS).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9989486336708069}, {"text": "F1 score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9869101047515869}, {"text": "alignments and alignment type (AT)", "start_pos": 86, "end_pos": 120, "type": "METRIC", "confidence": 0.712597097669329}, {"text": "alignments and alignment score (AS)", "start_pos": 122, "end_pos": 157, "type": "METRIC", "confidence": 0.8089860507420131}, {"text": "score (ATS)", "start_pos": 206, "end_pos": 217, "type": "METRIC", "confidence": 0.6382435113191605}]}, {"text": "Our optimization experiments showed us, that there are significant differences in annotations between datasets.", "labels": [], "entities": []}, {"text": "In particular S contains spelling mistakes, verbs are often singleton chunks in H, and 'to' and ''s' often start anew chunk in H, while they are annotated as part of the previous chunk in I and S.", "labels": [], "entities": []}, {"text": "Therefore we decided to configure our system differently for each dataset, based on a Multinomial Naive Bayes Classifier trained on input unigrams and bigrams implemented using scikitlearn).", "labels": [], "entities": []}, {"text": "F1-score obtained on training data with 10-fold cross-validation was 0.99.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9944813251495361}]}, {"text": "Our dataset configuration is as follows: we exclude stopwords from the calculation of similarity (1) for datasets H and I by using the NLTK corpus of stopwords; we remove non-singleton punctuation for dataset S; and we add rules to handle verb types (VBP, VBZ) as punctuation and ''s' as a preposition in chunking.", "labels": [], "entities": [{"text": "NLTK corpus", "start_pos": 135, "end_pos": 146, "type": "DATASET", "confidence": 0.9517976343631744}]}, {"text": "We optimized parameters for 3 runs according to different criteria.", "labels": [], "entities": []}, {"text": "Run 1 is optimized for the full label (ATS).", "labels": [], "entities": [{"text": "ATS", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.7756984829902649}]}, {"text": "We used our implementation of NeRoSim rules in the same order, except SI4, SI5, and RE1, which we excluded.", "labels": [], "entities": [{"text": "RE1", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.7736724019050598}]}, {"text": "In ASP this is configured by defining facts for nextStep(s, s ) where Run 2 is optimized for prediction of alignment (A), this is done by using all NeRoSim rules in their original order: we define nextStep(s, s ) for (s, s ) \u2208 {(noalic, equi1), (equi1, equi2), (equi2, equi3), (equi3, equi4), (equi4, equi5), In addition, for dataset S we perform automated spelling correction using Enchant.", "labels": [], "entities": [{"text": "ASP", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9492303729057312}, {"text": "spelling correction", "start_pos": 357, "end_pos": 376, "type": "TASK", "confidence": 0.8549248278141022}]}, {"text": "3 Run 3 is based the observation, that the scorer tool does not severely punish overlapping alignments in the F1-score of A.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9985709190368652}]}, {"text": "Hence we allow SIMI4, SIMI5, and REL1 to be applied simultaneously by defining nextStep(s, s ) for (s, s ) \u2208 {(noalic, equi1), (equi1, equi2), 3 http://www.abisource.com/projects/enchant/ (equi2, equi3), (equi3, equi4), (equi4, equi5), Accordingly, we expected Run 1 to perform best with respect to the ATS (and AT) metric, Run 2 to perform best with respect to A (and AS) metrics, and Run 3 to sometimes perform above other runs.", "labels": [], "entities": [{"text": "REL1", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9454748034477234}]}, {"text": "These expectations were confirmed by the results shown in the next section.", "labels": [], "entities": []}], "tableCaptions": []}