{"title": [{"text": "Deftor at SemEval-2016 Task 14: Taxonomy Enrichment using Definition Vectors", "labels": [], "entities": [{"text": "SemEval-2016 Task", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8070720434188843}, {"text": "Taxonomy Enrichment", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.8478760421276093}]}], "abstractContent": [{"text": "In this paper we describe the participation of the Joint Research Centre, EC, in task 14-Semantic Taxonomy Enrichment at SemEval 2016.", "labels": [], "entities": [{"text": "Joint Research Centre, EC", "start_pos": 51, "end_pos": 76, "type": "DATASET", "confidence": 0.713580983877182}, {"text": "Taxonomy Enrichment at SemEval 2016", "start_pos": 98, "end_pos": 133, "type": "TASK", "confidence": 0.7142166376113892}]}, {"text": "The algorithm which we propose transforms each candidate definition into a term vector, where each dimension represents a term and its value is calculated by TF.IDF.", "labels": [], "entities": [{"text": "TF.IDF", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.9620832800865173}]}, {"text": "We attach the candidate term as a hyponym to the WordNet synset with the most similar definition.", "labels": [], "entities": [{"text": "WordNet synset", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.9719436764717102}]}, {"text": "The results we obtained are encouraging , considering the simplicity of our approach.", "labels": [], "entities": []}, {"text": "The obtained F measure is below the average, but above one of the baselines.", "labels": [], "entities": [{"text": "F measure", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.990215390920639}]}], "introductionContent": [{"text": "In this paper we describe the participation of the Joint Research Centre, EC, in task 14 -Semantic Taxonomy Enrichment at SemEval 2016.", "labels": [], "entities": [{"text": "Joint Research Centre, EC", "start_pos": 51, "end_pos": 76, "type": "DATASET", "confidence": 0.7127351820468902}, {"text": "Semantic Taxonomy Enrichment at SemEval 2016", "start_pos": 90, "end_pos": 134, "type": "TASK", "confidence": 0.7452919085820516}]}, {"text": "We particpate for the first time in a similar task.", "labels": [], "entities": []}, {"text": "We opted fora relatively simple method for searching of relevant synsets, which does not exploit any external dictionary or another semantic resource.", "labels": [], "entities": []}, {"text": "We called our system Deftor (DEFinition vecTOR).", "labels": [], "entities": [{"text": "DEFinition vecTOR", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.5768672078847885}]}, {"text": "Deftor is a system which represents the definitions (glosses) as lexical vectors and finds the most similar one for each new lemma.", "labels": [], "entities": []}, {"text": "Automatic enrichment of taxonomies and knowledge bases is very important especially for rapidly changing domain.", "labels": [], "entities": []}, {"text": "The taxonomy enrichment task is quite challenging, mostly because of the many possibilities when attaching anew term to an existing taxonomy: First, anew word can be attached as a hyponym to different concepts, which describe it at different levels of abstraction.", "labels": [], "entities": [{"text": "taxonomy enrichment", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8975752890110016}]}, {"text": "For example, in WordNet the hurricane is a hyponym of cyclone, which is a hyponym of windstorm, which itself is a hyponym of storm and the storm is a hyponym of atmospheric phenomenon.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.9645532965660095}]}, {"text": "It is not always easy to decide whereto attach a concept: in the above mentioned case the definition of storm and windstorm are not very different.", "labels": [], "entities": []}, {"text": "In this case, it is also difficult to decide if anew concept should be merged with a similar concept from WordNet or it should be attached as a hyponym.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.947566032409668}]}, {"text": "Another problem are the multiple aspects from which a concept can be perceived.", "labels": [], "entities": []}, {"text": "For example, one can consider hurricane to be a natural disaster.", "labels": [], "entities": []}, {"text": "It is also a weather condition or cause of death.", "labels": [], "entities": []}, {"text": "All these considerations unfortunately make taxonomy enrichment task quite ambiguous and difficult to tackle.", "labels": [], "entities": [{"text": "taxonomy enrichment", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.933251291513443}]}, {"text": "In some cases, the right attachment of anew concept will be difficult also fora human expert.", "labels": [], "entities": []}, {"text": "Although the task is quite complicated, as we have pointed out, we applied a simple approach which is based on comparison of the lexical content of the definitions of the new concepts and the WordNet synsets.", "labels": [], "entities": [{"text": "WordNet synsets", "start_pos": 192, "end_pos": 207, "type": "DATASET", "confidence": 0.9401436746120453}]}, {"text": "Our approach to the taxonomy enrichment task represents each synset from WordNet and the candidate new terms as word vectors from their definitions and then attaches each new term as a hyponym to the synset for which the cosine similarity of its definition vector and the definition vector of the new term is the highest.", "labels": [], "entities": [{"text": "taxonomy enrichment", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7819387912750244}, {"text": "WordNet", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9690807461738586}]}, {"text": "The algorithm which we propose transforms each candidate definition into a definition vector, a term vector, where each dimension represents a term and its weight is calculated by TF.IDF.", "labels": [], "entities": [{"text": "TF.IDF", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.9615866541862488}]}, {"text": "In this way we represent each WordNet definition, as well as the definitions of the new terms for inclusion in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9183472394943237}, {"text": "WordNet", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9719957113265991}]}, {"text": "Moreover, we expanded each definition vector with the definitions of the words from this vector.", "labels": [], "entities": []}, {"text": "We then calculated the cosine similarity between each WordNet synset definition and the definition of the candidate term whose place in the WordNet hierarchy is to be identified.", "labels": [], "entities": []}, {"text": "Then, we attach the candidate term as a hyponym to the synset with the most similar definition.", "labels": [], "entities": []}, {"text": "Using this method we obtained results which are encouraging, considering the simplicity of our method -it relies solely on WordNet and no additional dictionaries or other resources were used.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.9634959101676941}]}, {"text": "The results we obtained are somehow below the average, but above the weaker baseline.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation shows that our system can be improved significantly, but still results are encouraging considering the simplicity of our approach.", "labels": [], "entities": []}, {"text": "The F1 of our only run was found to be 0.5132, which is below the baseline First word, first sense, but much above the baseline Random synset, which shows the feasibility of our approach.", "labels": [], "entities": [{"text": "F1", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9957350492477417}]}, {"text": "It goes without saying that our results can be improved, nevertheless we propose an approach whose main advantage is its simplicity and it is independent of external resources.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9849981665611267}]}, {"text": "Our method is potentially multilingual and can be applied on taxonomies in languages other than English.", "labels": [], "entities": []}, {"text": "Our system Deftor does not rely on any external knowledge and it uses only part-of-speech tagging and lemmatization as a pre-processing step.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.6901886165142059}]}, {"text": "Since P.O.S. taggers and lemmatizers are available in different languages, we can easily adapt Deftor between languages and between domains, since the exploited algorithm does not depend on the taxonomy domain.", "labels": [], "entities": [{"text": "P.O.S. taggers", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.7856249809265137}]}, {"text": "Moreover, the simplicity of our approach make it quite efficient and easy to implement.", "labels": [], "entities": []}], "tableCaptions": []}