{"title": [{"text": "You and me... in a vector space: modelling individual speakers with distributional semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "The linguistic experiences of a person are an important part of their individuality.", "labels": [], "entities": []}, {"text": "In this paper, we show that people can be modelled as vectors in a semantic space, using their personal interaction with specific language data.", "labels": [], "entities": []}, {"text": "We also demonstrate that these vectors can betaken as representative of 'the kind of person' they are.", "labels": [], "entities": []}, {"text": "We build over 4000 speaker-dependent subcorpora using logs of Wikipedia edits , which are then used to build distri-butional vectors that represent individual speakers.", "labels": [], "entities": []}, {"text": "We show that such 'person vec-tors' are informative to others, and they influence basic patterns of communication like the choice of one's interlocutor in conversation.", "labels": [], "entities": []}, {"text": "Tested on an information-seeking scenario, where natural language questions must be answered by addressing the most relevant individuals in a community , our system outperforms a standard information retrieval algorithm by a considerable margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional Semantics (DS)) is an approach to computational semantics which has historical roots in the philosophical work of Wittgenstein, and in particular in the claim that 'meaning is use', i.e. words acquire a semantics which is a function of the contexts in which they are used.", "labels": [], "entities": [{"text": "Distributional Semantics (DS))", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8059437692165374}]}, {"text": "The technique has been used in psycholinguistics to model various phenomena, from priming to similarity judgements, and even aspects of language acquisition.", "labels": [], "entities": [{"text": "similarity judgements", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.6467436105012894}, {"text": "language acquisition", "start_pos": 136, "end_pos": 156, "type": "TASK", "confidence": 0.710121214389801}]}, {"text": "The general idea is that an individual speaker develops the verbal side of his or her conceptual apparatus from the linguistic experiences he or she is exposed to, together with the perceptual situations surrounding those experiences.", "labels": [], "entities": []}, {"text": "One natural consequence of the distributional claim is that meaning is both speaker-dependent and community-bound.", "labels": [], "entities": []}, {"text": "On the one hand, depending on who they are, speakers will be exposed to different linguistic and perceptual experiences, and by extension develop separate vocabularies and conceptual representations.", "labels": [], "entities": []}, {"text": "For instance, a chef and a fisherman may have different representations of the word fish.", "labels": [], "entities": []}, {"text": "On the other hand, the vocabularies and conceptual representations of individual people should be close enough that they can successfully communicate: this is ensured by the fact that many linguistic utterances are shared amongst a community.", "labels": [], "entities": []}, {"text": "There is a counterpart to the claim that 'language is speaker-dependent': speakers are language-dependent.", "labels": [], "entities": []}, {"text": "That is, the type of person someone is can be correlated with their linguistic experience.", "labels": [], "entities": []}, {"text": "For instance, the fact that fish and boil are often seen in the linguistic environment of an individual may indicate that this individual has much to do with cooking (contrast with high co-occurrences offish and net).", "labels": [], "entities": []}, {"text": "In some contexts, linguistic data might even be the only source of information we have about a person: in an academic context, we often infer from the papers a person has written and cited which kind of expertise they might have.", "labels": [], "entities": []}, {"text": "This paper offers a model of individuals based on (a subset of) their linguistic experience.", "labels": [], "entities": []}, {"text": "That is, we model how, by being associated with particular types of language data, people develop a uniqueness representable as a vector in a semantic space.", "labels": [], "entities": []}, {"text": "Further, we evaluate those 'person vectors' along one particular dimension: the type of knowledge we expect them to hold.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first give a short introduction to the topic of modelling linguistic individuality ( \u00a72) and we discuss how DS is a suitable tool to represent the associated characteristics fora given person ( \u00a73).", "labels": [], "entities": []}, {"text": "We describe a model of individuals in a community using 'person vectors' ( \u00a74).", "labels": [], "entities": []}, {"text": "We then highlight the challenges associated with evaluating such vectors, and propose a prediction task which has for goal to identify someone with a particular expertise, given a certain information need ( \u00a75, \u00a76).", "labels": [], "entities": []}, {"text": "Concretely, we model a community of over 4000 individuals from their linguistic interaction with Wikipedia ( \u00a77).", "labels": [], "entities": []}, {"text": "We finally evaluate our model on the suggested task and compare results against a standard information retrieval algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "We note that the task we propose can be seen as an information retrieval (IR) problem over a dis-tributed network: a query is matched to some relevant knowledge unit, with all available knowledge being split across a number of 'peers' (the individuals in our community).", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.8414680004119873}]}, {"text": "So in order to know how well the system does at retrieving relevant information, we can use as benchmark standard IR software.", "labels": [], "entities": [{"text": "IR", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.9331005215644836}]}, {"text": "We compare the performance of our system with a classic, centralised IR algorithm, as implemented in the Apache Lucene search engine.", "labels": [], "entities": [{"text": "Apache Lucene search engine", "start_pos": 105, "end_pos": 132, "type": "DATASET", "confidence": 0.6433233693242073}]}, {"text": "Lucene is an open source library for implementing (unstructured) document retrieval systems, which has been employed in many full-text search engine systems (for an overview of the library, see).", "labels": [], "entities": [{"text": "Lucene", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.94488525390625}, {"text": "document retrieval", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.6778656393289566}]}, {"text": "We use the out-of-the-box 'standard' indexing solution provided by Lucene, 2 which roughly implements a term-by-document Vector Space Model, in which terms are lemmatised and associated to documents using their tf-idf scores) computed from the input Wikipedia corpus of our evaluation.", "labels": [], "entities": []}, {"text": "Similarly, queries are parsed using Lucene's standard query parser and then searched and ranked by the computed 'default' similarities.", "labels": [], "entities": []}, {"text": "Our hypothesis is that, if our system can match the performance of a well-known IR system, we can also conclude that the person vectors were a good summary of the information held by a particular agent.", "labels": [], "entities": []}, {"text": "The WikiQA dataset gives us information about the document d gold that was clicked on by users after issuing a particular query q.", "labels": [], "entities": [{"text": "WikiQA dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9732835590839386}]}, {"text": "This indicates that d gold was relevant for q, but does not give us information about which other documents might have also be deemed relevant by the user.", "labels": [], "entities": []}, {"text": "In this respect, the dataset differs from fully annotated IR collections like the TREC data.", "labels": [], "entities": [{"text": "TREC data", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.856726884841919}]}, {"text": "In what follows, we report Mean Reciprocal Rank (MRR), which takes into account that only one document per query is considered relevant in our dataset: where Q is the set of all queries, and P (q) is the precision of the system for query q.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 27, "end_pos": 53, "type": "METRIC", "confidence": 0.9638746778170267}, {"text": "precision", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.9985682964324951}]}, {"text": "P (q) itself is 185 given by: where r q is the rank at which the correct document is returned for query q, and the cutoff is a predefined number of considered results (e.g., top 20 documents).", "labels": [], "entities": []}, {"text": "The MRR scores for Lucene and our system are shown in.", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7312031388282776}, {"text": "Lucene", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.7264493703842163}]}, {"text": "The x-axis shows different cutoff points (e.g., cut-off point 10 means that we are only considering the top 10 documents returned by the system).", "labels": [], "entities": []}, {"text": "The graph gives results for the case where the agent contacts the p = 5 people potentially most relevant for the query.", "labels": [], "entities": []}, {"text": "We also tried m = {10, 20, 50} and found that end results are fairly stable, despite the fact that the chance of retrieving at least one 'useful' agent increases.", "labels": [], "entities": []}, {"text": "This is due to the fact that, as people are added to the first phase of querying, confusion increases (more documents are inspected) and the system is more likely to return the correct page at a slightly lower rank (e.g., as witnessed by the performance of Lucene's centralised indexing mechanism).", "labels": [], "entities": [{"text": "confusion", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9734679460525513}, {"text": "Lucene's centralised indexing", "start_pos": 257, "end_pos": 286, "type": "TASK", "confidence": 0.5584811046719551}]}, {"text": "Our hypothesis was that matching the performance of an IR algorithm would validate our model as a useful representation of a community.", "labels": [], "entities": []}, {"text": "We find, in fact, that our method considerably outperforms Lucene, reaching M RR = 0.31 form = 5 against M RR = 0.22.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.9278272986412048}, {"text": "M RR = 0.31 form", "start_pos": 76, "end_pos": 92, "type": "METRIC", "confidence": 0.9047616839408874}, {"text": "M RR", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.6811257004737854}]}, {"text": "This is a very interesting result, as it suggests that retaining the natural relationship between information and knowledge holders increases the ability of the system to retrieve it, and this, despite the intrinsic difficulty of searching in a distributed setting.", "labels": [], "entities": []}, {"text": "This is especially promising, as the implementation presented here is given in its purest form, without heavy pre-processing or parameter setting.", "labels": [], "entities": []}, {"text": "Aside from a shortlist of common stopwords, the agent only uses simple linear algebra operations over raw, non-lemmatised data.", "labels": [], "entities": []}, {"text": "M RR figures are not necessarily very intuitive, so we inspect how many times an agent is found who can answer the query (i.e. its memory store contains the document that was marked as holding the answer to the query in WikiQA).", "labels": [], "entities": []}, {"text": "We find that the system finds a helpful hand 39% of the time form = 5 and 52% at m = 50.", "labels": [], "entities": []}, {"text": "These relatively modest figures demonstrate the difficulty of our task and dataset.", "labels": [], "entities": []}, {"text": "We must however also acknowledge that finding appropriate helpers amongst a community of 4000 individuals is highly non-trivial.", "labels": [], "entities": []}, {"text": "Overall, the system is very precise once a good agent has been identified (i.e., it is likely to return the correct document in the first few results).", "labels": [], "entities": []}, {"text": "This is shown by the fact that the M RR only increases slightly between cut-off point 1 and 20, from 0.29 to 0.31 (compare with Lucene, which achieves M RR = 0.02 at rank 1).", "labels": [], "entities": [{"text": "M RR", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9196410179138184}, {"text": "Lucene", "start_pos": 128, "end_pos": 134, "type": "DATASET", "confidence": 0.9424691796302795}, {"text": "M RR", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9534828364849091}]}, {"text": "This behaviour can be explained by the fact that the agent overwhelmingly prefers 'small' memory sizes: 78% of the agents selected in the first phase of the querying process contain less than 100 documents.", "labels": [], "entities": []}, {"text": "This is an important aspect which should guide further modelling.", "labels": [], "entities": []}, {"text": "We hypothesise that people with larger memory stores are perhaps less attractive to the querying agent because their profiles are less topically defined (i.e., as the number of documents browsed by a user increases, it is more likely that they cover a wider range of topics).", "labels": [], "entities": []}, {"text": "As pointed out in \u00a74, we suggest that our person representations may need more structure, perhaps in the form of several coherent 'topic vectors'.", "labels": [], "entities": []}, {"text": "It makes intuitive sense to assume that a) the interests of a person are not necessarily close to each other (e.g. someone maybe a linguist and a hobby gardener); b) when a person with an information need selects 'who can help' amongst their acquaintances, they only consider the relevant aspects of an individual (e.g., the hobby gardener is a good match fora query on gardening, irrespectively of their other persona as a linguist).", "labels": [], "entities": []}, {"text": "Finally, we note that all figures reported here are below their true value (including those pertaining to Lucene).", "labels": [], "entities": [{"text": "Lucene", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.5314965844154358}]}, {"text": "This is because we attempt to retrieve the page labelled as containing the answer to the query in the WikiQA dataset.", "labels": [], "entities": [{"text": "WikiQA dataset", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.9791786372661591}]}, {"text": "Pages which are relevant but not contained in WikiQA are incorrectly given a score of 0.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.9193834066390991}]}, {"text": "For instance, the query what classes are considered humanities returns Outline of the humanities as the first answer, but the chosen document in WikiQA is Humanities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of documents across peo- ple. For example, 2939 agents contain 1-100 doc- uments.", "labels": [], "entities": []}, {"text": " Table 2: Redundancy of relevant documents  across people. For example, 176 documents are  found in one agent; 169 documents are found in  2-4 agents, etc.", "labels": [], "entities": []}]}