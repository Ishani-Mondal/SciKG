{"title": [{"text": "NaCTeM at SemEval-2016 Task 1: Inferring sentence-level semantic similarity from an ensemble of complementary lexical and sentence-level features", "labels": [], "entities": [{"text": "Inferring sentence-level semantic similarity", "start_pos": 31, "end_pos": 75, "type": "TASK", "confidence": 0.6767404899001122}]}], "abstractContent": [{"text": "We present a description of the system submitted to the Semantic Textual Similarity (STS) shared task at SemEval 2016.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) shared task at SemEval 2016", "start_pos": 56, "end_pos": 117, "type": "TASK", "confidence": 0.8386668454517018}]}, {"text": "The task is to assess the degree to which two sentences carry the same meaning.", "labels": [], "entities": []}, {"text": "We have designed two different methods to automatically compute a similarity score between sentences.", "labels": [], "entities": []}, {"text": "The first method combines a variety of semantic similarity measures as features in a machine learning model.", "labels": [], "entities": []}, {"text": "In our second approach, we employ training data from the Interpretable Similarity subtask to create a combined word-similarity measure and assess the importance of both aligned and unaligned words.", "labels": [], "entities": []}, {"text": "Finally, we combine the two methods into a single hybrid model.", "labels": [], "entities": []}, {"text": "Our best-performing run attains a score of 0.7732 on the 2015 STS evaluation data and 0.7488 on the 2016 STS evaluation data.", "labels": [], "entities": [{"text": "STS evaluation data", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.8188737829526266}, {"text": "STS evaluation data", "start_pos": 105, "end_pos": 124, "type": "DATASET", "confidence": 0.855255663394928}]}], "introductionContent": [{"text": "If you ask a computer if two sentences are identical, it will return an accurate decision in a split-second.", "labels": [], "entities": []}, {"text": "Ask it to do the same fora million sentence pairs and it will take a few seconds -far quicker than any human.", "labels": [], "entities": []}, {"text": "But similarity has many dimensions.", "labels": [], "entities": []}, {"text": "Ask a computer if two sentences mean the same and it will stall, yet the human can answer instantly.", "labels": [], "entities": []}, {"text": "Now we have the edge.", "labels": [], "entities": []}, {"text": "But what metrics do we use in our personal cognitive similarity-scoring systems?", "labels": [], "entities": []}, {"text": "The answer is at the heart of the semantic similarity task.", "labels": [], "entities": [{"text": "semantic similarity task", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.8094178040822347}]}, {"text": "In our solution, we have incorporated several categories of features from a variety of sources.", "labels": [], "entities": []}, {"text": "Our approach covers both low-level visual features such as length and edit-distance as well as high-level semantic features such as topic-models and alignment quality measures.", "labels": [], "entities": []}, {"text": "Throughout our approach, we have found it easier to consider similarity at the lexical level.", "labels": [], "entities": []}, {"text": "This is unsurprising, as each lexeme maybe seen as a semantic unit with the sentence's meaning built directly from a specific combination of lexemes.", "labels": [], "entities": []}, {"text": "We have explored several methods for abstracting our lexicallevel features to the sentence level as explained in Section 4.", "labels": [], "entities": []}, {"text": "Our methods are built from both intuitive features as well as the results of error-analyses on our trial runs.", "labels": [], "entities": []}, {"text": "We have combined multiple feature sources to build a powerful semantic-similarity classifier, discarding non-performing features as necessary.", "labels": [], "entities": []}], "datasetContent": [{"text": "As explained in previous sections, we have used three features sets to create three classification models, called Macro (from aggregation-based features), Micro (from alignment-based features) and Hybrid (including both feature sets).", "labels": [], "entities": []}, {"text": "According to the shared task guidelines, the performance has been measured by computing a correlation between predicted and gold standard (assigned by humans) scores in each data set, and then obtaining their average.", "labels": [], "entities": []}, {"text": "We have performed two main experiments.", "labels": [], "entities": []}, {"text": "Firstly, we have used the data that have been available for training and testing in the 2015 competition ( to select the best approach.", "labels": [], "entities": []}, {"text": "Then, we have created three final models on all available 2015 data and used them to label the test data provided by the organisers of the 2016 task.", "labels": [], "entities": []}, {"text": "shows the results of both experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Ten most useful variables from the aggregation ap-", "labels": [], "entities": []}, {"text": " Table 2: Ten most useful variables from the alignment approach", "labels": [], "entities": []}, {"text": " Table 3: Correlations of predicted and actual similarity scores", "labels": [], "entities": [{"text": "actual similarity scores", "start_pos": 40, "end_pos": 64, "type": "METRIC", "confidence": 0.663254976272583}]}]}