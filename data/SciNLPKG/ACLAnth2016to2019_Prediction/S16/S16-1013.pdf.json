{"title": [{"text": "thecerealkiller at SemEval-2016 Task 4: Deep Learning based System for Classifying Sentiment of Tweets on Two Point Scale", "labels": [], "entities": [{"text": "Classifying Sentiment of Tweets", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.9080939441919327}]}], "abstractContent": [{"text": "In this paper, we propose a deep learning system for classification of tweets on a two-point scale.", "labels": [], "entities": [{"text": "classification of tweets", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.8751747012138367}]}, {"text": "Our architecture consists of a multilay-ered recurrent neural network having gated recurrent units.", "labels": [], "entities": []}, {"text": "The network is pre-trained with a weakly labeled dataset of tweets to learn the sentiment specific embeddings.", "labels": [], "entities": []}, {"text": "Then it is fine tuned on the given training dataset of the task 4B in SemEval-2016.", "labels": [], "entities": []}, {"text": "The network does very little pre-processing for raw tweets and no post-processing at all.", "labels": [], "entities": []}, {"text": "The proposed system achieves 3rd rank on the leaderboard of task 4B.", "labels": [], "entities": [{"text": "3rd rank", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.8636382818222046}]}], "introductionContent": [{"text": "Task 4 of) -Sentiment Analysis in Twitter-turned out to be the most popular task of SemEval-2016.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter-turned", "start_pos": 12, "end_pos": 48, "type": "TASK", "confidence": 0.9083804786205292}]}, {"text": "Among its subtasks, sub-task B -Tweet classification according to a two-point scale -was the 2nd most popular subtask.", "labels": [], "entities": []}, {"text": "A total of 19 teams participated in it including ours.", "labels": [], "entities": []}, {"text": "In this paper, we propose a multilayerd RNN architecture for classifying tweets on a two-point scale, namely, positive and negative.", "labels": [], "entities": []}, {"text": "We first pretrain the network with a weakly labeled corpus of tweets, where labels are assigned based on the sentiment of the emoticon present in the tweets.", "labels": [], "entities": []}, {"text": "It helps the network to learn sentiment specific embeddings of the words in the tweet.", "labels": [], "entities": []}, {"text": "The network is then finetuned on the dataset of tweets provided as part of the sub-task 4B along with the training and development dataset of SemEval-2013 task 2.", "labels": [], "entities": []}, {"text": "In the second section, we describe our architecture.", "labels": [], "entities": []}, {"text": "In third section, we explain our approach to train the network.", "labels": [], "entities": []}, {"text": "After that, we describe the experimental setup and statistical properties of the data used.", "labels": [], "entities": []}, {"text": "In the end, we discuss our results on the test dataset.", "labels": [], "entities": [{"text": "test dataset", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.7827799916267395}]}, {"text": "We stood 3rd on the final leaderboard for sub-task 4B.", "labels": [], "entities": []}], "datasetContent": [{"text": "The statistical properties of training and testing datasets are provided in.", "labels": [], "entities": []}, {"text": "For evaluation, we use official scoring metric of Semeval-2016 task 4B -macro-averaged recallaverage of recalls for both positive and negative classes.", "labels": [], "entities": [{"text": "recallaverage of recalls", "start_pos": 87, "end_pos": 111, "type": "METRIC", "confidence": 0.8850263754526774}]}, {"text": "The chosen parameters of our network are as follows: the maximum input sequence length is set to 30, vocabulary size is 400000, dimensionality of word embedding (d) is 100, recurrent units hidden state vector size is 128, number of recurrent layers is 3, number of hidden unit in dense layer is 256 with relu activation.", "labels": [], "entities": []}, {"text": "We used a dropout of 50% after each layer while training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistical information of training and testing datasets.", "labels": [], "entities": []}, {"text": " Table 2: Resulting scores on testing dataset. AvgR was the", "labels": [], "entities": [{"text": "AvgR", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9541354179382324}]}]}