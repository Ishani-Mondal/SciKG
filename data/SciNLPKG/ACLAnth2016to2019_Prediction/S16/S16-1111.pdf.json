{"title": [{"text": "NORMAS at SemEval-2016 Task 1: SEMSIM: A Multi-Feature Approach to Semantic Text Similarity", "labels": [], "entities": [{"text": "Semantic Text Similarity", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6221882998943329}]}], "abstractContent": [{"text": "This paper presents the submission of our team (NORMAS) to the SemEval 2016 semantic textual similarity (STS) shared task.", "labels": [], "entities": [{"text": "SemEval 2016 semantic textual similarity (STS) shared task", "start_pos": 63, "end_pos": 121, "type": "TASK", "confidence": 0.8756064414978028}]}, {"text": "We submitted three system runs, each using a set of 36 features extracted from the training set.", "labels": [], "entities": []}, {"text": "The runs explore the use of the following three machine learning algorithms: Support Vector Regression, Elastic Net and Random Forest.", "labels": [], "entities": []}, {"text": "Each run was trained using sentence pairs from the STS 2012 training data.", "labels": [], "entities": [{"text": "STS 2012 training data", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.865157961845398}]}, {"text": "Features extracted include lexical, syntactic and semantic features.", "labels": [], "entities": []}, {"text": "This paper describes the features we designed for assessing the semantic similarity between sentence pairs, the models we build using these features and the performance obtained by the resulting systems on the 2016 evaluation data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computationally assessing the semantic similarity of natural language data has gained the attention of researchers in the field of computational linguistics.", "labels": [], "entities": []}, {"text": "Machines that are able to quantify the semantics of natural language have numerous important applications including: Document Similarity (, Word Similarity), Text Summarization (), Informtion Retrieval (, Plagiarism detection (), Paraphrase detection and especially Machine Translation ().", "labels": [], "entities": [{"text": "Document Similarity", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7770693004131317}, {"text": "Text Summarization", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.8025258779525757}, {"text": "Informtion Retrieval", "start_pos": 181, "end_pos": 201, "type": "TASK", "confidence": 0.8622457385063171}, {"text": "Plagiarism detection", "start_pos": 205, "end_pos": 225, "type": "TASK", "confidence": 0.7905018925666809}, {"text": "Paraphrase detection", "start_pos": 230, "end_pos": 250, "type": "TASK", "confidence": 0.9760661125183105}, {"text": "Machine Translation", "start_pos": 266, "end_pos": 285, "type": "TASK", "confidence": 0.7915923893451691}]}, {"text": "The semantic textual similarity (STS)) task measures the level of semantic equivalence between two approximately sentence sized snippets of texts on a graded scale from 0 (unrelated) to 5 (completely equivalent).", "labels": [], "entities": [{"text": "semantic textual similarity (STS))", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.6221885879834493}]}, {"text": "This paper describes our participation in the SemEval 2016 STS shared task (.", "labels": [], "entities": [{"text": "SemEval 2016 STS shared task", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.8942393302917481}]}, {"text": "Our system explores 36 lexical and semantic features (e.g., string matching, WordNet similarity, word overlap) in combination with three very distinct learning algorithms (Support Vector Regression, Elastic Net and Random forest).", "labels": [], "entities": [{"text": "string matching", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.7126061767339706}]}, {"text": "The remainder of this paper is organized as follows: Section 2 describes our feature set in detail and our approach to feature selection.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.8032126128673553}]}, {"text": "Section 3 describes our machine learning models with section 4 presenting our results on the shared task evaluation data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted two experiments, Pre-Submission and Post-Submission experiments.", "labels": [], "entities": []}, {"text": "summarizes the result of the Runs submitted across the datasets for the first experiment (Pre-Submission).", "labels": [], "entities": []}, {"text": "The scores in bold shows the best scores per dataset.", "labels": [], "entities": []}, {"text": "The scores with asterisk (*) appended shows the best scores under each Run.", "labels": [], "entities": []}, {"text": "We used the preliminary result of the 2016 STS task released by the organizers as the baseline for evaluation.", "labels": [], "entities": []}, {"text": "The preliminary result includes the median scores (Baseline-Median) and best scores (Baseline-Best) of the participating systems on each dataset.", "labels": [], "entities": []}, {"text": "The Baseline Best is the score of the top performing system for each dataset of the Semeval 2016 task(.", "labels": [], "entities": [{"text": "Semeval 2016 task", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.521716296672821}]}, {"text": "It can be seen from that our best scores were from #Run2 and #Run3.", "labels": [], "entities": [{"text": "Run2", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.9361274242401123}, {"text": "Run3", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9106451272964478}]}, {"text": "The Random Forest algorithm performed poorly compared to the other two Runs on all of the datasets.", "labels": [], "entities": []}, {"text": "All three Runs performed best on the Post-Editing dataset.", "labels": [], "entities": [{"text": "Post-Editing dataset", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.6978132724761963}]}, {"text": "This conforms to the pattern observed from the Baseline For SVR, we used the LibSvm scikit implementation with RBF kernel.", "labels": [], "entities": []}, {"text": "We set C=1.0, epsilon=0.2 and cache size=200.", "labels": [], "entities": [{"text": "epsilon", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9595327973365784}]}, {"text": "For Elastic Net we used alpha=0.5.", "labels": [], "entities": []}, {"text": "Random Forest, we used 100 trees, max depth=None and max leaf nodes=None.", "labels": [], "entities": [{"text": "Random Forest", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8979540467262268}]}, {"text": "Grid search was used for hyperparameter optimization.", "labels": [], "entities": [{"text": "Grid search", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6293662786483765}, {"text": "hyperparameter optimization", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.8484334647655487}]}, {"text": "8 http://scikit-learn.org/ scores.", "labels": [], "entities": []}, {"text": "Analysis of this dataset revealed that the sentences are longer and share more words.", "labels": [], "entities": []}, {"text": "Our worst performance across board is on the Answer-Answer dataset.", "labels": [], "entities": [{"text": "Answer-Answer dataset", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.8092826008796692}]}, {"text": "Inspecting the data reveals that the sentences are both short and tend to share words that have little or no impact on their overall meaning.", "labels": [], "entities": []}, {"text": "Our models may have been misguided by these spurious matching words.", "labels": [], "entities": []}, {"text": "Overall, our best systems, #Run2 and #Run3, have results very close to the Baseline-Best on three datasets and outclassed Baseline-Median on Question-question dataset.", "labels": [], "entities": [{"text": "Question-question dataset", "start_pos": 141, "end_pos": 166, "type": "DATASET", "confidence": 0.6386993527412415}]}, {"text": "Our system performance may have been handicapped by the limited amount of data we used for training.", "labels": [], "entities": []}, {"text": "Recall that we trained our system on only 2234 sentences of the 2012 training data.", "labels": [], "entities": []}, {"text": "The tiny size of the dataset was necessitated by the complexity of computing some of the semantic features.", "labels": [], "entities": []}, {"text": "Specifically, the WordNet similarity features used.", "labels": [], "entities": []}, {"text": "Also, analysis of our training data shows that the sentences are of few words (short) and with high term overlap.", "labels": [], "entities": []}, {"text": "Perhaps, our system could have performed better with more training data, especially if we had used a dataset with long sentences and also included more training data from previous STS evaluation tasks.", "labels": [], "entities": []}, {"text": "After the official evaluation, we reproduced our experiment using a larger training set.", "labels": [], "entities": []}, {"text": "The new training data contains 9902 sentences drawn from the 2012-2014 evaluation datasets.", "labels": [], "entities": []}, {"text": "Using more training data resulted in a notable improvement in performance.", "labels": [], "entities": []}, {"text": "The result from our top performing Runs across all datasets clearly surpass the BaselineMedian results.", "labels": [], "entities": []}, {"text": "The top scores for each dataset were also very close to the Baseline-Best results.", "labels": [], "entities": []}, {"text": "Our overall Mean of 0.711 surpasses the overall Mean of the Baseline-Median (0.683) and is far better than the 0.596 obtained by our submitted systems.", "labels": [], "entities": [{"text": "Mean", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.984119713306427}, {"text": "Mean", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9713546633720398}]}], "tableCaptions": [{"text": " Table 1: Summary of Pearson Correlation Evaluation Of The Submitted System", "labels": [], "entities": []}, {"text": " Table 2: Post-Submission Experiment With More Training Data (9902 sentence pairs)", "labels": [], "entities": []}]}