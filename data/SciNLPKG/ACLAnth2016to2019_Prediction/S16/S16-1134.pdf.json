{"title": [{"text": "QU-IR at SemEval 2016 Task 3: Learning to Rank on Arabic Community Question Answering Forums with Word Embedding", "labels": [], "entities": [{"text": "SemEval 2016 Task", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.5678284664948782}, {"text": "Arabic Community Question Answering Forums", "start_pos": 50, "end_pos": 92, "type": "TASK", "confidence": 0.7622508525848388}]}], "abstractContent": [{"text": "Resorting to community question answering (CQA) websites for finding answers has gained momentum in the past decade with the explosive rate at which social media has been proliferating.", "labels": [], "entities": [{"text": "community question answering (CQA) websites", "start_pos": 13, "end_pos": 56, "type": "TASK", "confidence": 0.836311800139291}]}, {"text": "With many questions left unanswered on those websites, automatic and smart question answering systems have seen light.", "labels": [], "entities": [{"text": "question answering", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7084180563688278}]}, {"text": "One of the main objectives of such systems is to harness the plethora of existing answered questions; hence transforming the problem to finding good answers to newly posed questions from similar previously-answered ones.", "labels": [], "entities": []}, {"text": "As SemEval 2016 Task 3 \"Community Question Answer-ing\" has focused on this problem, we have participated in the Arabic Subtask.", "labels": [], "entities": [{"text": "SemEval 2016 Task 3 \"Community Question Answer-ing", "start_pos": 3, "end_pos": 53, "type": "TASK", "confidence": 0.5829134434461594}, {"text": "Arabic Subtask", "start_pos": 112, "end_pos": 126, "type": "DATASET", "confidence": 0.8931668996810913}]}, {"text": "Our system has adopted a supervised learning approach in which a learning-to-rank model is trained over data (questions and answers) extracted from Arabic CQA forums using word2vec features generated from that data.", "labels": [], "entities": []}, {"text": "Our primary submission achieved a 29.7% improvement over the MAP score of the baseline.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9597262144088745}]}, {"text": "Post submission experiments were further conducted to integrate variations of the word2vec features to our system.", "labels": [], "entities": []}, {"text": "Integrating covariance word embedding features has raised the the improvement over the baseline to 37.9%.", "labels": [], "entities": [{"text": "the", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9781500697135925}]}], "introductionContent": [{"text": "The ubiquitous presence of community question answering (CQA) websites has motivated research in the direction of building automatic question answering (QA) systems that can benefit from previously-answered questions to answer newlyposed ones.", "labels": [], "entities": [{"text": "community question answering (CQA)", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.8246047099431356}, {"text": "question answering (QA)", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.8437693238258361}]}, {"text": "A core functionality of such systems is their ability to effectively rank previously-suggested answers with respect to their degree/probability of relevance to a posted question.", "labels": [], "entities": []}, {"text": "The ranking functionality is vital to push away irrelevant and low quality answers, which is commonplace in CQA as they are generally open with no restrictions on who can post or answer questions.", "labels": [], "entities": [{"text": "CQA", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.9175954461097717}]}, {"text": "To this effect, SemEval 2016 Task 3 \"Community Question Answering\" has emphasized the ranking component in the main task of the challenge.", "labels": [], "entities": [{"text": "SemEval 2016 Task 3", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7623265385627747}, {"text": "Community Question Answering", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.6032523214817047}]}, {"text": "We have participated in Task 3-Subtask D (Arabic Subtask) which is confined to the main task of ranking answers; given anew question and a set of 30 question-answer pairs (QApairs) retrieved by a search engine, re-rank those QApairs by their degree/probability of relevance to the new question.", "labels": [], "entities": []}, {"text": "shows an example of a question and four of its 30 given candidate question-answer pairs.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the experimental setup and results of our primary, contrastive-1 and contrastive-2 submissions, in addition to our post-submission experiments.", "labels": [], "entities": []}, {"text": "We have used the Arabic collection of questions and their potentially related question-answer pairs provided by Task3 organizers for training our models.", "labels": [], "entities": []}, {"text": "We evaluated those models using the development dataset of 7,355 question-QApairs instead of the full provided dataset of 7,385 question-QApairs; one question (out of the 250) and its potentially related 30 QApairs were not properly formed and thus excluded.", "labels": [], "entities": []}, {"text": "Another data preprocessing step was to parse and transform the XML files into flat files for easier data processing/tracking of question-answers.", "labels": [], "entities": [{"text": "data processing/tracking of question-answers", "start_pos": 100, "end_pos": 144, "type": "TASK", "confidence": 0.7929332653681437}]}, {"text": "The Gensim 4 tool was used to generate the word2vec model from training data . We used the learned model to compute our features as described in section 2.2.", "labels": [], "entities": []}, {"text": "Features were generated for the three data setups described in section 2.1.", "labels": [], "entities": []}, {"text": "RankLib 6 was used to create and evaluate our learning-to-rank models.", "labels": [], "entities": [{"text": "RankLib 6", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8910343647003174}]}, {"text": "Although we have experimented with a number of pairwise and listwise learning-to-rank algorithms, we adopted pointwise L2Rank algorithms in our submissions as they exhibited a relatively better performance than the other two categories.", "labels": [], "entities": []}, {"text": "Since MAP (Mean Average Precision) is the official evaluation measure to evaluate Task 3-Subtask D submissions, we focused our experiments and evaluations to optimize this measure.", "labels": [], "entities": [{"text": "MAP (Mean Average Precision)", "start_pos": 6, "end_pos": 34, "type": "METRIC", "confidence": 0.8582594891389211}]}, {"text": "Time constraints have withheld us from optimizing the other evaluation measures that are also adopted by the task's official scorer, such as F1 measure, accuracy, etc.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 141, "end_pos": 151, "type": "METRIC", "confidence": 0.979138970375061}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9984973669052124}]}, {"text": "Further experiments were conducted to explore the performance of Covariance Word Embedding (CovWE) and unigram features as compared to the Average Word Embedding (AvgWE) features.", "labels": [], "entities": []}, {"text": "In, we report the MAP scores achieved by these features using a dimensionality of 50 and 100, respectively, for representing the vectors of word embeddings.", "labels": [], "entities": [{"text": "MAP", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.6068426966667175}]}, {"text": "In our post-submission experiments, we only extracted features using the QQA data setup.", "labels": [], "entities": [{"text": "QQA data setup", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.9413424134254456}]}, {"text": "As such, we only include the results of our primary and contrastive-2 submissions in because their features were also extracted using the QQA data setup, unlike the contrastive-1 submission.", "labels": [], "entities": [{"text": "QQA data setup", "start_pos": 138, "end_pos": 152, "type": "DATASET", "confidence": 0.9317606290181478}]}, {"text": "Normalization MAP  In most of the experiments reported in, the MART algorithm was used for training the L2Rank models; whereas, for the AvgWE-100 and Unigrams experiment, the Random Forests algorithm was used.", "labels": [], "entities": [{"text": "Normalization MAP", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.6899586021900177}, {"text": "MART", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9267626404762268}, {"text": "AvgWE-100", "start_pos": 136, "end_pos": 145, "type": "DATASET", "confidence": 0.8771529793739319}]}, {"text": "In general, the MART algorithm performed better in the majority of our experiments that we have conducted but have not reported.", "labels": [], "entities": [{"text": "MART", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.6747432947158813}]}, {"text": "The main observations worth mentioning regarding the experiments in are: \u2022 Using tf.idf weighted uni-grams of the most frequent 5000 words along with word2vec features (AvgWE and CovWE) did not mark an improvement over using word2vec features solely.", "labels": [], "entities": []}, {"text": "\u2022 Normalization of feature values seem to have a tendency of enhancing the achieved MAP scores when applied.", "labels": [], "entities": [{"text": "MAP", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9321151971817017}]}, {"text": "For this reason, we include in the normalization scheme (if any) that was adopted in each experiment.", "labels": [], "entities": []}, {"text": "\u2022 The discriminant potential of the covariance word embedding features seem to be relatively stronger than that of average word embedding features.", "labels": [], "entities": []}, {"text": "For example, the features CovWE-50 and CovWE-100 have achieved relatively higher MAP scores than AvgWE-50 and AvgWE-100, respectively.", "labels": [], "entities": [{"text": "CovWE-50", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.8905354738235474}, {"text": "MAP scores", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9708147943019867}]}, {"text": "With their 41.07 and 40.68 MAP scores, CovWE features have achieved an improvement of about 37.9% over the baseline.", "labels": [], "entities": [{"text": "MAP", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9889324903488159}]}, {"text": "AvgWE-100 and AvgWE-50 followed with the MAP scores of 38.63 (primary submission score) and 36.01, respectively; hence, attaining lower improvements (29.7% and 20.9%) over the baseline.", "labels": [], "entities": [{"text": "AvgWE-100", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.6403052806854248}, {"text": "AvgWE-50", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.7092365026473999}, {"text": "MAP", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9896401166915894}, {"text": "primary submission score)", "start_pos": 62, "end_pos": 87, "type": "METRIC", "confidence": 0.7124903574585915}]}, {"text": "\u2022 The covariance word embedding features CovWE-50 and CovWE-100 have attained comparable MAP scores of 41.07 and 40.68, respectively.", "labels": [], "entities": [{"text": "MAP", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.98272705078125}]}, {"text": "Interestingly, the CovWE-50 experiment consumed 44.5 minutes to learn the L2Rank model, while the CovWE-100 experiment consumed 5.27 hours.", "labels": [], "entities": []}, {"text": "This finding is also suggesting that covariance word embedding features seem to have a relatively higher discriminating potential even with lower dimensions.", "labels": [], "entities": []}, {"text": "More rigorous benchmarking experiments might be needed to further verify the merit of the above implications.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The official MAP scores attained by our primary and", "labels": [], "entities": [{"text": "MAP", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.6268317699432373}]}]}