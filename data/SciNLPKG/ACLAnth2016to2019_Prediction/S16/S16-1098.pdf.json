{"title": [{"text": "ISCAS_NLP at SemEval-2016 Task 1: Sentence Similarity Based on Support Vector Regression using Multiple Features", "labels": [], "entities": [{"text": "ISCAS_NLP", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.6835781733194987}, {"text": "Sentence Similarity", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.9676257073879242}]}], "abstractContent": [{"text": "This paper describes our system developed for English Monolingual subtask (STS Core) of SemEval-2016 Task 1: \"Semantic Textual Similarity: A Unified Framework for Semantic Processing and Evaluation\".", "labels": [], "entities": [{"text": "English Monolingual subtask (STS Core) of SemEval-2016 Task 1", "start_pos": 46, "end_pos": 107, "type": "DATASET", "confidence": 0.6814962327480316}, {"text": "Semantic Textual Similarity", "start_pos": 110, "end_pos": 137, "type": "TASK", "confidence": 0.7252846956253052}]}, {"text": "We measure the similarity between two sentences using three different types of features, including word alignment-based similarity, sentence vector-based similarity and sentence constituent similarity.", "labels": [], "entities": []}, {"text": "The best performance of our submitted runs is a mean 0.69996 Pearson correlation which outperforms the median score from all participating systems.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 61, "end_pos": 80, "type": "METRIC", "confidence": 0.9425316452980042}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence of a sentence pair (.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8096966197093328}]}, {"text": "STS was first held in SemEval 2012 and has drawn considerable attention in recent years.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8064098954200745}, {"text": "SemEval 2012", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.6950811147689819}]}, {"text": "STS has been widely used in a lot of natural language processing tasks such as information retrieval, machine translation, question answering, text summarization, and soon.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7974657118320465}, {"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7913138270378113}, {"text": "question answering", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.8963192105293274}, {"text": "text summarization", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.7636300623416901}]}, {"text": "Previous methods for this task could be roughly divided into three categories: alignment approaches, vector space approaches and machine learning approaches.", "labels": [], "entities": []}, {"text": "Alignment approaches align words or phrases in a sentence pair, and then take the quality or coverage of alignments as similarity measure ).", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9677414298057556}]}, {"text": "Vector space approaches represent sentences as bag-ofwords vectors and take vector similarity as their similarity measure ().", "labels": [], "entities": []}, {"text": "Machine learning approaches combine different similarity measures and features using supervised machine learning models ().", "labels": [], "entities": []}, {"text": "In our system, we measure semantic text similarity by combining evidence from all above three categories.", "labels": [], "entities": [{"text": "semantic text similarity", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.7487676938374838}]}, {"text": "Specifically, we extract alignment-based similarity features, vector-based similarity features and sentence constituent similarity features from sentence pairs, and produce similarity scores between two sentences by combining these feature through a Support Vector Regression (SVR) model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We submitted three runs named S1, S2 and S3 before the deadline, but S3 is identical to S1 by mistake.", "labels": [], "entities": []}, {"text": "So in this paper, we just discuss S1 and S2.", "labels": [], "entities": []}, {"text": "lists the settings of run S1 and run S2.", "labels": [], "entities": []}, {"text": "For run S1, we used all the features described in this paper.", "labels": [], "entities": []}, {"text": "For run S2, we just used word alignment-based and sentence vector-based features.", "labels": [], "entities": [{"text": "word alignment-based", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7387161552906036}]}], "tableCaptions": [{"text": " Table 1: Settings of our submitted runs for SemEval 2016", "labels": [], "entities": [{"text": "SemEval", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.9231607913970947}]}, {"text": " Table 2: Performances on STS 2016 test data. Each num- ber in rows 1-5 is the correlation between system output  and human annotations for the corresponding data set. The  last row shows the values of the comprehensive results of  each run.", "labels": [], "entities": [{"text": "STS 2016 test data", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.7081396207213402}]}, {"text": " Table 3: Performance of three individual feature and our  best run (S1) on STS 2016 test sets.", "labels": [], "entities": [{"text": "STS 2016 test sets", "start_pos": 76, "end_pos": 94, "type": "DATASET", "confidence": 0.896930068731308}]}]}