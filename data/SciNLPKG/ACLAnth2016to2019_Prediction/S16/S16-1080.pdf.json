{"title": [{"text": "ECNU at SemEval-2016 Task 7: An Enhanced Supervised Learning Method for Lexicon Sentiment Intensity Ranking", "labels": [], "entities": [{"text": "ECNU at SemEval-2016 Task 7", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.818124794960022}, {"text": "Lexicon Sentiment Intensity", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.788648784160614}]}], "abstractContent": [{"text": "This paper describes our system submissions to task 7 in SemEval 2016, i.e., Determining Sentiment Intensity.", "labels": [], "entities": [{"text": "Determining Sentiment Intensity", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.8072649637858073}]}, {"text": "We participated the first two subtasks in English, which are to predict the sentiment intensity of a word or a phrase in English Twitter and General English domains.", "labels": [], "entities": []}, {"text": "To address this task, we present a supervised learning-to-rank system to predict the relevant scores, i.e., the strength associated with positive sentiment, for English words or phrases.", "labels": [], "entities": []}, {"text": "Multiple linguistic and sentiment features are adopted, e.g., Sentiment Lexicons, Sentiment Word Vectors, Word Vectors, Linguistic Features , etc.", "labels": [], "entities": []}, {"text": "Officially released results showed that our systems rank the 1st among all submissions in English, which proves the effectiveness of the proposed method.", "labels": [], "entities": []}], "introductionContent": [{"text": "The study of sentiment analysis is increasingly drawing attention of Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.9557633697986603}]}, {"text": "Many of the top performing sentiment analysis systems rely on sentiment lexicon (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9366476833820343}]}, {"text": "A sentiment lexicon is a list of words and phrases, such as \"excellent\", \"awful\" and \"not bad\", each of them is assigned with a positive or negative score reflecting its sentiment polarity and strength).", "labels": [], "entities": []}, {"text": "Higher scores indicate stronger sentiment strength.", "labels": [], "entities": []}, {"text": "However, many existing manually generated sentiment lexicons consist of lexicons with only sentiment orientation rather than sentiment strength.", "labels": [], "entities": []}, {"text": "For example, the words in BL ( are generally divided to two classes, i.e., positive and negative.", "labels": [], "entities": []}, {"text": "Although several sentiment lexicons have assigned discrete labels for terms, i.e., strong and weak, for example, MPQA), there is no continuous real-valued scores to indicate the intensity of sentiment so far.", "labels": [], "entities": []}, {"text": "The task of Determining Sentiment Intensity of English and Arabic Phrases intends to automatically create a sentiment lexicon with real-valued scores indicating the intensity of sentiment.", "labels": [], "entities": [{"text": "Determining Sentiment Intensity of English and Arabic Phrases", "start_pos": 12, "end_pos": 73, "type": "TASK", "confidence": 0.9105784744024277}]}, {"text": "The purpose of this task is to test the ability of an automatic system to predict a sentiment intensity score fora word or a phrase.", "labels": [], "entities": []}, {"text": "Phrases include negators, modals, intensifiers, and diminishers.", "labels": [], "entities": []}, {"text": "Given a list of terms, the participants are required to assign appropriate scores between 0 and 1, to indicate their strength of association with positive sentiment.", "labels": [], "entities": []}, {"text": "The task contains three subtasks (the first two are in English and the third is in Arabic) and we participated the first two subtasks in English.", "labels": [], "entities": []}, {"text": "The first General English Sentiment Modifiers Set contains phrases formed by a word and a modifier, where a modifier can be a negator, an auxiliary verb, a degree adverb, or even a combination of those above modifiers, e.g., \"would be very easy\", \"did not harm\", and \"would have been nice\".", "labels": [], "entities": []}, {"text": "The second English Twitter Mixed Polarity Set contains phrases made up of opposite polarity terms, such as \"lazy sundays\", \"best winter break\", \"couldn't stop smiling\", etc.", "labels": [], "entities": [{"text": "English Twitter Mixed Polarity Set", "start_pos": 11, "end_pos": 45, "type": "DATASET", "confidence": 0.9108498096466064}]}, {"text": "The official evaluation measure is Kendall correlation coefficient ().", "labels": [], "entities": [{"text": "Kendall correlation coefficient", "start_pos": 35, "end_pos": 66, "type": "METRIC", "confidence": 0.8394821882247925}]}, {"text": "In previous work, the task was treated as a regression problem, the word embedding is used as a feature (.", "labels": [], "entities": []}, {"text": "In addition, (Hamdan et al., 2015) adopted unsupervised approach by using: The framework of our proposed system.", "labels": [], "entities": []}, {"text": "several sentiment lexicons for computing the score for each twitter term and ranked them.", "labels": [], "entities": []}, {"text": "In this paper, we treated this task as a ranking problem, and used pair-wise strategy to train the model.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 elaborates the procedure of query reconstruction, data preprocessing, feature engineering and the learning-to-rank model builtin our systems.", "labels": [], "entities": [{"text": "query reconstruction", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.8281857073307037}, {"text": "feature engineering", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7631474435329437}]}, {"text": "Section 3 describes the data sets and experiments.", "labels": [], "entities": []}, {"text": "Finally, Section 4 concludes this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "This sentiment strength prediction task was severed as the subtask E of Sentiment Analysis on Tweet in SemEval 2015.", "labels": [], "entities": [{"text": "sentiment strength prediction", "start_pos": 5, "end_pos": 34, "type": "TASK", "confidence": 0.8136708537737528}, {"text": "Sentiment Analysis", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9202041327953339}, {"text": "Tweet in SemEval 2015", "start_pos": 94, "end_pos": 115, "type": "DATASET", "confidence": 0.6106984540820122}]}, {"text": "Thus, the trial (i.e., 15trial) and test (i.e., 15test) data in SemEval 2015, which contained 200 and 1, 315 records separately, are integrated as training data for this task.", "labels": [], "entities": []}, {"text": "The organizer provided 200 records as development data set for each domain (i.e., 16trial Twitter and 16trial General) and the labels are the same as before, where each record is labeled with a decimal in the range of 0 to 1 and the score is the strength associated with positive sentiment.", "labels": [], "entities": []}, {"text": "In consideration of the lack of training data, we expanded it with the Language Assessment by Mechanical Turk lexicon (i.e., LabMT) which automatically labeled by.", "labels": [], "entities": []}, {"text": "It contains 10, 222 words rated on a scale of 1(sad) to 9(happy).", "labels": [], "entities": []}, {"text": "Note that the labels in the LabMT are different from the standard data, we converted the score to the scale of 0 to 1 by min-max normalization, i.e., x\u2212min max\u2212min .  For this task, Kendall rank correlation coefficient (usually measures the association between two measured quantities) is used as the metric to compare the ranked lists.", "labels": [], "entities": [{"text": "Kendall rank correlation coefficient", "start_pos": 182, "end_pos": 218, "type": "METRIC", "confidence": 0.723339132964611}]}, {"text": "Besides, the scores for Spearman's Rank Correlation(a nonparametric measure of statistical dependence between two variables) is provided as well.", "labels": [], "entities": [{"text": "Spearman's Rank Correlation", "start_pos": 24, "end_pos": 51, "type": "METRIC", "confidence": 0.5574017092585564}]}, {"text": "The Kendall rank correlation coefficient is severed as the final official evaluation criteria.", "labels": [], "entities": [{"text": "Kendall rank correlation coefficient", "start_pos": 4, "end_pos": 40, "type": "METRIC", "confidence": 0.838200643658638}]}, {"text": "As we described in 2.1, several operations should be conducted to transform the raw data form to accommodate the ranking system.", "labels": [], "entities": []}, {"text": "Considering that the LabMT is a term list that has been automatically labeled, while the provided standard data is more precise, so we constructed each query in LabMT with 200 records.", "labels": [], "entities": []}, {"text": "With regard to the provided standard data (i.e., 15trial, 15test), each query was made up of 20 records.", "labels": [], "entities": []}, {"text": "To construct training data for English Twitter domain, the LabMT, 15train and 15test were utilized, whereas the 16trial Twitter was utilized as development data.", "labels": [], "entities": [{"text": "LabMT", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.9004553556442261}]}, {"text": "Several types of features have been proposed in 2.3, in order to conduct feature selection, we adopted hill climbing which is described as: keeping adding one type of feature at a time until no further improvement can be achieved.", "labels": [], "entities": []}, {"text": "The system's diversity of two domains lies on the different training data we utilized: all tweetrelated data were adopted in English Twitter, while the words and phrases with hashtag(#) or informal forms were removed for General English.", "labels": [], "entities": []}, {"text": "Thus, the filtered data of LabMT, 15trial and 15test were severed as training data and the 16trial General were utilized as development data.", "labels": [], "entities": [{"text": "LabMT", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.9189633131027222}]}, {"text": "The process of feature selection was similar with English Twitter domain except that the Emphasize feature was not used.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.6616640686988831}, {"text": "English Twitter domain", "start_pos": 50, "end_pos": 72, "type": "DATASET", "confidence": 0.9182283083597819}]}, {"text": "shows the results of feature selection experiments on the development data, which lists the optimal feature sets of two domains.", "labels": [], "entities": []}, {"text": "From, it is interesting to find: (1) The Sentiment Lexicon features make a considerable contribution, because that the used sentiment lexicons contain sentiment information to some extent and its sentiment scores in lexicons are closely related to the strength associated with positive sentiment.", "labels": [], "entities": []}, {"text": "For example, in BL, the scores of positive words are 1 and the negative words are represented as \u22121.", "labels": [], "entities": []}, {"text": "The Linguistic features (i.e., Negation and Emphasize) also contribute to performance.", "labels": [], "entities": []}, {"text": "As for Negation, the possible reason maybe that there are plentiful negators existed in training and development data and the emphatic words have similar situation.", "labels": [], "entities": []}, {"text": "(3) The NormalW2V and SWV are both effective features for this task.", "labels": [], "entities": [{"text": "NormalW2V", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.8102551698684692}]}, {"text": "In our further experiments which test the SWV and NormalW2V respectively, we found that SWV performs much better than NormalW2V, which showed that the Combined Sentiment Word Vector Model indeed captured sentiment information from abundant auto-labeled tweets.", "labels": [], "entities": []}, {"text": "(4) The ranking based system outperforms the regression method, which indicates that it is convincing to regard this labeling task as a ranking problem.", "labels": [], "entities": []}, {"text": "(5) Compared with the results on English Twitter domain, we notice that the performance is much better.", "labels": [], "entities": [{"text": "English Twitter domain", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.9680953423182169}]}, {"text": "Based on the observation on development data of two domains, we found that the phrases on English Twitter domain are made up of opposite polarity terms, e.g., \"happy accident\", \"couldn't stop smiling\", while the records in General English domain are much more ordinary.", "labels": [], "entities": [{"text": "English Twitter domain", "start_pos": 90, "end_pos": 112, "type": "DATASET", "confidence": 0.8827301859855652}, {"text": "General English domain", "start_pos": 223, "end_pos": 245, "type": "DATASET", "confidence": 0.8774402538935343}]}, {"text": "The diversity of data distribution results in the above mentioned gap.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of feature selection experiments for Twitter English and General English domains.", "labels": [], "entities": [{"text": "General English domains", "start_pos": 75, "end_pos": 98, "type": "DATASET", "confidence": 0.783680776755015}]}, {"text": " Table 2: Performances of our systems and the top-ranked sys-", "labels": [], "entities": []}]}