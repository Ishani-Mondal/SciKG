{"title": [{"text": "Samsung Poland NLP Team at SemEval-2016 Task 1: Necessity for diversity; combining recursive autoencoders, WordNet and ensemble methods to measure semantic similarity", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our proposed solutions designed fora STS core track within the Se-mEval 2016 English Semantic Textual Similarity (STS) task.", "labels": [], "entities": [{"text": "Se-mEval 2016 English Semantic Textual Similarity (STS) task", "start_pos": 84, "end_pos": 144, "type": "TASK", "confidence": 0.6712511330842972}]}, {"text": "Our method of similarity detection combines recursive autoencoders with a WordNet award-penalty system that accounts for semantic relatedness, and an SVM classifier, which produces the final score from similarity matrices.", "labels": [], "entities": [{"text": "similarity detection", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.8167722821235657}]}, {"text": "This solution is further supported by an ensemble classifier, combining an aligner with a bi-directional Gated Recurrent Neural Network and additional features , which then performs Linear Support Vector Regression to determine another set of scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "The tasks from the Semantic Textual Similarity (STS) contest have always attracted vivid interest from the NLP community.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) contest", "start_pos": 19, "end_pos": 60, "type": "TASK", "confidence": 0.8147045118468148}]}, {"text": "The goal is to measure the semantic similarity between two given sentences on a scale from 0 to 5, trying to emulate the idea of similarity degrees, thus replicating human language understanding.", "labels": [], "entities": []}, {"text": "After processing two pieces of text, semantic textual similarity software captures degrees of semantic equivalence.", "labels": [], "entities": []}, {"text": "One of the goals of the STS task is to create a unified framework for extracting and measuring semantic similarity.", "labels": [], "entities": [{"text": "extracting and measuring semantic similarity", "start_pos": 70, "end_pos": 114, "type": "TASK", "confidence": 0.6977819681167603}]}, {"text": "Improvements achieved in the course of this task can be useful in many research areas, such as question answering), machine translation, and plagiarism detection).", "labels": [], "entities": [{"text": "question answering", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.9195013344287872}, {"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.8477493822574615}, {"text": "plagiarism detection", "start_pos": 141, "end_pos": 161, "type": "TASK", "confidence": 0.7710429728031158}]}, {"text": "We present a solution designed to detect both the similarity between single words and longer, multiword phrases.", "labels": [], "entities": []}, {"text": "It employs two important components: the unfolding recursive autoencoder (RAE)) and the penalty-award weight system based on WordNet ().", "labels": [], "entities": [{"text": "unfolding recursive autoencoder (RAE))", "start_pos": 41, "end_pos": 79, "type": "METRIC", "confidence": 0.6259951939185461}, {"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.9720089435577393}]}, {"text": "First, RAE is used to perform unsupervised learning on parse trees, then the WordNet module adjusts the distances of RAE vectors using awards and penalties based on semantic similarities of words.", "labels": [], "entities": [{"text": "RAE", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9077924489974976}]}, {"text": "The complete pipeline includes a deep net (RAE) module, a WordNet module, a normalization module and a sentence similarity matrices computing module.", "labels": [], "entities": []}, {"text": "Another solution that ran in parallel to the RAE pipeline, was the monolingual word aligner (in some cases we used its corrected version with additional features, including a bag-of-words).", "labels": [], "entities": [{"text": "RAE pipeline", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.7569778263568878}]}, {"text": "Finally an ensemble classifier was used to perform Linear Support Vector Regression () over the results from all the other classifiers.", "labels": [], "entities": []}, {"text": "This included: the base word aligner (, bi-directional Gated Recurrent Neural Network (), the RAE with WordNet features and the corrected aligner.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to separately train models for each evaluation set, we created reference test sets that are similar to the evaluation sets, e.g. for headlines we used  We used these reference test sets to find the best parameters of our models and chose the best model for run EN2.", "labels": [], "entities": [{"text": "EN2", "start_pos": 270, "end_pos": 273, "type": "DATASET", "confidence": 0.9447327256202698}]}, {"text": "The final model uses all samples from sets assigned to each evaluation set in.", "labels": [], "entities": []}, {"text": "Our final results are presented in.", "labels": [], "entities": []}, {"text": "For the AE run, we used RAE with WordNet Features, as described in Section 2.1.", "labels": [], "entities": [{"text": "AE run", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.7302504479885101}, {"text": "RAE", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.7816488146781921}]}, {"text": "For each test, a separate classifier was created with its own training set, as presented in.", "labels": [], "entities": []}, {"text": "The mapping was based on the average number of words per sentence in the set.", "labels": [], "entities": []}, {"text": "For the EN1, run we used the ensemble model described in Section 2.3.", "labels": [], "entities": [{"text": "EN1", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.8923600912094116}]}, {"text": "In the EN2 model we chose either RAE or ensemble based on the results for test sets matched with evaluation sets.", "labels": [], "entities": [{"text": "RAE", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.8794926404953003}]}, {"text": "As shown in, the ensemble model (EN1) yields better results than RAE (AE) for all sets.", "labels": [], "entities": [{"text": "RAE (AE)", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.8940885365009308}]}, {"text": "Thus, the merged model (EN2) falls between the two.", "labels": [], "entities": []}, {"text": "We also present the results) of our solution for SemEval 2015 sets.", "labels": [], "entities": [{"text": "SemEval 2015 sets", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.7826955318450928}]}, {"text": "Comparing them with the best run from SemEval 2015 competition (weighted mean), we concluded that bi-GRU yields the worst results.", "labels": [], "entities": [{"text": "SemEval 2015 competition", "start_pos": 38, "end_pos": 62, "type": "DATASET", "confidence": 0.6359984576702118}]}, {"text": "Second and third worst results came from the modified aligner and RAE respectively.", "labels": [], "entities": [{"text": "RAE", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9944899082183838}]}, {"text": "The ensemble and merged model yield the best results that surpass the performance of the 2015 winning solution.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The synergy of all parts of the solution (weighted mean", "labels": [], "entities": []}, {"text": " Table 3: The results obtained over three runs in the evaluation period of SemEval 2016.", "labels": [], "entities": []}]}