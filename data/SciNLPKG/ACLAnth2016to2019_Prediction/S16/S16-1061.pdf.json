{"title": [{"text": "INF-UFRGS-OPINION-MINING at SemEval-2016 Task 6: Automatic Generation of a Training Corpus for Unsupervised Identification of Stance in Tweets", "labels": [], "entities": [{"text": "INF-UFRGS-OPINION-MINING", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.9469835162162781}, {"text": "Automatic Generation", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.6405631601810455}, {"text": "Unsupervised Identification of Stance in Tweets", "start_pos": 95, "end_pos": 142, "type": "TASK", "confidence": 0.7668838997681936}]}], "abstractContent": [{"text": "This paper describe a weakly supervised solution for detecting stance in tweets, submitted to the SemEval 2016 Stance Task.", "labels": [], "entities": [{"text": "SemEval 2016 Stance Task", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.6376350820064545}]}, {"text": "Our approach is based on the premise that stance can be exposed as positive or negative opinions , although not necessarily about the stance target itself.", "labels": [], "entities": []}, {"text": "Our system receives as input n-grams representing opinion targets and common terms used to denote stance (e.g. hash-tags), and use these features, together with the sentiment detection solutions, to automatically compose a large training corpus.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 165, "end_pos": 184, "type": "TASK", "confidence": 0.8980123400688171}]}, {"text": "Then, it applies a supervised learning algorithm to develop a stance prediction model.", "labels": [], "entities": [{"text": "stance prediction", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.9143882095813751}]}], "introductionContent": [{"text": "Sentiment analysis involves the automatic identification of opinions, feelings, evaluations, attitudes and emotions expressed by people in the written language.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.951966404914856}, {"text": "identification of opinions, feelings, evaluations, attitudes and emotions expressed by people in the written language", "start_pos": 42, "end_pos": 159, "type": "Description", "confidence": 0.7107659677664439}]}, {"text": "Popular lines of work in this field are opinion mining ( and emotion mining.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.9066091477870941}, {"text": "emotion mining", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.9236850738525391}]}, {"text": "Stance detection is a less explored problem, addressed as part of SemEval-2016 (International Workshop on Semantic Evaluation 2016), Task 6 1 . Stance detection is defined in this task as the automatic determination from text whether its author is in favor of the given target, against the given target, or whether neither inference is likely.", "labels": [], "entities": [{"text": "Stance detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.956299364566803}, {"text": "SemEval-2016 (International Workshop on Semantic Evaluation 2016)", "start_pos": 66, "end_pos": 131, "type": "TASK", "confidence": 0.6784624821609921}, {"text": "Stance detection", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.8925211727619171}]}, {"text": "The present work describes the solution developed for Task 6-b, which involves the unsupervised stance detection in tweets based solely in their content.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.7147734016180038}]}, {"text": "The target is Donald Trump, a possible republican presidential candidate, for which two sets of 1 http://alt.qcri.org/semeval2016/task6/ non-annotated tweets were supplied: a domain corpus with 78,156 tweets and a test corpus with 707 tweets for task evaluation purpose.", "labels": [], "entities": []}, {"text": "Another set of annotated tweets for stance detection was available as part of Task 6-a (supervised stance detection), which included 639 tweets about another possible candidate, Hillary Clinton.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.9832412600517273}, {"text": "stance detection)", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.8703589836756388}]}, {"text": "We used this annotated data to develop the proposed method, due to the similarity of problems.", "labels": [], "entities": []}, {"text": "Details of the Task 6 can be found at ( ).", "labels": [], "entities": []}, {"text": "The identification of stance can be complex even for humans (, and our strategy was to address it partly as an opinion mining problem.", "labels": [], "entities": [{"text": "identification of stance", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9012190500895182}, {"text": "opinion mining", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.7240471690893173}]}, {"text": "Stance can be exposed as positive or negative opinions, but not necessarily about the target of the stance problem.", "labels": [], "entities": []}, {"text": "For instance, when the opinion target is a politician, the target maybe his/her agenda for health or education, members of the same party, or opponents.", "labels": [], "entities": []}, {"text": "In addition, stance detection faces challenges common to sentiment analysis in general, such as use of vocabulary and slang specific of the media, orthography errors, sarcasm, etc.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.9854341447353363}, {"text": "sentiment analysis", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.9141202569007874}]}, {"text": "We developed a weakly supervised method for detecting stance in tweets.", "labels": [], "entities": [{"text": "detecting stance in tweets", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.831705778837204}]}, {"text": "Our method requires some side-related targets and key stance n-grams, which are used, together with sentiment detection solutions, to automatically label tweets with regard to a stance.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8062389492988586}]}, {"text": "The automatically labeled tweets are then used to train a classifier to detect stance in tweets, resulting in a stance prediction model.", "labels": [], "entities": [{"text": "stance prediction", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7809591293334961}]}], "datasetContent": [{"text": "We experimentally developed our method using the Hillary labeled corpus provided for Task 6-a, given that both stance targets are politicians in campaign.", "labels": [], "entities": [{"text": "Hillary labeled corpus", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.9179855982462565}]}, {"text": "To perform this test and assess the performance of our method, we automatically labeled the Hillary training corpus, and applied the predictive model only to the instances that were not filtered by any of the rules.", "labels": [], "entities": [{"text": "Hillary training corpus", "start_pos": 92, "end_pos": 115, "type": "DATASET", "confidence": 0.7690133055051168}]}, {"text": "displays the results obtained according to the rules used to prepare the training dataset.", "labels": [], "entities": []}, {"text": "We compared three different scenarios: (a) Stance Keywords rules (rules 1 and 2) combined with the neutral rule (rule 7); (b) Side-related target and polarity rules (3 to 7 rules); (c) All rules; We measured the precision, recall and F-measure of the instances that were included in the training dataset (columns Automatic labeling), predicted instances according to the trained model (columns Predictive Model), and the whole set of instances labeled either using the rules or the predictive model (columns Combined).", "labels": [], "entities": [{"text": "precision", "start_pos": 212, "end_pos": 221, "type": "METRIC", "confidence": 0.9993425011634827}, {"text": "recall", "start_pos": 223, "end_pos": 229, "type": "METRIC", "confidence": 0.9980694651603699}, {"text": "F-measure", "start_pos": 234, "end_pos": 243, "type": "METRIC", "confidence": 0.9965741634368896}]}, {"text": "Scores are detailed per class and weighted average.", "labels": [], "entities": []}, {"text": "With regard to automatic labeling, the best weighted F-score was obtained by the set of stance keyword rules (a), i.e. 69.66, despite the poor result for the Favor class.", "labels": [], "entities": [{"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9950584173202515}]}, {"text": "However, our objective was to improve detection of Against and Favor stances, even at the expense of a less favorable result for the None class.", "labels": [], "entities": [{"text": "Against", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9795439839363098}, {"text": "Favor", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.867582380771637}]}, {"text": "Thus the set of all rules (c), which presents the second better F-score, is more interesting because it yields good scores for both Favor and Against stances.", "labels": [], "entities": [{"text": "F-score", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9965797066688538}, {"text": "Favor", "start_pos": 132, "end_pos": 137, "type": "METRIC", "confidence": 0.9639262557029724}]}, {"text": "Considering the predictive model, even if the training set is less accurate using only the rules involving sentiment about a side-related target (b), the respective model yields more accurate predictions (i.e. weighted F-measure of 48.21%).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 219, "end_pos": 228, "type": "METRIC", "confidence": 0.9762499332427979}]}, {"text": "However, considering the Against and Favor classes, again the set of all rules (c) produced a slightly better result.", "labels": [], "entities": [{"text": "Against", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9957757592201233}, {"text": "Favor", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9585532546043396}]}, {"text": "Finally, when considering the combination of the instances labeled by the rules and the ones by the predictive model, the best results are displayed for the set of all rules (c), considering both the weighted F-measure, and the scores for the Against and Favor classes.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 209, "end_pos": 218, "type": "METRIC", "confidence": 0.9755989909172058}]}, {"text": "Thus, the set of all rules presents more balanced results throughout the 2 phases of the process, yielding the best final result.", "labels": [], "entities": []}, {"text": "It should be noticed that the results summarized in for Trump and Hillary corpora, respectively, were produced differently.", "labels": [], "entities": []}, {"text": "The test set provided for Task-b was labeled using the predictive model only, whereas the Hillary results were produced using the combined approach of rules and predictive model.", "labels": [], "entities": []}, {"text": "Considering that Trump corpus was much bigger, compared to Hillary's, we assumed for the task that the results of the predictive model would be more accurate.", "labels": [], "entities": [{"text": "Trump corpus", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.8832673728466034}]}, {"text": "By applying the combined approach over Trump test set instead, the result would be slightly better (43.8%, using the same evaluation criterion adopted for the task).", "labels": [], "entities": [{"text": "Trump test set", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.831673244635264}]}], "tableCaptions": [{"text": " Table 1: Trump Corpus: Confusion Matrix and Metrics", "labels": [], "entities": [{"text": "Trump Corpus", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.9821242094039917}]}, {"text": " Table 5: Domain Corpus Tweets X Rule", "labels": [], "entities": [{"text": "Domain Corpus Tweets X", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.7715652734041214}]}, {"text": " Table 6: Hillary Corpus: Confusion Matrix and Metrics", "labels": [], "entities": [{"text": "Hillary Corpus", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.9914056062698364}]}, {"text": " Table 7: Experiments with Hillary dataset", "labels": [], "entities": [{"text": "Hillary dataset", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.6822875887155533}]}]}