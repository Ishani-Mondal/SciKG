{"title": [{"text": "UWB at SemEval-2016 Task 1: Semantic Textual Similarity using Lexical, Syntactic, and Semantic Information", "labels": [], "entities": [{"text": "UWB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.932602047920227}, {"text": "Semantic Textual Similarity", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.692209800084432}]}], "abstractContent": [{"text": "We present our UWB system for Semantic Textual Similarity (STS) task at SemEval 2016.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.8235410835061755}]}, {"text": "Given two sentences, the system estimates the degree of their semantic similarity.", "labels": [], "entities": []}, {"text": "We use state-of-the-art algorithms for the meaning representation and combine them with the best performing approaches to STS from previous years.", "labels": [], "entities": [{"text": "meaning representation", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.8623577654361725}, {"text": "STS", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.9765326976776123}]}, {"text": "These methods benefit from various sources of information, such as lexical, syntactic, and semantic.", "labels": [], "entities": []}, {"text": "In the monolingual task, our system achieve mean Pearson correlation 75.7% compared with human annotators.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 49, "end_pos": 68, "type": "METRIC", "confidence": 0.9167395532131195}]}, {"text": "In the cross-lingual task, our system has correlation 86.3% and is ranked first among 26 systems.", "labels": [], "entities": [{"text": "correlation", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9895914196968079}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) is one of the core disciplines in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8191540936628977}, {"text": "Natural Language Processing (NLP)", "start_pos": 68, "end_pos": 101, "type": "TASK", "confidence": 0.6879367530345917}]}, {"text": "Assume we have two textual fragments (word phrases, sentences, paragraphs, or full documents), the goal is to estimate the degree of their semantic similarity.", "labels": [], "entities": []}, {"text": "STS systems are usually compared with the manually annotated data.", "labels": [], "entities": []}, {"text": "In the case of SemEval the data consist of pairs of sentences with a score between 0 and 5 (higher number means higher semantic similarity).", "labels": [], "entities": []}, {"text": "For example, English pair Two dogs play in the grass.", "labels": [], "entities": []}, {"text": "Two dogs playing in the snow. has a score 2.8, i.e. the sentences are not equivalent, but share some information.", "labels": [], "entities": []}, {"text": "This year, SemEval's STS is extended with the Spanish-English cross-lingual subtask, where e.g. the pair Tuve el mismo problema que t\u00fa.", "labels": [], "entities": [{"text": "SemEval's STS", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.6022583444913229}]}, {"text": "I had the same problem. has a score 4.8, which means nearly equivalent.", "labels": [], "entities": []}, {"text": "Each year STS belongs to one of the most popular tasks at SemEval competition.", "labels": [], "entities": [{"text": "SemEval competition", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.6981427669525146}]}, {"text": "The best STS system at SemEval 2012) used lexical similarity and Explicit Semantic Analysis (ESA) (.", "labels": [], "entities": [{"text": "SemEval 2012", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.8454174995422363}, {"text": "Explicit Semantic Analysis (ESA)", "start_pos": 65, "end_pos": 97, "type": "METRIC", "confidence": 0.8576093713442484}]}, {"text": "In SemEval 2013, the best model () used semantic models such as Latent Semantic Analysis (LSA), external information sources (WordNet) and n-gram matching techniques.", "labels": [], "entities": []}, {"text": "For SemEval 2014 and 2015 the best system comes from (.", "labels": [], "entities": []}, {"text": "They introduced new algorithm, which align the words between two sentences.", "labels": [], "entities": []}, {"text": "They showed that this approach can be efficiently used also for STS.", "labels": [], "entities": [{"text": "STS", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9852264523506165}]}, {"text": "Overview of systems participating in previous SemEval competitions can be found in (.", "labels": [], "entities": [{"text": "SemEval competitions", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.9040912687778473}]}, {"text": "The best performing systems from previous years are based on various architectures benefiting from lexical, syntactic, and semantic information.", "labels": [], "entities": []}, {"text": "In this work we try to use the best techniques presented during last years, enhance them, and combine into a single model.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: STS gold data from prior years.", "labels": [], "entities": [{"text": "STS gold data", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.6872603197892507}]}, {"text": " Table 4: Pearson correlations on cross-lingual STS  task of SemEval 2016. RR denote the run (system)  ranking and TR denote our team ranking.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9105128049850464}, {"text": "RR", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9472329616546631}, {"text": "TR", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.9798907041549683}]}, {"text": " Table 2: Pearson correlations on SemEval 2015 evaluation data and comparison with the best performing  system in this year.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.871522456407547}, {"text": "SemEval 2015 evaluation data", "start_pos": 34, "end_pos": 62, "type": "DATASET", "confidence": 0.7529213428497314}]}, {"text": " Table 3: Pearson correlations on monolingual STS task of SemEval 2016.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9057940542697906}]}]}