{"title": [{"text": "MTE-NN at SemEval-2016 Task 3: Can Machine Translation Evaluation Help Community Question Answering?", "labels": [], "entities": [{"text": "MTE-NN at SemEval-2016 Task", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.767268031835556}, {"text": "Machine Translation Evaluation", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.8543529311815897}, {"text": "Question Answering", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.6715636402368546}]}], "abstractContent": [{"text": "We present a system for answer ranking (SemEval-2016 Task 3, subtask A) that is a direct adaptation of a pairwise neural network model for machine translation evaluation (MTE).", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.8952548503875732}, {"text": "machine translation evaluation (MTE)", "start_pos": 139, "end_pos": 175, "type": "TASK", "confidence": 0.843744620680809}]}, {"text": "In particular, the network incorporates MTE features, as well as rich syntactic and semantic embeddings, and it efficiently models complex non-linear interactions between them.", "labels": [], "entities": []}, {"text": "With the addition of lightweight task-specific features, we obtained very encouraging experimental results, with sizeable contributions from both the MTE features and from the pairwise network architecture.", "labels": [], "entities": []}, {"text": "We also achieved good results on subtask C.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present a system for SemEval-2016 Task 3 on Community Question Answering (cQA), subtask A (English).", "labels": [], "entities": [{"text": "SemEval-2016 Task 3 on Community Question Answering (cQA)", "start_pos": 24, "end_pos": 81, "type": "TASK", "confidence": 0.7136213332414627}]}, {"text": "In that task, we are given a question from a community forum and a thread of associated text comments intended to answer the question, and the goal is to rank the comments according to their appropriateness to the question.", "labels": [], "entities": []}, {"text": "Since cQA forum threads are noisy, as many comments are not answers to the question, the challenge lies in learning to rank all good comments above all bad ones.", "labels": [], "entities": []}, {"text": "In this work, we approach subtask A from a novel perspective: by using notions of machine translation evaluation (MTE) to decide on the quality of a comment.", "labels": [], "entities": [{"text": "machine translation evaluation (MTE)", "start_pos": 82, "end_pos": 118, "type": "TASK", "confidence": 0.8264219611883163}]}, {"text": "In particular, we extend the MTE neural network framework from.", "labels": [], "entities": [{"text": "MTE neural network", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8564101457595825}]}, {"text": "We believe that this neural network is interesting for the cQA problem because: (i) it works in a pairwise fashion, i.e., given two translation hypotheses and a reference translation to compare to, the network decides which translation hypothesis is better; this is appropriate fora ranking problem; (ii) it allows for an easy incorporation of rich syntactic and semantic embedded representations of the input texts, and it efficiently models complex non-linear relationships among them; (iii) it uses a number of MT evaluation measures that have not been explored for the cQA task (e.g., TER, Meteor and BLEU).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 514, "end_pos": 527, "type": "TASK", "confidence": 0.894863098859787}, {"text": "TER", "start_pos": 589, "end_pos": 592, "type": "METRIC", "confidence": 0.9394367933273315}, {"text": "BLEU", "start_pos": 605, "end_pos": 609, "type": "METRIC", "confidence": 0.9950061440467834}]}, {"text": "The analogy we apply to adapt the neural MTE architecture to the cQA problem is the following: given two comments c 1 and c 2 from the question thread-which play the role of the two translation hypotheses-we have to decide whether c 1 is a better answer than c 2 to question q-which plays the role of the translation reference.", "labels": [], "entities": []}, {"text": "The two tasks seem similar: both reason about the similarity of two competing texts against a reference text, to decide which one is better.", "labels": [], "entities": []}, {"text": "However, there are some profound differences.", "labels": [], "entities": []}, {"text": "In MTE, the goal is to decide whether a hypothesis translation conveys the same meaning as the reference translation.", "labels": [], "entities": [{"text": "MTE", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9515861868858337}]}, {"text": "In cQA, it is to determine whether the comment is an appropriate answer to the question.", "labels": [], "entities": [{"text": "cQA", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.8699442744255066}]}, {"text": "Furthermore, in MTE we can expect shorter texts, which are much more similar among them.", "labels": [], "entities": [{"text": "MTE", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.5334376692771912}]}, {"text": "In cQA, the question and the intended answers might differ significantly both in length and in lexical content.", "labels": [], "entities": []}, {"text": "Thus, it is not clear a priori whether the MTE network can work well for cQA.", "labels": [], "entities": []}, {"text": "Here, we show that the analogy is convenient, allowing to achieve competitive results.", "labels": [], "entities": []}, {"text": "At competition time, we achieved the sixth best result on the task from a set of twelve systems.", "labels": [], "entities": []}, {"text": "Right after the competition we introduced some minor improvements and extra features, without changing the fundamental architecture of the network, which improved the MAP result by almost two points.", "labels": [], "entities": [{"text": "MAP", "start_pos": 167, "end_pos": 170, "type": "METRIC", "confidence": 0.5396387577056885}]}, {"text": "We also performed a more detailed experimental analysis of the system, checking the contribution of several features and parts of the NN architecture.", "labels": [], "entities": []}, {"text": "We observed that every single piece contributes important information to achieve the final performance.", "labels": [], "entities": []}, {"text": "While task-specific features are crucial, other aspects of the framework are relevant too: syntactic embeddings, MT evaluation measures, and pairwise training of the network.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.9099221527576447}]}, {"text": "Finally, we used our system for subtask A to solve subtask C, which asks to find good answers to anew question that was not asked before in the forum by reranking the answers to related questions.", "labels": [], "entities": []}, {"text": "For the purpose, we weighted the subtask A scores by the reciprocal rank of the related questions (following the order given by the organizers, i.e., the ranking by Google).", "labels": [], "entities": []}, {"text": "Without any subtask C specific addition, we achieved the fourth best result in the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Below we explain which part of the available data we used for training, as well as our basic settings.", "labels": [], "entities": []}, {"text": "Then, we present in detail our experiments and the evaluation results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Ablation study of our improved system on  the test data.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9523136615753174}]}, {"text": " Table 1: Comparison to the official results on SemEval-2016 Task 3, subtask A. The first column shows  the rank of the primary runs with respect to the official MAP score. The subindices in the results columns  show the rank of the primary runs with respect to the evaluation measure in the respective column.", "labels": [], "entities": [{"text": "SemEval-2016 Task 3", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.5311812957127889}, {"text": "MAP score", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.6524040102958679}]}]}