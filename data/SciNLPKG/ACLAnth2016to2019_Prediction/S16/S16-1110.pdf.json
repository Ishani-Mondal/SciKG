{"title": [{"text": "NUIG-UNLP at SemEval-2016 Task 1: Soft Alignment and Deep Learning for Semantic Textual Similarity", "labels": [], "entities": [{"text": "NUIG-UNLP", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9125009775161743}, {"text": "Soft Alignment", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.8098507523536682}, {"text": "Semantic Textual Similarity", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.576101024945577}]}], "abstractContent": [{"text": "We present a multi-feature system for computing the semantic similarity between two sentences.", "labels": [], "entities": []}, {"text": "We introduce the use of soft alignment for computing text similarity, and also evaluate different methods to produce it.", "labels": [], "entities": [{"text": "soft alignment", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7321928441524506}, {"text": "computing text similarity", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.6851029594739279}]}, {"text": "The main features used by our system are based on alignment and Explicit Semantic Analysis.", "labels": [], "entities": [{"text": "alignment", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.9223397374153137}, {"text": "Explicit Semantic Analysis", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.6151766975720724}]}, {"text": "Our system was above the median scores for 4 out of the 5 datasets at SemEval 2016 STS Task 1.", "labels": [], "entities": [{"text": "SemEval 2016 STS Task 1", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.7308340072631836}]}], "introductionContent": [{"text": "Semantic textual similarity is the task of deciding if two sentences express a similar or identical meaning and requires a deep understanding of a sentence and its meaning in order to achieve high performance.", "labels": [], "entities": [{"text": "Semantic textual similarity", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7925376693407694}]}, {"text": "Recent successful approaches to this problem have been based on the idea of creating monolingual alignments) indicating which words in each of the two sentences correspond to each other.", "labels": [], "entities": []}, {"text": "This is quite successful in many cases where many words have the same lemma, however when synonymous and semantically similar terms are used, it is much harder to construct alignment.", "labels": [], "entities": []}, {"text": "For this reason, we propose the use of soft alignments, where instead of producing a hard linking between individual words in the sentence, we instead produce a score indicating how likely one word in a sentence is to be aligned to another word in the other sentence.", "labels": [], "entities": []}, {"text": "We examine several methods that can be used to learn these alignments including word embeddings () and models based on deep learning that have been suggested for machine translation ( ).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.772901862859726}]}, {"text": "In addition, we look into recent models for sentence and document similarity that can leverage the large amount of loosely aligned text in particular those based on Explicit Semantic Analysis ( and recent extensions aimed at generating orthogonal representations.", "labels": [], "entities": [{"text": "sentence and document similarity", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.6355363577604294}]}, {"text": "While these novel techniques alone can achieve high performance on the task, we note that simple metrics such as the number of overlapping terms can produce reasonable performance.", "labels": [], "entities": []}, {"text": "For added robustness we combine features based on simple metrics with novel methods explored in this work as a multi-feature regression problem, which we solve by means of an M5 Decision tree (.", "labels": [], "entities": [{"text": "M5 Decision tree", "start_pos": 175, "end_pos": 191, "type": "DATASET", "confidence": 0.8064030806223551}]}, {"text": "The rest of the paper is structured as follows: we present our system in Section 2.", "labels": [], "entities": []}, {"text": "We then present both our internal evaluation results and the official Task 1 results in Section 3 and finally we conclude in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted a series of evaluations using data from previous SemEval challenges ( as a baseline as shown in.", "labels": [], "entities": [{"text": "SemEval challenges", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.9073185324668884}]}, {"text": "These results present the following configurations using 10-fold cross-validation: In addition, we noticed that different datasets tended to have a different distribution of scores.", "labels": [], "entities": []}, {"text": "As such we tried evaluating in two modes macrotraining where we trained the decision tree on all datasets simultaneously and microtraining where a decision tree was trained for each dataset and used only for this dataset.", "labels": [], "entities": []}, {"text": "As the microtraining results are much stronger, for the task, we developed a lightweight domain classifier that found the nearest training dataset to the test dataset and used the classifier trained on the most appropriate dataset in our evaluation runs.", "labels": [], "entities": []}, {"text": "This classifier used the size of the intersection of the set of 100 most frequent words in each dataset and we obtained 100% accuracy for this classifier in identifying datasets by 10-fold crossvalidation, i.e., we choose the classifier trained on the training set T which maximizes the following similarity to the test set T :", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9995525479316711}]}], "tableCaptions": [{"text": " Table 1: Pearson's Correlation achieved by configurations of our system during development on the SemEval 2014 dataset", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.6921588182449341}, {"text": "SemEval 2014 dataset", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.7883082032203674}]}, {"text": " Table 2: Official results from SemEval, giving Pearson's correlation on each dataset", "labels": [], "entities": [{"text": "SemEval", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.8723875880241394}, {"text": "Pearson's correlation", "start_pos": 48, "end_pos": 69, "type": "METRIC", "confidence": 0.9320445855458578}]}]}