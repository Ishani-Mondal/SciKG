{"title": [{"text": "SemEval-2016 Task 2: Interpretable Semantic Textual Similarity", "labels": [], "entities": [{"text": "SemEval-2016 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.882221907377243}, {"text": "Interpretable Semantic Textual Similarity", "start_pos": 21, "end_pos": 62, "type": "TASK", "confidence": 0.5488819256424904}]}], "abstractContent": [{"text": "The final goal of Interpretable Semantic Tex-tual Similarity (iSTS) is to build systems that explain which are the differences and com-monalities between two sentences.", "labels": [], "entities": [{"text": "Interpretable Semantic Tex-tual Similarity (iSTS)", "start_pos": 18, "end_pos": 67, "type": "TASK", "confidence": 0.6759340294769832}]}, {"text": "The task adds an explanatory level on top of STS, formalized as an alignment between the chunks in the two input sentences, indicating the relation and similarity score of each alignment.", "labels": [], "entities": [{"text": "STS", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9780609607696533}]}, {"text": "The task provides train and test data on three datasets: news headlines, image captions and student answers.", "labels": [], "entities": []}, {"text": "It attracted nine teams, total-ing 20 runs.", "labels": [], "entities": []}, {"text": "All datasets and the annotation guideline are freely available 1", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) ( measures the degree of equivalence in the underlying semantics of paired snippets of text.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.766278957327207}]}, {"text": "The idea of Interpretable STS (iSTS) is to explain why two sentences maybe related/unrelated, by supplementing the STS similarity score with an explanatory layer.", "labels": [], "entities": [{"text": "Interpretable STS (iSTS)", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.5931919932365417}, {"text": "STS similarity score", "start_pos": 115, "end_pos": 135, "type": "METRIC", "confidence": 0.8266295790672302}]}, {"text": "Our final goal would be to enable interpretable systems, that is, systems that are able to explain which are the differences and commonalities between two sentences.", "labels": [], "entities": []}, {"text": "For instance, let's assume the following two sentences drawn from a corpus of news headlines: 12 killed in bus accident in Pakistan 10 killed in road accident in NW Pakistan * * Authors listed in alphabetical order 1 http://at.qrci.org/semeval2016/task2/ The output of such a system would be something like the following: The two sentences talk about accidents with casualties in Pakistan, but they differ in the number of people killed (12 vs. 10) and level of detail: the first one specifies that it is a bus accident, and the second one specifies that the location is NW Pakistan.", "labels": [], "entities": []}, {"text": "While giving such explanations comes naturally to people, constructing algorithms and computational models that mimic human level performance represents a difficult Natural Language Understanding (NLU) problem, with applications in dialogue systems, interactive systems and educational systems.", "labels": [], "entities": [{"text": "Natural Language Understanding (NLU)", "start_pos": 165, "end_pos": 201, "type": "TASK", "confidence": 0.7442440489927927}]}, {"text": "In the iSTS 2015 pilot task (), we defined a first step of such an ambitious system, which we follow in 2016.", "labels": [], "entities": [{"text": "iSTS 2015 pilot task", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.8711346834897995}]}, {"text": "Given the input (a pair of sentences), participant systems need first to identify the chunks in each sentence, and then, align chunks across the two sentences, indicating the relation and similarity score of each alignment.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 188, "end_pos": 204, "type": "METRIC", "confidence": 0.8653676807880402}]}, {"text": "The relation can be one of equivalence, opposition, specificity, similarity or relatedness, and the similarity score can range from 1 to 5.", "labels": [], "entities": [{"text": "similarity", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9856089949607849}, {"text": "similarity score", "start_pos": 100, "end_pos": 116, "type": "METRIC", "confidence": 0.983564019203186}]}, {"text": "Unrelated chunks are left unaligned.", "labels": [], "entities": []}, {"text": "An optional tag can be added to alignments for the cases where there is a difference in factuality or polarity.", "labels": [], "entities": []}, {"text": "See for the manual alignment of the two sample sentences.", "labels": [], "entities": []}, {"text": "The alignments between chunks in can be used to produce the kind of explanations shown in the previous example.", "labels": [], "entities": []}, {"text": "In previous work, and produced a dataset where corresponding <=> : (SIMILAR 4) <=> :", "labels": [], "entities": []}], "datasetContent": [{"text": "The official evaluation is based on, which uses the F1 of precision and recall of token alignments (in the context of alignment for Machine Translation).", "labels": [], "entities": [{"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9989878535270691}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9960656762123108}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9979117512702942}, {"text": "Machine Translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7347168922424316}]}, {"text": "argue that F1 is a better measure than other alternatives such as the Alignment Error Rate.", "labels": [], "entities": [{"text": "F1", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9995309114456177}, {"text": "Alignment Error Rate", "start_pos": 70, "end_pos": 90, "type": "METRIC", "confidence": 0.6663511395454407}]}, {"text": "The idea is that, for each pair of chunks that are aligned, we consider that any pairs of tokens in the chunks are also aligned with some weight.", "labels": [], "entities": []}, {"text": "The weight of each token-token alignment is the inverse of the number of alignments of each token (so-called fan out factor,.", "labels": [], "entities": []}, {"text": "Precision is measured as the ratio of token-token alignments that exist in both system and gold standard files, divided by the number of alignments in the system.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9869852662086487}]}, {"text": "Recall is measured similarly, as the ratio of token-token alignments that exist in both system and gold-standard, divided by the number of alignments in the gold standard.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9856313467025757}]}, {"text": "Precision and recall are evaluated separately for all alignments of all pairs.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9921084046363831}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9991620779037476}]}, {"text": "Participating runs were evaluated using four different metrics: F1 where alignment type and score are ignored (alignment F1, F for short); F1 where alignment types need to match, but scores are ignored (type F1, +T for short); F1 where alignment type is ignored, but each alignment is penalized 515 Listing 1: Annotation format <s en t en c e id =\"6\" st at u s =\"\"> 12 k i ll ed in bu s ac c id en ti n Pa k i st an 10 k i ll ed in r o ad ac c id en ti n NW Pa k i st an . .", "labels": [], "entities": [{"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9870342016220093}, {"text": "F1", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.9823908805847168}, {"text": "F1", "start_pos": 227, "end_pos": 229, "type": "METRIC", "confidence": 0.9914435744285583}]}, {"text": "<a l i g n me n t > 1 <==> 1 / / SIMI / / 4 / / 12 <==> 10 2 <==> 2 / / EQUI / / 5 / / k i ll ed <==> k i ll ed 3 4 5 <==> 3 4 5 / / SPE1 / / 4 / / in bu s ac c id en t <==> in r o ad ac c id en t 6 7 <==> 6 7 8 / / SPE2 / / 4 / / in Pa k i st an <==> in NW Pa k i st an </ a l i g n me n t > </ s en t en c e > when scores do not match 6 (score F1, +S for short); and, F1 where alignment types need to match, and each alignment is penalized when scores do not match (type and score F1, +TS for short).", "labels": [], "entities": [{"text": "F1", "start_pos": 370, "end_pos": 372, "type": "METRIC", "confidence": 0.9987794756889343}]}, {"text": "The type and score F1 is the main overall metric.", "labels": [], "entities": [{"text": "type", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9752911329269409}, {"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.6496943235397339}]}, {"text": "Note that our evaluation procedure does not explicitly evaluate the chunking results.", "labels": [], "entities": []}, {"text": "The method implicitly penalizes chunking errors via the induced token-token alignments, using a soft penalty.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Overall test results for type and score (+TS) across datasets. Each row correspond to a system run, and each column to a", "labels": [], "entities": [{"text": "type and score (+TS)", "start_pos": 35, "end_pos": 55, "type": "METRIC", "confidence": 0.6599957148234049}]}, {"text": " Table 3: Test results in Headlines for both scenarios. Each row correspond to a system run, and each column to one evaluation", "labels": [], "entities": []}]}