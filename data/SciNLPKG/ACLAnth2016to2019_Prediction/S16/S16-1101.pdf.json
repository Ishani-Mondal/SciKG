{"title": [{"text": "GWU NLP at SemEval-2016 Shared Task 1: Matrix Factorization for Crosslingual STS", "labels": [], "entities": [{"text": "GWU NLP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.865593671798706}, {"text": "Crosslingual STS", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.7745838463306427}]}], "abstractContent": [{"text": "We present a matrix factorization model for learning cross-lingual representations for sentences.", "labels": [], "entities": []}, {"text": "Using sentence-aligned corpora, the proposed model learns distributed representations by factoring the given data into language-dependent factors and one shared factor.", "labels": [], "entities": []}, {"text": "As a result, input sentences from both languages can be mapped into fixed-length vectors and then compared directly using the cosine similarity measure, which achieves 0.8 Pearson correlation on Spanish-English semantic textual similarity.", "labels": [], "entities": [{"text": "cosine similarity measure", "start_pos": 126, "end_pos": 151, "type": "METRIC", "confidence": 0.8023252487182617}, {"text": "Pearson correlation", "start_pos": 172, "end_pos": 191, "type": "METRIC", "confidence": 0.9498945474624634}]}], "introductionContent": [{"text": "Semantic textual similarity (STS) is a measure of relatedness in meaning between a pair of variablelength textual snippets, such as sentences.", "labels": [], "entities": [{"text": "Semantic textual similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5453127225240072}]}, {"text": "Using unsupervised vector space models, words and sentences can be mapped into dense vector representations that capture implicit syntactic and semantic information.", "labels": [], "entities": []}, {"text": "These representations can then be directly compared using well-known distance or similairty measures, such as the Euclidean distance or cosine similarity, which reflect their overall semantic relatedness.", "labels": [], "entities": []}, {"text": "Such distributed representations of words, or word embeddings, can be learned using global word co-occurrence statistics as in matrix factorization models, or using local context as in neural probabilistic language models (.", "labels": [], "entities": []}, {"text": "A variable-length sentence can be mapped into a fixedlength vector either by combining word embeddings or directly learning sentence representations as in the paragraph vector model proposed in).", "labels": [], "entities": []}, {"text": "In crosslingual STS, the challenge is to compare sentences from two different languages.", "labels": [], "entities": [{"text": "crosslingual STS", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.8309599161148071}]}, {"text": "We address this problem by directly learning crosslingual vector representations for words and sentences, which allows us to calculate the STS scores without the need for explicit translation or mapping.", "labels": [], "entities": [{"text": "STS scores", "start_pos": 139, "end_pos": 149, "type": "METRIC", "confidence": 0.8888455629348755}]}, {"text": "Several models can be used for learning cross-lingual compositional representations ().", "labels": [], "entities": []}, {"text": "We propose a relatively simple and nuanced unsupervised model inspired by the monolingual weighted matrix factorization (WMF) model proposed in, which we extend to the cross-lingual setting.", "labels": [], "entities": []}, {"text": "The WMF model learns word representations by decomposing a sparse tf-idf matrix into two lowrank factor matrices representing words and sentences.", "labels": [], "entities": []}, {"text": "The weights are adjusted to reflect the confidence levels in reconstructing observed vs. missing words in the original matrix.", "labels": [], "entities": []}, {"text": "Representations for variable-length sequences can be calculated by minimizing the reconstruction error as described in Section 2.1.", "labels": [], "entities": []}, {"text": "In this paper, we propose to extend this model to the cross-lingual setting by modeling two languages in parallel to obtain shared semantic representations.", "labels": [], "entities": []}, {"text": "The proposed model has a simple loss function and only uses sentence-aligned data for learning the shared representations.", "labels": [], "entities": []}, {"text": "We describe the model in two variations in Section 2.2.", "labels": [], "entities": []}, {"text": "This model yields a performance of 0.8 Pearson correlation in Semeval's English-Spanish crosslingual STS task.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 39, "end_pos": 58, "type": "METRIC", "confidence": 0.9599174559116364}, {"text": "crosslingual STS task", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.7060088912645975}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Cross-lingual STS EN-SP Test results using Pearson", "labels": [], "entities": [{"text": "Cross-lingual STS EN-SP", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.6980587442715963}]}]}