{"title": [{"text": "YZU-NLP Team at SemEval-2016 Task 4: Ordinal Sentiment Classifi- cation Using a Recurrent Convolutional Network", "labels": [], "entities": []}], "abstractContent": [{"text": "Sentiment analysis of tweets has attracted considerable attention recently for potential use in commercial and public sector applications.", "labels": [], "entities": [{"text": "Sentiment analysis of tweets", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9391245245933533}]}, {"text": "Typical sentiment analysis classifies the sentiment of sentences into several discrete classes (e.g., positive and negative).", "labels": [], "entities": [{"text": "sentiment analysis classifies the sentiment of sentences", "start_pos": 8, "end_pos": 64, "type": "TASK", "confidence": 0.9105527145521981}]}, {"text": "The aim of Task 4 subtask C of SemEval-2016 is to classify the sentiment of tweets into an ordinal five-point scale.", "labels": [], "entities": [{"text": "classify the sentiment of tweets", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.8036009192466735}]}, {"text": "In this paper, we present a system that uses word embeddings and recurrent convolutional networks to complete the competition task.", "labels": [], "entities": []}, {"text": "The word embeddings provide a continuous vector representation of words for the recurrent convolutional network to use in building sentence vectors for multi-point classification.", "labels": [], "entities": [{"text": "multi-point classification", "start_pos": 152, "end_pos": 178, "type": "TASK", "confidence": 0.7528330385684967}]}, {"text": "The proposed method ranked second among eleven teams in terms of micro-averaged MAE (mean absolute error) and eighth for macro-averaged MAE.", "labels": [], "entities": [{"text": "micro-averaged MAE (mean absolute error)", "start_pos": 65, "end_pos": 105, "type": "METRIC", "confidence": 0.8011407937322345}]}], "introductionContent": [{"text": "Sentiment analysis seeks to detect and analyze sentiment within texts.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9375223815441132}]}, {"text": "Following the rapid increase of user generated content in the form of social media, sentiment analysis has attracted considerable interest.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.9496182203292847}]}, {"text": "Typical approaches to sentiment analysis classify the sentiment of a sentence into several discrete classes such as positive and negative polarities, or six basic emotions: anger, happiness, fear, sadness, disgust and surprise.", "labels": [], "entities": [{"text": "sentiment analysis classify the sentiment of a sentence", "start_pos": 22, "end_pos": 77, "type": "TASK", "confidence": 0.9201892986893654}]}, {"text": "Based on this representation, various techniques have been investigated including supervised learning and lexicon-based approaches.", "labels": [], "entities": []}, {"text": "Supervised learning approaches require training data for sentiment classification (, while lexicon-based approaches do not require training data but use a sentiment lexicon to determine the overall sentiment of a sentence.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.9034793376922607}]}, {"text": "A five-point scale () is also a popular way to evaluate sentiment.", "labels": [], "entities": []}, {"text": "Many companies, such as Amazon, Google, and Alibaba all use a multi-point scale to evaluate product or APP reviews.", "labels": [], "entities": []}, {"text": "Unlike typical classification approaches, ordinal classification can assign different ratings (e.g., very negative, negative, neutral, positive and very positive) according to sentiment strength.", "labels": [], "entities": [{"text": "ordinal classification", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7118330001831055}]}, {"text": "Task 4 subtask C of SemEval-2016 seeks to classify the sentiment of tweets into an ordinal five-point scale.", "labels": [], "entities": []}, {"text": "This paper presents a system that uses word embeddings ( and recurrent convolutional networks to this end.", "labels": [], "entities": []}, {"text": "The word embeddings can capture both semantic and syntactic information of words to provide a continuous vector representation of those words.", "labels": [], "entities": []}, {"text": "These word vectors are then used to build sentence vectors through a recurrent convolutional neural network.", "labels": [], "entities": []}, {"text": "For multi-point classification, we discretize the continuous sentiment intensity to a five partitions of equal intervals.", "labels": [], "entities": [{"text": "multi-point classification", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.8016239702701569}]}, {"text": "The proposed recurrent convolutional network consists of two parts: a convolutional neural network (CNN) () on the bottom to reduce the dimension of a sentence matrix, fol-lowed by along short-term memory (LSTM) () layer to form the sentence representation, and a linear regression layer on the top to fit the sentiment intensity of sentences.", "labels": [], "entities": []}, {"text": "The details of the CNN, LSTM and their combination are described in the following section.", "labels": [], "entities": [{"text": "CNN", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.9563480615615845}]}], "datasetContent": [{"text": "We evaluated the proposed CNN-LSTM model by submitting the results to the SemEval-2016 Task 4 subtask.", "labels": [], "entities": [{"text": "SemEval-2016 Task 4 subtask", "start_pos": 74, "end_pos": 101, "type": "DATASET", "confidence": 0.609499029815197}]}, {"text": "The statistics of the dataset used in this competition are summarized in.", "labels": [], "entities": []}, {"text": "As the original tweets maybe removed by Twitter users themselves, we can just download apart of the data in gold training, gold development, and gold development-test dataset.", "labels": [], "entities": [{"text": "gold development-test dataset", "start_pos": 145, "end_pos": 174, "type": "DATASET", "confidence": 0.6144908567269644}]}, {"text": "The distribution of sentiment labels shown in shows data imbalance.", "labels": [], "entities": []}, {"text": "Most of the data were annotated in [-1, 0, 1] labels, and only a few were annotated Very Negative (-2) or Very Positive (2).", "labels": [], "entities": []}, {"text": "As mentioned earlier, the proposed method consists of word embeddings and a recurrent convolutional network.", "labels": [], "entities": []}, {"text": "Both parts may have their own parameters for optimization.", "labels": [], "entities": []}, {"text": "For word embeddings, we used popular pre-trained word vectors from GloVe (.", "labels": [], "entities": []}, {"text": "GloVe is an unsupervised learning algorithm for learning word representation.", "labels": [], "entities": [{"text": "learning word representation", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.7223835984865824}]}, {"text": "Training is performed on aggregated global word co-occurrence statistics from a large corpus, and the resulting representation showcases interesting linear substructures in the word vector space.", "labels": [], "entities": []}, {"text": "They provide pretrained word vectors trained on 840B tokens from common crawls and have a length of 300.", "labels": [], "entities": []}, {"text": "Although the pre-trained word embeddings can capture important semantic and syntactic inName # (-2) # (-1) # (0) #   formation of words, they are not sufficient to capture sentiment behaviors in texts.", "labels": [], "entities": []}, {"text": "To further improve word embeddings to capture sentiment information, we trained our recurrent convolutional network using an additional dataset from the Vader corpus shows the experimental results after the release of test set ratings.", "labels": [], "entities": [{"text": "Vader corpus", "start_pos": 153, "end_pos": 165, "type": "DATASET", "confidence": 0.9304349422454834}]}, {"text": "We found that the CNN-LSTM achieved better performance on the development test set than the test set.", "labels": [], "entities": []}, {"text": "Conversely, the CNN alone yielded better performance on the test data than the development-test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Distributions of sentiment ratings. # (n) denotes  the number of tweets annotated with a rating of n in the  range of [-2, -1, ..., 2], corresponding to Strongly Negative,  Negative, Negative or Neutral, Positive, Strongly Positive,  respectively.", "labels": [], "entities": []}, {"text": " Table 1: Summary of data statistics.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.8964540958404541}]}, {"text": " Table 4: Results of CNN-LSTM and CNN alone.", "labels": [], "entities": [{"text": "CNN-LSTM", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9666835069656372}, {"text": "CNN", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.9615029096603394}]}]}