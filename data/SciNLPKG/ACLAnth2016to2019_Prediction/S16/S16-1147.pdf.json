{"title": [{"text": "USAAR at SemEval-2016 Task 11: Complex Word Identification with Sense Entropy and Sentence Perplexity", "labels": [], "entities": [{"text": "USAAR at SemEval-2016 Task 11", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.8646340012550354}, {"text": "Complex Word Identification", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.627352754275004}]}], "abstractContent": [{"text": "This paper describes an information-theoretic approach to complex word identification using a classifier based on an entropy based measure based on word senses and sentence-level perplexity features.", "labels": [], "entities": [{"text": "complex word identification", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6571579178174337}]}, {"text": "We describe the motivation behind these features based on information density and demonstrate that they perform modestly well in the complex word identification task in SemEval-2016.", "labels": [], "entities": [{"text": "word identification task", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.7847654223442078}]}, {"text": "We also discuss the possible improvements that can be made to future work by exploring the subjectivity of word complexity and more robust evaluation metrics for the complex word identification task.", "labels": [], "entities": [{"text": "word identification task", "start_pos": 174, "end_pos": 198, "type": "TASK", "confidence": 0.7782437900702158}]}], "introductionContent": [{"text": "Complex Word Identification (CWI) is the task of automatically identifying difficult words in a sentence.", "labels": [], "entities": [{"text": "Complex Word Identification (CWI)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7544618050257365}, {"text": "automatically identifying difficult words in a sentence", "start_pos": 49, "end_pos": 104, "type": "TASK", "confidence": 0.5825599389416831}]}, {"text": "It is an important subtask prior to the textual/lexical simplification task that pertains to the substitution of abstruse words with lucid variants which can be apprehended by a wider gamut of readers.", "labels": [], "entities": [{"text": "textual/lexical simplification", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6647246107459068}]}, {"text": "The aim of the CWI task is to annotate the difficult words as shown in the underlined examples in the previous paragraph, such that a lexical simplification system can produce the following sentence: It is an important subtask before the textual/lexical simplification task that concerns the replacement of difficult words with simpler variants which can be understood by a wider range of readers.", "labels": [], "entities": []}, {"text": "Lexical simplification is a specific case of lexical substitution where the complex words in a sentence are replaced with simpler words.", "labels": [], "entities": [{"text": "Lexical simplification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8913004100322723}, {"text": "lexical substitution", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.731682151556015}]}, {"text": "Historically, lexical substitution was conceived as a means to examine the issue of the appropriateness of a fixed word sense inventory in the word sense disambiguation task the \"sense\" of a polysemous word is correctly identified given a context sentence.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.7266596704721451}, {"text": "word sense disambiguation", "start_pos": 143, "end_pos": 168, "type": "TASK", "confidence": 0.6488388379414877}]}, {"text": "By allowing fluidity in the \"sense\" inventory and by quantifying how much the systems were able to generate good substitutes, these lexical substitutes would have built a word sense cluster of words that may not be covered by a set of pre-defined words in a sense inventory, e.g. Princeton WordNet and Open Multilingual WordNet).", "labels": [], "entities": [{"text": "Princeton WordNet", "start_pos": 280, "end_pos": 297, "type": "DATASET", "confidence": 0.9432292282581329}]}], "datasetContent": [{"text": "The dataset for the CWI task in SemEval-2016 is annotated at word level with binary labels; 1 for complex and 0 for non-complex.", "labels": [], "entities": []}, {"text": "presents the corpus statistics of the dataset provided for the CWI Task.", "labels": [], "entities": []}, {"text": "The organizers have decided to emulate the limited human language capacity with a small training set and a large testing set that reflects the relatively larger proportion of text that a human will encounter in reality.", "labels": [], "entities": []}, {"text": "However, we do note the stark difference between the percentage of complex words in the training and test data; it skews towards words being annotated with the noncomplex labels.", "labels": [], "entities": []}, {"text": "To compute the sense entropy, we annotated the dataset with lemmas using the PyWSD lemmatizer  and reference the lemmas to the Princeton WordNet.", "labels": [], "entities": [{"text": "PyWSD lemmatizer", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.7537572085857391}, {"text": "Princeton WordNet", "start_pos": 127, "end_pos": 144, "type": "DATASET", "confidence": 0.8364875316619873}]}, {"text": "The training and testing set comprise 2,237 and 88,221 words respectively.", "labels": [], "entities": []}, {"text": "Of the annotated words, the training and testing set has 1,903 and 20,016 unique lemmas and the WordNet covers 84.97% and 64.89% of these lemmas respectively.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9729414582252502}]}, {"text": "When a lemma is not covered by WordNet, we assign an entropy of 0 that indicates that the lemma's complexity is easily predictable and the classifier would assign the majority label to the word.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9495182037353516}]}, {"text": "To compute the sentence perplexity as presented in the previous section, we use the English Wikipedia section of the SeedLing corpus) and the news articles from the DSL Corpus Collection ( ) to train the language model using the KenLM tool ().", "labels": [], "entities": [{"text": "SeedLing corpus", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.8262794613838196}, {"text": "DSL Corpus Collection", "start_pos": 165, "end_pos": 186, "type": "DATASET", "confidence": 0.9669292767842611}]}, {"text": "On average, there are 11 annotated words per sentence and every word in the same sentence shares the same sentence perplexity.", "labels": [], "entities": []}, {"text": "Using both the sense entropy and sentence perplexity as features, we train a boosted tree binary classifier) using the Graphlab Create 2 machine learning toolkit to identify the word complexity.", "labels": [], "entities": [{"text": "Graphlab Create 2 machine learning toolkit", "start_pos": 119, "end_pos": 161, "type": "DATASET", "confidence": 0.8406551480293274}]}, {"text": "Interestingly, when we use the raw number of senses instead of sense entropy as a feature on various machine learning classifiers, the number of senses were uninformative and the classifiers either labels all words as complex or all words as noncomplex.", "labels": [], "entities": []}, {"text": "Complex Word Identification is a novel task and possibly the standard F-score and accuracy measures might not be reflective of the task difficulty or the system efficiency.", "labels": [], "entities": [{"text": "Complex Word Identification", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6865200797716776}, {"text": "F-score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9966529011726379}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9960417747497559}]}, {"text": "Given the binary nature of the classification task, we suggest the use of Matthews correlation coefficient that measures the measures the correlation coefficient between the observed and predicted binary labels, which can be viewed as a variant of the chisquare coefficient . It measures the discordant relations between the true and false positives and negatives and avoids the need to optimize the systems based on either accuracy or precision but a healthy fusion of both.", "labels": [], "entities": [{"text": "Matthews correlation coefficient", "start_pos": 74, "end_pos": 106, "type": "METRIC", "confidence": 0.8085703651110331}, {"text": "accuracy", "start_pos": 424, "end_pos": 432, "type": "METRIC", "confidence": 0.9981379508972168}, {"text": "precision", "start_pos": 436, "end_pos": 445, "type": "METRIC", "confidence": 0.9832742810249329}]}, {"text": "The coefficient value ranges from -1 to +1 where +1, 0  and -1 respectively represents perfect, random and inverse predictions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: CWI Task Dataset for SemEval-2016.", "labels": [], "entities": [{"text": "CWI Task Dataset", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.8596037824948629}]}, {"text": " Table 2: Comparative Results between our Systems, the Top Systems and Threshold-based Baselines in SemEval-2016 CWI Task.", "labels": [], "entities": [{"text": "SemEval-2016 CWI Task", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.6001513997713724}]}]}