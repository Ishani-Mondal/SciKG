{"title": [{"text": "ECNU at SemEval-2016 Task 3: Exploring Traditional Method and Deep Learning Method for Question Retrieval and Answer Ranking in Community Question Answering", "labels": [], "entities": [{"text": "ECNU at SemEval-2016 Task", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.8956939578056335}, {"text": "Question Retrieval", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8347156941890717}, {"text": "Answer Ranking in Community Question Answering", "start_pos": 110, "end_pos": 156, "type": "TASK", "confidence": 0.7007580399513245}]}], "abstractContent": [{"text": "This paper describes the system we submitted to the task 3 (Community Question Answering) in SemEval 2016, which contains three subtasks, i.e., Question-Comment Similarity (subtask A), Question-Question Similarity (subtask B), and Question-External Comment Similarity (subtask C).", "labels": [], "entities": [{"text": "Community Question Answering) in SemEval 2016", "start_pos": 60, "end_pos": 105, "type": "TASK", "confidence": 0.7327770335333688}, {"text": "Question-Question Similarity", "start_pos": 185, "end_pos": 213, "type": "TASK", "confidence": 0.733576238155365}, {"text": "Question-External Comment Similarity", "start_pos": 231, "end_pos": 267, "type": "TASK", "confidence": 0.6401732762654623}]}, {"text": "For subtask A, we employed three different methods to rank question-comment pair, i.e., supervised model using traditional features, Convolutional Neu-ral Network and Long-Short Term Memory Network.", "labels": [], "entities": []}, {"text": "For subtask B, we proposed two novel methods to improve semantic similarity estimation between question-question pair by integrating the rank information of question-comment pair.", "labels": [], "entities": [{"text": "semantic similarity estimation", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.6882586677869161}]}, {"text": "For subtask C, we implemented a two-step strategy to select out the similar questions and filter the unrelated comments with respect to the original question.", "labels": [], "entities": []}], "introductionContent": [{"text": "The purpose of Community Question Answering task in) is to provide a platform for finding good answers to new questions in a community-created discussion forum, where the main task (subtask C) is defined as follows: given anew question and a large collection of question-comment threads created by a user community, participants are required to rank the comments that are most useful for answering the new question.", "labels": [], "entities": [{"text": "Community Question Answering task", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.7080205529928207}]}, {"text": "Obviously, this main task consists of two optional subtasks, i.e., Question-Comment Similarity (subtask A, also known as answer ranking), which is to re-rank comments/answers according to their relevance with respect to the question, and QuestionQuestion Similarity (i.e., subtask B, also known as question retrieval), which is to retrieve the similar questions according to their semantic similarity with respect to the original question.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 298, "end_pos": 316, "type": "TASK", "confidence": 0.7038705199956894}]}, {"text": "To address subtask A, we explored a traditional machine learning method which uses multiple types of features, e.g., Word Match Features, Translationbased Features, and Lexical Semantic Similarity Features.", "labels": [], "entities": []}, {"text": "Additionally, for subtask A, we also built a Convolutional Neural Network (CNN) model and a bidirectional Long Short-Term Memory (BLST-M) model to learn joint representation for questioncomment (Q-C) pair.", "labels": [], "entities": []}, {"text": "For subtask B, besides IR method and traditional machine learning method, we also proposed two novel methods to improve semantic similarity estimation between question-question (Q-Q) pairs by integrating the rank information of Q-C pairs.", "labels": [], "entities": [{"text": "semantic similarity estimation between question-question (Q-Q) pairs", "start_pos": 120, "end_pos": 188, "type": "TASK", "confidence": 0.7951887249946594}]}, {"text": "Since subtask C can be regarded as a joint work of the two above-mentioned subtasks, we implemented a two-step strategy to first select out similar questions and then to filter out the unrelated comments with respect to the original question.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our system.", "labels": [], "entities": []}, {"text": "Section 3 describes experimental setting.", "labels": [], "entities": []}, {"text": "Section 4 and 5 report results on training and test sets.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first method is to adopt rank evaluation metrics, i.e., Spearman, Pearson, and Kendall Ranking Coefficient directly as similarity scores for question similarity estimation.", "labels": [], "entities": [{"text": "question similarity estimation", "start_pos": 145, "end_pos": 175, "type": "TASK", "confidence": 0.7551066875457764}]}, {"text": "Generally, these three nonparametric metrics are to measure the statistical dependence between two variables and to assess how similar between two variables.", "labels": [], "entities": []}, {"text": "In comment ranking, they are used to measure how similar the two rankings Q 0 -C and Q 1 -C are.", "labels": [], "entities": []}, {"text": "Based on our consideration, given one comment set C, if the two ranks of Q 0 -C and Q 1 -C are similar, the semantic similarity between Q 0 -Q 1 is high.", "labels": [], "entities": []}, {"text": "These three ranking correlation coefficients (i.e., Spearman, Pearson, and Kendall Coefficient) can be used directly as question similarity scores or used as additional ranking scores in combination with other features (described in Section 2.1) extracted from Q 0 -Q 1 pair.", "labels": [], "entities": []}, {"text": "To evaluate performance of the tasks, the M ean Average P recision (MAP) is adopted as official evaluation measure by the organizers which the MAP is defined as the mean of the averaged precision scores for queries.", "labels": [], "entities": [{"text": "M ean Average P recision (MAP)", "start_pos": 42, "end_pos": 72, "type": "METRIC", "confidence": 0.9254612177610397}, {"text": "precision", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.8864732980728149}]}], "tableCaptions": [{"text": " Table 1: Statistics of datasets.", "labels": [], "entities": []}, {"text": " Table 2: Results of subtask A using different methods. /.+0", "labels": [], "entities": []}, {"text": " Table 3: Results of subtask B using different methods.", "labels": [], "entities": []}, {"text": " Table 4: Results of subtask C with different traditional NLP", "labels": [], "entities": []}, {"text": " Table 5: Our results and the best results on three subtask test", "labels": [], "entities": []}]}