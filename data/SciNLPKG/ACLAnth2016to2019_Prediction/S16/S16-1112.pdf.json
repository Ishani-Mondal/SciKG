{"title": [{"text": "LIPN-IIMAS at SemEval-2016 Task 1: Random Forest Regression Experiments on Align-and-Differentiate and Word Embeddings penalizing strategies", "labels": [], "entities": [{"text": "Align-and-Differentiate and Word Embeddings penalizing", "start_pos": 75, "end_pos": 129, "type": "TASK", "confidence": 0.5476643204689026}]}], "abstractContent": [{"text": "This paper describes the SOPA-N system used by the LIPN-IIMAS team in Semeval 2016 Semantic Textual Similarity (Task 1).", "labels": [], "entities": [{"text": "Semeval 2016 Semantic Textual Similarity (Task 1)", "start_pos": 70, "end_pos": 119, "type": "TASK", "confidence": 0.6653492616282569}]}, {"text": "We based our work on the SOPA 2015 system.", "labels": [], "entities": [{"text": "SOPA 2015 system", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.7819113532702128}]}, {"text": "The SOPA-2015 system used 16 similarity features (including Wordnet, Information Retrieval and Syntactic Dependencies) within a Random Forest learning model.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9442158341407776}]}, {"text": "We expanded this system with an Align and Differentiate based strategy, word embeddings and penal-ization, which showed 6.8% of improvement on the development set.", "labels": [], "entities": []}, {"text": "However, we found that on the evaluation data for the 2016 STS shared task, the 2015 system outperformed our newer systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The SOPA system combines a regression model with multi-level similarity measures, from very simple ones (like edit distance) to more sophisticated ones (like IR-based or Wordnet similarity) (.", "labels": [], "entities": []}, {"text": "Our goal this year was to add an alignand-differentiate penalizing strategy based on () to our 2015 system (SOPA).", "labels": [], "entities": []}, {"text": "The previous version consists on 16 similarity features which are regressed using a Random Forest learning algorithm.", "labels": [], "entities": []}, {"text": "The rationale of the penalization was to account for cases when apparent distributional alignments are closer than their semantically equivalent (for instance, colors: while black and white are distributional close because are colors, but they are not semantically equivalent).", "labels": [], "entities": []}, {"text": "We present two versions of the enhanced system SOPA 100 and 1000 which refers to the number of estimation trees used by the Random Forest algorithm.", "labels": [], "entities": []}, {"text": "We find that augmenting our 2015 model with an align and differentiate module boosts performance on the 2015 evaluation data.", "labels": [], "entities": []}, {"text": "However, on the STS 2016 test data it only outperformed our previous approach on the plagiarism dataset.", "labels": [], "entities": [{"text": "STS 2016 test data", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.8931063264608383}]}, {"text": "A closer error analysis showed that the gap between SOPA 100 and the gold standard (GS) scores are systematically positive, meaning that our new system overestimated the semantic similarity between phrases.", "labels": [], "entities": []}, {"text": "Another interesting finding from the error analysis was that in those sentences where SOPA 100 outperform SOPA, it was also more accurate, given the fact that the SOPA 100 standard deviation from the gold standard annotation was smaller than the one of", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the following configurations for SOPA 100 and SOPA 1, 000 runs, they were applied to the features 17-23: \u2022 Words identification and tokenization was done using a simple space-based regular expression.", "labels": [], "entities": [{"text": "Words identification", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.8096246719360352}]}, {"text": "\u2022 Similarity in WordNet was obtained using the path similarity metric available in NLTK package ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.9466588497161865}]}, {"text": "\u2022 For the penalization by shift-externalalignment, the Spearman correlation between the positions of aligned words is computed and, if the correlation is above a threshold of 0.25, it is multiplied by the alignment score.", "labels": [], "entities": []}, {"text": "\u2022 When checking the aligned pairs of words in WordNet: if one word of the pair is listed as an antonym of the other, a penalty of 1.0 is applied.; if words are not antonyms, we search the lowest common hypernym for both synsets, then we walk the path from this hypernym to the root and if in this walk we visit one of the nodes considered DSC then a penalty of 0.5 is applied.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9522554874420166}, {"text": "DSC", "start_pos": 339, "end_pos": 342, "type": "DATASET", "confidence": 0.9408425688743591}]}, {"text": "\u2022 We used cosine as the measure of distance among vectors.", "labels": [], "entities": []}, {"text": "The vectors are extracted form the standard Google pretrained word2vec models with 300 dimensions.", "labels": [], "entities": []}, {"text": "1 \u2022 We used the list of English stop-words available in the NLTK package to filter stop-words from sentences.", "labels": [], "entities": [{"text": "NLTK package", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.8994044065475464}]}, {"text": "\u2022 We discarded alignments that are below an align-threshold of 0.3.", "labels": [], "entities": []}, {"text": "\u2022 Each unaligned word was considered out-ofcontext (OOC) and for each of those, a penalization of 1.0 was applied.", "labels": [], "entities": [{"text": "penalization", "start_pos": 82, "end_pos": 94, "type": "METRIC", "confidence": 0.960923433303833}]}, {"text": "The code of our implementation and experimentation is openly available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Final test results", "labels": [], "entities": []}, {"text": " Table 3: Development results (STS 2015)", "labels": [], "entities": [{"text": "STS 2015)", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.8062207301457723}]}, {"text": " Table 4: Re-run of 100 system with only align-and-differentiate", "labels": [], "entities": []}]}