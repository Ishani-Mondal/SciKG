{"title": [{"text": "SemEval-2016 Task 10: Detecting Minimal Semantic Units and their Meanings (DiMSUM)", "labels": [], "entities": [{"text": "SemEval-2016 Task 10", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8314223885536194}, {"text": "Detecting Minimal Semantic Units and their Meanings (DiMSUM)", "start_pos": 22, "end_pos": 82, "type": "TASK", "confidence": 0.7513334691524506}]}], "abstractContent": [{"text": "This task combines the labeling of multiword expressions and supersenses (coarse-grained classes) in an explicit, yet broad-coverage paradigm for lexical semantics.", "labels": [], "entities": []}, {"text": "Nine systems participated; the best scored 57.7% F 1 in a multi-domain evaluation setting, indicating that the task remains largely unresolved.", "labels": [], "entities": [{"text": "F 1", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9933896660804749}]}, {"text": "An error analysis reveals that a large number of instances in the data set are either hard cases, which no systems get right, or easy cases, which all systems correctly solve.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical analysis tasks, e.g., part-of-speech tagging, are rather successful applications of natural language processing (NLP).", "labels": [], "entities": [{"text": "Grammatical analysis", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.81453737616539}, {"text": "part-of-speech tagging", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.7095150202512741}, {"text": "natural language processing (NLP)", "start_pos": 96, "end_pos": 129, "type": "TASK", "confidence": 0.776316891113917}]}, {"text": "They are comprehensive, i.e., they operate under the assumption that all grammatically-relevant parts of a sentence will be analyzed: We do not expect a POS tagger to only know a subset of the tags in the language.", "labels": [], "entities": []}, {"text": "Most POS tags accommodate unseen words and adapt readily to new text genres.", "labels": [], "entities": []}, {"text": "Together, these factors indicate a representation which achieves broad coverage.", "labels": [], "entities": []}, {"text": "Explicit analysis of lexical semantics, by contrast, has been more difficult to scale to broad coverage owing to limited comprehensiveness and extensibility.", "labels": [], "entities": []}, {"text": "The dominant paradigm of fine-grained word sense disambiguation, WordNet, is difficult to annotate in corpora, results in considerable data sparseness, and does not readily generalize to out-of-vocabulary words.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6962554653485616}, {"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9626458883285522}]}, {"text": "While the main corpus with WordNet senses, SemCor, does reflect several text genres, it is hard to expand SemCor-style annotations to new genres, such as social web text or transcribed speech.", "labels": [], "entities": []}, {"text": "This severely limits the applicability of SemCor-based NLP tools and restricts opportunities for linguistic studies of lexical semantics in corpora.", "labels": [], "entities": []}, {"text": "To address this limitation, in the DiMSUM 2016 shared task, we challenged participants to analyze the lexical semantics of English sentences with a tagset integrating multiword expressions and noun and verb supersenses (following, on multiple nontraditional genres of text.", "labels": [], "entities": []}, {"text": "By moving away from fine-grained sense inventories and lexicalized, language-specific 2 annotation, we take a step in the direction of broadcoverage, coarse-grained lexical semantic analysis.", "labels": [], "entities": [{"text": "coarse-grained lexical semantic analysis", "start_pos": 150, "end_pos": 190, "type": "TASK", "confidence": 0.6563312411308289}]}, {"text": "We believe this departure from the classical lexical semantics paradigm will ultimately prove fruitful fora variety of NLP applications in a variety of genres.", "labels": [], "entities": []}, {"text": "The integrated lexical semantic representation ( \u00a72, \u00a73) has been annotated in an extensive benchmark data set comprising several nontraditional domains ( \u00a74).", "labels": [], "entities": []}, {"text": "Objective, controlled evaluation procedures ( \u00a75) facilitate a comparison of the 9 systems submitted as part of the official task ( \u00a76).", "labels": [], "entities": []}, {"text": "While the systems range in performance, all are below 60% in our composite evaluation, suggesting that further work is needed to make progress on this difficult task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We invited submissions in multiple data conditions.", "labels": [], "entities": []}, {"text": "The open condition encouraged participants to make wide use of any and all available resources, including for distant or direct supervision.", "labels": [], "entities": []}, {"text": "A closed condition encouraged controlled comparisons of algorithms by limiting their training to specific resources distributed for the task.", "labels": [], "entities": []}, {"text": "Lastly, we allowed fora semi-supervised closed condition, in which use of a specific large unla-  beled corpus-the Yelp Academic Dataset 15 -was permitted.", "labels": [], "entities": [{"text": "Yelp Academic Dataset 15", "start_pos": 115, "end_pos": 139, "type": "DATASET", "confidence": 0.9153094291687012}]}, {"text": "Teams were permitted to submit no more than one run per condition.", "labels": [], "entities": []}, {"text": "Only one team submitted a system in the semi-supervised closed condition.", "labels": [], "entities": []}, {"text": "All conditions had access to: 1) the annotated data we provided; 2) Brown clusterings ( computed from large corpora of tweets and web reviews; 16 and 3) the English WordNet lexicon.", "labels": [], "entities": [{"text": "English WordNet lexicon", "start_pos": 157, "end_pos": 180, "type": "DATASET", "confidence": 0.8742145895957947}]}, {"text": "The input attest time included POS tags.", "labels": [], "entities": []}, {"text": "No sentence-level metadata was provided in the input attest time: test set sentence IDs were obscured to hide the source domain, and the order of sentences was randomized to remove document structure.", "labels": [], "entities": []}, {"text": "The training data, however, marked the domain from which the sentence was drawn (REVIEWS or TWEETS); systems were free to make use of this information, so long as it was not required as part of the input attest time.", "labels": [], "entities": [{"text": "REVIEWS", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9792952537536621}, {"text": "TWEETS", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.5470054149627686}]}, {"text": "We provided an evaluation script to allow participants to check the format of system output and to compute all official scores.", "labels": [], "entities": []}, {"text": "The MWE measure looks at precision, recall, and predicted or gold MWE do not factor into this measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9995191097259521}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9993795156478882}]}, {"text": "To award partial credit for partial overlap between a predicted MWE and a gold MWE, these scores are computed based on links between consecutive tokens in an expression ().", "labels": [], "entities": []}, {"text": "The tokens must appear in order but do not need to be adjacent.", "labels": [], "entities": []}, {"text": "The precision is the proportion of predicted links whose words both belong to the same expression in the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9991341233253479}]}, {"text": "Recall is the same as precision, but swapping the predicted and gold annotations.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9842289686203003}, {"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9996849298477173}]}, {"text": "17 defines this measure in detail and illustrates the calculations for an example.", "labels": [], "entities": []}, {"text": "To isolate the supersense classification performance, we compute precision, recall, and F 1 of the supersense-labeled word tokens.", "labels": [], "entities": [{"text": "supersense classification", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.8288807570934296}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.99956876039505}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9996128678321838}, {"text": "F 1", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9887249171733856}]}, {"text": "The numerator of both precision and recall is the number of tokens labeled with the correct supersense.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9993953704833984}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9990930557250977}]}, {"text": "(This interacts slightly with MWE identification, however, as supersenses are only marked on the first token of MWEs.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9370355904102325}]}, {"text": "We do not mark supersenses on all words of the MWE to avoid giving MWEs a disproportionate influence on the supersense score.)", "labels": [], "entities": []}, {"text": "Finally, combined precision, recall, and F 1 aggregate the MWE and supersense subscores.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9997268319129944}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996821880340576}, {"text": "F 1", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9929525256156921}]}, {"text": "The combined precision ratio is computed from the MWE MWE Precision: The proportion of predicted links whose words both belong to the same expression in the gold standard.", "labels": [], "entities": [{"text": "precision ratio", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.9806633591651917}, {"text": "MWE MWE Precision", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.7276500662167867}]}, {"text": "MWE Recall: Same as precision, but swapping the predicted and gold annotations.", "labels": [], "entities": [{"text": "MWE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7728882431983948}, {"text": "Recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.7742084860801697}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9996094107627869}]}, {"text": "MWE precision of the bottom annotation relative to the top one is 25.", "labels": [], "entities": [{"text": "MWE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5732578039169312}, {"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.8233352303504944}]}, {"text": "(Note that a link between words w 1 and w 2 is \"matched\" if, in the other annotation, there is a path between w 1 and w 2 .) The MWE recall value is 34.", "labels": [], "entities": [{"text": "MWE", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.4609996974468231}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.8669565320014954}]}, {"text": "Supersense precision and recall are both 12.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9920226335525513}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9997981190681458}]}, {"text": "Combined precision/recall scores add the respective subscores' numerators and denominators: thus, combined precision is 2+1 5+2 = 37, and combined recall is 3+1 4+2 = 23.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9949433207511902}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9593186974525452}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9358490109443665}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9606379866600037}]}, {"text": "Combined F 1 is their harmonic mean, i.e. 1223. and supersense precision ratios by adding their numerators and denominators, and likewise for combined recall (see the example in.", "labels": [], "entities": [{"text": "F 1", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9811163246631622}, {"text": "precision ratios", "start_pos": 63, "end_pos": 79, "type": "METRIC", "confidence": 0.9502265155315399}, {"text": "recall", "start_pos": 151, "end_pos": 157, "type": "METRIC", "confidence": 0.9740688800811768}]}, {"text": "Within each domain, scores are computed as microaverages.", "labels": [], "entities": []}, {"text": "The official tri-domain scores reported here are domain macroaverages: per-domain measures are aggregated with the three domains weighted equally.", "labels": [], "entities": []}, {"text": "The main score, tri-domain combined F 1 , is the arithmetic mean of the three perdomain combined F 1 scores.", "labels": [], "entities": [{"text": "tri-domain combined F 1", "start_pos": 16, "end_pos": 39, "type": "METRIC", "confidence": 0.6392123177647591}]}, {"text": "(Some system papers report domain microaverages, which give less influence to the TED domain because it is the smallest of the domains in the test set.)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Source datasets and preprocessing to obtain 17-tag Universal POS tags (UPOS) version 1.2. Most sources  already contained some form of POS tags, which we automatically converted to UPOS. We added missing necessary  distinctions-e.g., UD-style UPOS distinguishes auxiliaries from main verbs, but Petrov-style (Petrov et al., 2011),  PTB (Marcus et al., 1993), or TweetNLP (Owoputi et al., 2013) POS tagsets do not. Disambiguation was done  manually or via a gold parse, where available. We also modified the tokenization of the Tweebank data, to be consistent  with UPOS conventions for English (e.g., separating clitics).  Only some portions of the data group sentences into documents: N/A = not applicable; N.A. = not available.", "labels": [], "entities": [{"text": "PTB", "start_pos": 342, "end_pos": 345, "type": "DATASET", "confidence": 0.8471667766571045}, {"text": "Tweebank data", "start_pos": 537, "end_pos": 550, "type": "DATASET", "confidence": 0.9124760031700134}]}, {"text": " Table 3: Annotated datasets: status of lexical semantic annotations (retained, revised, or newly annotated for this  task) per subcorpus; word token and MWE instance counts; number and proportion (out of all MWEs) that are gappy;  proportion of tokens that belong to an MWE; number of units labeled with a noun supersense, and proportion that are  MWEs; likewise for verb supersenses.  Additional statistics relatively consistent across domains: MWEs per word: mean/median .055 (lowest: TED, .044;  highest: STREUSLE, .090). Supersenses per word: mean/median 0.3. Just 8 MWEs contain more than one gap (all in  STREUSLE or Ritter).", "labels": [], "entities": [{"text": "TED", "start_pos": 488, "end_pos": 491, "type": "METRIC", "confidence": 0.9157023429870605}, {"text": "STREUSLE", "start_pos": 509, "end_pos": 517, "type": "METRIC", "confidence": 0.6248253583908081}]}, {"text": " Table 4: Main results on the test set. Scores are tri- domain combined F 1 percentages. Resource conditions  are described in  \u00a76.2.", "labels": [], "entities": [{"text": "F 1 percentages", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.8543980320294698}]}, {"text": " Table 5: Per-domain evaluation results. Figures are F 1  percentages. The best value in each section and column is  in bold. Refer to table 4 for the identities of the systems.", "labels": [], "entities": []}]}