{"title": [{"text": "CUFE at SemEval-2016 Task 4: A Gated Recurrent Model for Sentiment Classification", "labels": [], "entities": [{"text": "SemEval-2016 Task 4", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.5595362583796183}, {"text": "Sentiment Classification", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.9449935853481293}]}], "abstractContent": [{"text": "In this paper we describe a deep learning system that has been built for SemEval 2016 Task4 (Subtask A and B).", "labels": [], "entities": [{"text": "SemEval 2016 Task4", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.6602523922920227}]}, {"text": "In this work we trained a Gated Recurrent Unit (GRU) neural network model on top of two sets of word embeddings: (a) general word embed-dings generated from unsupervised neural language model; and (b) task specific word em-beddings generated from supervised neural language model that was trained to classify tweets into positive and negative categories.", "labels": [], "entities": []}, {"text": "We also added a method for analyzing and splitting multi-words hashtags and appending them to the tweet body before feeding it to our model.", "labels": [], "entities": []}, {"text": "Our models achieved 0.58 F1-measure for Subtask A (ranked 12/34) and 0.679 Recall for Subtask B (ranked 12/19).", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9989041090011597}, {"text": "Recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9971265196800232}]}], "introductionContent": [{"text": "Twitter is a huge microbloging service with more than 500 million tweets per day 1 from different locations in the world and in different languages.", "labels": [], "entities": []}, {"text": "This large, continuous, and dynamically updated content is considered a valuable resource for researchers.", "labels": [], "entities": []}, {"text": "However many issues should betaken into account while dealing with tweets, namely: (1) informal language used by the users; (2) spelling errors; (3) text in the tweet maybe referring to images, videos, or external URLs; (4) emoticons; (5) hashtags used (combining more than one word as a single word); (6) usernames used to call or notify other users; spam or irrelevant tweets; and (8) character limit fora tweet to 140 characters.", "labels": [], "entities": []}, {"text": "This poses many challenges when analyzing tweets for natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.7198922634124756}]}, {"text": "In this paper we describe our system used for SemEval) Subtasks A and B.", "labels": [], "entities": [{"text": "SemEval", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9510213136672974}]}, {"text": "Subtask A (Message Polarity Classification) requires classifying a tweet's sentiment as positive; negative; or neutral,.", "labels": [], "entities": []}, {"text": "Subtask B (Tweet classification according to a two-point scale) requires classifying a tweet's sentiment given a topic as positive or negative.", "labels": [], "entities": [{"text": "Tweet classification", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.8446063995361328}]}, {"text": "Our system uses a GRU neural network model () with one hidden layer on top of two sets of word embeddings that are slightly fine-tuned on each training set (see).", "labels": [], "entities": []}, {"text": "The first set of word embeddings is considered as general purpose embeddings and was obtained by training word2vec (Mikolov et al., 2013) on 20.5 million tweets that we crawled for this purpose.", "labels": [], "entities": []}, {"text": "The second set of word embeddings is considered as task specific set, and was obtained by training on a supervised sentiment analysis dataset using another GRU model.", "labels": [], "entities": []}, {"text": "We also added a method for analyzing multi-words hashtags by splitting them and appending them to the body of the tweet before feeding it to the GRU model.", "labels": [], "entities": []}, {"text": "In our experiments we tried both keeping the word embeddings static during the training or fine-tuning them and reported the result for each experiment.", "labels": [], "entities": []}, {"text": "We achieved 0.58 F1-measure for Subtask A (ranked 12/34) and 0.679 Recall for Subtask B (ranked 12/19).", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9986671209335327}, {"text": "Recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9972960352897644}]}], "datasetContent": [{"text": "In order to train and test our model for Subtask A, we used the dataset provided for SemEval-2016 Task 4 and SemEval-2013 Task 2.", "labels": [], "entities": []}, {"text": "We obtained 8,978 from the first dataset and 7,130 from the second, the remaining tweets were not available.", "labels": [], "entities": []}, {"text": "So, we ended up with a dataset of 16,108 tweets.", "labels": [], "entities": []}, {"text": "Regarding Subtask B we obtained 6,324 from SemEval-2016 provided dataset.", "labels": [], "entities": [{"text": "SemEval-2016 provided dataset", "start_pos": 43, "end_pos": 72, "type": "DATASET", "confidence": 0.8830764492352804}]}, {"text": "We partitioned both datasets into train and development portions of ratio 8:2.", "labels": [], "entities": []}, {"text": "shows the distribution of tweets for both Subtasks.", "labels": [], "entities": []}, {"text": "For optimizing our network weights we used Adam (), anew and computationally efficient stochastic optimization method.", "labels": [], "entities": []}, {"text": "All the experiments have been developed using Keras 4 deep learning library with Theano 5 backend and with CUDA enabled.", "labels": [], "entities": [{"text": "Keras 4 deep learning library", "start_pos": 46, "end_pos": 75, "type": "DATASET", "confidence": 0.8048381924629211}, {"text": "Theano 5 backend", "start_pos": 81, "end_pos": 97, "type": "DATASET", "confidence": 0.9292696913083395}]}, {"text": "The model was trained using the default parameters for Adam optimizer, and we tried either to keep the weights of embedding layer static or slightly fine-tune them by using a dropout probability equal to 0.9.", "labels": [], "entities": []}, {"text": "shows our results on the development part of the data set for Subtask A and B where we report the official performance measure for both subtasks).", "labels": [], "entities": []}, {"text": "From 3 the results it is shown that fine-tuning word embeddings with hashtags splitting gives the best results on the development set.", "labels": [], "entities": [{"text": "hashtags splitting", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7336475253105164}]}, {"text": "All our experiments were performed on a machine with Intel Core i7-4770 CPU @ 3.40GHz   of RAM and GeForce GT 640 GPU.", "labels": [], "entities": [{"text": "GeForce GT 640 GPU", "start_pos": 99, "end_pos": 117, "type": "DATASET", "confidence": 0.8467091023921967}]}, {"text": "shows our individual results on different SemEval datasets.", "labels": [], "entities": [{"text": "SemEval datasets", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.8975921869277954}]}, {"text": "shows our results for Subtask B. From the results and our rank in both Subtasks, we noticed that our system was not satisfactory compared to other teams this was due to the following reasons: 1.", "labels": [], "entities": []}, {"text": "We used the development set to validate our model in order to find the best learning parameters, However we mistakenly used the learning accuracy to find the optimal learning parameters especially the number of the training epochs.", "labels": [], "entities": []}, {"text": "This significantly affected our rank based on the official performance measure.", "labels": [], "entities": []}, {"text": "show the old and the new results after fixing this bug.", "labels": [], "entities": []}, {"text": "2. Most of the participating teams in this year competition used deep learning models and they used huge datasets (more than 50M tweets) to train and refine word embeddings according to the emotions of the tweet.", "labels": [], "entities": []}, {"text": "However, we only used 1.5M from sentiment140 corpus to generate task-specific embeddings.", "labels": [], "entities": [{"text": "sentiment140 corpus", "start_pos": 32, "end_pos": 51, "type": "DATASET", "confidence": 0.975585401058197}]}, {"text": "3. The model used for generating the task-specific embeddings for Subtask A should be trained on three classes not only two (positive, negative, and neutral) where if the tweet contains positive emotions like \":)\" should be positive, if it contains negative emotions like \":(\" should be negative, and if it contains both or none it should be neutral.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Tweets distribution for Subtask A and B", "labels": [], "entities": []}, {"text": " Table 3: Development results for Subtask A and B. Note: av-", "labels": [], "entities": []}, {"text": " Table 4: Results for Subtask A on different SemEval datasets.", "labels": [], "entities": [{"text": "SemEval datasets", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.834484875202179}]}, {"text": " Table 5: Result for Subtask B on SemEval 2016 dataset.", "labels": [], "entities": [{"text": "SemEval 2016 dataset", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.7762698729832967}]}]}