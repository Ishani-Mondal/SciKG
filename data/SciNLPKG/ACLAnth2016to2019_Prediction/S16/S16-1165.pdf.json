{"title": [{"text": "SemEval-2016 Task 12: Clinical TempEval", "labels": [], "entities": [{"text": "SemEval-2016 Task 12", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8817371129989624}]}], "abstractContent": [{"text": "Clinical TempEval 2016 evaluated temporal information extraction systems on the clinical domain.", "labels": [], "entities": [{"text": "temporal information extraction", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.6233782370885214}]}, {"text": "Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification.", "labels": [], "entities": [{"text": "time expression identification", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.6469499965508779}, {"text": "event expression identification", "start_pos": 83, "end_pos": 114, "type": "TASK", "confidence": 0.6657947897911072}, {"text": "temporal relation identification", "start_pos": 119, "end_pos": 151, "type": "TASK", "confidence": 0.6239589750766754}]}, {"text": "Participant systems were trained and evaluated on a corpus of clinical and pathology notes from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain.", "labels": [], "entities": [{"text": "TimeML", "start_pos": 144, "end_pos": 150, "type": "DATASET", "confidence": 0.9369775056838989}]}, {"text": "14 teams submitted a total of 40 system runs, with the best systems achieving near-human performance on identifying events and times.", "labels": [], "entities": []}, {"text": "On identifying temporal relations, there was a gap between the best systems and human performance, but the gap was less than half the gap of Clinical TempEval 2015.", "labels": [], "entities": [{"text": "Clinical TempEval 2015", "start_pos": 141, "end_pos": 163, "type": "DATASET", "confidence": 0.6861725052197775}]}], "introductionContent": [{"text": "The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (.", "labels": [], "entities": [{"text": "temporal information extraction", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.6133246521155039}]}, {"text": "Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations.", "labels": [], "entities": []}, {"text": "However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles.", "labels": [], "entities": []}, {"text": "In recent years, the community has moved toward testing such information extraction systems on clinical data) to broaden our understanding of the language of time beyond newswire expressions and structure.", "labels": [], "entities": []}, {"text": "Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, reliable and repeatable evaluation.", "labels": [], "entities": []}, {"text": "Participating systems are expected to take as input raw text, for example: April 23, 2014: The patient did not have any postoperative bleeding so we'll resume chemotherapy with a larger bolus on Friday even if there is slight nausea.", "labels": [], "entities": []}, {"text": "The systems are then expected to output annotations over the text, for example, those shown in.", "labels": [], "entities": []}, {"text": "That is, the systems should identify the time expressions, event expressions, attributes of those expressions, and temporal relations between them.", "labels": [], "entities": []}, {"text": "Clinical TempEval 2016 addressed one of the major challenges in Clinical TempEval 2015: data distribution.", "labels": [], "entities": [{"text": "Clinical TempEval 2016", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.8078078428904215}, {"text": "data distribution", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.6743196398019791}]}, {"text": "Because Clinical TempEval is based on real patient notes from the Mayo Clinic, participants go through a lengthy authorization process involving a data use agreement and an interview.", "labels": [], "entities": []}, {"text": "For Clinical TempEval 2016, we streamlined this process and were able to authorize data access for more than twice as many participants as Clinical TempEval 2015.", "labels": [], "entities": []}, {"text": "And since all the training and evaluation data distributed for Clinical TempEval 2015 was used as the training data for Clinical TempEval 2016, participants had more than a year to work on their systems.", "labels": [], "entities": [{"text": "Clinical TempEval 2015", "start_pos": 63, "end_pos": 85, "type": "DATASET", "confidence": 0.7930878003438314}]}, {"text": "The result was that four times as many teams participated.", "labels": [], "entities": []}], "datasetContent": [{"text": "All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F 1 : where S is the set of items predicted by the system and H is the set of items annotated by the humans.", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.9369404464960098}, {"text": "recall (R)", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9601085335016251}, {"text": "F 1", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9929302334785461}]}, {"text": "Applying these metrics only requires a definition of what is considered an \"item\" for each task.", "labels": [], "entities": []}, {"text": "\u2022 For evaluating the spans of event expressions or time expressions, items were tuples of (begin, end) character offsets.", "labels": [], "entities": []}, {"text": "Thus, systems only received credit for identifying events and times with exactly the same character offsets as the manually annotated ones.", "labels": [], "entities": []}, {"text": "\u2022 For evaluating the attributes of event expressions or time expressions -Class, Contextual Modality, Degree, Polarity and Type -items were tuples of (begin, end, value) where begin and end are character offsets and value is the value that was given to the relevant attribute.", "labels": [], "entities": [{"text": "Degree", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9654521346092224}]}, {"text": "Thus, systems only received credit for an event (or time) attribute if they both found an event (or time) with the correct character offsets and then assigned the correct value for that attribute.", "labels": [], "entities": []}, {"text": "\u2022 For relations between events and the document creation time, items were tuples of (begin, end, value), just as if it were an event attribute.", "labels": [], "entities": []}, {"text": "Thus, systems only received credit if they found a correct event and assigned the correct relation (BEFORE, OVERLAP, BEFORE-OVERLAP or AFTER) between that event and the document creation time.", "labels": [], "entities": [{"text": "BEFORE", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9939462542533875}, {"text": "OVERLAP", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.771341860294342}, {"text": "BEFORE-OVERLAP", "start_pos": 117, "end_pos": 131, "type": "METRIC", "confidence": 0.9924155473709106}, {"text": "AFTER)", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9278391003608704}]}, {"text": "In the second phase of the evaluation, when manual event annotations were provided as input, only recall (which in this case is equivalent to standard classification accuracy) is reported.", "labels": [], "entities": [{"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9995481371879578}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9129343032836914}]}, {"text": "\u2022 For narrative container relations, items were tuples of ((begin 1 , end 1 ), (begin 2 , end 2 )), where the begins and ends corresponded to the character offsets of the events or times participating in the relation.", "labels": [], "entities": []}, {"text": "Thus, systems only received credit fora narrative container relation if they found both events/times and correctly assigned a CONTAINS relation between them.", "labels": [], "entities": []}, {"text": "For event and time attributes, we also measure how accurately a system predicts the attribute values on just those events or times that the system predicted.", "labels": [], "entities": []}, {"text": "The goal here is to allow a comparison across systems for assigning attribute values, even when different systems produce different numbers of events and times.", "labels": [], "entities": []}, {"text": "This metric is calculated by dividing the F 1 on the attribute by the F 1 on identifying the spans: For narrative container relations, the P and R definitions were modified to take into account temporal closure, where additional relations are deterministically inferred from other relations (e.g., A CONTAINS B and B CONTAINS C, so A CONTAINS C): Similar measures were used in prior work (UzZaman and Allen, 2011) and, following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of human-annotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure).", "labels": [], "entities": [{"text": "F 1", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9561760127544403}, {"text": "precision", "start_pos": 448, "end_pos": 457, "type": "METRIC", "confidence": 0.9983422756195068}, {"text": "recall", "start_pos": 665, "end_pos": 671, "type": "METRIC", "confidence": 0.9958729147911072}]}], "tableCaptions": [{"text": " Table 1: Number of documents, event expressions, time expres-", "labels": [], "entities": []}, {"text": " Table 2: System performance and annotator agreement on TIMEX3 tasks: identifying the time expression's span (character offsets)", "labels": [], "entities": []}, {"text": " Table 3: System performance and annotator agreement on EVENT tasks: identifying the event expression's span (character offsets),", "labels": [], "entities": []}, {"text": " Table 4: System performance and annotator agreement on temporal relation tasks: identifying relations between events and the", "labels": [], "entities": []}]}