{"title": [{"text": "SemEval-2016 Task 6: Detecting Stance in Tweets", "labels": [], "entities": [{"text": "Detecting Stance in Tweets", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.918211355805397}]}], "abstractContent": [], "introductionContent": [{"text": "Stance detection is the task of automatically determining from text whether the author of the text is in favor of, against, or neutral towards a proposition or target.", "labels": [], "entities": [{"text": "Stance detection is the task of automatically determining from text whether the author of the text is in favor of, against, or neutral towards a proposition or target", "start_pos": 0, "end_pos": 166, "type": "Description", "confidence": 0.7510074645280838}]}, {"text": "The target maybe a person, an organization, a government policy, a movement, a product, etc.", "labels": [], "entities": []}, {"text": "For example, one can infer from Barack Obama's speeches that he is in favor of stricter gun laws in the US.", "labels": [], "entities": []}, {"text": "Similarly, people often express stance towards various target entities through posts on online forums, blogs, Twitter, Youtube, Instagram, etc.", "labels": [], "entities": []}, {"text": "Automatically detecting stance has widespread applications in information retrieval, text summarization, and textual entailment.", "labels": [], "entities": [{"text": "Automatically detecting stance", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8110659122467041}, {"text": "information retrieval", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.8157524466514587}, {"text": "text summarization", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.7935324311256409}, {"text": "textual entailment", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.797721266746521}]}, {"text": "The task we explore is formulated as follows: given a tweet text and a target entity (person, organization, movement, policy, etc.), automatic natural language systems must determine whether the tweeter is in favor of the given target, against the given target, or whether neither inference is likely.", "labels": [], "entities": []}, {"text": "For example, consider the target-tweet pair: Target: legalization of abortion (1) Tweet: The pregnant are more than walking incubators, and have rights!", "labels": [], "entities": []}, {"text": "We can deduce from the tweet that the tweeter is likely in favor of the target.", "labels": [], "entities": []}, {"text": "We annotated 4870 English tweets for stance towards six commonly known targets in the United States.", "labels": [], "entities": []}, {"text": "The data corresponding to five of the targets ('Atheism', 'Climate Change is a Real Concern', 'Feminist Movement', 'Hillary Clinton', and 'Legalization of Abortion') was used in a standard supervised stance detection task -Task A. About 70% of the tweets per target were used for training and the remaining for testing.", "labels": [], "entities": [{"text": "Legalization of Abortion')", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.8360635787248611}, {"text": "stance detection task -Task A.", "start_pos": 200, "end_pos": 230, "type": "TASK", "confidence": 0.8349393308162689}]}, {"text": "All of the data corresponding to the target 'Donald Trump' was used as test set in a separate task -Task B. No training data labeled with stance towards 'Donald Trump' was provided.", "labels": [], "entities": []}, {"text": "However, participants were free to use data from Task A to develop their models for Task B. Task A received submissions from 19 teams, wherein the highest classification F-score obtained was 67.82.", "labels": [], "entities": [{"text": "F-score", "start_pos": 170, "end_pos": 177, "type": "METRIC", "confidence": 0.805815577507019}]}, {"text": "Task B, which is particularly challenging due to lack of training data, received submissions from 9 teams wherein the highest classification F-score obtained was 56.28.", "labels": [], "entities": [{"text": "F-score", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.9457281827926636}]}, {"text": "The best performing systems used standard text classification features such as those drawn from n-grams, word vectors, and sentiment lexicons.", "labels": [], "entities": []}, {"text": "Some teams drew additional gains from noisy stance-labeled data created using distant supervision techniques.", "labels": [], "entities": []}, {"text": "A large number of teams used word embeddings and some used deep neural networks such as RNNs and convolutional neural nets.", "labels": [], "entities": []}, {"text": "Nonetheless, for Task A, none of these systems surpassed a baseline SVM classifier that uses word and character n-grams as features ().", "labels": [], "entities": []}, {"text": "Further, results are markedly worse for instances where the target of interest is not the target of opinion.", "labels": [], "entities": []}, {"text": "More gains can be expected in the future on both tasks, as researchers better understand this new task and data.", "labels": [], "entities": []}, {"text": "All of the data, an interactive visualization of the data, and the evaluation scripts are available on the task website as well as the homepage for this Stance project.", "labels": [], "entities": []}], "datasetContent": [{"text": "The stance annotations we use are described in detail in.", "labels": [], "entities": []}, {"text": "The same dataset was subsequently also annotated for target of opinion and sentiment (in addition to stance towards a given target) ().", "labels": [], "entities": []}, {"text": "These additional annotations are not part of the SemEval-2016 competition, but are made available for future research.", "labels": [], "entities": [{"text": "SemEval-2016 competition", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.6262947022914886}]}, {"text": "We summarize below all relevant details for this shared task: how we compiled a set of tweets and targets for stance annotation (Section 3.1), the questionnaire and crowdsourcing setup used for stance annotation, and an analysis of the stance annotations (Section 3.3).", "labels": [], "entities": [{"text": "stance annotation", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.8581367135047913}, {"text": "stance annotation", "start_pos": 194, "end_pos": 211, "type": "TASK", "confidence": 0.9052934348583221}]}, {"text": "We used the macro-average of the F1-score for 'favor' and the F1-score for 'against' as the bottom-line evaluation metric.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9974468946456909}, {"text": "F1-score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9953169822692871}]}, {"text": "where F favor and F against are calculated as shown below: Note that the evaluation measure does not disregard the 'neither' class.", "labels": [], "entities": [{"text": "F", "start_pos": 6, "end_pos": 7, "type": "METRIC", "confidence": 0.963703453540802}]}, {"text": "By taking the average F-score for only the 'favor' and 'against' classes, we treat 'neither' as a class that is not of interest-or 'negative' class in Information Retrieval (IR) terms.", "labels": [], "entities": [{"text": "F-score", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9976314306259155}, {"text": "Information Retrieval (IR)", "start_pos": 151, "end_pos": 177, "type": "TASK", "confidence": 0.8140114188194275}]}, {"text": "Falsely labeling negative class instances still adversely affects the scores of this metric.", "labels": [], "entities": []}, {"text": "If one uses simple accuracy as the evaluation metric, and if the negative class is very dominant (as is the casein IR), then simply labeling every instance with the negative class will obtain very high scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.997601330280304}]}, {"text": "If one randomly accesses tweets, then the probability that one can infer 'favor' or 'against' stance towards a pre-chosen target of interest is small.", "labels": [], "entities": []}, {"text": "This has motivated the IR-like metric used in this competition, even though we worked hard to have marked amounts of 'favor' and 'against' data in our training and test sets.", "labels": [], "entities": [{"text": "IR-like metric", "start_pos": 23, "end_pos": 37, "type": "METRIC", "confidence": 0.8291384279727936}]}, {"text": "This metric is also similar to how sentiment prediction was evaluated in recent SemEval competitions.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.9665698111057281}, {"text": "SemEval", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.9616511464118958}]}, {"text": "This evaluation metric can be seen as a microaverage of F-scores across targets (F-microT).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.975176215171814}]}, {"text": "Alternatively, one could determine the mean of the F avg scores for each of the targets-the macro average across targets (F-macroT).", "labels": [], "entities": [{"text": "F avg scores", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.9605839451154073}, {"text": "F-macroT)", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9362693727016449}]}, {"text": "Even though not the official competition metric, the F-macroT can easily be determined from the per-target F avg scores shown in the result tables of Section 5.", "labels": [], "entities": [{"text": "F-macroT", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9935698509216309}, {"text": "F avg", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.8953410685062408}]}, {"text": "The participants were provided with an evaluation script so that they could check the format of their submission and determine performance when gold labels were available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of instances in the Stance Train and Test sets for Task A and Task B.", "labels": [], "entities": [{"text": "Stance Train and Test sets", "start_pos": 43, "end_pos": 69, "type": "DATASET", "confidence": 0.7908261179924011}]}, {"text": " Table 2: Results for Task A, reporting the official competition metric as 'Overall Favg', along with F f avor and Fagainst over all", "labels": [], "entities": [{"text": "Favg", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.5141719579696655}, {"text": "F", "start_pos": 102, "end_pos": 103, "type": "METRIC", "confidence": 0.9763953685760498}, {"text": "Fagainst", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.8645346164703369}]}, {"text": " Table 3: Results for Task A (the official competition metric", "labels": [], "entities": []}, {"text": " Table 4: Results for Task B, reporting the official competition", "labels": [], "entities": []}, {"text": " Table 5: Results for Task B (the official competition metric", "labels": [], "entities": []}]}