{"title": [{"text": "IHS-RD-Belarus at SemEval-2016 Task 1: Multistage Approach for Measuring Semantic Similarity", "labels": [], "entities": [{"text": "Similarity", "start_pos": 82, "end_pos": 92, "type": "TASK", "confidence": 0.6288536190986633}]}], "abstractContent": [{"text": "This paper describes the system for rating the degree of semantic equivalence between two text snippets developed by IHS-RD-Belarus for the SemEval 2016 STS shared task (Task 1).", "labels": [], "entities": [{"text": "SemEval 2016 STS shared task", "start_pos": 140, "end_pos": 168, "type": "TASK", "confidence": 0.889892828464508}]}, {"text": "To predict the human ratings of text similarity we use a support vector regression model with multiple features representing similarity and difference scores calculated for each pair of sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Measuring semantic equivalence between two texts has become an emerging research subject in recent years.", "labels": [], "entities": [{"text": "Measuring semantic equivalence between two texts", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7602617392937342}]}, {"text": "Graded textual similarity notion can be applied to a wide range of NLP tasks such as paraphrase recognition, automatic machine translation evaluation, question answering, text summarization, information retrieval, etc.", "labels": [], "entities": [{"text": "Graded textual similarity notion", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6313816606998444}, {"text": "paraphrase recognition", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.8695231378078461}, {"text": "automatic machine translation evaluation", "start_pos": 109, "end_pos": 149, "type": "TASK", "confidence": 0.7055057883262634}, {"text": "question answering", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.8828759491443634}, {"text": "text summarization", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.7404400706291199}, {"text": "information retrieval", "start_pos": 191, "end_pos": 212, "type": "TASK", "confidence": 0.8124369978904724}]}, {"text": "The SemEval STS shared task has been held annually since 2012 () attracting numerous participating teams with various approaches, such as alignment of related content word sequences (, measuring similarity between vector representations of texts and using machine learning algorithms for computing multiple lexical, syntactic and semantic features.", "labels": [], "entities": [{"text": "SemEval STS shared task", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8549761772155762}, {"text": "alignment of related content word sequences", "start_pos": 138, "end_pos": 181, "type": "TASK", "confidence": 0.8669042189915975}]}, {"text": "In this article, we present the system developed by IHS-RD-Belarus for automated measuring of semantic similarity between two sentences using support vector regression (SVR) implemented in LIBSVM toolbox) with multiple features representing similarity and difference scores calculated for each pair of sentences.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the task and the data used to train our system.", "labels": [], "entities": []}, {"text": "Section 3 describes in detail the features used by our system.", "labels": [], "entities": []}, {"text": "Results and a short conclusion are presented in Section 4 and 5, respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "Gold standard scores are averaged over multiple human annotations.", "labels": [], "entities": []}, {"text": "Performance of the systems is assessed by computing the Pearson correlation between machine assigned semantic similarity scores and gold standard scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 56, "end_pos": 75, "type": "METRIC", "confidence": 0.9680249691009521}]}], "tableCaptions": [{"text": " Table 1: Performance on the 2016 STS Test Set", "labels": [], "entities": [{"text": "STS Test Set", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.8721211552619934}]}]}