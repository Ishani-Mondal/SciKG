{"title": [{"text": "CMU at SemEval-2016 Task 8: Graph-based AMR Parsing with Infinite Ramp Loss", "labels": [], "entities": [{"text": "CMU", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8318468332290649}, {"text": "AMR Parsing", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.8452751040458679}]}], "abstractContent": [{"text": "We present improvements to the JAMR parser as part of the SemEval 2016 Shared Task 8 on AMR parsing.", "labels": [], "entities": [{"text": "JAMR parser", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.58921317756176}, {"text": "SemEval 2016 Shared Task 8", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.7191420197486877}, {"text": "AMR parsing", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.855596125125885}]}, {"text": "The major contributions are: improved concept coverage using external resources and features, an improved aligner, and a novel loss function for structured prediction called infinite ramp, which is a generalization of the structured SVM to problems with un-reachable training instances.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our entry to the SemEval 2016 Shared Task 8 is a set of improvements to the system presented in.", "labels": [], "entities": [{"text": "SemEval 2016 Shared Task 8", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.6789023637771606}]}, {"text": "The improvements are: a novel training loss function for structured prediction, which we call \"infinite ramp,\" new sources for concepts, improved features, and improvements to the rule-based aligner in.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.6846411526203156}]}, {"text": "The overall architecture of the system and the decoding algorithms for concept identification and relation identification are unchanged from, and we refer readers seeking a complete understanding of the system to that paper.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.7458164095878601}, {"text": "relation identification", "start_pos": 98, "end_pos": 121, "type": "TASK", "confidence": 0.8564640581607819}]}], "datasetContent": [{"text": "Following the recommended train/dev./test split of LDC2015E86, our parser achieves 70% precision, 65% recall, and 67% F 1 Smatch on the LDC2015E86 test set.", "labels": [], "entities": [{"text": "LDC2015E86", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.9599953293800354}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9995356798171997}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9997428059577942}, {"text": "F 1 Smatch", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.8725405931472778}, {"text": "LDC2015E86 test set", "start_pos": 136, "end_pos": 155, "type": "DATASET", "confidence": 0.9745073715845743}]}, {"text": "The JAMR baseline on this same dataset is 55% F 1 Smatch, so the improvements are quite substantial.", "labels": [], "entities": [{"text": "JAMR baseline", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.6082755029201508}, {"text": "F 1 Smatch", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9288109143575033}]}, {"text": "On the SemEval 2016 Task 8 test set, our improved parser achieves 56% F 1 Smatch.", "labels": [], "entities": [{"text": "SemEval 2016 Task 8 test set", "start_pos": 7, "end_pos": 35, "type": "DATASET", "confidence": 0.7084214935700098}, {"text": "F 1 Smatch", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9331454833348592}]}, {"text": "We hypothesize that the lower performance of the parser on the SemEval Task 8 test set is due to drift in the AMR annotation scheme between the production of the LDC2015E86 training data and the SemEval test set.", "labels": [], "entities": [{"text": "SemEval Task 8 test set", "start_pos": 63, "end_pos": 86, "type": "DATASET", "confidence": 0.7285870790481568}, {"text": "LDC2015E86 training data", "start_pos": 162, "end_pos": 186, "type": "DATASET", "confidence": 0.902179479598999}, {"text": "SemEval test set", "start_pos": 195, "end_pos": 211, "type": "DATASET", "confidence": 0.8253171841303507}]}, {"text": "During that time, there were changes to the concept senses and the concept frame files.", "labels": [], "entities": []}, {"text": "Because the improvements in our parser were due to boosting recall in concept identification (and using the frame files to our advantage), our approach does not show as large improvements on the SemEval test set as on the LDC2015E86 test set.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9990662932395935}, {"text": "concept identification", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.7466589212417603}, {"text": "SemEval test set", "start_pos": 195, "end_pos": 211, "type": "DATASET", "confidence": 0.7918571333090464}, {"text": "LDC2015E86 test set", "start_pos": 222, "end_pos": 241, "type": "DATASET", "confidence": 0.9588584701220194}]}], "tableCaptions": []}