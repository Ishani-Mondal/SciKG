{"title": [{"text": "iLab-Edinburgh at SemEval-2016 Task 7: A Hybrid Approach for Determining Sentiment Intensity of Arabic Twitter Phrases", "labels": [], "entities": [{"text": "iLab-Edinburgh at SemEval-2016 Task 7", "start_pos": 0, "end_pos": 37, "type": "DATASET", "confidence": 0.8457958102226257}, {"text": "Determining Sentiment Intensity of Arabic Twitter Phrases", "start_pos": 61, "end_pos": 118, "type": "TASK", "confidence": 0.8267534119742257}]}], "abstractContent": [{"text": "This paper describes the iLab-Edinburgh Sentiment Analysis system, winner of the Ara-bic Twitter Task 7 in SemEval-2016.", "labels": [], "entities": [{"text": "iLab-Edinburgh Sentiment Analysis", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.6986051003138224}, {"text": "Ara-bic Twitter Task 7 in SemEval-2016", "start_pos": 81, "end_pos": 119, "type": "DATASET", "confidence": 0.7438043256600698}]}, {"text": "The system employs a hybrid approach of supervised learning and rule-based methods to predict a sentiment intensity (SI) score fora given Arabic Twitter phrase.", "labels": [], "entities": [{"text": "predict a sentiment intensity (SI) score", "start_pos": 86, "end_pos": 126, "type": "METRIC", "confidence": 0.7063379026949406}]}, {"text": "First, the supervised method uses an ensemble of trained linear regression models to produce an initial SI score for each given text instance.", "labels": [], "entities": []}, {"text": "Second, the resulting SI score is adjusted using a set of rules that exploit a number of publicly available sentiment lexica.", "labels": [], "entities": [{"text": "SI", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9563312530517578}]}, {"text": "The system demonstrates strong results of 0.536 Kendall score, ranking top in this task.", "labels": [], "entities": [{"text": "Kendall score", "start_pos": 48, "end_pos": 61, "type": "METRIC", "confidence": 0.9565636515617371}]}], "introductionContent": [{"text": "Sentiment Analysis (SA) concerns the automatic extraction and classification of sentiment-related information from a given text instance (.", "labels": [], "entities": [{"text": "Sentiment Analysis (SA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9147289991378784}, {"text": "automatic extraction and classification of sentiment-related information from a given text instance", "start_pos": 37, "end_pos": 136, "type": "TASK", "confidence": 0.8314533084630966}]}, {"text": "This is the first time SA on Arabic text is considered in an international competition, like SemEval.", "labels": [], "entities": [{"text": "SA", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9363343119621277}]}, {"text": "Most of previous work on SA is in English, but there have been recent attempts to address SA for Arabic, e.g. (. Previous work in this area has mainly focused on identifying the sentiment polarity in a given tweet/phrase, whereas within SemEval-2016 Task 7, the task is to predict the Sentiment Intensity (SI) in Arabic tweets.", "labels": [], "entities": [{"text": "Sentiment Intensity (SI)", "start_pos": 285, "end_pos": 309, "type": "METRIC", "confidence": 0.7774271070957184}]}, {"text": "That is, in addition to their prior association to a sentiment class, i.e. positive or negative, each text instance has an SI score that indicates the strength of its assigned sentiment on a scale from 0 to 1.", "labels": [], "entities": []}, {"text": "In this work, we use a combination of supervised learning and rule-based approaches, exploiting a number of publicly available sentiment lexica.", "labels": [], "entities": []}, {"text": "We find that the quality (rather than quantity) of these lexica influence system performance for the supervised part of the system.", "labels": [], "entities": []}, {"text": "Our best performing system demonstrates strong results of 0.536 Kendall score, ranking top in.", "labels": [], "entities": [{"text": "Kendall score", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.9333536028862}]}, {"text": "This type of hybrid approach between rule-based and statistical methods has been demonstrated to be successful in other shared tasks, such as dialogue state tracking ().", "labels": [], "entities": [{"text": "dialogue state tracking", "start_pos": 142, "end_pos": 165, "type": "TASK", "confidence": 0.6919750968615214}]}], "datasetContent": [{"text": "We experimented using different combinations of lexical features (word-lemma unigrams) to train the LR models used in the 1st part of the system.", "labels": [], "entities": []}, {"text": "The 2nd part is fixed throughout.", "labels": [], "entities": []}, {"text": "The reported results are the final outcomes for the entire system, i.e. after adjusting IS scores in phase 2.", "labels": [], "entities": [{"text": "IS scores", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9349527657032013}]}, {"text": "Overall, we recorded an average improvement of 14% for applying the 2nd phase of the system.", "labels": [], "entities": []}, {"text": "We report on Kendall's rank correlation coefficient (\u2327 ) and Spearman's coefficient (\u21e2) to account for SI ordering.", "labels": [], "entities": [{"text": "Kendall's rank correlation coefficient (\u2327 )", "start_pos": 13, "end_pos": 56, "type": "METRIC", "confidence": 0.7101567472730365}, {"text": "Spearman's coefficient (\u21e2)", "start_pos": 61, "end_pos": 87, "type": "METRIC", "confidence": 0.7753651887178421}, {"text": "SI ordering", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.9167586863040924}]}, {"text": "Further details of the task, data and competing systems can be found in the task description paper (.", "labels": [], "entities": []}, {"text": "System 1: (official submission to SemEval-16) This version of the system attained the best performance at a Kendall score of 0.5362 using lexical features based on the LabMT lexicon.", "labels": [], "entities": [{"text": "Kendall score", "start_pos": 108, "end_pos": 121, "type": "METRIC", "confidence": 0.9684623181819916}, {"text": "LabMT lexicon", "start_pos": 168, "end_pos": 181, "type": "DATASET", "confidence": 0.9628280401229858}]}, {"text": "This version has entered the competition and won Task 7.", "labels": [], "entities": []}, {"text": "System 2: This version uses features based on the Ar-tweet lexicon, which have resulted in a significantly (p <0.05) lower performance as compared to LabMT at 0.0243 \u2327 . A possible explanation for the performance variation is that the Ar-tweet lexicon is an auto-generated one.", "labels": [], "entities": []}, {"text": "This auto-generation makes it prone to the inclusion of 'indirect' sentiment indicators (i.e. indicators which are merely inferred by the SVM model), because, for example, they are likely to appear in a negative political context.", "labels": [], "entities": []}, {"text": "For instance, the SVM model assigned a strong negative weight of \ud97b\udf590.78 for the feature Bashar AlAsad, which is currently occurring in the context of the Syrian civil war.", "labels": [], "entities": []}, {"text": "argue that such a feature can become outdated/irrelevant at a different point in time.", "labels": [], "entities": []}, {"text": "Furthermore, human annotators, such as those recruited to annotate SemEval's test-set, are more likely to assign a neutral SI score to a feature like Bashar Al-Asad.", "labels": [], "entities": []}, {"text": "In future, we will explore setting threshold values to filter features with lower SVM-weights as a mechanism to avoid the presence of indirect sentiment-bearing features.", "labels": [], "entities": []}, {"text": "System 3: Using SLSA, this version of the system is able to attain a comparable score to that recorded  with LabMT.", "labels": [], "entities": [{"text": "LabMT", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.9403097033500671}]}, {"text": "Although auto-generated (see section 4.1), SLSA is able to attain a Kendall score of up to 0.5244.", "labels": [], "entities": [{"text": "SLSA", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.6527879238128662}, {"text": "Kendall score", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.9863130152225494}]}, {"text": "SLSA has the advantage of being more than 3 times larger in size than LabMT (System 1) and Ar-tweet (System 2).", "labels": [], "entities": [{"text": "LabMT", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.916936457157135}, {"text": "Ar-tweet", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.817255973815918}]}, {"text": "In addition, the auto-generated SI scores in SLSA differs from the ones generated with SVM in Ar-tweet (System 2), as the former relies on linking Arabic entries to their corresponding English synset in SentiWordNet ().", "labels": [], "entities": [{"text": "SI", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9394766688346863}]}, {"text": "Furthermore, being only based on MSA, SLSA can be assumed to be less noisy, especially when mapped to lemma-form, compared to slang and spelling variation in DA.", "labels": [], "entities": [{"text": "SLSA", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.943602979183197}]}, {"text": "System 4: Combining LabMT and Ar-tweet has resulted in a comparable performance to System 3 at a Kendall score of 0.5261.", "labels": [], "entities": [{"text": "Ar-tweet", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9640253186225891}, {"text": "Kendall score", "start_pos": 97, "end_pos": 110, "type": "METRIC", "confidence": 0.9705864191055298}]}, {"text": "However, this system is still notable to outperform that using LabMT on its own (see System 1).", "labels": [], "entities": [{"text": "LabMT", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.9497241973876953}]}, {"text": "A possible explanation is that the presence of Ar-tweet results in introducing more noise than improving coverage of features (see System 2), resulting in slight degrading below the score attained by LabMT on its own (System 1).", "labels": [], "entities": [{"text": "Ar-tweet", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9459949135780334}]}, {"text": "System 5: Combining LabMT and SLSA has resulted in a slight improvement over using SLSA on its own, but still not competing with the performance of LabMT.", "labels": [], "entities": []}, {"text": "However, when only considering phase 1 on its own, LabMT+SLSA performs best at a Kendall score of 0.377.", "labels": [], "entities": [{"text": "LabMT+SLSA", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.729353686173757}, {"text": "Kendall score", "start_pos": 81, "end_pos": 94, "type": "METRIC", "confidence": 0.9746197760105133}]}, {"text": "In future work, we plan to investigate possible interactions between SLSA and the lexica used in phase 2.", "labels": [], "entities": [{"text": "SLSA", "start_pos": 69, "end_pos": 73, "type": "TASK", "confidence": 0.8737728595733643}]}, {"text": "System 6: Finally, combining all the training data still cannot reach the performance of LabMT on its own.", "labels": [], "entities": [{"text": "LabMT", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.8654892444610596}]}, {"text": "It is also interesting to note that adding the auto-generated Ar-tweet caused a slight drop in Kendal score, compared to only using LabMT+SLSA (System 5).", "labels": [], "entities": [{"text": "Ar-tweet", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.8863044381141663}, {"text": "Kendal score", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.5555462539196014}, {"text": "LabMT+SLSA", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.8159183065096537}]}], "tableCaptions": [{"text": " Table 1: Examples of the most predictive word uni-grams in the Ar-tweet data-set as evaluated by an SVM.", "labels": [], "entities": [{"text": "Ar-tweet data-set", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.7896353006362915}]}, {"text": " Table 2. The reported results are the final out- comes for the entire system, i.e. after adjusting IS scores in phase 2. Overall, we recorded an average  improvement of 14% for applying the 2nd phase of  the system. We report on Kendall's rank correlation  coefficient (\u2327 ) and Spearman's coefficient (\u21e2) to ac- count for SI ordering. Further details of the task,  data and competing systems can be found in the task  description paper (", "labels": [], "entities": [{"text": "Kendall's rank correlation  coefficient (\u2327 )", "start_pos": 230, "end_pos": 274, "type": "METRIC", "confidence": 0.7873885716710772}, {"text": "ac- count", "start_pos": 309, "end_pos": 318, "type": "METRIC", "confidence": 0.9674522678057352}, {"text": "SI ordering", "start_pos": 323, "end_pos": 334, "type": "TASK", "confidence": 0.9841769933700562}]}, {"text": " Table 2: Results on SemEval'16 gold-standard test-set using different lexical features for LR models. *System 1 entered the", "labels": [], "entities": [{"text": "SemEval'16 gold-standard test-set", "start_pos": 21, "end_pos": 54, "type": "DATASET", "confidence": 0.7080064813296}]}]}