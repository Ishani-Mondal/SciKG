{"title": [{"text": "Know-Center at SemEval-2016 Task 5: Using Word Vectors with Typed Dependencies for Opinion Target Expression Extraction", "labels": [], "entities": [{"text": "Opinion Target Expression Extraction", "start_pos": 83, "end_pos": 119, "type": "TASK", "confidence": 0.6603279784321785}]}], "abstractContent": [{"text": "This paper describes our participation in SemEval-2016 Task 5 for Subtask 1, Slot 2.", "labels": [], "entities": [{"text": "SemEval-2016 Task 5", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8665347695350647}]}, {"text": "The challenge demands to find domain specific target expressions on sentence level that refer to reviewed entities.", "labels": [], "entities": []}, {"text": "The detection of target words is achieved by using word vectors and their grammatical dependency relationships to classify each word in a sentence into target or non-target.", "labels": [], "entities": []}, {"text": "A heuristic based function then expands the classified target words to the whole target phrase.", "labels": [], "entities": []}, {"text": "Our system achieved an F1 score of 56.816% for this task.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9895026087760925}]}], "introductionContent": [{"text": "Nowadays, modern technologies allow us to collect customer reviews and opinions in away that changed the sheer amount of information available to us.", "labels": [], "entities": []}, {"text": "For that matter the requirement to extract useful knowledge from this data rose up to a point where machine learning algorithms can help to accomplish this much faster and easier than humanly possible.", "labels": [], "entities": []}, {"text": "Natural language processing (NLP) emerges as an interfacing tool between human natural language and many technical fields such as machine learning and information extraction.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7686359335978826}, {"text": "information extraction", "start_pos": 151, "end_pos": 173, "type": "TASK", "confidence": 0.7806381285190582}]}, {"text": "This article describes our approach towards Opinion Target Expression (OTE) extraction as defined by Task 5 for Subtask 1, Slot 2 of the SemEval-2016 () challenge.", "labels": [], "entities": [{"text": "Opinion Target Expression (OTE) extraction", "start_pos": 44, "end_pos": 86, "type": "TASK", "confidence": 0.7378319544451577}]}, {"text": "The core goal behind Slot 2 in Subtask 1 of Task 5 is to extract consecutive words which, by means of a natural language, represent the opinion target expression.", "labels": [], "entities": [{"text": "Slot 2", "start_pos": 21, "end_pos": 27, "type": "TASK", "confidence": 0.8892199993133545}]}, {"text": "The opinion target expression is that part of a sentence which stands for the entity towards which an opinion is being expressed.", "labels": [], "entities": []}, {"text": "An example could be the word \"waitress\" in the sentence \"The waitress was very nice and courteous the entire evening.\".", "labels": [], "entities": []}, {"text": "The evaluation for Slot 2 fell into evaluation phase A, where provided systems were tested in order to return a list of target expressions for each given sentence in a review text.", "labels": [], "entities": [{"text": "Slot 2", "start_pos": 19, "end_pos": 25, "type": "TASK", "confidence": 0.8821932971477509}]}, {"text": "Each target expression was an annotation composed of the index of the starting and end character of the particular expression as well as its corresponding character string.", "labels": [], "entities": []}, {"text": "For our system we decided to used word vectors ().", "labels": [], "entities": []}, {"text": "Word vectors () are distributed representations which are designed to carry contextual information of words if their training meets certain criteria.", "labels": [], "entities": []}, {"text": "We also used typed grammatical dependencies to extract structural information from sentences.", "labels": [], "entities": []}, {"text": "Furthermore we used a sentiment parser to determine the polarity of words.", "labels": [], "entities": [{"text": "sentiment parser", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.6915490478277206}]}], "datasetContent": [{"text": "We determine how well our different features are performing by splitting the train data available and using 80% training and 20% test data.", "labels": [], "entities": []}, {"text": "In the performance on the target-word class of the individual features are shown depicting the performance of classifying single words as targets or non-targets.", "labels": [], "entities": []}, {"text": "The results for the similarly token-based approach outperforms the other approaches.", "labels": [], "entities": []}, {"text": "The weighted average for Token settles at 0.696 and very similar Token + combined typed dependencies at 0.697.", "labels": [], "entities": []}, {"text": "None of the word vector approaches outperforms these two.", "labels": [], "entities": []}, {"text": "Our submitted system is using the individual (directed) typed dependencies and the sentiment information combined with word vectors as features.", "labels": [], "entities": []}, {"text": "The official results for participating unconstrained systems for Slot 2: Opinion Target Extraction can be seen in", "labels": [], "entities": [{"text": "Slot 2: Opinion Target Extraction", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.6662829766670862}]}], "tableCaptions": [{"text": " Table 3: The resulting F1 scores for the target-word class using", "labels": [], "entities": [{"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.996781587600708}]}, {"text": " Table 4: Shown are evaluation F1 scores given by the SemEval-", "labels": [], "entities": [{"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9900660514831543}, {"text": "SemEval-", "start_pos": 54, "end_pos": 62, "type": "TASK", "confidence": 0.6119141578674316}]}, {"text": " Table 5. The table shows the F1-score for  all participating unconstrained systems. Our sys- tem was able to outperform the baseline and a few  others. Considering only unconstrained systems,  Know-Center reached rank 6 out of 10 (excluding  the baseline results).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9993809461593628}]}, {"text": " Table 5: Shown are the official evaluation results for Subtask", "labels": [], "entities": [{"text": "Subtask", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.854736864566803}]}]}