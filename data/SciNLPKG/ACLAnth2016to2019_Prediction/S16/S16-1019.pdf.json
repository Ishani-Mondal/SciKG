{"title": [{"text": "MIB at SemEval-2016 Task 4a: Exploiting lexicon-based features for sentiment analysis in Twitter", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.9144472181797028}]}], "abstractContent": [{"text": "This work presents our team solution for task 4a (Message Polarity Classification) at the Se-mEval 2016 challenge.", "labels": [], "entities": [{"text": "Message Polarity Classification) at the Se-mEval 2016 challenge", "start_pos": 50, "end_pos": 113, "type": "TASK", "confidence": 0.6446431146727668}]}, {"text": "Our experiments have been carried out over the Twitter dataset provided by the challenge.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.7545848786830902}]}, {"text": "We follow a supervised approach, exploiting a SVM polynomial kernel classifier trained with the challenge data.", "labels": [], "entities": []}, {"text": "The classifier takes as input advanced NLP features.", "labels": [], "entities": []}, {"text": "This paper details the features and discusses the achieved results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Revealing the sentiment behind a text is motivated by several reasons, e.g., to figure out how many opinions on a certain topic are positive or negative.", "labels": [], "entities": [{"text": "Revealing the sentiment behind a text", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8446801602840424}]}, {"text": "Also, it could be interesting to span positivity and negativity across a n-point scale.", "labels": [], "entities": []}, {"text": "As an example, a five-point scale is now widespread in digital scenarios where human ratings are involved: Amazon, TripAdvisor, Yelp, and many others, adopt the scale for letting their users rating products and services.", "labels": [], "entities": []}, {"text": "Under the big hat of sentiment analysis, polarity recognition attempts to classify texts into positive or negative, while the rating inference task tries to identify different shades of positivity and negativity, e.g., from strongly-negative, to strongly-positive.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9360408782958984}, {"text": "polarity recognition", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7509748041629791}]}, {"text": "There currently exists a number of popular challenges on the matter, as those included in the SemEval series on evaluations of computational semantic analysis systems . Both polarity recognition and rating inference have been applied https://en.wikipedia.org/wiki/SemEval to recommendation systems.", "labels": [], "entities": [{"text": "polarity recognition", "start_pos": 174, "end_pos": 194, "type": "TASK", "confidence": 0.7266813367605209}]}, {"text": "Recently, Academia has been focusing on the feasibility to apply sentiment analysis tasks to very short and informal texts, such as tweets (see, e.g. ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.9505557417869568}]}, {"text": "This paper shows the description of the system that we have setup for participating into the Semeval 2016 challenge in (), task 4a (Message Polarity Classification).", "labels": [], "entities": [{"text": "Semeval 2016 challenge", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.6803729931513468}, {"text": "Message Polarity Classification", "start_pos": 132, "end_pos": 163, "type": "TASK", "confidence": 0.652532716592153}]}, {"text": "We have adopted a supervised approach, a SVM polynomial kernel classifier trained with the data provided by the challenge, after extracting lexical and lexicon features from such data.", "labels": [], "entities": [{"text": "SVM polynomial kernel classifier", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.7487880885601044}]}, {"text": "The paper is organised as follows.", "labels": [], "entities": []}, {"text": "Next section briefly addresses related work in the area.", "labels": [], "entities": []}, {"text": "Section 3 describes the features extracted from the training data.", "labels": [], "entities": []}, {"text": "In Section 4, we present the results of our attempt to answer to the challenge.", "labels": [], "entities": []}, {"text": "Finally, we give concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have preliminarily built a prediction model trained with the 2016 challenge data, in details we have used the train and the devel data, respectively for training and evaluation.", "labels": [], "entities": [{"text": "2016 challenge data", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.7190122008323669}]}, {"text": "The prediction model is based on an SVM linear kernel classifier.", "labels": [], "entities": []}, {"text": "For the experiments, the classifier has been implemented through sklearn 6 in Python.", "labels": [], "entities": []}, {"text": "We have used a linear classifier suitable for handling unbalanced data: SGDClassifier with default parameters 7 . The model exploits the four groups of features presented in Section 3.", "labels": [], "entities": []}, {"text": "Upon extracting the features from the training dataset, we obtained 6,547 features.", "labels": [], "entities": []}, {"text": "In the following, we will show some feature ablation experiments, each of them corresponds to remove one category of features from the full set.", "labels": [], "entities": []}, {"text": "Results are in terms of Precision and Recall, see.", "labels": [], "entities": [{"text": "Precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9807948470115662}, {"text": "Recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9932958483695984}]}, {"text": "The features evaluation shows that we do not have a set of dominant features group, leading to a not satisfying discrimination among positive, negative, and neutral tweets.", "labels": [], "entities": []}, {"text": "tive class, but, overall, less than we have expected.", "labels": [], "entities": []}, {"text": "In (, the authors have proposed a similar approach to the one here presented.", "labels": [], "entities": []}, {"text": "The aim was to evaluate the sentiment of a large set of online reviews.", "labels": [], "entities": []}, {"text": "In online reviews, the textual opinion is usually accompanied by a numerical score, and sentiment analysis could be a valid alley for identifying misalignment between the score and the satisfaction expressed in the text.", "labels": [], "entities": []}, {"text": "Work in ( shows that the features' set was discriminant for evaluating the sentiment of the reviews.", "labels": [], "entities": []}, {"text": "In part, this would support the thesis that standard sentiment analysis approaches are more suitable for \"literary\" texts than for short, informal texts featured by tweets.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.9436758458614349}]}, {"text": "It is worth noting that the lexicons we rely on are based on lemmas, while there exist other lexicons that consider also the part of speech, see, e.g.,).", "labels": [], "entities": []}, {"text": "Let the reader consider the following tweet, from the SemEval 2016 training set: #OnThisDay1987 CBS records shipped out the largest preorder in the company's history for Michael Jackson's album Bad http://t.co/v4fkyOx2eW In this example, the word \"Bad\" should not be considered as a negative adjective, since it is an album name.", "labels": [], "entities": [{"text": "SemEval 2016 training set", "start_pos": 54, "end_pos": 79, "type": "DATASET", "confidence": 0.7783641964197159}, {"text": "OnThisDay1987 CBS records", "start_pos": 82, "end_pos": 107, "type": "DATASET", "confidence": 0.9068063696225485}]}, {"text": "However, in the current work, we have not discriminated between nouns and adjectives with same spelling.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: MIB results (Tweets 2016 -dev, all feats)", "labels": [], "entities": [{"text": "MIB", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7979702353477478}]}]}