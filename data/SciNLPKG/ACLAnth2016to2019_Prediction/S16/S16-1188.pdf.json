{"title": [{"text": "OCLSP at SemEval-2016 Task 9: Multilayered LSTM as a Neural Semantic Dependency Parser", "labels": [], "entities": [{"text": "SemEval-2016 Task 9", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.6235506137212118}]}], "abstractContent": [{"text": "Semantic dependency parsing aims at extracting arcs and semantic role labels for all words in a sentence.", "labels": [], "entities": [{"text": "Semantic dependency parsing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7753052910168966}]}, {"text": "In this paper, we propose a semantic dependency parser which is based on Long Short-term Memory and makes heavy use of embeddings of words and POS tags.", "labels": [], "entities": [{"text": "semantic dependency parser", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.6290055215358734}]}, {"text": "We describe in detail the implementation of the neural parser, including preprocessing, postpro-cessing and various input features, and show that the neural parser performs close to the top system in the shared task and is very good at capturing non-local dependencies.", "labels": [], "entities": []}, {"text": "We also discuss some issues related to the parser and how to improve it.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic dependency parsing is a form of semantic analysis where semantic roles for all words in a sentence are analyzed and specific semantic relations are assigned to each word pair ().", "labels": [], "entities": [{"text": "Semantic dependency parsing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7764320969581604}, {"text": "semantic analysis", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7234869301319122}]}, {"text": "Chinese semantic dependency parsing is especially of interest as there is a remarkable difference between syntactic and semantic dependencies in Chinese ().", "labels": [], "entities": [{"text": "Chinese semantic dependency parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6189640685915947}]}, {"text": "In the SemEval 2016 Task 9 Chinese Semantic Dependency Parsing shared task, semantic dependency parsing for two text genres, TEXT, which includes sentences from conversations and primary school textbooks, and NEWS, which contains newswire text, is explored.", "labels": [], "entities": [{"text": "SemEval 2016 Task 9 Chinese Semantic Dependency Parsing shared task", "start_pos": 7, "end_pos": 74, "type": "TASK", "confidence": 0.8802707672119141}, {"text": "semantic dependency parsing", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.635556141535441}, {"text": "TEXT", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9573950171470642}, {"text": "NEWS", "start_pos": 209, "end_pos": 213, "type": "DATASET", "confidence": 0.8324381113052368}]}, {"text": "We propose a neural parser with LSTM as the basic units and make heavy use of vectorial representations of basic linguistic units like words and POS tags.", "labels": [], "entities": []}, {"text": "In this paper, we give a detailed description of the neural parser and discuss a few issues related to the current implementation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three systems with slightly different configurations for the Graph Generation process were submitted for test.", "labels": [], "entities": [{"text": "Graph Generation", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.8819988965988159}]}, {"text": "The results are shown in  probability between the word and any candidate head.", "labels": [], "entities": []}, {"text": "It is the worst performing model among the three in terms of F scores.", "labels": [], "entities": [{"text": "F scores", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9850135147571564}]}, {"text": "The lbpgs and lbpg75 are both models with the graph generation algorithm.", "labels": [], "entities": [{"text": "lbpgs", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.914383053779602}, {"text": "lbpg75", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.8280818462371826}]}, {"text": "The difference between them is that lbpgs uses a \u03c3 value of 100, which means that all words have only one head in its output, whereas lbpg75 uses a \u03c3 value of 75 meaning that the extra arcs of a word must have a probability over 0.75.", "labels": [], "entities": [{"text": "lbpgs", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.8329322338104248}]}, {"text": "For the lbpgs system, it is interesting to see that only allowing one head per word does not seem to have a huge impact on the F scores.", "labels": [], "entities": [{"text": "F scores", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9801642894744873}]}, {"text": "It actually performs better on the NEWS dataset in terms of LF and UF.", "labels": [], "entities": [{"text": "NEWS dataset", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.9644873738288879}, {"text": "LF", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9624008536338806}, {"text": "UF", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9148617386817932}]}, {"text": "This indicates that the systems do not perform very well on the extra arcs, either predicting the wrong labels for the arcs or predicting the wrong arcs altogether.", "labels": [], "entities": []}, {"text": "However, the non-local scores see a big drop.", "labels": [], "entities": []}, {"text": "For the lbpg75 system, it is clear that by choosing a high threshold of extra arc probability, in TEXT at least, we get slightly higher F scores.", "labels": [], "entities": [{"text": "extra arc probability", "start_pos": 72, "end_pos": 93, "type": "METRIC", "confidence": 0.7909184694290161}, {"text": "TEXT", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9896922707557678}, {"text": "F", "start_pos": 136, "end_pos": 137, "type": "METRIC", "confidence": 0.9991030693054199}]}, {"text": "In both datasets, the non-local scores are the highest with this model, meaning that for the arcs with high confidence, the non-local dependency arcs within them are relatively accurate.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of models with different feature com-", "labels": [], "entities": []}, {"text": " Table 2: Results of the submitted models on test", "labels": [], "entities": []}]}