{"title": [{"text": "UNIMELB at SemEval-2016 Tasks 4A and 4B: An Ensemble of Neural Networks and a Word2Vec Based Model for Sentiment Classification", "labels": [], "entities": [{"text": "UNIMELB", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8937484622001648}, {"text": "Sentiment Classification", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.932846337556839}]}], "abstractContent": [{"text": "This paper describes our sentiment classification system for microblog-sized documents , and documents where a topic is present.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.9358999729156494}]}, {"text": "The system consists of a soft-voting ensemble of a word2vec language model adapted to classification, a convolu-tional neural network (CNN), and a long-short term memory network (LSTM).", "labels": [], "entities": []}, {"text": "Our main contribution consists of away to introduce topic information into this model, by concatenating a topic embedding, consisting of the averaged word embedding for that topic, to each word embedding vector in our neural networks.", "labels": [], "entities": []}, {"text": "When we apply our models to SemEval 2016 Task 4 sub-tasks A and B, we demonstrate that the ensemble performed better than any single classifier, and our method of including topic information achieves a substantial performance gain.", "labels": [], "entities": [{"text": "SemEval 2016 Task 4 sub-tasks A", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.7313844462235769}]}, {"text": "According to results on the official test sets, our model ranked 3rd for \u00ed \u00b5\u00ed\u00b0\u00b9 PN in the message-only subtask A (among 34 teams) and 1st for accuracy on the topic-dependent subtask B (among 19 teams).", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b0\u00b9 PN", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.8992064148187637}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9990587830543518}]}], "introductionContent": [{"text": "The rapid growth of user-generated content, much of which is sentiment-laden, has fueled an interest in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.9416989088058472}]}, {"text": "One popular form of sentiment analysis involves classifying a document into discrete classes, depending on whether it expresses positive or negative sentiment (or neither).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.9571963250637054}]}, {"text": "The classification can also be dependent upon a particular topic.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9530832767486572}]}, {"text": "In this work, we describe the method we used for the sentiment classification of tweets, with or without a topic.", "labels": [], "entities": [{"text": "sentiment classification of tweets", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.9313118606805801}]}, {"text": "Our approach to the document classification task consists of an ensemble of 3 classifiers via soft-voting, 2 of which are neural network models.", "labels": [], "entities": [{"text": "document classification task", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.8219290375709534}]}, {"text": "One is the convolutional neural network (CNN) architecture of, and another is a Long Short Term Memory (LSTM)-based network.", "labels": [], "entities": []}, {"text": "Both were first tuned on a distant-labelled data set.", "labels": [], "entities": [{"text": "distant-labelled data set", "start_pos": 27, "end_pos": 52, "type": "DATASET", "confidence": 0.8354488015174866}]}, {"text": "The third classifier adapted word2vec to output classification probabilities using Bayes' formula, a slightly modified version of.", "labels": [], "entities": []}, {"text": "Despite the word2vec classifier being intended as a baseline, and having a small weight in the ensemble, it proved crucial for the ensemble to work well.", "labels": [], "entities": []}, {"text": "To adapt our models to the case where a topic is present, in the neural network models, we concatenated the embedding vectors for each word with a topic embedding, which consisted of the element-wise average of all word vectors in a particular topic.", "labels": [], "entities": []}, {"text": "We applied our approach to SemEval 2016 Task 4, including the message-only subtask (Task A) and the topic-dependent subtask (Task B) (Nakov et al., to appear).", "labels": [], "entities": [{"text": "SemEval 2016 Task 4", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7651734203100204}]}, {"text": "Our model ranked third for \u00ed \u00b5\u00ed\u00b0\u00b9 PN in the message-only subtask A (among 34 teams) and first for accuracy 1 on the topic-dependent subtask B (among 19 teams).", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b0\u00b9 PN", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.8556391447782516}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9990671277046204}]}, {"text": "The source code for our approach is available at https://github.com/stevenxxiu/senti.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our models on SemEval 2016 Task 4 subtask A, the message-only subtask, and subtask B, the topic-dependent subtask.", "labels": [], "entities": [{"text": "SemEval 2016 Task 4 subtask", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.7370436787605286}]}, {"text": "Evaluation consisted of accuracy, macroaveraged \u00ed \u00b5\u00ed\u00b0\u00b9 1 across the positive and negative classes, which we denote \u00ed \u00b5\u00ed\u00b0\u00b9 PN , and macroaveraged recall across the positive and negative classes, which we denote \u00ed \u00b5\u00ed\u00bc\u008c PN .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9996188879013062}, {"text": "\u00ed \u00b5\u00ed\u00b0\u00b9 1", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.7133521512150764}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9916736483573914}]}], "tableCaptions": [{"text": " Table 1: Semeval-2016 data. The negative : neutral : positive split was 16 : 42 : 42 for all of  2016 Task A used. The negative : positive split was 19 : 81 for all of 2016 Task B used.", "labels": [], "entities": [{"text": "Semeval-2016 data", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.7755126655101776}]}, {"text": " Table 2: Official test scores and ranks for Task  A.", "labels": [], "entities": []}, {"text": " Table 3: Test scores and ranks for Task B. The  official run incorrectly used \u00ed \u00b5\u00ed\u00b0\u00b9 PN as the validation  metric.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b0\u00b9 PN", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.7969086319208145}]}, {"text": " Table 4: Results for Task A, sorted by \u00ed \u00b5\u00ed\u00b0\u00b9 PN .", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b0\u00b9 PN", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9036711156368256}]}, {"text": " Table 5: Results for Task B, sorted by \u00ed \u00b5\u00ed\u00bc\u008c PN .  The dashed line separates topic models from  message-only models. The lstm topic model has  poorer accuracy due to being optimized on \u00ed \u00b5\u00ed\u00bc\u008c PN .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9987701773643494}]}]}