{"title": [{"text": "NLDS-UCSC at SemEval-2016 Task 6: A Semi-Supervised Approach to Detecting Stance in Tweets", "labels": [], "entities": [{"text": "NLDS-UCSC at SemEval-2016 Task 6", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.8472801566123962}, {"text": "Detecting Stance in Tweets", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.8952220380306244}]}], "abstractContent": [{"text": "Stance classification aims to identify, fora particular issue under discussion, whether the speaker or author of a conversational turn has Pro (Favor) or Con (Against) stance on the issue.", "labels": [], "entities": [{"text": "Stance classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9342656135559082}]}, {"text": "Detecting stance in tweets is anew task proposed for SemEval-2016 Task6, involving predicting stance fora dataset of tweets on the topics of abortion, atheism, climate change, feminism and Hillary Clinton.", "labels": [], "entities": []}, {"text": "Given the small size of the dataset, our team created our own topic-specific training corpus by developing a set of high precision hashtags for each topic that were used to query the twitter API, with the aim of developing a large training corpus without additional human labeling of tweets for stance.", "labels": [], "entities": []}, {"text": "The hashtags selected for each topic were predicted to be stance-bearing on their own.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate good performance for our features for opinion-target pairs based on generalizing dependency features using sentiment lexicons.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social media websites such as microblogs, weblogs, and discussion forums are used by millions of users to express their opinions on almost everything from brands, celebrities, and events to important social and political issues.", "labels": [], "entities": []}, {"text": "In recent years, the microblogging service Twitter has emerged as one of the most popular and useful sources of user content, and recent research has begun to develop tools and computational models for tweet-level opinion and sentiment analysis.", "labels": [], "entities": [{"text": "tweet-level opinion and sentiment analysis", "start_pos": 202, "end_pos": 244, "type": "TASK", "confidence": 0.6806288123130798}]}, {"text": "Stance classification aims to identify, fora particular issue under discussion, whether the speaker or author of a conversational turn has a Pro (Favor) or Con (Against) stance on the issue Detecting stance in tweets is anew task proposed for.", "labels": [], "entities": [{"text": "Stance classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8921552002429962}, {"text": "Detecting stance in tweets", "start_pos": 190, "end_pos": 216, "type": "TASK", "confidence": 0.8805799186229706}]}, {"text": "The aim of the task is to determine user stance (FAVOR, AGAINST, or NONE) in a dataset of tweets on the five selected topics of abortion, atheism, climate change, feminism and Hillary Clinton.", "labels": [], "entities": [{"text": "FAVOR", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9953737854957581}, {"text": "AGAINST", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9936151504516602}, {"text": "NONE)", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9485376179218292}]}, {"text": "Consider the tweets in, which express stance toward the target issue Climate Change is a Real Concern.", "labels": [], "entities": []}, {"text": "It can be inferred that the author of tweet T1 is in favor of the target while the author of tweet T2 is clearly against the target.", "labels": [], "entities": []}, {"text": "However due to the brevity of tweets, there is not always sufficient information about the target to determine stance: in the case of tweet T3, we are unsure what major development the user is talking about.", "labels": [], "entities": []}, {"text": "In the case of tweet T4, we know the user acknowledges the existence of a drought, but we do not know their stance on the issue of climate change solely based on this information.", "labels": [], "entities": []}, {"text": "In such cases the stance of the tweets is labelled NONE for this issue.", "labels": [], "entities": [{"text": "NONE", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9045553803443909}]}, {"text": "The task is nontrivial due to the challenges of the tweet genre.", "labels": [], "entities": []}, {"text": "Tweets are often highly informal with language that is colorful and ungrammatical.", "labels": [], "entities": []}, {"text": "They may also involve sarcasm, making opinion-mining tasks more challenging ().", "labels": [], "entities": []}, {"text": "Users may assert their stance using factual or emotional content, and due to their restricted length, tweets may not be well structured or coherent.", "labels": [], "entities": []}, {"text": "As a result, NLP tools trained on wellstructured text do notwork well in, and new tools are constantly being developed.", "labels": [], "entities": []}, {"text": "Our approach to stance classification in tweets is primarily based on developing a suite of tools for processing Twitter that mirrors our previous work on stance classification in online forums (.", "labels": [], "entities": [{"text": "stance classification in tweets", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.9093099236488342}, {"text": "stance classification in online forums", "start_pos": 155, "end_pos": 193, "type": "TASK", "confidence": 0.8621992826461792}]}, {"text": "We develop generalized dependency features that capture expressed sentiment or attitude towards particular targets, using the Tweebo dependency parser ().", "labels": [], "entities": []}, {"text": "Given the small size of the official task dataset, we created our own topic-specific training corpus in a semi-supervised manner.", "labels": [], "entities": []}, {"text": "We developed a set of high precision hashtags for each topic that were used to query the Twitter API in order to create a large training corpus without additional human labeling of tweets for stance.", "labels": [], "entities": []}, {"text": "The hashtags and boolean combinations of hashatgs selected for each topic were predicted to be stancebearing on their own.", "labels": [], "entities": []}, {"text": "There has been considerable previous work on stance classification in online forums and in congressional debates (.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.9819689989089966}]}, {"text": "A number of these studies show that collective classification approaches perform well, and that the context (, and meta information such as author constraints are useful for stance classification).", "labels": [], "entities": [{"text": "stance classification", "start_pos": 174, "end_pos": 195, "type": "TASK", "confidence": 0.967974454164505}]}, {"text": "Collective classification is not possible in the current task because the only information provided is the text of each individual tweet.", "labels": [], "entities": [{"text": "Collective classification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.782726377248764}]}, {"text": "Inspired by earlier work), we apply a framework for developing features for opinion-target pairs based on generalized structural dependency features, using the LIWC dictionary as the basis for generalization).", "labels": [], "entities": [{"text": "LIWC dictionary", "start_pos": 160, "end_pos": 175, "type": "DATASET", "confidence": 0.8551633656024933}]}, {"text": "We also develop features to capture domain knowledge using PMI values for topic n-grams in order to improve the recognition of tweets with the NONE stance.", "labels": [], "entities": [{"text": "NONE stance", "start_pos": 143, "end_pos": 154, "type": "DATASET", "confidence": 0.7969458103179932}]}, {"text": "We describe our system and data in Sec.", "labels": [], "entities": []}, {"text": "2, our experimental set-up in Sec.", "labels": [], "entities": []}, {"text": "3, and our results and error analysis in Sec.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 23, "end_pos": 37, "type": "METRIC", "confidence": 0.9222551882266998}]}, {"text": "4. We conclude and discuss future directions in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "We explored a large number of machine learning algorithms and feature combinations, using the automatically harvested tweets as training and the training set provided for the task as our development data to fit the parameters for the final submitted NLDS-UCSC system.", "labels": [], "entities": []}, {"text": "3.1 describes the feature sets created using the development set.", "labels": [], "entities": []}, {"text": "To evaluate the effects of hashtags on the test set we explored two different ways to train the system.", "labels": [], "entities": []}, {"text": "presents the results on the test set with hashtags present in the dataset while), we leave one dependency element lexicalized and generalize the other to its LIWC category for LIWC dependency features.", "labels": [], "entities": []}, {"text": "We follow a similar process to produce generalized opinion dependencies using AFINN lexicon and opinion-lexicon-English by (Hu and Liu) replacing one element of the dependency with its sentiment score and leaving the other element lexicalized ().", "labels": [], "entities": []}, {"text": "1 2 Inspired by previous work on combining sentiment lexicons we used a combined sentiment score to denote the accuracy of a sentiment word rather than its strength.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9985054731369019}]}, {"text": "If dictionaries contradict one another on the sentiment polarity fora word, then the score is neutralized to zero.", "labels": [], "entities": []}, {"text": "If a single dictionary lists the polarity word, but it is unlisted or neutral in the other dictionary, then the score is 1 in the direction of the polarity.", "labels": [], "entities": []}, {"text": "If both dictionaries list a word with the same polarity, then the score is 2 in the direction of the polarity.", "labels": [], "entities": []}, {"text": "After calculating the combined sentiment score, we check if either of the previous two words is listed as a negation by LIWC, and invert the polarity if a negation is found(.", "labels": [], "entities": [{"text": "LIWC", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.7705828547477722}]}, {"text": "Pointwise Mutual Information (PMI): For each topic, we calculate normalized pointwise mutual information over a combination of an extended version of IAC 2.0, a topic annotated database of posts from debate forums (, and our own collected tweets for each topic.", "labels": [], "entities": [{"text": "Pointwise Mutual Information (PMI)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5990599344174067}, {"text": "IAC 2.0", "start_pos": 150, "end_pos": 157, "type": "DATASET", "confidence": 0.8700479865074158}]}, {"text": "IAC 2.0 includes several topics that are in overlap with the topics in the current task.", "labels": [], "entities": [{"text": "IAC 2.0", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.6010647416114807}]}, {"text": "We then create a pool of top-N percent PMI unigrams, bigrams, and trigrams for each topic and use the count of words in each tweet that are also in this pool as a feature.", "labels": [], "entities": []}, {"text": "We also use the highest PMI value of an n-gram in each tweet as a feature.", "labels": [], "entities": [{"text": "PMI", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9614025354385376}]}], "tableCaptions": [{"text": " Table 3: Best performing model for each topic on Dev Set w/ hashtags, along with F-measure for favor,  against, and their average.", "labels": [], "entities": [{"text": "Dev Set", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.8808774948120117}, {"text": "F-measure", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9982964396476746}]}, {"text": " Table 4: Feature ablation w/ hashtags for each topic on Test Set, along with F-measure for favor, against,  and their average.", "labels": [], "entities": [{"text": "Test Set", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.8868826925754547}, {"text": "F-measure", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9978669881820679}]}, {"text": " Table 5: Tweets without hashtags on the Test set, along with F-measure for favor, against and their average.", "labels": [], "entities": [{"text": "Test set", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9408025741577148}, {"text": "F-measure", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9981836676597595}]}]}