{"title": [{"text": "UofL at SemEval-2016 Task 4: Multi Domain word2vec for Twitter Sentiment Classification", "labels": [], "entities": [{"text": "Twitter Sentiment Classification", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.5984195570151011}]}], "abstractContent": [{"text": "In this paper, we present a transfer learning system for twitter sentiment classification and compare its performance using different feature sets that include different word representation vectors.", "labels": [], "entities": [{"text": "twitter sentiment classification", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.787988026936849}]}, {"text": "We utilized data from a different source domain to increase the performance of our system in the target domain.", "labels": [], "entities": []}, {"text": "Our approach was based on training various word2vec models on data from the source and target domains combined, then using these models to calculate the average word vector of all the word vectors in a tweet observation, then input the average word vector as a feature to our classifiers for training.", "labels": [], "entities": []}, {"text": "We further developed one doc2vec model that was trained on the positive, negative and neutral tweets in the target domain only.", "labels": [], "entities": []}, {"text": "We then used these models in calculating the average word vector for every tweet in the training set as a preprocessing step.", "labels": [], "entities": []}, {"text": "The final evaluation results show that our approach gave a prediction accuracy on the Twitter2016 test dataset that outperformed two teams that were among the top 10 in terms of AvgF1 scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.8304529786109924}, {"text": "Twitter2016 test dataset", "start_pos": 86, "end_pos": 110, "type": "DATASET", "confidence": 0.9682480494181315}, {"text": "AvgF1", "start_pos": 178, "end_pos": 183, "type": "METRIC", "confidence": 0.9956153631210327}]}], "introductionContent": [{"text": "Twitter sentiment analysis deals with classifying the polarity of a tweet as positive or negative or neutral.", "labels": [], "entities": [{"text": "Twitter sentiment analysis", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6850091715653738}]}, {"text": "We have participated in Semeval 2016 Twitter Sentiment Analysis subtask-A.", "labels": [], "entities": [{"text": "Semeval 2016 Twitter Sentiment Analysis subtask-A", "start_pos": 24, "end_pos": 73, "type": "TASK", "confidence": 0.6642916748921076}]}, {"text": "Where we had to predict the polarity of a whole tweet rather than part of a tweet.", "labels": [], "entities": []}, {"text": "We have started off with feature engineering and data preprocessing.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.845555305480957}, {"text": "data preprocessing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7419950366020203}]}, {"text": "Then we have divided our task into five classification tasks.", "labels": [], "entities": []}, {"text": "Then we worked on creating our feature sets that we later used to select the best classifier/feature set combination for each classification task as will be explained further in the paper.", "labels": [], "entities": []}, {"text": "In section 2 we explain our transfer learning approach, system components, data preprocessing, aggregation approach, word2vec training and doc2vec training.", "labels": [], "entities": []}, {"text": "In section 3, we present our experiments.", "labels": [], "entities": []}, {"text": "Then in section 4, we review and discuss our results.", "labels": [], "entities": []}, {"text": "Finally in section 5, we make conclusions and outline some future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have carried out a number of experiments to help in selecting the feature sets to use for each classifier as well as which classifier type (SVM or Logistic Regression or boosted trees) to use for every binary classification and for the arbiter classifier.", "labels": [], "entities": []}, {"text": "Appendix C contains a flowchart describing the system structure.", "labels": [], "entities": []}, {"text": "The flowchart illustrates our system's sentiment prediction process..0 shows the validation prediction accuracies for each binary classifier and on the arbiter classifier when varying the feature set.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.9276111423969269}]}, {"text": "The feature set, classifier combination was selected based on the combination that yielded the best validation prediction accuracies.", "labels": [], "entities": []}, {"text": "However, when evaluating our whole system or our final system output, we use fscore as a measure of system performance and not the test set prediction accuracy.", "labels": [], "entities": [{"text": "fscore", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.8492525815963745}]}, {"text": "We have set class weights to auto in all classifiers we trained to help protecting against data imbalance which would lead to misleading results.", "labels": [], "entities": []}, {"text": "Starting System Fscore on the test set 0.26 Final system Fscore on the test set 0.51: Fscore of the whole system when using the least performing classifier/feature set combinations and when using the best performing classifier/feature set combination.", "labels": [], "entities": [{"text": "Starting", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.951518177986145}, {"text": "Fscore", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.6774348020553589}, {"text": "Fscore", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9953022003173828}, {"text": "Fscore", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9974732995033264}]}, {"text": "shows the least Fscore of the system we started with (features set 1 for all classifiers in the system) and the final system fscore before submission on our test set.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9936811923980713}]}, {"text": "We used fscore as the overall system performance measure and not the system accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9891424775123596}]}, {"text": "However, we have put the validation accuracy as the top classifier/feature set combination selection criteria for the individual classifiers in our system while setting class weights to auto in all of our classifiers in our system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9502184987068176}]}, {"text": "It is clear from.0 that feature categories 2 and 4 were associated with the best performing classifiers.", "labels": [], "entities": []}, {"text": "As mentioned earlier feature category 2 uses word vectors in addition to the base features in feature set 1.", "labels": [], "entities": []}, {"text": "While feature category 4 uses word vectors and paragraph vectors with feature set 1.", "labels": [], "entities": []}, {"text": "Which indicates that the addition of paragraph vectors with word2vec vectors gave best validation accuracies with the positive/negative and the pos/neg/neutral classifiers.", "labels": [], "entities": []}, {"text": "However, it did not give the best validation accuracies with the positive/neutral and negative/nonnegative classifiers.", "labels": [], "entities": []}, {"text": "As Feature category 2 that uses word vectors with feature set 1 gave the best validation accuracies with the positive/neutral and the negative/nonnegative classifiers.", "labels": [], "entities": []}, {"text": "Finally, using only paragraph vectors with feature set 1 yielded the best validation accuracy with the negative/neutral classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.982528030872345}]}, {"text": "Even though our doc2vec model was only trained on data from the target domain (tweets), it managed to give slightly better validation accuracy than when using word2vec vectors trained on the source (amazon reviews) and target (tweets) domains combined.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9593076109886169}]}, {"text": "Nonetheless, for positive/negative, positive/neutral, negative/nonnegative, positive/negative/neutral classification using word2vec vectors that were generated by our word2vec models trained on the source and target domains combined gave better validation set accuracies than when using only the doc2vec vectors generated by our doc2vec model that was trained on the full_training_set (tweets) with feature set 1 only.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Validation Accuracy for different classifier/Feature  Set combinations.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9279226064682007}]}, {"text": " Table 1: Final classifier/feature set combinations selected.", "labels": [], "entities": []}]}