{"title": [{"text": "ECNU at SemEval-2016 Task 4: An Empirical Investigation of Traditional NLP Features and Word Embedding Features for Sentence-level and Topic-level Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "Topic-level Sentiment Analysis", "start_pos": 135, "end_pos": 165, "type": "TASK", "confidence": 0.5664699971675873}]}], "abstractContent": [{"text": "This paper reports our submissions to Task 4, i.e., Sentiment Analysis in Twitter (SAT), in SemEval 2016, which consists of five subtasks grouped into two levels: (1) sentence level, i.e., message polarity classification (subtask A), and (2) topic level, i.e., tweet classification and quantification according to two-point scale (subtask B and D) or five-point scale (sub-task C and E).", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter (SAT)", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.8049518678869519}, {"text": "message polarity classification", "start_pos": 189, "end_pos": 220, "type": "TASK", "confidence": 0.7356722156206766}, {"text": "tweet classification", "start_pos": 261, "end_pos": 281, "type": "TASK", "confidence": 0.717757374048233}]}, {"text": "We participated in all these five subtasks.", "labels": [], "entities": []}, {"text": "To address these subtasks, we investigated several traditional Natural Language Processing (NLP) features including sentiment lexicon, linguistic and domain specific features, and word embedding features together with supervised machine learning methods.", "labels": [], "entities": []}, {"text": "Officially released results showed that our systems rank above average.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, with the emergence of social media, more and more users have shared and obtained information through microblogging websites, such as Twitter.", "labels": [], "entities": []}, {"text": "As a result, a huge amount of available data attracts a lot of researchers.", "labels": [], "entities": []}, {"text": "SemEval 2016 provides such a universal platform for researchers to explore in the task of Sentiment Analysis in Twitter () (Task 4), which includes five subtasks grouped into two levels, i.e., sentence level and topic level.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.8617302477359772}]}, {"text": "Subtask A is a sentence level task aiming at sentiment polarity classification of the whole tweet.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.7755929927031199}]}, {"text": "The other four subtasks are at topic level, i.e., given one topic, the sentiment polarity of tweets are classified or assigned by a two-point scale (i.e., subtask B and D) and by a five-point scale (i.e., subtask C and E).", "labels": [], "entities": []}, {"text": "Specifically, subtask B is to identify the sentiment polarity label (i.e, Positive and Negative) of tweets with respect to the given topic while subtask D aims at estimating the sentiment distribution of tweets with respect to the given topic.", "labels": [], "entities": []}, {"text": "Both subtask B and Dare on a two-point scale.", "labels": [], "entities": [{"text": "B", "start_pos": 13, "end_pos": 14, "type": "METRIC", "confidence": 0.9957166314125061}, {"text": "Dare", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9936802387237549}]}, {"text": "The purposes of subtask C and E are similar with that of subtask B and D, except for using a five-point scale, that is, the class labels are of 5 values, i.e, 2, 1, 0, -1 and -2 representing Very Positive, Positive, Neutral, Negative and Very Negative.", "labels": [], "entities": []}, {"text": "Given the character limitations on tweets, sentiment orientation classification on tweets can be regarded as a sentence-level sentiment analysis.", "labels": [], "entities": [{"text": "sentiment orientation classification", "start_pos": 43, "end_pos": 79, "type": "TASK", "confidence": 0.871127168337504}, {"text": "sentence-level sentiment analysis", "start_pos": 111, "end_pos": 144, "type": "TASK", "confidence": 0.7365984718004862}]}, {"text": "Many researchers focus on feature engineering to improve the performance of SAT.", "labels": [], "entities": [{"text": "SAT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.7638315558433533}]}, {"text": "For example, () showed that one-hot representation on n-gram features is a relatively strong baseline.", "labels": [], "entities": []}, {"text": "Furthermore, proposed a state-of-the-art model which implemented several sentiment lexicons and a variety of manual features.", "labels": [], "entities": []}, {"text": "Apart from the traditional methods, more and more researchers have paid their attention to use deep learning methods.", "labels": [], "entities": []}, {"text": "Word embedding is one of such methods, where each word is represented as a continuous, low-dimension vector and has been applied into NLP tasks as a critical and fundamental step.", "labels": [], "entities": [{"text": "Word embedding", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6644099950790405}]}, {"text": "Commonly, there are several types of word embedding models, e.g., Bengio proposed a Neural Probabilistic Language Model (NNLM) in () to learn distributed representation for each word and Mikolov simplified the structure of NNLM and presented t-wo efficient log-linear models in (.", "labels": [], "entities": []}, {"text": "Moreover, () further proposed learning sentiment-based word embeddings to settle SAT.", "labels": [], "entities": [{"text": "SAT", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9123355746269226}]}, {"text": "Meanwhile, topicbased opinion always adheres on certain words or phrases rather than whole tweet.", "labels": [], "entities": []}, {"text": "To address topicbased SAT, () used the hashtag information, () utilized the topic model to extract topic information from tweets and ( ) picked out related words rather than all words in whole tweet as pending words for consequential feature extraction.", "labels": [], "entities": [{"text": "consequential feature extraction", "start_pos": 220, "end_pos": 252, "type": "TASK", "confidence": 0.7523935238520304}]}, {"text": "Previous work showed that feature engineering has a significant impact on this task.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8524846136569977}]}, {"text": "Thus, in this work, we presented multiple types of traditional NLP features to perform SAT, e.g., sentiment lexicon features (e.g., MPQA, IMDB, Bing Liu opinion lexicon, etc), linguistic features (e.g., negations, n-gram at the word level and character level, etc) and tweet specific features (e.g., emoticons, capital words, elongated words, hashtags, etc,).", "labels": [], "entities": [{"text": "SAT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9472384452819824}]}, {"text": "Besides, the word embedding features were adopted.", "labels": [], "entities": []}, {"text": "We also performed a series of experiments to select effective feature subsets and supervised machine learning algorithms with optimal parameters.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our system framework including preprocessing, feature engineering, evaluation metrics, etc.", "labels": [], "entities": []}, {"text": "The experiments are reported in Section 3.", "labels": [], "entities": []}, {"text": "Finally, this work is concluded in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "For subtask A, we used the macro-averaged F score of positive and negative classes (i.e., F macro = ) to evaluate the performance.", "labels": [], "entities": [{"text": "F score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9254787266254425}]}, {"text": "Subtask B and D just contain positive and negative labels.", "labels": [], "entities": []}, {"text": "The official metric for subtask B is macro-averaged recall among positive and negative (i.e., R macro = RP os +R N eg 2 ).", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9738412499427795}]}, {"text": "As for subtask D, it is Kullback-Leibler Divergence (KLD) among distributions of two classes (i.e., KLD(pos, neg) = c j \u2208pos,neg P (c j ) \u00b7 log , where P denotes the probability of predicted label and\u02c6Pand\u02c6 and\u02c6P is the probability of gold label).", "labels": [], "entities": []}, {"text": "There are 5 classes existing in subtask C and E, and the organizers adopted Macroaveraged Mean Absolute Error (i.e., M AE M ) and Earth Mover's Distance (EM D) among 5 predefined classes for two subtasks respectively, where the detail information of two metrics for evaluation is described in the official document available on the website 10 .   Since only tweet IDs are provided by organizers, different participants may collect different numbers of tweets due to missing tweets or system errors.", "labels": [], "entities": [{"text": "Macroaveraged Mean Absolute Error", "start_pos": 76, "end_pos": 109, "type": "METRIC", "confidence": 0.8694504946470261}, {"text": "M AE M )", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9253376424312592}, {"text": "Earth Mover's Distance (EM D)", "start_pos": 130, "end_pos": 159, "type": "METRIC", "confidence": 0.7383183464407921}]}, {"text": "Subtask B and Dare of the same data set.", "labels": [], "entities": [{"text": "Dare", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9920638203620911}]}, {"text": "And subtask C and E share one common data set.", "labels": [], "entities": []}, {"text": "The statistics of all datasets for these subtasks are shown in, and 3, respectively.", "labels": [], "entities": []}, {"text": "For subtask A, the training data set consists of four parts which are shown in, i.e., 2013train, 2013dev, 2016train and 2016dev.", "labels": [], "entities": [{"text": "training data set", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.7767139077186584}]}, {"text": "The data set 2013train means SemEval-2013 Task 2 training data set (, and the following data sets are named in the same way.", "labels": [], "entities": [{"text": "2013train", "start_pos": 13, "end_pos": 22, "type": "DATASET", "confidence": 0.9021509289741516}, {"text": "SemEval-2013 Task 2 training data set", "start_pos": 29, "end_pos": 66, "type": "DATASET", "confidence": 0.6644939382870992}]}, {"text": "Actually, in consideration of the difference of polarity distribution between data set 2016devtest and 2013&2014test, we just adopted 2016devtest as development data.", "labels": [], "entities": []}, {"text": "For subtask B, C, D and E, the data is divided into many topic sets.", "labels": [], "entities": []}, {"text": "In order to improve the performance of sentiment analysis, we performed feature selection experi-10 http://alt.qcri.org/semeval2016/task4/data/uploads/eval.pdf  ments on all subtasks and the optimum feature sets are shown in.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.9590440690517426}]}, {"text": "From, it is interesting to find that: (1) Negation features and tweet specific features such as emoticon and all-caps make contributions to almost all subtasks.", "labels": [], "entities": []}, {"text": "The feature set with the best performance of subtask B is not quite beneficial for subtask D even though they have the same data set, perhaps because of the essential difference between binary classification and binary quantification: in the latter, errors of different polarity compensate each other.", "labels": [], "entities": []}, {"text": "The similar observation is found in subtask C and E.", "labels": [], "entities": []}, {"text": "The sentiment lexicon features make contributions to performance improvement of subtask A, B and C, but are not quite useful for subtask D and E.", "labels": [], "entities": []}, {"text": "A possible reason is that the latter two subtasks focus on quantification analysis while the sentiment lexicon only contains sentiment orientation rather than sentiment strength.", "labels": [], "entities": [{"text": "quantification analysis", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.7740717828273773}]}, {"text": "The word embedding features are not as effective as expected.", "labels": [], "entities": []}, {"text": "It maybe because we obtained sentence vectors by the simplest combination method described above, which does not take into account contextual information and semantic relations among words.", "labels": [], "entities": []}, {"text": "Besides, since subtask B, C, D and E focus on topic-level sentiment analysis, we tried to extract features from related words rather than whole tweet.", "labels": [], "entities": [{"text": "topic-level sentiment analysis", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.6545107265313467}]}, {"text": "But the preliminary experimental results showed that extracting features from related words underperformed the latter strategy for extracting features.", "labels": [], "entities": [{"text": "extracting features from related words", "start_pos": 53, "end_pos": 91, "type": "TASK", "confidence": 0.8158098340034485}]}, {"text": "The possible reason is that in many cases a tweet only has one single sentiment polarity.", "labels": [], "entities": []}, {"text": "Thus the sentiment polarity of sentence can always represent that of topic and extracting features from the related words may drop important information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of data sets in training (train), development", "labels": [], "entities": []}, {"text": " Table 4: Results of feature selection experiments for subtask A, B, C, D and E in terms of the corresponding evaluation metrics on", "labels": [], "entities": []}]}