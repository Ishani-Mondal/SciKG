{"title": [{"text": "ICL-HD at SemEval-2016 Task 10: Improving the Detection of Minimal Semantic Units and their Meanings with an Ontology and Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents our system submitted for SemEval 2016 Task 10: Detecting Minimal Semantic Units and their Meanings (DiM-SUM; Schneider, Hovy, et al., 2016).", "labels": [], "entities": [{"text": "SemEval 2016 Task 10", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.9053574502468109}, {"text": "Detecting Minimal Semantic Units and their Meanings (DiM-SUM; Schneider, Hovy, et al., 2016)", "start_pos": 67, "end_pos": 159, "type": "TASK", "confidence": 0.6211644081692946}]}, {"text": "We extend AMALGrAM (Schneider and Smith, 2015) by tapping two additional information sources.", "labels": [], "entities": [{"text": "AMALGrAM", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.6963194012641907}]}, {"text": "The first information source uses a semantic knowledge base (YAGO3; Suchanek et al., 2007) to improve supersense tagging (SST) for named entities.", "labels": [], "entities": [{"text": "supersense tagging (SST)", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.832106065750122}]}, {"text": "The second information source employs word embeddings (GloVe; Pennington et al., 2014) to capture fine-grained latent semantics and therefore improving the supersense identification for both nouns and verbs.", "labels": [], "entities": [{"text": "GloVe; Pennington et al., 2014", "start_pos": 55, "end_pos": 85, "type": "DATASET", "confidence": 0.8895386287144252}]}, {"text": "We conduct a detailed evaluation and error analysis for our features and come to the conclusion that both our extensions lead to an improved detection for SST.", "labels": [], "entities": [{"text": "SST", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.9829785227775574}]}], "introductionContent": [{"text": "The SemEval 2016 Task 10 on Detecting Minimal Semantic Units of Meaning (DiMSUM) is concerned with the identification of semantic classes called supersenses for single words as well as multiword expressions (MWEs).", "labels": [], "entities": [{"text": "SemEval 2016 Task 10 on Detecting Minimal Semantic Units of Meaning (DiMSUM)", "start_pos": 4, "end_pos": 80, "type": "TASK", "confidence": 0.8409700180803027}]}, {"text": "Identifying supersenses in text allows for abstractions that characterize word meanings beyond superficial orthography ( as well as inferring representations that move towards language independence.", "labels": [], "entities": []}, {"text": "It has been used to extend named entity recognition and to support supervised word sense disambiguation as it provides partial disambiguation; as well as in syntactic parse re-ranking () as latent semantic features.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6418341199556986}, {"text": "word sense disambiguation", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.6631306906541189}, {"text": "syntactic parse re-ranking", "start_pos": 157, "end_pos": 183, "type": "TASK", "confidence": 0.736427903175354}]}, {"text": "The addition of MWEs -idiosyncratic interpretations that crossword boundaries () -takes into account that the supersense of a MWE is usually not predictable from the meaning of the individual lexemes.", "labels": [], "entities": []}, {"text": "The Task moreover distinguishes between continuous MWEs like \"high school n.group \" and discontinuous (gappy) MWEs like \"track people down v.social \".", "labels": [], "entities": []}, {"text": "The inventory of supersenses used for the task -41 supersense classes, consisting of 26 noun and 15 verb supersenses -is derived from WordNet's top-level hypernyms in the taxonomy.", "labels": [], "entities": []}, {"text": "They are designed to be broad enough to encompass all nouns and verbs). are the first to approach SST and MWE detection jointly with a discriminative model.", "labels": [], "entities": [{"text": "SST", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9836724400520325}, {"text": "MWE detection", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.9388975203037262}]}, {"text": "Most of the previous work focuses on each of the tasks in separate.", "labels": [], "entities": []}, {"text": "tried to raise attention for the issue of MWEs in general and analyzed different types of MWE.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 42, "end_pos": 46, "type": "TASK", "confidence": 0.951854407787323}]}, {"text": "employed latent semantic analysis to determine the decomposability of MWEs.", "labels": [], "entities": []}, {"text": "Finally many MWE lexicons have been built for different purposes which is why Schneider et al. picked up the issue to address MWE annotation for general purposes.", "labels": [], "entities": [{"text": "MWE annotation", "start_pos": 126, "end_pos": 140, "type": "TASK", "confidence": 0.8235057592391968}]}, {"text": "first trained and tested a discriminative model for SST of unambigu-ous nouns on data extracted from different versions of WordNet and achieved an accuracy of slightly over 52%.", "labels": [], "entities": [{"text": "SST of unambigu-ous nouns", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.8158710151910782}, {"text": "WordNet", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.9582576751708984}, {"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9995449185371399}]}, {"text": "applied an unsupervised approach based on vector-space word similarity and achieved 63% accuracy on the same data used by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9990113973617554}]}, {"text": "When revisiting the task achieved between 70% and 77% F-score using a HMM sequence tagger.", "labels": [], "entities": [{"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.999593198299408}]}], "datasetContent": [{"text": "In the following section we present the data we used, then the tools and parameters and finally the results of our experiments.", "labels": [], "entities": []}, {"text": "We conduct the experiments on the aforementioned DiMSUM data sets.", "labels": [], "entities": [{"text": "DiMSUM data sets", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.9169485171635946}]}, {"text": "We use the AMALGrAM 2.0 tagger as baseline system with the following parameters: 6 four training iterations, features that appeared less than five times were cutoff, a constraint for the decoding process that asserts that the \"O\" label is never followed by an \"I\", including loss term and a cost penalty of 100 for errors against recall.", "labels": [], "entities": [{"text": "loss term", "start_pos": 275, "end_pos": 284, "type": "METRIC", "confidence": 0.9673747420310974}, {"text": "recall", "start_pos": 330, "end_pos": 336, "type": "METRIC", "confidence": 0.9949204325675964}]}, {"text": "The effects of the YAGO feature on AMALGrAM can be seen in.", "labels": [], "entities": [{"text": "YAGO", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.7635936141014099}, {"text": "AMALGrAM", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.8623548150062561}]}, {"text": "Compared to the baseline (BL) the WordNet hypernyms have almost no effect on MWE detection (+0.02) while improving SST (+0.64).", "labels": [], "entities": [{"text": "WordNet hypernyms", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.9191612303256989}, {"text": "MWE detection", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.9381078481674194}, {"text": "SST", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.5812038779258728}]}, {"text": "The supersense rankings improve MWE detection (+0.49) as well as SST (+0.62).", "labels": [], "entities": [{"text": "MWE", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8530773520469666}, {"text": "detection", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.8129037022590637}, {"text": "SST", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.931837260723114}]}, {"text": "Combining the features further improves the detection for MWEs (+0.59) and SST (+1.13).", "labels": [], "entities": [{"text": "detection", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9577751159667969}, {"text": "MWEs", "start_pos": 58, "end_pos": 62, "type": "TASK", "confidence": 0.6188450455665588}]}, {"text": "The first experiment involves the evaluation of different word embedding dimensions.", "labels": [], "entities": []}, {"text": "shows a comparison between the baseline system (BL) which uses no word embeddings and our system adopting GloVe word embeddings with four dimension sizes: 50, 100, 200 and 300.", "labels": [], "entities": []}, {"text": "This experiment uses word embeddings in their given real-valued form without scaling them.", "labels": [], "entities": []}, {"text": "All systems using word embeddings show improved performance and are approximately one percent higher than the baseline.", "labels": [], "entities": []}, {"text": "In this experiment the system performs best with 50 dimensional word embeddings, after which the performance shows a slight decrease with growing vector dimensionality.", "labels": [], "entities": []}, {"text": "The second experiment evaluates the influence of unscaled versus scaled word embeddings using the method of which we described in section 3.4.", "labels": [], "entities": []}, {"text": "compares the F1-scores of four systems: the first -Glove50 -uses unscaled word embedding whereas the remaining three systems scale those word embeddings with varying \u03c3-values (0.01, 0.1 and 1).", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9979352951049805}]}, {"text": "According to their method works best with a \u03c3-value that scales the standard derivation to 0.1.", "labels": [], "entities": []}, {"text": "With our data   that value is \u03c3 = 0.1.", "labels": [], "entities": []}, {"text": "However our results contradict as our experiment shows that \u03c3 = 0.01 is more successful in predicting MWEs whereas \u03c3 = 1 is more suitable for the detection of supersenses as well as both combined.", "labels": [], "entities": [{"text": "predicting MWEs", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.7713727951049805}]}, {"text": "The third experiment evolves around the interaction between Brown clusters and word embeddings.", "labels": [], "entities": []}, {"text": "As both methods have a similar aim -capturing the semantic representation of words -it is of interest to distinguish their influence on the system performance.", "labels": [], "entities": []}, {"text": "To accomplish this we train the baseline system (BL) and our most successful system (Glove50, \u03c3 = 1) with (+brown) and without using Brown clusters (-brown).", "labels": [], "entities": [{"text": "BL", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.8800055384635925}, {"text": "Glove50", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9013236165046692}]}, {"text": "Our system profits from the Brown clusters as the F1-scores for all categories (MWE, SST and combined) improves.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9986145496368408}, {"text": "MWE", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.8547276258468628}]}], "tableCaptions": [{"text": " Table 2: Influence of unscaled (Glove50) versus scaled word", "labels": [], "entities": [{"text": "Glove50", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.8341363072395325}]}, {"text": " Table 3: Comparison of baseline (BL) and our most success-", "labels": [], "entities": [{"text": "Comparison of baseline (BL)", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.7205764849980673}]}, {"text": " Table 4: Accuracy (Acc), precision (P), recall (R) and F1-score", "labels": [], "entities": [{"text": "Accuracy (Acc)", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8490520566701889}, {"text": "precision (P)", "start_pos": 26, "end_pos": 39, "type": "METRIC", "confidence": 0.9580504149198532}, {"text": "recall (R)", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9658908247947693}, {"text": "F1-score", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9989675283432007}]}, {"text": " Table 5: Evaluation of the YAGO feature with F1-score com-", "labels": [], "entities": [{"text": "YAGO", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.519449770450592}, {"text": "F1-score", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9827982783317566}]}]}