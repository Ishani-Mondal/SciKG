{"title": [{"text": "RTM at SemEval-2016 Task 1: Predicting Semantic Similarity with Referential Translation Machines and Related Statistics", "labels": [], "entities": [{"text": "RTM", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8312109708786011}, {"text": "Predicting Semantic Similarity", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.8745597203572592}]}], "abstractContent": [{"text": "We use referential translation machines (RTMs) for predicting the semantic similarity of text in both STS Core and Cross-lingual STS.", "labels": [], "entities": [{"text": "referential translation machines (RTMs", "start_pos": 7, "end_pos": 45, "type": "TASK", "confidence": 0.74689981341362}]}, {"text": "RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource.", "labels": [], "entities": []}, {"text": "RTMs become 14th out of 26 submissions in Cross-lingual STS.", "labels": [], "entities": [{"text": "Cross-lingual STS", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8600859940052032}]}, {"text": "We also present rankings of various prediction tasks using the performance of RTM in terms of MRAER, a normalized relative absolute error metric.", "labels": [], "entities": [{"text": "MRAER", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.98812335729599}]}], "introductionContent": [], "datasetContent": [{"text": "In this section, we detail the training performance of our model based on major modeling differences with our previous RTM models on SemEval tasks.", "labels": [], "entities": [{"text": "SemEval tasks", "start_pos": 133, "end_pos": 146, "type": "TASK", "confidence": 0.9031404852867126}]}, {"text": "This year, we identified numeric expressions using regular expressions as a preprocessing step, which mainly identifies integers and real numbers that can have exponents.", "labels": [], "entities": []}, {"text": "After sending the test results, we further worked on the numeric expression identification to expand the types of identified expressions.", "labels": [], "entities": [{"text": "numeric expression identification", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.6728147466977438}]}, {"text": "We also experimented with language identification for STS Spanish.", "labels": [], "entities": [{"text": "language identification", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7553340196609497}, {"text": "STS Spanish", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.7099811136722565}]}, {"text": "Language identification is done using the manually corrected results starting from the output of automatic language identification tool mguesser.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7634912729263306}]}, {"text": "1 After language identification, corpora were split into English and Spanish rather than the shuffled format that was made available to the participants.", "labels": [], "entities": [{"text": "language identification", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.7162878215312958}]}, {"text": "We compare the performance after identification of numeric expressions and identification of the language using SVR.", "labels": [], "entities": []}, {"text": "Both STS Spanish models use previous years' training data from both STS English and STS Spanish for training, which total to 13823 instances.", "labels": [], "entities": [{"text": "STS English", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.8442883193492889}]}, {"text": "1 http://www.mnogosearch.org and presents the results before and after identification of numerics on STS English.", "labels": [], "entities": [{"text": "STS English", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.8969923853874207}]}, {"text": "We observe that identification of numerics improve the performance on the test set (bolded results).", "labels": [], "entities": []}, {"text": "presents the results on STS Spanish with the default shuffled setting and with the setting where we model the prediction as machine translation performance prediction from English to Spanish after identifying the language of each sentence in the training set.", "labels": [], "entities": [{"text": "machine translation performance prediction", "start_pos": 124, "end_pos": 166, "type": "TASK", "confidence": 0.7307041957974434}]}, {"text": "STS Spanish training dataset contains English sentences in majority and shuffled +numer-ics setting only use English corpora even though Spanish sentences are shuffled in the test set and eventually, shuffled +numerics setting obtains better results than language identified +numerics setting on the training set.", "labels": [], "entities": [{"text": "STS Spanish training dataset", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.895430862903595}]}, {"text": "Even so, we observe that identification of the language improve the performance on the test set (bolded results).", "labels": [], "entities": []}, {"text": "Training results on setting language identified +numerics is lower, which maybe due to the RTM model using language identified test corpus and the same training corpus as the shuffled +numerics setting.", "labels": [], "entities": []}, {"text": "plots the performance on the test set where instances are sorted according to the magnitude of the target scores.", "labels": [], "entities": []}, {"text": "For STS English, we observe decreasing AER and a valley of absolute errors, which maybe due to SVR preferring predictions close to the mean of train score distribution.", "labels": [], "entities": [{"text": "STS English", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.651705265045166}, {"text": "AER", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9993032217025757}, {"text": "absolute errors", "start_pos": 59, "end_pos": 74, "type": "METRIC", "confidence": 0.9444938898086548}]}], "tableCaptions": [{"text": " Table 1: Number of instances in the STS test set. Only some of", "labels": [], "entities": [{"text": "STS test set", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.8320069909095764}]}, {"text": " Table 2: STS English test results for each domain.", "labels": [], "entities": []}, {"text": " Table 3: STS Spanish test results.", "labels": [], "entities": [{"text": "STS Spanish test results", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.7613014578819275}]}, {"text": " Table 4: RTM top predictor results on STS English show that performance improve after identification of numerics on the test set.", "labels": [], "entities": [{"text": "STS English", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.7517140209674835}]}, {"text": " Table 5: STS English test results for each domain from new experiments. Domain % numerics lists the percentage of tokens", "labels": [], "entities": []}, {"text": " Table 6: RTM SVR results on STS Spanish show that performance improve after language identification on the test set.", "labels": [], "entities": [{"text": "RTM SVR", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.6708247661590576}, {"text": "STS Spanish", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.6345295906066895}, {"text": "language identification", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.7273932993412018}]}, {"text": " Table 7: STS Spanish test results from new experiments.", "labels": [], "entities": []}]}