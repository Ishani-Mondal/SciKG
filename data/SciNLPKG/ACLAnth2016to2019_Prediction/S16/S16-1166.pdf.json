{"title": [{"text": "SemEval-2016 Task 8: Meaning Representation Parsing", "labels": [], "entities": [{"text": "SemEval-2016 Task 8", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8833220402399699}, {"text": "Meaning Representation Parsing", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.9772967497507731}]}], "abstractContent": [{"text": "In this report we summarize the results of the SemEval 2016 Task 8: Meaning Representation Parsing.", "labels": [], "entities": [{"text": "SemEval 2016 Task 8", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.8351148515939713}, {"text": "Meaning Representation Parsing", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.9804857770601908}]}, {"text": "Participants were asked to generate Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs fora set of English sentences in the news and discussion forum domains.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 36, "end_pos": 73, "type": "METRIC", "confidence": 0.7939534982045492}]}, {"text": "Eleven sites submitted valid systems.", "labels": [], "entities": []}, {"text": "The availability of state-of-the-art baseline systems was a key factor in lowering the bar to entry; many submissions relied on CAMR (Wang et al., 2015b; Wang et al., 2015a) as a baseline system and added extensions to it to improve scores.", "labels": [], "entities": []}, {"text": "The evaluation set was quite difficult to parse, particularly due to creative approaches to word representation in the web forum portion.", "labels": [], "entities": [{"text": "word representation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.6972523033618927}]}, {"text": "The top scoring systems scored 0.62 F1 according to the Smatch (Cai and Knight, 2013) evaluation heuristic.", "labels": [], "entities": [{"text": "F1", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9993184804916382}, {"text": "Smatch (Cai and Knight, 2013) evaluation heuristic", "start_pos": 56, "end_pos": 106, "type": "DATASET", "confidence": 0.777990061044693}]}, {"text": "We show some sample sentences along with a comparison of system parses and perform quantitative ablative studies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8513260285059611}]}, {"text": "It includes entity identification and typing, PropBank semantic roles), individual entities playing multiple roles, as well as treatments of modality, negation, etc.", "labels": [], "entities": [{"text": "entity identification", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7868285179138184}]}, {"text": "AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj).", "labels": [], "entities": [{"text": "AMR abstracts", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8135852217674255}]}, {"text": "With the recent public release of a sizeable corpus of English/AMR pairs (LDC2014T12), there has (f / fear-01 :polarity \"-\" :ARG0 ( s / soldier ) :ARG1 ( d / die-01 :ARG1 s )) The soldier was not afraid of dying.", "labels": [], "entities": [{"text": "ARG1", "start_pos": 147, "end_pos": 151, "type": "DATASET", "confidence": 0.8164503574371338}]}, {"text": "The soldier was not afraid to die.", "labels": [], "entities": []}, {"text": "The soldier did not fear death.", "labels": [], "entities": []}, {"text": "been substantial interest in creating parsers to recover this formalism from plain text.", "labels": [], "entities": []}, {"text": "Several parsers were released in the past couple of years (.", "labels": [], "entities": []}, {"text": "This body of work constitutes many diverse and interesting scientific contributions, but it is difficult to adequately determine which parser is numerically superior, due to heterogeneous evaluation decisions and the lack of a controlled blind evaluation.", "labels": [], "entities": []}, {"text": "The purpose of this task, therefore, was to provide a competitive environment in which to determine one winner and award atrophy to said winner.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the specific purposes of this task, DEFT commissioned and LDC released an additional set of English sentences along with AMR annotations 2 that had not been previously seen.", "labels": [], "entities": [{"text": "DEFT", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.8602392673492432}, {"text": "LDC", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8520024418830872}]}, {"text": "This blind evaluation set consists of 1,053 sentences in a roughly 50/50 discussion forum/newswire split.", "labels": [], "entities": []}, {"text": "The distribution of sentences by source is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Split by domain of evaluation data.", "labels": [], "entities": []}, {"text": " Table 2: Main Results: Mean of five runs of Smatch  2.0.2 with five restarts per run is shown; Standard  deviation of F1 was about 0.0002 per system.", "labels": [], "entities": [{"text": "Main", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9707523584365845}, {"text": "Mean", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9995664954185486}, {"text": "Standard  deviation", "start_pos": 96, "end_pos": 115, "type": "METRIC", "confidence": 0.9459666013717651}, {"text": "F1", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.9920401573181152}]}, {"text": " Table 3: Ablation of instances, attributes, and relations.", "labels": [], "entities": []}]}