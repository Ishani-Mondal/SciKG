{"title": [{"text": "PMI-cool at SemEval-2016 Task 3: Experiments with PMI and Goodness Polarity Lexicons for Community Question Answering", "labels": [], "entities": [{"text": "PMI-cool at SemEval-2016 Task", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.856829822063446}, {"text": "Community Question Answering", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.5986745655536652}]}], "abstractContent": [{"text": "We describe our submission to SemEval-2016 Task 3 on Community Question Answering.", "labels": [], "entities": [{"text": "SemEval-2016 Task 3 on Community Question Answering", "start_pos": 30, "end_pos": 81, "type": "TASK", "confidence": 0.7123812351908002}]}, {"text": "We participated in subtask A, which asks to rerank the comments from the thread fora given forum question from good to bad.", "labels": [], "entities": []}, {"text": "Our approach focuses on the generation and use of goodness polarity lexicons, similarly to the sentiment polarity lexicons, which are very popular in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.9421678781509399}]}, {"text": "In particular, we use a combination of bootstrapping and pointwise mutual information to estimate the strength of association between a word (from a large unannotated set of question-answer threads) and the class of good/bad comments.", "labels": [], "entities": []}, {"text": "We then use various features based on these lexicons to train a regression model, whose predictions we use to induce the final comment ranking.", "labels": [], "entities": []}, {"text": "While our system was not very strong as it lacked important features, our lexicons contributed to the strong performance of another top-performing system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Online forums have been gaining a lot of popularity in recent years.", "labels": [], "entities": []}, {"text": "In these forums, one can ask a question, and based on the wisdom of the crowd, expect to get some good answers.", "labels": [], "entities": []}, {"text": "In practice, unless there is strong moderation, most such forums get populated with bad answers, which can be annoying for users as it takes time to read through all answers in along thread.", "labels": [], "entities": []}, {"text": "As the importance of the problem was recognized in the research community, this gave rise to two shared tasks on Community Question Answering at and).", "labels": [], "entities": [{"text": "Community Question Answering", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.6369087398052216}]}, {"text": "Here we describe the PMI-cool system, which we developed to participate in SemEval-2016 Task 3, subtask A, which asks to rerank the answers in a question-answer thread, ordering them from good to bad . As the name of our system suggests, our approach is heavily based on pointwise mutual information (PMI), which we use to estimate the association strength between a word and a class, e.g., the class of good or the class of bad comments.", "labels": [], "entities": [{"text": "SemEval-2016 Task 3", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7290177146593729}]}, {"text": "Based on this association strength, we perform bootstrapping in a large unannotated set of question-answer threads to generate specialized goodness polarity lexicons.", "labels": [], "entities": []}, {"text": "We then use various features based on these lexicons to train a regression model, whose predictions we use to induce the final comment ranking.", "labels": [], "entities": []}, {"text": "While our PMI-cool system did not perform very well at the competition as it lacked important features and as we found a bug in our submission, our goodness polarity lexicons proved useful and contributed to the strong performance of another topperforming system at SemEval-2016 Task 3: SUper team ().", "labels": [], "entities": [{"text": "SemEval-2016 Task 3", "start_pos": 266, "end_pos": 285, "type": "TASK", "confidence": 0.69801265001297}]}], "datasetContent": [{"text": "For training the prediction model for good versus bad answers, we used an SVM with a linear kernel as implemented in.", "labels": [], "entities": []}, {"text": "We treated each answer as a separate instance with all the above features, merging the PotentiallyUseful and the Bad labels under the bad class, and we ranked the answers based on the SVM score.", "labels": [], "entities": []}, {"text": "Here is a list of the useful features we experimented with: \u2022 SO SameAuthor; \u2022 SO AnswerNumber; \u2022 SO AnswerAuthor; \u2022 sum of SO(w) for the answer words; \u2022 sum of the sentiment for the answer words; \u2022 one feature for each personality trait: sum of the scores of the lexicon words for that trait in all posts by the answer author; \u2022 number of words with positive BootstrapedSO in the answer; \u2022 number of words with negative BootstrapedSO in the answer; \u2022 fraction of words with positive BootstrapedSO in the answer; \u2022 fraction of words with negative BootstrapedSO in the answer; \u2022 sum of the positive BootstrapedSO scores for the answer words; \u2022 sum of the negative BootstrapedSO scores for the answer words; \u2022 maximum value of BootstrapedSO fora word in the answer; \u2022 minimum value of BootstrapedSO fora word in the answer; \u2022 sum of BootstrapedSO scores for all answer words.", "labels": [], "entities": []}, {"text": "The MAP scores resulting from our experiments on the development dataset are shown in.", "labels": [], "entities": [{"text": "MAP", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8135097026824951}]}, {"text": "The first row shows the maximum possible score: it is lower than 1, as 33 of the 244 dev threads had no good answers.", "labels": [], "entities": []}, {"text": "Next, we show the MAP score when all features are enabled; we can see that it outperforms both the chronological and the random baseline, by 4 and 10 MAP points absolute, respectively.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9338086247444153}, {"text": "MAP", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.8841443061828613}]}, {"text": "The following four rows show results with some class of features disabled.", "labels": [], "entities": []}, {"text": "We can see that the personality features had virtually no impact on the results, sentiment had a minimal impact (0.2 MAP points), metadata had areal impact (1.8 MAP points), while the word PMI features had the largest impact (6.6 MAP points).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Ablation results on the development dataset.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9964901804924011}]}]}