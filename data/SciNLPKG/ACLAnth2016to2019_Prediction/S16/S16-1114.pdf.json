{"title": [{"text": "ASOBEK at SemEval-2016 Task 1: Sentence Representation with Character N-gram Embeddings for Semantic Textual Similarity", "labels": [], "entities": [{"text": "Sentence Representation", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.9726826846599579}, {"text": "Semantic Textual Similarity", "start_pos": 92, "end_pos": 119, "type": "TASK", "confidence": 0.6160524189472198}]}], "abstractContent": [{"text": "A growing body of research has recently been conducted on semantic textual similarity using a variety of neural network models.", "labels": [], "entities": [{"text": "semantic textual similarity", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6529436508814493}]}, {"text": "While recent research focuses on word-based representation for phrases, sentences and even paragraphs, this study considers an alternative approach based on character n-grams.", "labels": [], "entities": []}, {"text": "We generate embeddings for character n-grams using a continuous-bag-of-n-grams neural network model.", "labels": [], "entities": []}, {"text": "Three different sentence representations based on n-gram embeddings are considered.", "labels": [], "entities": []}, {"text": "Results are reported for experiments with bigram, trigram and 4-gram em-beddings on the STS Core dataset for SemEval-2016 Task 1.", "labels": [], "entities": [{"text": "STS Core dataset", "start_pos": 88, "end_pos": 104, "type": "DATASET", "confidence": 0.9722862044970194}, {"text": "SemEval-2016 Task 1", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.6003931065400442}]}], "introductionContent": [{"text": "This paper presents an approach for finding the degree of semantic similarity between sentence pairs.", "labels": [], "entities": []}, {"text": "Semantic textual similarity (STS) is of relevance to many NLP applications.", "labels": [], "entities": [{"text": "Semantic textual similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8009073932965597}]}, {"text": "Recent tasks in recognizing textual entailment, sentence completion and paraphrase identification are closely related.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.8489859700202942}, {"text": "sentence completion", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7872740924358368}, {"text": "paraphrase identification", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.8983733355998993}]}, {"text": "The approach described here makes use of a neural network (NN) algorithm (word2vec) that is typically used to generate word embeddings ().", "labels": [], "entities": []}, {"text": "Rather than generating vector representations for words however, we propose a character n-gram-to-vector approach.", "labels": [], "entities": []}, {"text": "A sentence is then represented as a vector generated through a combination of character n-gram embeddings.", "labels": [], "entities": []}, {"text": "The use of character level vectors has been proposed in a number of recent studies.", "labels": [], "entities": []}, {"text": "Subword language models that use the combination of characters, syllables and frequent words have been explored by.", "labels": [], "entities": []}, {"text": "Character-level language modeling has been performed for modeling OOV words, where using words as the atomic units of the model would not be sufficient to assign a probability score.", "labels": [], "entities": [{"text": "Character-level language modeling", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.658425509929657}]}, {"text": "In, word representations are composed of vectors of characters, called character to word (C2W).", "labels": [], "entities": []}, {"text": "The C2W vectors are used successfully for language modeling and POS tagging without any handcrafted features.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7887477576732635}, {"text": "POS tagging", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.8405818939208984}]}, {"text": "The resulting model is competitive on English POS tagging with the Stanford POS tagger word lookup tables and also produces a notable improvement in results for morphologically rich languages such as Turkish.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.7009041607379913}]}, {"text": "apply a simple convolutional neural network model, which uses character level inputs for word representations.", "labels": [], "entities": []}, {"text": "Again, this method outperforms the models that use word/morpheme level features in morphologically rich languages, while also having competitive results in English.", "labels": [], "entities": []}, {"text": "introduce a word hashing technique using character n-grams to scale up training of deep NN models for largescale web search applications.", "labels": [], "entities": [{"text": "word hashing", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.7640956044197083}]}, {"text": "Our motivation for exploring character n-grams derives in part from previous work we have conducted on paraphrase identification (PI).", "labels": [], "entities": [{"text": "paraphrase identification (PI)", "start_pos": 103, "end_pos": 133, "type": "TASK", "confidence": 0.8829792737960815}]}, {"text": "The PI task is that of deciding whether two sentences have the same meaning.", "labels": [], "entities": [{"text": "PI task", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.8356741070747375}]}, {"text": "We have shown that sentence representations based on bags or sets of character n-gram features can perform well at this task (.", "labels": [], "entities": []}, {"text": "It is hypothesized that n-grams are useful for capturing lexical similarity and perform a role similar to lemmatization whilst preserving differences.", "labels": [], "entities": []}, {"text": "Thus, sentence representations based on collections of n-grams as features may offer some advantages over representations based on words as features.", "labels": [], "entities": []}, {"text": "The current study aims to extend this earlier work by working with n-gram embeddings.", "labels": [], "entities": []}, {"text": "() introduced two new NN architectures that were applied to learning word embeddings: continuous-bag-of-words (CBOW) and skip grams (SG).", "labels": [], "entities": []}, {"text": "These NN models have been shown to perform well in many NLP areas such as STS and PI (.", "labels": [], "entities": [{"text": "STS", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.8153872489929199}]}, {"text": "Recent research has taken steps to extend these word vectors to sentences, paragraphs and documents (.", "labels": [], "entities": []}, {"text": "We introduce an alternative approach to obtaining sentence level embedding vectors that make use of character n-grams rather than words.", "labels": [], "entities": []}, {"text": "We adopt a model architecture that is analogous to CBOW, which we call continuous-bag-of-n-grams and notate as CBOnG throughout the paper.", "labels": [], "entities": []}, {"text": "In keeping with our earlier work on paraphrase identification (, preprocessing is kept to a minimum and no use is made of any manually constructed semantic or syntactic processing tools or resources.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.9413355886936188}]}, {"text": "Operationally, STS is similar to paraphrase identification.", "labels": [], "entities": [{"text": "STS", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9167104363441467}, {"text": "paraphrase identification", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.9426009356975555}]}, {"text": "The two tasks differ in that STS sentence pairs are assigned a degree of semantic equivalence instead of a binary paraphrase label.", "labels": [], "entities": []}, {"text": "STS shared tasks have produced a sizable amount of research on sentence similarity ().", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.6912487745285034}]}], "datasetContent": [{"text": "We used a dump of English Wikipedia articles 1 that includes 3,831,719 articles and 8,179,596 unique words.", "labels": [], "entities": [{"text": "English Wikipedia articles 1", "start_pos": 18, "end_pos": 46, "type": "DATASET", "confidence": 0.8923233896493912}]}, {"text": "Wikipedia provides a large and accessible collection of text consisting of wellformed sentences that is suitable for training purposes.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9153895378112793}]}, {"text": "We obtained a plain text representation of the documents by removing the data dump xml tags using the script provided in the Gensim package.", "labels": [], "entities": [{"text": "Gensim package", "start_pos": 125, "end_pos": 139, "type": "DATASET", "confidence": 0.8612167239189148}]}, {"text": "Matching the minimal pre-processing performed on the STS pairs, training data are only lowercased and cleared of punctuation.", "labels": [], "entities": []}, {"text": "The Wikipedia dataset is split into adjacent n-grams and spaces between words replaced with the \"-\" symbol.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.984639436006546}]}, {"text": "For example, the following sequence of trigrams would be produced from the text amazon gift.", "labels": [], "entities": []}, {"text": "-am ama maz azo zon on-n-g -gi gif ift ft-  To obtain semantic similarity scores for each pair of sentences \u00ed \u00b5\u00ed\u00b1\u0086 and \u00ed \u00b5\u00ed\u00b1\u0086 \u2032, the cosines of the corresponding vector representations are computed.", "labels": [], "entities": []}, {"text": "Previous work on the use of character n-grams for PI has shown that trigrams perform well.", "labels": [], "entities": []}, {"text": "The three runs chosen for submission to SemEval 2016's STS task use sentence representations constructed from trigram embeddings.", "labels": [], "entities": [{"text": "SemEval 2016's STS task", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.8934169888496399}]}, {"text": "Wikipedia articles were pruned using the Gensim package with default parameters.", "labels": [], "entities": []}, {"text": "A total of 108,452 articles are used to construct a character-trigram model for the experiments.", "labels": [], "entities": []}, {"text": "The statistical properties of the Wikipedia-trained model using character trigrams are shown in   Further experiments were conducted for sentence representations based on bigrams and 4-grams.", "labels": [], "entities": []}, {"text": "Although these were not submitted for the task, the results are also reported in the following section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistical properties of Wikipedia dataset used in our  experiment.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 36, "end_pos": 53, "type": "DATASET", "confidence": 0.9547160565853119}]}, {"text": " Table 2: Pearson Correlation Results using character trigrams", "labels": [], "entities": [{"text": "Pearson Correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8472088575363159}]}, {"text": " Table 3: Pearson Correlation Results using character bigrams  and 4-grams", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9056195616722107}]}]}