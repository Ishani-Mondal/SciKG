{"title": [{"text": "UniMelb at SemEval-2016 Task 3: Identifying Similar Questions by Combining a CNN with String Similarity Measures", "labels": [], "entities": [{"text": "Identifying Similar Questions", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.9135816891988119}]}], "abstractContent": [{"text": "This paper describes the results of the participation of The University of Melbourne in the community question-answering (CQA) task of SemEval 2016 (Task 3-B).", "labels": [], "entities": [{"text": "question-answering (CQA) task of SemEval 2016 (Task 3-B)", "start_pos": 102, "end_pos": 158, "type": "TASK", "confidence": 0.6195569684108099}]}, {"text": "We obtained a MAP score of 70.2% on the test set, by combining three classifiers: a NaiveBayes clas-sifier and a support vector machine (SVM) each trained over lexical similarity features, and a convolutional neural network (CNN).", "labels": [], "entities": [{"text": "MAP", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9027069211006165}]}, {"text": "The CNN uses word embeddings and machine translation evaluation scores as features.", "labels": [], "entities": [{"text": "CNN", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9167656302452087}, {"text": "machine translation evaluation", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.7517569661140442}]}], "introductionContent": [{"text": "In this paper we present the system we submitted for the community question-answering (CQA) task of the SemEval 2016 workshop (Task 3-B:).", "labels": [], "entities": [{"text": "community question-answering (CQA) task of the SemEval 2016 workshop", "start_pos": 57, "end_pos": 125, "type": "TASK", "confidence": 0.6095726137811487}]}, {"text": "By finding an automatic way to answer new questions based on existing ones, we unlock an enormous wealth of information stored in online CQA archives.", "labels": [], "entities": [{"text": "CQA archives", "start_pos": 137, "end_pos": 149, "type": "DATASET", "confidence": 0.9283339381217957}]}, {"text": "In the task as specified for the SemEval workshop, we were given 70 query questions.", "labels": [], "entities": [{"text": "SemEval workshop", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.8371016383171082}]}, {"text": "Each question had at most ten candidate questions, which we were to re-rank according to their similarity to the query question.", "labels": [], "entities": []}, {"text": "Each question consisted of a title and a description.", "labels": [], "entities": []}, {"text": "The data was taken from the Qatar Living forum.", "labels": [], "entities": [{"text": "Qatar Living forum", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.9734174013137817}]}, {"text": "The training data set was small: 267 queries, with ten candidate duplicate questions each.", "labels": [], "entities": []}, {"text": "The candidate questions were originally labelled according to the three classes: RELEVANT, PERFECT-MATCH and IRRELEVANT.", "labels": [], "entities": [{"text": "RELEVANT", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9972025156021118}, {"text": "PERFECT-MATCH", "start_pos": 91, "end_pos": 104, "type": "METRIC", "confidence": 0.9948931932449341}, {"text": "IRRELEVANT", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9845693707466125}]}, {"text": "Subsequently, however, RELEVANT and PERFECTMATCH were merged into a single class.", "labels": [], "entities": [{"text": "RELEVANT", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9913076758384705}, {"text": "PERFECTMATCH", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.9874362945556641}]}, {"text": "In an ideal ranking, documents with 1 http://www.qatarliving.com/forum relevant labels were to be ranked higher than the IR-RELEVANT documents.", "labels": [], "entities": [{"text": "IR-RELEVANT documents", "start_pos": 121, "end_pos": 142, "type": "DATASET", "confidence": 0.8355046808719635}]}, {"text": "The system we submitted combines the predictions of three different classifiers through simple voting.", "labels": [], "entities": []}, {"text": "The first two classifiers (naive Bayes and SVM) made use of semantic similarity measures as features, and the third one was a convolutional neural network (CNN) that used word embeddings and machine translation evaluation scores as input.", "labels": [], "entities": []}, {"text": "The combined system achieved a MAP score of 70.2% on the test set.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9843513667583466}]}], "datasetContent": [{"text": "In this section, we will describe the experimental setup and the results of our experiments.", "labels": [], "entities": []}, {"text": "presents basic statistics for the SemEval-2016 Task 3-B dataset.", "labels": [], "entities": [{"text": "SemEval-2016 Task 3-B dataset", "start_pos": 34, "end_pos": 63, "type": "DATASET", "confidence": 0.7834760993719101}]}, {"text": "Each query question was paired up with at most ten archived questions, which we had to re-rank according to relevance.", "labels": [], "entities": []}, {"text": "The dataset was partitioned into three components: (1) a training set, (2) a development set, and (3) a test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The official evaluation results for the SemEval-2016 Test Set", "labels": [], "entities": [{"text": "SemEval-2016 Test Set", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.8512593309084574}]}, {"text": " Table 3: The official evaluation results for the SemEval-2016 Development Set", "labels": [], "entities": [{"text": "SemEval-2016 Development Set", "start_pos": 50, "end_pos": 78, "type": "DATASET", "confidence": 0.8278116385142008}]}]}