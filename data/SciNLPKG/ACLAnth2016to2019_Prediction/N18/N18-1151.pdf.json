{"title": [{"text": "Encoding Conversation Context for Neural Keyphrase Extraction from Microblog Posts", "labels": [], "entities": [{"text": "Neural Keyphrase Extraction from Microblog Posts", "start_pos": 34, "end_pos": 82, "type": "TASK", "confidence": 0.7131832639376322}]}], "abstractContent": [{"text": "Existing keyphrase extraction methods suffer from data sparsity problem when they are conducted on short and informal texts, especially microblog messages.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.8056605160236359}]}, {"text": "Enriching context is one way to alleviate this problem.", "labels": [], "entities": []}, {"text": "Considering that conversations are formed by reposting and replying messages, they provide useful clues for recognizing essential content in target posts and are therefore helpful for keyphrase identification.", "labels": [], "entities": [{"text": "keyphrase identification", "start_pos": 184, "end_pos": 208, "type": "TASK", "confidence": 0.8148225247859955}]}, {"text": "In this paper, we present a neural keyphrase extraction framework for microblog posts that takes their conversation context into account, where four types of neural encoders, namely, averaged embedding, RNN, attention, and memory networks, are proposed to represent the conversation context.", "labels": [], "entities": [{"text": "neural keyphrase extraction", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.7730293869972229}]}, {"text": "Experimental results on Twitter and Weibo datasets 1 show that our framework with such encoders outper-forms state-of-the-art approaches.", "labels": [], "entities": [{"text": "Weibo datasets 1", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.9667510191599528}]}], "introductionContent": [{"text": "The increasing popularity of microblogs results in a huge volume of daily-produced user-generated data.", "labels": [], "entities": []}, {"text": "As a result, such explosive growth of data far outpaces human beings' reading and understanding capacity.", "labels": [], "entities": []}, {"text": "Techniques that can automatically identify critical excerpts from microblog posts are therefore in growing demand.", "labels": [], "entities": []}, {"text": "Keyphrase extraction is one of the techniques that can meet this demand, because it is defined to identify salient phrases, generally formed by one or multiple words, for representing key focus and main topics fora given collection).", "labels": [], "entities": [{"text": "Keyphrase extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8820899426937103}]}, {"text": "Particularly for microblogs, keyphrase extraction has been proven useful to downstream applications such as information retrieval (Choi * Work was done during the internship at Tencent AI Lab.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8734664618968964}, {"text": "information retrieval", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.8196984827518463}]}, {"text": "Our datasets are released at: http://ai.tencent.", "labels": [], "entities": []}, {"text": "com/ailab/Encoding_Conversation_Context_ for_Neural_Keyphrase_Extraction_from_ Microblog_Posts.html Target post for keyphrase extraction: \"I will curse you in that forum\" is the lowest of low.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.7755527794361115}]}, {"text": "You are an embarrassment president Duterte.", "labels": [], "entities": []}, {"text": "Messages forming a conversation:: any head of state will be irked if asked to report to another head of state: Really?", "labels": [], "entities": []}, {"text": "Did Obama really asked Duterte to report to him?", "labels": [], "entities": []}, {"text": "LOL: An example conversation about \"president Duterte\" on Twitter.: The i-th message in conversation ordered by their positing time.", "labels": [], "entities": [{"text": "LOL", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9199674129486084}]}, {"text": "president Duterte: keyphrase to be detected; Italic words: words that are related to the main topic in conversations and can indicate the keyphrase.", "labels": [], "entities": []}, {"text": "et al.,), text summarization (), event tracking (, etc.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7455407977104187}, {"text": "event tracking", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.867070198059082}]}, {"text": "To date, most efforts on keyphrase extraction on microblogs treat messages as independent documents or sentences, and then apply ranking-based models or sequence tagging models ( ) on them.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.8069871366024017}]}, {"text": "It is arguable that these methods are suboptimal for recognizing salient content from short and informal messages due to the severe data sparsity problem.", "labels": [], "entities": [{"text": "recognizing salient content from short and informal messages", "start_pos": 53, "end_pos": 113, "type": "TASK", "confidence": 0.7507393583655357}]}, {"text": "Considering that microblogs allow users to form conversations on issues of interests by reposting with comments 2 and replying to messages for voicing opinions on previous discussed points, these conversations can enrich context for short messages (, and have been proven useful for identifying topicrelated content (.", "labels": [], "entities": []}, {"text": "For example, Table 1 displays a target post with keyphrase \"president Duterte\" and its reposting and replying messages forming a conversation.", "labels": [], "entities": []}, {"text": "Easily identified, critical words are mentioned multiple times in conversations.", "labels": [], "entities": []}, {"text": "Such as in, keyword \"Duterte\" re-occurs in the conversation.", "labels": [], "entities": []}, {"text": "Also, topic-relevant content, e.g., \"head of state\", \"another head of state\", \"Obama\", helps to indicate keyphrase \"president Duterte\".", "labels": [], "entities": []}, {"text": "Such contextual information embedded in a conversation is nonetheless ignored for keyphrase extraction in existing approaches.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.8324383199214935}]}, {"text": "In this paper, we present a neural keyphrase extraction framework that exploits conversation context, which is represented by neural encoders for capturing salient content to help in indicating keyphrases in target posts.", "labels": [], "entities": [{"text": "neural keyphrase extraction", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.718869149684906}]}, {"text": "Conversation context has been proven useful in many NLP tasks on social media, such as sentiment analysis), summarization (, and sarcasm detection (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.9593664109706879}, {"text": "summarization", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.9920220375061035}, {"text": "sarcasm detection", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.8921792209148407}]}, {"text": "We use four context encoders in our model, namely, averaged embedding, RNN), attention (, and memory networks (, which are proven useful in text representation ().", "labels": [], "entities": [{"text": "text representation", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.773626834154129}]}, {"text": "Particularly in this task, to the best of our knowledge, we are the first to encode conversations for detecting keyphrases in microblog posts.", "labels": [], "entities": []}, {"text": "Experimental results on Twitter and Sina Weibo datasets demonstrate that, by effectively encoding context in conversations, our proposed approach outperforms existing approaches by a large margin.", "labels": [], "entities": [{"text": "Sina Weibo datasets", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.8874451120694479}]}, {"text": "Quantitative and qualitative analysis suggest that our framework performs robustly on keyphrases with various length.", "labels": [], "entities": []}, {"text": "Some encoders such as memory networks can detect salient and topic-related content, whose occurrences are highly indicative of keyphrases.", "labels": [], "entities": []}, {"text": "In addition, we test ranking-based models with and without considering conversations.", "labels": [], "entities": []}, {"text": "The results also confirm that conversation context can boost keyphrase extraction of ranking-based models.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.8020735383033752}]}], "datasetContent": [{"text": "Our experiments are conducted on two datasets collected from Twitter and Weibo 3 , respectively.", "labels": [], "entities": [{"text": "Weibo 3", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9300267100334167}]}, {"text": "The Twitter dataset is constructed based on TREC2011 microblog track 4 . To recover conversations, we used Tweet Search API 5 to retrieve full information of a tweet with its \"in reply to status id\" included.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.757239043712616}, {"text": "TREC2011 microblog track", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.9270577828089396}]}, {"text": "Recursively, we searched the \"in reply to\" tweet till the entire conversation is recovered.) for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.5864631036917368}]}, {"text": "In particular, Weibo conversations have an relatively wide range (from 3 to 8,846 words), e.g., one conversation could contain up to 447 messages.", "labels": [], "entities": []}, {"text": "If use the maximum length of all conversations as the input length for encoders, padding the inputs will lead to a sparse matrix.", "labels": [], "entities": []}, {"text": "Therefore, for long conversations (with more than 10 messages), we use KLSum () to produce summaries with a length of 10 messages and then encode the produced summaries.", "labels": [], "entities": []}, {"text": "In contrast, we do not summarize Twitter conversations because their length range is much narrower (from 4 to 1,035 words).", "labels": [], "entities": [{"text": "summarize Twitter conversations", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.908895214398702}]}, {"text": "8  proves that 90% of the hashtagannotated keyphrases match human annotations.", "labels": [], "entities": []}, {"text": "http://www.cs.cmu.edu/ \u02dc ark/TweetNLP/ 10 https://github.com/NLPIR-team/NLPIR  Section 4.1 to 4.5 present quantitative and qualitative analysis of our neural keyprhase extraction models.", "labels": [], "entities": []}, {"text": "Section 4.6 reports the performance of ranking-based models where we test the general applicability of incorporating conversation context to non-neural keyphase extraction methods.", "labels": [], "entities": []}, {"text": "We also tried BiRNN and BiGRU as keyphrase taggers and as context encoders.", "labels": [], "entities": [{"text": "BiGRU", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.6119363307952881}, {"text": "keyphrase taggers", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.6868284493684769}]}, {"text": "They are outperformed by BiLSTM.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.8803463578224182}]}, {"text": "We don't report these results due to the space limitation.", "labels": [], "entities": []}, {"text": "two datasets have over 45 words on average).", "labels": [], "entities": []}, {"text": "The reason for GRU is that its forget gates maybe not well trained to process important content when the training set is small.", "labels": [], "entities": [{"text": "GRU", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.7179408073425293}]}], "tableCaptions": [{"text": " Table 3: Statistics of two datasets. Train, Dev, and  Test denotes training, development, and test set,  respectively. # of annot. msgs: number of mes- sages with keyphrase annotation, each containing  conversation context. # of msgs in context: av- erage count of message in conversation context.  Context length: average count of words in conver- sation context. Vocab: vocabulary size.", "labels": [], "entities": []}, {"text": " Table 4: Comparisons of the average F1 scores (%) and their standard deviations measured on Twitter over  the results of models with 5 sets of parameters for random initialization. The left half reports results of  single-layer taggers; The right half reports results of joint-layer taggers. Each column: results of the same  tagger with different encoders. Each row: results of different taggers with the same encoder. No Encoder:  taggers without encoding context. Abbreviations for context encoders: Avg Emb -averaged embedding;  Att (LSTM) -attention on LSTM; Att (BiLSTM) -attention on BiLSTM; MemNN -memory networks.", "labels": [], "entities": [{"text": "F1", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9965721368789673}, {"text": "Avg Emb -averaged embedding", "start_pos": 504, "end_pos": 531, "type": "METRIC", "confidence": 0.8637365698814392}, {"text": "Att (BiLSTM) -attention", "start_pos": 565, "end_pos": 588, "type": "METRIC", "confidence": 0.8177246749401093}]}, {"text": " Table 5: Comparisons of F1 scores on Weibo. The abbreviations are defined the same as those in", "labels": [], "entities": [{"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9970118999481201}, {"text": "Weibo", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9878756999969482}]}, {"text": " Table 6: The F1 scores of BiLSTM taggers mea- sured on test instances without conversation con- text (%). SL BiLSTM and JL BiLSTM denote  keyphrase tagger as single-layer and joint-layer  BiLSTM, respectively. The other abbreviations  are defined the same as those in", "labels": [], "entities": [{"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9993438124656677}, {"text": "BiLSTM taggers", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.6794677674770355}]}, {"text": " Table 8: Precision, recall, and F1 scores of  ranking-based baselines (%). w/o context: each  target post is treated as a document; w/ con- text: each conversation and its corresponding tar- get post is treated as a document.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9991739392280579}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9969878792762756}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9997881054878235}]}]}