{"title": [{"text": "Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens", "labels": [], "entities": [{"text": "Zero-shot Sequence Labeling", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6333450774351755}, {"text": "Transferring Knowledge from Sentences to Tokens", "start_pos": 29, "end_pos": 76, "type": "TASK", "confidence": 0.8410391708215078}]}], "abstractContent": [{"text": "Can attention-or gradient-based visualization techniques be used to infer token-level labels for binary sequence tagging problems, using networks trained only on sentence-level la-bels?", "labels": [], "entities": [{"text": "binary sequence tagging", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.7001717885335287}]}, {"text": "We construct a neural network architecture based on soft attention, train it as a binary sentence classifier and evaluate against token-level annotation on four different datasets.", "labels": [], "entities": []}, {"text": "Inferring token labels from a network provides a method for quantitatively evaluating what the model is learning, along with generating useful feedback in assistance systems.", "labels": [], "entities": []}, {"text": "Our results indicate that attention-based methods are able to predict token-level labels more accurately, compared to gradient-based methods, sometimes even rivaling the supervised oracle network .", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence labeling is a structured prediction task where systems need to assign the correct label to every token in the input sequence.", "labels": [], "entities": [{"text": "Sequence labeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9382166266441345}]}, {"text": "Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detection, are often formulated as variations of sequence labeling.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.6991137564182281}, {"text": "named entity recognition", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6096619069576263}, {"text": "error detection", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.6581380516290665}]}, {"text": "Recent state-of-the-art models make use of bidirectional LSTM architectures), character-based representations (, and additional external features (.", "labels": [], "entities": []}, {"text": "Optimization of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the task of performing sequence labeling without having access to any training data with token-level annotation.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.6333870738744736}]}, {"text": "Instead of training the model directly to predict the label for each token, the model is optimized using a sentence-level objective and a modified version of the attention mechanism is then used to infer labels for individual words.", "labels": [], "entities": []}, {"text": "While this approach is not expected to outperform a fully supervised sequence labeling method, it opens possibilities for making use of text classification datasets where collecting token-level annotation is not possible or cost-effective.", "labels": [], "entities": [{"text": "text classification", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7078586220741272}]}, {"text": "Inferring token-level labels from a text classification network also provides a method for analyzing and interpreting the model.", "labels": [], "entities": [{"text": "text classification", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7074355483055115}]}, {"text": "Previous work has used attention weights to visualize the focus of neural models in the input data.", "labels": [], "entities": []}, {"text": "However, these analyses have largely been qualitative examinations, looking at only a few examples from the datasets.", "labels": [], "entities": []}, {"text": "By formulating the task as a zero-shot labeling problem, we can provide quantitative evaluations of what the model is learning and where it is focusing.", "labels": [], "entities": []}, {"text": "This will allow us to measure whether the features that the model is learning actually match our intuition, provide informative feedback to end-users, and guide our development of future model architectures.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of zero-shot sequence labeling on 3 different datasets.", "labels": [], "entities": [{"text": "zero-shot sequence labeling", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.5985299348831177}]}, {"text": "In each experiment, the models are trained using only sentence-level annotation and then evaluated based on token-level annotation.", "labels": [], "entities": []}, {"text": "Results for the experiments are presented in Tables 1 and 2.", "labels": [], "entities": []}, {"text": "We first report the sentence-level F-measure in order to evaluate the performance on the general text classification objective.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9080947637557983}, {"text": "general text classification", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.6671573321024576}]}, {"text": "Next, we report the Mean Average Precision (MAP) at returning the active/positive tokens.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 20, "end_pos": 48, "type": "METRIC", "confidence": 0.9702800710995992}]}, {"text": "This measure quency achieve high recall values, but comparatively lower precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9994400143623352}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9987331032752991}]}, {"text": "On the FCE dataset, the F-score is considerably lower at 28.27% -this is due to the difficulty of the task and the supervised system also achieves only 34.76%.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9718734323978424}, {"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9991672039031982}]}, {"text": "The attentionbased system outperforms the alternatives on both of the SemEval evaluations.", "labels": [], "entities": []}, {"text": "The task of detecting sentiment on the token level is quite difficult overall as many annotations are context-specific and require prior knowledge.", "labels": [], "entities": [{"text": "detecting sentiment", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8973165452480316}]}, {"text": "For example, in order to correctly label the phrase \"have Superbowl\" as positive, the system will need to understand that organizing the Superbowl is a positive event for the city.", "labels": [], "entities": []}, {"text": "Performance on the sentence-level classification task is similar for the different architectures on the CoNLL 2010 and FCE datasets, whereas the composition method based on attention obtains an advantage on the SemEval datasets.", "labels": [], "entities": [{"text": "sentence-level classification task", "start_pos": 19, "end_pos": 53, "type": "TASK", "confidence": 0.812746544679006}, {"text": "CoNLL 2010", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.9370966553688049}, {"text": "FCE datasets", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.8528589308261871}, {"text": "SemEval datasets", "start_pos": 211, "end_pos": 227, "type": "DATASET", "confidence": 0.7872027456760406}]}, {"text": "Since the latter architecture achieves competitive performance and also allows for attention-based token labeling, it appears to be the better choice.", "labels": [], "entities": [{"text": "token labeling", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.6708493381738663}]}, {"text": "Analysis of the token-level MAP scores shows that the attention-based sequence labeling model achieves the best performance even when ignoring classification thresholds and evaluating the task through ranking.", "labels": [], "entities": [{"text": "attention-based sequence labeling", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.5931406219800314}]}, {"text": "contains example outputs from the attention-based models, trained on each of the four datasets.", "labels": [], "entities": []}, {"text": "In the first example, the uncertainty detector correctly picks up \"would appreciate if\" and \"possible\", and the error detection model focuses most on the misspelling \"Definetely\".", "labels": [], "entities": []}, {"text": "Both the positive and negative sentiment models have assigned a high weight to the word \"disappointing\", which is something we observed in other examples as well.", "labels": [], "entities": []}, {"text": "The system will learn to focus on phrases that help it detect positive sentiment, but the presence of negative sentiment provides implicit evidence that the overall label is likely not positive.", "labels": [], "entities": []}, {"text": "This is a by-product of the 3-way classification task and future work could investigate methods for extending zero-shot classification to better match this requirement.", "labels": [], "entities": [{"text": "3-way classification task", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7204542855421702}, {"text": "zero-shot classification", "start_pos": 110, "end_pos": 134, "type": "TASK", "confidence": 0.7672330439090729}]}, {"text": "In the second example, the system correctly labels the phrase \"what would be suitable?\" as uncertain, and part of the phrase \"I'm not really sure\" as negative.", "labels": [], "entities": []}, {"text": "It also labels \"specifying\" as an error, possibly expecting a comma before it.", "labels": [], "entities": []}, {"text": "In the third example, the error detection model labels \"Internet\" for the missing determiner, but also captures a more difficult error in \"depended\", which is an incorrect form of the word given the context.", "labels": [], "entities": [{"text": "error detection", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.6436982154846191}]}], "tableCaptions": [{"text": " Table 1: Results for different system configurations on the CoNLL 2010 and FCE datasets. Reporting sentence- level F 1 , token-level Mean Average Precision (MAP), and token-level precision/recall/F 1 .", "labels": [], "entities": [{"text": "CoNLL 2010 and FCE datasets", "start_pos": 61, "end_pos": 88, "type": "DATASET", "confidence": 0.8817120671272278}, {"text": "Reporting sentence- level F 1", "start_pos": 90, "end_pos": 119, "type": "METRIC", "confidence": 0.7651277333498001}, {"text": "token-level Mean Average Precision (MAP)", "start_pos": 122, "end_pos": 162, "type": "METRIC", "confidence": 0.8742411392075675}, {"text": "precision/recall/F 1", "start_pos": 180, "end_pos": 200, "type": "METRIC", "confidence": 0.8281601170698801}]}, {"text": " Table 2: Results for different system configurations on the SemEval Twitter sentiment dataset, separated into pos- itive and negative sentiment detection. Reporting sentence-level F 1 , token-level Mean Average Precision (MAP),  and token-level precision/recall/F 1 .", "labels": [], "entities": [{"text": "SemEval Twitter sentiment dataset", "start_pos": 61, "end_pos": 94, "type": "DATASET", "confidence": 0.6401482596993446}, {"text": "Reporting sentence-level F 1", "start_pos": 156, "end_pos": 184, "type": "METRIC", "confidence": 0.6537169441580772}, {"text": "token-level Mean Average Precision (MAP)", "start_pos": 187, "end_pos": 227, "type": "METRIC", "confidence": 0.8517659306526184}, {"text": "precision/recall/F 1", "start_pos": 246, "end_pos": 266, "type": "METRIC", "confidence": 0.8159815569718679}]}]}