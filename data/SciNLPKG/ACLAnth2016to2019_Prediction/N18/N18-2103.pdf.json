{"title": [{"text": "Objective Function Learning to Match Human Judgements for Optimization-Based Summarization", "labels": [], "entities": [{"text": "Objective Function Learning to Match Human Judgements", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7280072569847107}, {"text": "Summarization", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.7972637414932251}]}], "abstractContent": [{"text": "Supervised summarization systems usually rely on supervision at the sentence or n-gram level provided by automatic metrics like ROUGE, which act as noisy proxies for human judgments.", "labels": [], "entities": [{"text": "Supervised summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6357026994228363}, {"text": "ROUGE", "start_pos": 128, "end_pos": 133, "type": "METRIC", "confidence": 0.9711445569992065}]}, {"text": "In this work, we learn a summary-level scoring function \u03b8 including human judgments as supervision and automatically generated data as regularization.", "labels": [], "entities": []}, {"text": "We extract summaries with a genetic algorithm using \u03b8 as a fitness function.", "labels": [], "entities": []}, {"text": "We observe strong and promising performances across datasets in both automatic and manual evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of extractive summarization can naturally be cast as a discrete optimization problem where the text source is considered as a set of sentences and the summary is created by selecting an optimal subset of the sentences under a length constraint.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.6306425034999847}]}, {"text": "This view entails defining an objective function which is to be maximized by some optimization technique.", "labels": [], "entities": []}, {"text": "In the ideal case, this objective function would encode all the relevant quality aspects of a summary, such that by maximizing all these quality aspects we would obtain the best possible summary.", "labels": [], "entities": []}, {"text": "However, we find several issues with the objective function in previous work on optimizationbased summarization.", "labels": [], "entities": [{"text": "optimizationbased summarization", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.6612291932106018}]}, {"text": "First, the choice of the objective function is based on ad-hoc assumptions about which quality aspects of a summary are relevant (.", "labels": [], "entities": []}, {"text": "This bias can be mitigated via supervised techniques guided by data.", "labels": [], "entities": []}, {"text": "In practice, these approaches use signals at the sentence ( or n-gram) level and then define a combination function to estimate the quality of the whole summary.", "labels": [], "entities": []}, {"text": "This combination \u03b8 determines the trade-off between conflicting quality aspects (importance vs redundancy) encoded in the objective function by making simplistic assumptions to ensure convenient mathematical properties of \u03b8 like linearity or submodularity ().", "labels": [], "entities": []}, {"text": "This restriction comes from computational considerations without conceptual justifications.", "labels": [], "entities": []}, {"text": "More importantly, the supervision signal comes from automatic metrics like ROUGE) which are convenient but noisy approximations for human judgment.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.9885118007659912}]}, {"text": "In this work, we propose to learn the objective function \u03b8 at the summary-level from a pool of manually annotated system summaries to ensure the extraction of summaries considered good by humans.", "labels": [], "entities": []}, {"text": "This explicitly targets the extraction of high-quality summaries as measured by humans and limits undesired gaming of the target evaluation metric.", "labels": [], "entities": []}, {"text": "However, the number of data points is relatively low and the learned \u03b8 might not be well-behaved (high \u03b8 scores for bad summaries) pushing the optimizer to explore regions of the feature space unseen during training where \u03b8 wrongly assumes high scores.", "labels": [], "entities": []}, {"text": "To prevent this scenario, we rely on a large amount of noisy but automatic training data providing supervision on a larger span of the feature space.", "labels": [], "entities": []}, {"text": "Intuitively, it can be viewed as a kind of regularization.", "labels": [], "entities": []}, {"text": "By defining \u03b8 directly at the summary-level, one has access to features like redundancy or global information content without the need to define a combination function from individual sentence scores.", "labels": [], "entities": []}, {"text": "Any feature available at the sentence or n-gram level can be transferred to the summarylevel (by summation), while the summary-level perspective provides access to new features capturing the interactions between sentences.", "labels": [], "entities": []}, {"text": "Furthermore, recent works have demonstrated that global optimization using genetic algorithms without im-posing any mathematical restrictions on \u03b8 is feasible.", "labels": [], "entities": [{"text": "global optimization", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8753053843975067}]}, {"text": "In summary, our contributions are: (1) We propose to learn a summary-level scoring function \u03b8 and use human judgments as supervision.", "labels": [], "entities": []}, {"text": "We demonstrate a simple regularization strategy based on automatic data generation to improve the behavior of \u03b8 under optimization.", "labels": [], "entities": []}, {"text": "(3) We perform both automatic and manual evaluation of the extracted summaries, which indicate competitive performances.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset We use two multi-document summarization datasets from the Text Analysis Conference (TAC) shared tasks: TAC-2008 and TAC-2009.", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC)", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.7599139958620071}, {"text": "TAC-2008", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.8334890604019165}, {"text": "TAC-2009", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.8838428258895874}]}, {"text": "TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9648954272270203}, {"text": "TAC-2009", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.8658496141433716}]}, {"text": "Each topic consists of 10 news articles to be summarized in a maximum of 100 words.", "labels": [], "entities": []}, {"text": "We use only the so-called initial summaries (A summaries), but not the update part.", "labels": [], "entities": []}, {"text": "We used these datasets because all system summaries and the 4 reference summaries were manually evaluated by NIST assessors for content selection (with Pyramid) and overall responsiveness.", "labels": [], "entities": [{"text": "content selection", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.7145538330078125}]}, {"text": "At the time of the shared tasks, 57 systems were submitted to TAC-2008 and 55 to TAC-2009.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9572486281394958}, {"text": "TAC-2009", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9737051129341125}]}, {"text": "For our experiments, we use the Pyramid and the responsiveness annotations.", "labels": [], "entities": [{"text": "Pyramid", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9685639142990112}]}, {"text": "With our notations, for example with TAC-2009, we haven = 55 scored system summaries, m = 44 topics, Di contains 10 documents and \u03b8 i contains 4 reference summaries.", "labels": [], "entities": [{"text": "TAC-2009", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.8934145569801331}]}, {"text": "We also use the recently created German dataset DBS () which contains 10 heterogeneous topics.", "labels": [], "entities": [{"text": "German dataset DBS", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.897246241569519}]}, {"text": "For each topic, 5 summaries were evaluated by trained human annotators but only for content selection with Pyramid.", "labels": [], "entities": []}, {"text": "The summaries have variable sizes and are about 500 words long.", "labels": [], "entities": []}, {"text": "Baselines (1) ICSI () is a global linear optimization approach that extracts a summary by solving a maximum coverage problem considering the most frequent bigrams in the source documents.", "labels": [], "entities": [{"text": "ICSI", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.6447158455848694}, {"text": "global linear optimization", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.6252217491467794}]}, {"text": "ICSI has been among the best systems in a standard ROUGE evaluation ( .  Objective function learning In this section, we measure how well our models can predict human judgments.", "labels": [], "entities": [{"text": "ICSI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7631090879440308}]}, {"text": "We train each \u03b8 in a leave-one-out cross-validation setup for each dataset and compare their performance to the summary scoring function of baselines like it was done previously (.", "labels": [], "entities": []}, {"text": "Each individual feature is also included in the baselines.", "labels": [], "entities": []}, {"text": "Correlations are measured with two complementary metrics: Spearman's \u03c1 and Normalized Discounted Cumulative Gain (NDCG).", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.9072713057200114}, {"text": "Normalized Discounted Cumulative Gain (NDCG)", "start_pos": 75, "end_pos": 119, "type": "METRIC", "confidence": 0.8156709841319493}]}, {"text": "Spearman's \u03c1 is a rank correlation metric, which compares the ordering of systems induced by \u03b8 and the ordering of systems induced by human judgments.", "labels": [], "entities": []}, {"text": "NDCG is a metric that compares ranked lists and puts more emphasis on the top elements with logarithmic decay weighting.", "labels": [], "entities": [{"text": "NDCG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9003906846046448}]}, {"text": "Intuitively, it captures how well \u03b8 can recognize the best summaries.", "labels": [], "entities": []}, {"text": "The optimization scenario benefits from high NDCG scores because only summaries with high \u03b8 scores are extracted.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "For simplicity, we report the average over the 3 datasets.", "labels": [], "entities": []}, {"text": "Each \u03b8 is compared against the best performing baseline for the data annotation type it was trained on (R2, responsiveness or pyramid).", "labels": [], "entities": []}, {"text": "The trained models perform substantially and consistently bet-5 Best baseline for R2 and Responsiveness is: KL divergence on bigrams; for Pyramid: KL divergence on unigrams ter than the best baselines.", "labels": [], "entities": [{"text": "Responsiveness", "start_pos": 89, "end_pos": 103, "type": "METRIC", "confidence": 0.9089229702949524}]}, {"text": "They have a high correlation with human judgments and are capable of identifying good summaries.", "labels": [], "entities": []}, {"text": "However, we need to test whether the combination of the three \u03b8's is well behaved under optimization.", "labels": [], "entities": []}, {"text": "For this, we perform an evaluation of the summaries extracted by the genetic optimizer.", "labels": [], "entities": []}, {"text": "Summaries Evaluation Now, we evaluate the summaries extracted by the genetic optimizer with \u03b8 as fitness function (noted (\u03b8, Gen)).", "labels": [], "entities": []}, {"text": "We still train \u03b8 with leave-one-out cross-validation.", "labels": [], "entities": []}, {"text": "To evaluate summaries, we report the ROUGE variant identified by as strongly correlating with human evaluation methods: ROUGE-2 (R2) recall with stemming and stopwords not removed.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.8902491927146912}, {"text": "ROUGE-2 (R2)", "start_pos": 120, "end_pos": 132, "type": "METRIC", "confidence": 0.8693676143884659}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.5961227416992188}]}, {"text": "We also report JS2, the Jensen-Shannon divergence between bigrams in the reference summaries and the candidate system summary ().", "labels": [], "entities": [{"text": "JS2", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.5896524786949158}]}, {"text": "The last metric is S3 ( , a combination of several existing metrics trained explicitly to maximize its correlation with human judgments.", "labels": [], "entities": []}, {"text": "Finally, our approach aims at improving summarization systems based on human judgments, therefore we also setup a manual evaluation for the two English datasets.", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.985481321811676}, {"text": "English datasets", "start_pos": 144, "end_pos": 160, "type": "DATASET", "confidence": 0.6910611540079117}]}, {"text": "Two annotators were given the summaries of every system for 10 randomly selected topic of both TAC-2008 and TAC-2009.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.9439141750335693}, {"text": "TAC-2009", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.8168079853057861}]}, {"text": "They annotated (with a Cohen's kappa of 0.73) summaries on a LIKERT scale following the responsiveness guidelines.", "labels": [], "entities": [{"text": "LIKERT scale", "start_pos": 61, "end_pos": 73, "type": "METRIC", "confidence": 0.9528144001960754}]}, {"text": "The results are reported in.", "labels": [], "entities": []}, {"text": "We perform significance testing with Approximate Random Testing to compare differences between two means in cross-validation . While \u03b8's trained on human judgments have a high correlation with human judgments, they behave badly under optimization.", "labels": [], "entities": []}, {"text": "This effect is much less visible for \u03b8 R2 because the data points have been sampled to cover the feature space.", "labels": [], "entities": []}, {"text": "We observe the effectiveness of the regularization because each \u03b8 R2/pyr/resp performs much worse individually than the combined \u03b8.", "labels": [], "entities": [{"text": "regularization", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.965330958366394}]}, {"text": "We also note that (\u03b8 R2 , Gen) performs on par with the other supervised baseline SFOUR but both are outperformed by exploiting human judgments.", "labels": [], "entities": [{"text": "SFOUR", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.4508211314678192}]}, {"text": "(\u03b8, Gen) is consistently and often significantly better than baselines across datasets and metrics.", "labels": [], "entities": [{"text": "Gen)", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8860596716403961}]}, {"text": "In particular, humans tend to prefer the summaries extracted by  (\u03b8, Gen).", "labels": [], "entities": []}, {"text": "Manual inspection of summaries reveals that (\u03b8, Gen) has lower redundancy than previous baselines thanks to summary-level features.", "labels": [], "entities": [{"text": "Gen", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9045993685722351}]}, {"text": "Important Features Since we used a linear regression, we can estimate the contribution of a feature by the amplitude of its associated weight.", "labels": [], "entities": []}, {"text": "The two best features (n-gram distributions and redundancy) are summary-level features, which confirms the advantage of using a summary-level scoring function.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of learned \u03b8's compared to  the best baselines for each type annotation types.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of systems across 3 datasets evaluated with ROUGE-2 recall; JS divergence on  bigrams; S3 and Human annotations.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9609188437461853}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.7193670272827148}, {"text": "JS divergence", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.5232305824756622}]}]}