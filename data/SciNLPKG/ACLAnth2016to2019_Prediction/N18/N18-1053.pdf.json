{"title": [{"text": "Solving Data Sparsity for Aspect based Sentiment Analysis using Cross-linguality and Multi-linguality", "labels": [], "entities": [{"text": "Aspect based Sentiment Analysis", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.7312650382518768}]}], "abstractContent": [{"text": "Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining , text mining etc.", "labels": [], "entities": [{"text": "data mining", "start_pos": 128, "end_pos": 139, "type": "TASK", "confidence": 0.85598424077034}, {"text": "text mining", "start_pos": 142, "end_pos": 153, "type": "TASK", "confidence": 0.8095542192459106}]}, {"text": "The issue of data spar-sity poses a great challenge in creating efficient word representation model for solving the underlying problem.", "labels": [], "entities": []}, {"text": "The problem is more intensified in resource-poor scenario due to the absence of sufficient amount of corpus.", "labels": [], "entities": []}, {"text": "In this work, we propose to minimize the effect of data sparsity by leveraging bilingual word embeddings learned through a parallel corpus.", "labels": [], "entities": []}, {"text": "We train and evaluate Long Short Term Memory (LSTM) based architecture for aspect level sentiment classification.", "labels": [], "entities": [{"text": "aspect level sentiment classification", "start_pos": 75, "end_pos": 112, "type": "TASK", "confidence": 0.7312799394130707}]}, {"text": "The neural network architecture is further assisted by the hand-crafted features for the prediction.", "labels": [], "entities": []}, {"text": "We show the efficacy of the proposed model against state-of-the-art methods in two experimental setups i.e. multilingual and cross-lingual.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis) tries to automatically extract the subjective information from a user written textual content and classifies it into one of the predefined set of classes, e.g. positive, negative, neutral or conflict.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8808965086936951}]}, {"text": "Sentiment analysis performed on coarser level (i.e. document or sentence level) does not provide enough information fora user who is critical of finer details such as battery life of a laptop or service of a restaurant etc.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9160298705101013}]}, {"text": "Aspect level sentiment analysis (ABSA) () serves such a purpose, which first identifies the features (or aspects) mentioned in the text and then classifies it into one of the target classes.", "labels": [], "entities": [{"text": "Aspect level sentiment analysis (ABSA)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.806058440889631}]}, {"text": "For example, the following review is fora restaurant where the writer shares her/his experience.", "labels": [], "entities": []}, {"text": "Though s/he likes the food but certainly not happy with the service.", "labels": [], "entities": []}, {"text": "One of the best food we had in awhile but the service was very disappointing.", "labels": [], "entities": []}, {"text": "Analyzing such reviews on sentence level will reflect only an overall sentiment (i.e. conflict) of the sentence ignoring critical information such as food and service qualities.", "labels": [], "entities": []}, {"text": "However, ABSA will first identify all the aspects in the text (i.e. food and service) and then associate positive with food and negative with service.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.4416729211807251}]}, {"text": "Identification of aspect terms is also known as aspect term extraction or opinion target extraction.", "labels": [], "entities": [{"text": "Identification of aspect terms", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8775117993354797}, {"text": "aspect term extraction or opinion target extraction", "start_pos": 48, "end_pos": 99, "type": "TASK", "confidence": 0.6416936644486019}]}, {"text": "In this work, we focus on the second problem i.e. aspect level sentiment classification.", "labels": [], "entities": [{"text": "aspect level sentiment classification", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.6556795090436935}]}, {"text": "Literature survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years.", "labels": [], "entities": [{"text": "Literature survey", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9485199153423309}, {"text": "sentiment analysis", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.9620994329452515}]}, {"text": "However, most of these researches are focused on resource-rich language like English.", "labels": [], "entities": []}, {"text": "Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9470654726028442}]}, {"text": "Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-trivial to solve.", "labels": [], "entities": []}, {"text": "The research on ABSA involving Indian languages has started only very recently, for e.g. (Akhtar et al., 2016a,b).", "labels": [], "entities": [{"text": "ABSA", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.972878634929657}]}], "datasetContent": [{"text": "In this section, we describe the datasets, experimental setup, results and provide necessary analysis.", "labels": [], "entities": []}, {"text": "We use Hindi ABSA dataset released by (Akhtar et al., 2016a) for our evaluation purpose.", "labels": [], "entities": [{"text": "Hindi ABSA dataset released", "start_pos": 7, "end_pos": 34, "type": "DATASET", "confidence": 0.736236684024334}]}, {"text": "A total of 5,417 review sentences are present along with 4,509 aspect terms.", "labels": [], "entities": []}, {"text": "Each aspect term belongs to one of the four sentiment classes: 'positive', 'negative', 'neutral' and 'conflict'.", "labels": [], "entities": []}, {"text": "We split the dataset into 70%, 10% and 20% as training, development and test, respectively for the experiment.", "labels": [], "entities": []}, {"text": "For French case, we use the SemEval-2016 shared task on ABSA () restaurant dataset.", "labels": [], "entities": [{"text": "ABSA () restaurant dataset", "start_pos": 56, "end_pos": 82, "type": "DATASET", "confidence": 0.7643110752105713}]}, {"text": "It consists of 2,429 review sentences and 3,482 aspect terms.", "labels": [], "entities": []}, {"text": "In cross-lingual setup, we utilize English dataset of SemEval-2014 shared task on ABSA () for training and Hindi ABSA dataset for testing.", "labels": [], "entities": [{"text": "Hindi ABSA dataset", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.5542529920736948}]}, {"text": "The English dataset comprises of product reviews in two domains i.e. restaurant and laptop.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8850240409374237}]}, {"text": "However, we only employ laptop domain dataset as most of the reviews in Hindi ABSA datasets belong to the electronics domain.", "labels": [], "entities": [{"text": "laptop domain dataset", "start_pos": 24, "end_pos": 45, "type": "DATASET", "confidence": 0.6804155806700388}, {"text": "Hindi ABSA datasets", "start_pos": 72, "end_pos": 91, "type": "DATASET", "confidence": 0.593154231707255}]}, {"text": "For training in cross-lingual setup, we combine the training and gold test dataset together.", "labels": [], "entities": [{"text": "gold test dataset", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.686639666557312}]}, {"text": "In total, there are 3,845 review sentences comprising of 3,012 aspect terms.", "labels": [], "entities": []}, {"text": "For EnglishFrench case, we use English restaurant dataset of SemEval-2016 shared task on ABSA ( for the training and French ABSA dataset () for evaluation.", "labels": [], "entities": [{"text": "English restaurant dataset", "start_pos": 31, "end_pos": 57, "type": "DATASET", "confidence": 0.5866923630237579}, {"text": "French ABSA dataset", "start_pos": 117, "end_pos": 136, "type": "DATASET", "confidence": 0.7827327648798624}]}, {"text": "The SemEval-2016 English restaurant dataset contains 3,365 aspect terms across 2,676 review sentences.", "labels": [], "entities": [{"text": "SemEval-2016 English restaurant dataset", "start_pos": 4, "end_pos": 43, "type": "DATASET", "confidence": 0.56776362657547}]}, {"text": "We use Python based neural network library, Keras for implementation.", "labels": [], "entities": []}, {"text": "For English-Hindi, all the four classes (namely positive, negative, neutral and conflict) were considered, whereas for English-French three classes (all except conflict 4 http://keras.io class) were used for classification.", "labels": [], "entities": []}, {"text": "Since there is no false class, we use accuracy value as metric to measure the performance of the system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.99888676404953}]}, {"text": "Also, we utilize accuracy value for the direct comparison with the existing state-of-the-art systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9993846416473389}]}, {"text": "LSTM network is trained with early stopping criteria on (i.e. preserving best learned parameter at each epoch).", "labels": [], "entities": []}, {"text": "We set the number of epochs and patience value as 100 & 20 respectively.", "labels": [], "entities": [{"text": "patience", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9940008521080017}]}, {"text": "In other words, we run the experiments for maximum 100 epochs and if validation loss does not reduce for consecutive 20 epochs training stops and reports the best epoch attained so far.", "labels": [], "entities": []}, {"text": "As activation function, we utilize 'tanh' at the intermediate layers, while for classification, we use 'softmax' at the output layer.", "labels": [], "entities": []}, {"text": "To prevent the network from over-fitting, we incorporate an efficient regularization technique called 'Dropout'.", "labels": [], "entities": []}, {"text": "At each layer of training, dropout skips few hidden neurons randomly.", "labels": [], "entities": []}, {"text": "We fix dropout rate to be 45% during training while for optimization we use 'adam' optimizer ().", "labels": [], "entities": []}, {"text": "Experimental results for aspect sentiment classification in multi-lingual and cross-lingual setups are reported in for both the language pairs.", "labels": [], "entities": [{"text": "aspect sentiment classification", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.90951536099116}]}, {"text": "In total, we evaluate our model for four cases i.e. a. En-Hi multi-lingual, b.", "labels": [], "entities": []}, {"text": "The non-root four-boxed nodes report performance of the respective methods for the four cases.", "labels": [], "entities": []}, {"text": "The left subtree represents LSTM based baseline system that utilizes monolingual word embedding (WE) (i.e. word2vec model trained only on 7.2M Hindi and French sentences respectively).", "labels": [], "entities": []}, {"text": "Whereas the right subtree represents usage of bilingual word embeddings in all the cases.", "labels": [], "entities": []}, {"text": "Comparison between monolingual WE and bilingual WE shows competing results.", "labels": [], "entities": []}, {"text": "Monolingual WE (a M : 63.64%) in multi-lingual scenario performs better than the bilingual WE (a B : 62.51%) for English-Hindi case, while bilingual WE (c B : 70.89%) reports better performance as compared with monolingual WE (a M : 66.29%) for English-French case.", "labels": [], "entities": []}, {"text": "We observe a performance loss of approx. 1 point with bilingual embeddings for English-Hindi case.", "labels": [], "entities": []}, {"text": "However, after addressing the problem of data sparsity (i.e. when OOV words are translated and corresponding English word embeddings are computed) the same LSTM network reports an improved accuracy value of 64.83% (a BO ) for English-Hindi case, thus observ- ing a performance increase of more than 2 points.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9988443851470947}, {"text": "BO )", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.9794694781303406}]}, {"text": "For English-French case, we also observe the improvement with embeddings of OOVs.", "labels": [], "entities": []}, {"text": "This suggests that the richness of target language (English) word embeddings helps the system to efficiently solve the problem encountered in resource-poor source language.", "labels": [], "entities": []}, {"text": "Since the resources are limited for resource-poor language we try to leverage the high-quality lexicon features of English in our system.", "labels": [], "entities": []}, {"text": "Consequently, we introduce the extracted features of Section 3.2 to the network.", "labels": [], "entities": []}, {"text": "For EnglishHindi multi-lingual scenario, the performance increments from A1 to A2 to A3 indicate that the resource-richness of English language plays a crucial role in classification.", "labels": [], "entities": []}, {"text": "While we incorporate English side lexicon features for English-French multi-lingual scenario, we observe no performance improvement like the others.", "labels": [], "entities": []}, {"text": "For this case, our system reports an accuracy of 72.42% with (c BOF 1 ) and without (c BO ) the use of extra features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.999660849571228}, {"text": "BOF 1", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9803698062896729}, {"text": "BO", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9935792684555054}]}, {"text": "Results of cross-lingual setup for English-Hindi case, where we train the network utilizing English dataset and evaluate the model on Hindi dataset, are reported in row 2 of the four-boxed nodes in.", "labels": [], "entities": []}, {"text": "The baseline model for cross-lingual setups (left subtree of) employs monolingual word embeddings of English and Hindi for training and testing respectively.", "labels": [], "entities": []}, {"text": "Since the vector spaces of two different languages are completely unrelated, it is no surprise that the baseline system achieves merely 16.29% (b M ) accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9671082496643066}]}, {"text": "Using only the bilingual word embeddings the system achieves 48.94% (b B ) accuracy.", "labels": [], "entities": [{"text": "48.94% (b B )", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.6961070150136948}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.5824981927871704}]}, {"text": "By increasing the coverage of input word embeddings using machine translation the proposed system obtains an increased accuracy of 50.79% (b BO ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9993875026702881}, {"text": "BO", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.7555025815963745}]}, {"text": "This improvement inaccuracy, again, justifies the use of translated words for obtaining the word embeddings.", "labels": [], "entities": []}, {"text": "Further, with the inclusion of target-side lexicon based features our proposed approach reports a significant performance improvement of approximately 6-10 points for all the three archi- Results of English-French cross-lingual scenario are reported in row 4 of the four-boxed nodes in.", "labels": [], "entities": []}, {"text": "We observe similar phenomenon in cross-lingual setup with the English-French case as well.", "labels": [], "entities": []}, {"text": "The baseline system, where we utilize separate monolingual WE for training and testing in English and French respectively, reports an accuracy of 50.69% (d M ), while employing bilingual embeddings the system obtains a sharp jump of approx. 13 points with an accuracy value of 63.64% (d B ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9994068145751953}, {"text": "accuracy", "start_pos": 259, "end_pos": 267, "type": "METRIC", "confidence": 0.9994593262672424}]}, {"text": "Further, with the inclusion of OOV words and lexicon features performance of the system improves to 65.32% (d BO ) and 69.49% ), respectively.", "labels": [], "entities": [{"text": "BO", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9898865222930908}]}, {"text": "We observe four phenomena from these results: i) use of lexicon-based features is the driving force in predicting the sentiment; ii) qualitative lexicons of the resource-rich language can assist in solving the problems of resource-poor languages; iii) embeddings of the OOV words improves the performance of the system with or without assistance of extra features; and iv) use of separate LSTMs (one for word embeddings and the other for features) helps the network to efficiently extract relevant features for prediction without interfering each other (except for the multi-lingual EnglishFrench scenario).", "labels": [], "entities": [{"text": "predicting the sentiment", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.8722351392110189}]}], "tableCaptions": [{"text": " Table 1: Comparative analysis of monolingual embed- dings and bilingual embeddings in multi-lingual setup.", "labels": [], "entities": []}]}