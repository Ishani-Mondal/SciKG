{"title": [{"text": "What we need to learn if we want to do and not just talk", "labels": [], "entities": []}], "abstractContent": [{"text": "In task-oriented dialog, agents need to generate both fluent natural language responses and correct external actions like database queries and updates.", "labels": [], "entities": []}, {"text": "We show that methods that achieve state of the art performance on synthetic datasets, perform poorly in real world dialog tasks.", "labels": [], "entities": []}, {"text": "We propose a hybrid model, where nearest neighbor is used to generate fluent responses and Sequence-to-Sequence (Seq2Seq) type models ensure dialogue coherency and generate accurate external actions.", "labels": [], "entities": []}, {"text": "The hybrid model on an internal customer support dataset achieves a 78% relative improvement in fluency , and a 200% improvement in external call accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9030669927597046}]}], "introductionContent": [{"text": "Many commercial applications of artificial agents require task-oriented conversational agents that help customers achieve a specific goal, such as making or cancelling a payment or reservation ().", "labels": [], "entities": []}, {"text": "These chatbots must extract relevant information from the user, provide relevant knowledge to her, and issue appropriate system calls to achieve the goal.", "labels": [], "entities": []}, {"text": "Supervised approaches such as seq2seq models (), have recently gained attention in non-task oriented dialog, due to their ability to perform end-to-end learning from expert dialogues 1 , removing the need for many of the independent modules in traditional systems such as, natural language understanding, dialog state tracker and natural language generator.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 273, "end_pos": 303, "type": "TASK", "confidence": 0.6619976262251536}, {"text": "dialog state tracker", "start_pos": 305, "end_pos": 325, "type": "TASK", "confidence": 0.7051420410474142}]}, {"text": "Seq2Seq models have also shown promising results on small domain or synthetic task-oriented dialog datasets.", "labels": [], "entities": []}, {"text": "However, performance was much worse when we applied these models to real world datasets.", "labels": [], "entities": []}, {"text": "This is in part because end-to-end methods, in general, require large amounts of data before they are able to generate fluent textual responses.", "labels": [], "entities": []}, {"text": "In real world settings, words chosen by human users and agents are not constrained to a fixed vocabulary, and hence we see many lexical variations even among semantically similar dialogs.", "labels": [], "entities": []}, {"text": "To ensure that information is both conveyed and understood, we want responses to be fluent as well as coherent.", "labels": [], "entities": []}, {"text": "We say a response is coherent if it is a sensible response in the dialogue context.", "labels": [], "entities": []}, {"text": "Table 1 shows responses generated by a variant of the seq2seq model, when trained on real customeragent chat transcripts.", "labels": [], "entities": []}, {"text": "The response of the chatbot during the fourth turn 2 in, accepting the customer's expression of gratitude, is coherent and fluent.", "labels": [], "entities": []}, {"text": "Coherence of a response does not necessarily guarantee fluency.", "labels": [], "entities": []}, {"text": "The generated response during the second turn is coherent but not fluent.", "labels": [], "entities": []}, {"text": "On our customer support dataset, seq2seq models performed well with salutations, but performed poorly both in terms of fluency and coherency on intermediate responses.", "labels": [], "entities": []}, {"text": "The reason being, salutations contain minimal lexical variations across dialogs and occur more frequently when compared to other utterances.", "labels": [], "entities": []}, {"text": "( use beam search decoding in Neural Machine Translation to mitigate fluency issues on larger translation datasets.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6990109086036682}]}, {"text": "Typically increasing the beam size improves translation quality, however, increasing beam sizes in Neural MT has shown to produce poor translations (.", "labels": [], "entities": [{"text": "Neural MT", "start_pos": 99, "end_pos": 108, "type": "TASK", "confidence": 0.5488513112068176}]}, {"text": "We propose nearest neighbor based approaches that can directly use and replay available expert utterances.", "labels": [], "entities": []}, {"text": "This removes the need for the models to learn the grammar of the language, and allows the models to focus on learning what to say, rather than how to say it.", "labels": [], "entities": []}, {"text": "The nearest neighbor-based show that they perform poorly in predicting external actions and at ensuring dialogue level coherency.", "labels": [], "entities": []}, {"text": "In contrast, the skip-connection seq2seq models we propose here, learn when to produce external actions and produce more coherent dialogues.", "labels": [], "entities": []}, {"text": "We propose a hybrid model that brings together the strengths of both the approaches.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: \u2022 We propose skip-connections to handle multiturn dialogue that outperforms previous models.", "labels": [], "entities": []}, {"text": "\u2022 We propose a hybrid model where nearest neighbor-based models generate fluent responses and skip-connection models generate accurate responses and external actions.", "labels": [], "entities": []}, {"text": "We show the effectiveness of the belief state representations obtained from the skip-connection model by comparing against previous approaches.", "labels": [], "entities": []}, {"text": "\u2022 To the best of our knowledge, our paper makes the first attempt at evaluating state of the art models on a large real world task with human users.", "labels": [], "entities": []}, {"text": "We show that methods that achieve state of the art performance on synthetic datasets, perform poorly in real world dialog tasks.", "labels": [], "entities": []}, {"text": "Comparing Tables 2 and 3, we seethe impact of moving from synthetic to real world datasets, and as a result, find issues with previously proposed models that may have been obscured by the simplicity and regularity of synthetic datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use data from bAbI (Task1 and Task2) to evaluate our models.", "labels": [], "entities": []}, {"text": "Other dialog tasks in bAbI require the model to mimic a knowledge base i.e., memorize it.", "labels": [], "entities": []}, {"text": "This is not a suitable strategy for our application, since in practice knowledge bases undergo frequent changes, making this infeasible.", "labels": [], "entities": []}, {"text": "In the bAbI task, the user interacts with an agent in a simulated restaurant reservation application, by providing her constraints, such as place, cuisine, number of people or price range.", "labels": [], "entities": []}, {"text": "The agent or chatbot performs external actions or SQL-like queries (api call) to retrieve information from the knowledge base of restaurants.", "labels": [], "entities": []}, {"text": "We used 80% of the data for training (of which 10% was used for validation) and the remaining 20% for testing.", "labels": [], "entities": []}, {"text": "We also evaluate our models on an internal customer support dataset of 160k chat transcripts containing 3 million interactions.", "labels": [], "entities": []}, {"text": "We limit the number of turns to 20.", "labels": [], "entities": []}, {"text": "We will refer to this dataset as CS large.", "labels": [], "entities": []}, {"text": "We perform spell correction, deidentification to remove customer sensitive information, lexical normalization particularly of lingo words such as, lol and ty.", "labels": [], "entities": [{"text": "spell correction", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.8247517049312592}, {"text": "lexical normalization", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.7534248530864716}]}, {"text": "Generalizing such entities reduces the amount of training data required.", "labels": [], "entities": []}, {"text": "The values must be reinserted, currently by a human in the loop.", "labels": [], "entities": []}, {"text": "We have also masked product and the organization name in the examples.", "labels": [], "entities": []}, {"text": "The use of MT evaluation metrics to evaluate dialogue fluency with just one reference has been debated ( . There is still no good alternative to evaluate dialog systems, and so we continue to report fluency using BLEU (BiLingual Evaluation Understudy (), in addition to other metrics and human evaluations.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.8953421413898468}, {"text": "BLEU", "start_pos": 213, "end_pos": 217, "type": "METRIC", "confidence": 0.9929224252700806}]}, {"text": "Coherency also requires measuring correctness of the external actions which we measure using a metric we call, Exact Query Match (EQM), which represents the fraction of times the api call matched the ground truth query issued by the human agent.", "labels": [], "entities": [{"text": "Exact Query Match (EQM)", "start_pos": 111, "end_pos": 134, "type": "METRIC", "confidence": 0.7733599791924158}]}, {"text": "We do not assign any credit to partial matches.", "labels": [], "entities": []}, {"text": "In addition, we report the precision (P), recall (R) and accuracy (Acc) achieved by the models in predicting whether to make an api call (positive) or not (negative).", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 27, "end_pos": 40, "type": "METRIC", "confidence": 0.9188390076160431}, {"text": "recall (R)", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9460737258195877}, {"text": "accuracy (Acc)", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.8531227856874466}]}, {"text": "Obtaining and aligning api calls with the chat transcripts is often complex as such information is typically stored in multiple confidential logs.", "labels": [], "entities": []}, {"text": "In order to measure coherency with respect to api calls, we randomly sampled 1000 chat tran- scripts and asked human agents to hand annotate the api calls wherever appropriate.", "labels": [], "entities": []}, {"text": "We will refer to this labeled dataset as CS small.", "labels": [], "entities": []}, {"text": "One caveat to the above evaluations is that they are based on customer responses to the actual human agent interactions, and are not fully indicative of how customers would react to the real automated system in practice.", "labels": [], "entities": []}, {"text": "Another disadvantage of using automated evaluation with just one reference, is that the score (BLEU) penalizes valid responses that maybe lexically different from the available agent response.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9866570830345154}]}, {"text": "To overcome this issue, we conducted online experiments with human agents.", "labels": [], "entities": []}, {"text": "We used 5 human users and 2 agents.", "labels": [], "entities": []}, {"text": "On average each user interacted with an agent on 10 different issues that needed resolution.", "labels": [], "entities": []}, {"text": "To compare against our baseline, each user interacted with the Model 4, 5 and 10 using the same issues.", "labels": [], "entities": []}, {"text": "This resulted in \u2248 50 dialogues from each of the models.", "labels": [], "entities": []}, {"text": "After every response from the user, the human agent was allowed to select one of the top five responses the system selected.", "labels": [], "entities": []}, {"text": "We refer to the selected response as A.", "labels": [], "entities": [{"text": "A", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9897040724754333}]}, {"text": "The human agent was asked to make minimal modifications to the selected response, resulting in a response A . If the responses suggested were completely irrelevant, the human agent was allowed to type in the most suitable response.", "labels": [], "entities": []}, {"text": "We then computed the BLEU between the system generated responses (As) and human generated responses (A s), referred to as Online-BLEU in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9990028738975525}]}, {"text": "Since the human agent only made minimal changes where appropriate, we believe the BLEU score would now be more correlated to human judgments.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9874255359172821}]}, {"text": "Since CS large did not contain any api calls, we only report BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9993591904640198}]}, {"text": "The results obtained with models 4, 5 and 10 on CS large are shown in (column 4).", "labels": [], "entities": [{"text": "CS large", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9406745731830597}]}, {"text": "Model 10 performs better than Models 4 and 5.", "labels": [], "entities": []}, {"text": "We do not measure inter-annotator agreement as each human user can take a different dialog trajectory.", "labels": [], "entities": []}, {"text": "We noticed that the approach mimics certain interesting human behavior.", "labels": [], "entities": []}, {"text": "For example, in, the chatbot detects that the user is frustrated and responds with smileys and even makes exceptions on the return policy.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results with variants of the seq2seq model on the bAbI dataset.", "labels": [], "entities": [{"text": "bAbI dataset", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.964292049407959}]}, {"text": " Table 3: Results with the Nearest Neighbor approach on customer support data (CS small).", "labels": [], "entities": [{"text": "customer support data (CS small)", "start_pos": 56, "end_pos": 88, "type": "DATASET", "confidence": 0.633692170892443}]}, {"text": " Table 4: Results with the Nearest Neighbor approach on customer support data (CS large).", "labels": [], "entities": [{"text": "customer support data (CS large)", "start_pos": 56, "end_pos": 88, "type": "DATASET", "confidence": 0.6388503781386784}]}]}