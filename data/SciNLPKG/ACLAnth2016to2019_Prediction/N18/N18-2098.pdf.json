{"title": [{"text": "A Mixed Hierarchical Attention based Encoder-Decoder Approach for Standard Table Summarization", "labels": [], "entities": [{"text": "Standard Table Summarization", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.6868596971035004}]}], "abstractContent": [{"text": "Structured data summarization involves generation of natural language summaries from structured input data.", "labels": [], "entities": [{"text": "Structured data summarization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7601951360702515}, {"text": "generation of natural language summaries", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.5886378109455108}]}, {"text": "In this work, we consider summarizing structured data occurring in the form of tables as they are prevalent across a wide variety of domains.", "labels": [], "entities": [{"text": "summarizing structured data", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.8894694844881693}]}, {"text": "We formulate the standard table summarization problem, which deals with tables conforming to a single pre-defined schema.", "labels": [], "entities": [{"text": "table summarization", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.5250604599714279}]}, {"text": "To this end, we propose a mixed hierarchical attention based encoder-decoder model which is able to leverage the structure in addition to the content of the tables.", "labels": [], "entities": []}, {"text": "Our experiments on the publicly available WEATHERGOV dataset show around 18 BLEU (\u223c 30%) improvement over the current state-of-the-art.", "labels": [], "entities": [{"text": "WEATHERGOV dataset", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.9069433510303497}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9997137188911438}]}], "introductionContent": [{"text": "Abstractive summarization techniques from structured data seek to exploit both structure and content of the input data.", "labels": [], "entities": []}, {"text": "The type of structure on the input side can be highly varied ranging from key-value pairs (e.g. WIKIBIO (), source code (, ontologies (, or tables (, each of which require significantly varying approaches.", "labels": [], "entities": [{"text": "WIKIBIO", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.8485535979270935}]}, {"text": "In this paper, we focus on generating summaries from tabular data.", "labels": [], "entities": [{"text": "summaries", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.9300639033317566}]}, {"text": "Now, inmost practical applications such as finance, healthcare or weather, data in a table are arranged in rows and columns where the schema is known beforehand.", "labels": [], "entities": []}, {"text": "However, change in the actual data values can necessitate drastically different output summaries.", "labels": [], "entities": []}, {"text": "Examples shown in the figure 1 have a predefined schema obtained from the WEATHERGOV dataset () and its corresponding weather report summary.", "labels": [], "entities": [{"text": "WEATHERGOV dataset", "start_pos": 74, "end_pos": 92, "type": "DATASET", "confidence": 0.9425472021102905}]}, {"text": "Therefore, the problem that we seek to address in this paper is to generate abstractive summaries of tables conforming to a predefined fixed schema (as opposed to cases where the schema is unknown).", "labels": [], "entities": []}, {"text": "We refer to this setting as standard table summarization problem.", "labels": [], "entities": [{"text": "summarization", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.7230278253555298}]}, {"text": "Another problem that could be formulated is one in which the output summary is generated from multiple tables as proposed in a recent challenge (this setting is out of the scope of this paper).", "labels": [], "entities": []}, {"text": "Now, as the schema is fixed, simple rule based techniques ( or template based solutions could be employed.", "labels": [], "entities": []}, {"text": "However, due to the vast space of selection (which attributes to use in the summary based on the current value it takes) and generation (how to express these selected attributes in natural language) choices possible, such approaches are not scalable in terms of the number of templates as they demand hand-crafted rules for both selection and generation.", "labels": [], "entities": []}, {"text": "We attempt to solve the problem of standard table summarization by leveraging the hierarchical nature of fixed-schema tables.", "labels": [], "entities": [{"text": "standard table summarization", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.544173131386439}]}, {"text": "In other words, rows consist of a fixed set of attributes and a table is defined by a set of rows.", "labels": [], "entities": []}, {"text": "We cast this problem into a mixed hierarchical attention model following the encode-attend-decode ( paradigm.", "labels": [], "entities": []}, {"text": "In this approach, there is static attention on the attributes to compute the row representation followed by dynamic attention on the rows, which is subsequently fed to the decoder.", "labels": [], "entities": []}, {"text": "This formulation is theoretically more efficient than the fully dynamic hierarchical attention framework followed by.", "labels": [], "entities": []}, {"text": "Also, our model does not need sophisticated sampling or sparsifying techniques like (, thus, retaining differentiability.", "labels": [], "entities": []}, {"text": "To demonstrate the efficacy of our approach, we transform the publicly available WEATHERGOV dataset ( into fixed-schema tables, which is then used for our experiments.", "labels": [], "entities": [{"text": "WEATHERGOV dataset", "start_pos": 81, "end_pos": 99, "type": "DATASET", "confidence": 0.9192554652690887}]}], "datasetContent": [{"text": "Dataset and methodology: To evaluate our model we have used WEATHERGOV dataset () which is the standard benchmark dataset to evaluate tabular data summarization techniques.", "labels": [], "entities": [{"text": "WEATHERGOV dataset", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.8150321841239929}, {"text": "tabular data summarization", "start_pos": 134, "end_pos": 160, "type": "TASK", "confidence": 0.6449614266554514}]}, {"text": "We compared the performance of our model against the state-of-the-art work of MBW (, as well as two other baseline models KL () and ALK ( 'SChc', 'Chc' are encoded as '00100000000000', '00010000000000' and '00001000000000' respectively.", "labels": [], "entities": [{"text": "MBW", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.9733133316040039}]}, {"text": "Similar works for directions, for example 'NW', 'NNE' and 'NE' are encoded as '00000100000000', '00000011000000' and '00000001000000' resp.", "labels": [], "entities": []}, {"text": "Time interval were also encoded as ordinal encodings, for example '6-21' is encoded as '111100' and '6-13' is encoded as '110000', the six bits corresponding to six atomic time intervals available in the dataset.", "labels": [], "entities": []}, {"text": "Other attributes and words were encoded as one-hot vectors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Anecdotal example. Records which contain all null attributes are not shown in the example. MB100-4  and MB20-2 correspond to mode-bucket-0-100-4 & mode-bucket-0-20-2 resp. in the dataset.", "labels": [], "entities": []}]}