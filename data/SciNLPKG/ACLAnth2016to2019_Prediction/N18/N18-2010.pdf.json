{"title": [{"text": "Natural Language Generation by Hierarchical Decoding with Linguistic Patterns", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6140597065289816}]}], "abstractContent": [{"text": "Natural language generation (NLG) is a critical component in spoken dialogue systems.", "labels": [], "entities": [{"text": "Natural language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7596732626358668}]}, {"text": "Classic NLG can be divided into two phases: (1) sentence planning: deciding on the overall sentence structure, (2) surface realization: determining specific word forms and flattening the sentence structure into a string.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7222864478826523}, {"text": "surface realization", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7646586000919342}]}, {"text": "Many simple NLG models are based on recurrent neu-ral networks (RNN) and sequence-to-sequence (seq2seq) model, which basically contains a encoder-decoder structure; these NLG models generate sentences from scratch by jointly optimizing sentence planning and surface realization using a simple cross entropy loss training criterion.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 258, "end_pos": 277, "type": "TASK", "confidence": 0.7878702282905579}]}, {"text": "However, the simple encoder-decoder architecture usually suffers from generating complex and long sentences, because the decoder has to learn all grammar and diction knowledge.", "labels": [], "entities": []}, {"text": "This paper introduces a hierarchical decoding NLG model based on linguistic patterns in different levels, and shows that the proposed method outperforms the traditional one with a smaller model size.", "labels": [], "entities": []}, {"text": "Furthermore , the design of the hierarchical decoding is flexible and easily-extensible in various NLG systems 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken dialogue systems that can help users to solve complex tasks have become an emerging research topic in artificial intelligence and natural language processing areas.", "labels": [], "entities": []}, {"text": "A typical dialogue system pipeline contains a speech recognizer, a natural language understanding component, a dialogue manager, and a natural language generator (NLG).", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7215509414672852}]}, {"text": "The first two authors have equal contributions.", "labels": [], "entities": []}, {"text": "The source code is available at https://github.", "labels": [], "entities": []}, {"text": "com/MiuLab/HNLG.", "labels": [], "entities": [{"text": "HNLG", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.6193554401397705}]}, {"text": "NLG is a critical component in a dialogue system, where its goal is to generate the natural language given the semantics provided by the dialogue manager.", "labels": [], "entities": []}, {"text": "As the endpoint of interacting with users, the quality of generated sentences is crucial for user experience.", "labels": [], "entities": []}, {"text": "The common and mostly adopted method is the rule-based (or template-based) method), which can ensure the natural language quality and fluency.", "labels": [], "entities": []}, {"text": "Considering that designing templates is time-consuming and the scalability issue, data-driven approaches have been investigated for open-domain NLG tasks.", "labels": [], "entities": []}, {"text": "Recent advances in recurrent neural networkbased language model (RNNLM) () have demonstrated the capability of modeling long-term dependency by leveraging RNN structure.", "labels": [], "entities": []}, {"text": "Previous work proposed an RNNLM-based NLG) that can be trained on any corpus of dialogue actutterance pairs without any semantic alignment and hand-crafted features.", "labels": [], "entities": [{"text": "RNNLM-based NLG", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.7997962236404419}]}, {"text": "Sequence-to-sequence (seq2seq) generators) further offer better results by leveraging encoder-decoder structure: previous model encoded syntax trees and dialogue acts into sequences as inputs of attentional seq2seq model (.", "labels": [], "entities": []}, {"text": "However, it is challenging to generate long and complex sentences by the simple encoder-decoder structure due to grammar complexity and lack of diction knowledge.", "labels": [], "entities": []}, {"text": "This paper proposes a hierarchical decoder leveraging linguistic patterns, where the decoding hierarchy is constructed in terms of part-of-speech (POS) tags.", "labels": [], "entities": []}, {"text": "The original single decoding process is separated into a multi-level decoding hierarchy, where each decoding layer generates words associated with a specific POS set.", "labels": [], "entities": []}, {"text": "The experiments show that our proposed method outperforms the", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The NLG performance reported on BLEU, ROUGE-1, ROUGE-2, and ROUGE-L of models (%).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9986162185668945}, {"text": "ROUGE-1", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9572314620018005}, {"text": "ROUGE-2", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9001691341400146}, {"text": "ROUGE-L", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9129635691642761}]}]}