{"title": [{"text": "Consistent CCG Parsing over Multiple Sentences for Improved Logical Reasoning", "labels": [], "entities": [{"text": "Consistent CCG Parsing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7171540458997091}, {"text": "Improved Logical Reasoning", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.5632385611534119}]}], "abstractContent": [{"text": "In formal logic-based approaches to Recognizing Textual Entailment (RTE), a Combinatory Categorial Grammar (CCG) parser is used to parse input premises and hypotheses to obtain their logical formulas.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.8771700660387675}]}, {"text": "Here, it is important that the parser processes the sentences consistently ; failing to recognize a similar syntactic structure results in inconsistent predicate argument structures among them, in which case the succeeding theorem proving is doomed to failure.", "labels": [], "entities": []}, {"text": "In this work, we present a simple method to extend an existing CCG parser to parse a set of sentences consistently, which is achieved with an inter-sentence modeling with Markov Random Fields (MRF).", "labels": [], "entities": []}, {"text": "When combined with existing logic-based systems, our method always shows improvement in the RTE experiments on English and Japanese languages.", "labels": [], "entities": [{"text": "RTE", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.8168315291404724}]}], "introductionContent": [{"text": "While today's neural network-based syntactic parsers () have proven successful on sentence level modeling, it is still challenging to accurately process texts that go beyond a single sentence (e.g. coreference resolution, discourse structure analysis).", "labels": [], "entities": [{"text": "sentence level modeling", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.6469105879465739}, {"text": "coreference resolution", "start_pos": 198, "end_pos": 220, "type": "TASK", "confidence": 0.9413917064666748}, {"text": "discourse structure analysis", "start_pos": 222, "end_pos": 250, "type": "TASK", "confidence": 0.7129605015118917}]}, {"text": "In this work we focus, among others, on the consistent analysis of multiple sentences in a document.", "labels": [], "entities": [{"text": "analysis of multiple sentences in a document", "start_pos": 55, "end_pos": 99, "type": "TASK", "confidence": 0.7713077749524798}]}, {"text": "This is as an important problem in reasoning tasks as other document analysis.", "labels": [], "entities": [{"text": "reasoning tasks", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.9163133800029755}]}, {"text": "RTE is an elemental technology for semantic analysis of multiple sentences, where, given a text (T) and a hypothesis (H), a system determines if T entails H.", "labels": [], "entities": [{"text": "RTE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8601095676422119}, {"text": "semantic analysis of multiple sentences", "start_pos": 35, "end_pos": 74, "type": "TASK", "confidence": 0.8593512535095215}]}, {"text": "Existing methods based on formal logic obtain logical formulas for T and H using an off-the-shelf CCG parser, and then feed them to a theorem prover.", "labels": [], "entities": []}, {"text": "The standard approach to mapping CCG trees ontological formulas is to assign \u03bb-terms to the words in a sentence   and combine them in a bottom-up fashion.", "labels": [], "entities": []}, {"text": "Here, when the parser fails to make consistent analyses for T and H, the succeeding inference component is also doomed to failure.", "labels": [], "entities": []}, {"text": "In, when the parser wrongly analyzes \"man exercising\" in H as \"man\" modifying \"exercising\", the entailment relation cannot be established, due to the different argument structures of exercise in the resulting formulas.", "labels": [], "entities": []}, {"text": "While it is ideal to enhance the overall performance of a parser, it is not cheaply obtainable.", "labels": [], "entities": []}, {"text": "Additionally, neural network-based parsers are susceptible to subtle changes in the input and thus hard to inspect and modify its parameters to change its prediction.", "labels": [], "entities": []}, {"text": "Due to this, we cannot expect that a particular pair of words across multiple sentences be always analyzed in a consistent manner.", "labels": [], "entities": []}, {"text": "In this work, we solve the inconsistency prob-lem above by adapting the inter-sentence model of to CCG parsing.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 99, "end_pos": 110, "type": "TASK", "confidence": 0.7201922237873077}]}, {"text": "Their motivation is to exploit the similarities among test sentences to overcome situations where the amount of the training data is scarce or its domain is different from the test data.", "labels": [], "entities": []}, {"text": "The method based on dual decomposition tries to find parse trees fora set of sentences that agree with an MRF, which encourages the assignment of a similar structure to similar contexts.", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.826431006193161}]}, {"text": "In our approach, we aim to eliminate wrong logical formulas such as in by rewarding consistent CCG parses across sentences.", "labels": [], "entities": []}, {"text": "This, in turn, is achieved by rewarding the consistent assignment of categories to the terminals.", "labels": [], "entities": []}, {"text": "This works for CCG parsing, as its derivation is mostly determined by the terminal categories.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.7746070325374603}]}, {"text": "The key of our approach is that by combining A* parsing of with dual decomposition, we can keep small the latency incurred by the use of the iterative algorithm.", "labels": [], "entities": []}, {"text": "We conducted experiments using two stateof-the-art logic-based systems) and two RTE datasets for English and Japanese languages.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.7950885891914368}]}, {"text": "Our method always shows improvement compared to the baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "English In English experiment, we test the performance of ccg2lambda and LangPro (Abzianidze, 2017) on SICK dataset ( . As mentioned earlier, these systems try to prove whether T entails H, by applying a theorem prover to the logical formulas converted from the CCG trees.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.7553853392601013}]}, {"text": "We report results for ccg2lambda with the default settings (with SPSA abduction;) and results for two versions of LangPro, one which is described in Abzianidze (2015) (henceforth we refer to it as LangPro15) and the other in Abzianidze  compare our results with depccg without the MRF and baselines reported in the above papers that use EasyCCG (.", "labels": [], "entities": [{"text": "MRF", "start_pos": 281, "end_pos": 284, "type": "METRIC", "confidence": 0.8441418409347534}, {"text": "EasyCCG", "start_pos": 337, "end_pos": 344, "type": "DATASET", "confidence": 0.9157924056053162}]}, {"text": "In MRF, a context node is constructed when two or more words from both T and H share the same surface form.", "labels": [], "entities": [{"text": "MRF", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9070240259170532}]}, {"text": "Exceptionally, some pairs of categories are allowed to be aligned with score \u03b4 1 : a pair of noun modifier (N/N ) and verb tense (S ng \\N P ), which are categories for present participles, and a pair of nominal modifier (N/N ) and noun (N ).", "labels": [], "entities": []}, {"text": "In the experiment using ccg2lambda the pairs of categories of transitive and intransitive verbs, ((S X \\N P )/N P , S X \\N P ) and ((S X \\N P )/P P , S X \\N P ), for any feature X are also allowed with \u03b4 1 . For the hyperparamters, we conducted grid search over [0.0, 0.1, . .", "labels": [], "entities": []}, {"text": ", 0.9] for each \u03b4 i in the MRF s.t. \u03b4 1 \u2265 \u03b4 2 \u2265 \u03b4 3 and found that \u03b4 1 = 0.9, \u03b4 2 = 0.1, \u03b4 3 = 0.0 works the best on SICK trial set.", "labels": [], "entities": [{"text": "MRF s.t. \u03b4", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.910547931989034}, {"text": "SICK trial set", "start_pos": 117, "end_pos": 131, "type": "DATASET", "confidence": 0.7953928311665853}]}, {"text": "We set \u03b1 = 0.0002 and K = 500 in Alg.", "labels": [], "entities": []}, {"text": "1. We decay \u03b1 by 0.9 in every iteration.", "labels": [], "entities": []}, {"text": "Japanese In Japanese experiment, we evaluate ccg2lambda's performance on JSeM dataset (.", "labels": [], "entities": [{"text": "JSeM dataset", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9050381183624268}]}, {"text": "To construct an MRF graph, we processed RTE problems with kuromoji and made a context node fora noun or a verb followed by an adverb.", "labels": [], "entities": [{"text": "MRF", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.8719742894172668}]}, {"text": "The reason why we use bigram POS tag-based context is that the graph construction based on the surface form has resulted in poor RTE performance, by overgenerating MRF constraints.", "labels": [], "entities": []}, {"text": "This maybe due to the fact that Japanese sentences are usually tokenized into smaller units.", "labels": [], "entities": []}, {"text": "We used depccg and the same hyperparameters as English experiment.: RTE results using ccg2lambda on JSeM", "labels": [], "entities": [{"text": "RTE", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.8038259148597717}, {"text": "JSeM", "start_pos": 100, "end_pos": 104, "type": "DATASET", "confidence": 0.8430367112159729}]}], "tableCaptions": [{"text": " Table 1: RTE results on test section of SICK", "labels": [], "entities": [{"text": "RTE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9284406304359436}, {"text": "SICK", "start_pos": 41, "end_pos": 45, "type": "TASK", "confidence": 0.5572101473808289}]}, {"text": " Table 2: RTE results using ccg2lambda on JSeM", "labels": [], "entities": [{"text": "RTE", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5690808296203613}, {"text": "JSeM", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.7541701793670654}]}]}