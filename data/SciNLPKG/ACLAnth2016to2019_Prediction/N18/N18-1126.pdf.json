{"title": [{"text": "Context Sensitive Neural Lemmatization with Lematus", "labels": [], "entities": [{"text": "Context Sensitive Neural Lemmatization", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8017122596502304}]}], "abstractContent": [{"text": "The main motivation for developing context-sensitive lemmatizers is to improve performance on unseen and ambiguous words.", "labels": [], "entities": []}, {"text": "Yet previous systems have not carefully evaluated whether the use of context actually helps in these cases.", "labels": [], "entities": []}, {"text": "We introduce Lematus, a lemma-tizer based on a standard encoder-decoder architecture , which incorporates character-level sentence context.", "labels": [], "entities": []}, {"text": "We evaluate its lemmatiza-tion accuracy across 20 languages in both a full data setting and a lower-resource setting with 10k training examples in each language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9717787504196167}]}, {"text": "In both settings, we show that including context significantly improves results against a context-free version of the model.", "labels": [], "entities": []}, {"text": "Context helps more for ambiguous words than for unseen words, though the latter has a greater effect on overall performance differences between languages.", "labels": [], "entities": []}, {"text": "We also compare to three previous context-sensitive lemmatization systems , which all use pre-extracted edit trees as well as hand-selected features and/or additional sources of information such as tagged training data.", "labels": [], "entities": []}, {"text": "Without using any of these, our context-sensitive model outperforms the best competitor system (Lemming) in the full-data setting, and performs on par in the lower-resource setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lemmatization is the process of determining the dictionary form of a word (e.g. swim) given one of its inflected variants (e.g. swims, swimming, swam, swum).", "labels": [], "entities": [{"text": "Lemmatization is the process of determining the dictionary form of a word (e.g. swim) given one of its inflected variants (e.g. swims, swimming, swam, swum", "start_pos": 0, "end_pos": 155, "type": "Description", "confidence": 0.7474237930390143}]}, {"text": "Data-driven lemmatizers face two main challenges: first, to generalize beyond the training data in order to lemmatize unseen words; and second, to disambiguate ambiguous wordforms from their sentence context.", "labels": [], "entities": []}, {"text": "In Latvian, for example, the wordform \"cel\u00b8ucel\u00b8u\" is ambiguous when considered in isolation: it could bean inflected variant of the verb \"celt\" (to lift) or the nouns \"celis\" (knee) or \"cel\u00b8\u0161cel\u00b8\u0161\" (road); without context, the lemmatizer can only guess.", "labels": [], "entities": []}, {"text": "By definition, sentence context (or latent information derived from it, such as the target word's morphosyntactic tags) is needed in order to correctly lemmatize ambiguous forms such as the example above.", "labels": [], "entities": []}, {"text": "Previous researchers have also assumed that context should help in lemmatizing unseen words)-i.e., that the context contains useful features above and beyond those in the wordform itself.", "labels": [], "entities": []}, {"text": "Nevertheless, we are not aware of any previous work that has attempted to quantify how much (or even whether) context actually helps in both of these cases.", "labels": [], "entities": []}, {"text": "Several previous papers on contextsensitive lemmatization have reported results on unseen words, and some have compared versions of their systems that use context in different ways), but there are few if any direct comparisons between context-sensitive and context-free systems, nor have results been reported on ambiguous forms.", "labels": [], "entities": []}, {"text": "This paper presents Lematus-a system that adapts the neural machine translation framework of to learn context sensitive lemmatization using an encoder-decoder model.", "labels": [], "entities": []}, {"text": "Context is represented simply using the character contexts of each form to be lemmatized, meaning that our system requires fewer training resources than previous systems: only a corpus with its lemmatized forms, without the need for POS tags ( or word embeddings trained on a much larger corpus (.", "labels": [], "entities": []}, {"text": "We evaluate Lematus on data from 20 typologically varied languages, both using the full training data from the Universal Dependencies project (, as well as a lower-resource scenario with only 10k training tokens per language.", "labels": [], "entities": []}, {"text": "We compare results to three previous systems and to a context-free version of our own system, including results on both unseen and ambiguous words.", "labels": [], "entities": []}, {"text": "We also examine the extent to which the rate of unseen and ambiguous words in a language can predict lemmatization performance.", "labels": [], "entities": []}, {"text": "On average across the 20 languages, the contextsensitive version of Lematus achieves significantly higher lemmatization accuracy than its context-free counterpart in both the low-resource and full-data settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9672903418540955}]}, {"text": "It also outperforms the best competitor system () in the fulldata setting, and does as well as Lemming in the low-resource setting.", "labels": [], "entities": []}, {"text": "Thus, even without explicitly training on or predicting POS tags, Lematus seems able to implicitly learn similar information from the raw character context.", "labels": [], "entities": []}, {"text": "Analysis of our full-data results shows that including context in the model improves its accuracy more on ambiguous words (from 88.8% to 92.4% on average) than on unseen words (from 83.6% to 84.3% on average).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9989683628082275}]}, {"text": "This suggests that, to the extent that unseen words can be correctly lemmatized at all, the wordform itself provides much of the information needed to do so, and Lematus effectively exploits that information-indeed, Lematus without context outperforms all previous contextsensitive models on lemmatizing unseen words.", "labels": [], "entities": []}, {"text": "Finally, our cross-linguistic analysis indicates that the proportions of unseen words and ambiguous words in a language are anti-correlated.", "labels": [], "entities": []}, {"text": "Altogether, then, our results suggest that context-free neural lemmatization is surprisingly effective, and maybe a reasonable option if the language contains many unseen words but few ambiguous ones.", "labels": [], "entities": [{"text": "context-free neural lemmatization", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.6015482346216837}]}, {"text": "Context is likely to help inmost languages, but the main boost is for languages with higher ambiguity.", "labels": [], "entities": []}], "datasetContent": [{"text": "We contend that the difficulty of the lemmatization task largely depends on three factors: morphological productivity, lexical ambiguity and morphological regularity.", "labels": [], "entities": []}, {"text": "One aim of our work is to investigate the extent to which it is possible to predict lemmatization performance fora particular language by operationalizing and measuring these properties.", "labels": [], "entities": []}, {"text": "Therefore in this section we provide statistics and some analysis of the datasets used in our experiments.", "labels": [], "entities": []}, {"text": "We use the standard splits of the Universal Dependency Treebank (UDT) v2.0 5 (Nivre et al., 2017) datasets for 20 languages: Arabic, Basque, Croatian, Dutch 6 , Estonian, Finnish, German, Greek, Hindi, Hungarian, Italian, Latvian, Polish, Portuguese, Romanian, Russian, Slovak, Slovene, Turkish and Urdu.", "labels": [], "entities": [{"text": "Universal Dependency Treebank (UDT) v2.0 5 (Nivre et al., 2017) datasets", "start_pos": 34, "end_pos": 106, "type": "DATASET", "confidence": 0.7601782996207476}]}, {"text": "See for training and development data sizes.", "labels": [], "entities": []}, {"text": "Because the amount of training data varies widely between languages, we perform some of our language analysis (and later, system evaluation) on a subset of the data, where we use only the first 10k tokens in each language for training.", "labels": [], "entities": []}, {"text": "The 10k setting provides a clearer comparison between languages in terms of their productivity, ambiguity, and regularity, and also gives a sense of how much training data is needed to achieve good performance.", "labels": [], "entities": []}, {"text": "One of the main purposes of data-driven lemmatization is to handle unseen words attest time, yet languages with differing morphological productivity will have very different proportions of unseen words.", "labels": [], "entities": []}, {"text": "shows the percentage of tokens in the development sets of each language that are not seen in training.", "labels": [], "entities": []}, {"text": "Two conditions are given: the full training/development sets, and train/dev sets that are controlled in size across languages.", "labels": [], "entities": []}, {"text": "For  the languages with large data sets, the percentage of unseen words is (unsurprisingly) higher when training data is reduced to 10k.", "labels": [], "entities": []}, {"text": "However, these differences are often small compared to the differences between languages, suggesting that productivity is likely to affect lemmatization performance as much as training data size.", "labels": [], "entities": []}, {"text": "Lexical ambiguity is the other major motivation for context-sensitive lemmatization.", "labels": [], "entities": []}, {"text": "To quantify how frequently lemmatizers have to rely on context, shows the percentage of ambiguous tokens in each language, in either the full or reduced training sets.", "labels": [], "entities": []}, {"text": "We define ambiguity empirically: ambiguous tokens are wordforms occurring with more than one lemma within the training set.", "labels": [], "entities": []}, {"text": "Overall, the level of measured ambiguity tends to be lower than the proportion of unseen tokens.", "labels": [], "entities": []}, {"text": "Many of the languages with high productivity (e.g., Russian, Slovak, Slovene, Turkish) have low levels of ambiguity, while others (Arabic, Urdu) trend the opposite way.", "labels": [], "entities": []}, {"text": "Indeed, across all 20 languages, the levels of productivity and ambiguity are negatively correlated, with a rank correlation of -0.57 after controlling for training data size.", "labels": [], "entities": []}, {"text": "7 This is not surprising, since given a set of morphosyntactic functions, they must either be expressed using distinct forms (leading to higher productivity) or non-distinct forms (leading to higher ambiguity).", "labels": [], "entities": []}, {"text": "The final characteristic that we would expect to make some languages easier than others is morphological regularity, but it is unclear how to measure this property directly without an in-depth understanding of the morphophonological rules of a language.", "labels": [], "entities": []}, {"text": "Nevertheless, the presence of many irregular forms, or other phenomena such as vowel harmony or spelling changes, complicates lemmatization and will likely affect accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9912750720977783}]}, {"text": "Training Parameters We use a mini batch size of 60 and a maximum sequence length of 75.", "labels": [], "entities": [{"text": "sequence length", "start_pos": 65, "end_pos": 80, "type": "METRIC", "confidence": 0.955889880657196}]}, {"text": "For training we use stochastic gradient descent, Adadelta, with a gradient clipping threshold of 1.0, recurrent Bayesian dropout probability 0.2 (Gal and Ghahramani, 2016) and weight normalization.", "labels": [], "entities": [{"text": "recurrent Bayesian dropout probability 0.2", "start_pos": 102, "end_pos": 144, "type": "METRIC", "confidence": 0.8585301399230957}]}, {"text": "We use early stopping with patience 10.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.7778203785419464}]}, {"text": "We use the first 10 epochs as a burn-in period, after which at the end of every second epoch we evaluate the current model's lemmatization exact match accuracy on the development set and keep this model if it performs better than the previous best model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.5557277202606201}]}, {"text": "When making predictions we use beam-search decoding with abeam of size 12.", "labels": [], "entities": []}, {"text": "Baselines To train models we use the default settings for Morfette and Lemming.", "labels": [], "entities": [{"text": "Morfette", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.9411123991012573}, {"text": "Lemming", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.664779007434845}]}, {"text": "Ch-2017 requires word embeddings, for which we use fastText 9 (.", "labels": [], "entities": [{"text": "fastText 9", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.8708990216255188}]}, {"text": "For Ch-2017 we set the number of training epochs to 100 and implement early stopping with patience 10.", "labels": [], "entities": []}, {"text": "We leave the remaining model parameters as suggested by.", "labels": [], "entities": []}, {"text": "We also use a lookup-based baseline (Baseline).", "labels": [], "entities": []}, {"text": "For words that have been observed in training, it outputs the most frequent lemma (or the first observed lemma, if the options are equally frequent).", "labels": [], "entities": []}, {"text": "For unseen words it outputs the wordform itself as the hypothesized lemma.", "labels": [], "entities": []}, {"text": "Context Representation We aim to use a context representation that works well across multiple languages, rather than to tune the context individually to each language.", "labels": [], "entities": [{"text": "Context Representation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8603079319000244}]}, {"text": "In preliminary experiments, we explored several different context representations: words, sub-word units, and N surrounding characters, for different values of N . These experiments were carried out on only six languages.", "labels": [], "entities": []}, {"text": "Three of these (Latvian, Polish and Turkish) were also used in our main experiments, while three (Bulgarian, Hebrew, and Persian) were not, due to problems getting all the baseline systems to run on those languages.", "labels": [], "entities": []}, {"text": "For the word level context representation (Words), we use all words in the left and the right sentence contexts.", "labels": [], "entities": []}, {"text": "For the character level context representations (N-Ch) we experiment with N = 0, 5, 10, 15, 20, or 25 characters of left and right contexts.", "labels": [], "entities": []}, {"text": "For the sub-word unit context representation, we use byte pair encoding (BPE), which has shown good results for neural machine translation a single parameter-the number of merge operations.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 112, "end_pos": 138, "type": "TASK", "confidence": 0.6303677757581075}]}, {"text": "Suitable values for this parameter depend on the application and vary from 10k in language modeling () to 50k in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7368144094944}]}, {"text": "We aim to use BPE to extract a few salient and frequently occurring strings, such as affixes, therefore we set the number of BPE merge operations to 500.", "labels": [], "entities": []}, {"text": "We use BPE-encoded left and right sentence contexts that amount up to 20 characters of the original text.", "labels": [], "entities": []}, {"text": "Since we hoped to use context to help with ambiguous words, we looked specifically at ambiguous word performance in choosing the best context representation.", "labels": [], "entities": []}, {"text": "11 summarizes Lematus' performance on ambiguous tokens using different sentence context representations.", "labels": [], "entities": []}, {"text": "There is no context representation that works best for all six languages, but the 20-Ch system seems to work reasonably well in all cases, and the best on average.", "labels": [], "entities": []}, {"text": "We therefore use the 20-Ch context in our main experiments.", "labels": [], "entities": []}, {"text": "Note that this choice was based on a relatively small number of experiments and it is quite possible that further tuning the BPE parameter, or the number of BPE units or words of context (or tuning separately for each language) could lead to better overall results.", "labels": [], "entities": [{"text": "BPE", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.8996948599815369}]}, {"text": "Evaluation To evaluate models, we use test and development set lemmatization exact match accuracy.", "labels": [], "entities": [{"text": "development set lemmatization exact match accuracy", "start_pos": 47, "end_pos": 97, "type": "METRIC", "confidence": 0.5431916564702988}]}, {"text": "When calculating lemmatization accuracy we ignore casing of the tokens and ommit punctuation tokens and those tokens that contain digits or any of the following characters: @+.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9692198038101196}]}, {"text": "/.  Patterns Across Languages In Section 4, we hypothesized that the success of data-driven lemmatization depends on a language's productivity, ambiguity, and regularity.", "labels": [], "entities": []}, {"text": "We now explore the extent to which our results support this hypothesis.", "labels": [], "entities": []}, {"text": "First, we examine the correlation between the overall performance of our best system on each language and the percentage of unseen ( or ambiguous words (, middle) in that language.", "labels": [], "entities": []}, {"text": "As expected, there is a strong negative correlation between the percentage of unseen words and the accuracy of Lematus 20-Ch: the rank correlation is R = \u22120.73 (p < 0.001; we use rank correlation because it is less sensitive to outliers than is linear correlation, and the plot clearly shows several outliers.)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.999528169631958}, {"text": "rank correlation", "start_pos": 130, "end_pos": 146, "type": "METRIC", "confidence": 0.8378211855888367}]}, {"text": "In contrast to our original prediction, however, Lematus 20-Ch is actually more accurate for languages with greater ambiguity (R = 0.44, p = 0.05).", "labels": [], "entities": []}, {"text": "The most likely explanation is that ambiguity is negatively correlated with productivity.", "labels": [], "entities": []}, {"text": "Since there tend to be more unseen than ambiguous words, and since accuracy is typically lower for unseen than ambiguous words, higher ambiguity (which implies fewer unseen words) can actually lead to higher overall accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9985264539718628}, {"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.99289870262146}]}, {"text": "Our earlier results also suggested that the main benefit of Lematus 20-Ch over Lematus 0-Ch is for ambiguous words.", "labels": [], "entities": []}, {"text": "To confirm this, we looked at the extent to which the difference in performance between the two systems correlates with the percentage of unseen or ambiguous words in a language.", "labels": [], "entities": []}, {"text": "As expected, this analysis suggests that including context in the model helps more for languages with more ambiguity (R = 0.67, p < 0.001).", "labels": [], "entities": [{"text": "R", "start_pos": 118, "end_pos": 119, "type": "METRIC", "confidence": 0.985531210899353}]}, {"text": "In contrast, Lematus 20-Ch provides less benefit over Lematus 0-Ch for the languages with more unseen words (R = \u22120.75, p < 0.0001).", "labels": [], "entities": []}, {"text": "Again, we assume the latter result is due to the negative correlation between ambiguity and productivity.", "labels": [], "entities": []}, {"text": "So far, our results and analysis show a clear relationship between productivity and ambiguity, and also suggest that using context for lemmatization maybe unnecessary (or at least less beneficial) for languages with many unseen words but low am-biguity.", "labels": [], "entities": []}, {"text": "However, there are remaining differences between languages that are more difficult to explain.", "labels": [], "entities": []}, {"text": "For example, one might expect that for languages with more training data, the system would learn better generalizations and lemmatization accuracy on unseen words would be higher.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9803656935691833}]}, {"text": "However, (right), which plots accuracy on unseen words in each language as a function of training data size, illustrates that there is no significant correlation between the two variables (R = 0.32, p = 0.16).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9978082776069641}]}, {"text": "In some languages (e.g., Hungarian, in the top left) Lematus performs very well on unseen words even with little training data, while in others (e.g., Arabic, along the bottom) it performs poorly despite relatively large training data.", "labels": [], "entities": []}, {"text": "We assume that regularity (and perhaps the nonconcatenative nature of Arabic) must be playing an important role here, but we leave for future work the question of how to operationalize and measure regularity in order to further test this hypothesis.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Lemmatization exact match accuracy on ambiguous tokens of dev sets, for baseline and for  Lematus using various context representations: N characters, Byte Pair Encoding units, or words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.8924385905265808}]}]}