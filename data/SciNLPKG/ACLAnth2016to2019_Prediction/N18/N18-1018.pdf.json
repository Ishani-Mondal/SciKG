{"title": [{"text": "Query and Output: Generating Words by Querying Distributed Word Representations for Paraphrase Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Most recent approaches use the sequence-to-sequence model for paraphrase generation.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.966582179069519}]}, {"text": "The existing sequence-to-sequence model tends to memorize the words and the patterns in the training dataset instead of learning the meaning of the words.", "labels": [], "entities": []}, {"text": "Therefore, the generated sentences are often grammatically correct but semantically improper.", "labels": [], "entities": []}, {"text": "In this work, we introduce a novel model based on the encoder-decoder framework, called Word Embedding Attention Network (WEAN).", "labels": [], "entities": [{"text": "Word Embedding Attention Network (WEAN)", "start_pos": 88, "end_pos": 127, "type": "TASK", "confidence": 0.6175751345498222}]}, {"text": "Our proposed model generates the words by querying distributed word representations (i.e. neu-ral word embeddings), hoping to capturing the meaning of the according words.", "labels": [], "entities": []}, {"text": "Following previous work, we evaluate our model on two paraphrase-oriented tasks, namely text simplification and short text abstractive summa-rization.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7511754333972931}]}, {"text": "Experimental results show that our model outperforms the sequence-to-sequence baseline by the BLEU score of 6.3 and 5.5 on two English text simplification datasets, and the ROUGE-2 F1 score of 5.7 on a Chi-nese summarization dataset.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9829036295413971}, {"text": "ROUGE-2 F1 score", "start_pos": 173, "end_pos": 189, "type": "METRIC", "confidence": 0.8626946806907654}]}, {"text": "Moreover, our model achieves state-of-the-art performances on these three benchmark datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Paraphrase is a restatement of the meaning of a text using other words.", "labels": [], "entities": [{"text": "Paraphrase is a restatement of the meaning of a text", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8631410598754883}]}, {"text": "Many natural language generation tasks are paraphrase-orientated, such as text simplification and short text summarization.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 5, "end_pos": 32, "type": "TASK", "confidence": 0.699114203453064}, {"text": "text simplification", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.752312570810318}, {"text": "short text summarization", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.6385838786760966}]}, {"text": "Text simplification is to make the text easier to read and understand, especially for poor readers, while short text summarization is to generate a brief sentence to describe the short texts (e.g. posts on the social media).", "labels": [], "entities": [{"text": "Text simplification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.745534211397171}, {"text": "short text summarization", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.6603048841158549}]}, {"text": "Most recent approaches use sequence-to-sequence model for paraphrase generation (.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.9268312454223633}]}, {"text": "It compresses the source text information into dense vectors with the neural encoder, and the neural decoder generates the target text using the compressed vectors.", "labels": [], "entities": []}, {"text": "Although neural network models achieve success in paraphrase generation, there are still two major problems.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.9697334468364716}]}, {"text": "One of the problem is that the existing sequence-to-sequence model tends to memorize the words and the patterns in the training dataset instead of the meaning of the words.", "labels": [], "entities": []}, {"text": "The main reason is that the word generator (i.e. the output layer of the decoder) does not model the semantic information.", "labels": [], "entities": []}, {"text": "The word generator, which consists of a linear transformation and a softmax operation, converts the Recurrent Neural Network (RNN) output from a small dimension (e.g. 500) to a much larger dimension (e.g. 50,000 words in the vocabulary), where each dimension represents the score of each word.", "labels": [], "entities": []}, {"text": "The latent assumption of the word generator is that each word is independent and the score is irrelevant to each other.", "labels": [], "entities": []}, {"text": "Therefore, the scores of a word and its synonyms maybe of great difference, which means the word generator learns the word itself rather than the relationship between words.", "labels": [], "entities": []}, {"text": "The other problem is that the word generator has a huge number of parameters.", "labels": [], "entities": []}, {"text": "Suppose we have a sequence-to-sequence model with a hidden size of 500 and a vocabulary size of 50,000.", "labels": [], "entities": []}, {"text": "The word generator has up to 25 million parameters, which is even larger than other parts of the encoder-decoder model in total.", "labels": [], "entities": []}, {"text": "The huge size of parameters will result in slow convergence, because there area lot of parameters to be learned.", "labels": [], "entities": []}, {"text": "Moreover, under the distributed framework, the more parameters a model has, the more bandwidth and memory it consumes.", "labels": [], "entities": []}, {"text": "To tackle both of the problems, we propose a novel model called Word Embedding Attention Network (WEAN).", "labels": [], "entities": [{"text": "Word Embedding Attention Network (WEAN)", "start_pos": 64, "end_pos": 103, "type": "TASK", "confidence": 0.6540811061859131}]}, {"text": "The word generator of WEAN is attention based, instead of the simple linear softmax operation.", "labels": [], "entities": [{"text": "WEAN", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.6605451107025146}]}, {"text": "In our attention based word generator, the RNN output is a query, the candidate words are the values, and the corresponding word representations are the keys.", "labels": [], "entities": []}, {"text": "In order to predict the word, the attention mechanism is used to select the value matching the query most, by means of querying the keys.", "labels": [], "entities": []}, {"text": "In this way, our model generates the words according to the distributed word representations (i.e. neural word embeddings) in a retrieval style rather than the traditional generative style.", "labels": [], "entities": []}, {"text": "Our model is able to capture the semantic meaning of a word by referring to its embedding.", "labels": [], "entities": []}, {"text": "Besides, the attention mechanism has a much smaller number of parameters compared with the linear transformation directly from the RNN output space to the vocabulary space.", "labels": [], "entities": []}, {"text": "The reduction of the parameters can increase the convergence rate and speedup the training process.", "labels": [], "entities": [{"text": "convergence rate", "start_pos": 49, "end_pos": 65, "type": "METRIC", "confidence": 0.9582641422748566}]}, {"text": "Moreover, the word embedding is updated from three sources: the input of the encoder, the input of the decoder, and the query of the output layer.", "labels": [], "entities": []}, {"text": "Following previous work, we evaluate our model on two paraphrase-oriented tasks, namely text simplification and short text abstractive summarization.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7623710334300995}, {"text": "short text abstractive summarization", "start_pos": 112, "end_pos": 148, "type": "TASK", "confidence": 0.6873745620250702}]}, {"text": "Experimental results show that our model outperforms the sequence-tosequence baseline by the BLEU score of 6.3 and 5.5 on two English text simplification datasets, and the ROUGE-2 F1 score of 5.7 on a Chinese summarization dataset.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.981362909078598}, {"text": "ROUGE-2 F1 score", "start_pos": 172, "end_pos": 188, "type": "METRIC", "confidence": 0.8587295810381571}, {"text": "Chinese summarization dataset", "start_pos": 201, "end_pos": 230, "type": "DATASET", "confidence": 0.6903891166051229}]}, {"text": "Moreover, our model achieves state-of-the-art performances on all of the benchmark datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following the previous work, we test our model on the following two paraphrase orientated tasks: text simplification and short text abstractive summarization.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7588953077793121}, {"text": "short text abstractive summarization", "start_pos": 121, "end_pos": 157, "type": "TASK", "confidence": 0.675381138920784}]}, {"text": "The datasets are both from the alignments between English Wikipedia website 2 and Simple English Wikipedia website.", "labels": [], "entities": [{"text": "English Wikipedia website 2", "start_pos": 50, "end_pos": 77, "type": "DATASET", "confidence": 0.9656345546245575}, {"text": "Simple English Wikipedia website", "start_pos": 82, "end_pos": 114, "type": "DATASET", "confidence": 0.9307821094989777}]}, {"text": "The Simple English Wikipedia is built for \"the children and adults who are learning the English language\", and the articles are composed with \"easy words and short sentences\".", "labels": [], "entities": [{"text": "Simple English Wikipedia", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.7901021838188171}]}, {"text": "Therefore, Simple English Wikipedia is a natural public simplified text corpus.", "labels": [], "entities": [{"text": "Simple English Wikipedia", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.6957371830940247}]}, {"text": "\u2022 Parallel Wikipedia Simplification Corpus (PWKP  Our evaluation metric is ROUGE score (, which is popular for summarization evaluation.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.9846582412719727}, {"text": "summarization evaluation", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.9548022449016571}]}, {"text": "The metrics compare an automatically produced summary against the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS).", "labels": [], "entities": [{"text": "longest common subsequence (LCS)", "start_pos": 167, "end_pos": 199, "type": "METRIC", "confidence": 0.7898405293623606}]}, {"text": "Following previous work, we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9647943377494812}, {"text": "ROUGE-2", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9640916585922241}, {"text": "ROUGE-L (LCS)", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9417876750230789}]}, {"text": "Large Scale Chinese Social Media Short Text Summarization Dataset (LCSTS): LCSTS is constructed by.", "labels": [], "entities": [{"text": "Large Scale Chinese Social Media Short Text Summarization", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.538298700004816}]}, {"text": "The dataset consists of more than 2,400,000 text-summary pairs, constructed from a famous Chinese social media website called Sina Weibo.", "labels": [], "entities": []}, {"text": "It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III.", "labels": [], "entities": [{"text": "PART", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.8474864363670349}]}, {"text": "All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5.", "labels": [], "entities": []}, {"text": "We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III.", "labels": [], "entities": [{"text": "PART", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.8878659605979919}, {"text": "PART", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.8773482441902161}]}, {"text": "Following the previous work, we use PART I as training set, PART II as validation set, and PART III as test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic evaluation of our model and other  related systems on PWKP datasets. The results are re- ported on the test sets.", "labels": [], "entities": [{"text": "PWKP datasets", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9776752889156342}]}, {"text": " Table 2: Automatic evaluation of our model and other  related systems on EW-SEW datasets. The results are  reported on the test sets.", "labels": [], "entities": [{"text": "EW-SEW datasets", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.9766961932182312}]}, {"text": " Table 3: Human evaluation of our model and other re- lated systems on PWKP and EW-SEW datasets. The  results are reported on the test sets.", "labels": [], "entities": [{"text": "PWKP", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.959097146987915}, {"text": "EW-SEW datasets", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.8231787085533142}]}, {"text": " Table 4: ROUGE F1 score on the LCSTS test set. R- 1, R-2, and R-L denote ROUGE-1, ROUGE-2, and  ROUGE-L, respectively. The models with a suffix of  'W' in the table are word-based, while the rest of mod- els are character-based.", "labels": [], "entities": [{"text": "ROUGE F1 score", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8067166407903036}, {"text": "LCSTS test set", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.9635880589485168}]}, {"text": " Table 5: The number of the parameters in the out- put layer. The numbers of rest parameters between  Seq2seq and WEAN are the same.", "labels": [], "entities": [{"text": "WEAN", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.8572189807891846}]}]}