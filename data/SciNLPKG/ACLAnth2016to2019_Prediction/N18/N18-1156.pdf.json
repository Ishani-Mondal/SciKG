{"title": [{"text": "Neural Storyline Extraction Model for Storyline Generation from News Articles", "labels": [], "entities": [{"text": "Neural Storyline Extraction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6996155778566996}, {"text": "Storyline Generation", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7814119160175323}]}], "abstractContent": [{"text": "Storyline generation aims to extract events described on news articles under a certain topic and reveal how those events evolve overtime.", "labels": [], "entities": [{"text": "Storyline generation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6845484673976898}]}, {"text": "Most existing approaches first train supervised models to extract events from news articles published in different time periods and then link relevant events into coherent stories.", "labels": [], "entities": []}, {"text": "They are domain dependent and cannot deal with unseen event types.", "labels": [], "entities": []}, {"text": "To tackle this problem , approaches based on probabilistic graphic models jointly model the generations of events and storylines without annotated data.", "labels": [], "entities": []}, {"text": "However , the parameter inference procedure is too complex and models often require longtime to converge.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel neural network based approach to extract structured representations and evolution patterns of storylines without using annotated data.", "labels": [], "entities": []}, {"text": "In this model, title and main body of a news article are assumed to share the similar storyline distribution.", "labels": [], "entities": []}, {"text": "Moreover, similar documents described in neighboring time periods are assumed to share similar storyline distributions.", "labels": [], "entities": []}, {"text": "Based on these assumptions, structured representations and evolution patterns of sto-rylines can be extracted.", "labels": [], "entities": []}, {"text": "The proposed model has been evaluated on three news corpora and the experimental results show that it out-performs state-of-the-art approaches accuracy and efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9994033575057983}]}], "introductionContent": [{"text": "With the development of the internet, massive information about current events is generated and propagated continuously on online news media sites.", "labels": [], "entities": []}, {"text": "It is difficult for the public to digest such large volumes of information effectively.", "labels": [], "entities": []}, {"text": "Storyline generation, aiming at summarizing the development of certain related events, has been intensively studied recently (.", "labels": [], "entities": [{"text": "Storyline generation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8158297538757324}, {"text": "summarizing the development of certain related events", "start_pos": 32, "end_pos": 85, "type": "TASK", "confidence": 0.7422970107623509}]}, {"text": "In general, storyline can be considered as an event cluster where event-related news articles are ordered and clustered depending on both content and temporal similarity.", "labels": [], "entities": []}, {"text": "Different ways of calculating content and temporal similarity can be used to cluster related events.", "labels": [], "entities": []}, {"text": "Bayesian nonparametric models could also be used to tackle this problem by describing the storyline generating process using probabilistic graphical models ().", "labels": [], "entities": [{"text": "storyline generating", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.8268688321113586}]}, {"text": "Nevertheless, most existing approaches extract events independently and link relevant events in a post-processing step.", "labels": [], "entities": []}, {"text": "More recently, proposed a non-parametric generative model to extract storylines which is combined with Chinese Restaurant Processes (CRPs) to determine the number of storylines automatically.", "labels": [], "entities": []}, {"text": "However, the parameter inference procedure is too complex and the model requires longtime to converge.", "labels": [], "entities": []}, {"text": "This makes it impractical to be deployed in real-world applications.", "labels": [], "entities": []}, {"text": "Recently, deep learning techniques have been successfully applied to various natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.7129489555954933}]}, {"text": "Several approaches () such as word2vec have been proved efficient in representing rich syntactic and semantic information in text.", "labels": [], "entities": []}, {"text": "Therefore, it would be interesting to combine the advantage of both probabilistic graphical model and deep neural networks.", "labels": [], "entities": []}, {"text": "There have been some efforts in exploring this in recent years.", "labels": [], "entities": []}, {"text": "For example,  proposed a gaussian mixture neural topic model incorporating both the ordering of words and the semantic meaning of sentences into a topic model.", "labels": [], "entities": []}, {"text": "explained topic models from the perspective of neural networks and proposed a neural topic model where the representation of words and documents are combined into a unified framework.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, there is no attempt in extracting structured repre-sentation of storylines from text using neural network based approaches.", "labels": [], "entities": [{"text": "extracting structured repre-sentation of storylines from text", "start_pos": 62, "end_pos": 123, "type": "TASK", "confidence": 0.7228331310408456}]}, {"text": "In this paper, we propose a novel neural model for storyline generation without the use of any annotated data.", "labels": [], "entities": [{"text": "storyline generation", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.9445815980434418}]}, {"text": "In specific, we assume that the storyline distributions of a document's title and its main body are similar.", "labels": [], "entities": []}, {"text": "A pairwise ranking approach is used to optimize the model.", "labels": [], "entities": []}, {"text": "We also assume that similar documents described in neighboring time periods should share similar storyline distributions.", "labels": [], "entities": []}, {"text": "Hence, the model learned in the previous time period can be used for guiding the learning of the model in the current period.", "labels": [], "entities": []}, {"text": "Based on the two assumptions, relevant events can be extracted and linked.", "labels": [], "entities": []}, {"text": "Furthermore, storyline filtering based on confidence scores is performed.", "labels": [], "entities": [{"text": "storyline filtering", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.9332095682621002}]}, {"text": "This makes it possible to generate new storylines.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are summarized below: \u2022 We propose a novel neural network based model to extract structured representations and evolution patterns of storylines.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, it is the first attempt to perform storyline generation based on neural network without any annotated data.", "labels": [], "entities": [{"text": "storyline generation", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.9520910382270813}]}, {"text": "\u2022 The proposed approach has been evaluated on three corpora and a significant improvement on F-measure is achieved when compared to the state-of-the-art approaches.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9975870847702026}]}, {"text": "Moreover, the proposed approach only requires a faction of the training time in comparison with the second best approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experimental results of the proposed approach in comparison to the baselines on Dataset I, II and III are presented in.", "labels": [], "entities": []}, {"text": "For Dataset I, as it is hard to know the ground-truth of storylines, we only report the precision value by manually examining the extracted storylines.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9995548129081726}]}, {"text": "It can be observed from that the proposed approach achieves the best performance on the three datasets.", "labels": [], "entities": []}, {"text": "In specific, for Dataset I, NSEM extracts more storylines and with a higher precision value.", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9975825548171997}]}, {"text": "For Dataset II containing 77 storylines, NSEM extracts 81 storylines among which  61 are correct and outperforms DSEM with 2% in F-measure.", "labels": [], "entities": [{"text": "NSEM", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.5941735506057739}, {"text": "DSEM", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.46156829595565796}, {"text": "F-measure", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.993988037109375}]}, {"text": "For dataset III consisting of 30 storylines, NSEM extracted 27 storylines among which 21 are correct.", "labels": [], "entities": [{"text": "NSEM", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.8701976537704468}]}, {"text": "Although its recall value is the same as DSEM, its precision value is nearly 3% higher which results in better F-measure.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9996265172958374}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9997084736824036}, {"text": "F-measure", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.997585654258728}]}], "tableCaptions": [{"text": " Table 4.1.  Among which the Dataset III includes 30 different  types of manually annotated storylines which are  categorized into four types: (1) long-term story- lines which last for more than 2 weeks; (2) short- term storylines which last for less than 1 week; (3)  intermittent storylines which last for more than 2  weeks in total, but stop for a time and then ap- pear again; (4) new storylines which emerge in the  middle of the period, not at the beginning.", "labels": [], "entities": []}, {"text": " Table 1: Statistics of the three datasets.", "labels": [], "entities": []}, {"text": " Table 2: Performance comparison of the storyline ex- traction results on Dataset I, II and III.", "labels": [], "entities": []}, {"text": " Table 3: The performances of NSEM with different S.", "labels": [], "entities": [{"text": "NSEM", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.8394852876663208}]}]}