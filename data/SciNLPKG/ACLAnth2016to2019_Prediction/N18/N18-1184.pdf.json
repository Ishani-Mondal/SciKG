{"title": [{"text": "Unsupervised Induction of Linguistic Categories with Records of Reading, Speaking, and Writing", "labels": [], "entities": []}], "abstractContent": [{"text": "When learning POS taggers and syntactic chunkers for low-resource languages, different resources maybe available, and often all we have is a small tag dictionary, motivating type-constrained unsupervised induction.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.7480919361114502}]}, {"text": "Even small dictionaries can improve the performance of unsupervised induction algorithms.", "labels": [], "entities": []}, {"text": "This paper shows that performance can be further improved by including data that is readily available or can be easily obtained for most languages, i.e., eye-tracking, speech, or keystroke logs (or any combination thereof).", "labels": [], "entities": []}, {"text": "We project information from all these data sources into shared spaces, in which the union of words is represented.", "labels": [], "entities": []}, {"text": "For English unsuper-vised POS induction, the additional information , which is not required attest time, leads to an average error reduction on Ontonotes domains of 1.5% over systems augmented with state-of-the-art word embeddings.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.8023385107517242}]}, {"text": "On Penn Treebank the best model achieves 5.4% error reduction over a word embeddings baseline.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 3, "end_pos": 16, "type": "DATASET", "confidence": 0.994684487581253}, {"text": "error reduction", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.9567322134971619}]}, {"text": "We also achieve significant improvements for syntactic chunk induction.", "labels": [], "entities": [{"text": "syntactic chunk induction", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.7936930855115255}]}, {"text": "Our analysis shows that improvements are even bigger when the available tag dictionaries are smaller.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is a core assumption in linguistics that humans have knowledge of grammar and that they use this knowledge to generate and process language.", "labels": [], "entities": []}, {"text": "Reading, writing, and talking leave traces of this knowledge and in psycholinguistics this data is used to analyze our grammatical competencies.", "labels": [], "entities": []}, {"text": "Psycholinguists are typically interested in falsifying a specific hypothesis about our grammatical competencies and therefore collect data with this hypothesis in mind.", "labels": [], "entities": []}, {"text": "In NLP, we typically require big, representative corpora.", "labels": [], "entities": []}, {"text": "NLP usually has inLea Frermann carried out this work while at the University of Edinburgh.", "labels": [], "entities": [{"text": "NLP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9012712240219116}]}, {"text": "duced the models from expensive corpus annotations by professional linguists, but recently, a few researchers have shown that data traces from human processing can be used directly to improve NLP models (.", "labels": [], "entities": []}, {"text": "In this paper, we investigate whether unsupervised POS induction and unsupervised syntactic chunking can be improved using human text processing traces.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.8901114165782928}]}, {"text": "We also explore what traces are beneficial, and how they are best combined.", "labels": [], "entities": []}, {"text": "Our work supplements psycholinguistic research by evaluating human data on larger scale than usual, but more robust unsupervised POS induction also contributes to NLP for low-resource languages for which professional annotators are hard to find, and where instead, data from native speakers can be used to augment unsupervised learning.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.7256125509738922}]}, {"text": "We explore three different modalities of data reflecting human processing plus standard, pretrained distributional word embeddings for comparison, but also because some modalities might fare better when combined with distributional vectors.", "labels": [], "entities": []}, {"text": "Data reflecting human processing come from reading (two different eye-tracking corpora), speaking (prosody), and typing (keystroke logging).", "labels": [], "entities": []}, {"text": "We test three different methods of combining the different word representations: a) canonical correlation analysis (CCA)) and b) singular value decomposision and inverted softmax feature projection (SVD+IS) () and c) simple concatenation of feature vectors.", "labels": [], "entities": []}, {"text": "Contributions We present experiments in unsupervised POS and syntactic chunk induction using multi-modal word representations, obtained from records of reading, speaking, and writing.", "labels": [], "entities": [{"text": "syntactic chunk induction", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.7197167674700419}]}, {"text": "Individually, all modalities are known to contain syntactic processing signals, but to the best of our knowledge, we are the first to combine them in one model.", "labels": [], "entities": []}, {"text": "Our work extends on previous work in several respects: (a) We compare using data traces from gaze, speech, and keystrokes.", "labels": [], "entities": []}, {"text": "(b) We consider three ways of combining such information that do not require access to data from all modalities for all words.", "labels": [], "entities": []}, {"text": "(c) While some previous work assumed access to gaze data attest time, our models do not assume access to any modalities attest time.", "labels": [], "entities": []}, {"text": "(d) We evaluate how much the additional information helps, depending on the size of the available tag dictionary.", "labels": [], "entities": []}, {"text": "(e) While related work on keystrokes and prosody focused on a single feature, all our word representations are multidimensional and continuous.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our application of these word representations and their combinations is unsupervised POS and syntactic chunk induction, but before presenting our projection methods in \u00a74 and our experiments in \u00a75, we present a preliminary evaluation of the different modalities using word association norms.", "labels": [], "entities": [{"text": "POS", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.7597799897193909}, {"text": "syntactic chunk induction", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.6719165543715159}]}, {"text": "shows the weighted correlation between cosine distances in the representations and the human ratings in the word association norm datasets available at wordvectors.org).", "labels": [], "entities": []}, {"text": "Eigenwords, not surprisingly, correlates better than the representation based on processing data -with the exception of prosody.", "labels": [], "entities": []}, {"text": "The correlation with prosody is non-significant, however, because of the small sample size.", "labels": [], "entities": []}, {"text": "We now have word representations from different, complementary modalities, with very different coverages, but all including a small overlap.", "labels": [], "entities": []}, {"text": "We assume that the different modalities contain complementary human text processing traces because they reflect different cognitive processes, which motivates us to combine these different sources of information.", "labels": [], "entities": []}, {"text": "Our assumption is confirmed in the evaluation.", "labels": [], "entities": []}, {"text": "The fact that we have very low coverage for some modalities, and the fact that we have an overlap between all our vocabularies, specifically motivates an approach, in which we use the intersection of word types to learn a projection from two or more of these modalities into a shared space.", "labels": [], "entities": []}, {"text": "Obviously, we can also simply concatenate our representations, but because of the low coverage of some modalities and because co-projecting modalities has some regularization effect, we hypothesize that it is better to learn a projection into a shared space.", "labels": [], "entities": []}, {"text": "This hypothesis is verified by the results in \u00a76.", "labels": [], "entities": []}, {"text": "This section presents our POS and syntactic chunk induction experiments.", "labels": [], "entities": [{"text": "syntactic chunk induction", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.6694681843121847}]}, {"text": "We present the datasets we used in our experiments, the sequence tagging architecture, based on second-order hidden Markov models, as well as the dictionary we used to constrain inference at training and test time.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7191394716501236}]}], "tableCaptions": [{"text": " Table 1: Results on word association norms from  wordvectors.org Correlation weighted by number  of found pairs per word embedding type.", "labels": [], "entities": []}, {"text": " Table 3: Chunk tagging accuracy. Best models  from CCA, SVD+IS and concatenation. Model  section on development set. * p < .001 Mcnemar  mid-p test when comparing to no embeddings.  \u2020  p < .001 Mcnemar mid-p test when comparing to  Eigenwords.)", "labels": [], "entities": [{"text": "Chunk tagging", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.6468911170959473}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9735327959060669}]}, {"text": " Table 4: POS tagging accuracies for baselines and the model combinations that performed best on  newswire development data (NW). Best performance per domain is boldfaced. *) p < .001 McNemar  mid-p test when compared to the no embeddings condition for the corresponding test set.  \u2020) p < .001  McNemar mid-p test when compared to eigenwords for the corresponding test set.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7553076148033142}, {"text": "newswire development data (NW)", "start_pos": 98, "end_pos": 128, "type": "DATASET", "confidence": 0.8195972641309103}]}, {"text": " Table 5: Graph similarities in [0, \u221e), 0 = identical.", "labels": [], "entities": [{"text": "Graph", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9651558995246887}]}]}