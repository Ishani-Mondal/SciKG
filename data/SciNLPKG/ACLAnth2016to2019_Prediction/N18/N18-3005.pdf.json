{"title": [{"text": "Data Collection fora Production Dialogue System: A Clinc Perspective", "labels": [], "entities": []}], "abstractContent": [{"text": "Industrial dialogue systems such as Apple Siri and Google Assistant require large scale diverse training data to enable their sophisticated conversation capabilities.", "labels": [], "entities": []}, {"text": "Crowdsourc-ing is a scalable and inexpensive data collection method, but collecting high quality data efficiently requires thoughtful orchestration of crowdsourcing jobs.", "labels": [], "entities": []}, {"text": "Prior study of data collection process has focused on tasks with limited scope and performed intrinsic data analysis , which may not be indicative of impact on trained model performance.", "labels": [], "entities": [{"text": "data collection process", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7937533656756083}]}, {"text": "In this paper, we present a study of crowdsourcing methods fora user intent classification task in one of our deployed dialogue systems.", "labels": [], "entities": [{"text": "user intent classification task", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.7197288349270821}]}, {"text": "Our task requires classification over 47 possible user intents and contains many intent pairs with subtle differences.", "labels": [], "entities": []}, {"text": "We consider different crowdsourcing job types and job prompts, quantitatively analyzing the quality of collected data and downstream model performance on a test set of real user queries from production logs.", "labels": [], "entities": []}, {"text": "Our observations provide insight into how design decisions impact crowdsourced data quality, with clear recommendations for future data collection for dialogue systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Large, high quality corpora are crucial in the development of effective machine learning models in many areas, including Natural Language Processing (NLP).", "labels": [], "entities": []}, {"text": "The performance of the machine learning models, especially deep learning models, depend heavily on the quantity and quality of the training data.", "labels": [], "entities": []}, {"text": "Developing dialogue systems such as Apple Siri, Google Assistant and Amazon Alexa poses a significant challenge for data collection as we need to do rapid prototyping and bootstrapping to train new virtual assistant capabilities.", "labels": [], "entities": [{"text": "data collection", "start_pos": 116, "end_pos": 131, "type": "TASK", "confidence": 0.8178769052028656}]}, {"text": "The use of crowdsourcing has enabled the creation of large corpora at relatively low cost ( and is critical in collecting the quantities of data required to train models with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9651235938072205}]}, {"text": "However, designing effective methodologies for data collection with the crowd is largely an open research question (.", "labels": [], "entities": []}, {"text": "From the perspective of Clinc, a young AI company creating innovative conversational AI experiences, there exists a major challenge when collecting data to build a dialogue system.", "labels": [], "entities": []}, {"text": "We have observed that the complexity of building production-grade dialogue system is often substantially greater than those studied in the research community.", "labels": [], "entities": []}, {"text": "For example, one of our production deployed dialogue systems requires intent classification among 47 different intents to meet product specifications, whereas most academic datasets for text classification only have a small number (i.e., 2-14) of classes ().", "labels": [], "entities": [{"text": "intent classification", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.7080662697553635}, {"text": "text classification", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.7291941791772842}]}, {"text": "The few datasets that have a large number of classes, such as RCV-1 (), distribute intents across many distinct topics.", "labels": [], "entities": []}, {"text": "We address the significantly more challenging problem of handling many intents within a single domain, specifically personal finance and wealth management, requiring the classifier to carefully distinguish between nuanced intent topics.", "labels": [], "entities": []}, {"text": "Therefore, a large amount of high-quality training data tailored to our targeted problem is critical for creating the best user experience in our production dialogue system.", "labels": [], "entities": []}, {"text": "Crowdsourcing offers a promising solution by massively parallelizing data collection efforts across a large pool of workers at relatively low cost.", "labels": [], "entities": []}, {"text": "Because of the involvement of crowd workers, collecting high-quality data efficiently requires careful orchestration of crowdsourcing jobs, including their instructions and prompts.", "labels": [], "entities": []}, {"text": "In order to collect the large-scale tailored dataset we need via crowdsourcing, there are several research questions we need to answer: \u2022 How can we evaluate the effectiveness of crowdsourcing methods and the quality of the datasets collected via these methods?", "labels": [], "entities": []}, {"text": "\u2022 During the data collection process, how can we identify the point when additional data would have diminishing returns on the performance of the downstream trained models?", "labels": [], "entities": [{"text": "data collection", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.7637516260147095}]}, {"text": "\u2022 Which crowdsourcing method yields the highest-quality training data for intent classification in a production dialogue system?", "labels": [], "entities": [{"text": "intent classification", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.8546226322650909}]}, {"text": "There is limited work on effective techniques to evaluate a crowdsourcing method and the data collected using that method.", "labels": [], "entities": []}, {"text": "Prior work has focused on intrinsic analysis of the data, lacking quantitative investigation of the data's impact on downstream model performance (.", "labels": [], "entities": []}, {"text": "In this paper, we propose two novel modelindepedent metrics to evaluate dataset quality.", "labels": [], "entities": []}, {"text": "Specifically, we introduce (1) coverage, quantifying how well a training set covers the expression space of a certain task, and (2) diversity, quantifying the heterogeneity of sentences in the training set.", "labels": [], "entities": []}, {"text": "We verify the effectiveness of both metrics by correlating them with the model accuracy of two well-known algorithms, SVM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.910487949848175}]}, {"text": "We show that while diversity gives a sense of the variation in the data, coverage closely correlates with the model accuracy and serves as an effective metric for evaluating training data quality.", "labels": [], "entities": [{"text": "coverage", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9717261791229248}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9868614077568054}]}, {"text": "We then describe in detail two crowdsourcing methods we use to collect intent classification data for our deployed dialogue system.", "labels": [], "entities": [{"text": "intent classification", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.69106525182724}]}, {"text": "The key ideas of these two methods are (1) describing the intent as a scenario or (2) providing an example sentence to be paraphrased.", "labels": [], "entities": []}, {"text": "We experiment multiple variants of these methods by varying the number and type of prompts and collect training data using each variant.", "labels": [], "entities": []}, {"text": "We perform metric and accuracy evaluation of these datasets and show that using a mixture of different prompts and sampling paraphrasing exmples from real user queries yield training data with higher coverage and diversity and lead to better performing models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9977595806121826}]}, {"text": "These observations have impacted the way that we collect data and are improving the quality of our production system.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first verify that diversity and coverage provide insight regarding training data quality.", "labels": [], "entities": [{"text": "coverage", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9514185786247253}]}, {"text": "We compare trends in these metrics with trends in model accuracy as the amount of training data is increased.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9942459464073181}]}, {"text": "We then evaluate the performance of the scenario-driven and paraphrase methods and their variants by comparing the quality of training data collected via these methods.", "labels": [], "entities": []}, {"text": "Finally, we explore sampling paraphrasing examples from the test set and compare against manually generation by engineers.", "labels": [], "entities": []}, {"text": "This data is collected using a combination of generic and specific paraphrase examples. and 4 show diversity, coverage, and accuracy of the SVM and FastText models as we vary the number of training examples for scenario-driven and paraphrase-based jobs, respectively.", "labels": [], "entities": [{"text": "coverage", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9744908213615417}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9994614720344543}]}, {"text": "In this experiment, we use a combination of both generic and specific scenarios and paraphrasing examples.", "labels": [], "entities": []}, {"text": "We observe that for both scenario and paraphrase jobs, the diversity starts high (> 0.90) with a few hundred training samples and stay stable as training data size increases.", "labels": [], "entities": []}, {"text": "This means that the new training examples generally have a low percentage of n-grams overlap and along distance) with the existing examples, therefore maintaining the overall high diversity.", "labels": [], "entities": []}, {"text": "This indicates that the newly introduced examples are generally creative contributions from the crowd and not repeats or straightforward rephrase of the existing samples with the same words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of generic and specific scenario description and paraphrasing prompts.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy, coverage and diversity for  the six template + prompt conditions considered, all  with ~4.7K training samples.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9966384172439575}, {"text": "coverage", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9924160242080688}, {"text": "diversity", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9697713851928711}]}, {"text": " Table 3: Comparison of manually generating prompts  and sampling from test set, evaluated on half of the test  data (kept blind in sampling).", "labels": [], "entities": []}, {"text": " Table 4: Accuracy, coverage and diversity of  paraphrasing jobs using 1-5 prompts sampled from the  test set, with constant training set size (~4.7K).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9719240069389343}, {"text": "coverage", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9697239995002747}]}]}