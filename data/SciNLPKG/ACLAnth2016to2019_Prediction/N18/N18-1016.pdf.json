{"title": [{"text": "Discourse-Aware Neural Rewards for Coherent Text Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we investigate the use of discourse-aware rewards with reinforcement learning to guide a model to generate long, coherent text.", "labels": [], "entities": []}, {"text": "In particular, we propose to learn neural rewards to model cross-sentence ordering as a means to approximate desired discourse structure.", "labels": [], "entities": [{"text": "cross-sentence ordering", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.7463115751743317}]}, {"text": "Empirical results demonstrate that a generator trained with the learned reward produces more coherent and less repetitive text than models trained with cross-entropy or with reinforcement learning with commonly used scores as rewards.", "labels": [], "entities": []}], "introductionContent": [{"text": "Defining an ideal loss for training text generation models remains an open research question.", "labels": [], "entities": [{"text": "training text generation", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6084861755371094}]}, {"text": "Many existing approaches based on variants of recurrent neural networks) are trained using cross-entropy loss (, often augmented with additional terms for topic coverage or task-specific supervision (.", "labels": [], "entities": []}, {"text": "Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE), BLEU (), or CIDEr (.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.9870823621749878}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9966720342636108}]}, {"text": "Another current line of research therefore explores training generation models that directly optimize the target evaluation measure () using reinforcement learning methods such as the REINFORCE algorithm.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.8699748516082764}]}, {"text": "* Work done while author was at Microsoft Research", "labels": [], "entities": [{"text": "Microsoft Research", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.8431841731071472}]}], "datasetContent": [{"text": "We use the Now You're Cooking dataset with the same training/test/development splits from.", "labels": [], "entities": [{"text": "Now You're Cooking dataset", "start_pos": 11, "end_pos": 37, "type": "DATASET", "confidence": 0.7758935213088989}]}, {"text": "For training, we use 109567 recipes with 1000 recipes set aside for both development and test.", "labels": [], "entities": []}, {"text": "We perform a human evaluation on 100 recipes sampled from the test set to evaluate our model on four aspects of recipe quality: fluency, ingredient use, title completion, and action ordering.", "labels": [], "entities": [{"text": "action ordering", "start_pos": 175, "end_pos": 190, "type": "TASK", "confidence": 0.7142544835805893}]}, {"text": "For each example, three judges from Amazon Mechanical Turk are shown a pair of recipes, each generated by a different model and asked to select the recipe that is better according to the criteria above.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.9298336903254191}]}, {"text": "For ingredient use, judges select the recipe that uses more of the ingredients correctly.", "labels": [], "entities": []}, {"text": "For title completion, we ask judges to select the recipe that best completes the dish described in the recipe title.", "labels": [], "entities": [{"text": "title completion", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.6927451491355896}]}, {"text": "Finally, for action ordering, judges choose the recipe that better links subtasks in the recipes.", "labels": [], "entities": [{"text": "action ordering", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8013952374458313}]}], "tableCaptions": [{"text": " Table 1: Evaluation results for generated sequences by models and baselines. We bold the top performing result.  The second to fourth columns list word-level scores. Columns AB1, AB4, and AR-L list action-level scores ( \u00a76.1).  Columns SCB1, SCB4, and SCR-L list state change level scores ( \u00a76.1).", "labels": [], "entities": []}, {"text": " Table 2: Human evaluation measuring proportion of  winners. Upper table compares MLE baseline with RO  + B4 model. Lower table compares BLEU-1 baseline  with RO + B4 model.", "labels": [], "entities": [{"text": "MLE baseline", "start_pos": 82, "end_pos": 94, "type": "METRIC", "confidence": 0.831853061914444}, {"text": "BLEU-1", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9953188300132751}]}, {"text": " Table 3: Proportion of winners for long generated  recipes. Upper table compares MLE baseline with RO  + B4 model. Lower table compares BLEU-1 baseline  with mixed RO + B4 model.", "labels": [], "entities": [{"text": "BLEU-1", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9732218384742737}]}]}