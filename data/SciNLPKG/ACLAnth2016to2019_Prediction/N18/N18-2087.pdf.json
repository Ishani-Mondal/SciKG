{"title": [{"text": "Unsupervised Disambiguation of Syncretism in Inflected Lexicons", "labels": [], "entities": []}], "abstractContent": [{"text": "Lexical ambiguity makes it difficult to compute various useful statistics of a corpus.", "labels": [], "entities": []}, {"text": "A given word form might represent any of several morphological feature bundles.", "labels": [], "entities": []}, {"text": "One can, however, use unsupervised learning (as in EM) to fit a model that probabilistically disam-biguates word forms.", "labels": [], "entities": []}, {"text": "We present such an approach , which employs a neural network to smoothly model a prior distribution over feature bundles (even rare ones).", "labels": [], "entities": []}, {"text": "Although this basic model does not consider a token's context , that very property allows it to operate on a simple list of unigram type counts, partitioning each count among different analyses of that un-igram.", "labels": [], "entities": []}, {"text": "We discuss evaluation metrics for this novel task and report results on 5 languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Inflected lexicons-lists of morphologically inflected forms-are commonplace in NLP.", "labels": [], "entities": []}, {"text": "Such lexicons currently exist for over 100 languages in a standardized annotation scheme (, making them one of the most multi-lingual annotated resources in existence.", "labels": [], "entities": []}, {"text": "These lexicons are typically annotated at the type level, i.e., each word type is listed with its possible morphological analyses, divorced from sentential context.", "labels": [], "entities": []}, {"text": "One might imagine that most word types are unambiguous.", "labels": [], "entities": []}, {"text": "However, many inflectional systems are replete with a form of ambiguity termed syncretism-a systematic merger of morphological slots.", "labels": [], "entities": []}, {"text": "In English, some verbs have five distinct inflected forms, but regular verbs (the vast majority) merge two of these and so distinguish only four.", "labels": [], "entities": []}, {"text": "The verb \u0081s \u0097i\u0084n g has the past tense form sang but the participial form sung; the verb \u007ft a\u0086l\u0086k, on the other hand, employs talked for both functions.", "labels": [], "entities": []}, {"text": "The form talked is, thus, said to be syncretic.", "labels": [], "entities": []}, {"text": "Our task is to partition the count of talked in a corpus between the past-tense and participial readings, respectively.: Full paradigms for the German nouns W\u008b o&r%t (\"word\") and He\u0084r&r# (\"gentleman\") with abbreviated and tabularized UniMorph annotation.", "labels": [], "entities": []}, {"text": "The syncretic forms are bolded and colored by ambiguity class.", "labels": [], "entities": []}, {"text": "Note that, while in the plural the nominative and accusative are always syncretic across all paradigms, the same is not true in the singular.", "labels": [], "entities": []}, {"text": "In this paper, we model a generative probability distribution over annotated word forms, and fit the model parameters using the token counts of unannotated word forms.", "labels": [], "entities": []}, {"text": "The resulting distribution predicts how to partition each form's token count among its possible annotations.", "labels": [], "entities": []}, {"text": "While our method actually deals with all ambiguous forms in the lexicon, it is particularly useful for syncretic forms because syncretism is often systematic and pervasive.", "labels": [], "entities": []}, {"text": "In English, our unsupervised procedure learns from the counts of irregular pairs like sang-sung that a verb's past tense tends to be more frequent than its past participle.", "labels": [], "entities": []}, {"text": "These learned parameters are then used to disambiguate talked.", "labels": [], "entities": []}, {"text": "The method can also learn from regular paradigms.", "labels": [], "entities": []}, {"text": "For example, it learns from the counts of pairs like runs-run that singular third-person forms are common.", "labels": [], "entities": []}, {"text": "It then uses these learned parameters to guess that tokens of run are often singular or third-person (though never both at once, because the lexicon does not list that as a possible analysis of run).", "labels": [], "entities": []}], "datasetContent": [{"text": "Since our model is a tractable generative model, we may easily evaluate its perplexity on held-out tokens.", "labels": [], "entities": []}, {"text": "For each language, we randomly partition the observed surface tokens into 80% training, 10% development, and 10% test.", "labels": [], "entities": []}, {"text": "We then estimate the parameters of our model by maximizing (3) on the counts from the training portion, selecting hyperparameters such that the estimated parameters 2 minimize perplexity on the development portion.", "labels": [], "entities": []}, {"text": "We then report perplexity on the test portion.", "labels": [], "entities": []}, {"text": "Using the same hyperparameters, we now train our latent-variable model p \u03b8 without supervision on 100% of the observed surface forms f . We now measure how poorly, for the average surface form type f , we recovered the maximum-likelihood distribution\u02c6ptribution\u02c6 tribution\u02c6p(t, , s | f ) that would be estimated with supervision in terms of KL-divergence: We can see that this formula reduces to a simple average over disambiguated tokens i.", "labels": [], "entities": []}, {"text": "Each language constitutes a separate experiment.", "labels": [], "entities": []}, {"text": "In each case we obtain our lexicon from the UniMorph project and our surface form counts from Wikipedia.", "labels": [], "entities": []}, {"text": "To approximate supervised counts to estimat\u00ea pin the KL evaluation, we analyzed the surface form tokens in Wikipedia (in context) using the tool in, as trained on the disambiguated Universal Dependencies (UD) corpora.", "labels": [], "entities": []}, {"text": "We wrote a script to convert the resulting analyses from UD format into t, , s, f tuples in UniMorph format for five languages-Czech (cs), German (de), Finnish (fi), Hebrew (he), Swedish (sv)-each of which displays both kinds of ambiguity in its UniMorph lexicon.", "labels": [], "entities": []}, {"text": "Lexicons with these approximate supervised counts are provided as supplementary material.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for the best performing neural network  (hyperparameters selected on dev) and the three base- lines under both performance metrics. Best are bolded.", "labels": [], "entities": []}]}