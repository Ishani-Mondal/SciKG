{"title": [{"text": "Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time", "labels": [], "entities": []}], "abstractContent": [{"text": "Dynamic topic modeling facilitates the identification of topical trends overtime in temporal collections of unstructured documents.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.7242012023925781}]}, {"text": "We introduce a novel unsupervised neural dynamic topic model named as Recurrent Neural Network-Replicated Softmax Model (RNN-RSM), where the discovered topics at each time influence the topic discovery in the subsequent time steps.", "labels": [], "entities": []}, {"text": "We account for the temporal ordering of documents by explicitly mod-eling a joint distribution of latent topical dependencies overtime, using distributional es-timators with temporal recurrent connections.", "labels": [], "entities": []}, {"text": "Applying RNN-RSM to 19 years of articles on NLP research, we demonstrate that compared to state-of-the art topic models, RNN-RSM shows better generalization, topic interpretation , evolution and trends.", "labels": [], "entities": [{"text": "topic interpretation", "start_pos": 158, "end_pos": 178, "type": "TASK", "confidence": 0.7227861434221268}]}, {"text": "We also introduce a metric (named as SPAN) to quantify the capability of dynamic topic model to capture word evolution in topics overtime.", "labels": [], "entities": [{"text": "SPAN", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.8316372632980347}]}], "introductionContent": [{"text": "Topic Detection and Tracking ( is an important area of natural language processing to find topically related ideas that evolve overtime in a sequence of text collections and exhibit temporal relationships.", "labels": [], "entities": [{"text": "Topic Detection and Tracking", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8741106688976288}]}, {"text": "The temporal aspects of these collections can present valuable insight into the topical structure of the collections and can be quantified by modeling the dynamics of the underlying topics discovered overtime.", "labels": [], "entities": []}, {"text": "Problem Statement: We aim to generate temporal topical trends or automatic overview timelines of topics fora time sequence collection of documents.", "labels": [], "entities": []}, {"text": "This involves the following three tasks in dynamic topic analysis: (1) Topic Structure Detection (TSD): Identifying main topics in the document collection.", "labels": [], "entities": [{"text": "dynamic topic analysis", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6396179298559824}, {"text": "Topic Structure Detection (TSD)", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.7219336082537969}]}, {"text": "(2) Topic Evolution Detection (TED): Detecting the emergence of anew topic  and recognizing how it grows or decays overtime.", "labels": [], "entities": [{"text": "Topic Evolution Detection (TED)", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8513554433981577}]}, {"text": "Temporal Topic Characterization (TTC): Identifying the characteristics for each of the main topics in order to track the words' usage (keyword trends) fora topic overtime i.e. topical trend analysis for word evolution.", "labels": [], "entities": [{"text": "Temporal Topic Characterization (TTC)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.67412169277668}, {"text": "word evolution", "start_pos": 203, "end_pos": 217, "type": "TASK", "confidence": 0.7225581705570221}]}, {"text": "Probabilistic static topic models, such as Latent Dirichlet Allocation (LDA) ( and its variants ( have been investigated to examine the emergence of topics from historical documents.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 43, "end_pos": 75, "type": "METRIC", "confidence": 0.8648908972740174}]}, {"text": "Another variant known as Replicated Softmax (RSM)) has demonstrated better generalization in log-probability and retrieval, compared to LDA.", "labels": [], "entities": []}, {"text": "Prior works () have investigated Bayesian modeling of topics in time-stamped documents.", "labels": [], "entities": [{"text": "Bayesian modeling of topics in time-stamped documents", "start_pos": 33, "end_pos": 86, "type": "TASK", "confidence": 0.7880703168255943}]}, {"text": "Particularly, developed a LDA based dynamic topic model (DTM) to capture the evolution of topics in a time sequence collection of documents; however they do not capture explicitly the topic popularity and usage of specific terms overtime.", "labels": [], "entities": []}, {"text": "We propose a family of probabilistic time series models with distributional estimators to explicitly model the dynamics of the underlying topics, introducing temporal latent topic dependencies.", "labels": [], "entities": []}, {"text": "To model temporal dependencies in high dimen-", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the processed dataset (, consisting of EMNLP and ACL conference papers from the year 1996 through 2014.", "labels": [], "entities": [{"text": "EMNLP and ACL conference papers", "start_pos": 46, "end_pos": 77, "type": "DATASET", "confidence": 0.8562169194221496}]}, {"text": "We combine papers for each year from the two venues to prepare the document collections overtime.", "labels": [], "entities": []}, {"text": "We use ExpandRank ( to extract top 100 keyphrases for each paper, including unigrams and bigrams.", "labels": [], "entities": []}, {"text": "We split the bigrams to unigrams to create a dictionary of all unigrams and bigrams.", "labels": [], "entities": []}, {"text": "The dictionary size (K) and word count are 3390 and 5.19 M, respectively.", "labels": [], "entities": [{"text": "dictionary size (K)", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9110988259315491}]}, {"text": "We evaluate RNN-RSM against static (RSM, LDA) and dynamic (DTM) topics models for topic and keyword evolution in NLP research overtime.", "labels": [], "entities": [{"text": "topic and keyword evolution", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.6273794770240784}]}, {"text": "Individual 19 different RSM and LDA models are trained for each year, while DTM 2 and RNN-RSM are trained over the years with 19 time steps, where paper collections fora year is input at each time step.", "labels": [], "entities": [{"text": "RSM", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9630584716796875}, {"text": "DTM", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.8582921624183655}, {"text": "RNN-RSM", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.8297163248062134}]}, {"text": "RNN-RSM is initialized with RSM (W vh , bv, b h ) trained for the year 2014.", "labels": [], "entities": [{"text": "RNN-RSM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9192213416099548}]}, {"text": "We use perplexity to choose the number of topics (=30).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Hyperparameters for RNN-RSM model", "labels": [], "entities": []}, {"text": " Table 3: State-of-the-art Comparison: Generalization  (PPL and Err), Topic Interpretation (COH) and Evolu- tion (TTD) in DTM and RNN-RSM models", "labels": [], "entities": [{"text": "Evolu- tion (TTD)", "start_pos": 101, "end_pos": 118, "type": "METRIC", "confidence": 0.9034774402777354}]}, {"text": " Table 4: Topics (top 15 words) with the highest and lowest drifts (cosine) observed in DTM and RNN-RSM", "labels": [], "entities": [{"text": "DTM", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.8837719559669495}, {"text": "RNN-RSM", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.6278819441795349}]}, {"text": " Table 5: Topics with the highest and lowest coherence", "labels": [], "entities": []}, {"text": " Table 6: SPAN (S k ) for selected terms, avg-SPAN and  set ||  Q|| by LDA, RSM, DTM and RNN-RSM", "labels": [], "entities": []}]}