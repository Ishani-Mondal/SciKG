{"title": [{"text": "Text Segmentation as a Supervised Learning Task", "labels": [], "entities": [{"text": "Text Segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7421192526817322}]}], "abstractContent": [{"text": "Text segmentation, the task of dividing a document into contiguous segments based on its semantic structure, is a longstanding challenge in language understanding.", "labels": [], "entities": [{"text": "Text segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7125678360462189}, {"text": "language understanding", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.7268961071968079}]}, {"text": "Previous work on text segmentation focused on unsupervised methods such as clustering or graph search, due to the paucity in labeled data.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7795405387878418}]}, {"text": "In this work, we formulate text segmentation as a supervised learning problem, and present a large new dataset for text segmentation that is automatically extracted and labeled from Wikipedia.", "labels": [], "entities": [{"text": "formulate text segmentation", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.6553333202997843}, {"text": "text segmentation", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.7239663004875183}]}, {"text": "Moreover, we develop a segmen-tation model based on this dataset and show that it generalizes well to unseen natural text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text segmentation is the task of dividing text into segments, such that each segment is topically coherent, and cutoff points indicate a change of topic).", "labels": [], "entities": [{"text": "Text segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.716411903500557}]}, {"text": "This provides basic structure to a document in away that can later be used by downstream applications such as summarization and information extraction.", "labels": [], "entities": [{"text": "summarization", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.9906631708145142}, {"text": "information extraction", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.8309628665447235}]}, {"text": "Existing datasets for text segmentation are small in size, and are used mostly for evaluating the performance of segmentation algorithms.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7697610259056091}]}, {"text": "Moreover, some datasets were synthesized automatically and thus do not represent the natural distribution of text in documents.", "labels": [], "entities": []}, {"text": "Because no large labeled dataset exists, prior work on text segmentation tried to either come up with heuristics for identifying whether two sentences discuss the same topic), or to model topics explicitly with methods such as LDA () that assign a topic to each paragraph or sentence.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7229681313037872}]}, {"text": "* Both authors contributed equally to this paper and the order of authorship was determined randomly.", "labels": [], "entities": []}, {"text": "Recent developments in Natural Language Processing have demonstrated that casting problems as supervised learning tasks overlarge amounts of labeled data is highly effective compared to heuristic-based systems or unsupervised algorithms ().", "labels": [], "entities": []}, {"text": "Therefore, in this work we (a) formulate text segmentation as a supervised learning problem, where a label for every sentence in the document denotes whether it ends a segment, (b) describe anew dataset, WIKI-727K, intended for training text segmentation models.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7022473812103271}, {"text": "WIKI-727K", "start_pos": 204, "end_pos": 213, "type": "DATASET", "confidence": 0.9235792756080627}, {"text": "text segmentation", "start_pos": 237, "end_pos": 254, "type": "TASK", "confidence": 0.7271727919578552}]}, {"text": "WIKI-727K comprises more than 727,000 documents from English Wikipedia, where the table of contents of each document is used to automatically segment the document.", "labels": [], "entities": [{"text": "WIKI-727K", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9480504393577576}]}, {"text": "Since this dataset is large, natural, and covers a variety of topics, we expect it to generalize well to other natural texts.", "labels": [], "entities": []}, {"text": "Moreover, WIKI-727K provides a better benchmark for evaluating text segmentation models compared to existing datasets.", "labels": [], "entities": [{"text": "WIKI-727K", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.8933954238891602}, {"text": "text segmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7557122707366943}]}, {"text": "We make WIKI-727K and our code publicly available at https: //github.com/koomri/text-segmentation.", "labels": [], "entities": [{"text": "WIKI-727K", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.9437398314476013}]}, {"text": "To demonstrate the efficacy of this dataset, we develop a hierarchical neural model in which a lower-level bidirectional LSTM creates sentence representations from word tokens, and then a higher-level LSTM consumes the sentence representations and labels each sentence.", "labels": [], "entities": []}, {"text": "We show that our model outperforms prior methods, demonstrating the importance of our dataset for future progress in text segmentation.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.79924076795578}]}], "datasetContent": [{"text": "The most common dataset for evaluating performance on text segmentation was created by.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7598708868026733}]}, {"text": "It is a synthetic dataset containing 920 documents, where each document is a concatena-tion of 10 random passages from the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.9116675555706024}]}, {"text": "created a dataset of their own, which consists of 5 manually-segmented political manifestos from the Manifesto project.", "labels": [], "entities": []}, {"text": "1) also used English Wikipedia documents to evaluate text segmentation.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7369463443756104}]}, {"text": "They defined two datasets, one with 100 documents about major cities and one with 118 documents about chemical elements.", "labels": [], "entities": []}, {"text": "provides additional statistics on each dataset.", "labels": [], "entities": []}, {"text": "Thus, all existing datasets for text segmentation are small and cannot benefit from the advantages of training supervised models over labeled data.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.79119473695755}]}, {"text": "For this work we have created anew dataset, which we name WIKI-727K.", "labels": [], "entities": [{"text": "WIKI-727K", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.9790676832199097}]}, {"text": "It is a collection of 727,746 English Wikipedia documents, and their hierarchical segmentation, as it appears in their table of contents.", "labels": [], "entities": []}, {"text": "We randomly partitioned the documents into a train (80%), development (10%), and test (10%) set.", "labels": [], "entities": []}, {"text": "Different text segmentation use-cases require different levels of granularity.", "labels": [], "entities": [{"text": "text segmentation use-cases", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.781687597433726}]}, {"text": "For example, for segmenting text by overarching topic it makes sense to train a model that predicts only top-level segments, which are typically vary in topic -for example, \"History\", \"Geography\", and \"Demographics\".", "labels": [], "entities": []}, {"text": "For segmenting a radio broadcast into separate news stories, which requires finer granularity, it makes sense to train a model to predict sub-segments.", "labels": [], "entities": [{"text": "segmenting a radio broadcast into separate news stories", "start_pos": 4, "end_pos": 59, "type": "TASK", "confidence": 0.8342930823564529}]}, {"text": "Our dataset provides the entire segmentation information, and an application may choose the appropriate level of granularity.", "labels": [], "entities": []}, {"text": "To generate the data, we performed the following preprocessing steps for each Wikipedia document: \u2022 Removed all photos, tables, Wikipedia template elements, and other non-text elements.", "labels": [], "entities": []}, {"text": "\u2022 Removed single-sentence segments, documents with less than three segments, and documents where most segments were filtered.", "labels": [], "entities": []}, {"text": "\u2022 Divided each segment into sentences using the PUNKT tokenizer of the NLTK library ().", "labels": [], "entities": []}, {"text": "This is necessary for the use of our dataset as a benchmark, as without a well-defined sentence segmentation, it is impossible to evaluate different models.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.7911972403526306}]}, {"text": "We view WIKI-727K as suitable for text segmentation because it is natural, open-domain, and has a well-defined segmentation.", "labels": [], "entities": [{"text": "WIKI-727K", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.9317981600761414}, {"text": "text segmentation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7877115309238434}]}, {"text": "Moreover, neural network models often benefit from a wealth of training data, and our dataset can easily be further expanded at very little cost.", "labels": [], "entities": []}, {"text": "We evaluate our method on the WIKI-727 test set, Choi's synthetic dataset, and the two small Wikipedia datasets (CITIES, ELEMENTS) introduced by.", "labels": [], "entities": [{"text": "WIKI-727 test set", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9836875995000204}, {"text": "Choi's synthetic dataset", "start_pos": 49, "end_pos": 73, "type": "DATASET", "confidence": 0.8029033839702606}, {"text": "Wikipedia datasets", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.9218254387378693}]}, {"text": "We compare our model performance with those reported by and GRAPHSEG.", "labels": [], "entities": [{"text": "GRAPHSEG", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9910399317741394}]}, {"text": "In addition, we evaluate the performance of a random baseline model, which starts anew segment after every sentence with probability 1 k , where k is the average segment size in the dataset.", "labels": [], "entities": []}, {"text": "Because our test set is large, it is difficult to evaluate some of the existing methods, which are computationally demanding.", "labels": [], "entities": []}, {"text": "Thus, we introduce WIKI-50, a set of 50 randomly sampled test documents from WIKI-727K.", "labels": [], "entities": [{"text": "WIKI-50", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9524544477462769}, {"text": "WIKI-727K", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.9708778858184814}]}, {"text": "We use WIKI-50 to evaluate systems that are too slow to evaluate on the entire test set.", "labels": [], "entities": [{"text": "WIKI-50", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9380967020988464}]}, {"text": "We also provide human segmentation performance results on WIKI-50.", "labels": [], "entities": [{"text": "human segmentation", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.5379673838615417}, {"text": "WIKI-50", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9890243411064148}]}, {"text": "We use the P k metric as defined in to evaluate the performance of our model.", "labels": [], "entities": []}, {"text": "P k is the probability that when passing a sliding window of size k over sentences, the sentences at the boundaries of the window will be incorrectly classified as belonging to the same segment (or vice versa).", "labels": [], "entities": []}, {"text": "To match the setup of, we also provide the P k metric fora sliding window over words when evaluating on the datasets from their paper.", "labels": [], "entities": []}, {"text": "Following, we set k to half of the average segment size in the ground-truth segmentation.", "labels": [], "entities": []}, {"text": "For evaluations we used the SEGEVAL package.", "labels": [], "entities": [{"text": "SEGEVAL package", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.8054697513580322}]}, {"text": "In addition to segmentation accuracy, we also report runtime when running on a mid-range laptop CPU.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.973362922668457}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.954672634601593}]}, {"text": "We note that segmentation results are not always directly comparable.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9782821536064148}]}, {"text": "For example, require that all documents in the dataset discuss the same topic, and so their method is not directly applicable to WIKI-50.", "labels": [], "entities": [{"text": "WIKI-50", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.9617676734924316}]}, {"text": "Nevertheless, we attempt a comparison in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on various text segmentation datasets.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.6944276988506317}]}, {"text": " Table 2: P k Results on the test set.", "labels": [], "entities": []}]}