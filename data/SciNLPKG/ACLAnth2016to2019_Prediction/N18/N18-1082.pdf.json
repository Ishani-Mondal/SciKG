{"title": [{"text": "Embedding Syntax and Semantics of Prepositions via Tensor Decomposition", "labels": [], "entities": []}], "abstractContent": [{"text": "Prepositions are among the most frequent words in English and play complex roles in the syntax and semantics of sentences.", "labels": [], "entities": []}, {"text": "Not surprisingly, they pose well-known difficulties in automatic processing of sentences (prepo-sitional attachment ambiguities and idiosyn-cratic uses in phrases).", "labels": [], "entities": [{"text": "automatic processing of sentences", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.7276123464107513}]}, {"text": "Existing methods on preposition representation treat prepositions no different from content words (e.g., word2vec and GloVe).", "labels": [], "entities": [{"text": "preposition representation", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.8767763674259186}]}, {"text": "In addition, recent studies aiming at solving prepositional attachment and preposition selection problems depend heavily on external linguistic resources and use dataset-specific word representations.", "labels": [], "entities": [{"text": "preposition selection", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.6619327664375305}]}, {"text": "In this paper we use word-triple counts (one of the triples being a preposition) to capture a preposition's interaction with its attachment and complement.", "labels": [], "entities": []}, {"text": "We then derive preposition embeddings via tensor decomposition on a large unlabeled corpus.", "labels": [], "entities": []}, {"text": "We reveal anew geometry involving Hadamard products and empirically demonstrate its utility in paraphrasing phrasal verbs.", "labels": [], "entities": []}, {"text": "Furthermore, our preposition embeddings are used as simple features in two challenging downstream tasks: preposition selection and prepositional attachment disambiguation.", "labels": [], "entities": [{"text": "preposition selection", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.8357161283493042}, {"text": "prepositional attachment disambiguation", "start_pos": 131, "end_pos": 170, "type": "TASK", "confidence": 0.7763368487358093}]}, {"text": "We achieve results comparable to or better than the state-of-the-art on multiple standardized datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Prepositions area linguistically closed class comprising some of the most frequent words; they play an important role in the English language since they encode rich syntactic and semantic information.", "labels": [], "entities": []}, {"text": "Many preposition-related tasks are challenging in computational linguistics because of their polysemous nature and flexible usage patterns.", "labels": [], "entities": []}, {"text": "An accurate understanding and representation of prepositions' linguistic role is key to several important NLP tasks such as grammatical error correction and prepositional phrase attachment.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 124, "end_pos": 152, "type": "TASK", "confidence": 0.5567208727200826}, {"text": "prepositional phrase attachment", "start_pos": 157, "end_pos": 188, "type": "TASK", "confidence": 0.6764058073361715}]}, {"text": "A first-order approach is to represent prepositions as real-valued vectors via word embeddings such as word2vec () and GloVe).", "labels": [], "entities": []}, {"text": "Word embeddings have brought a renaissance in NLP research; they have been very successful in capturing word similarities as well as analogies (both syntactic and semantic) and are now mainstream in nearly all downstream NLP tasks (such as question-answering (.", "labels": [], "entities": []}, {"text": "Despite this success, available literature does not highlight any specific properties of word embeddings of prepositions.", "labels": [], "entities": []}, {"text": "Indeed, many of the common prepositions have very similar vector representations as shown in for preposition vectors trained using word2vec and GloVe (Tensor embedding is our proposed representation for prepositions).", "labels": [], "entities": [{"text": "GloVe", "start_pos": 144, "end_pos": 149, "type": "METRIC", "confidence": 0.8510828614234924}]}, {"text": "While this suggests that using available representations for prepositions diminishes the distinguishing aspect between prepositions, one could hypothesize that this is primarily because standard word embedding algorithms treat prepositions no different from other content words such as verbs and nouns, i.e., embeddings are created based on co-occurrences with other words.", "labels": [], "entities": []}, {"text": "However, prepositions are very frequent and cooccur with nearly all words, which means that their co-occurrence ought to be treated differently.", "labels": [], "entities": []}, {"text": "Modern descriptive linguistic theory proposes 896 to understand a preposition via its interactions with both the head it attaches to (termed head) and its complement).", "labels": [], "entities": []}, {"text": "This theory naturally suggests that one should count co-occurrences of a given preposition with pairs of neighboring words.", "labels": [], "entities": []}, {"text": "One way of achieving this would be by considering a tensor of triples (word 1 , word 2 , preposition), where we do not restrict word 1 and word 2 to be the head and complement words; instead we model a preposition's interaction with all pairs of neighboring words via a slice of a tensor X, where the slice is populated byword co-occurrences restricted to a context window of the specific preposition.", "labels": [], "entities": []}, {"text": "Thus, the tensor dimension is N \u00d7 N \u00d7 K where N is the vocabulary size and K is the number of prepositions; since K \u2248 50, we note that N K.", "labels": [], "entities": []}, {"text": "Using such a representation, we notice that the resulting tensor is low rank and use it to extract embeddings for both preposition and nonpreposition words.", "labels": [], "entities": []}, {"text": "In doing so, we use a combination of standard ideas from word representations (such as weighted spectral decomposition as in GloVe () and tensor decompositions (alternating least squares (ALS) methods).", "labels": [], "entities": []}, {"text": "We find that the preposition embeddings extracted in this manner are discriminative (see the preposition similarity of the tensor embedding in).", "labels": [], "entities": []}, {"text": "Note that the smaller the cosine similarity is, the more distinct the representations are from each other.", "labels": [], "entities": []}, {"text": "We demonstrate that the resulting preposition representation captures the core linguistic properties of prepositions-the attachment and the complement properties.", "labels": [], "entities": []}, {"text": "Using both intrinsic evaluations and downstream tasks, we show this by providing new state-of-the-art results on well-known NLP tasks involving prepositions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We show that the Hadamard product of the embeddings of a verb and a preposition that together make a phrasal verb, closely approximates the representation of this phrasal verb's paraphrase as a single verb.", "labels": [], "entities": []}, {"text": "Example: v made v from \u2248 v produced v, where represents the Hadamard product (i.e., elementwise multiplication) of two vectors and v is a constant vector (not associated with a specific word and is defined later); this approximation validates that prepositional semantics are appropriately encoded into their trained embeddings.", "labels": [], "entities": []}, {"text": "We provide a mathematical interpretation for this new geometry while empirically demonstrating the paraphrasing of compositional phrasal verbs.", "labels": [], "entities": []}, {"text": "Extrinsic evaluations: Our preposition embeddings are used as features fora simple classifier in two well-known challenging downstream NLP classification tasks.", "labels": [], "entities": [{"text": "NLP classification", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7894724905490875}]}, {"text": "In both tasks, we perform as well as or strictly better than the state-of-the-art on multiple standardized datasets.", "labels": [], "entities": []}, {"text": "Preposition selection: While the context in which a preposition occurs governs the choice of the preposition, the specific preposition by itself significantly influences the semantics of the context in which it occurs.", "labels": [], "entities": [{"text": "Preposition selection", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8299919068813324}]}, {"text": "Furthermore, the choice of the right preposition fora given context can be very subtle.", "labels": [], "entities": []}, {"text": "This idiosyncratic behavior of prepositions is the reason behind preposition errors being one of the most frequent error types made by second language English speakers ().", "labels": [], "entities": []}, {"text": "We demonstrate the utility of the preposition embeddings in the preposition selection task, which is to choose the correct preposition to a given sentence.", "labels": [], "entities": []}, {"text": "We show this fora large set of contexts-7, 000 combined instances from the CoNLL-2013 and the SE datasets ().", "labels": [], "entities": [{"text": "CoNLL-2013", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.9593695998191833}, {"text": "SE datasets", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.8435306251049042}]}, {"text": "Our approach achieves 6% and 2% absolute improvement over the previous state-of-the-art results on the respective datasets.", "labels": [], "entities": [{"text": "absolute", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9725406765937805}]}], "tableCaptions": [{"text": " Table 1: Cosine similarity between pairs of centered  prepositions using some word embeddings", "labels": [], "entities": []}, {"text": " Table 2: Paraphrasing of prepositional phrases.", "labels": [], "entities": [{"text": "Paraphrasing of prepositional phrases", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.8182494938373566}]}, {"text": " Table 4: Performance on preposition selection.", "labels": [], "entities": [{"text": "preposition selection", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.8414252698421478}]}, {"text": " Table 5: Ablation analysis in preposition selection.", "labels": [], "entities": [{"text": "Ablation analysis", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8594111204147339}, {"text": "preposition selection", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.8243581056594849}]}, {"text": " Table 6: Accuracy in prepositional attachment disambiguation.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9944091439247131}, {"text": "prepositional attachment disambiguation", "start_pos": 22, "end_pos": 61, "type": "TASK", "confidence": 0.7721279760201772}]}, {"text": " Table 7: Ablation analysis in preposition attachment disambiguation.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.970963180065155}, {"text": "preposition attachment disambiguation", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.8423279523849487}]}]}