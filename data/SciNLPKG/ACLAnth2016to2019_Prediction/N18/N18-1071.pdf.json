{"title": [{"text": "Collective Entity Disambiguation with Structured Gradient Tree Boosting", "labels": [], "entities": [{"text": "Collective Entity Disambiguation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5270004769166311}]}], "abstractContent": [{"text": "We present a gradient-tree-boosting-based structured learning model for jointly disam-biguating named entities in a document.", "labels": [], "entities": []}, {"text": "Gradient tree boosting is a widely used machine learning algorithm that underlies many top-performing natural language processing systems.", "labels": [], "entities": [{"text": "Gradient tree boosting", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.576728900273641}]}, {"text": "Surprisingly, most works limit the use of gradient tree boosting as a tool for regular classification or regression problems, despite the structured nature of language.", "labels": [], "entities": [{"text": "gradient tree boosting", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7070708473523458}]}, {"text": "To the best of our knowledge, our work is the first one that employs the structured gradient tree boosting (SGTB) algorithm for collective entity disambiguation.", "labels": [], "entities": [{"text": "structured gradient tree boosting (SGTB)", "start_pos": 73, "end_pos": 113, "type": "TASK", "confidence": 0.7273116196904864}, {"text": "collective entity disambiguation", "start_pos": 128, "end_pos": 160, "type": "TASK", "confidence": 0.6722894112269083}]}, {"text": "By defining global features over previous disambiguation decisions and jointly modeling them with local features, our system is able to produce globally optimized entity assignments for mentions in a document.", "labels": [], "entities": []}, {"text": "Exact inference is prohibitively expensive for our globally normalized model.", "labels": [], "entities": []}, {"text": "To solve this problem, we propose Bidirectional Beam Search with Gold path (BiBSG), an approximate inference algorithm that is a variant of the standard beam search algorithm.", "labels": [], "entities": []}, {"text": "BiBSG makes use of global information from both past and future to perform better local search.", "labels": [], "entities": [{"text": "BiBSG", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9596219062805176}]}, {"text": "Experiments on standard benchmark datasets show that SGTB significantly improves upon published results.", "labels": [], "entities": [{"text": "SGTB", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.9421533942222595}]}, {"text": "Specifically, SGTB outper-forms the previous state-of-the-art neural system by near 1% absolute accuracy on the popular AIDA-CoNLL dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.916814386844635}, {"text": "AIDA-CoNLL dataset", "start_pos": 120, "end_pos": 138, "type": "DATASET", "confidence": 0.9538227617740631}]}], "introductionContent": [{"text": "Entity disambiguation (ED) refers to the process of linking an entity mention in a document to its corresponding entity record in a reference knowledge base (e.g., Wikipedia or Freebase).", "labels": [], "entities": [{"text": "Entity disambiguation (ED)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8337461173534393}]}, {"text": "As a core information extraction task, ED plays an important role in the language understanding pipeline, underlying a variety of downstream applications such as relation extraction, knowledge base population (, and question answering.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7233342677354813}, {"text": "language understanding", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.7494536936283112}, {"text": "relation extraction", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.9129215180873871}, {"text": "question answering", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.9051289260387421}]}, {"text": "This task is challenging because of the inherent ambiguity between mentions and the referred entities.", "labels": [], "entities": []}, {"text": "Consider, for example, the mention 'Washington', which can be linked to a city, a state, a person, an university, or a lake (.", "labels": [], "entities": []}, {"text": "Fortunately, simple and effective features have been proposed to capture the ambiguity that are designed to model the similarity between a mention (and its local context) and a candidate entity, as well as the relatedness between entities that co-occur in a single document.", "labels": [], "entities": []}, {"text": "These are typically statistical features estimated from entitylinked corpora, and similarity features that are pre-computed using distance metrics such as cosine.", "labels": [], "entities": []}, {"text": "For example, a key feature for ED is the prior probability of an entity given a specific mention, which is estimated from mention-entity cooccurrence statistics.", "labels": [], "entities": [{"text": "ED", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9289118647575378}]}, {"text": "This simple feature alone can yield 70% to 80% accuracy on both news and Twitter texts (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.998814582824707}]}, {"text": "To capture the non-linear relationships between the low-dimensional dense features like statistical features, sophisticated machine learning models such as neural networks and gradient tree boosting are preferred over linear models.", "labels": [], "entities": []}, {"text": "In particular, gradient tree boosting has been shown to be highly competitive for ED in recent work (.", "labels": [], "entities": [{"text": "gradient tree boosting", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.6698235174020132}, {"text": "ED", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.8637847900390625}]}, {"text": "However, although achieving appealing results, existing gradient-tree-boosting-based ED systems typically operate on each individual mention, without attempting to jointly resolve entity mentions in a document together.", "labels": [], "entities": []}, {"text": "Joint entity disambiguation has been shown to significantly boost performance when used in conjunction with other machine learning techniques).", "labels": [], "entities": [{"text": "Joint entity disambiguation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.709916333357493}]}, {"text": "However, how to train a global gradient tree boosting model that produces coherent entity assignments for all the mentions in a document is still an open question.", "labels": [], "entities": [{"text": "global gradient tree boosting", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.6370646357536316}]}, {"text": "In this work, we present, to the best of our knowledge, the first structured gradient tree boosting (SGTB) model for collective entity disambiguation.", "labels": [], "entities": [{"text": "structured gradient tree boosting (SGTB)", "start_pos": 66, "end_pos": 106, "type": "TASK", "confidence": 0.7603064264569964}, {"text": "collective entity disambiguation", "start_pos": 117, "end_pos": 149, "type": "TASK", "confidence": 0.6685805916786194}]}, {"text": "Building on the general SGTB framework introduced by, we develop a globally normalized model for ED that employs a conditional random field (CRF) objective ().", "labels": [], "entities": []}, {"text": "The model permits the utilization of global features defined between the current entity candidate and the entire decision history for previous entity assignments, which enables the global optimization for all the entity mentions in a document.", "labels": [], "entities": []}, {"text": "As discussed in prior work, globally normalized models are more expressive than locally normalized models.", "labels": [], "entities": []}, {"text": "As in many other global models, our SGTB model suffers from the difficulty of computing the partition function (normalization term) for training and inference.", "labels": [], "entities": []}, {"text": "We adopt beam search to address this problem, in which we keep track of multiple hypotheses and sum over the paths in the beam.", "labels": [], "entities": [{"text": "beam search", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.8693915009498596}]}, {"text": "In particular, we propose Bidirectional Beam Search with Gold path (BiBSG) technique that is specifically designed for SGTB model training.", "labels": [], "entities": [{"text": "Beam Search", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.7049261927604675}, {"text": "SGTB model training", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.9021895329157511}]}, {"text": "Compared to standard beam search strategies, BiBSG reduces model variance and also enjoys the advantage in its ability to consider both past and future information when predicting an output.", "labels": [], "entities": [{"text": "BiBSG", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.6561269164085388}]}, {"text": "Our contributions are: \u2022 We propose a SGTB model for collectively disambiguating entities in a document.", "labels": [], "entities": [{"text": "SGTB", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.9657331109046936}]}, {"text": "By jointly modeling local decisions and global structure, SGTB is able to produce globally optimal entity assignments for all the mentions.", "labels": [], "entities": [{"text": "SGTB", "start_pos": 58, "end_pos": 62, "type": "TASK", "confidence": 0.7562475800514221}]}, {"text": "\u2022 We present BiBSG, an efficient algorithm for approximate bidirectional inference.", "labels": [], "entities": [{"text": "BiBSG", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.8517024517059326}]}, {"text": "The algorithm is tailored to SGTB models, which can reduce model variance by generating more point-wise functional gradients for estimating the auxiliary regression models.", "labels": [], "entities": []}, {"text": "\u2022 SGTB achieves state-of-the-art (SOTA) results on various popular ED datasets, and it outperforms the previous SOTA systems by 1-2% absolute accuracy on the AIDACoNLL (Hoffart et al., 2011) dataset.", "labels": [], "entities": [{"text": "ED datasets", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.7649014890193939}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9504750967025757}, {"text": "AIDACoNLL (Hoffart et al., 2011) dataset", "start_pos": 158, "end_pos": 198, "type": "DATASET", "confidence": 0.8697288897302415}]}], "datasetContent": [{"text": "In this section, we evaluate SGTB on some of the most popular datasets for ED.", "labels": [], "entities": [{"text": "SGTB", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.7818538546562195}, {"text": "ED", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.8169941306114197}]}, {"text": "After describing the experimental setup, we compare SGTB with previous state-of-the-art (SOTA) ED systems and present our main findings in \u00a7 5.3.", "labels": [], "entities": []}, {"text": "Following previous work, we evaluate our models on both in-domain and cross-domain testing settings.", "labels": [], "entities": []}, {"text": "In particular, we train our models on AIDA-train set, tune hyperparameters on AIDAdev set, and test on AIDA-test set (in-domain testing) and all other datasets (cross-domain testing).", "labels": [], "entities": [{"text": "AIDA-train set", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.8495796322822571}, {"text": "AIDAdev set", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8943714201450348}, {"text": "AIDA-test set", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.8721480369567871}]}, {"text": "We follow prior work and report in-KB accuracies for AIDA-test and Bag-of-Title (BoT) F1 scores for the other test sets.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9542188048362732}, {"text": "AIDA-test", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.6424263715744019}, {"text": "Bag-of-Title (BoT) F1 scores", "start_pos": 67, "end_pos": 95, "type": "METRIC", "confidence": 0.7395982245604197}]}, {"text": "Two AIDA-CoNLL specific resources have been widely used in previous work.", "labels": [], "entities": []}, {"text": "In order to have fair comparisons with these works, we also adopt them only for the AIDA datasets.", "labels": [], "entities": [{"text": "AIDA datasets", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9700894951820374}]}, {"text": "First, we use a mention prior obtained from aliases to candidate entities released by along with the two priors described in \u00a7 4.1.", "labels": [], "entities": []}, {"text": "Second, we also experiment with PPRforNED, an entity candidate selection system released by.", "labels": [], "entities": []}, {"text": "It is unclear how candidates were pruned, but the entity candidates generated by this system have high recall and low ambiguity, and they contribute to some of the best results reported for AIDA-test).", "labels": [], "entities": [{"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9993699193000793}, {"text": "AIDA-test", "start_pos": 190, "end_pos": 199, "type": "DATASET", "confidence": 0.652947187423706}]}, {"text": "Competitive systems We implement four competitive ED systems, and three of them are based on variants of our proposed SGTB algorithm.", "labels": [], "entities": []}, {"text": "8 Gradient tree boosting is a local model that employs only local features to make independent decisions for every entity mention.", "labels": [], "entities": [{"text": "8 Gradient tree boosting", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.56911201775074}]}, {"text": "Note that our local model is different from that presented by, where they treat ED as binary classification for each mention-entity pair.", "labels": [], "entities": []}, {"text": "SGTB-BS is a Structured Gradient Tree Boosting model trained with Beam Search with early update strategy.", "labels": [], "entities": [{"text": "SGTB-BS", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7510125637054443}]}, {"text": "SGTB-BSG uses Beam Search with Gold path training strategy presented in \u00a7 3.1.", "labels": [], "entities": [{"text": "SGTB-BSG", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7714034914970398}]}, {"text": "Finally, SGTBBiBSG exploits Bidirectional Beam Search with Gold path to leverage information from both past and future for better local search.", "labels": [], "entities": []}, {"text": "In addition, we compare against best published results on all the datasets.", "labels": [], "entities": []}, {"text": "To ensure fair comparisons, we group results according to candidate selection system that different ED systems adopted.", "labels": [], "entities": []}, {"text": "Parameter tuning We tune all the hyperparameters on the AIDA-dev set.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9384249448776245}, {"text": "AIDA-dev set", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9598997831344604}]}, {"text": "We use recommended hyperparameter values from scikit-learn to train regression trees, except for the maximum depth of the tree, which we choose from {3, 5, 8}.", "labels": [], "entities": []}, {"text": "After a set of preliminary experiments, we select the beam size from {3, 4, 5, 6}.", "labels": [], "entities": []}, {"text": "The best values for the two hyperparameters are 3 and 4 respectively.", "labels": [], "entities": []}, {"text": "As mentioned in \u00a7 2, the learning rate is set to 1.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.9425545632839203}]}, {"text": "We train SGTB for at most 500 epochs (i.e., fit at most 500 regression trees).", "labels": [], "entities": []}, {"text": "During training, we check the performance on the development set every 25 epochs to perform early stopping.", "labels": [], "entities": []}, {"text": "Training takes 3 hours for SGTB-BS and SGTB-BSG, and takes 9 hours for SGTB-BiBSG on 16 threads.", "labels": [], "entities": [{"text": "SGTB-BS", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.7831757664680481}, {"text": "SGTB-BSG", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.8827643990516663}]}], "tableCaptions": [{"text": " Table 1: Statistics of the ED datasets used in this work.", "labels": [], "entities": [{"text": "ED datasets", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.9117375910282135}]}, {"text": " Table 2: In-domain evaluation: in-KB accuracy results  on the AIDA-test set. Checked PPRforNED indicates  that the system uses PPRforNED (Pershina et al., 2015)  to select candidate entities.The best results are in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.996440589427948}, {"text": "AIDA-test set", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9240715205669403}]}, {"text": " Table 3: Cross-domain evaluation: Bag-of-Title (BoT) F1 results on ED datasets. The best results are in bold.", "labels": [], "entities": [{"text": "Bag-of-Title (BoT)", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.4502175450325012}, {"text": "F1", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.7311981320381165}, {"text": "ED datasets", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.8626849055290222}]}]}