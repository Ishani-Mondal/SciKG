{"title": [], "abstractContent": [{"text": "Automatic colorization is the process of adding color to greyscale images.", "labels": [], "entities": []}, {"text": "We condition this process on language, allowing end users to manipulate a colorized image by feeding in different captions.", "labels": [], "entities": []}, {"text": "We present two different architectures for language-conditioned colorization, both of which produce more accurate and plausible colorizations than a language-agnostic version.", "labels": [], "entities": []}, {"text": "Through this language-based framework, we can dramatically alter colorizations by manipulating descriptive color words in captions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic image colorization (-the process of adding color to a greyscale image-is inherently underspecified.", "labels": [], "entities": [{"text": "image colorization", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6873451769351959}]}, {"text": "Unlike background scenery such as sky or grass, many common foreground objects could plausibly be of any color, such as a person's clothing, a bird's feathers, or the exterior of a car.", "labels": [], "entities": []}, {"text": "Interactive colorization seeks human input, usually in the form of clicks or strokes on the image with a selected color, to reduce these ambiguities ().", "labels": [], "entities": [{"text": "Interactive colorization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7177151143550873}]}, {"text": "We introduce the task of colorization from natural language, a previously unexplored source of color specifications.", "labels": [], "entities": []}, {"text": "Many use cases for automatic colorization involve images paired with language.", "labels": [], "entities": []}, {"text": "For example, comic book artwork is normally first sketched in black-and-white by a penciller; afterwards, a colorist selects a palette that thematically reinforces the written script to produce the final colorized art.", "labels": [], "entities": []}, {"text": "Similarly, older black-and-white films are often colorized for modern audiences based on cues from dialogue and narration).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate FCNN, CONCAT, and FILM using accuracy in ab space (shown by to be a poor substitute for plausibility) and with crowdsourced experiments that ask workers to judge colorization plausibility, quality, and the colorization flexibly reflects language manipulations.", "labels": [], "entities": [{"text": "FCNN", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.8365936875343323}, {"text": "FILM", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9343141913414001}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9988590478897095}]}, {"text": "summarizes our results; while there is no clear winner between FILM and CONCAT, both rely on language to produce higher-quality colorizations than those generated by FCNN.", "labels": [], "entities": [{"text": "FILM", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.68117356300354}, {"text": "FCNN", "start_pos": 166, "end_pos": 170, "type": "DATASET", "confidence": 0.9383107423782349}]}, {"text": "We train all of our models on the 82,783 images in the  We run three human evaluations of our models on the Crowdflower platform to evaluate their plausibility, overall quality, and how well they condition their output on language.", "labels": [], "entities": [{"text": "Crowdflower platform", "start_pos": 108, "end_pos": 128, "type": "DATASET", "confidence": 0.9393678307533264}]}, {"text": "Each evaluation is run using a random subset of 100 caption/image pairs from the MSCOCO validation set, and we obtain five judgments per pair.", "labels": [], "entities": [{"text": "MSCOCO validation set", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.8670584559440613}]}, {"text": "Plausibility given caption: We show workers a caption along with three images generated by FCNN, CONCAT, and FILM.", "labels": [], "entities": [{"text": "FCNN", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.9520133137702942}, {"text": "FILM", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.851576566696167}]}, {"text": "They choose the image that best depicts the caption; if multiple images accurately depict the caption, we ask them to choose the most realistic.", "labels": [], "entities": []}, {"text": "FCNN does not receive the caption as input, so it makes sense that its output is only chosen 20% of the time; there is no significant difference between CONCAT and FILM in plausibility given the caption.", "labels": [], "entities": [{"text": "FCNN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9663699269294739}, {"text": "FILM", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.976767361164093}]}, {"text": "Colorization quality: Workers receive a pair of images, a ground-truth MSCOCO image and a generated output from one of our three architectures,: The top row contains successes from our caption manipulation task generated by FILM and CONCAT, respectively.", "labels": [], "entities": [{"text": "caption manipulation task", "start_pos": 185, "end_pos": 210, "type": "TASK", "confidence": 0.8631620009740194}, {"text": "FILM", "start_pos": 224, "end_pos": 228, "type": "METRIC", "confidence": 0.5988720655441284}]}, {"text": "The second row shows examples of how captions guide FILM to produce more accurate colorizations than FCNN (failure cases outlined in red).", "labels": [], "entities": [{"text": "FILM", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.5592244267463684}, {"text": "FCNN", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.7931881546974182}]}, {"text": "The final row contains, from left to right, particularly eye-catching colorizations from both CONCAT and FILM, a case where FILM fails to localize properly, and an image whose unnatural caption causes artifacts in CONCAT. and are asked to choose the image that was not colored by a computer.", "labels": [], "entities": [{"text": "FILM", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.5416858792304993}]}, {"text": "The goal is to fool workers into selecting the generated images; the \"fooling rates\" for all three architectures are comparable, which indicates that we do not reduce colorization quality by conditioning on language.", "labels": [], "entities": []}, {"text": "Caption manipulation: Our last evaluation measures how much influence the caption has on the CONCAT and FILM models.", "labels": [], "entities": [{"text": "Caption manipulation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.950524091720581}, {"text": "CONCAT", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.7148611545562744}, {"text": "FILM", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.789162814617157}]}, {"text": "We generate three different colorizations of a single image by swapping out different colors in the caption (e.g., blue car, red car, green car).", "labels": [], "entities": []}, {"text": "Then, we provide workers with a single caption (e.g., green car) and ask them to choose which image best depicts the caption.", "labels": [], "entities": []}, {"text": "If our models cannot localize and color the appropriate object, workers will be unable to select an appropriate image.", "labels": [], "entities": []}, {"text": "Fortunately, CONCAT and FILM are both robust to caption manipulations).", "labels": [], "entities": [{"text": "CONCAT", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.8956298232078552}, {"text": "FILM", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9698772430419922}, {"text": "caption manipulations", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.9155417084693909}]}], "tableCaptions": []}