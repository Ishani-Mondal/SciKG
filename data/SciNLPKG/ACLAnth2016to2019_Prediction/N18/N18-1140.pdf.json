{"title": [{"text": "CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension *", "labels": [], "entities": [{"text": "CliCR: A Dataset of Clinical Case Reports", "start_pos": 0, "end_pos": 41, "type": "DATASET", "confidence": 0.8562492951750755}, {"text": "Machine Reading Comprehension", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.6718053023020426}]}], "abstractContent": [{"text": "We present anew dataset for machine comprehension in the medical domain.", "labels": [], "entities": []}, {"text": "Our dataset uses clinical case reports with around 100,000 gap-filling queries about these cases.", "labels": [], "entities": []}, {"text": "We apply several baselines and state-of-the-art neural readers to the dataset, and observe a considerable gap in performance (20% F1) between the best human and machine readers.", "labels": [], "entities": [{"text": "F1", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.9990541338920593}]}, {"text": "We analyze the skills required for successful answering and show how reader performance varies depending on the applicable skills.", "labels": [], "entities": [{"text": "answering", "start_pos": 46, "end_pos": 55, "type": "TASK", "confidence": 0.7778167128562927}]}, {"text": "We find that inferences using domain knowledge and object tracking are the most frequently required skills, and that recognizing omitted information and spatio-temporal reasoning are the most difficult for the machines.", "labels": [], "entities": [{"text": "object tracking", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7263538390398026}, {"text": "recognizing omitted information", "start_pos": 117, "end_pos": 148, "type": "TASK", "confidence": 0.8652900258700053}, {"text": "spatio-temporal reasoning", "start_pos": 153, "end_pos": 178, "type": "TASK", "confidence": 0.7031574845314026}]}], "introductionContent": [{"text": "Machine comprehension is a task in which a system reads a text passage and then answers questions about it.", "labels": [], "entities": [{"text": "Machine comprehension", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7622849643230438}]}, {"text": "The progress in machine comprehension heavily depends on the introduction of new datasets , which encourages the development of new algorithms and deepens our understanding of the (linguistic) challenges that can or cannot be tackled well by these algorithms.", "labels": [], "entities": []}, {"text": "Recently, a number of reading comprehension datasets have been proposed ( \u00a7 2), differing in various aspects such as mode of construction, answer-query formulation and required understanding skills.", "labels": [], "entities": [{"text": "answer-query formulation", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.7770863771438599}]}, {"text": "Most are open-domain datasets built from news, fiction and Wikipedia texts.", "labels": [], "entities": []}, {"text": "For specialized domains, however, large machine comprehension datasets are extremely scarce (), and * We provide the information about accessing the dataset, as well as the code for the experiments, at http://github.", "labels": [], "entities": []}, {"text": "com/clips/clicr.", "labels": [], "entities": []}], "datasetContent": [{"text": "Numerous general-domain datasets have been recently created to allow machine comprehension using data-intensive methods.", "labels": [], "entities": []}, {"text": "These datasets were collected from Wikipedia (, in which queries are created by removing a word or a named entity from the running text in a book; and, who similarly to us blank out entities in abstractive CNN and Daily Mail summaries, but who are only concerned with short proper nouns and short passages.", "labels": [], "entities": [{"text": "CNN and Daily Mail summaries", "start_pos": 206, "end_pos": 234, "type": "DATASET", "confidence": 0.724366158246994}]}, {"text": "Who-did-what () requires the reader to select the person name from a short candidate list that best answers the query about a news event.", "labels": [], "entities": []}, {"text": "They do not use summaries for query formation but remove a named entity from the initial sentence in a news article, and then perform information retrieval to find independent passages relevant to the query.", "labels": [], "entities": [{"text": "query formation", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7941826581954956}]}, {"text": "Another cloze dataset for language understanding is ROCStories (), but it is targeted more towards script knowledge evaluation, and only contains five-sentence stories.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7458736300468445}, {"text": "script knowledge evaluation", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.6737382610638937}]}, {"text": "Another related task is predicting rare entities only, with a focus on improving a reading comprehension system with external knowledge sources (.", "labels": [], "entities": [{"text": "predicting rare entities", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.9167307615280151}]}, {"text": "Another popular way of creating datasets for reading comprehension is crowdsourcing).", "labels": [], "entities": []}, {"text": "These datasets exist primarily for the general domain; for specialized domains where background knowledge is crucial, crowdsourcing is intuitively less suitable (), although some positive precedent exists for example in crowdsourcing annotations of radiology reports ().", "labels": [], "entities": []}, {"text": "Compared to automated dataset construction, crowdsourcing is more likely to provide highquality queries and answers.", "labels": [], "entities": []}, {"text": "On the other hand, human question generation may also lead to less varied datasets as questions would tend to be of wh-type; for cloze datasets, the questions maybe more varied and might require readers to possess a different set of skills.", "labels": [], "entities": [{"text": "question generation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7025697827339172}]}, {"text": "We collected the articles from BMJ Case Reports 2 . The data span the years 2005-2016 and amount to almost 12 thousand reports.", "labels": [], "entities": [{"text": "BMJ Case Reports 2", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.9435315579175949}]}, {"text": "We removed the HTML boilerplate from the crawled reports using jusText , segmented and tokenized the texts with cTakes (, and annotated the medical entities using Clamp (.", "labels": [], "entities": []}, {"text": "We apply two simple heuristics to refine the recognized entities and to decrease their sparsity.", "labels": [], "entities": []}, {"text": "Namely, we move the function words (determiners and pronouns) from the beginning of the entity outside of it, and we adjust the entity boundary so that it does not include a parenthetical at the end of the entity.", "labels": [], "entities": []}, {"text": "Clamp assigns entities following the i2b2-2010 shared task specifications).", "labels": [], "entities": []}, {"text": "For each entity, a concept unique identifier (CUI) is also available, which links it to the UMLS R Metathesaurus R ().", "labels": [], "entities": [{"text": "UMLS R Metathesaurus R", "start_pos": 92, "end_pos": 114, "type": "DATASET", "confidence": 0.9132926911115646}]}, {"text": "To check the quality of the recognized entities, we carried out a small manual analysis on 250 entities.", "labels": [], "entities": []}, {"text": "We found that in 89% of cases, the boundaries were correct and defined a true entity.", "labels": [], "entities": []}, {"text": "Wrongly recognized cases occurred mostly when two entities were coordinated and recognized as one; when a verb was wrongly included in the entity; or when a pre-modifier was left out.", "labels": [], "entities": []}, {"text": "We now describe the dataset in more detail, starting with the general statistics summarized in.", "labels": [], "entities": []}, {"text": "It is worth pointing out that the support passages are rather long, which stems from the data origin (journal articles).", "labels": [], "entities": []}, {"text": "We show the passage length distribution in, which has the average length of 1,466 tokens.", "labels": [], "entities": []}, {"text": "Furthermore, passages are rich with medical entities.", "labels": [], "entities": []}, {"text": "There is little repetition of answers-the total of around 100,000 queries are answered by 50,000 distinct entities.", "labels": [], "entities": [{"text": "repetition", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.948493480682373}]}, {"text": "Upon extending the answer set with UMLS R we introduce on average four alternative answers for each original one.", "labels": [], "entities": []}, {"text": "In 59% of instances, the answer entity is found verbatim in the relevant passage.", "labels": [], "entities": []}, {"text": "The answers can belong to any of the problem, treatment or test categories, and usually consist of multiple words).", "labels": [], "entities": []}, {"text": "The diversity of medical specialties represented in the articles is shown in.", "labels": [], "entities": []}, {"text": "A model f takes as input a passage-query pair and outputs an answer\u00e2answer\u02c6answer\u00e2.", "labels": [], "entities": []}, {"text": "We carryout the evaluation We assume the candidate entities are known in advance.", "labels": [], "entities": []}, {"text": "In our case, the answer is a word or a word phrase representing a medical entity.", "labels": [], "entities": []}, {"text": "Alternatively, one could also take the UMLS R CUI identifier as the answering unit.", "labels": [], "entities": [{"text": "UMLS R CUI identifier", "start_pos": 39, "end_pos": 60, "type": "DATASET", "confidence": 0.8672935515642166}]}, {"text": "However, in that case, it would mean that sometimes the original word phrase is lost.", "labels": [], "entities": []}, {"text": "This is because entity linking with CUIs can be noisy, and only apart of a word phrase maybe linked to the ontology.", "labels": [], "entities": []}, {"text": "In the current setup, we are able to keep both the original word phrase as well as the extended answers.", "labels": [], "entities": []}, {"text": "The CUI information is still an integral part of the answer field in our dataset, so it can be used by other researchers if preferred. with different metrics described below.", "labels": [], "entities": []}, {"text": "The final score m fora metric v is obtained by averaging over the test set: (4) Since there are multiple correct answers A, we take the highest scoring answer\u00e2answer\u02c6answer\u00e2 at each instance, as done in.", "labels": [], "entities": []}, {"text": "Note that in the dataset we do not supply the candidate answers; in the experiments, we constrain the candidates to the set of entities in the passage.", "labels": [], "entities": []}, {"text": "The two standardly used metrics for machine comprehension evaluation are the exact match (EM) and the F1 score.", "labels": [], "entities": [{"text": "machine comprehension evaluation", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.7174810568491617}, {"text": "exact match (EM)", "start_pos": 77, "end_pos": 93, "type": "METRIC", "confidence": 0.9739391803741455}, {"text": "F1 score", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9856486618518829}]}, {"text": "For EM, the predicted and the ground truth answers must match precisely, safe for articles, punctuation and case distinction (same for other metrics).", "labels": [], "entities": [{"text": "EM", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8574241399765015}]}, {"text": "F1 metric is applied per instance and measures the overlap between the prediction\u00e2prediction\u02c6prediction\u00e2 and the ground truth a, which are treated as bags of words.", "labels": [], "entities": [{"text": "F1 metric", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9662759602069855}]}, {"text": "While these two metrics are arguably sufficient in news-style machine comprehension where the entities are proper nouns which allow for little variation and synonymy, in our case the medical entities are often mostly common nouns modified by specifiers and qualifiers.", "labels": [], "entities": []}, {"text": "To take into account potentially large lexical and word-order variation, we use two additional metrics.", "labels": [], "entities": []}, {"text": "First, we measure BLEU () for n-grams of length 2 (shortly, B2) and 4 (B4) using the package by, with which we aim to capture contiguity of tokens in longer answers.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9991175532341003}]}, {"text": "Second, it may occur that answers contain no word overlap yet still be good candidates because of their semantical relatedness, as in \"renal failure\"-\"kidney breakdown\".", "labels": [], "entities": []}, {"text": "We take this into account by using an embedding metric (Emb), in which we construct mean vectors for both ground-truth and system answer sequences, and then compare them with the cosine similarity.", "labels": [], "entities": []}, {"text": "This and other embedding metrics for evaluation were previously studied in dialog-system research (.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data statistics based on the lowercased dataset.  For N of tokens in passages, we count each passage ex- actly once, although several queries are normally asso- ciated with a passage.", "labels": [], "entities": []}, {"text": " Table 3: Answer type statistics.", "labels": [], "entities": [{"text": "Answer type", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9262514114379883}]}, {"text": " Table 4: Answering results on the test set. EM and F1  scores are percentages. The human scores (in italics)  are based on the validation set.", "labels": [], "entities": [{"text": "Answering", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9807372093200684}, {"text": "EM", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9821558594703674}, {"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9795390367507935}]}]}