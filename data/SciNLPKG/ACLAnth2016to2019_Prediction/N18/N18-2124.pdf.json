{"title": [{"text": "Visually Guided Spatial Relation Extraction from Text", "labels": [], "entities": [{"text": "Spatial Relation Extraction", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.8259896636009216}]}], "abstractContent": [{"text": "Extraction of spatial relations from sentences with complex/nesting relationships is very challenging as often needs resolving inherent semantic ambiguities.", "labels": [], "entities": [{"text": "Extraction of spatial relations from sentences with complex/nesting relationships", "start_pos": 0, "end_pos": 81, "type": "TASK", "confidence": 0.8709371469237588}]}, {"text": "We seek help from visual modality to fill the information gap in the text modality and resolve spatial semantic ambiguities.", "labels": [], "entities": []}, {"text": "We use various recent vision and language datasets and techniques to train inter-modality alignment models, visual relationship classifiers and propose a novel global inference model to integrate these components into our structured output prediction model for spatial role and relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 278, "end_pos": 297, "type": "TASK", "confidence": 0.7517134547233582}]}, {"text": "Our global inference model enables us to utilize the visual and geometric relationships between objects and improves the state-of-art results of spatial information extraction from text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Significant progress has been made in spatial language understanding by mapping natural language text to spatial ontologies ( . The research results show that spatial entities can be extracted with a good accuracy, however, spatial relation extraction is still challenging (.", "labels": [], "entities": [{"text": "spatial language understanding", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.7139106591542562}, {"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.991645872592926}, {"text": "spatial relation extraction", "start_pos": 224, "end_pos": 251, "type": "TASK", "confidence": 0.6346797148386637}]}, {"text": "Particularly, when the sentences convey more than one relationship, finding the right links between the spatial objects and spatial prepositions becomes difficult.", "labels": [], "entities": []}, {"text": "For example, the spatial meaning of There is a car in front of the house on the left, can be interpreted in different ways: (A car in front of the house) on the left or A car in front of (the house on the left).", "labels": [], "entities": []}, {"text": "This issue is related to the well-known prepositional phrase attachments (pp-attachments) syntactic ambiguity which is problematic for our goal of spatial semantic extraction too.", "labels": [], "entities": [{"text": "spatial semantic extraction", "start_pos": 147, "end_pos": 174, "type": "TASK", "confidence": 0.7028425137201945}]}, {"text": "The previous research shows some of these ambiguities can be resolved by simultaneously reasoning from the associated image.", "labels": [], "entities": []}, {"text": "Consider the scene in, we can easily resolve the ambiguity and choose the correct interpretation with the help of the associated image.", "labels": [], "entities": []}, {"text": "Although we do not directly tackle the task of pp-attachment here, resolving this issue will help our task to find the accurate link between the spatial prepositions (i.e. spatial indicators) and spatial objects/roles (trajector and landmark).", "labels": [], "entities": []}, {"text": "The spatial semantic links can go beyond the syntactic links/attachments, therefore merely fixing the preposition attachments is not sufficient for our task.", "labels": [], "entities": []}, {"text": "We exploit the image to find the right preposition that describes the relationships between the spatial roles, for example on the left can be a relationship between the house and implicit landmark picture as well as a car and implicit landmark picture.", "labels": [], "entities": []}, {"text": "There are many recent works on combining vision and language for domains such as image captioning, visual image retrieval (, visual question answering (, activity recognition, visual relation extraction () and object localization (.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.7219855636358261}, {"text": "visual image retrieval", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.607216089963913}, {"text": "visual question answering", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.6051187415917715}, {"text": "activity recognition", "start_pos": 154, "end_pos": 174, "type": "TASK", "confidence": 0.7220218330621719}, {"text": "visual relation extraction", "start_pos": 176, "end_pos": 202, "type": "TASK", "confidence": 0.6029190818468729}]}, {"text": "We aim at exploiting models from visual modality to boost the models trained by the text modality and improve spatial role labeling task (SpRL)).", "labels": [], "entities": [{"text": "spatial role labeling task (SpRL))", "start_pos": 110, "end_pos": 144, "type": "TASK", "confidence": 0.5943104411874499}]}, {"text": "The most related work to ours is () in which they connected phrases to ground-truth labeled segments using word embedding similarity to generate additional visual features, whereas, in this work, we train actual inter-modality alignment models to include visual information in our model.", "labels": [], "entities": []}, {"text": "The challenges are 1) existing textual datasets for SpRL does not have enough examples to train such visual models, therefore, such models need to be trained on external datasets and later incorporated in our multi-modal setting, 2) Aligning text entities with image entities is a complex and challenging task itself.", "labels": [], "entities": [{"text": "Aligning text entities", "start_pos": 233, "end_pos": 255, "type": "TASK", "confidence": 0.8785392642021179}]}, {"text": "Each modality in isolation represents spatial relations imperfectly, however, each one can reflect different types of spatial relation better than the other.", "labels": [], "entities": []}, {"text": "If we can handle the mentioned challenges and combine the two modalities then vision modality fills the information gap of the text modality and improves the information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 158, "end_pos": 180, "type": "TASK", "confidence": 0.7728779315948486}]}, {"text": "To overcome the above challenges, we 1) trained two visual models namely word-segment alignment, trained on ImageClef Referring Expression Dataset 1 to connect the two modalities, and preposition classifier, trained on Visual Genome dataset () to help in link disambiguation, and 2) generated a unified graph, based on both image and text data and proposed a global machine learning model to exploit the information from the companion images.", "labels": [], "entities": [{"text": "word-segment alignment", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.7633995711803436}, {"text": "ImageClef Referring Expression Dataset", "start_pos": 108, "end_pos": 146, "type": "DATASET", "confidence": 0.6282127723097801}, {"text": "Visual Genome dataset", "start_pos": 219, "end_pos": 240, "type": "DATASET", "confidence": 0.6647473573684692}, {"text": "link disambiguation", "start_pos": 255, "end_pos": 274, "type": "TASK", "confidence": 0.7232100665569305}]}, {"text": "The contribution of this paper includes a) exploiting the visual information to solve the SpRL task and improving the state-of-the-art results significantly b) forming a global inference model that imposes the consistency constraints on the decisions made based on the two modalities c) exploiting external vision and language datasets to inject external knowledge into our models d) augmenting an existing dataset which is annotated by spatial semantics with the image segment alignments, this dataset will help the evaluation of the existing methods for combining vision and language for fine-grained spatial semantic extractions.", "labels": [], "entities": [{"text": "SpRL task", "start_pos": 90, "end_pos": 99, "type": "TASK", "confidence": 0.9227059483528137}, {"text": "fine-grained spatial semantic extractions", "start_pos": 590, "end_pos": 631, "type": "TASK", "confidence": 0.6911799758672714}]}], "datasetContent": [{"text": "We report the experimental results of our model and compare it with the state-of-the-art (Kordjamshidi et al., 2017a) model, referred here as M0 model.", "labels": [], "entities": []}, {"text": "A role prediction is considered correct if there is a phrase overlap between the ground-truth and predicted roles and each relation is counted as correct when all three arguments are correct.", "labels": [], "entities": []}, {"text": "All the base classifiers described in Section 2.1 are sparse perceptrons.", "labels": [], "entities": []}, {"text": "We use Saul ( to implement the models and solve the global inference of Section 2.", "labels": [], "entities": []}, {"text": "to this dataset to align phrases in the text with the segments of the related images using brat tool.", "labels": [], "entities": []}, {"text": "The alignments are used only for evaluations and are publicly available.", "labels": [], "entities": []}, {"text": "Word-Segment Alignment: We implemented and trained classifiers per words as described in Section 2.2 for the most frequent words in ReferItGame dataset using ( approach.", "labels": [], "entities": [{"text": "Word-Segment Alignment", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6579096019268036}, {"text": "ReferItGame dataset", "start_pos": 132, "end_pos": 151, "type": "DATASET", "confidence": 0.853940486907959}]}, {"text": "We evaluated the trained model on both ReferitGame and CLEF testset, and obtained 64% and 45% accuracy respectively.", "labels": [], "entities": [{"text": "ReferitGame", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.8320062160491943}, {"text": "CLEF testset", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.8856949210166931}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9996582269668579}]}, {"text": "This trained model is used to align words and segments in CLEF dataset.", "labels": [], "entities": [{"text": "CLEF dataset", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.93044513463974}]}, {"text": "The end-to-end evaluation results show that the models trained by this external dataset are helpful though those are not highly accurate for every referring word.", "labels": [], "entities": []}, {"text": "Preposition Classifier: As described in Section 2.2, these are trained on a subset of Visual Genome dataset described in Section 3.1 and evaluated on both Visual Genome and CLEF test sets.", "labels": [], "entities": [{"text": "Visual Genome dataset", "start_pos": 86, "end_pos": 107, "type": "DATASET", "confidence": 0.7803868651390076}, {"text": "CLEF test sets", "start_pos": 173, "end_pos": 187, "type": "DATASET", "confidence": 0.8999025623003641}]}, {"text": "shows five best prepositions result whereas the result for other prepositions is less than 20% F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9994527697563171}]}, {"text": "Spatial Relations: The experimental results in show that our baseline model (BM) is as good as the state-of-the-art model (M0).", "labels": [], "entities": []}, {"text": "Incorporating isAligned feature (in GT and AC models) further improves the results because having the phrases visualized in the image increases the confidence scores of the spatial role and relation classifiers and leads to a higher recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 233, "end_pos": 239, "type": "METRIC", "confidence": 0.9980289340019226}]}, {"text": "The global inference over constraints in BM+C significantly improves the performance of BM (about 5% F1).", "labels": [], "entities": [{"text": "F1", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9986144304275513}]}, {"text": "GT+P results show that inter-modality constraints help in improving the results (about 2% F1) which indicates some of the visual relations successfully confirmed and boosted their corresponding relations in the text modality.", "labels": [], "entities": [{"text": "F1", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9993910789489746}]}, {"text": "However, this improvement is limited which is expected considering the low performance of Preposition Classifier.", "labels": [], "entities": []}, {"text": "The GT+P results indicate the significance of the visual information in our model when the correct alignments are provided.", "labels": [], "entities": []}, {"text": "The alignment classifiers in the AC model also slightly improve the BM+C.", "labels": [], "entities": [{"text": "BM+C", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9179505308469137}]}, {"text": "However, as it is visible in AC+P results, when we have both noisy alignments and noisy visual relations the results drop slightly compared to AC.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results, where P and R denote precision and recall respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.999344527721405}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9988571405410767}]}]}