{"title": [], "abstractContent": [{"text": "We present two neural models for event fac-tuality prediction, which yield significant performance gains over previous models on three event factuality datasets: FactBank, UW, and MEANTIME.", "labels": [], "entities": [{"text": "event fac-tuality prediction", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.7096239328384399}, {"text": "FactBank", "start_pos": 162, "end_pos": 170, "type": "DATASET", "confidence": 0.8745259642601013}, {"text": "UW", "start_pos": 172, "end_pos": 174, "type": "DATASET", "confidence": 0.7771809697151184}, {"text": "MEANTIME", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.6153879761695862}]}, {"text": "We also present a substantial expansion of the It Happened portion of the Universal Decompositional Semantics dataset, yielding the largest event factuality dataset to date.", "labels": [], "entities": [{"text": "Universal Decompositional Semantics dataset", "start_pos": 74, "end_pos": 117, "type": "DATASET", "confidence": 0.5026453882455826}]}, {"text": "We report model results on this extended factuality dataset as well.", "labels": [], "entities": []}], "introductionContent": [{"text": "A central function of natural language is to convey information about the properties of events.", "labels": [], "entities": []}, {"text": "Perhaps the most fundamental of these properties is factuality: whether an event happened or not.", "labels": [], "entities": []}, {"text": "A natural language understanding system's ability to accurately predict event factuality is important for supporting downstream inferences that are based on those events.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 2, "end_pos": 32, "type": "TASK", "confidence": 0.686739444732666}]}, {"text": "For instance, if we aim to construct a knowledge base of events and their participants, it is crucial that we know which events to include and which ones not to.", "labels": [], "entities": []}, {"text": "The event factuality prediction task (EFP) involves labeling event-denoting phrases (or their heads) with the (non)factuality of the events denoted by those phrases).", "labels": [], "entities": [{"text": "event factuality prediction task (EFP)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.7956550461905343}]}, {"text": "exemplifies such an annotation for the phrase headed by leave in (1), which denotes a factual event (\u2295=factual, =nonfactual).", "labels": [], "entities": []}, {"text": "(1) Jo failed to leave no trace.", "labels": [], "entities": []}, {"text": "\u2295 In this paper, we present two neural models of event factuality (and several variants thereof).", "labels": [], "entities": []}, {"text": "We show that these models significantly outperform previous systems on four existing event factuality datasets -FactBank, the UW dataset (, MEAN-TIME (, and Universal De- compositional Semantics It Happened v1 (UDS-IH1;) -and we demonstrate the efficacy of multi-task training and ensembling in this setting.", "labels": [], "entities": [{"text": "FactBank", "start_pos": 112, "end_pos": 120, "type": "DATASET", "confidence": 0.899011492729187}, {"text": "UW dataset", "start_pos": 126, "end_pos": 136, "type": "DATASET", "confidence": 0.9856002926826477}, {"text": "MEAN-TIME", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.8765184879302979}]}, {"text": "In addition, we collect and release an extension of the UDS-IH1 dataset, which we refer to as UDS-IH2, to cover the entirety of the English Universal Dependencies v1.2 (EUD1.2) treebank (, thereby yielding the largest event factuality dataset to date.", "labels": [], "entities": [{"text": "UDS-IH1 dataset", "start_pos": 56, "end_pos": 71, "type": "DATASET", "confidence": 0.9490546584129333}, {"text": "English Universal Dependencies v1.2 (EUD1.2) treebank", "start_pos": 132, "end_pos": 185, "type": "DATASET", "confidence": 0.9276880025863647}]}, {"text": "We begin with theoretical motivation for the models we propose as well as discussion of prior EFP datasets and systems ( \u00a72).", "labels": [], "entities": [{"text": "EFP datasets", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.8409749269485474}]}, {"text": "We then describe our own extension of the UDS-IH1 dataset ( \u00a73), followed by our neural models ( \u00a74).", "labels": [], "entities": [{"text": "UDS-IH1 dataset", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.8968696296215057}]}, {"text": "Using the data we collect, along with the existing datasets, we evaluate our models ( \u00a76) in five experimental settings ( \u00a75) and analyze the results ( \u00a77).", "labels": [], "entities": []}], "datasetContent": [{"text": "Saur\u00ed and Pustejovsky (2009) present the FactBank corpus of event factuality annotations, built on top of the TimeBank corpus ().", "labels": [], "entities": [{"text": "FactBank corpus", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.9550921618938446}, {"text": "TimeBank corpus", "start_pos": 110, "end_pos": 125, "type": "DATASET", "confidence": 0.974699467420578}]}, {"text": "These annotations (performed by trained annotators) are discrete, consisting of an epistemic modal {certain, probable, possible} and a polarity {+,\u2212}.", "labels": [], "entities": []}, {"text": "In FactBank, factuality judgments are with respect to a source; following recent work, here we consider only judgments with respect to a single source: the author.", "labels": [], "entities": [{"text": "FactBank", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.7703015208244324}]}, {"text": "The smaller MEAN-TIME corpus) includes sim- ilar discrete factuality annotations.", "labels": [], "entities": [{"text": "MEAN-TIME corpus", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.8488075435161591}]}, {"text": "de re-annotate a portion of FactBank using crowd-sourced ordinal judgments to capture pragmatic effects on readers' factuality judgments.", "labels": [], "entities": [{"text": "FactBank", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.960735559463501}]}, {"text": "construct an event factuality dataset -henceforth, UW -on the TempEval-3 data () using crowdsourced annotations on a [\u22123, 3] scale (certainly did not happen to certainly did), with over 13,000 predicates.", "labels": [], "entities": [{"text": "UW", "start_pos": 51, "end_pos": 53, "type": "DATASET", "confidence": 0.8197901844978333}, {"text": "TempEval-3 data", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.9484114646911621}]}, {"text": "Adopting the [\u22123, 3] scale of, assemble a Unified Factuality dataset, mapping the discrete annotations of both FactBank and MEANTIME onto the UW scale.", "labels": [], "entities": [{"text": "Unified Factuality dataset", "start_pos": 42, "end_pos": 68, "type": "DATASET", "confidence": 0.6370159884293874}, {"text": "FactBank", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.9172616004943848}, {"text": "MEANTIME", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.772604763507843}, {"text": "UW scale", "start_pos": 142, "end_pos": 150, "type": "DATASET", "confidence": 0.9522442817687988}]}, {"text": "Each scalar annotation corresponds to a token representing the event, and each sentence may have more than one annotated token.", "labels": [], "entities": []}, {"text": "The UDS-IH1 dataset (White et al., 2016) consists of factuality annotations over 6,920 event tokens, obtained with another crowdsourcing protocol.", "labels": [], "entities": [{"text": "UDS-IH1 dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8889854848384857}]}, {"text": "We adopt this protocol, described in \u00a73, to collect roughly triple this number of annotations.", "labels": [], "entities": []}, {"text": "We train and evaluate our factuality prediction models on this new dataset, UDS-IH2, as well as the unified versions of UW, FactBank, and MEANTIME.", "labels": [], "entities": [{"text": "factuality prediction", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.846325159072876}, {"text": "UDS-IH2", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.8689640760421753}, {"text": "UW", "start_pos": 120, "end_pos": 122, "type": "DATASET", "confidence": 0.9659580588340759}, {"text": "FactBank", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.8280904293060303}]}, {"text": "shows the number of annotated predicates in each split of each factuality dataset used in this paper.", "labels": [], "entities": []}, {"text": "Annotations relevant to event factuality and polarity appear in a number of other resources, including the Penn Discourse Treebank (, MPQA Opinion Corpus (Wiebe and), the LU corpus of author belief commitments (, and the ACE and ERE formalisms.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 107, "end_pos": 130, "type": "DATASET", "confidence": 0.9649413824081421}, {"text": "MPQA Opinion Corpus", "start_pos": 134, "end_pos": 153, "type": "DATASET", "confidence": 0.839653730392456}, {"text": "LU corpus of author belief commitments", "start_pos": 171, "end_pos": 209, "type": "DATASET", "confidence": 0.8841587702433268}]}, {"text": "annotate Twitter data for factuality.", "labels": [], "entities": []}, {"text": "Implementation We implement both the LbiLSTM and T-biLSTM models using pytorch 0.2.0.", "labels": [], "entities": []}, {"text": "The L-biLSTM model uses the stock implementation of the stacked bidirectional linear chain LSTM found in pytorch, and the TbiLSTM model uses a custom implementation, which we make available at decomp.net.", "labels": [], "entities": []}, {"text": "Word embeddings We use the 300-dimensional GloVe 42B uncased word embeddings) with an UNK embedding whose dimensions are sampled iid from a Uniform[-1,1].", "labels": [], "entities": []}, {"text": "We do not tune these embeddings during training.", "labels": [], "entities": []}, {"text": "Hidden state sizes We set the dimension of the hidden states h  to 300 for all layers of the stacked L-and stacked TbiLSTMs -the same size as the input word embeddings.", "labels": [], "entities": []}, {"text": "This means that the input to the regression model is 600-dimensional, for the stacked Land T-biLSTMs, and 1200-dimensional, for the stacked H-biLSTM.", "labels": [], "entities": []}, {"text": "For the hidden layer of the regression component, we set the dimension to half the size of the input hidden state: 300, for  the stacked L-and T-biLSTMs, and 600, for the stacked H-biLSTM.", "labels": [], "entities": []}, {"text": "Bidirectional layers We consider stacked L-, T-, and H-biLSTMs with either one or two layers.", "labels": [], "entities": []}, {"text": "In preliminary experiments, we found that networks with three layers badly overfit the training data.", "labels": [], "entities": []}, {"text": "Dependency parses For the T-and H-biLSTMs, we use the gold dependency parses provided in EUD1.2 when training and testing on UDS-IH2.", "labels": [], "entities": [{"text": "EUD1.2", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.9784500002861023}, {"text": "UDS-IH2", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.8567765951156616}]}, {"text": "On FactBank, MEANTIME, and UW, we follow in using the automatic dependency parses generated by the parser in spaCy (Honnibal and Johnson, 2015).", "labels": [], "entities": [{"text": "FactBank", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.8900383710861206}, {"text": "MEANTIME", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.753193199634552}, {"text": "UW", "start_pos": 27, "end_pos": 29, "type": "DATASET", "confidence": 0.9504048824310303}]}, {"text": "3 Lexical features Recent work on neural models in the closely related domain of genericity/habituality prediction suggests that inclusion of hand-annotated lexical features can improve classification performance.", "labels": [], "entities": [{"text": "genericity/habituality prediction", "start_pos": 81, "end_pos": 114, "type": "TASK", "confidence": 0.9047037959098816}]}, {"text": "To assess whether similar performance gains can be obtained here, we experiment with lexical features for simple factive and implicative verbs.", "labels": [], "entities": []}, {"text": "When in use, these features are concatenated to the network's input word embeddings so that, in principle, they may interact with one another and inform other hidden states in the biLSTM, akin to how verbal implicatives and factives are observed to influence the factuality of their complements.", "labels": [], "entities": []}, {"text": "The hidden state size is increased to match the input embedding size.", "labels": [], "entities": []}, {"text": "We consider two types: Signature features We compute binary features based on a curated list of 92 simple implicative and 95 factive verbs including their their type-level \"implication signatures,\" as compiled by.", "labels": [], "entities": []}, {"text": "These signatures characterize the implicative or factive behavior of a verb with respect to its complement clause, how this behavior changes (or does not change) under negation, and how it composes with other such verbs under nested recursion.", "labels": [], "entities": []}, {"text": "We create one indicator feature for each signature type.", "labels": [], "entities": []}, {"text": "Mined features Using a simplified set of pattern matching rules over Common Crawl data), we follow the insights of Pavlick and Callison-Burch (2016) -henceforth, PC -and use corpus mining to automatically score verbs for implicativeness.", "labels": [], "entities": [{"text": "Common Crawl data", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.7767128944396973}]}, {"text": "The insight of PC lies in observation that \"the main sentence containing an implicative predicate and the complement sentence necessarily agree in tense.\"", "labels": [], "entities": []}, {"text": "Accordingly, PC devise a tense agreement score -effectively, the ratio of times an embedding predicate's tense matches the tense of the predicate it embeds -to predict implicativeness in English verbs.", "labels": [], "entities": []}, {"text": "Their scoring method involves the use of fine-grained POS tags, the Stanford Temporal Tagger (, and a number of heuristic rules, which resulted in a confirmation that tense agreement statistics are predictive of implicativeness, illustrated in part by observing a near perfect separation of a list of implicative and non-implicative verbs from: Implicative (bold) and non-implicative (not bold) verbs from Karttunen (1971a) are nearly separable by our tense agreement scores, replicating the results of PC.", "labels": [], "entities": []}, {"text": "We replicate this finding by employing a simplified pattern matching method over 3B sentences of raw Common Crawl text.", "labels": [], "entities": []}, {"text": "We efficiently search for instances of any pattern of the form: I $VERB to * $TIME, where $VERB and $TIME are pre-instantiated variables so their corresponding tenses are known, and ' * ' matches anyone to three whitespace-separated tokens at runtime (not preinstantiated).", "labels": [], "entities": [{"text": "VERB", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9334081411361694}]}, {"text": "Our results in: All 2-layer systems, and 1-layer systems if best in column.", "labels": [], "entities": []}, {"text": "State-of-the-art in bold; \u2020 is best in column (with row shaded in purple).", "labels": [], "entities": []}, {"text": "Key: L=linear, T=tree, H=hybrid, (1,2)=# layers, S=single-task specific, G=single-task general, +lexfeats=with all lexical features, MultiSimp=multi-task simple, MultiBal=multi-task balanced, MultiFoc=multi-task focused, w/UDS-IH2=trained on all data incl.", "labels": [], "entities": []}, {"text": "All-3.0 is the constant baseline.", "labels": [], "entities": []}, {"text": "replication of PC's findings.", "labels": [], "entities": []}, {"text": "Prior work such as by PC is motivated in part by the potential for corpuslinguistic findings to be used as fodder in downstream predictive tasks: we include these agreement scores as potential input features to our networks to test whether contemporary models do in fact benefit from this information.", "labels": [], "entities": []}, {"text": "Training For all experiments, we use stochastic gradient descent to train the LSTM parameters and regression parameters end-to-end with the Adam optimizer (), using the default learning rate in pytorch (1e-3).", "labels": [], "entities": []}, {"text": "We consider five training regimes: 6 1.", "labels": [], "entities": []}, {"text": "SINGLE-TASK SPECIFIC (-S) Train a separate instance of the network for each dataset, training only on that dataset.", "labels": [], "entities": [{"text": "SINGLE-TASK SPECIFIC", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.7071191966533661}]}, {"text": "2. SINGLE-TASK GENERAL (-G) Train one instance of the network on the simple concatenation of all unified factuality datasets, {FactBank, UW, MEANTIME}.", "labels": [], "entities": [{"text": "SINGLE-TASK", "start_pos": 3, "end_pos": 14, "type": "METRIC", "confidence": 0.9275660514831543}, {"text": "GENERAL", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.66314297914505}, {"text": "MEANTIME", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.8200832605361938}]}, {"text": "3. MULTI-TASK SIMPLE (-MULTISIMP) Same with each of five past tense phrases (\"yesterday,\" \"last week,\" etc.) and five corresponding future tense phrases (\"tomorrow,\" \"next week,\" etc).", "labels": [], "entities": [{"text": "MULTI-TASK", "start_pos": 3, "end_pos": 13, "type": "METRIC", "confidence": 0.8262209296226501}, {"text": "MULTISIMP", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.8734427690505981}]}, {"text": "See Supplement for further details.", "labels": [], "entities": [{"text": "Supplement", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.6107485294342041}]}, {"text": "Multi-task can have subtly different meanings in the NLP community; following terminology from, our use is best described as \"semantically equivalent transfer\" with simultaneous (MULT) network training.", "labels": [], "entities": []}, {"text": "as SINGLE-TASK GENERAL, except the network maintains a distinct set of regression parameters for each dataset; all other parameters (LSTM) remain tied.", "labels": [], "entities": []}, {"text": "\"w/UDS-IH2\" is specified if UDS-IH2 is included in training.", "labels": [], "entities": []}, {"text": "We filter our dataset to remove annotators with very low agreement in two ways: (i) based on the their agreement with other annotators on the HAP-PENED question; and (ii) based on the their agreement with other annotators on the CONFIDENCE question.", "labels": [], "entities": [{"text": "HAP-PENED question", "start_pos": 142, "end_pos": 160, "type": "DATASET", "confidence": 0.7978161573410034}, {"text": "CONFIDENCE question", "start_pos": 229, "end_pos": 248, "type": "DATASET", "confidence": 0.8129488527774811}]}, {"text": "For the HAPPENED question, we computed, for each pair of annotators and each item that both of those annotators annotated, whether the two responses were equal.", "labels": [], "entities": [{"text": "HAPPENED question", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.6504732519388199}]}, {"text": "We then fit a random effects logistic regression to response equality with random intercepts for annotator.", "labels": [], "entities": []}, {"text": "The Best Linear Unbiased Predictors (BLUPs) for each annotator were then extracted and z-scored.", "labels": [], "entities": [{"text": "Linear Unbiased Predictors (BLUPs)", "start_pos": 9, "end_pos": 43, "type": "METRIC", "confidence": 0.7343324720859528}]}, {"text": "Annotators were removed if their z-scored BLUP was less than -2.", "labels": [], "entities": [{"text": "Annotators", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9061245322227478}, {"text": "BLUP", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9551217555999756}]}, {"text": "For the CONFIDENCE question, we first riditscored the ratings by annotator; and for each pair of annotators and each item that both of those annotators annotated, we computed the difference between the two ridit-scored confidences.", "labels": [], "entities": []}, {"text": "We then fit a random effects linear regression to the resulting difference after logit-transformation with random intercepts for annotator.", "labels": [], "entities": []}, {"text": "The same BLUPbased exclusion procedure was then used.", "labels": [], "entities": [{"text": "BLUPbased exclusion", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.4773527532815933}]}, {"text": "This filtering results in the exclusion of one annotator, who is excluded for low agreement on HAPPENED.", "labels": [], "entities": []}, {"text": "4,179 annotations are removed in the filtering, but because we remove only a single annotator, there remains at least one annotation for every predicate.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Implicative (bold) and non-implicative (not  bold) verbs from Karttunen (1971a) are nearly sepa- rable by our tense agreement scores, replicating the re- sults of PC.", "labels": [], "entities": []}, {"text": " Table 4: All 2-layer systems, and 1-layer systems if best in column. State-of-the-art in bold;  \u2020 is best in column  (with row shaded in purple). Key: L=linear, T=tree, H=hybrid, (1,2)=# layers, S=single-task specific, G=single- task general, +lexfeats=with all lexical features, MultiSimp=multi-task simple, MultiBal=multi-task balanced,  MultiFoc=multi-task focused, w/UDS-IH2=trained on all data incl. UDS-IH2. All-3.0 is the constant baseline.", "labels": [], "entities": []}, {"text": " Table 5: Mean gold labels, counts, and MAE for L- biLSTM(2)-S and T-biLSTM(2)-S model predictions  on UDS-IH2-dev, grouped by modals and negation.", "labels": [], "entities": [{"text": "MAE", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9991914629936218}, {"text": "UDS-IH2-dev", "start_pos": 103, "end_pos": 114, "type": "DATASET", "confidence": 0.7957749962806702}]}, {"text": " Table 6: Mean predictions for linear (L-biLSTM-S", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9803756475448608}]}, {"text": " Table 9: MAE of L-biLSTM(2)-S and L-biLSTM(2)- S+lexfeats, for predictions on events in UDS-IH2-dev  that are xcomp-governed by an infinitival-taking verb.", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9971454739570618}, {"text": "UDS-IH2-dev", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.9070160984992981}]}]}