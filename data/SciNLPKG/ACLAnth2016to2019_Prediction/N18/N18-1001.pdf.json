{"title": [{"text": "Label-aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition APEX Data and Knowledge Management Lab", "labels": [], "entities": [{"text": "Double Transfer Learning", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8114808996518453}, {"text": "Cross-Specialty Medical Named Entity Recognition APEX Data and Knowledge Management", "start_pos": 41, "end_pos": 124, "type": "TASK", "confidence": 0.7539206296205521}]}], "abstractContent": [{"text": "We study the problem of named entity recognition (NER) from electronic medical records, which is one of the most fundamental and critical problems for medical text mining.", "labels": [], "entities": [{"text": "named entity recognition (NER) from electronic medical records", "start_pos": 24, "end_pos": 86, "type": "TASK", "confidence": 0.8429138481616973}, {"text": "medical text mining", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.629236618677775}]}, {"text": "Medical records which are written by clini-cians from different specialties usually contain quite different terminologies and writing styles.", "labels": [], "entities": []}, {"text": "The difference of specialties and the cost of human annotation makes it particularly difficult to train a universal medical NER system.", "labels": [], "entities": [{"text": "NER", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.9058137536048889}]}, {"text": "In this paper, we propose a label-aware double transfer learning framework (La-DTL) for cross-specialty NER, so that a medical NER system designed for one specialty could be conveniently applied to another one with minimal annotation efforts.", "labels": [], "entities": [{"text": "NER", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.6379448771476746}]}, {"text": "The trans-ferability is guaranteed by two components: (i) we propose label-aware MMD for feature representation transfer, and (ii) we perform parameter transfer with a theoretical upper bound which is also label aware.", "labels": [], "entities": [{"text": "feature representation transfer", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.6876354912916819}]}, {"text": "We conduct extensive experiments on 12 cross-specialty NER tasks.", "labels": [], "entities": [{"text": "NER tasks", "start_pos": 55, "end_pos": 64, "type": "TASK", "confidence": 0.891618400812149}]}, {"text": "The experimental results demonstrate that La-DTL provides consistent accuracy improvement over strong baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9994196891784668}]}, {"text": "Besides , the promising experimental results on non-medical NER scenarios indicate that La-DTL is potential to be seamlessly adapted to a wide range of NER tasks.", "labels": [], "entities": [{"text": "NER", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9726347327232361}, {"text": "NER tasks", "start_pos": 152, "end_pos": 161, "type": "TASK", "confidence": 0.9132702946662903}]}], "introductionContent": [{"text": "The development of hospital information system and medical informatics drives the leverage of various medical data fora more efficient and intelligent medical care service.", "labels": [], "entities": []}, {"text": "Among many kinds of medical data, electronic health records (EHRs) are one of the most valuable and informative data as they contain detailed information about the patients and the clinical practices.", "labels": [], "entities": []}, {"text": "EHRs are essential to many intelligent clinical applications, such * Weinan Zhang is the corresponding author.", "labels": [], "entities": []}, {"text": "as hospital quality control and clinical decision support systems ().", "labels": [], "entities": []}, {"text": "Most of EHRs are recorded in an unstructured form, i.e., natural language.", "labels": [], "entities": []}, {"text": "Hence, extracting structured information from EHRs using natural language processing (NLP), e.g., named entity recognition (NER) and entity linking, plays a fundamental role in medical informatics (.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 98, "end_pos": 128, "type": "TASK", "confidence": 0.8099174698193868}, {"text": "entity linking", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.7457886338233948}]}, {"text": "In this paper, we focus on medical NER from EHRs, which is a fundamental task and is widely studied in the research community ().", "labels": [], "entities": [{"text": "NER", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.7718884348869324}]}, {"text": "In practice, the difficulty of building a universally robust and high-performance medical NER system lies in the variety of medical terminologies and expressions among different departments of specialties and hospitals.", "labels": [], "entities": []}, {"text": "However, building separate NER systems for so many specialties comes with a prohibitively high cost.", "labels": [], "entities": []}, {"text": "The data privacy issue further discourages the sharing of the data across departments or hospitals, making it more difficult to train a canonical NER system to be applied everywhere.", "labels": [], "entities": []}, {"text": "This raises a natural question: if we have sufficient annotated EHRs data in one source specialty, can we distill the knowledge and transfer it to help training models in a related target specialty with few annotations?", "labels": [], "entities": []}, {"text": "By transferring the knowledge we can achieve higher performance in target specialties with lower annotation cost and bypass the data sharing concerns.", "labels": [], "entities": []}, {"text": "This is commonly referred to as transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.9726868271827698}]}, {"text": "Current state-of-the-art transfer learning methods for NER are mainly based on deep neural networks, which perform an end-to-end training to distill sequential dependency patterns in the natural language.", "labels": [], "entities": [{"text": "NER", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.972521185874939}]}, {"text": "These transfer learning methods include (i) feature representation transfer (, which normally lever-1 ages deep neural networks to learn a close feature mapping between the source and target domains, and (ii) parameter transfer (, which performs parameter sharing or joint training to get the target-domain model parameters close to those of the source-domain model.", "labels": [], "entities": [{"text": "feature representation transfer", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6913454333941141}, {"text": "parameter transfer", "start_pos": 209, "end_pos": 227, "type": "TASK", "confidence": 0.6947495937347412}]}, {"text": "To the best of our knowledge, there is no previous literature working on transfer learning for NER in the medical domain, or even in a larger scope, i.e., medical natural language processing.", "labels": [], "entities": [{"text": "NER", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9571494460105896}, {"text": "medical natural language processing", "start_pos": 155, "end_pos": 190, "type": "TASK", "confidence": 0.6268582344055176}]}, {"text": "In this paper, we propose a novel NER transfer learning framework, namely label-aware double transfer learning (La-DTL): (i) We leverage bidirectional long-short term memory (Bi-LSTM) network () to automatically learn the text representations, based on which we perform a label-aware feature representation transfer.", "labels": [], "entities": [{"text": "NER transfer learning", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.9591851035753886}, {"text": "label-aware feature representation transfer", "start_pos": 272, "end_pos": 315, "type": "TASK", "confidence": 0.6539871767163277}]}, {"text": "We propose a variant of maximum mean discrepancy (MMD) (, namely label-aware MMD (La-MMD), to explicitly reduce the domain discrepancy of feature representations of tokens with the same label between two domains.", "labels": [], "entities": [{"text": "maximum mean discrepancy (MMD)", "start_pos": 24, "end_pos": 54, "type": "METRIC", "confidence": 0.7742377122243246}]}, {"text": "(ii) Based on the learned feature representations from Bi-LSTM, two conditional random field (CRF) models are performed for sequence labeling for source and target domain separately, where parameter transfer learning is performed.", "labels": [], "entities": [{"text": "parameter transfer learning", "start_pos": 189, "end_pos": 216, "type": "TASK", "confidence": 0.7540730635325114}]}, {"text": "Specifically, an upper bound of KL divergence between the source and target domain's CRF label distributions is added over the emission and transition matrices across the source and target CRF models to explore the shareable parts of the parameters.", "labels": [], "entities": []}, {"text": "Both (i) and (ii) have a labelaware characteristic, which will be discussed later.", "labels": [], "entities": []}, {"text": "We further argue that label-aware characteristic is crucial for transfer learning in sequence labeling problems, e.g., NER, because only when the corresponding labels are matched, can the \"similar\" contexts (i.e. feature representation) and model parameters be efficiently borrowed to improve the label prediction.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.9120084345340729}, {"text": "NER", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9112083911895752}, {"text": "label prediction", "start_pos": 297, "end_pos": 313, "type": "TASK", "confidence": 0.7513072490692139}]}, {"text": "Extensive experiments are conducted on 12 cross-specialty medical NER tasks with real-world EHRs.", "labels": [], "entities": [{"text": "NER tasks", "start_pos": 66, "end_pos": 75, "type": "TASK", "confidence": 0.7623977661132812}]}, {"text": "The experimental results demonstrate that La-DTL provides consistent accuracy improvement over strong baselines, with overall 2.62% to 6.70% absolute F1-score improvement over the state-of-the-art methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9994377493858337}, {"text": "F1-score", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9666053056716919}]}, {"text": "Besides, the promising experimental results on other two non-medical NER scenarios indicate that La-DTL has the potential to be seamlessly adapted to a wide range of NER tasks.", "labels": [], "entities": [{"text": "NER", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9672884345054626}, {"text": "NER tasks", "start_pos": 166, "end_pos": 175, "type": "TASK", "confidence": 0.9161756932735443}]}], "datasetContent": [{"text": "In this section, we evaluate La-DTL 1 and other baseline methods on 12 cross-specialty NER problems based on real-world datasets.", "labels": [], "entities": [{"text": "NER", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9245650172233582}]}, {"text": "The experimental results show that La-DTL steadily outperforms other baseline models in all tasks significantly.", "labels": [], "entities": []}, {"text": "We also conduct further ablation study and robustness study.", "labels": [], "entities": [{"text": "ablation", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9937790036201477}]}, {"text": "We evaluate La-DTL on two more nonmedical NER transfer tasks to validate its general efficacy over a wide range of applications.", "labels": [], "entities": [{"text": "NER transfer", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9741537272930145}]}, {"text": "To show La-DTL could be applied in a wide range of NER transfer learning scenarios, we make experiments on two non-medical NER tasks.", "labels": [], "entities": [{"text": "NER transfer learning", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.938562015692393}, {"text": "NER tasks", "start_pos": 123, "end_pos": 132, "type": "TASK", "confidence": 0.8807604312896729}]}, {"text": "Corpora's details are shown in.", "labels": [], "entities": []}, {"text": "WeiboNER Transfer Following;, we transfer knowledge from SighanNER (MSR corpus of the sixth SIGHAN Workshop on Chinese language processing) to WeiboNER (a social media NER corpus) (.", "labels": [], "entities": [{"text": "WeiboNER", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9041134715080261}, {"text": "SighanNER (MSR corpus of the sixth", "start_pos": 57, "end_pos": 91, "type": "DATASET", "confidence": 0.8540608372007098}, {"text": "SIGHAN Workshop on Chinese language processing)", "start_pos": 92, "end_pos": 139, "type": "TASK", "confidence": 0.8158950635365078}, {"text": "WeiboNER", "start_pos": 143, "end_pos": 151, "type": "DATASET", "confidence": 0.8956544995307922}]}, {"text": "Results in show that La-DTL outperforms all the baseline methods in Chinese social media domain.", "labels": [], "entities": []}, {"text": "fer learning scenarios with mismatched label sets and languages like English.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sentence numbers for CM-NER corpus.", "labels": [], "entities": [{"text": "CM-NER corpus", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.868352472782135}]}, {"text": " Table 2: Results (F1-score %) of 12 cross-specialty medical NER tasks. C, R, N, G are short for the department  of Cardiology, Respiratory, Neurology, and Gastroenterology, respectively.  \u2020 indicates La-DTL outperforms the 6  baselines significantly (p < 0.05).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9966903924942017}, {"text": "NER tasks", "start_pos": 61, "end_pos": 70, "type": "TASK", "confidence": 0.7318261861801147}]}, {"text": " Table 3: Sentence numbers for non-medical corpora.", "labels": [], "entities": []}, {"text": " Table 4: Results (F1-score %) of WeiboNER transfer.   *  indicates the result reported in the corresponding ref- erence.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9987598657608032}, {"text": "WeiboNER transfer", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.9775616526603699}, {"text": "ref- erence", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.7141380906105042}]}]}