{"title": [{"text": "Pieces of Eight: 8-bit Neural Machine Translation *", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.7718101938565572}]}], "abstractContent": [{"text": "Neural machine translation has achieved levels of fluency and adequacy that would have been surprising a short time ago.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6784456471602122}]}, {"text": "Output quality is extremely relevant for industry purposes, however it is equally important to produce results in the shortest time possible, mainly for latency-sensitive applications and to control cloud hosting costs.", "labels": [], "entities": []}, {"text": "In this paper we show the effectiveness of translating with 8-bit quanti-zation for models that have been trained using 32-bit floating point values.", "labels": [], "entities": [{"text": "translating", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.9786685109138489}]}, {"text": "Results show that 8-bit translation makes a non-negligible impact in terms of speed with no degradation inaccuracy and adequacy.", "labels": [], "entities": [{"text": "speed", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9831761121749878}]}], "introductionContent": [{"text": "Neural machine translation (NMT) () has recently achieved remarkable performance improving fluency and adequacy over phrase-based machine translation and is being deployed in commercial settings (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7742342948913574}, {"text": "phrase-based machine translation", "start_pos": 117, "end_pos": 149, "type": "TASK", "confidence": 0.6747267047564188}]}, {"text": "However, this comes at a cost of slow decoding speeds compared to phrase-based and syntax-based SMT (see section 3).", "labels": [], "entities": [{"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.8409875631332397}]}, {"text": "NMT models are generally trained using 32-bit floating point values.", "labels": [], "entities": []}, {"text": "At training time, multiple sentences can be processed in parallel leveraging graphical processing units (GPUs) to good advantage since the data is processed in batches.", "labels": [], "entities": []}, {"text": "This is also true for decoding for non-interactive applications such as bulk document translation.", "labels": [], "entities": [{"text": "bulk document translation", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.586864689985911}]}, {"text": "Why is fast execution on CPUs important?", "labels": [], "entities": []}, {"text": "First, CPUs are cheaper than GPUs.", "labels": [], "entities": []}, {"text": "Fast CPU computation will reduce commercial deployment costs.", "labels": [], "entities": []}, {"text": "Second, for low-latency applications such as speech-to-speech translation (Neubig et al., * A piece of eight was a Spanish dollar that was divided into 8 reales, also known as Real de a Ocho.", "labels": [], "entities": [{"text": "speech-to-speech translation", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.7608661651611328}]}, {"text": "2017a), it is important to translate individual sentences quickly enough so that users can have an application experience that responds seamlessly.", "labels": [], "entities": []}, {"text": "Translating individual sentences with NMT requires many memory bandwidth intensive matrixvector or matrix-narrow matrix multiplications (.", "labels": [], "entities": []}, {"text": "In addition, the batch size is 1 and GPUs do not have a speed advantage over CPUs due to the lack of adequate parallel work (as evidenced by increasingly difficult batching scenarios in dynamic frameworks ().", "labels": [], "entities": []}, {"text": "Others have successfully used low precision approximations to neural net models.", "labels": [], "entities": []}, {"text": "explored 8-bit quantization for feedforward neural nets for speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.8514699339866638}]}, {"text": "Devlin (2017) explored 16-bit quantization for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.811948150396347}]}, {"text": "In this paper we show the effectiveness of 8-bit decoding for models that have been trained using 32-bit floating point values.", "labels": [], "entities": []}, {"text": "Results show that 8-bit decoding does not hurt the fluency or adequacy of the output, while producing results up to 4-6x times faster.", "labels": [], "entities": []}, {"text": "In addition, implementation is straightforward and we can use the models as is without altering training.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 reviews the attentional model of translation to be sped up, Section 3 presents our 8-bit quantization in our implementation, Section 4 presents automatic measurements of speed and translation quality plus human evaluations, Section 5 discusses the results and some illustrative examples, Section 6 describes prior work, and Section 7 concludes the paper.", "labels": [], "entities": [{"text": "translation", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.9805012941360474}]}], "datasetContent": [{"text": "These automatic results suggest that 8-bit quantization can be done without perceptible degradation.", "labels": [], "entities": []}, {"text": "To confirm this, we carried out a human evaluation experiment.", "labels": [], "entities": []}, {"text": "In, we show the results of performing human evaluations on some of the same language pairs in the previous section.", "labels": [], "entities": []}, {"text": "An independent native speaker of the language being translated to/from different than English (who is also proficient in English) scored 100 randomly selected sentences.", "labels": [], "entities": []}, {"text": "The sentences were shuffled during the evaluation to avoid evaluator bias towards different runs.", "labels": [], "entities": []}, {"text": "We employ a scale from 0 to 5, with 0 being unintelligible and 5 being perfect translation.", "labels": [], "entities": []}, {"text": "Language 32-bit 8-bit En  The that the automatic scores shown in the previous section are also sustained", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Model training data and vocabulary sizes", "labels": [], "entities": []}, {"text": " Table 5: Test data sizes and sentence lengths", "labels": [], "entities": []}, {"text": " Table 7: Human evaluation scores for 8-bit and 32-bit  systems. All tests are news domain.", "labels": [], "entities": []}]}