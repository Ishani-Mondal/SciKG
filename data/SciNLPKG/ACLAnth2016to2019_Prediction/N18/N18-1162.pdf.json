{"title": [{"text": "A Hierarchical Latent Structure for Variational Conversation Modeling", "labels": [], "entities": [{"text": "Variational Conversation Modeling", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.8275574445724487}]}], "abstractContent": [{"text": "Variational autoencoders (VAE) combined with hierarchical RNNs have emerged as a powerful framework for conversation model-ing.", "labels": [], "entities": []}, {"text": "However, they suffer from the notorious degeneration problem, where the decoders learn to ignore latent variables and reduce to vanilla RNNs.", "labels": [], "entities": []}, {"text": "We empirically show that this degeneracy occurs mostly due to two reasons.", "labels": [], "entities": []}, {"text": "First, the expressive power of hierarchical RNN decoders is often high enough to model the data using only its decoding distributions without relying on the latent variables.", "labels": [], "entities": []}, {"text": "Second, the conditional VAE structure whose generation process is conditioned on a context, makes the range of training targets very sparse; that is, the RNN decoders can easily overfit to the training data ignoring the latent variables.", "labels": [], "entities": []}, {"text": "To solve the degeneration problem, we propose a novel model named Variational Hierarchical Conversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical structure of latent variables, and (2) exploiting an utterance drop regularization.", "labels": [], "entities": []}, {"text": "With evaluations on two datasets of Cornell Movie Dialog and Ubuntu Dialog Corpus, we show that our VHCR successfully utilizes latent variables and outperforms state-of-the-art models for conversation generation.", "labels": [], "entities": [{"text": "Cornell Movie Dialog", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.9678244392077128}, {"text": "Ubuntu Dialog Corpus", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.8993782003720602}, {"text": "conversation generation", "start_pos": 188, "end_pos": 211, "type": "TASK", "confidence": 0.8322166502475739}]}, {"text": "Moreover, it can perform several new utterance control tasks, thanks to its hierarchical latent structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conversation modeling has been along interest of natural language research.", "labels": [], "entities": [{"text": "Conversation modeling", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8866388201713562}]}, {"text": "Recent approaches for data-driven conversation modeling mostly build upon recurrent neural networks (RNNs) (.", "labels": [], "entities": [{"text": "data-driven conversation modeling", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.6707579692204794}]}, {"text": "use a hierarchical RNN structure to model the context of conversation.", "labels": [], "entities": []}, {"text": "further exploit an utterance latent variable in the hierarchical RNNs by incorporating the variational autoencoder (VAE) framework.", "labels": [], "entities": []}, {"text": "VAEs enable us to train a latent variable model for natural language modeling, which grants us several advantages.", "labels": [], "entities": [{"text": "natural language modeling", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.6420647104581197}]}, {"text": "First, latent variables can learn an interpretable holistic representation, such as topics, tones, or high-level syntactic properties.", "labels": [], "entities": []}, {"text": "Second, latent variables can model inherently abundant variability of natural language by encoding its global and long-term structure, which is hard to be captured by shallow generative processes (e.g. vanilla RNNs) where the only source of stochasticity comes from the sampling of output words.", "labels": [], "entities": []}, {"text": "In spite of such appealing properties of latent variable models for natural language modeling, VAEs suffer from the notorious degeneration problem () that occurs when a VAE is combined with a powerful decoder such as autoregressive RNNs.", "labels": [], "entities": [{"text": "natural language modeling", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.6895109812418619}]}, {"text": "This issue makes VAEs ignore latent variables, and eventually behave as vanilla RNNs.", "labels": [], "entities": []}, {"text": "also note this degeneration issue by showing that a VAE with a RNN decoder prefers to model the data using its decoding distribution rather than using latent variables, from bits-back coding perspective.", "labels": [], "entities": []}, {"text": "To resolve this issue, several heuristics have been proposed to weaken the decoder, enforcing the models to use latent variables.", "labels": [], "entities": []}, {"text": "For example, propose some heuristics, including KL annealing and word drop regularization.", "labels": [], "entities": [{"text": "word drop regularization", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.7534257968266805}]}, {"text": "However, these heuristics cannot be a complete solution; for example, we observe that they fail to prevent the degeneracy in VHRED (, a conditional VAE model equipped with hierarchical RNNs for conversation modeling.", "labels": [], "entities": [{"text": "VHRED", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.9243142008781433}, {"text": "conversation modeling", "start_pos": 194, "end_pos": 215, "type": "TASK", "confidence": 0.8085835874080658}]}, {"text": "The objective of this work is to propose a novel VAE model that significantly alleviates the degen-eration problem.", "labels": [], "entities": [{"text": "VAE", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.8996701240539551}]}, {"text": "Our analysis reveals that the causes of the degeneracy are two-fold.", "labels": [], "entities": []}, {"text": "First, the hierarchical structure of autoregressive RNNs is powerful enough to predict a sequence of utterances without the need of latent variables, even with the word drop regularization.", "labels": [], "entities": []}, {"text": "Second, we newly discover that the conditional VAE structure where an utterance is generated conditioned on context, i.e. a previous sequence of utterances, induces severe data sparsity.", "labels": [], "entities": []}, {"text": "Even with a large-scale training corpus, there only exist very few target utterances when conditioned on the context.", "labels": [], "entities": []}, {"text": "Hence, the hierarchical RNNs can easily memorize the context-to-utterance relations without relying on latent variables.", "labels": [], "entities": []}, {"text": "We propose a novel model named Variational Hierarchical Conversation RNN (VHCR), which involves two novel features to alleviate this problem.", "labels": [], "entities": [{"text": "Variational Hierarchical Conversation RNN", "start_pos": 31, "end_pos": 72, "type": "TASK", "confidence": 0.6302720457315445}]}, {"text": "First, we introduce a global conversational latent variable along with local utterance latent variables to build a hierarchical latent structure.", "labels": [], "entities": []}, {"text": "Second, we propose anew regularization technique called utterance drop.", "labels": [], "entities": [{"text": "utterance drop", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.8518987894058228}]}, {"text": "We show that our hierarchical latent structure is not only crucial for facilitating the use of latent variables in conversation modeling, but also delivers several additional advantages, including gaining control over the global context in which the conversation takes place.", "labels": [], "entities": [{"text": "conversation modeling", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.6917837560176849}]}, {"text": "Our major contributions are as follows: (1) We reveal that the existing conditional VAE model with hierarchical RNNs for conversation modeling (e.g. () still suffers from the degeneration problem, and this problem is caused by data sparsity per context that arises from the conditional VAE structure, as well as the use of powerful hierarchical RNN decoders.", "labels": [], "entities": []}, {"text": "(2) We propose a novel variational hierarchical conversation RNN (VHCR), which has two distinctive features: a hierarchical latent structure and anew regularization of utterance drop.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, our VHCR is the first VAE conversation model that exploits the hierarchical latent structure.", "labels": [], "entities": [{"text": "VAE conversation", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7341568171977997}]}, {"text": "(3) With evaluations on two benchmark datasets of Cornell Movie Dialog (Danescu-NiculescuMizil and Lee, 2011) and Ubuntu Dialog Corpus (, we show that our model improves the conversation performance in multiple metrics over state-of-the-art methods, including HRED ( , and VHRED) with existing degeneracy solutions such as the word drop, and the bag-of-words loss ().", "labels": [], "entities": [{"text": "Cornell Movie Dialog", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.9680800835291544}, {"text": "Ubuntu Dialog Corpus", "start_pos": 114, "end_pos": 134, "type": "DATASET", "confidence": 0.8926242391268412}, {"text": "HRED", "start_pos": 260, "end_pos": 264, "type": "METRIC", "confidence": 0.8710839152336121}, {"text": "VHRED", "start_pos": 273, "end_pos": 278, "type": "METRIC", "confidence": 0.6371766328811646}]}], "datasetContent": [{"text": "We evaluate the performance of conversation generation using two benchmark datasets: 1) Cornell Movie Dialog Corpus (Danescu-, containing about 1 million multi-turn conversations from Ubuntu IRC channels.", "labels": [], "entities": [{"text": "conversation generation", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7383729368448257}, {"text": "Cornell Movie Dialog Corpus", "start_pos": 88, "end_pos": 115, "type": "DATASET", "confidence": 0.9758697003126144}]}, {"text": "In both datasets, we truncate utterances longer than 30 words.", "labels": [], "entities": []}, {"text": "We compare our approach with four baselines.", "labels": [], "entities": []}, {"text": "They are combinations of two state-ofthe-art models of conversation generation with different solutions to the degeneracy.", "labels": [], "entities": [{"text": "conversation generation", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7557301223278046}]}, {"text": "Automatic evaluation of conversational systems is still a challenging problem ().", "labels": [], "entities": []}, {"text": "Based on literature, we report three quantitative metrics: i) the negative log-likelihood (the variational bound for variational models), ii) embedding-based metrics (, and iii) human evaluation via Amazon Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 199, "end_pos": 227, "type": "DATASET", "confidence": 0.9236625333627065}]}, {"text": "summarizes the per-word negative loglikelihood (NLL) evaluated on the test sets of two datasets.", "labels": [], "entities": [{"text": "per-word negative loglikelihood (NLL)", "start_pos": 15, "end_pos": 52, "type": "METRIC", "confidence": 0.7463223338127136}]}, {"text": "For variational models, we instead present the variational bound of the negative loglikelihood in Eq.", "labels": [], "entities": [{"text": "variational", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.977769136428833}]}, {"text": "2, which consists of the reconstruction error term and the KL divergence term.", "labels": [], "entities": [{"text": "reconstruction error term", "start_pos": 25, "end_pos": 50, "type": "METRIC", "confidence": 0.7614516615867615}, {"text": "KL divergence term", "start_pos": 59, "end_pos": 77, "type": "METRIC", "confidence": 0.7887499531110128}]}, {"text": "The KL divergence term can measure how much each model utilizes the latent variables.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of Negative Log-likelihood. The  inequalities denote the variational bounds. w.d and  u.d., and bow denote the word drop, the utterance  drop, and the auxiliary bag-of-words loss respec- tively.", "labels": [], "entities": []}, {"text": " Table 2: KL divergence decomposition. VHRED  and VHCR are trained with word drop and utter- ance drop respectively.", "labels": [], "entities": [{"text": "KL divergence decomposition", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8865169882774353}, {"text": "VHRED", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9062283635139465}, {"text": "VHCR", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.866493284702301}]}, {"text": " Table 3: Results of embedding-based metrics. 1- turn and 3-turn responses of models per context.", "labels": [], "entities": []}]}