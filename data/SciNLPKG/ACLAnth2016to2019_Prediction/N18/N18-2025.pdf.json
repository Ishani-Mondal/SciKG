{"title": [{"text": "Character-based Neural Networks for Sentence Pair Modeling", "labels": [], "entities": [{"text": "Sentence Pair Modeling", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.8296465873718262}]}], "abstractContent": [{"text": "Sentence pair modeling is critical for many NLP tasks, such as paraphrase identification, semantic textual similarity, and natural language inference.", "labels": [], "entities": [{"text": "Sentence pair modeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9286546309789022}, {"text": "paraphrase identification", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.9079390168190002}]}, {"text": "Most state-of-the-art neu-ral models for these tasks rely on pretrained word embedding and compose sentence-level semantics in varied ways; however, few works have attempted to verify whether we really need pretrained embeddings in these tasks.", "labels": [], "entities": []}, {"text": "In this paper, we study how effective subword-level (character and character n-gram) representations are in sentence pair modeling.", "labels": [], "entities": [{"text": "sentence pair modeling", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.8017889857292175}]}, {"text": "Though it is well-known that subword models are effective in tasks with single sentence input, including language modeling and machine translation, they have not been systematically studied in sentence pair modeling tasks where the semantic and string similarities between texts matter.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7197659760713577}, {"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7413653284311295}, {"text": "sentence pair modeling", "start_pos": 193, "end_pos": 215, "type": "TASK", "confidence": 0.6755838195482889}]}, {"text": "Our experiments show that subword models without any pretrained word embedding can achieve new state-of-the-art results on two social media datasets and competitive results on news data for paraphrase identification .", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 190, "end_pos": 215, "type": "TASK", "confidence": 0.9339497983455658}]}], "introductionContent": [{"text": "Recently, there have been various neural network models proposed for sentence pair modeling tasks, including semantic similarity (, paraphrase identification (, natural language inference, etc.", "labels": [], "entities": [{"text": "sentence pair modeling tasks", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.7673287615180016}, {"text": "paraphrase identification", "start_pos": 132, "end_pos": 157, "type": "TASK", "confidence": 0.8321362435817719}]}, {"text": "Most, if not all, of these state-of-the-art neural models () have achieved the best performances for these tasks by using pretrained word embeddings, but results without pretraining are less frequently reported or noted.", "labels": [], "entities": []}, {"text": "In fact, we will show that, even with fixed randomized word vectors, the pairwise word interaction model) based on contextual word vector similarities can still achieve strong performance by capturing identical words and similar surface context features.", "labels": [], "entities": []}, {"text": "Moreover, pretrained word embeddings generally have poor coverage in social media domain where out-of-vocabulary rate often reaches over 20% (.", "labels": [], "entities": []}, {"text": "We investigated the effectiveness of subword units, such as characters and character n-grams, in place of words for vector representations in sentence pair modeling.", "labels": [], "entities": [{"text": "sentence pair modeling", "start_pos": 142, "end_pos": 164, "type": "TASK", "confidence": 0.7240033348401388}]}, {"text": "Though it is well-known that subword representations are effective to model out-of-vocabulary words in many NLP tasks with a single sentence input, such as machine translation (, language modeling (, and sequence labeling (dos), they are not systematically studied in the tasks that concern pairs of sentences.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7910728454589844}, {"text": "language modeling", "start_pos": 179, "end_pos": 196, "type": "TASK", "confidence": 0.7138787657022476}, {"text": "sequence labeling", "start_pos": 204, "end_pos": 221, "type": "TASK", "confidence": 0.673974871635437}]}, {"text": "Unlike in modeling individual sentences, subword representations have impacts not only on the out-ofvocabulary words but also more directly on the relation between two sentences, which is calculated based on vector similarities in many sentence pair modeling approaches (more details in Section 2.1).", "labels": [], "entities": []}, {"text": "For example, while subwords may capture useful string similarities between a pair of sentences (e.g. spelling or morphological variations: sister and sista, teach and teaches), they could introduce errors (e.g. similarly spelled words with completely different meanings: ware and war).", "labels": [], "entities": []}, {"text": "To better understand the role of subword embedding in sentence pair modeling, we performed experimental comparisons that vary (1) the type of subword unit, (2) the composition function, and (3) the datasets of different characteristics.", "labels": [], "entities": [{"text": "sentence pair modeling", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.770739863316218}]}, {"text": "We also presented experiments with language modeling as an auxiliary multi-task learning objective, showing consistent improvements.", "labels": [], "entities": []}, {"text": "Taken together, subword and language modeling establish new state-of-the-art results in two social media datasets and competitive results in a news dataset for paraphrase identification without using any pretrained word embeddings.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 160, "end_pos": 185, "type": "TASK", "confidence": 0.8387624621391296}]}], "datasetContent": [{"text": "We performed experiments on three benchmark datasets for paraphrase identification; each contained pairs of naturally occurring sentences manually labeled as paraphrases and non-paraphrases for binary classification: Twitter URL (Lan et al., 2017) was collected from tweets sharing the same URL with major news outlets such as @CNN.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.9265821576118469}]}, {"text": "This dataset keeps a balance between formal and informal language.", "labels": [], "entities": []}, {"text": "PIT-2015 () comes from the Task 1 of Semeval 2015 and was collected from tweets under the same trending topic, which contains varied topics and language styles.", "labels": [], "entities": [{"text": "PIT-2015", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.5592216849327087}]}, {"text": "MSRP () was derived from clustered news articles reporting the same event informal language.", "labels": [], "entities": [{"text": "MSRP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7228916883468628}]}, {"text": "shows vital statistics for all three datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of three benchmark datasets for paraphrase identification. The training and testing sizes are in  numbers of sentence pairs. The number of unique in-vocabulary (INV) and out-of-vocabulary (OOV) words are  calculated based on the publicly available GloVe embeddings (details in Section 3.2).", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.9668495059013367}]}, {"text": " Table 2: Results in F1 scores on Twitter-URL, PIT-2015 and MSRP datasets. The best performance figure in each  dataset is denoted in bold typeface and the second best is denoted by an underline. Without using any pretrained  word embeddings, the Subword+LM models achieve better or competitive performance compared to word models.", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9954902529716492}, {"text": "PIT-2015", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8853147029876709}, {"text": "MSRP datasets", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.9295792877674103}]}, {"text": " Table 4: Comparison of F1 scores between the original PWI model with 19-layer CNN for aggregation and the  simplified model without 19-layer CNN on Twitter-URL, PIT-2015 and MSRP datasets. The number of parameters  and training time per epoch shown are based on the Twitter URL dataset and a single NVIDIA Pascal P100 GPU.", "labels": [], "entities": [{"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9986913800239563}, {"text": "MSRP datasets", "start_pos": 175, "end_pos": 188, "type": "DATASET", "confidence": 0.87704136967659}, {"text": "Twitter URL dataset", "start_pos": 267, "end_pos": 286, "type": "DATASET", "confidence": 0.7640255292256674}]}, {"text": " Table 5: Character and word overlap comparison.", "labels": [], "entities": [{"text": "Character and word overlap comparison", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.6688291370868683}]}]}