{"title": [], "abstractContent": [{"text": "This work introduces anew problem, rela-tional summarization, in which the goal is to generate a natural language summary of the relationship between two lexical items in a corpus , without reference to a knowledge base.", "labels": [], "entities": [{"text": "rela-tional summarization", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.619985818862915}]}, {"text": "Motivated by the needs of novel user interfaces , we define the task and give examples of its application.", "labels": [], "entities": []}, {"text": "We also present anew query-focused method for finding natural language sentences which express relationships.", "labels": [], "entities": []}, {"text": "Our method allows for summarization of more than two times more query pairs than baseline relation extractors, while returning measurably more readable output.", "labels": [], "entities": [{"text": "summarization", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.987514317035675}]}, {"text": "Finally, to help guide future work, we analyze the challenges of re-lational summarization using both a news and asocial media corpus.", "labels": [], "entities": [{"text": "re-lational summarization", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.5164008587598801}]}], "introductionContent": [{"text": "Research on automatic summarization aims to help users understand large document sets.", "labels": [], "entities": [{"text": "summarization", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.7958189249038696}]}, {"text": "However, the details of how textual summaries might actually be presented to users are often ignored.", "labels": [], "entities": []}, {"text": "We propose that user interfaces which display noteworthy terms or concepts present the need for relational summaries: descriptions of the relationship between two entities or noun phrases from a corpus.", "labels": [], "entities": []}, {"text": "Examples of such interfaces include: commandline software for examining noteworthy terms or phrases, point-and-click browsers which display named entities and their interconnections on a network diagram (, concept map browsers) and document search engines which suggest terms relevant to a query, such as the related searches displayed on Wikipedia info boxes from Google.", "labels": [], "entities": []}, {"text": "In all such settings a natural question arises: what is the nature of the relationship between the entities or concepts shown in the interface?", "labels": [], "entities": []}, {"text": "One particular interface which presents the need fora relational summary is shown in.", "labels": [], "entities": []}, {"text": "Relational questions are ubiquitous and varied.", "labels": [], "entities": []}, {"text": "What is the relationship between the \"City of London\" and \"goaldelivery of Newgate\" in 18th century court records?", "labels": [], "entities": [{"text": "City of London\"", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.8241177946329117}, {"text": "Newgate\" in 18th century court records", "start_pos": 75, "end_pos": 113, "type": "DATASET", "confidence": 0.8179987839290074}]}, {"text": "What is the relationship between \"Advanced Integrated Systems\" and \"United Arab Emirates\" in the Paradise Papers?", "labels": [], "entities": [{"text": "United Arab Emirates\" in the Paradise Papers", "start_pos": 68, "end_pos": 112, "type": "DATASET", "confidence": 0.6570374444127083}]}, {"text": "What does \"dad\" have to do with \"mom\" on the subreddit discussion forum Relationship Advice?", "labels": [], "entities": []}, {"text": "This study seeks to answer such questions by examining the problem of relational summarization, which lies at the intersection of prior work on summarization and relation extraction.", "labels": [], "entities": [{"text": "relational summarization", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.7300575375556946}, {"text": "summarization", "start_pos": 144, "end_pos": 157, "type": "TASK", "confidence": 0.9693655371665955}, {"text": "relation extraction", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.8185414671897888}]}, {"text": "Unlike previous efforts at summarizing relationships, our approach focuses on answering user queries about the connections between two particular terms, without ref- Figure 2: A relational summary is a synopsis of all sentences which mention two terms, denoted (t 1 ) and (t 2 ).", "labels": [], "entities": [{"text": "summarizing relationships", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.917250394821167}]}, {"text": "We refer to such sentences as a mention set.", "labels": [], "entities": []}, {"text": "In the figure above (t 1 ) is Jean-Bertrand Aristide and (t 2 ) is United States.", "labels": [], "entities": []}, {"text": "To create a summary first requires identifying all statements in the mention set which coherently describe some relationship between (t 1 ) and (t 2 ).", "labels": [], "entities": []}, {"text": "This candidate set generation task is a prerequisite for the subsequent summary construction task: selecting the top K candidates to create a summary.", "labels": [], "entities": []}, {"text": "In this work, we offer a method for the first task and show how the second task will likely require a diversity of summarization techniques ( \u00a76).", "labels": [], "entities": []}, {"text": "erencing a knowledge graph (.", "labels": [], "entities": []}, {"text": "In order to answer such queries we: \u2022 Formally define the problem ( \u00a72), which we divide into two subtasks: candidate set generation and summary construction.", "labels": [], "entities": [{"text": "candidate set generation", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.6286510527133942}, {"text": "summary construction", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.6856954097747803}]}, {"text": "\u2022 Provide anew method for the candidate set generation task ( \u00a74), which we show outperforms baseline relation extraction techniques ( \u00a75) in terms of readability and yield.", "labels": [], "entities": [{"text": "candidate set generation task", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.6966466754674911}, {"text": "relation extraction", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7425318658351898}]}, {"text": "\u2022 Analyze the summary construction task for future work ( \u00a76), demonstrating that different summarization techniques are likely most appropriate for different mention sets.", "labels": [], "entities": [{"text": "summary construction task", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.7985403140385946}]}], "datasetContent": [{"text": "Any relational summarization system should deliver a high-quality summary when a user queries for two terms.", "labels": [], "entities": [{"text": "relational summarization", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8188317716121674}]}, {"text": "Therefore, ideally, a system should generate the largest possible candidate set, without returning incoherent relation statements.", "labels": [], "entities": []}, {"text": "We thus Enhanced dependencies allow fora token to have more than one incoming edge (i.e., multiple parents).", "labels": [], "entities": []}, {"text": "If there is more than one incoming edge, we pick an edge at random.", "labels": [], "entities": []}, {"text": "evaluate our query-focused generation method in terms of both readability and yield (total relation statements recalled).", "labels": [], "entities": [{"text": "yield", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9712168574333191}]}, {"text": "Our method generates three times more relation statements than OpenIE systems, allowing for summarization of two times more query pairs.", "labels": [], "entities": [{"text": "summarization", "start_pos": 92, "end_pos": 105, "type": "TASK", "confidence": 0.9803206920623779}]}, {"text": "We also achieve higher scores in a test of human coherence judgements.", "labels": [], "entities": []}, {"text": "More concretely, we evaluate our compressionbased method for generating candidate sets against two relation extractor baselines on two very different corpora: (1) all comments from the large \"relationships\" 11 subreddit from June, 2015 -September, 2017 and a collection of New York Times articles from 1987 to 2007 which mention the country \"Haiti\".", "labels": [], "entities": []}, {"text": "For each corpus, we first find a collection of multiword phrases using the phrasemachine package () which extracts all multiword, noun phrase terms from the corpus.", "labels": [], "entities": []}, {"text": "After extracting all terms, we determine the top 100 terms, by count.", "labels": [], "entities": []}, {"text": "We then examine all nonempty mention sets for all possible combinations of two top terms.", "labels": [], "entities": []}, {"text": "A mention set is a set of sentences which mention two terms ( \u00a72).", "labels": [], "entities": []}, {"text": "We examine all mention sets because an investigator should be able to investigate any entity she chooses while analyzing a corpus.", "labels": [], "entities": []}, {"text": "In subsequent experiments, we require all relation statements be less than or equal to J = 75 characters, which excludes overly verbose relation statements which are unsuitable for many user interfaces.", "labels": [], "entities": []}, {"text": "Off-the-shelf relation extractors generate 3-tuples from each mention set.", "labels": [], "entities": []}, {"text": "Some of those 3-tuples might have one argument which is equal to (t 1 ) and another argument which is equal to (t 2 ).", "labels": [], "entities": []}, {"text": "Each such 3-tuple can be linearized into a string of the form (t 1 ) r (t 2 ) to generate a candidate set.", "labels": [], "entities": []}, {"text": "However, we find that using extractors in this manner achieves a low yield (total number of extracted relations).", "labels": [], "entities": []}, {"text": "A low yield is undesirable both because it limits the number of mention sets which maybe summarized and generates fewer relation statements from which to select an optimal relational summary.", "labels": [], "entities": []}, {"text": "More precisely, we identify the 3-tuples which an OpenIE system extracts from a mention set such that exactly one argument from the triple is equal 13 to (t 1 ) and exactly one argument from the triple is equal to (t 2 ).", "labels": [], "entities": []}, {"text": "We refer to these 3-tuples as \"matching\".", "labels": [], "entities": []}, {"text": "We then count (1) the total number of mention sets which contain at least one matching 3-tuple and (2) the total number matching 3-tuples across all mention sets.", "labels": [], "entities": []}, {"text": "We refer to such counts as the yield of a candidate generation system.", "labels": [], "entities": []}, {"text": "We measure the yield of Stanford OpenIE and) on the New York Times and Reddit corpora, and compare each system to our compression-based approach ( \u00a74).", "labels": [], "entities": [{"text": "Stanford OpenIE", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.742451548576355}, {"text": "New York Times and Reddit corpora", "start_pos": 52, "end_pos": 85, "type": "DATASET", "confidence": 0.8233877221743265}]}, {"text": "We measure these two relation extractors because Stanford OpenIE is included with the popular CoreNLP software and ClausIE achieves the highest recall in two systematic studies of relation extractors).", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8148855268955231}, {"text": "ClausIE", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.7086179852485657}, {"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9992247819900513}, {"text": "relation extractors", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.7494098842144012}]}, {"text": "We find that, for the great majority of sentences, relation extractors do not extract any relations between (t 1 ) and (t 2 ).", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7097362726926804}]}, {"text": "Moreover, for many mention sets, the number of relations extracted with off-the-shelf systems is often zero.", "labels": [], "entities": []}, {"text": "We show these results in.", "labels": [], "entities": []}, {"text": "This suggests that although relation summarization is superficially similar to relation extraction, off-the-shelf extractors are poor tools for creating textual units to summarize mention sets.", "labels": [], "entities": [{"text": "relation summarization", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.8433352410793304}, {"text": "relation extraction", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.8415417969226837}, {"text": "summarize mention sets", "start_pos": 170, "end_pos": 192, "type": "TASK", "confidence": 0.8806951840718588}]}, {"text": "Very often, two terms are related to each other in ways which are simply not captured by relation extractors.", "labels": [], "entities": []}, {"text": "Note that OpenIE systems might not extract the literal string (t1) or (t2) as arguments.", "labels": [], "entities": []}, {"text": "For instance, if (t1) is \"Merkel\" the OpenIE system might extract the argument \"Angela Merkel\".", "labels": [], "entities": []}, {"text": "If some term and some argument from a relational triple share the same head token in the dependency parse of the sentence we say that they are equal.", "labels": [], "entities": []}, {"text": "Falke and Gurevych (2017c) employ a similar equality criterion.", "labels": [], "entities": []}, {"text": "In extremely rare cases, tokenization mismatches between CoreNLP and ClausIE make it impossible to apply this criterion.", "labels": [], "entities": [{"text": "ClausIE", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.8858137130737305}]}, {"text": "For our compression-based approach, we count all cases where p(c = 1 | s, (t1) r (t2)) > .5 as extracting a relation statement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Highest and lowest coherence predictions from the set United States -Jean-Bertrand Aristide", "labels": [], "entities": [{"text": "United States -Jean-Bertrand Aristide", "start_pos": 64, "end_pos": 101, "type": "DATASET", "confidence": 0.7426363229751587}]}, {"text": " Table 5: We compare Stanford OpenIE, ClausIE and our headline-based compression method for the candidate set  generation task on two different corpora (Haiti articles from New York Times, and the Reddit relationships forum)  in terms of (1) how many entity pairs have a non-empty candidate set, (2) how many total relation statements are  generated, and (3) the average human judgment of acceptability ( \u00a75.2). For yield measures, the upper bound on  the left shows the total number of non-empty entity pairs (i.e. how many pairs actually cooccur in at least one  sentence, out of all  100", "labels": [], "entities": [{"text": "ClausIE", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9216387867927551}, {"text": "candidate set  generation task", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.7472363114356995}]}]}