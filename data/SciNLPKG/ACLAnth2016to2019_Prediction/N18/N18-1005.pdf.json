{"title": [{"text": "Fortification of Neural Morphological Segmentation Models for Polysynthetic Minimal-Resource Languages", "labels": [], "entities": [{"text": "Fortification of Neural Morphological Segmentation", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.652734911441803}]}], "abstractContent": [{"text": "Morphological segmentation for polysynthetic languages is challenging, because a word may consist of many individual morphemes and training data can be extremely scarce.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9549501836299896}]}, {"text": "Since neural sequence-to-sequence (seq2seq) models define the state of the art for morphological segmentation in high-resource settings and for (mostly) European languages, we first show that they also obtain competitive performance for Mexican polysynthetic languages in minimal-resource settings.", "labels": [], "entities": []}, {"text": "We then propose two novel multi-task training approaches-one with, one without need for external un-labeled resources-, and two corresponding data augmentation methods, improving over the neural baseline for all languages.", "labels": [], "entities": []}, {"text": "Finally, we explore cross-lingual transfer as a third way to fortify our neural model and show that we can train one single multilingual model for related languages while maintaining comparable or even improved performance, thus reducing the amount of parameters by close to 75%.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.765855461359024}]}, {"text": "We provide our morphological segmentation datasets for Mexicanero, Nahuatl, Wixarika and Yorem Nokki for future research.", "labels": [], "entities": [{"text": "Mexicanero", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.9237146377563477}]}], "introductionContent": [{"text": "Due to the advent of computing technologies to indigenous communities allover the world, natural language processing (NLP) applications * *The first two authors contributed equally.", "labels": [], "entities": []}, {"text": "for languages with limited computer-readable textual data are getting increasingly important.", "labels": [], "entities": []}, {"text": "This contrasts with current research, which focuses strongly on approaches which require large amounts of training data, e.g., deep neural networks.", "labels": [], "entities": []}, {"text": "Those are not trivially applicable to minimal-resource settings with less than 1, 000 available training examples.", "labels": [], "entities": []}, {"text": "We aim at closing this gap for morphological surface segmentation, the task of splitting a word into the surface forms of its smallest meaning-bearing units, its morphemes.", "labels": [], "entities": [{"text": "morphological surface segmentation", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.6719614267349243}]}, {"text": "Recovering morphemes provides information about unknown words and is thus especially important for polysynthetic languages with a high morpheme-to-word ratio and a consequently large overall number of words.", "labels": [], "entities": [{"text": "Recovering morphemes", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8943086564540863}]}, {"text": "To illustrate how segmentation helps understanding unknown multiplemorpheme words, consider an example in this paper's language of writing: even if the word unconditionally did not appear in a given training corpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly.", "labels": [], "entities": []}, {"text": "Due to its importance for down-stream tasks (, segmentation has been tackled in many different ways, considering unsupervised), supervised) and semisupervised settings).", "labels": [], "entities": [{"text": "segmentation", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.9617338180541992}]}, {"text": "Here, we add three new questions to this line of research: (i) Are data-hungry neural network models applicable to segmentation of polysynthetic languages in minimal-resource settings?", "labels": [], "entities": []}, {"text": "(ii) How can the performance of neural networks for surface segmentation be improved if we have only unlabeled or no external data at hand?", "labels": [], "entities": [{"text": "surface segmentation", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7555128037929535}]}, {"text": "(iii) Is crosslingual transfer for this task possible between related languages?", "labels": [], "entities": [{"text": "crosslingual transfer", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.7544762194156647}]}, {"text": "The last two questions are crucial: While for many languages it is difficult to obtain the number of annotated examples used in earlier work on (semi-)supervised methods, a limited amount might still be obtainable.", "labels": [], "entities": []}, {"text": "We experiment on four polysynthetic Mexican languages: Mexicanero, Nahuatl, Wixarika and Yorem Nokki (details in \u00a72).", "labels": [], "entities": []}, {"text": "The datasets we use are, as far as we know, the first computer-readable datasets annotated for morphological segmentation in those languages.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 95, "end_pos": 121, "type": "TASK", "confidence": 0.7653172016143799}]}, {"text": "Our experiments show that neural seq2seq models perform on par with or better than other strong baselines for our polysynthetic languages in a minimal-resource setting.", "labels": [], "entities": []}, {"text": "However, we further propose two novel multi-task approaches and two new data augmentation methods.", "labels": [], "entities": []}, {"text": "Combining them with our neural model yields up to 5.05% absolute accuracy or 3.40% F1 improvements over our strongest baseline.", "labels": [], "entities": [{"text": "absolute", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.947011411190033}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9171385765075684}, {"text": "F1", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9997569918632507}]}, {"text": "Finally, following earlier work on cross-lingual knowledge transfer for seq2seq tasks, we investigate training one single model for all languages, while sharing parameters.", "labels": [], "entities": [{"text": "cross-lingual knowledge transfer", "start_pos": 35, "end_pos": 67, "type": "TASK", "confidence": 0.6303262412548065}]}, {"text": "The resulting model performs comparably to or better than the individual models, but requires only roughly as many parameters as one single model.", "labels": [], "entities": []}, {"text": "To sum up, we make the following contributions: (i) we confirm the applicability of neural seq2seq models to morphological segmentation of polysynthetic languages in minimalresource settings; (ii) we propose two novel multi-task training approaches and two novel data augmentation methods for neural segmentation models; (iii) we investigate the effectiveness of cross-lingual transfer between related languages; and (iv) we provide morphological segmentation datasets for Mexicanero, Nahuatl, Wixarika and Yorem Nokki.", "labels": [], "entities": [{"text": "cross-lingual transfer between related languages", "start_pos": 363, "end_pos": 411, "type": "TASK", "confidence": 0.805505120754242}]}], "datasetContent": [{"text": "To create our datasets, we make use of both segmentable (i.e., consisting of multiple morphemes) and non-segmentable (i.e., consisting of one single morpheme) words described in books of the collection Archive of Indigenous Languages in Mexicanero), Nahuatl (Lastra, Wixarika (), and Yorem Nokki.", "labels": [], "entities": []}, {"text": "Statistics about the data in the four languages are displayed in Tables 1, 2 and 3.", "labels": [], "entities": []}, {"text": "We include segmentable as well as non-segmentable words into our datasets in order to ensure that our methods can correctly decide against splitting up single morphemes.", "labels": [], "entities": []}, {"text": "The phrases in all languages are mostly parallel, such that the corpora are roughly equivalent.", "labels": [], "entities": []}, {"text": "Therefore, we can compare the morphology of translated words (cf.), noticing that the language with most agglutination is Wixarika, with an average rate of 3.25 morphemes per word; the other languages have an average of close to 2.2 morphemes per word.", "labels": [], "entities": []}, {"text": "This higher morphological complexity naturally produces data sparsity at the token level.", "labels": [], "entities": []}, {"text": "Also, we can notice that Wixarika has more unique words than the rest of our studied languages.", "labels": [], "entities": []}, {"text": "However, Nahuatl has with 810 the highest number of unique morphemes.", "labels": [], "entities": []}, {"text": "In order to make follow-up work on minimal-resource settings for morphological segmentation easily comparable, we provide predefined splits of our datasets   use 20% for development and the rest for training.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.8117108047008514}]}, {"text": "The final numbers of words per dataset and language are shown in.", "labels": [], "entities": []}, {"text": "First, we evaluate using accuracy on the token level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9994176626205444}]}, {"text": "Thus, an example counts as correct if and only if the output of the system matches the reference solution exactly, i.e., if all output symbols are predicted correctly.", "labels": [], "entities": []}, {"text": "Our second evaluation metric is border F1, which measures how many segment boundaries are predicted correctly by the model.", "labels": [], "entities": [{"text": "border F1", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.7821492552757263}]}, {"text": "While we use this metric because it is common for segmentation tasks, it is not ideal for our models since those are not guaranteed to preserve the input character sequence.", "labels": [], "entities": [{"text": "segmentation tasks", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9112439453601837}]}, {"text": "We handle this problem as follows: In order to compare borders, we identify them by the position of their preceding letter, i.e., if in both the model's guess and the gold solution a segment border appears after the second character, it counts as correct.", "labels": [], "entities": []}, {"text": "Note that this comes with the disadvantage of erroneously inserted characters leading to all subsequent segment borders being counted as incorrect.", "labels": [], "entities": []}, {"text": "shows that accuracy and F1 seem to be highly correlated for our task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9996950626373291}, {"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.999762237071991}]}, {"text": "The test results also give an answer to our first research question: The neural model S2S performs on par with CRF, the strongest baseline, for all languages but Nahuatl.", "labels": [], "entities": []}, {"text": "Further, S2S and CRF both outperform MORF and FC by a wide margin.", "labels": [], "entities": [{"text": "MORF", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.5379930734634399}, {"text": "FC", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.6362667083740234}]}, {"text": "We may thus conclude that neural models are indeed applicable to segmentation of polysynthetic languages in a low-resource setting.", "labels": [], "entities": []}, {"text": "We keep all model parameters and the training regime as described in \u00a76.3.", "labels": [], "entities": []}, {"text": "However, our training data now consists of a combination of all available training data for all 4 languages.", "labels": [], "entities": []}, {"text": "In order to enable the model to differentiate between the tasks, M-Lang S-Lang BestMTT BestDA Mex.", "labels": [], "entities": [{"text": "M-Lang S-Lang BestMTT BestDA Mex", "start_pos": 65, "end_pos": 97, "type": "DATASET", "confidence": 0.59600909948349}]}, {"text": ".: Accuracies of our model trained on all languages (M-Lang) and the models trained on single languages (S-Lang).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 3, "end_pos": 13, "type": "METRIC", "confidence": 0.9870119690895081}]}, {"text": "The highest multi-task and data augmentation accuracies are repeated for an easy comparison.", "labels": [], "entities": []}, {"text": "we prepend one language-specific input symbol to each instance.", "labels": [], "entities": []}, {"text": "This corresponds to having one embedding in the input which marks the task.", "labels": [], "entities": []}, {"text": "An example training instance for Yorem Nokki is where L=YN indicates the language.", "labels": [], "entities": []}, {"text": "Due to the previous high correlation between accuracy and F1 we only use accuracy on the word level as the evaluation metric for this experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9992569088935852}, {"text": "F1", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9993915557861328}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9979380965232849}]}], "tableCaptions": [{"text": " Table 1: The most frequent morphs (m.) together with  their frequencies (frq.) in our datasets.", "labels": [], "entities": []}, {"text": " Table 3: Number of words, segmentable words (Seg- Words), total morphs (Morphs), and unique morphs  (UniMorphs) in our datasets. Seg/W: proportion  of words consisting or more than one morpheme;  Morphs/W: morphemes per word; MaxMorphs: maxi- mum number of morphemes found in one word.", "labels": [], "entities": [{"text": "maxi- mum number", "start_pos": 238, "end_pos": 254, "type": "METRIC", "confidence": 0.8295396119356155}]}, {"text": " Table 4: Performances of our multi-task and data augmentation approaches compared to all baselines described in  the text. The reported results for neural models are averages over 5 training runs. Best results per language and  metric are in bold.", "labels": [], "entities": []}, {"text": " Table 5: Accuracies of our model trained on all lan- guages (M-Lang) and the models trained on single lan- guages (S-Lang). The highest multi-task and data aug- mentation accuracies are repeated for an easy compar- ison.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9987000226974487}]}]}