{"title": [{"text": "SRL4ORL: Improving Opinion Role Labeling Using Multi-Task Learning With Semantic Role Labeling", "labels": [], "entities": [{"text": "SRL4ORL", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7310923933982849}, {"text": "Improving Opinion Role Labeling", "start_pos": 9, "end_pos": 40, "type": "TASK", "confidence": 0.7627311870455742}, {"text": "Semantic Role Labeling", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.622999886671702}]}], "abstractContent": [{"text": "For over a decade, machine learning has been used to extract opinion-holder-target structures from text to answer the question Who expressed what kind of sentiment towards what?.", "labels": [], "entities": []}, {"text": "Recent neural approaches do not outperform the state-of-the-art feature-based models for Opinion Role Labeling (ORL).", "labels": [], "entities": [{"text": "Opinion Role Labeling (ORL)", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.7696894109249115}]}, {"text": "We suspect this is due to the scarcity of labeled training data and address this issue using different multi-task learning (MTL) techniques with a related task which has substantially more data, i.e. Semantic Role Labeling (SRL).", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 200, "end_pos": 228, "type": "TASK", "confidence": 0.7720784942309061}]}, {"text": "We show that two MTL models improve significantly over the single-task model for labeling of both holders and targets, on the development and the test sets.", "labels": [], "entities": [{"text": "MTL", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9312664866447449}]}, {"text": "We found that the vanilla MTL model which makes predictions using only shared ORL and SRL features, performs the best.", "labels": [], "entities": []}, {"text": "With deeper analysis we determine what works and what might be done to make further improvements for ORL.", "labels": [], "entities": [{"text": "ORL", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.4513149559497833}]}], "introductionContent": [{"text": "Fine-Grained Opinion Analysis (FGOA) aims to: (i) detect opinion expressions (O) that convey attitudes such as sentiments, agreements, beliefs or intentions (like feared in example (1)), (ii) measure their intensity (e.g. strong), (iii) identify their holders (H), i.e. entities that express an attitude (e.g. it), (iv) identify their targets (T), i.e. entities or propositions at which the attitude is directed (e.g. violence) and (v) classify their targetdependent attitude (e.g. negative sentiment) 1 . (1) Australia said H [feared] T if voters thought the election had been stolen.", "labels": [], "entities": [{"text": "Fine-Grained Opinion Analysis (FGOA)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7856219013532003}]}, {"text": "As the commonly accepted benchmark corpus MPQA () uses span-based annotations to represent opinion entities (opinions, Examples are drawn from MPQA ().", "labels": [], "entities": [{"text": "MPQA", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9002060890197754}, {"text": "MPQA", "start_pos": 143, "end_pos": 147, "type": "DATASET", "confidence": 0.947638750076294}]}, {"text": "holders and targets), the task is usually approached with sequence labeling techniques and the BIO encoding scheme (.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.5956258177757263}, {"text": "BIO", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.6794160604476929}]}, {"text": "Initially pipeline models were proposed which first predict opinion expressions and then, given an opinion, label its opinion roles, i.e. holders and targets ().", "labels": [], "entities": []}, {"text": "Pipeline models have been substituted with so-called joint models that simultaneously identify all opinion entities, and predict which opinion role is related to which opinion (.", "labels": [], "entities": []}, {"text": "Recently an LSTM-based joint model was proposed) that unlike the prior work ( does not depend on external resources (such as syntactic parsers or named entity recognizers).", "labels": [], "entities": []}, {"text": "The neural variant does not outperform the feature-based CRF model) in Opinion Role Labeling (ORL).", "labels": [], "entities": [{"text": "Opinion Role Labeling (ORL)", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.7476423581441244}]}, {"text": "Both the neural and the CRF joint models achieve about 55% F1 score for predicting which targets relate to which opinions in MPQA.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9855776131153107}]}, {"text": "Thus, these models are not yet ready to answer the question this line of research is usually motivated with: Who expressed what kind of sentiment towards what?.", "labels": [], "entities": []}, {"text": "Our goal is to investigate the limitations of neural models in solving different subtasks of FGOA on MPQA and to gain a better understanding of what is solved and what is next.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.7690404653549194}]}, {"text": "We suspect that one of the fundamental obstacles for neural models trained on MPQA is its small size.", "labels": [], "entities": []}, {"text": "One way to address scarcity of labeled data is to use multi-task learning (MTL) with appropriate auxiliary tasks.", "labels": [], "entities": []}, {"text": "A promising auxiliary task candidate for ORL is Semantic Role Labeling (SRL), the task of predicting predicate-argument structure of a sentence, which answers the question Who did what to whom, where and when?.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.7959623833497366}, {"text": "predicting predicate-argument structure of a sentence", "start_pos": 90, "end_pos": 143, "type": "TASK", "confidence": 0.8880181312561035}]}, {"text": "--A0 -A1 AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADVthink.: Output of the SRL demo.", "labels": [], "entities": [{"text": "Output", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9267070889472961}, {"text": "SRL demo", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.8067988753318787}]}, {"text": "illustrates the output of the SRL demo 2 for example (1), following the PropBank SRL scheme) . SRL4ORL.", "labels": [], "entities": [{"text": "SRL demo 2", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.7573310732841492}, {"text": "PropBank SRL scheme", "start_pos": 72, "end_pos": 91, "type": "DATASET", "confidence": 0.9363191922505697}, {"text": "SRL4ORL", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9166169762611389}]}, {"text": "The semantic roles of the predicate fear (marked blue bold) correspond to the opinion roles H and T, according to MPQA.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.9787762761116028}]}, {"text": "For this reason, the output of SRL systems has been commonly used for feature-based FGOA models.", "labels": [], "entities": []}, {"text": "Additionally, a considerable amount of training data is available for training SRL models in Sec.", "labels": [], "entities": [{"text": "SRL", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9704616069793701}]}, {"text": "3), which made neural SRL models successful (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.7593889236450195}]}, {"text": "Although SRL is similar in nature to ORL, it cannot solve ORL for all cases.", "labels": [], "entities": [{"text": "SRL", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.8430124521255493}]}, {"text": "In example (2) holder and target of the predicate please correspond to A1, A0 semantic roles respectively, wheres for the predicate fear in (1) holder and target correspond to A0, A1 respectively.", "labels": [], "entities": [{"text": "A1", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9675039052963257}]}, {"text": "We took into account this observation when deciding on an appropriate MTL model by splitting its parameters into shared and task-specific ones (i.e. hard-parameter sharing).", "labels": [], "entities": [{"text": "MTL", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9652867317199707}]}], "datasetContent": [{"text": "For both tasks we adopt evaluation metrics from prior work.", "labels": [], "entities": []}, {"text": "For SRL, precision is defined as the proportion of semantic roles predicted by a system which are correct, recall is the proportion of gold roles which are predicted by a system, F1 score is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "SRL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9813215136528015}, {"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.999204695224762}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9989161491394043}, {"text": "F1 score", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9910933673381805}, {"text": "precision", "start_pos": 212, "end_pos": 221, "type": "METRIC", "confidence": 0.9988665580749512}, {"text": "recall", "start_pos": 226, "end_pos": 232, "type": "METRIC", "confidence": 0.993002712726593}]}, {"text": "In case of ORL, we report 10-fold CV and repeated 4-fold CV with binary F1 score and proportional F1 score, for holders and targets separately.", "labels": [], "entities": [{"text": "ORL", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.7865031957626343}, {"text": "F1 score", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9510734379291534}, {"text": "F1 score", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9619689583778381}]}, {"text": "Binary precision is defined as the proportion of predicted holders (targets) that overlap with the gold holder (target), binary recall is the proportion of gold holders (targets) for which the model predicts an overlapping holder (target).", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.5662265419960022}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.7203969955444336}]}, {"text": "Proportional recall measures the proportion of the overlap between a gold holder (target) and an overlapping predicted holder (target), proportional precision measures the proportion of the overlap between a predicted holder (target) and an overlapping gold holder (target).", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9008855223655701}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.7991340160369873}]}, {"text": "F1 scores are the harmonic means of the corresponding precision and recall.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9729903638362885}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9993606209754944}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.997088611125946}]}], "tableCaptions": [{"text": " Table 2: Datasets w/ nb. of SRL predicates/ORL opin- ions in train, dev & test set, size of label inventory.", "labels": [], "entities": [{"text": "ORL", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9679636359214783}]}, {"text": " Table 3: ORL 10-fold CV results.", "labels": [], "entities": [{"text": "ORL 10-fold CV", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9180228511492411}]}, {"text": " Table 4: ORL repeated 4-fold CV results.", "labels": [], "entities": [{"text": "ORL", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9925469756126404}]}, {"text": " Table 6: The dev examples for which both models (FS-MTL, Z&X-STL) correctly predict the target in 6/8 trials.", "labels": [], "entities": [{"text": "FS-MTL", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.5410073399543762}]}, {"text": " Table 8: Statistics of target prediction.", "labels": [], "entities": [{"text": "target prediction", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7495300769805908}]}]}