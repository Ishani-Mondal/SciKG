{"title": [{"text": "SHAPED: Shared-Private Encoder-Decoder for Text Style Adaptation", "labels": [], "entities": [{"text": "Text Style Adaptation", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7256040175755819}]}], "abstractContent": [{"text": "Supervised training of abstractive language generation models results in learning conditional probabilities over language sequences based on the supervised training signal.", "labels": [], "entities": [{"text": "abstractive language generation", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.6277639071146647}]}, {"text": "When the training signal contains a variety of writing styles, such models may end up learning an 'average' style that is directly influenced by the training data make-up and cannot be controlled by the needs of an application.", "labels": [], "entities": []}, {"text": "We describe a family of model archi-tectures capable of capturing both generic language characteristics via shared model parameters , as well as particular style characteristics via private model parameters.", "labels": [], "entities": []}, {"text": "Such models are able to generate language according to a specific learned style, while still taking advantage of their power to model generic language phenomena.", "labels": [], "entities": []}, {"text": "Furthermore, we describe an extension that uses a mixture of output distributions from all learned styles to perform on-the-fly style adaptation based on the textual input alone.", "labels": [], "entities": []}, {"text": "Experimentally, we find that the proposed models consistently outperform models that encapsulate single-style or average-style language generation capabilities.", "labels": [], "entities": []}], "introductionContent": [{"text": "Encoder-decoder models have recently pushed forward the state-of-the-art performance on a variety of language generation tasks, including machine translation (), text summarization (, dialog systems (, and image captioning (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.8118245601654053}, {"text": "text summarization", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.7770584225654602}, {"text": "image captioning", "start_pos": 206, "end_pos": 222, "type": "TASK", "confidence": 0.714877188205719}]}, {"text": "This framework consists of an encoder that reads the input data and encodes it as a sequence of vectors, which is in turn used by a decoder to generate an- * Work done as an intern at Google AI.", "labels": [], "entities": []}, {"text": "other sequence of vectors used to produce output symbols step by step.", "labels": [], "entities": []}, {"text": "The prevalent approach to training such a model is to update all the model parameters using all the examples in the training data (over multiple epochs).", "labels": [], "entities": []}, {"text": "This is a reasonable approach, under the assumption that we are modeling a single underlying distribution in the data.", "labels": [], "entities": []}, {"text": "However, in many applications and for many natural language datasets, there exist multiple underlying distributions, characterizing a variety of language styles.", "labels": [], "entities": []}, {"text": "For instance, the widely-used Gigaword dataset ( consists of a collection of articles written by various publishers (The New York Times, Agence France Presse, Xinhua News, etc.), each with its own style characteristics.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.9349983632564545}, {"text": "Agence France Presse, Xinhua News", "start_pos": 137, "end_pos": 170, "type": "DATASET", "confidence": 0.810305098692576}]}, {"text": "Training a model's parameters on all the training examples results in an averaging effect across style characteristics, which may lower the quality of the outputs; additionally, this averaging effect maybe completely undesirable for applications that require a level of control over the output style.", "labels": [], "entities": []}, {"text": "At the opposite end of the spectrum, one can choose to train one independent model per each underlying distribution (assuming we have the appropriate signals for identifying them at training time).", "labels": [], "entities": []}, {"text": "This approach misses the opportunity to exploit common properties shared by these distributions (e.g., generic characteristics of a language, such as noun-adjective position), and leads to models that are under-trained due to limited data availability per distribution.", "labels": [], "entities": []}, {"text": "In order to address these issues, we propose a novel neural architecture called SHAPED (sharedprivate encoder-decoder).", "labels": [], "entities": []}, {"text": "This architecture has both shared encoder/decoder parameters that are updated based on all the training examples, as well as private encoder/decoder parameters that are updated using only examples from their corresponding underlying training distributions.", "labels": [], "entities": []}, {"text": "In addition to learning different parametrization between the shared model and the private models, we jointly learn a classifier to estimate the probability of each example belonging to each of the underlying training distributions.", "labels": [], "entities": []}, {"text": "In such a setting, the shared parameters ('shared model') are expected to learn characteristics shared by the entire set of training examples (i.e., language generic), whereas each private parameter set ('private model') learns particular characteristics (i.e., style specific) of their corresponding training distribution.", "labels": [], "entities": []}, {"text": "At the same time, the classifier is expected to learn a probability distribution over the labels used to identify the underlying distributions present in the input data.", "labels": [], "entities": []}, {"text": "At test time, there are two possible scenarios.", "labels": [], "entities": []}, {"text": "In the first one, the input signal explicitly contains information about the underlying distribution (e.g., the publisher's identity).", "labels": [], "entities": []}, {"text": "In this case, we feed the data into the shared model and also the corresponding private model, and perform sequence generation based on a concatenation of their vector outputs; we refer to this model as the SHAPED model.", "labels": [], "entities": [{"text": "sequence generation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.6889564841985703}]}, {"text": "Ina second scenario, the information about the underlying distribution is either not available, or it refers to a distribution that was not seen during training.", "labels": [], "entities": []}, {"text": "In this case, we feed the data into the shared model and all the private models; the output distribution of the symbols of the decoding sequence is estimated using a mixture of distributions from all the decoders, weighted according to the classifier's estimates for that particular example; we refer to this model as the Mix-SHAPED model.", "labels": [], "entities": []}, {"text": "We test our models on the headline-generation task based on the aforementioned Gigaword dataset.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 79, "end_pos": 95, "type": "DATASET", "confidence": 0.9802913963794708}]}, {"text": "When the publisher's identity is presented as part of the input, we show that the SHAPED model significantly surpasses the performance of the shared encoder-decoder baseline, as well as the performance of private models (where one individual, per-publisher model is trained for each in-domain style).", "labels": [], "entities": []}, {"text": "When the publisher's identity is not presented as part of the input (i.e., not presented at run-time but revealed at evaluation-time for measurement purposes), we show that the Mix-SHAPED model exhibits a high level of classification accuracy based on textual inputs alone (accuracy percentage in the 80s overall, varying by individual publisher), while its generation accuracy still surpasses the performance of the baseline models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 234, "end_pos": 242, "type": "METRIC", "confidence": 0.8514687418937683}, {"text": "accuracy", "start_pos": 274, "end_pos": 282, "type": "METRIC", "confidence": 0.997300922870636}, {"text": "accuracy", "start_pos": 369, "end_pos": 377, "type": "METRIC", "confidence": 0.8564210534095764}]}, {"text": "Finally, when the publisher's identity is unknown to the model (i.e., a publisher that was not part of the training dataset), we show that the Mix-SHAPED model performance far surpasses the shared model performance, due to the ability of the Mix-SHAPED model to perform on-the-fly adaptation of output style.", "labels": [], "entities": []}, {"text": "This feat comes from our model's ability to perform two distinct tasks: match the incoming, previously-unseen input style to existing styles learned at training time, and use the correlations learned at training time between input and output style characteristics to generate style-appropriate token sequences.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform a battery of quantitative experiments, designed to answer several main questions: 1) Do the proposed model improve generation performance over alternative approaches?", "labels": [], "entities": []}, {"text": "2) Can a style classifier built using an auxiliary loss provide a reliable estimate on text style?", "labels": [], "entities": []}, {"text": "3) In the case of unknown style, does the Mix-SHAPED model improve generation performance over alternative approaches?", "labels": [], "entities": []}, {"text": "4) To what extent do our models capture style characteristics as opposed to, say, content characteristics?", "labels": [], "entities": []}, {"text": "We perform our experiments using text summarization as the main task.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7329921424388885}]}, {"text": "More precisely, we train and evaluate headline generation models using the publicly-available Gigaword dataset ().", "labels": [], "entities": [{"text": "headline generation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8329786062240601}, {"text": "Gigaword dataset", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.9793302416801453}]}, {"text": "We compare the following models: \u2022 A Shared encoder-decoder model (S) trained on all styles in D; \u2022 A suite of Private encoder-decoder models (P), each one trained on a particular style from D = {AFP, APW, NYT, XIN}; 1 \u2022 A SHAPED model (SP) trained on all styles in D; attest time, the style of test data is provided to the model; the article is only run through its style-specific private network and shared network (style classifier is not needed); \u2022 A Mix-SHAPED model (M-SP) trained on all styles in D; attest time, the style of article is not provided to the model; the output is computed using the mixture model, with the estimated style probabilities from the style classifier used as weights.", "labels": [], "entities": []}, {"text": "When testing on the out-of-domain styles CNA/LTW, we only compare the Shared (S) model with the Mix-SHAPED (M-SP) model, as the others cannot properly handle this scenario.", "labels": [], "entities": []}, {"text": "As hyper-parameters for the model instantiation, we used 500-dimension word embeddings, and a three-layer, 500-dimension GRU-cell RNN architecture; the encoder was instantiated as a bidirectional RNN.", "labels": [], "entities": []}, {"text": "The lengths of the input and output sequences were truncated to 40 and 20 tokens, respectively.", "labels": [], "entities": []}, {"text": "All the models were optimized using Adagrad, with an initial learning rate of 0.01.", "labels": [], "entities": []}, {"text": "The training procedure was done over mini-batches of size 128, and the updates were done asynchronously across 40 workers for 5M steps.", "labels": [], "entities": []}, {"text": "The encoder/decoder word embedding and the output projection matrices were tied to minimize the number of parameters.", "labels": [], "entities": []}, {"text": "To avoid the slowness from the softmax operator overlarge vocabulary sizes, and also mitigate the impact of outof-vocabulary tokens, we applied a subtokenization method ( , which invertibly transforms a native token into a sequence of subtokens from a limited vocabulary (here set to 32K).", "labels": [], "entities": []}, {"text": "Comparison with Previous Work In the next section, we report our main results using the indomain and out-of-domain (w.r.t. the selected publisher styles) test sets described above, since these test sets have a balanced publisher style frequency that allows us to measure the impact of our style-adaptation models.", "labels": [], "entities": []}, {"text": "However, we also report here the performance of our Shared (S) baseline model (with the above hyper-parameters) on the original 2K test set used in.", "labels": [], "entities": [{"text": "2K test set", "start_pos": 128, "end_pos": 139, "type": "DATASET", "confidence": 0.8637033502260844}]}, {"text": "On that test set, our S model obtains 30.13 F1 ROUGE-L score, compared to 28.34 ROUGE-L obtained by the ABS+ model (, and 30.64 ROUGE-L obtained by the words-lvt2k-1sent model (.", "labels": [], "entities": [{"text": "F1 ROUGE-L score", "start_pos": 44, "end_pos": 60, "type": "METRIC", "confidence": 0.8794037302335104}, {"text": "ROUGE-L", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9680018424987793}, {"text": "ABS+ model", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.8862486680348715}]}, {"text": "This comparison indicates that our S model is a competitive baseline, making the comparisons against the SP and M-SP models meaningful when using our indomain and out-of-domain test sets.", "labels": [], "entities": []}, {"text": "Model capacity In order to remove the possibility that the improved performance of the SP model is due simply to an increased model size compared to the S model, we perform an experiment in which we triple the size of the GRU cell dimensions for the S model.", "labels": [], "entities": []}, {"text": "However, we find no significant performance difference compared to the   Style embedding A competitive approach to modeling different styles is to directly encode the style information into the embedding space.", "labels": [], "entities": []}, {"text": "In , the style label is converted into a one-hot vector and is concatenated with the word embedding at each time step in the S model.", "labels": [], "entities": []}, {"text": "The outputs of this model are at 36.68 ROUGE-L, slightly higher than the baseline S model, but significantly lower than the SP model performance (37.52 ROUGE-L).", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9962111711502075}, {"text": "ROUGE-L", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.956851601600647}]}, {"text": "Another style embedding approach is to augment the S model with continuous trainable style embeddings for each predefined style label, similar to).", "labels": [], "entities": []}, {"text": "The resulting outputs achieve 37.2 ROUGE-L, which is better than the S model with one-hot style embedding, but still worse than the SP method (statistically significant at p-value=0.025 using paired t-test).", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9991685152053833}]}, {"text": "However, neither of these approaches apply to the cases when the style is out-of-domain or unknown during testing.", "labels": [], "entities": []}, {"text": "In contrast, such cases are handled naturally by the proposed M-SP model.", "labels": [], "entities": []}, {"text": "Ensemble model Another question is whether the SP model simply benefits from ensembling multiple models rather than style adaptation.", "labels": [], "entities": [{"text": "style adaptation", "start_pos": 116, "end_pos": 132, "type": "TASK", "confidence": 0.6753187030553818}]}, {"text": "To answer this question, we apply a uniform mixture over the private model output along with the shared model output, rather than using the learnt probability distribution from the style classifier.", "labels": [], "entities": []}, {"text": "The ROUGE-1/2/L scores are 39.9/19.7/37.0.", "labels": [], "entities": [{"text": "ROUGE-1/2/L", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9374922990798951}]}, {"text": "They are higher than the S model but significantly lower than the SP model and the M-SP model (pvalue 0.016).", "labels": [], "entities": []}, {"text": "This result confirms that the information that the style classifier encodes is beneficiary, and leads to improved performance.", "labels": [], "entities": []}, {"text": "Style vs. Content Previous experiments indicate that the SP and M-SP models have superior generation accuracy, but it is unclear to what extent the difference comes from improved modeling of style versus modeling of content.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9495017528533936}]}, {"text": "To clarify this issue, we performed an experiment in which we replace the named entities appearing in both article and headline with corresponding entity tags, in effect suppressing almost completely any content signal.", "labels": [], "entities": []}, {"text": "For instance, given an input such as \"China called Thursday on the parties involved in talks on North Korea's nuclear program to show flexibility as a deadline for implementing the first steps of a breakthrough deal approached.\", paired with goldtruth output \"China urges flexibility as NKorea deadline approaches\", we replaced the named entities with their types, and obtained: \" In, we show an example which indi-article the org 2 is to forge non 1 with the org 3 located in loc 2 , loc 1 , the per 0 of the loc 0 org 4 said tuesday . title loc 0 org 0 to forge non 0 with loc 1 org 1 output by S org 0 to org 1 in non 0 output by M-SP loc 0 org 0 to forge non 0 with loc 1 org 1 article loc 0 -born per 0 per 0 will pay non 1 here next month to per 1 , the org 2 ( org 1 ) per 1 who per 1 perished in an non 2 in february , the org 3 said thursday . title per 0 to pay non 0 to late org 1 org 0 output by S per 0 to visit org 0 in non 0 output by M-SP per 0 to pay non 0 to org 1 org 0: Examples of input article (and groundtruth title) and output generated by Sand M-SP.", "labels": [], "entities": []}, {"text": "Named entities in the training instances (both article and title) are replaced the entity type.", "labels": [], "entities": []}, {"text": "cates the ability of style adaptation benifiting summarization.", "labels": [], "entities": [{"text": "style adaptation benifiting summarization", "start_pos": 21, "end_pos": 62, "type": "TASK", "confidence": 0.7535389065742493}]}, {"text": "For instance, we find that both CNA and XIN make more frequent use of the style pattern \"xxx will/to yyy . .", "labels": [], "entities": []}, {"text": "., zzz said ???day\" (about 15% of CNA articles contain this pattern, while only 2% of LTW articles have it).", "labels": [], "entities": []}, {"text": "From, we can see that the S model sometimes misses or misuses the verb in its output, while the M-SP model does a much better job at capturing both the verb/action as well as other relations (via prepositions, etc.) shows the estimated style probabilities over the four styles AFP/APW/XIN/NYT for CNA and LTW, under this experiment condition.", "labels": [], "entities": [{"text": "AFP", "start_pos": 277, "end_pos": 280, "type": "METRIC", "confidence": 0.9120724201202393}]}, {"text": "We observe that, in this version as well, CNA is closely matching the style of XIN, while LTW is matching that of NYT.", "labels": [], "entities": [{"text": "NYT", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.9194194078445435}]}, {"text": "The distribution is similar to the one in, albeit a bit flatter as a result of content removal.", "labels": [], "entities": []}, {"text": "As such, it supports the conclusion that the classifier indeed learns style (in addition to content) characteristics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE F1 scores on the combined  AFP/APW/XIN/NYT in-domain test set.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9834544658660889}, {"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.790207028388977}, {"text": "AFP/APW/XIN/NYT in-domain test set", "start_pos": 43, "end_pos": 77, "type": "DATASET", "confidence": 0.8909995079040527}]}, {"text": " Table 2: ROUGE F1 scores on out-of-domain style test sets CNA and LTW.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9903871417045593}, {"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.8173686861991882}]}, {"text": " Table 3: Examples of input article (and groundtruth title) and output generated by S and M-SP. Named  entities in the training instances (both article and title) are replaced the entity type.", "labels": [], "entities": []}]}