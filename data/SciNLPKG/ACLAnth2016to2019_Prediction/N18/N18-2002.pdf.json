{"title": [], "abstractContent": [{"text": "We present an empirical study of gender bias in coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.9620486199855804}]}, {"text": "We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender.", "labels": [], "entities": []}, {"text": "With these Winogender schemas, we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.8224480152130127}]}], "introductionContent": [{"text": "There is a classic riddle: A man and his son get into a terrible car crash.", "labels": [], "entities": []}, {"text": "The father dies, and the boy is badly injured.", "labels": [], "entities": []}, {"text": "In the hospital, the surgeon looks at the patient and exclaims, \"I can't operate on this boy, he's my son!\"", "labels": [], "entities": []}, {"text": "That a majority of people are reportedly unable to solve this riddle 1 is taken as evidence of underlying implicit gender bias (): many first-time listeners have difficulty assigning both the role of \"mother\" and \"surgeon\" to the same entity.", "labels": [], "entities": []}, {"text": "As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.9639531373977661}]}, {"text": "As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases, this work investigates how gender biases manifest in coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 164, "end_pos": 186, "type": "TASK", "confidence": 0.920902281999588}]}, {"text": "There are many ways one could approach this question; here we focus on gender bias with respect to occupations, for which we have corresponding U.S. employment statistics.", "labels": [], "entities": []}, {"text": "Our approach is to construct a challenge dataset in The surgeon is the boy's mother.", "labels": [], "entities": []}, {"text": "the style of Winograd schemas, wherein a pronoun must be resolved to one of two previouslymentioned entities in a sentence designed to be easy for humans to interpret, but challenging for data-driven systems ().", "labels": [], "entities": []}, {"text": "In our setting, one of these mentions is a person referred to by their occupation; by varying only the pronoun's gender, we are able to test the impact of gender on resolution.", "labels": [], "entities": [{"text": "resolution", "start_pos": 165, "end_pos": 175, "type": "TASK", "confidence": 0.9705789685249329}]}, {"text": "With these \"Winogender schemas,\" we demonstrate the presence of systematic gender bias in multiple publiclyavailable coreference resolution systems, and that occupation-specific bias is correlated with employment statistics.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.7417981624603271}]}, {"text": "We release these test sentences to the public.", "labels": [], "entities": []}, {"text": "In our experiments, we represent gender as a categorical variable with either two or three possible values: female, male, and (in some cases) neutral.", "labels": [], "entities": []}, {"text": "These choices reflect limitations of the textual and real-world datasets we use.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Correlation values for Figures 3 and 4.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8847777843475342}]}, {"text": " Table 2.)  Because coreference systems need to make dis- crete choices about which mentions are coref- erent, percentage-wise differences in real-world  statistics may translate into absolute differences  in system predictions. For example, the occupa- tion \"manager\" is 38.5% female in the U.S. ac- cording to real-world statistics (BLS); mentions of  \"manager\" in text are only 5.18% female (B&L  resource); and finally, as viewed through the be- havior of the three coreference systems we tested, no managers are predicted to be female. This il- lustrates two related phenomena: first, that data- driven NLP pipelines are susceptible to sequential  amplification of bias throughout a pipeline, and  second, that although the gender statistics from  B&L correlate with BLS employment statistics,  they are systematically male-skewed (", "labels": [], "entities": [{"text": "B&L  resource", "start_pos": 395, "end_pos": 408, "type": "DATASET", "confidence": 0.8315893858671188}]}, {"text": " Table 2: System accuracy (%) bucketed by gender and  difficulty (so-called \"gotchas,\" shaded in purple). For  female pronouns, a \"gotcha\" sentence is one where ei- ther (1) the correct answer is OCCUPATION but the oc- cupation is < 50% female (according to BLS); or (2)  the occupation is \u2265 50% female but the correct answer  is PARTICIPANT; this is reversed for male pronouns.  Systems do uniformly worse on \"gotchas.\"", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9945899248123169}, {"text": "OCCUPATION", "start_pos": 196, "end_pos": 206, "type": "METRIC", "confidence": 0.950089693069458}, {"text": "BLS", "start_pos": 258, "end_pos": 261, "type": "DATASET", "confidence": 0.5687941312789917}, {"text": "PARTICIPANT", "start_pos": 330, "end_pos": 341, "type": "METRIC", "confidence": 0.9063781499862671}]}]}