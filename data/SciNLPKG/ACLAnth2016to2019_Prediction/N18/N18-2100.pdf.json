{"title": [{"text": "Key2Vec: Automatic Ranked Keyphrase Extraction from Scientific Articles using Phrase Embeddings", "labels": [], "entities": [{"text": "Automatic Ranked Keyphrase Extraction from Scientific Articles", "start_pos": 9, "end_pos": 71, "type": "TASK", "confidence": 0.7016398097787585}]}], "abstractContent": [{"text": "Keyphrase extraction is a fundamental task in natural language processing that facilitates mapping of documents to a set of representative phrases.", "labels": [], "entities": [{"text": "Keyphrase extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8841744959354401}]}, {"text": "In this paper, we present an un-supervised technique (Key2Vec) that leverages phrase embeddings for ranking keyphrases extracted from scientific articles.", "labels": [], "entities": [{"text": "ranking keyphrases extracted from scientific articles", "start_pos": 100, "end_pos": 153, "type": "TASK", "confidence": 0.7699896593888601}]}, {"text": "Specifically , we propose an effective way of processing text documents for training multi-word phrase embeddings that are used for thematic representation of scientific articles and ranking of keyphrases extracted from them using theme-weighted PageRank.", "labels": [], "entities": []}, {"text": "Evaluations are performed on benchmark datasets producing state-of-the-art results.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The final ranked keyphrases obtained using the Key2Vec methodology as described in the previous section is evaluated on the popular Inspec and SemEval 2010 datasets.", "labels": [], "entities": [{"text": "Inspec and SemEval 2010 datasets", "start_pos": 132, "end_pos": 164, "type": "DATASET", "confidence": 0.8124518156051636}]}, {"text": "The Inspec dataset) is composed of 2000 abstracts of scientific articles divided into sets of 1000, 500, and 500, as training, validation and test datasets respectively.", "labels": [], "entities": [{"text": "Inspec dataset)", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9090358018875122}]}, {"text": "Each document has two lists of keyphrases assigned by humans -controlled, which are assigned by the authors, and uncontrolled, which are freely assigned by the readers.", "labels": [], "entities": []}, {"text": "The controlled keyphrases are mostly abstractive, whereas the uncontrolled ones are mostly extractive (.", "labels": [], "entities": []}, {"text": "The Semeval 2010 dataset ( consists of 284 full length ACM articles divided into a test set of size 100, training set of size 144 and trial set of size 40.", "labels": [], "entities": [{"text": "Semeval 2010 dataset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7176555295785269}]}, {"text": "Each article has two sets of human assigned keyphrases: the authorassigned and reader-assigned ones, equivalent to the controlled and uncontrolled categories, respectively of the Inspec dataset.", "labels": [], "entities": [{"text": "Inspec dataset", "start_pos": 179, "end_pos": 193, "type": "DATASET", "confidence": 0.9585846364498138}]}, {"text": "We only use the test datasets for our evaluations and combine the annotated controlled and uncontrolled keyphrases.", "labels": [], "entities": []}, {"text": "The ranked keyphrases are evaluated using exact match evaluation metric as used in SemEval 2010 Task 5.", "labels": [], "entities": [{"text": "SemEval 2010 Task 5", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8376104533672333}]}, {"text": "We match the keyphrases in the annotated documents in the benchmark datasets with those generated by Key2Vec, and calculate microaveraged precision, recall and F-score (\u03b2 = 1), respectively.", "labels": [], "entities": [{"text": "microaveraged", "start_pos": 124, "end_pos": 137, "type": "METRIC", "confidence": 0.8813329339027405}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.8061744570732117}, {"text": "recall", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.9995612502098083}, {"text": "F-score", "start_pos": 160, "end_pos": 167, "type": "METRIC", "confidence": 0.9990567564964294}]}, {"text": "In the evaluation, we check the performance over the top 5, 10 and 15 candidates returned by Key2Vec.", "labels": [], "entities": []}, {"text": "The performance of Key2Vec on the metrics is shown in. shows a comparison of Key2Vec with some of the state-of-the-art systems giving best performances on the Inspec and SemEval 2010 datasets, respectively.", "labels": [], "entities": [{"text": "Inspec and SemEval 2010 datasets", "start_pos": 159, "end_pos": 191, "type": "DATASET", "confidence": 0.7590343713760376}]}], "tableCaptions": [{"text": " Table 2: Performance of Key2Vec over combined controlled and uncontrolled annotated keyphrases for Inspec and  SemEval 2010 datasets.", "labels": [], "entities": [{"text": "Inspec and  SemEval 2010 datasets", "start_pos": 100, "end_pos": 133, "type": "DATASET", "confidence": 0.7047448039054871}]}, {"text": " Table 4: Comparison of Key2Vec with some state-of-the-art systems (Danesh et al., 2015; Bougouin et al., 2013;  Lopez and Romary, 2010) for Avg. F1@10 on SemEval 2010 dataset.", "labels": [], "entities": [{"text": "Avg. F1@10", "start_pos": 141, "end_pos": 151, "type": "DATASET", "confidence": 0.6503271043300629}, {"text": "SemEval 2010 dataset", "start_pos": 155, "end_pos": 175, "type": "DATASET", "confidence": 0.8443315426508585}]}]}