{"title": [{"text": "Guiding Neural Machine Translation with Retrieved Translation Pieces", "labels": [], "entities": [{"text": "Guiding Neural Machine Translation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7621394991874695}]}], "abstractContent": [{"text": "One of the difficulties of neural machine translation (NMT) is the recall and appropriate translation of low-frequency words or phrases.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.8471610049406687}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9936728477478027}, {"text": "appropriate translation of low-frequency words or phrases", "start_pos": 78, "end_pos": 135, "type": "TASK", "confidence": 0.7155113177640098}]}, {"text": "In this paper, we propose a simple, fast, and effective method for recalling previously seen translation examples and incorporating them into the NMT decoding process.", "labels": [], "entities": []}, {"text": "Specifically, for an input sentence, we use a search engine to retrieve sentence pairs whose source sides are similar with the input sentence, and then collect n-grams that are both in the retrieved target sentences and aligned with words that match in the source sentences, which we call \"translation pieces\".", "labels": [], "entities": []}, {"text": "We compute pseudo-probabilities for each retrieved sentence based on similarities between the input sentence and the retrieved source sentences, and use these to weight the retrieved translation pieces.", "labels": [], "entities": []}, {"text": "Finally , an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces.", "labels": [], "entities": []}, {"text": "We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient.", "labels": [], "entities": [{"text": "NMT translation", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.9676826000213623}, {"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9994274377822876}, {"text": "narrow domain translation tasks", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.7659759074449539}]}, {"text": "It also causes little increase in the translation time, and compares favorably to another alternative retrieval-based method with respect to accuracy, speed, and simplicity of implementation.", "labels": [], "entities": [{"text": "translation", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9471821784973145}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9993416666984558}]}], "introductionContent": [{"text": "Neural machine translation (NMT) (;) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7695539693037668}, {"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7744744122028351}]}, {"text": "However, it has also been noted that compared to alternatives such as phrase-based translation (, NMT has trouble with lowfrequency words or phrases (, and also generalizing across domains (.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.747650533914566}]}, {"text": "A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons) and phrase tables (, adjust model structures to be more conducive to generalization, or incorporate additional information about domain () or topic ( ) in translation models.", "labels": [], "entities": []}, {"text": "In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.", "labels": [], "entities": []}, {"text": "1 These methods -reminiscent of translation memory) or example-based translation) -are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words.", "labels": [], "entities": [{"text": "example-based translation", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.7108262479305267}]}, {"text": "Currently there are two main approaches to doing so.  and use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus.", "labels": [], "entities": []}, {"text": "uses the retrieved sentence pairs as additional inputs to the NMT model to help NMT in translating the input sen- Input:", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Data sets. The last line is the average length  of English sentences.", "labels": [], "entities": []}, {"text": " Table 4. We also compared our method with the  search engine guided NMT model (SGNMT,", "labels": [], "entities": []}, {"text": " Table 6: Similarities between the training set and the  whole/divided test sets.", "labels": [], "entities": []}, {"text": " Table 7:  Translation results (BLEU) for the  whole/divided test sets.", "labels": [], "entities": [{"text": "Translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.8716926574707031}, {"text": "BLEU)", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9223887324333191}]}, {"text": " Table 8: Statistics for similarities between each test  sentence and the training set as computed by Equa- tion 11 for the WMT 2017 en-de task (3004 sentences)  and our JRC-Acquis en-de task (1689 sentences).", "labels": [], "entities": [{"text": "WMT 2017 en-de task", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7153871059417725}]}, {"text": " Table 9: Translation results (BLEU) of 1/0 reward.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6304845213890076}, {"text": "BLEU)", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.8723805546760559}]}, {"text": " Table 11: Translation time (seconds).", "labels": [], "entities": [{"text": "Translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9202290177345276}]}, {"text": " Table 12: Comparison with SGNMT.", "labels": [], "entities": [{"text": "SGNMT", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.6022413969039917}]}]}