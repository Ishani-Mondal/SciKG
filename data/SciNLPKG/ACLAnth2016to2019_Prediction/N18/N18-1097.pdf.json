{"title": [{"text": "Comparing Automatic and Human Evaluation of Local Explanations for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.879559338092804}]}], "abstractContent": [{"text": "Text classification models are becoming increasingly complex and opaque, however for many applications it is essential that the models are interpretable.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7245404571294785}]}, {"text": "Recently, a variety of approaches have been proposed for generating local explanations.", "labels": [], "entities": []}, {"text": "While robust evaluations are needed to drive further progress, so far it is unclear which evaluation approaches are suitable.", "labels": [], "entities": []}, {"text": "This paper is a first step towards more robust evaluations of local explanations.", "labels": [], "entities": []}, {"text": "We evaluate a variety of local explanation approaches using automatic measures based on word deletion.", "labels": [], "entities": []}, {"text": "Furthermore, we show that an evaluation using a crowdsourcing experiment correlates moderately with these automatic measures and that a variety of other factors also impact the human judgements.", "labels": [], "entities": []}], "introductionContent": [{"text": "While the impact of machine learning is increasing rapidly in society, machine learning systems have also become increasingly complex and opaque.", "labels": [], "entities": []}, {"text": "Classification models are usually evaluated based on prediction performance alone (e.g., by measuring the accuracy, recall, and precision) and the interpretability of these models has generally been undervalued.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9991623163223267}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9964512586593628}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9991400241851807}]}, {"text": "However, the importance of interpretable models is increasingly being recognized.", "labels": [], "entities": []}, {"text": "First, higher interpretability could lead to more effective models by revealing incompleteness in the problem formalization, by revealing confounding factors that could lead to biased models, and by supporting error analyses or feature discovery.", "labels": [], "entities": [{"text": "feature discovery", "start_pos": 228, "end_pos": 245, "type": "TASK", "confidence": 0.7011865079402924}]}, {"text": "Second, with the increasing adoption of machine learning approaches for humanities and social science research, there is also an increasing need for systems that support exploratory analyses and theory development.", "labels": [], "entities": []}, {"text": "Various approaches have been explored to increase the interpretability of machine learning models.", "labels": [], "entities": [{"text": "interpretability of machine learning models", "start_pos": 54, "end_pos": 97, "type": "TASK", "confidence": 0.753019905090332}]}, {"text": "This paper focuses on local explanation, which aims to explain the prediction for an individual instance (e.g.,).", "labels": [], "entities": [{"text": "local explanation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7356455624103546}]}, {"text": "A study by found that providing local explanations could help improve the acceptance of movie recommendation systems.", "labels": [], "entities": [{"text": "acceptance of movie recommendation", "start_pos": 74, "end_pos": 108, "type": "TASK", "confidence": 0.6620560958981514}]}, {"text": "Local explanations can come in different forms.", "labels": [], "entities": []}, {"text": "For example, identify the most influential training documents fora particular prediction.", "labels": [], "entities": []}, {"text": "The most common type of local explanation involves identifying the important parts of the input fora prediction, such as the most predictive words in a document fora text classification model.", "labels": [], "entities": [{"text": "document fora text classification", "start_pos": 152, "end_pos": 185, "type": "TASK", "confidence": 0.6252563968300819}]}, {"text": "In this paper we focus on local explanations for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8313049376010895}]}, {"text": "Below is a fragment of a movie review.", "labels": [], "entities": []}, {"text": "The words identified by a local explanation method to explain a neural network prediction are in bold.", "labels": [], "entities": []}, {"text": "The review is labeled with a negative sentiment, but the classifier incorrectly predicted a positive sentiment.", "labels": [], "entities": []}, {"text": "The highlighted words help us understand why.", "labels": [], "entities": []}, {"text": "steve martin is one of the funniest men alive.", "labels": [], "entities": []}, {"text": "if you can take that as a true statement, then your disappointment at this film will equal mine.", "labels": [], "entities": []}, {"text": "martin can be hilarious, creating some of the best laugh-out-loud experiences that have ever taken place in movie theaters.", "labels": [], "entities": []}, {"text": "you won't find any of them here.", "labels": [], "entities": []}, {"text": "Words such as funniest and hilarious were important for the prediction.", "labels": [], "entities": [{"text": "prediction", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.9833800196647644}]}, {"text": "Besides providing evidence fora predicted label, some local explanations can also provide evidence against a predicted label.", "labels": [], "entities": []}, {"text": "For example, in the above example, the word disappointment was one of the highest ranked words against the predicted label.", "labels": [], "entities": []}, {"text": "Ineffective approaches could generate misleading explanations, but evaluating local explanations is challenging.", "labels": [], "entities": []}, {"text": "A variety of approaches has been used, including only visual inspection (), intrinsic evaluation approaches such as measuring the impact of deleting the identified words on the classifier output (, and user studies (.", "labels": [], "entities": []}, {"text": "Contributions To further progress in this area, it is imperative to have a better understanding of how to evaluate local explanations.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: \u2022 Comparison of local explanation methods for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8323001563549042}]}, {"text": "We present an in-depth comparison between three local explanation approaches (and a random baseline) using two different automatic evaluation measures on two text classification tasks (Section 4).", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 158, "end_pos": 183, "type": "TASK", "confidence": 0.7766312559445699}]}, {"text": "\u2022 Automatic versus human evaluation.", "labels": [], "entities": [{"text": "Automatic versus human evaluation", "start_pos": 2, "end_pos": 35, "type": "TASK", "confidence": 0.6166825219988823}]}, {"text": "Automatic evaluations, such as those based on word deletions, are frequently used since they enable rapid iterations and are easy to reproduce.", "labels": [], "entities": []}, {"text": "However, it is unclear to what extent they correspond with human-based evaluations.", "labels": [], "entities": []}, {"text": "We show that the automatic measures correlate moderately with human judgements in a task setting and that other factors also impact human judgement.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the datasets, the classification models and the local explanation approaches used in our experiments.", "labels": [], "entities": []}, {"text": "We experiment with two datasets: \u2022 Twenty newsgroups (20news).", "labels": [], "entities": []}, {"text": "The Twenty Newsgroups dataset has been used in several studies on ML interpretability ().", "labels": [], "entities": [{"text": "Twenty Newsgroups dataset", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.8364551663398743}, {"text": "ML interpretability", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.9635624885559082}]}, {"text": "Similar to, we only distinguish between Christianity and Atheism.", "labels": [], "entities": []}, {"text": "We use the 20news-bydate version, and randomly reserve 20% of the training data for development.", "labels": [], "entities": []}, {"text": "Movie reviews with polarity labels (positive versus negative sentiment) from.", "labels": [], "entities": []}, {"text": "We use the version from.", "labels": [], "entities": []}, {"text": "The dataset is randomly split into a train (60%), development (20%) and test (20%) set.", "labels": [], "entities": []}, {"text": "In this section we explore automatic evaluation of local explanations.", "labels": [], "entities": []}, {"text": "Local explanations should exhibit high local fidelity, i.e. they should match the underlying model in the neighborhood of the instance ().", "labels": [], "entities": []}, {"text": "An explanation with low local fidelity could be misleading.", "labels": [], "entities": []}, {"text": "Because we generate explanations for the predicted class (rather than the ground truth), explanations with high local fidelity do not necessarily need to match human intuition, for example when the classifier is weak ( . Ideally, the evaluation metrics are model agnostic and do not require information that may not always be available such as probability outputs.", "labels": [], "entities": []}, {"text": "This paper focuses on local fidelity, but other aspects might also be desired, such as sparsity ().", "labels": [], "entities": []}, {"text": "We measure local fidelity by deleting words in the order of their estimated importance for the prediction.", "labels": [], "entities": []}, {"text": "generated explanations with the correct class as target.", "labels": [], "entities": []}, {"text": "By deleting the identified words, accuracy increased for incorrect predictions and decreased for correct predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9995213747024536}]}, {"text": "However, their approach assumes knowledge of the ground-truth labels.", "labels": [], "entities": []}, {"text": "We take an alternative, but similar, approach.", "labels": [], "entities": []}, {"text": "Words are also deleted according to their estimated importance, e.g. w 1 ...w n with w 1 the word with the highest importance score, but for the predicted class instead.", "labels": [], "entities": []}, {"text": "For each document, we measure the number of words that need to be deleted before the prediction switches to another class (the switching point), normalized by the number of words in the document.", "labels": [], "entities": []}, {"text": "For example, a value of 0.10 indicates that 10% of the words needed to be deleted before the prediction changed.", "labels": [], "entities": []}, {"text": "An advantage of this approach is that ground-truth labels are not needed and that it can be applied to blackbox classifiers, we only need to know the predicted class.", "labels": [], "entities": []}, {"text": "Furthermore, the approach acts on the raw input.", "labels": [], "entities": []}, {"text": "It requires no knowledge of the underlying feature representation (e.g., the actual features might be on the character level).", "labels": [], "entities": []}, {"text": "We also experiment with the measure proposed by , referred to as the area over the perturbation curve (AOPC): where f (x \\ 1..k ) is the probability for the predicted class when words 1..k are removed and \u00b7\u00b7 p(x) denotes the average over the documents.", "labels": [], "entities": [{"text": "perturbation curve (AOPC)", "start_pos": 83, "end_pos": 108, "type": "METRIC", "confidence": 0.6066223204135894}]}, {"text": "This approach is also based on deleting words, but it is more fine-grained since it uses probability values rather than predicted labels.", "labels": [], "entities": []}, {"text": "It also enables evaluating negative evidence.", "labels": [], "entities": []}, {"text": "A drawback is that AOPC requires access to probability estimates of a classifier.", "labels": [], "entities": []}, {"text": "In this paper, K is set to 10.", "labels": [], "entities": []}, {"text": "For LR, the exact contribution of individual features to a prediction is known and the words in the document that contributed most to the prediction can be computed directly.", "labels": [], "entities": []}, {"text": "For this classifier, the optimal approach corresponds to the omission approach.", "labels": [], "entities": []}, {"text": "reports the results by measuring the effect of word deletions and reporting the average switching point.", "labels": [], "entities": []}, {"text": "Lower values indicate that the method was better capable of identifying the words that contributed most towards the predicted class, because on average fewer words needed to be deleted to change a prediction.", "labels": [], "entities": []}, {"text": "shows the AOPC values with a cut-off at 10.", "labels": [], "entities": [{"text": "AOPC", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9951930046081543}]}, {"text": "We measure AOPC in two settings: removing positive evidence (higher values indicate a more effective explanation) and negative evidence (lower values indicate a more effective explanation).", "labels": [], "entities": [{"text": "AOPC", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.8076221346855164}]}, {"text": "In the previous section we evaluated the local explanation approaches using automatic measures.", "labels": [], "entities": []}, {"text": "However, the explanations are meant to be presented to humans.", "labels": [], "entities": []}, {"text": "We therefore turn to evaluating the explanations using crowdsourcing.", "labels": [], "entities": []}, {"text": "We analyze the usefulness of the generated explanations in a task setting and analyze to what extent the automatic measures correspond to the human-based evaluations.", "labels": [], "entities": []}, {"text": "The crowdsourcing experiments are run on CrowdFlower.", "labels": [], "entities": []}, {"text": "Only crowdworkers from Australia, Canada, Ireland, United Kingdom and the United States and with quality levels two or three were accepted.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: AOPC results. For each method, AOPC is used to evaluate the words identified to be supportive of the  predicted class (positive evidence) and words identified to be supportive of the other class (negative evidence). For  LIME, results are reported for different sample sizes.", "labels": [], "entities": [{"text": "AOPC", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9720944166183472}, {"text": "AOPC", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9973217844963074}]}, {"text": " Table 3: The % of words that needs to be deleted to  change the prediction (the switching point).", "labels": [], "entities": []}, {"text": " Table 4: Spearman correlation between prediction con- fidence and AOPC and the switching point (SP) for the  MLP classifier on the movie dataset.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8714469373226166}, {"text": "AOPC", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9920496344566345}, {"text": "switching point (SP)", "start_pos": 80, "end_pos": 100, "type": "METRIC", "confidence": 0.9328875660896301}]}, {"text": " Table 5: Results forward prediction task, with the accuracy (acc), average confidence (conf) and the number of  judgements (n). The results are separated according to TP (true positive), TN (true negative), FP (false positive)  and FN (false negative) predictions, and the number of words shown (#w).", "labels": [], "entities": [{"text": "accuracy (acc)", "start_pos": 52, "end_pos": 66, "type": "METRIC", "confidence": 0.8055175393819809}, {"text": "average confidence (conf)", "start_pos": 68, "end_pos": 93, "type": "METRIC", "confidence": 0.9110297799110413}, {"text": "FP", "start_pos": 208, "end_pos": 210, "type": "METRIC", "confidence": 0.9762706160545349}, {"text": "FN", "start_pos": 233, "end_pos": 235, "type": "METRIC", "confidence": 0.8905507922172546}]}, {"text": " Table 6: Confidence and accuracy results", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9993650317192078}]}, {"text": " Table 7: Forward prediction task with noisy explanations on the movie dataset and the saliency method", "labels": [], "entities": [{"text": "Forward prediction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9652886092662811}, {"text": "movie dataset", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.7956936955451965}]}, {"text": " Table 8: Spearman correlation between automatic mea- sures and crowd accuracy. Significance:  *  p < 0.05,   *  *  p < 0.01,  *  *  *  p < 0.001", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9117404818534851}, {"text": "automatic mea- sures", "start_pos": 39, "end_pos": 59, "type": "METRIC", "confidence": 0.807872012257576}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9631085395812988}]}]}