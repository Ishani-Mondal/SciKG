{"title": [{"text": "Behavior Analysis of NLI Models: Uncovering the Influence of Three Factors on Robustness", "labels": [], "entities": [{"text": "Behavior Analysis of NLI Models", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8173346161842346}]}], "abstractContent": [{"text": "Natural Language Inference is a challenging task that has received substantial attention, and state-of-the-art models now achieve impressive test set performance in the form of accuracy scores.", "labels": [], "entities": [{"text": "Natural Language Inference", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7163939674695333}, {"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9979231953620911}]}, {"text": "Here, we go beyond this single evaluation metric to examine robustness to semantically-valid alterations to the input data.", "labels": [], "entities": []}, {"text": "We identify three factors-insensitivity , polarity and unseen pairs-and compare their impact on three SNLI models under a variety of conditions.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 102, "end_pos": 106, "type": "TASK", "confidence": 0.8815851211547852}]}, {"text": "Our results demonstrate a number of strengths and weaknesses in the models' ability to generalise to new in-domain instances.", "labels": [], "entities": []}, {"text": "In particular, while strong performance is possible on unseen hypernyms, unseen antonyms are more challenging for all the models.", "labels": [], "entities": []}, {"text": "More generally, the models suffer from an insensitivity to certain small but semantically significant alterations, and are also often influenced by simple statistical correlations between words and training labels.", "labels": [], "entities": []}, {"text": "Overall , we show that evaluations of NLI models can benefit from studying the influence of factors intrinsic to the models or found in the dataset used.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of Natural Language Inference (NLI) has received a lot of attention and has elicited models which have achieved impressive results on the Stanford NLI (SNLI) dataset.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.8172019918759664}, {"text": "Stanford NLI (SNLI) dataset", "start_pos": 147, "end_pos": 174, "type": "DATASET", "confidence": 0.9141601622104645}]}, {"text": "Such results are impressive due to the linguistic knowledge required to solve the task (.", "labels": [], "entities": []}, {"text": "However, the ever-growing complexity of these models inhibits a full understanding of the phenomena that they capture.", "labels": [], "entities": []}, {"text": "Also known as Recognizing Textual Entailment.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment", "start_pos": 14, "end_pos": 44, "type": "TASK", "confidence": 0.8820865948994955}]}, {"text": "As a consequence, evaluating these models purely on test set performance may not yield enough insight into the complete repertoire of abilities learned and any possible abnormal behaviors (.", "labels": [], "entities": []}, {"text": "A similar case can be observed in models from other domains; take as an example an image classifier that predicts based on the image's background rather than on the target object (, or a classifier used in social contexts that predicts a label based on racial attributes.", "labels": [], "entities": []}, {"text": "In both examples, the models exploit a bias (an undesired pattern hidden in the dataset) to enhance accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.994989812374115}]}, {"text": "In such cases, the models may appear to be robust to new and even challenging test instances; however, this behavior maybe due to spurious factors, such as biases.", "labels": [], "entities": []}, {"text": "Assessing to what extent the models are robust to these contingencies just by looking attest accuracy is, therefore, difficult.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9866424798965454}]}, {"text": "In this work we aim to study how certain factors affect the robustness of three pre-trained NLI models (a conditional encoder, the DAM model (, and the ESIM model).", "labels": [], "entities": []}, {"text": "We call these target factors insensitivity (not recognizing anew instance), polarity (a word-pair bias), and unseen pairs (recognizing the semantic relation of new word pairs).", "labels": [], "entities": []}, {"text": "We became aware of these factors based on an exploration of the models' behavior, and we hypothesize that these factors systematically influence the behavior of the models.", "labels": [], "entities": []}, {"text": "In order to systematically test if the above factors affect robustness, we propose a set of challenging instances for the models: We sample a set of instances from SNLI data, we apply a transformation on this set that yields anew set of instances, and we test both how well the models classify these new instances and whether the target factors influence the models' behavior.", "labels": [], "entities": [{"text": "SNLI data", "start_pos": 164, "end_pos": 173, "type": "DATASET", "confidence": 0.8979893028736115}]}, {"text": "The transformation (swapping a pair of words between premise and hypothesis sentences) is intended to yield both easy and difficult instances to challenge the models, but easy fora human to annotate them.", "labels": [], "entities": []}, {"text": "We draw motivation to study the robustness of NLI models from previous work on evaluating complex models ().", "labels": [], "entities": []}, {"text": "Furthermore, we base our approach on the discipline of behavioral science which provides methodologies for analyzing how certain factors influence the behavior of subjects understudy.", "labels": [], "entities": []}, {"text": "We aim to answer the research questions: How robust is the predictive behavior of the pre-trained models under our transformation to input data?", "labels": [], "entities": []}, {"text": "Do the target factors (insensitivity, polarity, and unseen pairs) influence the prediction of the models?", "labels": [], "entities": []}, {"text": "Are these factors common across models?", "labels": [], "entities": []}, {"text": "Our results show that the models are robust mainly where the semantics of the new instances do not change significantly with respect to the sampled instances and thus the class labels remain unaltered; i.e., the models are insensitive to our transformation to input data.", "labels": [], "entities": []}, {"text": "However, when the class labels change, the models significantly drop accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9983925223350525}]}, {"text": "In addition, the models exploit a bias, polarity, to stay robust when facing new instances.", "labels": [], "entities": []}, {"text": "We also find that the models are able to cope with unseen word pairs under a hypernym relation, but not with those under an antonym relation, suggesting their inability to learn asymmetric relation.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Stanford NLI dataset was created with the purpose of training deep neural models while providing human-annotated data.", "labels": [], "entities": [{"text": "Stanford NLI dataset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9382768074671427}]}, {"text": "Each instance was created by providing a premise sentence, harvested from a pre-existing dataset, to a crowdsource worker who was instructed to produce three hypothesis sentences, one for each NLI class (entailment, neutral, contradiction).", "labels": [], "entities": []}, {"text": "This process yielded a balanced dataset containing around 570K instances.", "labels": [], "entities": []}, {"text": "In this experiment we use sets IA and I TA1 . Swapping antonyms seems to have no effect on the overall performance of the DAM model on I TA1 when compared to IA , and little effect on ESIM.", "labels": [], "entities": [{"text": "ESIM", "start_pos": 184, "end_pos": 188, "type": "DATASET", "confidence": 0.6551896333694458}]}, {"text": "Thus these two models appear to be robust to this transformation.", "labels": [], "entities": []}, {"text": "Nonetheless, further analysis will not support the conclusion that both models have learned that antonymy is symmetric, and we will show that this seemingly robust behavior is due to confounding factors and not due to inference abilities.", "labels": [], "entities": []}, {"text": "Accuracy scores of CE model seem to reveal that it is much less robust to the antonym swap, with performance significantly dropping by roughly 10.5% according to a McNemar's test.", "labels": [], "entities": [{"text": "CE", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.7933357954025269}]}, {"text": "Insensitivity Because instances in I TA1 are contradiction, we perform a proxy experiment to understand the models' sensitivity.", "labels": [], "entities": [{"text": "I TA1", "start_pos": 35, "end_pos": 40, "type": "TASK", "confidence": 0.5083312839269638}]}, {"text": "From IA , we substitute one of the antonyms in each word pair (in each instance) with a hyponym, hypernym, or synonym 6 of the other.", "labels": [], "entities": [{"text": "IA", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.8119552135467529}]}, {"text": "Doing this on both the premise and hypothesis yields two new samples, I TA2 and I TA3 , which we manually annotate.", "labels": [], "entities": []}, {"text": "Examples of control (Example 7) and transformed (Example 8) instances are given below, showing the replacement of young, in the hypothesis, with aged, a synonym of elderly from the premise.", "labels": [], "entities": []}, {"text": "This transformation changes gold-label from contradiction to neutral.", "labels": [], "entities": []}, {"text": "Approximately, half the sample yields such changes in gold-label.", "labels": [], "entities": []}, {"text": "This transformation leads to a considerable drop in overall performance for all models when accuracy scores on sets I TA2 and I TA3 are compared to the accuracy on the control instances in IA : up to 0.175 (CE), 0.201 (DAM), and 0.24 (ESIM) points.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9988679885864258}, {"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9984934329986572}, {"text": "CE)", "start_pos": 207, "end_pos": 210, "type": "METRIC", "confidence": 0.971485435962677}]}, {"text": "To test if insensitivity to the transformation is associated with these behaviors, we measure accuracy only on those instances that changed gold-label (Subset 1 from the sets I TA2 and I TA3 ), where we see a further reduction in performance for all models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.999432384967804}]}, {"text": "2-way tests of independence provide strong evidence for the insensitivity of the models (CE: \u03c7 2 (1) = 73.33, DAM: \u03c7 2 (1) = 108.30, ESIM: \u03c7 2 (1) = 175.34).", "labels": [], "entities": [{"text": "CE: \u03c7 2 (1)", "start_pos": 89, "end_pos": 100, "type": "METRIC", "confidence": 0.8390426295144218}, {"text": "DAM", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.5230935215950012}, {"text": "ESIM", "start_pos": 133, "end_pos": 137, "type": "DATASET", "confidence": 0.4994239807128906}]}, {"text": "shows the case for ESIM: most of its incorrect predictions are due to predicting the same label on both control and transformed instances when these two type of instances have different gold labels.", "labels": [], "entities": []}, {"text": "Paradoxically, this effect works in the models' favour in the antonym swapping case (I TA1 ) because all the gold-labels remain as contradiction.", "labels": [], "entities": [{"text": "I TA1 )", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.7518571217854818}]}, {"text": "Thus ignoring the transformation will avoid any loss in performance.", "labels": [], "entities": []}, {"text": "In this experiment, we use samples E A and ETA . Swapping antonyms has little effect on the performance of all models, where the biggest drop comes from DAM (0.029 points).", "labels": [], "entities": [{"text": "ETA", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.919885516166687}, {"text": "DAM", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.9987851977348328}]}, {"text": "However, the CE model performs quite poorly at both samples (0.508 and 0.48 accuracy points on E A and ETA ); this drop in performance, with respect to the in situ condition, suggests that the repeated sentence context is too different from the structure of the training instances for the CE model to generalize effectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9962450861930847}]}, {"text": "In this condition, we refrain from analyzing the effect of insensitivity, since doing so would require a transformation similar to that in the in situ condition, which might add an extra layer of change and the results may turn difficult to interpret.", "labels": [], "entities": []}, {"text": "Unseen Word Pairs Accuracy scores strongly suggest that the models are weak at dealing with unseen antonym pairs (Subset 2 of ETA in); drops in performance on this subset range from 0.315 up to 0.429 points across the three models.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.838060200214386}]}, {"text": "Tests of homogeneity show strong evidence of this weakness for all models (CE: \u03c7 2 (1) = 15.91, DAM: \u03c7 2 (1) = 59.17, ESIM: \u03c7 2 (1) = 44.72).", "labels": [], "entities": [{"text": "CE", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.8565807938575745}, {"text": "DAM", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.5213398337364197}]}, {"text": "Comparing results on this subset with those of Subset 2 in I TA1 , we notice that ESIM and DAM keep similar behavior, but CE seems to be strongly affected by this context type.", "labels": [], "entities": [{"text": "I TA1", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.7926285266876221}, {"text": "CE", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.952494204044342}]}, {"text": "Polarity All models perform poorly in the subset of instances where polarity disagrees with gold label of the instance (Subset 3 of ETA ), showing that the models' behavior rely on this bias.", "labels": [], "entities": []}, {"text": "These results are highly significant (CE: \u03c7 2 (6) = 34.37, DAM: \u03c7 2 (6) = 136.99, ESIM: \u03c7 2 (6) = 103.47).", "labels": [], "entities": [{"text": "CE", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9305492043495178}, {"text": "DAM", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.569003164768219}, {"text": "ESIM", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.5387734770774841}]}, {"text": "This is further evidence that the models get confused with a simple reversal of an antonym pair.", "labels": [], "entities": []}, {"text": "We now study the effect on the robustness of the systems when we swap hypernym and hyponym word pairs in in situ instances.", "labels": [], "entities": []}, {"text": "Whole sample accuracy scores in significantly drop, according to McNemar's tests, by 0.25 (ESIM), 0.285 (CE), and 0.128 (DAM) points when we compare scores on control instances (I H ) with those on transformed instances (I TH ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9976571798324585}, {"text": "ESIM)", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.9559337496757507}, {"text": "CE)", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9816519916057587}]}, {"text": "We investigate the role of our target factors on these behaviors.", "labels": [], "entities": []}, {"text": "Insensitivity Around 42% of the instances in I TH (Subset 1) have different gold label from those in I H . On these instances, the models' results are severely impaired: CE and ESIM models' performances drop to close-to-random (0.271 and 0.315), while DAM decreases by 0.18 points.", "labels": [], "entities": [{"text": "DAM", "start_pos": 252, "end_pos": 255, "type": "METRIC", "confidence": 0.9983362555503845}]}, {"text": "All models' errors on this subset are strongly associated with failure to change the predicted class (CE:\u03c7 2 (1) = 90.73, DAM:\u03c7 2 (1) = 101.52, ESIM:\u03c7 2 (1) = 150.92).", "labels": [], "entities": [{"text": "CE:\u03c7 2 (1)", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.8780592169080462}, {"text": "DAM", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.4987005591392517}, {"text": "ESIM", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.47581034898757935}]}, {"text": "In contrast to the casein Experiment 1, insensitivity acts in detriment of the models' robustness when gold labels change after the transformation.", "labels": [], "entities": []}, {"text": "Unseen Word Pairs Whereas model performance was significantly worse on unseen antonym pairs, this effect is not obvious on the hyponymhypernym results (Subset 2 of I TH ).", "labels": [], "entities": []}, {"text": "In fact, all models have a slightly higher accuracy on this subset than overall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9991816878318787}]}, {"text": "Homogeneity tests find no evidence of an association between unseen word pairs and incorrect predictions for any model (CE:\u03c7 2 (1) = 0.00036, p = 0.98, DAM:\u03c7 2 (1) = 0.98, p = 0.32, ESIM:\u03c7 2 (1) = 0.178, p = 0.67).", "labels": [], "entities": [{"text": "CE", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.8744409084320068}]}, {"text": "This effect maybe explained by the models exploiting information from word embeddings.", "labels": [], "entities": []}, {"text": "It has been shown that word embeddings are able to capture hypernymy (; thus the models may use this information to generalize to unseen hypernym pairs.", "labels": [], "entities": []}, {"text": "Polarity We find very strong evidence for an association between polarity and class label predicted on sample I H for all models (CE:\u03c7 2 (10) = 168.40, DAM:\u03c7 2 (10) = 182.76, ESIM:\u03c7 2 (10) = 157.76).", "labels": [], "entities": [{"text": "CE", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.7999811768531799}, {"text": "DAM", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.487858384847641}, {"text": "ESIM", "start_pos": 175, "end_pos": 179, "type": "DATASET", "confidence": 0.5872880816459656}]}, {"text": "However, for sample I TH , only DAM keeps this strong correlation (\u03c7 2 (14) = 47.71).", "labels": [], "entities": [{"text": "DAM", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.7336934804916382}]}, {"text": "In the case of CE, we find weak evidence in favour of this correlation on instances of I TH (\u03c7 2 (14) = 25.27, p = 0.03).", "labels": [], "entities": [{"text": "I TH", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.7063218057155609}]}, {"text": "For ESIM we find no evidence of correlation (\u03c7 2 (14) = 22.72, p = 0.06), thus we do not reject the null hypothesis.", "labels": [], "entities": [{"text": "ESIM", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7707606554031372}, {"text": "correlation", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9884620904922485}]}, {"text": "Polarity's influence can be observed in Subset 3 of I H, where we observe a drop inaccuracy for instances whose gold labels do not match the polarity of the word pairs, compared to the accuracy of the whole sample; this means that when the models have polarity as a cue, they improve performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9983419179916382}]}, {"text": "All models' performance significantly drop (p < 0.01) after our transformation by 0.208 (CE), 0.061 (DAM) and 0.195 (ESIM) points, where performance of ESIM is comparable to that of CE on both samples, E H and E TH . Compared to the in situ condition, DAM's performance improves, opposite to CE's and ESIM's behavior.", "labels": [], "entities": [{"text": "ESIM", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9349684715270996}]}, {"text": "Insensitivity The drop in performance described above can be partially explained by insensitivity to changes in gold label, since around 93% of the instances in E TH changed gold-label with respect to E H . We find strong statistical evidence for this hypothesis (CE:\u03c7 2 (1) = 175.19, DAM:\u03c7 2 (1) = 158.62, ESIM:\u03c7 2 (1) = 252.27).", "labels": [], "entities": [{"text": "CE:\u03c7 2 (1)", "start_pos": 264, "end_pos": 274, "type": "METRIC", "confidence": 0.8809896962983268}, {"text": "ESIM", "start_pos": 307, "end_pos": 311, "type": "DATASET", "confidence": 0.6634155511856079}]}, {"text": "However, in the case of DAM, this factor seems to play a small role on its behavior as seen when we compare accuracy on Subset 1 with that of the whole transformed sample.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9991227984428406}]}, {"text": "Insensitivity seems to have a bigger influence on the models when the transformed instances are closer to the training set: Accuracy scores on Subset 1 from I TH are smaller than those on Subset 1 from E TH . Unseen Word Pairs Similar to the in situ condition, our homogeneity tests show no evidence for incorrect predictions being due to unseen word pairs (CE:\u03c7 2 (1) = 0.35, p = 0.55, DAM:\u03c7 2 (1) = 2.43, p = 0.11, ESIM:\u03c7 2 (1) = 0.183, p = 0.66).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9920952916145325}, {"text": "ESIM:\u03c7 2 (1)", "start_pos": 417, "end_pos": 429, "type": "METRIC", "confidence": 0.728521432195391}]}, {"text": "We posit the same explanation as before: Models may use hypernymy information contained in the embeddings.", "labels": [], "entities": []}, {"text": "Polarity We find statistically high correlation of the models' predictions with the polarity of the word pairs in the instances from both samples, E H (CE:\u03c7 2 (10) = 261.77, DAM:\u03c7 2 (10) = 312.67, ESIM:\u03c7 2 (10) = 176.38) and E TH (CE:\u03c7 2 (14) = 56.52, DAM:\u03c7 2 (14) = 258.09, ESIM:\u03c7 2 (10) = 105.70).", "labels": [], "entities": [{"text": "DAM", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.6218499541282654}, {"text": "E TH", "start_pos": 225, "end_pos": 229, "type": "METRIC", "confidence": 0.7661944925785065}]}, {"text": "This evidence indicates that all models use, to some extent, the polarity as a feature for predicting class labels.", "labels": [], "entities": [{"text": "predicting class labels", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.8241716623306274}]}], "tableCaptions": [{"text": " Table 1: Accuracy scores of all models. Exp: experiment number. Whole sample: accuracy scores on the  whole sample. Subset 1: subset of transformed instances that have different gold label with respect to  the control instances they were generated from. Subset 2: subset of transformed instances that contain  word pairs unseen at training time. Subset 3: subset of control or transformed instances containing word  pairs whose polarity does not match the instance's gold label.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9955329895019531}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9979568719863892}]}, {"text": " Table 2: Contingency table for ESIM: Predictions  on transformed instances with different gold labels  from those of the control instances.", "labels": [], "entities": []}, {"text": " Table 4: Contingency table for DAM: Predictions  distributed according to the polarity of target word  pairs found in the transformed instances.", "labels": [], "entities": []}]}