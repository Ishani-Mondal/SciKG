{"title": [{"text": "Colorless green recurrent networks dream hierarchically", "labels": [], "entities": []}], "abstractContent": [{"text": "Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language.", "labels": [], "entities": []}, {"text": "We investigate hereto what extent RNNs learn to track abstract hierarchical syntactic structure.", "labels": [], "entities": []}, {"text": "We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions.", "labels": [], "entities": []}, {"text": "We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (\"The colorless green ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas I ate with the chair sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep furiously\"), and, for Italian, we compare model performance to human intuitions.", "labels": [], "entities": [{"text": "colorless green ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas I ate with the chair sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep", "start_pos": 108, "end_pos": 324, "type": "TASK", "confidence": 0.7867441902289519}]}, {"text": "Our language-model-trained RNNs make reliable predictions about long-distance agreement , and do not lag much behind human performance.", "labels": [], "entities": []}, {"text": "We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent neural networks are general sequence processing devices that do not explicitly encode the hierarchical structure that is thought to be essential to natural language (.", "labels": [], "entities": []}, {"text": "Early work using artificial languages showed that they may nevertheless be able to approximate context-free languages.", "labels": [], "entities": []}, {"text": "More recently, RNNs have achieved impressive results in large-scale tasks such as language modeling for speech recognition and machine translation, and are by now standard tools for sequential natural language tasks (e.g.,.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7253564596176147}, {"text": "speech recognition", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.6884194761514664}, {"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7763528823852539}]}, {"text": "This suggests that RNNs may learn to track grammatical structure even when trained on noisier natural data.", "labels": [], "entities": [{"text": "track grammatical structure", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6430191099643707}]}, {"text": "The conjecture is supported by the success of RNNs as feature extractors for syntactic parsing (e.g.,.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.6906115263700485}]}, {"text": "directly evaluated the extent to which RNNs can approximate hierarchical structure in corpus-extracted natural language data.", "labels": [], "entities": []}, {"text": "They tested whether RNNs can learn to predict English subject-verb agreement, a task thought to require hierarchical structure in the general case (\"the girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl the boys like.", "labels": [], "entities": [{"text": "predict English subject-verb agreement", "start_pos": 38, "end_pos": 76, "type": "TASK", "confidence": 0.7074321880936623}, {"text": "girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl", "start_pos": 153, "end_pos": 237, "type": "TASK", "confidence": 0.7841626931639278}]}, {"text": ". is is is is is is is is is is is is is is is is is or are are are are are are are are are are are are are are are are are?\").", "labels": [], "entities": []}, {"text": "Their experiments confirmed that RNNs can, in principle, handle such constructions.", "labels": [], "entities": []}, {"text": "However, in their study RNNs could only succeed when provided with explicit supervision on the target task.", "labels": [], "entities": []}, {"text": "Linzen and colleagues argued that the unsupervised language modeling objective is not sufficient for RNNs to induce the syntactic knowledge necessary to cope with long-distance agreement.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7281590402126312}]}, {"text": "The current paper reevaluates these conclusions.", "labels": [], "entities": []}, {"text": "We strengthen the evaluation paradigm of Linzen and colleagues in several ways.", "labels": [], "entities": []}, {"text": "Most importantly, their analysis did not rule out the possibility that RNNs might be relying on semantic or collocational/frequency-based information, rather than purely on syntactic structure.", "labels": [], "entities": []}, {"text": "about what typically barks (dogs, not neighbourhoods), without relying on more abstract structural cues.", "labels": [], "entities": []}, {"text": "Ina follow-up study to Linzen and colleagues', observed that RNNs are better at long-distance agreement when they construct rich lexical representations of words, which suggests effects of this sort might indeed beat play.", "labels": [], "entities": []}, {"text": "We introduce a method to probe the syntactic abilities of RNNs that abstracts away from potential lexical, semantic and frequency-based confounds.", "labels": [], "entities": []}, {"text": "Inspired by insight that \"grammaticalness cannot be identified with meaningfulness\" (p.", "labels": [], "entities": []}, {"text": "106), we test long-distance agreement both in standard corpus-extracted examples and in comparable nonce sentences that are grammatical but completely meaningless, e.g., (paraphrasing Chomsky): \"The colorless green ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas I ate with the chair sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep furiously\".", "labels": [], "entities": [{"text": "colorless green ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas I ate with the chair sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep", "start_pos": 199, "end_pos": 433, "type": "TASK", "confidence": 0.7945537701249122}]}, {"text": "We extend the previous work in three additional ways.", "labels": [], "entities": []}, {"text": "First, alongside English, which has few morphological cues to agreement, we examine Italian, Hebrew and Russian, which have richer morphological systems.", "labels": [], "entities": []}, {"text": "Second, we go beyond subject-verb agreement and develop an automated method to harvest a variety of long-distance number agreement constructions from treebanks.", "labels": [], "entities": []}, {"text": "Finally, for Italian, we collect human judgments for the tested sentences, providing an important comparison point for RNN performance.", "labels": [], "entities": []}, {"text": "We focus on the more interesting unsupervised setup, where RNNs are trained to perform generic, large-scale language modeling (LM): they are not given explicit evidence, at training time, that they must focus on long-distance agreement, but they are rather required to track a multitude of cues that might help with word prediction in general.", "labels": [], "entities": [{"text": "language modeling (LM)", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.8093225121498108}, {"text": "word prediction", "start_pos": 316, "end_pos": 331, "type": "TASK", "confidence": 0.7836735844612122}]}, {"text": "RNNs trained with a LM objective solve the long-distance agreement problem well, even on nonce sentences.", "labels": [], "entities": []}, {"text": "The pattern is consistent across languages, and, crucially, not far from human performance in Italian.", "labels": [], "entities": []}, {"text": "Moreover, RNN performance on language modeling (measured in terms of perplexity) is a good predictor of long-distance agreement accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9221644401550293}]}, {"text": "This suggests that the ability to capture structural generalizations is an important aspect of what makes the best RNN architectures so good The code to reproduce our experiments and the data used for training and evaluation, including the human judgments in Italian, can be found at https://github.com/ facebookresearch/colorlessgreenRNNs.", "labels": [], "entities": []}, {"text": "Since our positive results contradict, to some extent, those of, we also replicate their relevant experiment using our best RNN (an LSTM).", "labels": [], "entities": []}, {"text": "We outperform their models, suggesting that a careful architecture/hyperparameter search is crucial to obtain RNNs that are not only good at language modeling, but able to extract syntactic generalizations.", "labels": [], "entities": []}, {"text": "2 Constructing a long-distance agreement benchmark Overview.", "labels": [], "entities": [{"text": "Overview", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9395692944526672}]}, {"text": "We construct our number agreement test sets as follows.", "labels": [], "entities": []}, {"text": "Original sentences are automatically extracted from a dependency treebank.", "labels": [], "entities": []}, {"text": "They are then converted into nonce sentences by substituting all content words with random words with the same morphology, resulting in grammatical but nonsensical sequences.", "labels": [], "entities": []}, {"text": "An LM is evaluated on its predictions for the target (second) word in the dependency, in both the original and nonce sentences.", "labels": [], "entities": []}, {"text": "Agreement relations, such as subject-verb agreement in English, are an ideal test bed for the syntactic abilities of LMs, because the form of the second item (the target) is predictable from the first item (the cue).", "labels": [], "entities": []}, {"text": "Crucially, the cue and the target are linked by a structural relation, where linear order in the word sequence does not matter ( . In all these cases, the number of the main verb \"thinks\" is determined by its subject (\"girl\"), and this relation depends on the syntactic structure of the sentence, not on the linear sequence of words.", "labels": [], "entities": []}, {"text": "As the last sentence shows, the word directly preceding the verb can even be a noun with the opposite number (\"friends\"), but this does not influence the structurally-determined form of the verb.", "labels": [], "entities": []}, {"text": "When the cue and the target are adjacent (\"the girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks thinks.", "labels": [], "entities": []}, {"text": "\"), an LM can predict the target without access to syntactic structure: it can simply extract the relevant morphosyntactic features of words (e.g., number) and record the co-occurrence frequencies of patterns such as NP lur VP lur  ber of words can occur between the elements of the agreement relation.", "labels": [], "entities": []}, {"text": "We limit ourselves to number agreement (plural or singular), as it is the only overt agreement feature shared by all of the languages we study.", "labels": [], "entities": []}, {"text": "We started by collecting pairs of part-of-speech (POS) tags connected by a dependency arc.", "labels": [], "entities": []}, {"text": "Independently of which element is the head of the relation, we refer to the first item as the cue and to the second as the target.", "labels": [], "entities": []}, {"text": "We additionally refer to the POS sequence characterizing the entire pattern as a construction, and to the elements in the middle as context.", "labels": [], "entities": []}, {"text": "For each candidate construction, we collected all of the contexts in the corpus that intervene between the cue and the target (we define contexts as the sequence of POS tags of the top-level nodes in the dependency subtrees).", "labels": [], "entities": []}, {"text": "For example, for the English subject-verb agreement construction shown in, the context is defined by VERB (head of the relative clause) and ADV (adverbial modifier of the target verb), which together dominate the sequence \"the boys like often\".", "labels": [], "entities": [{"text": "English subject-verb agreement construction", "start_pos": 21, "end_pos": 64, "type": "TASK", "confidence": 0.6080886349081993}, {"text": "VERB", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9938012957572937}]}, {"text": "For the Russian adjective-noun agreement construction in, the context is NOUN, because in the dependency grammar we use the noun \"moment\" is the head of the prepositional phrase \"at that moment\", which modifies the adjective \"deep\".", "labels": [], "entities": [{"text": "Russian adjective-noun agreement construction", "start_pos": 8, "end_pos": 53, "type": "TASK", "confidence": 0.5700053200125694}, {"text": "NOUN", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9103094339370728}]}, {"text": "The candidate agreement pair and the context form a construction, which is characterized by a sequence of POS tags, e.g., NOUN VERB ADV VERB or VERB NOUN CCONJ VERB.", "labels": [], "entities": [{"text": "NOUN VERB ADV VERB", "start_pos": 122, "end_pos": 140, "type": "METRIC", "confidence": 0.7599507123231888}, {"text": "VERB NOUN CCONJ VERB", "start_pos": 144, "end_pos": 164, "type": "DATASET", "confidence": 0.5695885941386223}]}, {"text": "Our constructions do not necessarily correspond to standard syntactic structures.", "labels": [], "entities": []}, {"text": "The English subject-verb agreement construction NOUN VERB VERB, for example, matches both object and subject relative clause contexts, e.g., \"girl girl girl girl girl is is is is is\" and \"girls girls girls girls girls girls girls girls girls girls girls girls girls girls girls girls girls who stayed at home were were were were were were were were were were were were were were were were were\".", "labels": [], "entities": [{"text": "NOUN VERB VERB", "start_pos": 48, "end_pos": 62, "type": "METRIC", "confidence": 0.730501651763916}]}, {"text": "Conversely, standard syntactic structures might be split between different constructions, e.g., relative clause contexts occur in both NOUN VERB VERB and NOUN VERB ADV VERB constructions (the latter is illustrated by the English example in).", "labels": [], "entities": [{"text": "NOUN VERB VERB", "start_pos": 135, "end_pos": 149, "type": "METRIC", "confidence": 0.5184055467446645}, {"text": "NOUN VERB ADV VERB constructions", "start_pos": 154, "end_pos": 186, "type": "TASK", "confidence": 0.46563541889190674}]}, {"text": "Construction contexts can contain a variable numbers of words.", "labels": [], "entities": []}, {"text": "Since we are interested in challenging cases, we only considered cases in which at least three tokens intervened between the cue and the target.", "labels": [], "entities": []}, {"text": "In the next step, we excluded constructions in which the candidate cue and target did not agree in number in all of the instances of the construction in the treebank (if both the cue and the target were morphologically annotated for number).", "labels": [], "entities": []}, {"text": "This step retained English subject-verb constructions, for example, but excluded verb-object constructions, since any form of a verb can appear both with singular and plural objects.", "labels": [], "entities": []}, {"text": "To focus on robust agreement patterns, we only kept constructions with at least 10 instances of both plural and singular agreement.", "labels": [], "entities": []}, {"text": "When applied to the treebanks we used (see Section 3), this step resulted in between two (English) and 21 (Russian) constructions per lan-guage.", "labels": [], "entities": []}, {"text": "English has the poorest morphology and consequently the lowest number of patterns with identifiable morphological agreement.", "labels": [], "entities": []}, {"text": "Only the VP-conjunction construction was identified in all four languages.", "labels": [], "entities": []}, {"text": "Subject-verb agreement constructions were extracted in all languages but Russian; Russian has relatively flexible word order and a noun dependent preceding ahead verb is not necessarily its subject.", "labels": [], "entities": [{"text": "Subject-verb agreement constructions", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6786298851172129}]}, {"text": "The full list of extracted constructions in English and Italian is given in Tables 2 and 3, respectively.", "labels": [], "entities": []}, {"text": "For the other languages, seethe Supplementary Material (SM).", "labels": [], "entities": [{"text": "Supplementary Material (SM)", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.5112593054771424}]}, {"text": "Our \"original\" sentence test set included all sentences from each construction where all words from the cue and up to and including the target occurred in the LM vocabulary (Section 3), and where the singular/plural counterpart of the target occurred in the treebank and in the language model vocabulary (this is required by the evaluation procedure outlined below).", "labels": [], "entities": []}, {"text": "The total counts of constructions and original sentences in our test sets are provided in Table 1.", "labels": [], "entities": []}, {"text": "The average number of context words separating the cue and the target ranged from 3.6 (Hebrew) to 4.5 (Italian).", "labels": [], "entities": []}, {"text": "We generated nine nonce variants of each original sentence as follows.", "labels": [], "entities": []}, {"text": "Each content word (noun, verb, adjective, proper noun, numeral, adverb) in the sentence was substituted by another random content word from the treebank with matching POS and morphological features.", "labels": [], "entities": []}, {"text": "To avoid forms that are ambiguous between several POS, which are particularly frequent in English (e.g., plural noun and singular verb forms), we excluded the forms that appeared with a different POS more than 10% of the time in the treebank.", "labels": [], "entities": []}, {"text": "Function words (determiners, pronouns, adpositions, particles) and punctuation were left intact.", "labels": [], "entities": []}, {"text": "For example, we generated the nonce (1b) from the original sentence (1a): ( Note that our generation procedure is based on morphological features and does not guarantee that argument structure constraints are respected The SM is available as a standalone file on the project's public repository.", "labels": [], "entities": []}, {"text": "(e.g., \"it stays the shuttle\" in (1b)).", "labels": [], "entities": []}, {"text": "For each sentence in our test set, we retrieved from our treebank the form that is identical to the agreement target in all morphological features except number (e.g., \"finds\" instead of \"find\" in (1b)).", "labels": [], "entities": []}, {"text": "Given a sentence with prefix pup to and excluding the target, we then compute the probabilities P (t 1 |p) and P (t 2 |p) for the singular and plural variants of the target, t 1 and t 2 , based on the language model.", "labels": [], "entities": []}, {"text": "Following, we say that the model identified the correct target if it assigned a higher probability to the form with the correct number.", "labels": [], "entities": []}, {"text": "In (1b), for example, the model should assign a higher probability to \"finds\" than \"find\".", "labels": [], "entities": []}], "datasetContent": [{"text": "We extracted our test sets from the Italian, English, Hebrew and Russian Universal Dependency treebanks (UD, v2.0,.", "labels": [], "entities": [{"text": "Universal Dependency treebanks", "start_pos": 73, "end_pos": 103, "type": "DATASET", "confidence": 0.6198087831338247}]}, {"text": "The English and Hebrew treebanks were post-processed to obtain a richer morphological annotation at the word level (see SM for details).", "labels": [], "entities": []}, {"text": "Training data for Italian, English and Russian were extracted from the respective Wikipedias.", "labels": [], "entities": []}, {"text": "We downloaded recent dumps, extracted the raw text from them using WikiExtractor and tokenized it with TreeTagger.", "labels": [], "entities": [{"text": "TreeTagger", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.9648910164833069}]}, {"text": "We also used the TreeTagger lemma annotation to filter out sentences with more than 5% unknown words.", "labels": [], "entities": []}, {"text": "For Hebrew, we used the preprocessed Wikipedia corpus made available by Yoav Goldberg.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.862031102180481}]}, {"text": "We extracted 90M token subsets for each language, shuffled them by sentence and split them into training and validation sets (8-to-1 proportion).", "labels": [], "entities": []}, {"text": "For LM training, we included the 50K most frequent words in each corpus in the vocabulary, replacing the other tokens with the UNK symbol.", "labels": [], "entities": []}, {"text": "The validation set perplexity values we report below exclude unknown tokens.", "labels": [], "entities": []}, {"text": "We experimented with simple RNNs (sRNNs, Elman, 1990), and their most successful variant, long-short term memory models (LSTMs, Hochreiter and Schmidhu-ber, 1997).", "labels": [], "entities": []}, {"text": "We use the PyTorch RNN implementation.", "labels": [], "entities": [{"text": "PyTorch RNN implementation", "start_pos": 11, "end_pos": 37, "type": "DATASET", "confidence": 0.9134244720141093}]}, {"text": "We trained the models with two hidden layer dimensionalities (650 and 200 units), and a range of batch sizes, learning rates and dropout rates.", "labels": [], "entities": []}, {"text": "See SM for details on hyperparameter tuning.", "labels": [], "entities": []}, {"text": "In general, a larger hidden layer size was the best predictor of lower perplexity.", "labels": [], "entities": []}, {"text": "Given that our LSTMs outperformed our sRNNs, our discussion of the results will focus on the former; we will use the terms LSTM and RNN interchangeably.", "labels": [], "entities": []}, {"text": "We consider three baselines: first, a unigram baseline, which picks the most frequent form in the training corpus out of the two candidate target forms (singular or plural); second, a 5-gram model with Kneser-Ney smoothing) trained using the IRSTLM package) and queried using KenLM (Heafield, 2011); and third, a 5-gram LSTM, which only had access to windows of five tokens ().", "labels": [], "entities": []}, {"text": "Compared to KN, the 5-gram LSTM can generalize to unseen ngrams thanks to its embedding layer and recurrent connections.", "labels": [], "entities": []}, {"text": "However, it cannot discover longdistance dependency patterns that span more than five words.", "labels": [], "entities": []}, {"text": "See SM for details on the hyperparameters of this baseline.", "labels": [], "entities": []}, {"text": "We presented the full Italian test set (119 original and 1071 nonce sentences) to human subjects through the Amazon Mechanical Turk interface.", "labels": [], "entities": [{"text": "Italian test set", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.9196929931640625}, {"text": "Amazon Mechanical Turk interface", "start_pos": 109, "end_pos": 141, "type": "DATASET", "confidence": 0.9265760630369186}]}, {"text": "8 We picked Italian because, being morphologically richer, it features more varied long-distance constructions than English.", "labels": [], "entities": []}, {"text": "Subjects were requested to be native Italian speakers.", "labels": [], "entities": []}, {"text": "They were presented with a sentence up to and excluding the target.", "labels": [], "entities": []}, {"text": "The singular and plural forms of the target were presented below the sentence (in random order), and subjects were asked to select the more plausible form.", "labels": [], "entities": []}, {"text": "To prevent long-distance agreement patterns from being too salient, we mixed the test set with the same number of filler sentences.", "labels": [], "entities": []}, {"text": "We started from original fillers, which were random treebankextracted sentences up to a content word in singular or plural form.", "labels": [], "entities": []}, {"text": "We then generated nonce fillers from the original ones using the procedure outlined in Section 2.", "labels": [], "entities": []}, {"text": "A control subset of 688 fillers was manually selected by a linguistically-trained: Experimental results for all languages averaged across the five best models in terms of perplexity on the validation set.", "labels": [], "entities": []}, {"text": "Original/Nonce rows report percentage accuracy, and the numbers in small print represent standard deviation within the five best models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9172430634498596}]}, {"text": "Italian native speaker as unambiguous cases.", "labels": [], "entities": []}, {"text": "To make sure we were only using data from native (or at least highly proficient) Italian speakers, we filtered out the responses of subjects who chose the wrong target in more than 20% of the fillers.", "labels": [], "entities": []}, {"text": "We collected on average 9.5 judgments for each item (minimum 5 judgments).", "labels": [], "entities": []}, {"text": "To account for the variable number of judgments across sentences, accuracy rates were first calculated within each sentence and then averaged across sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9992720484733582}]}], "tableCaptions": [{"text": " Table 1: Experimental results for all languages av- eraged across the five best models in terms of per- plexity on the validation set. Original/Nonce rows  report percentage accuracy, and the numbers in  small print represent standard deviation within the  five best models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.8713676333427429}]}, {"text": " Table 2: LSTM accuracy in the constructions  N V V (subject-verb agreement with an interven- ing embedded clause) and V NP conj V (agree- ment between conjoined verbs separated by a  complement of the first verb).", "labels": [], "entities": [{"text": "LSTM", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.6704121828079224}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9637907147407532}]}, {"text": " Table 3: Subject and LSTM accuracy on the Italian test set, by construction and averaged.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.7856462597846985}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9102688431739807}, {"text": "Italian test set", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.9728008111317953}]}]}