{"title": [{"text": "Combining Character and Word Information in Neural Machine Translation Using a Multi-Level Attention", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.6480301717917124}]}], "abstractContent": [{"text": "Natural language sentences, being hierarchical , can be represented at different levels of granularity, like words, subwords, or characters.", "labels": [], "entities": []}, {"text": "But most neural machine translation systems require the sentence to be represented as a sequence at a single level of granularity.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.8159132401148478}]}, {"text": "It can be difficult to determine which granularity is better fora particular translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.8969094455242157}]}, {"text": "In this paper, we improve the model by incorporating multiple levels of granularity.", "labels": [], "entities": []}, {"text": "Specifically, we propose (1) an encoder with character attention which augments the (sub)word-level representation with character-level information; (2) a decoder with multiple attentions that enable the representations from different levels of granularity to control the translation cooperatively.", "labels": [], "entities": []}, {"text": "Experiments on three translation tasks demonstrate that our proposed models out-perform the standard word-based model, the subword-based model and a strong character-based model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) models) learn to map from source language sentences to target language sentences via continuous-space intermediate representations.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7634361485640208}]}, {"text": "Since word is usually thought of as the basic unit of language communication, early NMT systems built these representations starting from the word level (.", "labels": [], "entities": []}, {"text": "Later systems tried using smaller units such as subwords to address the problem of out-of-vocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "Although they obtain reasonable results, these word or sub-word methods still have some potential weaknesses.", "labels": [], "entities": []}, {"text": "First, the learned representations * Corresponding author. of (sub)words are based purely on their contexts, but the potentially rich information inside the unit itself is seldom explored.", "labels": [], "entities": []}, {"text": "Taking the Chinese word (bei-da-shang) as an example, the three characters in this word area passive voice marker, \"hit\" and \"wound\", respectively.", "labels": [], "entities": [{"text": "wound", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.9941054582595825}]}, {"text": "The meaning of the whole word, \"to be wounded\", is fairly compositional.", "labels": [], "entities": []}, {"text": "But this compositionality is ignored if the whole word is treated as a single unit.", "labels": [], "entities": []}, {"text": "Secondly, obtaining the word or sub-word boundaries can be non-trivial.", "labels": [], "entities": []}, {"text": "For languages like Chinese and Japanese, a word segmentation step is needed, which must usually be trained on labeled data.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7303504794836044}]}, {"text": "For languages like English and German, word boundaries are easy to detect, but subword boundaries need to be learned by methods like BPE.", "labels": [], "entities": []}, {"text": "In both cases, the segmentation model is trained only in monolingual data, which may result in units that are not suitable for translation.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.9697902202606201}]}, {"text": "On the other hand, there have been multiple efforts to build models operating purely at the character level (.", "labels": [], "entities": []}, {"text": "But splitting this finely can increase potential ambiguities.", "labels": [], "entities": []}, {"text": "For example, the Chinese word (hong-cha) means \"black tea,\" but the two characters means \"red\" and \"tea,\" respectively.", "labels": [], "entities": []}, {"text": "It shows that modeling the character sequence alone may not be able to fully utilize the information at the word or sub-word level, which may also lead to an inaccurate representation.", "labels": [], "entities": []}, {"text": "A further problem is that character sequences are longer, making them more costly to process with a recurrent neural network model (RNN).", "labels": [], "entities": []}, {"text": "While both word-level and character-level information can be helpful for generating better representations, current research which tries to exploit both word-level and character-level information only composed the word-level representation by character embeddings with the word boundary information ( or replaces the word representation with its inside characters when encountering the out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel encoder-decoder model that makes use of both character and word information.", "labels": [], "entities": []}, {"text": "More specifically, we augment the standard encoder to attend to individual characters to generate better source word representations ( \u00a73.1).", "labels": [], "entities": []}, {"text": "We also augment the decoder with a second attention that attends to the source-side characters to generate better translations ( \u00a73.2).", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of the proposed model, we carryout experiments on three translation tasks: Chinese-English, EnglishChinese and English-German.", "labels": [], "entities": []}, {"text": "Our experiments show that: (1) the encoder with character attention achieves significant improvements over the standard word-based attention-based NMT system and a strong character-based NMT system; (2) incorporating source character information into the decoder by our multi-scale attention mechanism yields a further improvement, and (3) our modifications also improve a subword-based NMT model.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work that uses the source-side character information for all the (sub)words in the sentence to enhance a (sub)word-based NMT model in both the encoder and decoder.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on three translation tasks: Chinese-English (Zh-En), English-Chinese (En-Zh) and English-German (En-De).", "labels": [], "entities": []}, {"text": "We write Zh\u2194En to refer to the Zh-En and En-Zh tasks together.", "labels": [], "entities": []}, {"text": "For Zh\u2194En, the parallel training data consists of 1.6M sentence pairs extracted from LDC corpora, with 46.6M Chinese words and 52.5M English words, respectively.", "labels": [], "entities": []}, {"text": "1 We use the NIST MT02 evaluation data as development data, and MT03, MT04, MT05, and MT06 as test data.", "labels": [], "entities": [{"text": "NIST MT02 evaluation data", "start_pos": 13, "end_pos": 38, "type": "DATASET", "confidence": 0.8648925274610519}, {"text": "MT03", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.9030482769012451}, {"text": "MT04", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8256953358650208}, {"text": "MT05", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.8355422616004944}, {"text": "MT06", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.9061766862869263}]}, {"text": "The Chinese side of the corpora is word segmented using ICT-CLAS.", "labels": [], "entities": [{"text": "word segmented", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7062762081623077}, {"text": "ICT-CLAS", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.7387282252311707}]}, {"text": "The English side of the corpora is lowercased and tokenized.", "labels": [], "entities": []}, {"text": "For En-De, we conduct our experiments on the WMT17 corpus.", "labels": [], "entities": [{"text": "WMT17 corpus", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.9701922535896301}]}, {"text": "We use the pre-processed parallel training data for the shared news translation task provided by the task organizers.", "labels": [], "entities": [{"text": "shared news translation task", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.720224030315876}]}, {"text": "The dataset consitst of 5.6M sentence pairs.", "labels": [], "entities": []}, {"text": "We use newstest2016 as the development set and evaluate the models on newstest2017.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the encoder with character attention and the encoder with word attention. Char-att and  Word-att denotes the encoder with character attention and the encoder with word attention, respectively.", "labels": [], "entities": []}, {"text": " Table 2: Performance of different systems on the Chinese-English and English-Chinese translation tasks. Our  encoder with character attention (Char-att) improves over all other models on Zh-En and over the word-based  baseline on En-Zh. Adding our decoder with multi-scale attention (Multi-att) outperforms all other models.", "labels": [], "entities": [{"text": "character attention (Char-att)", "start_pos": 123, "end_pos": 153, "type": "METRIC", "confidence": 0.5655127108097077}]}, {"text": " Table 3: Comparison of our models on top of the BPE-based NMT model and the original BPE-based model on  the Chinese-English and English-Chinese translation tasks. Our models improve over the BPE baselines.", "labels": [], "entities": [{"text": "BPE-based NMT model", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.8495158354441324}, {"text": "English-Chinese translation tasks", "start_pos": 130, "end_pos": 163, "type": "TASK", "confidence": 0.7184706727663676}, {"text": "BPE baselines", "start_pos": 193, "end_pos": 206, "type": "DATASET", "confidence": 0.9032790064811707}]}, {"text": " Table 4: Case-sensitive BLEU on the English-German  translation tasks. Our systems improve over a baseline  BPE system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9401207566261292}]}, {"text": " Table 5: Sample translations. For each example, we show the source, the reference and the translation from our  best model. \"Ours\" means our model with both Char-att and Multi-att.", "labels": [], "entities": []}, {"text": " Table 6: Translation performance on source sentences  with and without OOV words. \"Ours\" means our model  with both Char-att and Multi-att.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.965490460395813}]}]}