{"title": [{"text": "An Evaluation of Image-based Verb Prediction Models against Human Eye-tracking Data", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent research in language and vision has developed models for predicting and disam-biguating verbs from images.", "labels": [], "entities": []}, {"text": "Here, we ask whether the predictions made by such models correspond to human intuitions about visual verbs.", "labels": [], "entities": []}, {"text": "We show that the image regions a verb prediction model identifies as salient fora given verb correlate with the regions fixated by human observers performing a verb classification task.", "labels": [], "entities": [{"text": "verb prediction", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.742004930973053}, {"text": "verb classification task", "start_pos": 160, "end_pos": 184, "type": "TASK", "confidence": 0.7891969680786133}]}], "introductionContent": [{"text": "Recent research in language and vision has applied fundamental NLP tasks in a multimodal setting.", "labels": [], "entities": []}, {"text": "An example is word sense disambiguation (WSD), the task of assigning a word the correct meaning in a given context.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.8588098088900248}]}, {"text": "WSD traditionally uses textual context, but disambiguation can be performed using an image context instead, relying on the fact that different word senses are often visually distinct.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8712948560714722}]}, {"text": "Early work has focused on the disambiguation of nouns (, but more recent research has proposed visual sense disambiguation models for verbs.", "labels": [], "entities": []}, {"text": "This is a considerably more challenging task, as unlike objects (denoted by nouns), actions (denoted by verbs) are often not clearly localized in an image.", "labels": [], "entities": []}, {"text": "propose a two-stage approach, consisting of a verb prediction model, which labels an image with potential verbs, followed by a visual sense disambiguation model, which uses the image to determine the correct verb senses.", "labels": [], "entities": [{"text": "verb prediction", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7283355295658112}]}, {"text": "While this approach achieves good verb prediction and sense disambiguation accuracy, it is not clear to what extend the model captures human intuitions about visual verbs.", "labels": [], "entities": [{"text": "verb prediction", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.7127436697483063}, {"text": "sense disambiguation", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.6668690145015717}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9180622100830078}]}, {"text": "Specifically, it is interesting to ask whether the image regions that the model identifies as salient fora given verb correspond to the regions a human observer relies on when determining which verb is depicted.", "labels": [], "entities": []}, {"text": "The output of a verb prediction model can be visualized as a heatmap over the image, where hot colors indicate the most salient areas fora given task (see for examples).", "labels": [], "entities": [{"text": "verb prediction", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7179548889398575}]}, {"text": "In the same way, we can determine which regions a human observes attends to by eye-tracking them while viewing the image.", "labels": [], "entities": []}, {"text": "Eye-tracking data consists a stream of gaze coordinates, which can also be turned into a heatmap.", "labels": [], "entities": []}, {"text": "Model predictions correspond to human intuitions if the two heatmaps correlate.", "labels": [], "entities": []}, {"text": "In the present paper, we show that the heatmaps generated by the verb prediction model of correlate well with heatmaps obtained from human observers performing a verb classification task.", "labels": [], "entities": [{"text": "verb prediction", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7253396213054657}, {"text": "verb classification task", "start_pos": 162, "end_pos": 186, "type": "TASK", "confidence": 0.7955117324988047}]}, {"text": "We achieve a higher correlation than a range of baselines (center bias, visual salience, and model combinations), indicating that the verb prediction model successfully identifies those image regions that are indicative of the verb depicted in the image.", "labels": [], "entities": [{"text": "verb prediction", "start_pos": 134, "end_pos": 149, "type": "TASK", "confidence": 0.7062155902385712}]}], "datasetContent": [{"text": "The PASCAL VOC 2012 Actions Fixation dataset (Mathe and Sminchisescu, 2013) contains 9,157 images covering 10 action classes (phoning, reading, jumping, running, walking, riding bike, riding horse, playing instrument, taking photo, using computer).", "labels": [], "entities": [{"text": "PASCAL VOC 2012 Actions Fixation dataset (Mathe and Sminchisescu, 2013)", "start_pos": 4, "end_pos": 75, "type": "DATASET", "confidence": 0.8845986265402573}]}, {"text": "Each image is annotated with the eyefixations of eight human observers who, for each image, were asked to recognize the action depicted and respond with one of the class labels.", "labels": [], "entities": []}, {"text": "Participants were given three seconds to freely view an image while the x-and y-coordinates of their gaze positions were recorded.", "labels": [], "entities": []}, {"text": "(Note that the original dataset also contained a control condition in which four participants performed visual search; we do not use the data from this control condition.)", "labels": [], "entities": []}, {"text": "In  the eye-tracking setup used, including information on measurement error, please refer to, who used the same setup as.", "labels": [], "entities": []}, {"text": "While actions and verbs are distinct concepts (, we can still use the PAS-CAL Actions Fixation data to evaluate our model.", "labels": [], "entities": [{"text": "PAS-CAL Actions Fixation data", "start_pos": 70, "end_pos": 99, "type": "DATASET", "confidence": 0.6308647096157074}]}, {"text": "When predicting a verb, the model presumably has to attend to the same regions that humans fixate on when working out which action is depicted -all the actions in the dataset are verb-based, hence recognizing the verb is part of recognizing the action.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Table of average rank correlation scores for the verb prediction model (M), compared with the upper  bound of average human-human agreement (H), center bias (CB) baseline (Clarke and Tatler, 2014), and salience  map (SM) baseline (Liu and Han, 2016). Results are reported on the validation set of the PASCAL VOC 2012  Actions Fixation data (Mathe and Sminchisescu, 2013). The best score for each class is shown in bold (except  upper bound). Model combination are by mean of heatmaps.", "labels": [], "entities": [{"text": "verb prediction", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.7018924206495285}, {"text": "center bias (CB) baseline", "start_pos": 155, "end_pos": 180, "type": "METRIC", "confidence": 0.912442535161972}, {"text": "salience  map (SM) baseline", "start_pos": 212, "end_pos": 239, "type": "METRIC", "confidence": 0.8847455978393555}, {"text": "PASCAL VOC 2012  Actions Fixation data", "start_pos": 311, "end_pos": 349, "type": "DATASET", "confidence": 0.8024920523166656}]}]}