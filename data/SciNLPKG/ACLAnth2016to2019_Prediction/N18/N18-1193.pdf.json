{"title": [{"text": "Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos", "labels": [], "entities": [{"text": "Emotion Recognition in Dyadic Dialogue Videos", "start_pos": 34, "end_pos": 79, "type": "TASK", "confidence": 0.7705856064955393}]}], "abstractContent": [{"text": "Emotion recognition in conversations is crucial for the development of empathetic machines.", "labels": [], "entities": [{"text": "Emotion recognition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.907029390335083}]}, {"text": "Present methods mostly ignore the role of inter-speaker dependency relations while classifying emotions in conversations.", "labels": [], "entities": []}, {"text": "In this paper, we address recognizing utterance-level emotions in dyadic conversational videos.", "labels": [], "entities": []}, {"text": "We propose a deep neural framework, termed conversational memory network, which leverages contextual information from the conversation history.", "labels": [], "entities": []}, {"text": "The framework takes a multimodal approach comprising audio, visual and textual features with gated recurrent units to model past utterances of each speaker into memories.", "labels": [], "entities": []}, {"text": "Such memories are then merged using attention-based hops to capture inter-speaker dependencies.", "labels": [], "entities": []}, {"text": "Experiments show an accuracy improvement of 3\u22124% over the state of the art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9996426105499268}]}], "introductionContent": [{"text": "Development of machines with emotional intelligence has been a long-standing goal of AI.", "labels": [], "entities": []}, {"text": "With the increasing infusion of interactive systems in our lives, the need for empathetic machines with emotional understanding is paramount.", "labels": [], "entities": []}, {"text": "Previous research in affective computing has looked at dialogues as an essential basis to learn emotional dynamics (;.", "labels": [], "entities": [{"text": "learn emotional dynamics", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.6789069573084513}]}, {"text": "Since the advent of Web 2.0, dialogue videos have proliferated across the internet through platforms like movies, webinars, and video chats.", "labels": [], "entities": []}, {"text": "Emotion detection from such resources can benefit numerous fields like counseling, public opinion mining ( , financial forecasting (, and intelligent systems such as smart homes and chatbots (.", "labels": [], "entities": [{"text": "Emotion detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9249567687511444}, {"text": "public opinion mining", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.6570965250333151}, {"text": "financial forecasting", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.7259030491113663}]}, {"text": "In this paper, we analyze emotion detection in videos of dyadic conversations.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7448746860027313}]}, {"text": "A dyadic conversation is a form of a dialogue between two entities.", "labels": [], "entities": []}, {"text": "We propose a conversational memory network (CMN), which uses a multimodal approach for emotion detection in utterances (a unit of speech bound by breathes or pauses) of such conversational videos.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7628120183944702}]}, {"text": "Emotional dynamics in a conversation is known to be driven by two prime factors: self and interspeaker emotional influence;.", "labels": [], "entities": []}, {"text": "Self-influence relates to the concept of emotional inertia, i.e., the degree to which a person's feelings carryover from one moment to another.", "labels": [], "entities": []}, {"text": "Inter-speaker emotional influence is another trait where the other person acts as an influencer in the speaker's emotional state.", "labels": [], "entities": []}, {"text": "Conversely, speakers also tend to mirror emotions of their counterparts (.", "labels": [], "entities": []}, {"text": "provides an example from the dataset showing the presence of these two traits in a dialogue.", "labels": [], "entities": []}, {"text": "Existing works in the literature do not capitalize on these two factors.", "labels": [], "entities": []}, {"text": "Context-free systems infer emotions based only on the current utterance in the conversation (.", "labels": [], "entities": []}, {"text": "Whereas, state-of-the-art context-based networks like, use long short-term memory (LSTM) networks to model speaker-based context that suffers from incapability of long-range summarization and unweighted influence from context, leading to model bias.", "labels": [], "entities": []}, {"text": "Our proposed CMN incorporates these factors by using emotional context information present in the conversation history.", "labels": [], "entities": []}, {"text": "It improves speakerbased emotion modeling by using memory networks which are efficient in capturing long-term dependencies and summarizing task-specific details using attention models (.", "labels": [], "entities": [{"text": "speakerbased emotion modeling", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.7289928793907166}, {"text": "summarizing task-specific details", "start_pos": 127, "end_pos": 160, "type": "TASK", "confidence": 0.8800497651100159}]}, {"text": "Specifically, the memory cells of CMN are continuous vectors that store the context information found in the utterance histories.", "labels": [], "entities": []}, {"text": "CMN also models interplay of these memories to capture interspeaker dependencies.", "labels": [], "entities": [{"text": "CMN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8661736845970154}]}, {"text": "CMN first extracts multimodal features (audio, visual, and text) for all utterances in a video.", "labels": [], "entities": [{"text": "CMN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7897348403930664}]}, {"text": "In order to detect the emotion of a particular utterance, say u i , it gathers its histories by collecting previous utterances within a context window.", "labels": [], "entities": []}, {"text": "Separate histories are created for both speakers.", "labels": [], "entities": []}, {"text": "These histories are then modeled into memory cells using gated recurrent units (GRUs).", "labels": [], "entities": []}, {"text": "After that, CMN reads both the speaker's memories and employs attention mechanism on them, in order to find the most useful historical utterances to classify u i . The memories are then merged with u i using an addition operation weighted by the attention scores.", "labels": [], "entities": []}, {"text": "This is done to model inter-speaker influences and dynamics.", "labels": [], "entities": []}, {"text": "The whole cycle is repeated for multiple hops and finally, this merged representation of utterance u i is used to classify its emotion category.", "labels": [], "entities": []}, {"text": "The contributions of this paper can be summarized as follows: 1.", "labels": [], "entities": []}, {"text": "We propose an architecture, termed CMN, for emotion detection in a dyadic conversation that considers utterance histories of both the speaker to model emotional dynamics.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7523592412471771}]}, {"text": "The architecture is extensible to multi-speaker conversations in formats such as textual dialogues or conversational videos.", "labels": [], "entities": []}, {"text": "2. When applied to videos, we adopt a multimodal approach to extract diverse features from utterances.", "labels": [], "entities": []}, {"text": "It also makes our model robust to missing information.", "labels": [], "entities": []}, {"text": "3. CMN provides a significant increase inaccuracy of 3 \u2212 4% over previous state-of-the-art networks.", "labels": [], "entities": [{"text": "CMN", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.8175898790359497}, {"text": "inaccuracy", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9880267381668091}]}, {"text": "One variant called CMN self which does not consider the inter-speaker relation in emotion detection also outperforms the state of the art by a significant margin.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7488506734371185}]}, {"text": "The remainder of the paper is organized as follows: Section 2 provides a brief literature review; Section 3 formalizes the problem statement; Section 4 describes the proposed method in detail; exSo you're leaving tomorrow.", "labels": [], "entities": []}, {"text": "[sad] Yeah, they just called.", "labels": [], "entities": []}, {"text": "I don't know what to say.", "labels": [], "entities": []}, {"text": "I don't want to go but I don't have a choice.", "labels": [], "entities": []}, {"text": "I am afraid when you leave you won't comeback.", "labels": [], "entities": []}, {"text": "I have to do this.", "labels": [], "entities": []}, {"text": "Do you think I want to miss seeing her (their daughter) grow?", "labels": [], "entities": []}, {"text": "You don't have to do this.", "labels": [], "entities": []}, {"text": "We're not a complete family without you being here.", "labels": [], "entities": []}, {"text": "Well Ill comeback, what are you not willing to wait for me?", "labels": [], "entities": []}, {"text": "I feel much better now.", "labels": [], "entities": []}, {"text": "[ang] We are not a complete family here, you don't understand.", "labels": [], "entities": []}, {"text": "Person B Person A Time: An abridged dialogue from the dataset.", "labels": [], "entities": []}, {"text": "Person A (wife) is leaving B (husband) fora work assignment.", "labels": [], "entities": []}, {"text": "Initially both A and B are emotionally driven by their own emotional inertia.", "labels": [], "entities": []}, {"text": "In the end, emotional influence can be seen when B, despite being sad, reacts angrily to A's angry statement.", "labels": [], "entities": []}, {"text": "perimental results are covered in Section 5; finally, Section 6 provides concluding remarks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Percentage of occurrence of different cases in the", "labels": [], "entities": []}, {"text": " Table 2: Comparison of CMN and its variants with state-of-the-art models (Section 5.2.2). All results use multi- modal features. We report scores using weighted accuracy (WAA) and unweighted recall (UAR). UAR is a popular  metric that is used when dealing with imbalanced classes (Rosenberg, 2012). Results are an average of 10 runs  with varied weight initializations. We assert significance when p < 0.05 under McNemar's test.", "labels": [], "entities": [{"text": "accuracy (WAA)", "start_pos": 162, "end_pos": 176, "type": "METRIC", "confidence": 0.8919353485107422}, {"text": "recall (UAR)", "start_pos": 192, "end_pos": 204, "type": "METRIC", "confidence": 0.9486054927110672}, {"text": "McNemar's test", "start_pos": 414, "end_pos": 428, "type": "DATASET", "confidence": 0.8272219300270081}]}, {"text": " Table 3: Comparison of CMN to all the baselines in  different modalities. Weighted accuracy is used as the  metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9131051301956177}]}]}