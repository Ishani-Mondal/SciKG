{"title": [{"text": "Read and Comprehend by Gated-Attention Reader with More Belief", "labels": [], "entities": []}], "abstractContent": [{"text": "Gated-Attention (GA) Reader has been effective for reading comprehension.", "labels": [], "entities": [{"text": "Gated-Attention (GA) Reader", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5814381003379822}]}, {"text": "GA Reader makes two assumptions: (1) a uni-directional attention that uses an input query to gate token encodings of a document; (2) encoding at the cloze position of an input query is considered for answer prediction.", "labels": [], "entities": [{"text": "GA Reader", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8429327607154846}, {"text": "answer prediction", "start_pos": 200, "end_pos": 217, "type": "TASK", "confidence": 0.9266501367092133}]}, {"text": "In this paper, we propose Collaborative Gating (CG) and Self-Belief Aggregation (SBA) to address the above assumptions respectively.", "labels": [], "entities": []}, {"text": "In CG, we first use an input document to gate token en-codings of an input query so that the influence of irrelevant query tokens maybe reduced.", "labels": [], "entities": []}, {"text": "Then the filtered query is used to gate token encodings of an document in a collab-orative fashion.", "labels": [], "entities": []}, {"text": "In SBA, we conjecture that query tokens other than the cloze token maybe informative for answer prediction.", "labels": [], "entities": [{"text": "SBA", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.900456428527832}, {"text": "answer prediction", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.9043315052986145}]}, {"text": "We apply self-attention to link the cloze token with other tokens in a query so that the importance of query tokens with respect to the cloze position are weighted.", "labels": [], "entities": []}, {"text": "Then their evidences are weighted, propagated and aggregated for better reading comprehension.", "labels": [], "entities": []}, {"text": "Experiments show that our approaches advance the state-of-the-art results in CNN, Daily Mail, and Who Did What public test sets.", "labels": [], "entities": [{"text": "CNN", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.9287149906158447}, {"text": "Daily Mail", "start_pos": 82, "end_pos": 92, "type": "DATASET", "confidence": 0.9143935143947601}, {"text": "Who Did What public test sets", "start_pos": 98, "end_pos": 127, "type": "DATASET", "confidence": 0.7264963587125143}]}], "introductionContent": [{"text": "Recently, machine reading has received a lot of attention in the research community.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8691906034946442}]}, {"text": "Several largescale datasets of cloze-style query-document pairs have been introduced to measure machine reading capability.", "labels": [], "entities": []}, {"text": "Deep leaning has been used for text comprehension with state-of-the-art approaches using attention mechanism.", "labels": [], "entities": [{"text": "text comprehension", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.8483653366565704}]}, {"text": "One simple and effective approach is based on Gated Attention (GA) (.", "labels": [], "entities": [{"text": "Gated Attention (GA)", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7943129777908325}]}, {"text": "Viewing the attention mechanism as word alignment, GA uses document-to-query attention to align each word * indicates equal contribution position of a document with a word token in a query in a \"soft\" manner.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7134757936000824}]}, {"text": "Then the expected encoding of the query, which can be viewed as a masking vector, is computed for each word position of a document.", "labels": [], "entities": []}, {"text": "Through a gating function such as the element-wise product, each dimension of a token encoding in a document is interacted with the query for information filtering.", "labels": [], "entities": [{"text": "information filtering", "start_pos": 142, "end_pos": 163, "type": "TASK", "confidence": 0.7625156342983246}]}, {"text": "Intuitively, each token of a document becomes queryaware.", "labels": [], "entities": []}, {"text": "Through the gating mechanism, only relevant information in the document is kept for further processing.", "labels": [], "entities": []}, {"text": "Moreover, multi-hop reasoning is applied that performs layer-wise information filtering to improve machine reading performance.", "labels": [], "entities": [{"text": "information filtering", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7091279774904251}]}, {"text": "In this paper, we propose Collaborative Gating (CG) that attempts to model bi-directional information filtering between query-document pairs.", "labels": [], "entities": []}, {"text": "We first apply query-to-document attention so that each token encoding of a query becomes document-aware.", "labels": [], "entities": []}, {"text": "Then we use the filtered query and apply usual document-to-query attention to filter the document.", "labels": [], "entities": []}, {"text": "Bi-directional attention mechanisms are performed in a collaborative manner.", "labels": [], "entities": []}, {"text": "Multi-hop reasoning is then applied like in the GA Reader.", "labels": [], "entities": [{"text": "GA Reader", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.9259587526321411}]}, {"text": "Intuitively, bi-directional attention may capture complementary information for better machine comprehension (.", "labels": [], "entities": []}, {"text": "By filtering query-document pairs, we hope that feature representation at the final layer will be more precise for answer prediction.", "labels": [], "entities": [{"text": "answer prediction", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.9082755148410797}]}, {"text": "Our experiments have shown that CG can yield further improvement compared to GA Reader.", "labels": [], "entities": [{"text": "GA Reader", "start_pos": 77, "end_pos": 86, "type": "TASK", "confidence": 0.6951695680618286}]}, {"text": "Another contribution is the introduction of selfattention mechanism in GA Reader.", "labels": [], "entities": [{"text": "GA Reader", "start_pos": 71, "end_pos": 80, "type": "TASK", "confidence": 0.5792372822761536}]}, {"text": "One assumption made by GA Reader is that at the final layer for answer prediction, only the cloze position of a query is considered for computing the evidence scores of entity candidates.", "labels": [], "entities": [{"text": "GA Reader", "start_pos": 23, "end_pos": 32, "type": "DATASET", "confidence": 0.8511819541454315}, {"text": "answer prediction", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.9497657716274261}]}, {"text": "We conjecture that surrounding words in a query maybe related to the cloze position and thus provide addition-al evidence for answer prediction.", "labels": [], "entities": [{"text": "answer prediction", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.9084330499172211}]}, {"text": "Therefore, we employ self-attention to weight each token of the query with respect to the cloze token.", "labels": [], "entities": []}, {"text": "Our proposed Self-Belief Aggregation (SBA) amounts to compute the expected encoding at the cloze position which can be viewed as evidence propagation from other word positions.", "labels": [], "entities": []}, {"text": "Then similarity scores between the expected cloze token and the candidate entities of the document are computed and aggregated at the final layer.", "labels": [], "entities": [{"text": "similarity", "start_pos": 5, "end_pos": 15, "type": "METRIC", "confidence": 0.9673205018043518}]}, {"text": "Our experiments have shown that SBA can improve machine reading performance over GA Reader.", "labels": [], "entities": [{"text": "SBA", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8993929028511047}, {"text": "GA Reader", "start_pos": 81, "end_pos": 90, "type": "TASK", "confidence": 0.5955046862363815}]}, {"text": "This paper is organized as follows: In Section 2, we briefly describe related work.", "labels": [], "entities": []}, {"text": "Section 3 gives our proposed approaches to improve GA Reader.", "labels": [], "entities": [{"text": "GA Reader", "start_pos": 51, "end_pos": 60, "type": "TASK", "confidence": 0.8833579123020172}]}, {"text": "We present experimental results in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we summarize and conclude with future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We provide experimental evaluation on our proposed approaches on public datasets in this section.", "labels": [], "entities": []}, {"text": "News stories from CNN and Daily Mail ( 1 were used to evaluate our approaches.", "labels": [], "entities": []}, {"text": "In particular, a query was generated by replacing an entity in the summary with @place-holder.", "labels": [], "entities": []}, {"text": "Furthermore, entities in the news articles were anonymized to erase the world knowledge and co-occurrence effect for reading comprehension.", "labels": [], "entities": []}, {"text": "Word embeddings of these anonymized entities are thus less informative.", "labels": [], "entities": []}, {"text": "Another dataset was Who Did What 2 (WD-W) (, constructed from the LD-C English Gigaword newswire corpus.", "labels": [], "entities": [{"text": "Who Did What 2 (WD-W)", "start_pos": 20, "end_pos": 41, "type": "DATASET", "confidence": 0.6726345547607967}, {"text": "LD-C English Gigaword newswire corpus", "start_pos": 66, "end_pos": 103, "type": "DATASET", "confidence": 0.8124791383743286}]}, {"text": "Document pairs appeared around the same time period and with shared entities were chosen.", "labels": [], "entities": []}, {"text": "Then, one article was selected as document and another article formed a cloze-style query.", "labels": [], "entities": []}, {"text": "Queries that were answered easily by the baseline were removed to make the task more challenging.", "labels": [], "entities": []}, {"text": "Two versions of the WDW datasets were considered for experiments: a smaller \"strict\" version and a larger but noisy \"relaxed\" version.", "labels": [], "entities": [{"text": "WDW datasets", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.9462834596633911}]}, {"text": "Both shared the same validation and test sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Validation and test accuracies on CNN, Daily Mail and WDW. Results marked with  \u2020 are previously  published results.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.7171966433525085}, {"text": "CNN", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9775903820991516}, {"text": "Daily Mail", "start_pos": 49, "end_pos": 59, "type": "DATASET", "confidence": 0.9566658437252045}, {"text": "WDW", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.6346945762634277}]}, {"text": " Table 2: Performance of Collaborative Gating under  different settings on the CNN corpus.", "labels": [], "entities": [{"text": "CNN corpus", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.9606733024120331}]}]}