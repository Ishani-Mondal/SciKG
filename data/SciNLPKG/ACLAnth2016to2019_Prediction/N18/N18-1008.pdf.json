{"title": [{"text": "Tied Multitask Learning for Neural Speech Translation", "labels": [], "entities": [{"text": "Neural Speech Translation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7298808495203654}]}], "abstractContent": [{"text": "We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions.", "labels": [], "entities": [{"text": "neural translation of speech", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.8369954377412796}]}, {"text": "First, we introduce a model where the second task de-coder receives information from the decoder of the first task, since higher-level intermediate representations should provide useful information.", "labels": [], "entities": []}, {"text": "Second, we apply regularization that encourages transitivity and invertibility.", "labels": [], "entities": []}, {"text": "We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation.", "labels": [], "entities": [{"text": "low-resource speech transcription", "start_pos": 109, "end_pos": 142, "type": "TASK", "confidence": 0.680391271909078}]}, {"text": "It also leads to better performance when using attention information for word discovery over unsegmented input.", "labels": [], "entities": [{"text": "word discovery", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.8009021580219269}]}], "introductionContent": [{"text": "Recent efforts in endangered language documentation focus on collecting spoken language resources, accompanied by spoken translations in a high resource language to make the resource interpretable ().", "labels": [], "entities": [{"text": "endangered language documentation", "start_pos": 18, "end_pos": 51, "type": "TASK", "confidence": 0.691983699798584}]}, {"text": "For example, the BULB project ( ) used the LIGAikuma mobile app ( to collect parallel speech corpora between three Bantu languages and French.", "labels": [], "entities": [{"text": "BULB", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.7327247262001038}]}, {"text": "Since it's common for speakers of endangered languages to speak one or more additional languages, collection of such a resource is a realistic goal.", "labels": [], "entities": []}, {"text": "Speech can be interpreted either by transcription in the original language or translation to another language.", "labels": [], "entities": []}, {"text": "Since the size of the data is extremely small, multitask models that jointly train a model for both tasks can take advantage of both signals.", "labels": [], "entities": []}, {"text": "Our contribution lies in improving the sequence-to-sequence multitask learning paradigm, by drawing on two intuitive notions: that higher-level representations are more useful than lower-level representations, and that translation should be both transitive and invertible.", "labels": [], "entities": []}, {"text": "Higher-level intermediate representations, such as transcriptions, should in principle carry information useful for an end task like speech translation.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.7312702685594559}]}, {"text": "A typical multitask setup ( shares information at the level of encoded frames, but intuitively, a human translating speech must work from a higher level of representation, at least at the level of phonemes if not syntax or semantics.", "labels": [], "entities": []}, {"text": "Thus, we present a novel architecture for tied multitask learning with sequence-to-sequence models, in which the decoder of the second task receives information not only from the encoder, but also from the decoder of the first task.", "labels": [], "entities": []}, {"text": "In addition, transitivity and invertibility are two properties that should hold when mapping between levels of representation or across languages.", "labels": [], "entities": []}, {"text": "We demonstrate how these two notions can be implemented through regularization of the attention matrices, and how they lead to further improved performance.", "labels": [], "entities": []}, {"text": "We evaluate our models in three experiment settings: low-resource speech transcription and translation, word discovery on unsegmented input, and high-resource text translation.", "labels": [], "entities": [{"text": "low-resource speech transcription and translation", "start_pos": 53, "end_pos": 102, "type": "TASK", "confidence": 0.757626748085022}, {"text": "word discovery", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.8192440271377563}, {"text": "text translation", "start_pos": 159, "end_pos": 175, "type": "TASK", "confidence": 0.753574550151825}]}, {"text": "Our highresource experiments are performed on English, French, and German.", "labels": [], "entities": []}, {"text": "Our low-resource speech experiments cover a wider range of linguistic diversity: Spanish-English, Mboshi-French, and AinuEnglish.", "labels": [], "entities": []}, {"text": "In the speech transcription and translation tasks, our proposed model leads to improved performance against all baselines as well as previous multitask architectures.", "labels": [], "entities": [{"text": "speech transcription and translation", "start_pos": 7, "end_pos": 43, "type": "TASK", "confidence": 0.7228992730379105}]}, {"text": "We observe improvements of up to 5% character error rate in the transcription task, and up to 2.8% character-level BLEU in the translation task.", "labels": [], "entities": [{"text": "character error rate", "start_pos": 36, "end_pos": 56, "type": "METRIC", "confidence": 0.7824243307113647}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9680882692337036}, {"text": "translation task", "start_pos": 127, "end_pos": 143, "type": "TASK", "confidence": 0.9179710745811462}]}, {"text": "However, we didn't observe similar improvements in the text translation experiments.", "labels": [], "entities": [{"text": "text translation", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.8133639395236969}]}, {"text": "Finally, on the word discovery task, we improve upon previous work by about 3% F-score on both tokens and types.", "labels": [], "entities": [{"text": "word discovery task", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.9085413416226705}, {"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9993320107460022}]}], "datasetContent": [{"text": "On all experiments, the encoder and the decoder(s) have 2 layers of LSTM units with hidden state size and attention size of 1024, and embedding size of 1024.", "labels": [], "entities": []}, {"text": "For this high resource scenario, we only train fora maximum of 40 epochs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on our speech datasets.", "labels": [], "entities": []}, {"text": " Table 2: The multitask models outperform the baseline single-task model and the pivot approach (auto/text) on all  language pairs tested. The triangle model also outperforms the simple multitask models on both tasks in almost all  cases. The best results for each dataset and task are highlighted.", "labels": [], "entities": []}, {"text": " Table 4: BLEU scores for each model and translation direction s \u2192 t. In the multitask, cascade, and triangle  models, x stands for the third language, other than s and t. In each column, the best results are highlighted. The  non-highlighted results are statistically significantly worse than the single-task baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991651773452759}]}]}