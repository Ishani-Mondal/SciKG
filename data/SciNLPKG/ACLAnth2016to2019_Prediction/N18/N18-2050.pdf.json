{"title": [{"text": "A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling", "labels": [], "entities": [{"text": "Intent Detection", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.6772083640098572}, {"text": "Slot Filling", "start_pos": 75, "end_pos": 87, "type": "TASK", "confidence": 0.9094819128513336}]}], "abstractContent": [{"text": "Intent detection and slot filling are two main tasks for building a spoken language under-standing(SLU) system.", "labels": [], "entities": [{"text": "Intent detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8288996815681458}, {"text": "slot filling", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.8940325677394867}]}, {"text": "Multiple deep learning based models have demonstrated good results on these tasks.", "labels": [], "entities": []}, {"text": "The most effective algorithms are based on the structures of sequence to sequence models (or \"encoder-decoder\" models), and generate the intents and semantic tags either using separate models((Yao et al., 2014; Mesnil et al., 2015; Peng and Yao, 2015; Kurata et al., 2016; Hahn et al., 2011)) or a joint model ((Liu and Lane, 2016a; Hakkani-T\u00fcr et al., 2016; Guo et al., 2014)).", "labels": [], "entities": []}, {"text": "Most of the previous studies, however, either treat the intent detection and slot filling as two separate parallel tasks, or use a sequence to sequence model to generate both semantic tags and intent.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7132564187049866}, {"text": "slot filling", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.765853226184845}]}, {"text": "Most of these approaches use one (joint) NN based model (including encoder-decoder structure) to model two tasks, hence may not fully take advantage of the cross-impact between them.", "labels": [], "entities": []}, {"text": "In this paper, new Bi-model based RNN semantic frame parsing network structures are designed to perform the intent detection and slot filling tasks jointly, by considering their cross-impact to each other using two correlated bidirectional LSTMs (BLSTM).", "labels": [], "entities": [{"text": "RNN semantic frame parsing network", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.7258429288864136}, {"text": "intent detection", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.7052726149559021}, {"text": "slot filling", "start_pos": 129, "end_pos": 141, "type": "TASK", "confidence": 0.715743288397789}]}, {"text": "Our Bi-model structure with a decoder achieves state-of-the-art result on the benchmark ATIS data (Hemphill et al., 1990; Tur et al., 2010), with about 0.5% intent accuracy improvement and 0.9 % slot filling improvement.", "labels": [], "entities": [{"text": "ATIS data", "start_pos": 88, "end_pos": 97, "type": "DATASET", "confidence": 0.8187007009983063}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9141918420791626}]}], "introductionContent": [{"text": "The research on spoken language understanding (SLU) system has progressed extremely fast during the past decades.", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 16, "end_pos": 51, "type": "TASK", "confidence": 0.8205014963944753}]}, {"text": "Two important tasks in an SLU system are intent detection and slot filling.", "labels": [], "entities": [{"text": "SLU", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9559475779533386}, {"text": "intent detection", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.9080548882484436}, {"text": "slot filling", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.9094165563583374}]}, {"text": "These two tasks are normally considered as parallel tasks but may have cross-impact on each other.", "labels": [], "entities": []}, {"text": "The intent detection is treated as an utterance classification problem, which can be modeled using conventional classifiers including regression, support vector machines (SVMs) or even deep neural networks).", "labels": [], "entities": [{"text": "intent detection", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7695169150829315}, {"text": "utterance classification", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8229674100875854}]}, {"text": "The slot filling task can be formulated as a sequence labeling problem, and the most popular approaches with good performances are using conditional random fields (CRFs) and recurrent neural networks (RNN) as recent works (.", "labels": [], "entities": [{"text": "slot filling task", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9307294686635336}, {"text": "sequence labeling", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.6871896982192993}]}, {"text": "Some works also suggested using one joint RNN model for generating results of the two tasks together, by taking advantage of the sequence to sequence) (or encoderdecoder) model, which also gives decent results as in literature(.", "labels": [], "entities": []}, {"text": "In this paper, Bi-model based RNN structures are proposed to take the cross-impact between two tasks into account, hence can further improve the performance of modeling an SLU system.", "labels": [], "entities": []}, {"text": "These models can generate the intent and semantic tags concurrently for each utterance.", "labels": [], "entities": []}, {"text": "In our Bi-model structures, two task-networks are built for the purpose of intent detection and slot filling.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7406605035066605}, {"text": "slot filling", "start_pos": 96, "end_pos": 108, "type": "TASK", "confidence": 0.7760014533996582}]}, {"text": "Each task-network includes one BLSTM with or without a LSTM decoder (Hochreiter and Schmidhuber, 1997; Graves and).", "labels": [], "entities": []}, {"text": "The paper is organized as following: In section 2, a brief overview of existing deep learning approaches for intent detection and slot fillings are given.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 109, "end_pos": 125, "type": "TASK", "confidence": 0.8693865239620209}, {"text": "slot fillings", "start_pos": 130, "end_pos": 143, "type": "TASK", "confidence": 0.8623448312282562}]}, {"text": "The new proposed Bi-model based RNN approach will be illustrated in detail in section 3.", "labels": [], "entities": []}, {"text": "In section 4, two experiments on different datasets will be given.", "labels": [], "entities": []}, {"text": "One is performed on the ATIS benchmark dataset, in order to demonstrate a state-of-the-art result for both semantic parsing tasks.", "labels": [], "entities": [{"text": "ATIS benchmark dataset", "start_pos": 24, "end_pos": 46, "type": "DATASET", "confidence": 0.9642809430758158}, {"text": "semantic parsing", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.7359558939933777}]}, {"text": "The other experiment is tested on our internal multi-domain dataset by comparing our new algorithm with the current best performed RNN based joint model in literature for intent detection and slot filling.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 171, "end_pos": 187, "type": "TASK", "confidence": 0.8253616094589233}, {"text": "slot filling", "start_pos": 192, "end_pos": 204, "type": "TASK", "confidence": 0.885610818862915}]}], "datasetContent": [{"text": "In this section, our new proposed Bi-model structures are trained and tested on two datasets, one is the public ATIS dataset ( containing audio recordings of flight reservations, and the other is our self-collected datset in three different domains: Food, Home and Movie.", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.7534516155719757}]}, {"text": "The ATIS dataset used in this paper follows the same format as in ().", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9383890330791473}]}, {"text": "The training set contains 4978 utterance and the test set contains 893 utterance, with a total of 18 intent classes and 127 slot labels.", "labels": [], "entities": []}, {"text": "The number of data for our self-collected dataset will be given in the corresponding experiment sections with a more detailed explanation.", "labels": [], "entities": []}, {"text": "The performance is evaluated based on the classification accuracy for intent detection task and F1-score for slot filling task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.8493109941482544}, {"text": "intent detection task", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.8392486373583475}, {"text": "F1-score", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9992762207984924}, {"text": "slot filling task", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.8578411142031351}]}, {"text": "Our first experiment is conducted on the ATIS benchmark dataset, and compared with the current existing approaches, by evaluating their intent detection accuracy and slot filling F1 scores.", "labels": [], "entities": [{"text": "ATIS benchmark dataset", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.962248682975769}, {"text": "intent detection", "start_pos": 136, "end_pos": 152, "type": "TASK", "confidence": 0.6861520856618881}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.7248485088348389}, {"text": "F1 scores", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9199162423610687}]}, {"text": "A  detailed comparison is given in.", "labels": [], "entities": []}, {"text": "Some of the models are designed for single slot filling task, hence only F1 scores are given.", "labels": [], "entities": [{"text": "slot filling task", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.8349588712056478}, {"text": "F1 scores", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9822080433368683}]}, {"text": "It can be observed that the new proposed Bi-model structures outperform the current state-of-the-art results on both intent detection and slot filling tasks, and the Bi-model with a decoder also outperform that without a decoder on our ATIS dataset.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.7160799503326416}, {"text": "slot filling tasks", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7904408971468607}, {"text": "ATIS dataset", "start_pos": 236, "end_pos": 248, "type": "DATASET", "confidence": 0.9747328162193298}]}, {"text": "The current Bi-model with a decoder shows the state-of-the-art performance on ATIS benchmark dataset with 0.9% improvement on F1 score and 0.5% improvement on intent accuracy.", "labels": [], "entities": [{"text": "Bi-model", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9012555480003357}, {"text": "ATIS benchmark dataset", "start_pos": 78, "end_pos": 100, "type": "DATASET", "confidence": 0.934512992699941}, {"text": "F1 score", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9861098527908325}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.7728034257888794}]}, {"text": "It is worth noticing that the complexities of encoder-decoder based models are normally higher than the models without using encoderdecoder structures, since two networks are used and more parameters need to be updated.", "labels": [], "entities": []}, {"text": "This is another reason why we use two models with/without using encoder-decoder structures to demonstrate the new bi-model structure design.", "labels": [], "entities": []}, {"text": "It can also be observed that the model with a decoder gives a better result due to its higher complexity.", "labels": [], "entities": []}, {"text": "2. It is also shown in the table that the joint model in () achieves better performance on intent detection task with slight degradation on slot filling, so a joint model is not necessary always better for both tasks.", "labels": [], "entities": [{"text": "intent detection task", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.7916882733503977}, {"text": "slot filling", "start_pos": 140, "end_pos": 152, "type": "TASK", "confidence": 0.662361353635788}]}, {"text": "The bi-model approach overcomes this issue by generating two tasks' results separately.", "labels": [], "entities": []}, {"text": "3. Despite the absolute improvement of intent accuracy and F1 scores are only 0.5% and 0.9% on ATIS dataset, the relative improvement is not small.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8928660750389099}, {"text": "F1 scores", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9861119985580444}, {"text": "ATIS dataset", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.9733427166938782}]}, {"text": "For intent accuracy, the number of wrongly classified utterances in test dataset reduced from 14 to 9, which gives us the 35.7% relative improvement on intent accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.8157870769500732}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.5645161271095276}]}, {"text": "Similarly, the relative improvement on F1 score is 22.63%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9456690549850464}]}], "tableCaptions": [{"text": " Table 1: Performance of Different Models on ATIS  Dataset", "labels": [], "entities": [{"text": "ATIS  Dataset", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9436988532543182}]}, {"text": " Table 1. Some  of the models are designed for single slot filling  task, hence only F1 scores are given. It can  be observed that the new proposed Bi-model  structures outperform the current state-of-the-art  results on both intent detection and slot filling  tasks, and the Bi-model with a decoder also  outperform that without a decoder on our ATIS  dataset. The current Bi-model with a decoder  shows the state-of-the-art performance on ATIS  benchmark dataset with 0.9% improvement on F1  score and 0.5% improvement on intent accuracy.  Remarks:", "labels": [], "entities": [{"text": "slot filling  task", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7944715321063995}, {"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9988466501235962}, {"text": "intent detection and slot filling  tasks", "start_pos": 226, "end_pos": 266, "type": "TASK", "confidence": 0.7373463114102682}, {"text": "ATIS  dataset", "start_pos": 347, "end_pos": 360, "type": "DATASET", "confidence": 0.9784955978393555}, {"text": "ATIS  benchmark dataset", "start_pos": 441, "end_pos": 464, "type": "DATASET", "confidence": 0.9745773474375407}, {"text": "F1  score", "start_pos": 490, "end_pos": 499, "type": "METRIC", "confidence": 0.9865619838237762}, {"text": "Remarks", "start_pos": 542, "end_pos": 549, "type": "METRIC", "confidence": 0.9630486965179443}]}, {"text": " Table 2: Performance Comparison between Bi-model  Structures and Attention BiRNN", "labels": [], "entities": []}]}