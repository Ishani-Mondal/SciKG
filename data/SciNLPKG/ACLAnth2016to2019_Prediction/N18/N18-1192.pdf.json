{"title": [], "abstractContent": [{"text": "The long short-term memory (LSTM) language model (LM) has been widely investigated for automatic speech recognition (ASR) and natural language processing (NLP).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 87, "end_pos": 121, "type": "TASK", "confidence": 0.8145247598489126}, {"text": "natural language processing (NLP)", "start_pos": 126, "end_pos": 159, "type": "TASK", "confidence": 0.7698002358277639}]}, {"text": "Although excellent performance is obtained for large vocabulary tasks, tremendous memory consumption prohibits the use of LSTM LMs in low-resource devices.", "labels": [], "entities": []}, {"text": "The memory consumption mainly comes from the word embedding layer.", "labels": [], "entities": []}, {"text": "In this paper, a novel binarized LSTM LM is proposed to address the problem.", "labels": [], "entities": []}, {"text": "Words are encoded into binary vectors and other LSTM parameters are further binarized to achieve high memory compression.", "labels": [], "entities": []}, {"text": "This is the first effort to investigate binary LSTMs for large vocabulary language modeling.", "labels": [], "entities": [{"text": "large vocabulary language modeling", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.6670499220490456}]}, {"text": "Experiments on both English and Chinese LM and ASR tasks showed that binarization can achieve a compression ratio of 11.3 without any loss of LM and ASR performance and a compression ratio of 31.6 with acceptable minor performance degradation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models (LMs) play an important role in natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.7777985845293317}]}, {"text": "Ngram language models used to be the most popular language models.", "labels": [], "entities": []}, {"text": "Considering the previous N-1 words, N-gram language models predict the next word.", "labels": [], "entities": []}, {"text": "However, this leads to the loss of longterm dependencies.", "labels": [], "entities": []}, {"text": "The sample space size increases exponentially as N grows, which induces data sparseness.", "labels": [], "entities": []}, {"text": "Neural network (NN) based models were first introduced into language modeling in 2003.", "labels": [], "entities": [{"text": "language modeling in 2003", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.8074921667575836}]}, {"text": "Given contexts with a fixed size, the model can calculate the probability distribution of the next word.", "labels": [], "entities": []}, {"text": "However, the problem of long-term dependencies still remained, be-cause the context window is fixed.", "labels": [], "entities": []}, {"text": "Currently, recurrent neural network (RNN) based models are widely used on natural language processing (NLP) tasks for excellent performance.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 74, "end_pos": 113, "type": "TASK", "confidence": 0.7534388474055699}]}, {"text": "Recurrent structures in neural networks can solve the problem of long-term dependencies to a great extent.", "labels": [], "entities": []}, {"text": "Some gate based structures, such as long short-term memory (LSTM)) and gated recurrent unit (GRU) () improve the recurrent structures and achieve state-of-the-art performance on most NLP tasks.", "labels": [], "entities": []}, {"text": "However, neural network models occupy tremendous memory space so that it is almost impossible to put the models into low-resource devices.", "labels": [], "entities": []}, {"text": "In practice, the vocabulary is usually very large.", "labels": [], "entities": []}, {"text": "So the memory consumption mainly comes from the embedding layers.", "labels": [], "entities": []}, {"text": "And, the word embedding parameters are floating point values, which adds to the memory consumption.", "labels": [], "entities": []}, {"text": "The first contribution in this paper is that a novel language model, the binarized embedding language model (BELM) is proposed to reduce the memory consumption.", "labels": [], "entities": [{"text": "BELM", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.8947613835334778}]}, {"text": "Words are represented in the form of binarized vectors.", "labels": [], "entities": []}, {"text": "Thus, the consumption of memory space is significantly reduced.", "labels": [], "entities": []}, {"text": "Another contribution in the paper is that we binarize the LSTM language model combined with the binarized embeddings to further compress the parameter space.", "labels": [], "entities": []}, {"text": "All the parameters in the LSTM language model are binarized.", "labels": [], "entities": []}, {"text": "Experiments are conducted in language modeling and automatic speech recognition (ASR) rescoring tasks.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7712720930576324}, {"text": "automatic speech recognition (ASR) rescoring tasks", "start_pos": 51, "end_pos": 101, "type": "TASK", "confidence": 0.8232703506946564}]}, {"text": "Our model performs well without any loss of performance at a compression ratio of 11.3 and still has acceptable results with only a minor loss of performance even at a compression ratio of 31.6.", "labels": [], "entities": [{"text": "compression", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9608520269393921}]}, {"text": "Investigations are also made to evaluate whether the binarized embeddings lose information.", "labels": [], "entities": []}, {"text": "Experiments are conducted on word similarity tasks.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.8334805965423584}]}, {"text": "The results show the binarized embeddings generated by our models still perform well on the two datasets.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows, section 2 is the related work.", "labels": [], "entities": []}, {"text": "Section 3 explains the proposed language model and section 4 shows the experimental setup and results.", "labels": [], "entities": []}, {"text": "Finally, conclusions will be given in section 5 and we describe future work in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model is evaluated on the English Penn TreeBank (PTB) (, Chinese short message (SMS) and SWB-Fisher (SWB).", "labels": [], "entities": [{"text": "English Penn TreeBank (PTB)", "start_pos": 30, "end_pos": 57, "type": "DATASET", "confidence": 0.8822780152161916}]}, {"text": "The Penn TreeBank corpus is a famous English dataset, with a vocabulary size of 10K and 4.8% words out of vocabulary (OOV), which is widely used to evaluate the performance of a language model.", "labels": [], "entities": [{"text": "Penn TreeBank corpus", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9914954503377279}, {"text": "words out of vocabulary (OOV)", "start_pos": 93, "end_pos": 122, "type": "METRIC", "confidence": 0.6256686406476157}]}, {"text": "The training set contains approximately 42K sentences with 887K words.", "labels": [], "entities": []}, {"text": "The Chinese SMS corpus is collected from short messages.", "labels": [], "entities": [{"text": "Chinese SMS corpus", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9072806040445963}]}, {"text": "The corpus has a vocabulary size of about 40K.", "labels": [], "entities": []}, {"text": "The training set contains 380K sentences with 1931K words.", "labels": [], "entities": []}, {"text": "The SWB-Fisher corpus is an English corpus containing approximately 2.5M sentences with 24.9M words.", "labels": [], "entities": [{"text": "SWB-Fisher corpus", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8328746259212494}]}, {"text": "The corpus has a vocabulary size of about 30K.", "labels": [], "entities": []}, {"text": "hub5e is the dataset for the SWB ASR task.", "labels": [], "entities": [{"text": "SWB ASR task", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.809749166170756}]}, {"text": "We also evaluate the word embeddings produced by our models on two word similarity datasets.", "labels": [], "entities": []}, {"text": "The models are trained on the Text8 corpus to extract the word embeddings.", "labels": [], "entities": [{"text": "Text8 corpus", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9630184471607208}]}, {"text": "The Text8 corpus is published by Google and collected from Wikipedia.", "labels": [], "entities": [{"text": "Text8 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9289355576038361}]}, {"text": "Text8 contains about 17M words with a vocabulary size of about 47k.", "labels": [], "entities": []}, {"text": "The WordSimilarity-353(WS-353) Test Collection contains two sets of English word pairs along with human-assigned similarity judgments.", "labels": [], "entities": [{"text": "WordSimilarity-353(WS-353) Test Collection", "start_pos": 4, "end_pos": 46, "type": "DATASET", "confidence": 0.8975673317909241}]}, {"text": "The collection can be used to train and test computer algorithms implementing semantic similarity measures.", "labels": [], "entities": []}, {"text": "A combined set (combined) is provided that contains a list of all 353 words, along with their mean similarity scores.", "labels": [], "entities": [{"text": "mean similarity scores", "start_pos": 94, "end_pos": 116, "type": "METRIC", "confidence": 0.7532333930333456}]}, {"text": "() The MEN dataset consists of 3,000 word pairs, randomly selected from words that occur at least 700 times in the freely available ukWaC and Wackypedia corpora combined (size: 1.9B and 820M tokens, respectively) and at least 50 times (as tags) in the open-sourced subset of the ESP game dataset.", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.8382622301578522}, {"text": "ukWaC", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.9888831973075867}, {"text": "ESP game dataset", "start_pos": 279, "end_pos": 295, "type": "DATASET", "confidence": 0.8830332358678182}]}, {"text": "In order to avoid picking unrelated pairs only, the pairs are sampled so that they represent a balanced range of relatedness levels according to a text-based semantic score ().", "labels": [], "entities": []}, {"text": "First, we conduct experiments on the PTB, SWB and Text8 corpora respectively to evaluate language modeling performance.", "labels": [], "entities": [{"text": "PTB", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.8603621125221252}, {"text": "Text8 corpora", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.8806261122226715}]}, {"text": "We use perplexity (PPL) as the metric to evaluate models of different sizes.", "labels": [], "entities": []}, {"text": "Then, the models are evaluated on ASR rescoring tasks.", "labels": [], "entities": [{"text": "ASR rescoring tasks", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.9258615175882975}]}, {"text": "Rescoring the 100-best sentences generated by the weighted finite state transducer (WFST), the model is evaluated byword error rate (WER).", "labels": [], "entities": [{"text": "byword error rate (WER)", "start_pos": 114, "end_pos": 137, "type": "METRIC", "confidence": 0.9338042040665945}]}, {"text": "Finally, we conduct experiments on word similarity tasks to evaluate whether the word embeddings produced by our models lose any information.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.761365125576655}]}, {"text": "For traditional RNN based language models, the memory consumption mainly comes from the embedding layers (both input and output layers).", "labels": [], "entities": []}, {"text": "However, when the hidden layer size grows, the memory consumption of the RNN module also becomes larger.", "labels": [], "entities": []}, {"text": "So the total memory usage relates to both the vocabulary size and hidden layer size, as mentioned in section 3.4.", "labels": [], "entities": []}, {"text": "Experiments are conducted in language modeling to evaluate the model on the PTB, SWB, and SMS corpora respectively.", "labels": [], "entities": [{"text": "PTB", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.9312185049057007}]}, {"text": "In language modeling tasks, we regularize the networks using dropout().", "labels": [], "entities": [{"text": "language modeling tasks", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7866437236467997}]}, {"text": "We use stochastic gradient descent (SGD) for optimization.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD", "start_pos": 7, "end_pos": 39, "type": "TASK", "confidence": 0.7200002670288086}]}, {"text": "The batch size is set to 64.", "labels": [], "entities": []}, {"text": "For the PTB corpus, the dropout rate is tuned for different training settings.", "labels": [], "entities": [{"text": "PTB corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.9387049078941345}]}, {"text": "For the SWB corpus, we do not use dropout technique.", "labels": [], "entities": [{"text": "SWB corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.8688968122005463}]}, {"text": "For the SMS corpus, the dropout rate is set to 0.25.", "labels": [], "entities": [{"text": "SMS corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.8319979608058929}]}, {"text": "We train models of different sizes on the three corpora and record the memory usage of the trained models.", "labels": [], "entities": []}, {"text": "The initial learning rate is set to 1.0 for all settings.", "labels": [], "entities": []}, {"text": "Since PTB is a relatively small dataset and the convergence rates of the BELM and the BLLM are slower than LSTM language model, we reduce the learning rate by half every three epochs if the perplexity on the validation set is not reduced.", "labels": [], "entities": [{"text": "PTB", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.8186817169189453}, {"text": "BELM", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.8756696581840515}, {"text": "BLLM", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.8056355714797974}]}, {"text": "For the other experiments, the learning rate is always reduced by half every epoch if the perplexity on the validation set is not reduced.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9557014107704163}]}, {"text": "As introduced in section 3, the bias of the output embedding layer is omitted.", "labels": [], "entities": []}, {"text": "Adding bias term in the output embedding layer leads to small performance degradation in the BELM and the BLLM model, although it leads to a small improvement in the LSTM model.", "labels": [], "entities": [{"text": "BELM", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9633306264877319}, {"text": "BLLM", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.7950439453125}]}, {"text": "This phenomenon maybe related to optimization problems.", "labels": [], "entities": []}, {"text": "Because the total memory usage relates to both the vocabulary size and hidden layer size, the memory reduction on various corpora is quite different.", "labels": [], "entities": []}, {"text": "For our BELM model, the floating point embedding parameters are replaced by single bits, which could significantly reduce the memory usage.", "labels": [], "entities": []}, {"text": "On the PTB corpus, the BELM models even outperform the baseline LSTM LM.", "labels": [], "entities": [{"text": "PTB corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9871964454650879}, {"text": "BELM", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9766787886619568}]}, {"text": "The small model (500 LSTM units) has a relative PPL improvement of 4.1% and achieves a compression ratio of 4.3 and the large model (1000 LSTM units) also has a relative PPL improvement of 4.1% and achieves a compression ratio of 2.6.", "labels": [], "entities": [{"text": "PPL", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9730916023254395}]}, {"text": "On the SWB corpus, the BELM models still perform well compared with the baseline model and achieve compression ratios of 9.4 and 5.8 respectively for the small and large models.", "labels": [], "entities": [{"text": "SWB corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9170089960098267}, {"text": "BELM", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9768096804618835}, {"text": "compression", "start_pos": 99, "end_pos": 110, "type": "METRIC", "confidence": 0.9833531379699707}]}, {"text": "On the SMS corpus, the BELMs model also gains relative PPL improvements of 0.2% and 1.9%, and achieves compression ratios of 11.3 and 7.1 respectively.", "labels": [], "entities": [{"text": "SMS corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.8426439762115479}, {"text": "BELMs", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9741628766059875}, {"text": "compression", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.9608682990074158}]}, {"text": "In summary, the BELM model performs as well as the baseline model both on English and Chinese corpora, and reduces the memory consumption to a large extent.", "labels": [], "entities": [{"text": "BELM", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.982532262802124}]}, {"text": "The BLLM model, however, does not outperform the baseline model, but still has acceptable results with a minor loss of performance.", "labels": [], "entities": [{"text": "BLLM", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9592757225036621}]}, {"text": "Since both the LSTM model and the embeddings are binarized, the total compression ratio is quite significant.", "labels": [], "entities": []}, {"text": "The average compression ratio is about 32.0, so the memory consumption of the language model is significantly reduced.", "labels": [], "entities": []}, {"text": "We also study the performance of pruning the LSTM language model.", "labels": [], "entities": []}, {"text": "We prune each parameter matrix and the embedding layers with various pruning rates respectively, and fine-tune the model with various dropout rates.", "labels": [], "entities": []}, {"text": "In our experiments, pruning 75% parameter nodes hardly affects the performance.", "labels": [], "entities": []}, {"text": "However, if we try pruning more parameter nodes, the perplexity increases rapidly.", "labels": [], "entities": []}, {"text": "For example, for the English PTB dataset, when we prune 95% parameter nodes of the embedding layers of an LSTM language model (500 LSTM units), the perpexity will increase from 91.8 to 112.3.", "labels": [], "entities": [{"text": "English PTB dataset", "start_pos": 21, "end_pos": 40, "type": "DATASET", "confidence": 0.8504652182261149}]}, {"text": "When we prune 95% parameter nodes of an LSTM language model (500 LSTM units), the perplexity will increase from 91.8 to 132.3.", "labels": [], "entities": []}, {"text": "Therefore, the effect of pruning is not as good as binarization for the language modeling task.", "labels": [], "entities": [{"text": "language modeling task", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.811038076877594}]}, {"text": "Binarization can be considered as a special case of quantization, which quantizes the parameters to pairs of opposite numbers.", "labels": [], "entities": []}, {"text": "So, compared to normal quantization, binarization can achieve a better compression ratio.", "labels": [], "entities": []}, {"text": "In addition, for binarization, we do not need to determine the position of each unique values in advance.", "labels": [], "entities": []}, {"text": "Therefore, binarization is more flexible than quantization.", "labels": [], "entities": []}, {"text": "We then study the effect of extra binary linear layers in the BLLM.", "labels": [], "entities": [{"text": "BLLM", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.7233819961547852}]}, {"text": "The additional binary linear layer after the input embedding layer and the additional binary linear layer in front of the output embedding layer are removed respectively in this experiment.", "labels": [], "entities": []}, {"text": "We use well-trained embeddings to initialize the corresponding embedding layers and do the binarization using the method proposed in () when the additional binary linear layer is removed.", "labels": [], "entities": []}, {"text": "The perplexities are listed in.", "labels": [], "entities": []}, {"text": "No-i means no additional binary linear layer after the input embedding layer.", "labels": [], "entities": []}, {"text": "No-o means no additional binary linear layer in front of the output embedding layer.", "labels": [], "entities": []}, {"text": "No-io means no additional binary linear layers.", "labels": [], "entities": []}, {"text": "The experiment is conducted on the PTB corpus.", "labels": [], "entities": [{"text": "PTB corpus", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.9841155707836151}]}, {"text": "If the additional binary linear layer after the input embedding layer is removed, the performance does not drop, and even becomes better when the hidden layer size is 1000.", "labels": [], "entities": []}, {"text": "Although the additional binary layer after the input embedding layer is removed, the float version of the input embeddings of BLLM no-i is initialized with well-trained embeddings, while the BLLM is not initialized with the well-trained embeddings.", "labels": [], "entities": [{"text": "BLLM", "start_pos": 191, "end_pos": 195, "type": "DATASET", "confidence": 0.731194257736206}]}, {"text": "We think initialization is the reason why the BLLM no-i performs comparatively to the BLLM.", "labels": [], "entities": [{"text": "BLLM", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.6623111367225647}]}, {"text": "We also observe a PPL increase of 1-2 points for BLLM no-i if the input embeddings are not pre-trained (not listed in the table).", "labels": [], "entities": []}, {"text": "This phenomenon prompts us to pretrain embeddings, which we leave to future work.", "labels": [], "entities": []}, {"text": "Once the additional binary linear layer in front of the output embedding layer is removed, the performance degradation is serious.", "labels": [], "entities": []}, {"text": "This shows that the output embeddings of the language model should not be directly binarized; the additional binary linear layer should be inserted to enhance the model's capacity, especially for low dimensional models.", "labels": [], "entities": []}, {"text": "Experiments are conducted on the ASR rescoring task to evaluate the model on the hub5e and SMS corpora.", "labels": [], "entities": [{"text": "ASR rescoring task", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.8672999739646912}]}, {"text": "Hub5e is a test dataset of the SWB corpus which we use for ASR rescoring tasks.", "labels": [], "entities": [{"text": "Hub5e", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9596506953239441}, {"text": "SWB corpus", "start_pos": 31, "end_pos": 41, "type": "DATASET", "confidence": 0.8669195473194122}, {"text": "ASR rescoring tasks", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.9550173083941141}]}, {"text": "For the hub5e dateset, A VDCNN ( (very deep CNN) model on the 300-hour task is applied as the acoustic model.", "labels": [], "entities": [{"text": "hub5e dateset", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.8238726556301117}, {"text": "VDCNN", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.8344807028770447}]}, {"text": "For the Chinese SMS dataset, the acoustic model is a CD-DNN-HMM model.", "labels": [], "entities": [{"text": "Chinese SMS dataset", "start_pos": 8, "end_pos": 27, "type": "DATASET", "confidence": 0.8203370173772176}]}, {"text": "The weighted finite state transducer (WFST) is produced with a 4-gram language model.", "labels": [], "entities": []}, {"text": "Then our language models are utilized to rescore the 100-best candidates.", "labels": [], "entities": []}, {"text": "The models are evaluated by the metric of word error rate (WER).", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 42, "end_pos": 63, "type": "METRIC", "confidence": 0.8864507973194122}]}], "tableCaptions": [{"text": " Table 2: Performances on the English PTB corpus", "labels": [], "entities": [{"text": "English PTB corpus", "start_pos": 30, "end_pos": 48, "type": "DATASET", "confidence": 0.9483567873636881}]}, {"text": " Table 3: Performance on the English SWB corpus", "labels": [], "entities": [{"text": "English SWB corpus", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.9540021220842997}]}, {"text": " Table 4: Performance on the Chinese SMS corpus", "labels": [], "entities": [{"text": "Chinese SMS corpus", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.9771543939908346}]}, {"text": " Table 5. No-i means no additional binary linear  layer after the input embedding layer. No-o means  no additional binary linear layer in front of the out- put embedding layer. No-io means no additional  binary linear layers. The experiment is conducted  on the PTB corpus.", "labels": [], "entities": [{"text": "PTB corpus", "start_pos": 262, "end_pos": 272, "type": "DATASET", "confidence": 0.991410106420517}]}, {"text": " Table 5: Performances on the English PTB corpus", "labels": [], "entities": [{"text": "English PTB corpus", "start_pos": 30, "end_pos": 48, "type": "DATASET", "confidence": 0.9493250449498495}]}, {"text": " Table 6: Performances on ASR rescoring tasks", "labels": [], "entities": [{"text": "ASR rescoring", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.9803300201892853}]}, {"text": " Table 7:  Language modeling performance on the  Text8 corpus", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7647784948348999}, {"text": "Text8 corpus", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.9624641835689545}]}, {"text": " Table 8: Performances on the word similarity tasks", "labels": [], "entities": [{"text": "word similarity", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7435809969902039}]}]}