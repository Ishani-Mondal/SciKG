{"title": [{"text": "Automated Essay Scoring in the Presence of Biased Ratings", "labels": [], "entities": [{"text": "Automated Essay Scoring", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6557579934597015}]}], "abstractContent": [{"text": "Studies in Social Sciences have revealed that when people evaluate someone else, their evaluations often reflect their biases.", "labels": [], "entities": []}, {"text": "As a result , rater bias may introduce highly subjective factors that make their evaluations inaccurate.", "labels": [], "entities": []}, {"text": "This may affect automated essay scoring models in many ways, as these models are typically designed to model (potentially biased) essay raters.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.7516131401062012}]}, {"text": "While there is sizeable literature on rater effects in general settings, it remains unknown how rater bias affects automated essay scoring.", "labels": [], "entities": [{"text": "automated essay scoring", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.622072676817576}]}, {"text": "To this end, we present anew annotated corpus containing essays and their respective scores.", "labels": [], "entities": []}, {"text": "Different from existing corpora , our corpus also contains comments provided by the raters in order to ground their scores.", "labels": [], "entities": []}, {"text": "We present features to quantify rater bias based on their comments, and we found that rater bias plays an important role in automated essay scoring.", "labels": [], "entities": [{"text": "automated essay scoring", "start_pos": 124, "end_pos": 147, "type": "TASK", "confidence": 0.6078886886437734}]}, {"text": "We investigated the extent to which rater bias affects models based on hand-crafted features.", "labels": [], "entities": []}, {"text": "Finally, we propose to rectify the training set by removing essays associated with potentially biased scores while learning the scoring model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated Essay Scoring (AES) aims at developing models that can grade essays automatically or with reduced involvement of human raters.", "labels": [], "entities": [{"text": "Automated Essay Scoring (AES)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7564902553955714}]}, {"text": "AES systems may rely not only on grammars, but also on more complex features such as semantics, discourse and pragmatics).", "labels": [], "entities": []}, {"text": "Thus, a prominent approach to AES is to learn scoring models from previously graded samples, by modeling the scoring process of human raters.", "labels": [], "entities": [{"text": "AES", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9768171906471252}]}, {"text": "When given the same set of essays to evaluate and enough graded samples, AES systems tend to achieve high agreement levels with trained human raters.", "labels": [], "entities": []}, {"text": "While research in AES has focused on designing scoring models that maximize the agreement with human raters, there is alack of discussion on how biased are human ratings.", "labels": [], "entities": []}, {"text": "Despite making judgments on a common dimension, raters maybe influenced by their attitudes, their cultural background, and their political and economic views.", "labels": [], "entities": []}, {"text": "Since AES models are designed to learn by analyzing human-graded essays, AES models could inherit rating biases present in the scores from human raters, and this may result in systematic errors.", "labels": [], "entities": []}, {"text": "Thus, our objective in this paper is to examine the extent to which rater bias affects the effectiveness of stateof-the-art AES models.", "labels": [], "entities": []}, {"text": "A deeper understanding of such factors may help mitigating the effects of rater bias, enabling AES models to achieve greater objectivity.", "labels": [], "entities": []}, {"text": "In order to study the effects of rater bias in essay scoring, we created an annotated corpus containing essays written by high school students as part of a standardized Brazilian national exam.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.715073823928833}]}, {"text": "Our corpus contains a number of essays, written in Portuguese, along with their respective scores.", "labels": [], "entities": []}, {"text": "Further, raters must also provide a comment for each essay in order to ground their scores.", "labels": [], "entities": []}, {"text": "As in we built subjectivity and sentiment lexicons that serve as features to represent the comments, that is, rater comments are represented according to the subjectivity distribution as given by specific subjectivity cues in our lexicons.", "labels": [], "entities": []}, {"text": "We present empirical evidence suggesting that the subjectivity distribution within rater comment is a proxy for the score that is given to the essay.", "labels": [], "entities": []}, {"text": "More specifically, very low (or very high) scores are associated with essays for which rater comments showed a very particular subjectivity distribution.", "labels": [], "entities": []}, {"text": "We also investigated the relationship be-tween subjectivity distribution and the misalignment between human raters and AES models.", "labels": [], "entities": []}, {"text": "Interestingly, the subjectivity distribution becomes very characteristic as the misalignment increases.", "labels": [], "entities": []}, {"text": "Our main contributions are three-fold: \u2022 We built subjectivity lexicons for the Portuguese language.", "labels": [], "entities": []}, {"text": "These lexicons include words and phrases associated with different subjectivity dimensions \u2212 sentiments, factive verbs, entailments, intensifiers and hedges.", "labels": [], "entities": []}, {"text": "We identify biased language within rater comments by calculating the word mover's distance () between comments and the lexicons.", "labels": [], "entities": []}, {"text": "This approach benefits from large unsupervised corpora, that can be used to learn effective word embeddings (.", "labels": [], "entities": []}, {"text": "By identifying biased language, we observed that biases can work to inflate essay scores or to deflate them.", "labels": [], "entities": []}, {"text": "\u2022 We employ a set of linguistic features in order to learn different AES models, and we evaluate the effects of biased ratings in the efficacy of these models.", "labels": [], "entities": []}, {"text": "In summary, biased ratings affect AES models in different ways, but in general the misalignment between human rater and the AES model is more acute when the rater shows biased language in their comments.", "labels": [], "entities": []}, {"text": "\u2022 We propose simple ways of preventing and reducing the negative effects of biased ratings while learning AES models.", "labels": [], "entities": []}, {"text": "Results in a controlled experimental setting revealed that detecting and removing biased ratings from the training set lead to significant improvements in automated essay scoring.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 165, "end_pos": 178, "type": "TASK", "confidence": 0.6909203380346298}]}, {"text": "In the remainder of this paper, Section 2 discusses related work on automated essay scoring.", "labels": [], "entities": [{"text": "automated essay scoring", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.6298525532086691}]}, {"text": "Section 3 describes the features used for learning AES models, as well as the features used for identifying biased language in rater comments.", "labels": [], "entities": []}, {"text": "Further, our debiasing approach is also discussed in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 describes the data, the setup and the results of our empirical evaluation.", "labels": [], "entities": []}, {"text": "Finally, Section 5 provides our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the data we used to learn and evaluate different AES models.", "labels": [], "entities": []}, {"text": "Then, we discuss our evaluation procedure and report the results obtained with our debiasing approach.", "labels": [], "entities": []}, {"text": "In particular, our experiments aim to answer the following research questions: RQ1: How scores are distributed across the essays?", "labels": [], "entities": [{"text": "RQ1", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.7893157005310059}]}, {"text": "How aligned with human raters are different AES models?", "labels": [], "entities": [{"text": "AES", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.6682499051094055}]}, {"text": "RQ2: Does subjectivity in rater comments vary depending on the given score?", "labels": [], "entities": [{"text": "RQ2", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6248807311058044}]}, {"text": "RQ3: Does subjectivity in rater comments vary depending on the misalignment between the AES model and the human rater?", "labels": [], "entities": [{"text": "RQ3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7634580731391907}]}, {"text": "RQ4: Can we mitigate the effects of biased ratings?", "labels": [], "entities": [{"text": "RQ4", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9702774286270142}]}], "tableCaptions": [{"text": " Table 1: \u03ba numbers for different models with varying \u03b1  values. There are potentially biased ratings in the test  set.", "labels": [], "entities": []}, {"text": " Table 2: \u03ba numbers for different models with varying  \u03b1 values. Ratings in the the test set are likely to be  unbiased.", "labels": [], "entities": []}]}