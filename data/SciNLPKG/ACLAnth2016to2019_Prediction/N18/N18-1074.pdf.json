{"title": [{"text": "FEVER: a large-scale dataset for Fact Extraction and VERification", "labels": [], "entities": [{"text": "FEVER", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9118736982345581}, {"text": "Fact Extraction", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.9026079475879669}, {"text": "VERification", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.45994502305984497}]}], "abstractContent": [{"text": "In this paper we introduce anew publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9693995714187622}, {"text": "Fact Extraction", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.5792160481214523}]}, {"text": "It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.", "labels": [], "entities": []}, {"text": "The claims are classified as SUPPORTED, REFUTED or NOTENOUGHINFO by annota-tors achieving 0.6841 in Fleiss \u03ba.", "labels": [], "entities": [{"text": "SUPPORTED", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9615041017532349}, {"text": "REFUTED", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9950900077819824}, {"text": "NOTENOUGHINFO", "start_pos": 51, "end_pos": 64, "type": "METRIC", "confidence": 0.9218869805335999}, {"text": "Fleiss \u03ba", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9776226282119751}]}, {"text": "For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment.", "labels": [], "entities": []}, {"text": "To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles.", "labels": [], "entities": []}, {"text": "The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9993795156478882}, {"text": "labeling a claim accompanied", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.8734717965126038}]}, {"text": "Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.8683160543441772}, {"text": "claim verification", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.777735710144043}]}], "introductionContent": [{"text": "The ever-increasing amounts of textual information available combined with the ease in sharing it through the web has increased the demand for verification, also referred to as fact checking.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 177, "end_pos": 190, "type": "TASK", "confidence": 0.8363488912582397}]}, {"text": "While it has received a lot of attention in the context of journalism, verification is important for other domains, e.g. information in scientific publications, product reviews, etc.", "labels": [], "entities": []}, {"text": "In this paper we focus on verification of textual claims against textual sources.", "labels": [], "entities": []}, {"text": "When compared to textual entailment (TE)/natural language inference (), the key difference is that in these tasks the passage to verify each claim is given, and in recent years it typically consists a single sentence, while in verification systems it is retrieved from a large set of documents in order to form the evidence.", "labels": [], "entities": [{"text": "textual entailment (TE)/natural language inference", "start_pos": 17, "end_pos": 67, "type": "TASK", "confidence": 0.8088665455579758}]}, {"text": "Another related task is question answering (QA), for which approaches have recently been extended to handle large-scale resources such as.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.9157113075256348}]}, {"text": "However, questions typically provide the information needed to identify the answer, while information missing from a claim can often be crucial in retrieving refuting evidence.", "labels": [], "entities": []}, {"text": "For example, a claim stating \"Fiji's largest island is Kauai.\" can be refuted by retrieving \"Kauai is the oldest Hawaiian Island.\" as evidence.", "labels": [], "entities": []}, {"text": "Progress on the aforementioned tasks has benefited from the availability of large-scale datasets.", "labels": [], "entities": []}, {"text": "However, despite the rising interest in verification and fact checking among researchers, the datasets currently used for this task are limited to a few hundred claims.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.839354932308197}]}, {"text": "Indicatively, the recently conducted Fake News Challenge () with 50 participating teams used a dataset consisting of 300 claims verified against 2,595 associated news articles which is orders of magnitude smaller than those used for TE and QA.", "labels": [], "entities": [{"text": "Fake News Challenge", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.7863840659459432}, {"text": "TE and QA", "start_pos": 233, "end_pos": 242, "type": "TASK", "confidence": 0.555332233508428}]}, {"text": "In this paper we present anew dataset for claim verification, FEVER: Fact Extraction and VERification.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9899317026138306}, {"text": "Fact Extraction", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.6409536749124527}, {"text": "VERification", "start_pos": 89, "end_pos": 101, "type": "METRIC", "confidence": 0.9642488360404968}]}, {"text": "It consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as SUPPORTED, REFUTED or NOTENOUGHINFO.", "labels": [], "entities": [{"text": "REFUTED", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.9893203973770142}, {"text": "NOTENOUGHINFO", "start_pos": 141, "end_pos": 154, "type": "DATASET", "confidence": 0.7812920808792114}]}, {"text": "For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim (see).", "labels": [], "entities": []}, {"text": "The claims were generated by human annotators extracting claims from Wikipedia and mutating them in a variety of ways, some of which were meaning-altering.", "labels": [], "entities": []}, {"text": "The verification of each claim was conducted in a separate annotation process by annotators who were aware of the page but not the sentence from which original claim was extracted and thus in 31.75% of the claims more than one sentence was considered appropriate evidence.", "labels": [], "entities": []}, {"text": "Claims require composition of evidence from multiple sentences in 16.82% of cases.", "labels": [], "entities": []}, {"text": "Furthermore, in 12.15% of the claims, this evidence was taken from multiple pages.", "labels": [], "entities": []}, {"text": "To ensure annotation consistency, we developed suitable guidelines and user interfaces, resulting in inter-annotator agreement of 0.6841 in Fleiss \u03ba) in claim verification classification, and 95.42% precision and 72.36% recall in evidence retrieval.", "labels": [], "entities": [{"text": "Fleiss \u03ba", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9779261946678162}, {"text": "claim verification classification", "start_pos": 153, "end_pos": 186, "type": "TASK", "confidence": 0.7773453990618387}, {"text": "precision", "start_pos": 199, "end_pos": 208, "type": "METRIC", "confidence": 0.999656081199646}, {"text": "recall", "start_pos": 220, "end_pos": 226, "type": "METRIC", "confidence": 0.9995266199111938}]}, {"text": "To characterize the challenges posed by FEVER we develop a pipeline approach which, given a claim, first identifies relevant documents, then selects sentences forming the evidence from the documents and finally classifies the claim w.r.t. evidence.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.4026157259941101}]}, {"text": "The best performing version achieves 31.87% accuracy in verification when requiring correct evidence to be retrieved for claims SUP-PORTED or REFUTED, and 50.91% if the correctness of the evidence is ignored, both indicating the difficulty but also the feasibility of the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9995063543319702}, {"text": "REFUTED", "start_pos": 142, "end_pos": 149, "type": "METRIC", "confidence": 0.9729981422424316}]}, {"text": "We also conducted oracle experiments in which components of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sentences containing the evidence.", "labels": [], "entities": []}, {"text": "In addition to publishing the data via our website 1 , we also publish the annotation interfaces 2 and the baseline system to stimulate further research on verification.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset was constructed in two stages 4 : Claim Generation Extracting information from Wikipedia and generating claims from it.", "labels": [], "entities": [{"text": "Claim Generation Extracting information from Wikipedia", "start_pos": 46, "end_pos": 100, "type": "TASK", "confidence": 0.8116540908813477}]}, {"text": "Claim Labeling Classifying whether a claim is supported or refuted by Wikipedia and selecting the evidence for it, or deciding there's not enough information to make a decision.", "labels": [], "entities": [{"text": "Claim Labeling Classifying whether a claim", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7984125514825186}]}, {"text": "We partitioned the annotated claims into training, development and test sets.", "labels": [], "entities": []}, {"text": "We ensured that each Wikipedia page used to generate claims occurs in exactly one set.", "labels": [], "entities": []}, {"text": "We reserved a further 19,998 examples for use as a test set fora shared task.", "labels": [], "entities": []}, {"text": "Predicting whether a claim is SUPPORTED, RE-FUTED or NOTENOUGHINFO is a 3-way classification task that we evaluate using accuracy.", "labels": [], "entities": [{"text": "RE-FUTED", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.99236661195755}, {"text": "NOTENOUGHINFO", "start_pos": 53, "end_pos": 66, "type": "METRIC", "confidence": 0.7758401036262512}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.997820258140564}]}, {"text": "In the case of the first two classes, appropriate evidence must be provided, at a sentence-level, to justify the classification.", "labels": [], "entities": []}, {"text": "We consider an answer returned correct for the first two classes only if correct evidence is returned.", "labels": [], "entities": []}, {"text": "Given that the development and test datasets have balanced class distributions, a random baseline will have \u223c 33% accuracy if one ignores the requirement for evidence for SUPPORTED and REFUTED.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9993502497673035}, {"text": "REFUTED", "start_pos": 185, "end_pos": 192, "type": "METRIC", "confidence": 0.9283069968223572}]}, {"text": "We evaluate the correctness of the evidence retrieved by computing the F 1 -score of all the predicted sentences in comparison to the humanannotated sentences for those claims requiring evidence on our complete pipeline system (Section 5.7).", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9920221269130707}]}, {"text": "As in, some claims require multihop inference involving sentences from more than one document to be correctly supported as SUP-PORTED/REFUTED.", "labels": [], "entities": [{"text": "REFUTED", "start_pos": 134, "end_pos": 141, "type": "METRIC", "confidence": 0.9117724895477295}]}, {"text": "In this case all sentences must be selected for the evidence to be marked as correct.", "labels": [], "entities": []}, {"text": "We report this as the proportion of fully supported claims.", "labels": [], "entities": []}, {"text": "Some claims maybe equally supported by different pieces of evidence; in this case one complete set of sentences should be predicted.", "labels": [], "entities": []}, {"text": "Systems that select information that the annotators did not will be penalized in terms of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.999314546585083}]}, {"text": "We recognize that it is not feasible to ensure that the evidence selection annotations are complete, nevertheless we argue that they are useful for automatic evaluation during system development.", "labels": [], "entities": []}, {"text": "For a more reliable evaluation we advocate crowd-sourcing annotations of false-positive predictions at a later date in a similar manner to the TAC KBP Slot Filler Validation (Ellis et al., 2016).", "labels": [], "entities": [{"text": "TAC KBP Slot Filler Validation", "start_pos": 143, "end_pos": 173, "type": "TASK", "confidence": 0.5845336794853211}]}], "tableCaptions": [{"text": " Table 1: Dataset split sizes for SUPPORTED, REFUTED  and NOTENOUGHINFO (NEI) classes", "labels": [], "entities": [{"text": "REFUTED", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9465845823287964}]}, {"text": " Table 2: Dev. set document retrieval evaluation.", "labels": [], "entities": [{"text": "Dev. set document retrieval evaluation", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.7431706885496775}]}, {"text": " Table 3: Oracle classification on claims in the develop- ment set using gold sentences as evidence", "labels": [], "entities": [{"text": "Oracle classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9446049332618713}, {"text": "develop- ment set", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.6261011064052582}]}, {"text": " Table 4: Full pipeline results on development set", "labels": [], "entities": []}, {"text": " Table 5: Oracle accuracy on claims in the dev. set using  gold documents as evidence (c.f.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9914519786834717}, {"text": "dev. set", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.6872398356596628}]}, {"text": " Table 6: Pipeline accuracy on the dev. set without the  sentence selection module (c.f.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9785918593406677}]}]}