{"title": [{"text": "Learning to Generate Wikipedia Summaries for Underserved Languages from Wikidata", "labels": [], "entities": [{"text": "Learning to Generate Wikipedia Summaries", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.5812346577644348}]}], "abstractContent": [{"text": "While Wikipedia exists in 287 languages, its content is unevenly distributed among them.", "labels": [], "entities": []}, {"text": "In this work, we investigate the generation of open domain Wikipedia summaries in under-served languages using structured data from Wikidata.", "labels": [], "entities": [{"text": "generation of open domain Wikipedia summaries", "start_pos": 33, "end_pos": 78, "type": "TASK", "confidence": 0.7075269917647043}]}, {"text": "To this end, we propose a neural network architecture equipped with copy actions that learns to generate single-sentence and comprehensible textual summaries from Wikidata triples.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of the proposed approach by evaluating it against a set of baselines on two languages of different natures: Arabic, a morphological rich language with a larger vocabulary than English, and Esperanto, a constructed language known for its easy acquisition.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite the fact that Wikipedia exists in 287 languages, the existing content is unevenly distributed.", "labels": [], "entities": []}, {"text": "The content of the most under-resourced Wikipedias is maintained by a limited number of editors -they cannot curate the same volume of articles as the editors of large Wikipedia languagespecific communities.", "labels": [], "entities": []}, {"text": "It is therefore of the utmost social and cultural interests to address languages for which native speakers have only access to an impoverished Wikipedia.", "labels": [], "entities": []}, {"text": "In this paper, we propose an automatic approach to generate textual summaries that can be used as a starting point for the editors of the involved Wikipedias.", "labels": [], "entities": []}, {"text": "We propose an end-to-end trainable model that generates a textual summary given a set of KB triples as input.", "labels": [], "entities": []}, {"text": "We apply our model on two languages that have a severe lack of both editors and articles on Wikipedia: Esperanto is an easily acquired artificially created language which makes it less data needy and a more suitable starting point \u2020 The authors contributed equally to this for exploring the challenges of this task.", "labels": [], "entities": []}, {"text": "Arabic is a morphologically rich language that is much more challenging to work, mainly due to its significantly larger vocabulary.", "labels": [], "entities": []}, {"text": "As shown in both Arabic and Esperanto suffer a severe lack of content and active editors compared to the English Wikipedia which is currently the biggest one in terms of number of articles.", "labels": [], "entities": []}, {"text": "Our research is mostly related to previous work on adapting the general encoder-decoder framework for the generation of Wikipedia summaries (.", "labels": [], "entities": []}, {"text": "Nonetheless, all these approaches focus on task of biographies generation, and only in Englishthe language with the most language resources and knowledge bases available.", "labels": [], "entities": [{"text": "biographies generation", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.8449611365795135}]}, {"text": "In contrast with these works, we explore the generation of sentences in an open-domain, multilingual context.", "labels": [], "entities": []}, {"text": "The model from () takes the Wikipedia infobox as an input, while uses a sequence of slot-value pairs extracted from Wikidata.", "labels": [], "entities": [{"text": "Wikipedia infobox", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.9804690778255463}]}, {"text": "Both models are only able to generate single-subject relationships.", "labels": [], "entities": []}, {"text": "In our model the input triples go beyond the single-subject relationships of a Wikipedia infobox or a Wikidata page about a specific item (Section 2).", "labels": [], "entities": []}, {"text": "Similarly to our approach, the model proposed by) accepts a set of triples as input, however, it leverages instance-type-related information from DBpedia in order to generate text that addresses rare or unseen entities.", "labels": [], "entities": []}, {"text": "Our solution is much broader since it does not rely on the assumption that unseen triples will adopt the same pattern of properties and entities' instance types pairs as the ones that have been used for training.", "labels": [], "entities": []}, {"text": "To this end, we use copy actions over the labels of entities in the input triples.", "labels": [], "entities": []}, {"text": "This relates to previous works in machine translation which deals with rare or unseen word problem for translating names and numbers in text.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7640385031700134}, {"text": "translating names and numbers in text", "start_pos": 103, "end_pos": 140, "type": "TASK", "confidence": 0.8141502539316813}]}, {"text": "( propose a model that generates positional placeholders pointing to some words in source sentence and copy it to target sentence (copy actions).", "labels": [], "entities": []}, {"text": "We evaluate our approach by measuring how close our synthesised summaries can be to actual summaries in Wikipedia against two other baselines of different natures: a language model, and an information retrieval template-based solution.", "labels": [], "entities": []}, {"text": "Our model substantially outperforms all the baselines in all evaluation metrics in both Esperanto and Arabic.", "labels": [], "entities": []}, {"text": "In this work we present the following contributions: i) We investigate the task of generating textual summaries from Wikidata triples in underserved Wikipedia languages across multiple domains, and ii) We use an end-toend model with copy actions adapted to this task.", "labels": [], "entities": [{"text": "generating textual summaries from Wikidata triples in underserved Wikipedia languages", "start_pos": 83, "end_pos": 168, "type": "TASK", "confidence": 0.7377901673316956}]}, {"text": "Our datasets, results, and experiments are available at: https://github.com/pvougiou/ Wikidata2Wikipedia.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to train our models to generate summaries from Wikidata triples, we introduce anew dataset for text generation from KB triples in a multilingual setting and align it with the triples of its corresponding Wikidata Item.", "labels": [], "entities": [{"text": "text generation from KB triples", "start_pos": 104, "end_pos": 135, "type": "TASK", "confidence": 0.7540460884571075}]}, {"text": "For each Wikipedia article, we extract and tokenise the first introductory sentence and align it with triples where its corresponding item appears as a subject or an object in the Wikidata truthy dump.", "labels": [], "entities": [{"text": "Wikidata truthy dump", "start_pos": 180, "end_pos": 200, "type": "DATASET", "confidence": 0.8359420498212179}]}, {"text": "In order to create the surface form tuples (i.e. Section 2.3), we identify occurrences of entities in the text along with their verbalisations.", "labels": [], "entities": []}, {"text": "We rely on keyword matching against labels from Wikidata expanded by the global language fallback chain introduced by Wikimedia 2 to overcome the lack of non-English labels in Wikidata ( ).", "labels": [], "entities": [{"text": "keyword matching", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7095173001289368}]}, {"text": "For the property placeholders, we use the distant supervision assumption for relation extraction (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8455193936824799}]}, {"text": "Entities that participate in relations with the main entity of the article are being replaced with their corresponding property placeholder tag.", "labels": [], "entities": []}, {"text": "shows statistics on the two corpora that we used for the training of our systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Recent page statistics and number of unique  words (vocab. size) of Esperanto, Arabic and English  Wikipedias.", "labels": [], "entities": []}, {"text": " Table 3: Dataset statistics in Arabic and Esperanto.", "labels": [], "entities": []}, {"text": " Table 4: Automatic evaluation of our model against all other baselines using BLEU 1-4, ROUGE and METEOR  for both Arabic and Esperanto Validation and Test set", "labels": [], "entities": [{"text": "BLEU 1-4", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9665374755859375}, {"text": "ROUGE", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.990156888961792}, {"text": "METEOR", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9892107248306274}]}]}