{"title": [{"text": "Object Counts! Bringing Explicit Detections Back into Image Captioning", "labels": [], "entities": [{"text": "Image Captioning", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.6948688179254532}]}], "abstractContent": [{"text": "The use of explicit object detectors as an intermediate step to image captioning-which used to constitute an essential stage in early work-is often bypassed in the currently dominant end-to-end approaches, where the language model is conditioned directly on a mid-level image embedding.", "labels": [], "entities": [{"text": "image captioning-which", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7224482148885727}]}, {"text": "We argue that explicit detections provide rich semantic information, and can thus be used as an interpretable representation to better understand why end-to-end image captioning systems work well.", "labels": [], "entities": []}, {"text": "We provide an in-depth analysis of end-to-end image captioning by exploring a variety of cues that can be derived from such object detections.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.7848216593265533}]}, {"text": "Our study reveals that end-to-end image cap-tioning systems rely on matching image representations to generate captions, and that encoding the frequency, size and position of objects are complementary and all play a role in forming a good image representation.", "labels": [], "entities": []}, {"text": "It also reveals that different object categories contribute in different ways towards image cap-tioning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Image captioning (IC), or image description generation, is the task of automatically generating a sentential textual description fora given image.", "labels": [], "entities": [{"text": "Image captioning (IC)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8512103140354157}, {"text": "image description generation", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.7246889273325602}]}, {"text": "Early work on IC tackled the task by first running object detectors on the image and then using the resulting explicit detections as input to generate a novel textual description, e.g. ().", "labels": [], "entities": [{"text": "IC", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9717724323272705}]}, {"text": "With the advent of sequence-to-sequence approaches to IC, e.g. (, coupled with the availability of large image description datasets, the performance of IC systems showed marked improvement, at least according to automatic evaluation metrics like Meteor) and CIDEr ().", "labels": [], "entities": [{"text": "IC", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9812313914299011}, {"text": "Meteor", "start_pos": 246, "end_pos": 252, "type": "DATASET", "confidence": 0.9104238748550415}]}, {"text": "The currently dominant neural-based IC systems are often trained end-to-end, using parallel (image, caption) datasets.", "labels": [], "entities": []}, {"text": "Such systems are essentially sequential language models conditioned directly on some mid-level image features, such as an image embedding extracted from a pre-trained Convolutional Neural Network (CNN).", "labels": [], "entities": []}, {"text": "Thus, they bypass the explicit detection phase of previous methods and instead generate captions directly from image features.", "labels": [], "entities": []}, {"text": "Despite significant progress, it remains unclear why such systems work.", "labels": [], "entities": []}, {"text": "A major problem with these IC systems is that they are less interpretable than conventional pipelined methods which use explicit detections.", "labels": [], "entities": []}, {"text": "We believe that it is timely to again start exploring the use of explicit object detections for image captioning.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.7168972045183182}]}, {"text": "Explicit detections offer rich semantic information, which can be used to model the entities in the image as well as their interactions, and can be used to better understand image captioning.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 174, "end_pos": 190, "type": "TASK", "confidence": 0.7449671924114227}]}, {"text": "Recent work showed that conditioning an end-to-end IC model on visual representations that implicitly encode object details yields reasonably good captions.", "labels": [], "entities": []}, {"text": "Nevertheless, it is still unclear why this works, and what aspects of the representation allow for such a good performance.", "labels": [], "entities": []}, {"text": "In this paper, we study end-to-end IC in the context of explicit detections) by exploring a variety of cues that can be derived from such detections to determine what information from such representations helps image captioning, and why.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 211, "end_pos": 227, "type": "TASK", "confidence": 0.71710304915905}]}, {"text": "To our best knowledge, our work is the first experimental analysis of end-to-end IC frameworks that uses object-level information that is highly interpretable as a tool for understanding such systems.", "labels": [], "entities": []}, {"text": "Our main contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We provide an in-depth analysis of the performance of end-to-end IC using a simple, yet effective 'bag of objects' representation that is interpretable, and generates good captions despite being low-dimensional and highly sparse (Section 3).", "labels": [], "entities": []}, {"text": "2. We investigate whether other spatial cues can be used to provide information complementary to frequency counts (Section 3).", "labels": [], "entities": []}, {"text": "3. We study the effect of incorporating different spatial information of individual object instances from explicit detections (Section 4).", "labels": [], "entities": []}, {"text": "4. We analyze the contribution of the categories in representations for IC by ablating individual categories from them (Section 5).", "labels": [], "entities": [{"text": "IC", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9381511807441711}]}, {"text": "Our hypothesis is that there are important components derived from explicit detections that can be used to effectively inform IC.", "labels": [], "entities": [{"text": "IC", "start_pos": 126, "end_pos": 128, "type": "TASK", "confidence": 0.9041779041290283}]}, {"text": "Our study confirms our hypothesis, and that features such as the frequency, size and position of objects all play a role in forming a good image representation to match their corresponding representations in the training set.", "labels": [], "entities": []}, {"text": "Our findings also show that different categories contribute differently to IC, and this partly depends on how likely they are to be mentioned in the caption given that they are depicted in the image.", "labels": [], "entities": [{"text": "IC", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.981513500213623}]}, {"text": "The results of our investigation will help further work towards more interpretable image captioning.", "labels": [], "entities": [{"text": "interpretable image captioning", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.6006227632363638}]}], "datasetContent": [{"text": "By comparing the CIDEr score changes against the frequency counts of object annotations in the training set (top row), there does not seem to be a clear correlation between depiction frequency and CIDEr.", "labels": [], "entities": []}, {"text": "Categories like bear are infrequent but led to a large drop in score; likewise, chair and dining table are frequent but do not affect the results: Correlation between changes in CIDEr score from category ablation and the frequency of depiction of the category (f (v c )) against the probably of it being mentioned in the caption given depiction ((p(t c |v c )).", "labels": [], "entities": [{"text": "probably", "start_pos": 283, "end_pos": 291, "type": "METRIC", "confidence": 0.9731183648109436}]}, {"text": "In contrast, the frequency of a category being mentioned given that it is depicted is a better predictor for the changes in CIDEr scores in general (middle row).", "labels": [], "entities": []}, {"text": "Animate objects seem to be important to IC and are often mentioned in captions ( ).", "labels": [], "entities": [{"text": "IC", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9673791527748108}]}, {"text": "Interestingly, removing spoon greatly affects the results even though it is not frequent in captions.", "labels": [], "entities": [{"text": "spoon", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9305747151374817}]}, {"text": "presents the rank correlation (Spearman's \u03c1 and Kendall's \u03c4 , two-tailed test) between changes in CIDEr and the two heuristics.", "labels": [], "entities": [{"text": "rank correlation", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.8464050889015198}]}, {"text": "While both heuristics are positively correlated with the changes in CIDEr, we can conclude that the frequency of being mentioned (given that it is depicted) is better correlated with the score changes than the frequency of depiction.", "labels": [], "entities": []}, {"text": "Of course, the categories are not mutually exclusive and object co-occurrence may also play a role.", "labels": [], "entities": []}, {"text": "However, we leave this analysis for future work.", "labels": [], "entities": []}, {"text": "shows an example when the category person is removed from the feature vector.", "labels": [], "entities": []}, {"text": "Here, the model does not generate any text related to person, as the training set contains images of clocks without people in it.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: CIDEr scores for image captioning using bag  of objects variants as visual representations. We com- pare the results of using ground truth annotations (GT)  and the output of a detector (Detect). As comparison  we also provide, in the first row, the results of using a  ResNet-152 POOL5 CNN image embedding with our  implementation of an end-to-end IC system.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7332445532083511}, {"text": "ground truth annotations (GT)", "start_pos": 136, "end_pos": 165, "type": "METRIC", "confidence": 0.7137155135472616}]}, {"text": " Table 2: CIDEr scores for captioning comparing the  use of min, max or average pooling of either object size  or distance features, using ground truth annotations.", "labels": [], "entities": []}, {"text": " Table 3: k-Nearest Neighbour (k=5) trial on the ground  truth bag of objects (Freq.) and the projected bag of  objects (Proj.) representations. The references are cap- tions of 5-nearest images in each space. Exact repre- sents a subset of 2301 samples where all the 5 neigh- bours have 0 distance (replicas) and \u00ac represents near- est neighbours that are not replicas of the test image.", "labels": [], "entities": []}, {"text": " Table 4: CIDEr scores for image captioning using rep- resentations encoding spatial information of instances  derived from ground truth annotations, with either fixed  hyperparameters (Section 3.1) or with hyperparameter  tuning.  \u2020 Results taken from (Yin and Ordonez, 2017).", "labels": [], "entities": [{"text": "image captioning", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7251944094896317}]}, {"text": " Table 6: Full results for image captioning using ground truth bag of objects variants as visual representations, for  metrics BLEU, Meteor, ROUGE L , CIDEr and SPICE.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.734347864985466}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9960748553276062}, {"text": "ROUGE L", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.9630911350250244}]}, {"text": " Table 7: Full results for image captioning, using representations encoding spatial information of instances derived  from ground truth annotations with fixed hyperparameters, for metrics BLEU, Meteor, ROUGE L , CIDEr and  SPICE.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7302378416061401}, {"text": "BLEU", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.9969460368156433}, {"text": "ROUGE L", "start_pos": 202, "end_pos": 209, "type": "METRIC", "confidence": 0.9298612177371979}]}]}