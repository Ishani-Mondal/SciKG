{"title": [{"text": "Robust Machine Comprehension Models via Adversarial Training", "labels": [], "entities": []}], "abstractContent": [{"text": "It is shown that many published models for the Stanford Question Answering Dataset (Ra-jpurkar et al., 2016) lack robustness, suffering an over 50% decrease in F1 score during adversarial evaluation based on the AddSent (Jia and Liang, 2017) algorithm.", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset", "start_pos": 47, "end_pos": 82, "type": "DATASET", "confidence": 0.8274829238653183}, {"text": "F1 score", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9909726679325104}]}, {"text": "It has also been shown that retraining models on data generated by AddSent has limited effect on their robustness.", "labels": [], "entities": [{"text": "AddSent", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.8859072327613831}]}, {"text": "We propose a novel alternative adversary-generation algorithm, AddSentDiverse, that significantly increases the variance within the adversarial training data by providing effective examples that punish the model for making certain superficial assumptions.", "labels": [], "entities": []}, {"text": "Further, in order to improve robustness to AddSent's semantic perturbations (e.g., antonyms), we jointly improve the model's semantic-relationship learning capabilities in addition to our AddSentDiverse-based adversarial training data augmentation.", "labels": [], "entities": []}, {"text": "With these additions, we show that we can make a state-of-the-art model significantly more robust, achieving a 36.5% increase in F1 score under many different types of adversar-ial evaluation while maintaining performance on the regular SQuAD task.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9868115484714508}]}], "introductionContent": [{"text": "We explore the task of reading comprehension based question answering (Q&A), where we focus on the Stanford Question Answering Dataset (SQuAD) (, in which models answer questions about paragraphs taken from Wikipedia.", "labels": [], "entities": [{"text": "reading comprehension based question answering (Q&A)", "start_pos": 23, "end_pos": 75, "type": "TASK", "confidence": 0.7779809594154358}, {"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 99, "end_pos": 142, "type": "DATASET", "confidence": 0.8226947358676365}]}, {"text": "Significant progress has been made with deep end to end neural-attention models, with some achieving above human level performance on the test set (.", "labels": [], "entities": []}, {"text": "However, as shown recently by, these models are very fragile when presented with adversarially generated data.", "labels": [], "entities": []}, {"text": "They proposed AddSent, which creates a semantically-irrelevant sentence containing a fake answer that resembles the question syntactically, and appends it to the context.", "labels": [], "entities": []}, {"text": "Many state-ofthe-art models exhibit a nearly 50% reduction in F1 score on AddSent, showing their over-reliance on syntactic similarity and limited semantic understanding.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.985958456993103}]}, {"text": "Importantly, this is in part due to the nature of the SQuAD dataset.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.851578027009964}]}, {"text": "Most questions in the dataset have answer spans embedded in sentences that are syntactically similar to the question.", "labels": [], "entities": []}, {"text": "Thus during training, the model is rarely punished for answering questions based on syntactic similarity, and learns it as a reliable approach to Q&A.", "labels": [], "entities": [{"text": "Q&A", "start_pos": 146, "end_pos": 149, "type": "TASK", "confidence": 0.9288019935290018}]}, {"text": "This correlation between syntactic similarity and correctness is of course not true in general: the adversaries generated by AddSent ( are syntactically similar to the question but do not answer them.", "labels": [], "entities": [{"text": "correctness", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9732479453086853}]}, {"text": "The models' failures on AddSent demonstrates their ignorance of this aspect of the task.", "labels": [], "entities": []}, {"text": "presented some initial attempts to fix this problem by retraining the BiDAF model () with adversaries generated with AddSent.", "labels": [], "entities": []}, {"text": "But they showed that the method is not very effective, as slight modifications (e.g., different positioning of the distractor sentence in the paragraph and different fake answer set) to the adversary generation algorithm attest time have drastic impact on the retrained model's performance.", "labels": [], "entities": []}, {"text": "In this paper, we show that their method of adversarial training failed because the specificity of the AddSent algorithm along with the lack of naturally-occurring counterexamples allow models to learn superficial clues regarding what is a 'distractor' and subsequently ignore it; thus significantly limiting their robustness.", "labels": [], "entities": []}, {"text": "Instead, we first introduce a novel algorithm, AddSentDiverse, for generating adversarial examples with signifi-cantly higher variance (by varying the locations where the distractors are placed and expanding the set of fake answers), so that the model is punished during training time for making these superficial assumptions about the distractor.", "labels": [], "entities": []}, {"text": "We show that an AddSentDiverse-based adversariallytrained model beats an AddSent-trained model across 3 different adversarial test sets, showing an average improvement of 24.22% in F1 score, demonstrating a general increase in robustness.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9791970551013947}]}, {"text": "However, even with our diversified adversarial training data, the model is still not fully resilient to AddSent-style attacks, e.g., its antonymy-style semantic perturbations.", "labels": [], "entities": []}, {"text": "Hence, we next add semantic relationship features to the model to let it directly identify such relationships between the context and question.", "labels": [], "entities": []}, {"text": "Interestingly, we see that these additions only increase model robustness when trained adversarially, because intuitively in the non-adversarially-trained setup, there are not enough negative (adversarial) examples for the model to learn how to use its semantic features.", "labels": [], "entities": []}, {"text": "Overall, we demonstrate that with our adversarial training method and model improvement, we can increase the performance of a state-of-theart model by 36.46% on the AddSent evaluation set.", "labels": [], "entities": [{"text": "AddSent evaluation set", "start_pos": 165, "end_pos": 187, "type": "DATASET", "confidence": 0.6495852569739023}]}, {"text": "Although we focused on the AddSent adversary (, our method of effective adversarial training by eliminating superficial statistical correlations (with joint model capability improvements) are generalizable to other similar insertion-based adversaries for Q&A tasks.", "labels": [], "entities": [{"text": "Q&A tasks", "start_pos": 255, "end_pos": 264, "type": "TASK", "confidence": 0.8939751237630844}]}], "datasetContent": [{"text": "Models are evaluated on the original SQuAD dev set and 4 adversarial datasets: AddSent, the adversarial evaluation set by, and 3 variations of AddSent: AddSentPrepend, where the distractor is prepended to the context, AddSentRandom, where the distractor is randomly inserted into the context, and AddSentMod, where a different set of fake answers is used and the distractor is prepended to the context.", "labels": [], "entities": [{"text": "SQuAD dev set", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.7196783423423767}]}, {"text": "Experiments measure the soft F1 score and all of the adversarial evaluations are modeldependent, following the style of AddSent, where multiple adversaries are generated for each exam-ple in the evaluation set and the model's worst performance among the variants is recorded.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9546418786048889}]}, {"text": "In our main experiment, we compare the BSAE model's performance on different test sets when trained with three different training sets: the original SQuAD data (Original-SQuAD), SQuAD data augmented with AddSent generated adversaries (similar to adversarial training conducted by Jia and Liang), and SQuAD data augmented with our AddSentDiverse generated adversaries.", "labels": [], "entities": []}, {"text": "For the latter two, we run the respective adversarial generation algorithms on the training set, and add randomly selected adversarial examples such that they makeup 20% of the total training data.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "First, as shown, the AddSent-trained model is notable to perform well on test sets where the distractors are not inserted at the end, e.g., the AddSentRandom adversarial test set.", "labels": [], "entities": [{"text": "AddSentRandom adversarial test set", "start_pos": 144, "end_pos": 178, "type": "DATASET", "confidence": 0.6955237314105034}]}, {"text": "On the other hand, it can be seen that retraining with AddSentDiverse boosts performance of the model significantly across all adversarial datasets, indicating a general increase in robustness.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1 performance of the BSAE model trained and tested on different regular/adversarial datasets.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9985750913619995}, {"text": "BSAE", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.612576961517334}]}, {"text": " Table 2: F1 performance of the BSAE model trained on  datasets with different distractor placement strategies.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9981306195259094}, {"text": "BSAE", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.587043821811676}]}, {"text": " Table 3: F1 performance of the BSAE model trained  on datasets with different answer generation strategies.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9984819293022156}, {"text": "BSAE", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.5588732957839966}, {"text": "answer generation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.754912793636322}]}, {"text": " Table 4: Regular and adversarial training with BSAE  and BSAE+SA (with synonym/antonym features).", "labels": [], "entities": [{"text": "BSAE", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.9193779230117798}, {"text": "BSAE+SA", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.7306934197743734}]}, {"text": " Table 5: F1 Performance of the BSAE model trained on  datasets with different distractor placement strategies.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9985589385032654}, {"text": "BSAE", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.6008185148239136}, {"text": "distractor placement", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.6948002725839615}]}]}