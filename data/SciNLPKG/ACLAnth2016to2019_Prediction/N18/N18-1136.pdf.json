{"title": [{"text": "Identifying Semantic Divergences in Parallel Text without Annotations", "labels": [], "entities": [{"text": "Identifying Semantic Divergences", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9113472700119019}]}], "abstractContent": [{"text": "Recognizing that even correct translations are not always semantically equivalent, we automatically detect meaning divergences in parallel sentence pairs with a deep neural model of bilingual semantic similarity which can be trained for any parallel corpus without any manual annotation.", "labels": [], "entities": []}, {"text": "We show that our semantic model detects divergences more accurately than models based on surface features derived from word alignments, and that these divergences matter for neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 174, "end_pos": 200, "type": "TASK", "confidence": 0.6378991603851318}]}], "introductionContent": [{"text": "Parallel sentence pairs are sentences that are translations of each other, and are therefore often assumed to convey the same meaning in the source and target language.", "labels": [], "entities": []}, {"text": "Occasional mismatches between source and target have been primarily viewed as alignment noise () due to imperfect sentence alignment tools in parallel corpora drawn from translated texts, or the noisy process of extracting parallel segments from non-parallel corpora ().", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7690771818161011}]}, {"text": "In contrast, we view translation as a process that inherently introduces meaning mismatches, so that even correctly aligned sentence pairs are not necessarily semantically equivalent.", "labels": [], "entities": []}, {"text": "This can happen for many reasons: translation lexical choice often involves selecting between near synonyms that introduce language-specific nuances, typological divergences lead to structural mismatches, differences in discourse organization can make it impossible to find oneto-one sentence alignments ().", "labels": [], "entities": [{"text": "translation lexical choice", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.9434997240702311}]}, {"text": "Cross-linguistic variations in other discourse phenomena such as coreference, discourse relation and modality) compounded with translation effects that distinguish \"translationese\" from original text () might also lead to meaning mismatches between source and target.", "labels": [], "entities": []}, {"text": "In this paper, we aim to provide empirical evidence that semantic divergences exist in parallel corpora and matter for downstream applications.", "labels": [], "entities": []}, {"text": "This requires an automatic method to distinguish semantically equivalent sentence pairs from semantically divergent pairs, so that parallel corpora can be used more judiciously in downstream cross-lingual NLP applications.", "labels": [], "entities": []}, {"text": "We propose a semantic model to automatically detect whether a sentence pair is semantically divergent (Section 3).", "labels": [], "entities": []}, {"text": "While prior work relied on surface cues to detect mis-aligments, our approach focuses on comparing the meaning of words and overlapping text spans using bilingual word embeddings () and a deep convolutional neural network.", "labels": [], "entities": []}, {"text": "Crucially, training this model requires no manual annotation.", "labels": [], "entities": []}, {"text": "Noisy supervision is obtained automatically borrowing techniques developed for parallel sentence extraction ().", "labels": [], "entities": [{"text": "parallel sentence extraction", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.6205846965312958}]}, {"text": "Our model can thus easily be trained to detect semantic divergences in any parallel corpus.", "labels": [], "entities": []}, {"text": "We extensively evaluate our semanticallymotivated models on intrinsic and extrinsic tasks: detection of divergent examples in two parallel English-French data sets (Section 5), and data selection for English-French and VietnameseEnglish machine translation (MT) (Section 6).The semantic models significantly outperform other methods on the intrinsic task, and help select data to train neural machine translation faster with no loss in translation quality.", "labels": [], "entities": [{"text": "English-French and VietnameseEnglish machine translation (MT)", "start_pos": 200, "end_pos": 261, "type": "TASK", "confidence": 0.7348914779722691}, {"text": "neural machine translation", "start_pos": 386, "end_pos": 412, "type": "TASK", "confidence": 0.7459054589271545}]}, {"text": "Taken together, these results provide empirical evidence that sentencealignment does not necessarily imply semantic equivalence, and that this distinction matters in practice fora downstream NLP application.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using the two test sets obtained above, we can evaluate the accuracy of our cross-lingual semantic divergence detector, and compare it against a diverse set of baselines in controlled settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9995473027229309}, {"text": "cross-lingual semantic divergence", "start_pos": 76, "end_pos": 109, "type": "TASK", "confidence": 0.6322574118773142}]}, {"text": "We test our hypothesis that semantic divergences are more than alignment mismatches by comparing the semantic divergence detector with models that capture mis-alignment (Section 5.2) or translation (Section 5.3).", "labels": [], "entities": []}, {"text": "Then, we compare the deep convolutional architecture of the semantic divergence model, with a much simpler model that directly compares bilingual sentence embeddings (Section 5.4).", "labels": [], "entities": []}, {"text": "Finally, we compare our model trained on synthetic examples with a supervised classifier used in prior work to predict finer-grained textual entailment categories based on manually created training examples (Section 5.5).", "labels": [], "entities": []}, {"text": "Except for the entailment classifier which uses external resources, all models are trained on the exact same parallel corpora (OpenSubtitles or CommonCrawl for evaluating on the corresponding test bed.)", "labels": [], "entities": [{"text": "CommonCrawl", "start_pos": 144, "end_pos": 155, "type": "DATASET", "confidence": 0.9293192625045776}]}, {"text": "Among the baseline methods, the nonentailment model is the weakest.", "labels": [], "entities": []}, {"text": "While it uses manually constructed training examples, these examples are drawn from distant domains, and the categories annotated do not exactly match the task As in the prior work, alignments are trained on 2M sentence pairs from Europarl () using the Berkeley aligner ().", "labels": [], "entities": [{"text": "Europarl", "start_pos": 231, "end_pos": 239, "type": "DATASET", "confidence": 0.9734688401222229}]}, {"text": "The classifier is the linear SVM from Scikit-Learn.", "labels": [], "entities": []}, {"text": "In contrast, the other models benefit from training on examples drawn from the same corpus as each test set.", "labels": [], "entities": []}, {"text": "Next, the machine translation based model and the sentence embedding model, both of which are unsupervised, are significantly weaker than the two supervised models trained on synthetic data, highlighting the benefits of the automatically constructed divergence examples.", "labels": [], "entities": []}, {"text": "The strength of the semantic similarity model compared to the sentence embeddings model highlights the benefits of the fine-grained representation of bilingual sentence pairs as a similarity cube, as opposed to the bag-of-words sentence embedding representation.", "labels": [], "entities": []}, {"text": "Finally, despite training on the same noisy synthetic data as the parallel v/s non-parallel system, the semantic similarity model is better able to detect meaning divergences.", "labels": [], "entities": []}, {"text": "This highlights the benefits of (1) meaning comparison between words in a shared embedding space, over the discrete translation dictionary used by the baseline, and of (2) the deep convolutional neural network which enables the semantic comparison of overlapping spans in sentence pairs, as opposed to more local word alignment features.", "labels": [], "entities": [{"text": "meaning comparison between words", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.8164452463388443}, {"text": "word alignment", "start_pos": 313, "end_pos": 327, "type": "TASK", "confidence": 0.7639863789081573}]}, {"text": "Having established the effectiveness of the semantic divergence detector, we now measure the impact of divergences on a downstream task, machine translation.", "labels": [], "entities": [{"text": "semantic divergence detector", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.7864317695299784}, {"text": "machine translation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7648482620716095}]}, {"text": "As in our prior work, we take a data selection approach, selecting the least divergent examples in a parallel corpus based on a range of divergence detectors, and comparing the translation quality of the resulting neural MT systems.", "labels": [], "entities": [{"text": "data selection", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7122860848903656}]}], "tableCaptions": [{"text": " Table 2: Intrinsic evaluation on crowdsourced semantic equivalence vs. divergence testsets. We report overall F- score, as well as precision (P), recall (R) and F-score (F) for the equivalent (+) and divergent (-) classes separately.  Semantic similarity yields better results across the board, with larger improvements on the divergent class.", "labels": [], "entities": [{"text": "F- score", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9923047224680582}, {"text": "precision (P)", "start_pos": 132, "end_pos": 145, "type": "METRIC", "confidence": 0.9531500041484833}, {"text": "recall (R)", "start_pos": 147, "end_pos": 157, "type": "METRIC", "confidence": 0.9522399455308914}, {"text": "F-score (F)", "start_pos": 162, "end_pos": 173, "type": "METRIC", "confidence": 0.9471810460090637}]}, {"text": " Table 3: English-French decoding results. BLEU  scores are either averaged across 3 runs (\"Avg.\") or ob- tained via ensemble decoding (\"Ensemble\"). SEMAN- TIC SIMILARITY reaches BLEU scores on par with  ALL with only half of the training data. SEMANTIC  SIMILARITY scores marked with * are significanly bet- ter (p < 0.05) than the corresponding ALL scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9983373880386353}, {"text": "Avg.", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9554502964019775}, {"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9983976483345032}]}, {"text": " Table 4: Vietnamese-English decoding results: drop- ping 10% of the data based on SEMANTIC SIMILAR- ITY does not hurt BLEU, and performs significantly (p  < 0.05) better than RANDOM selection.", "labels": [], "entities": [{"text": "drop- ping", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.735272228717804}, {"text": "SEMANTIC SIMILAR- ITY", "start_pos": 83, "end_pos": 104, "type": "METRIC", "confidence": 0.7451464459300041}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9984110593795776}]}]}