{"title": [{"text": "Automatic Stance Detection Using End-to-End Memory Networks", "labels": [], "entities": [{"text": "Automatic Stance Detection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7585885922114054}]}], "abstractContent": [{"text": "We present an effective end-to-end memory network model that jointly (i) predicts whether a given document can be considered as relevant evidence fora given claim, and (ii) extracts snippets of evidence that can be used to reason about the factuality of the target claim.", "labels": [], "entities": []}, {"text": "Our model combines the advantages of con-volutional and recurrent neural networks as part of a memory network.", "labels": [], "entities": []}, {"text": "We further introduce a similarity matrix at the inference level of the memory network in order to extract snippets of evidence for input claims more accurately.", "labels": [], "entities": []}, {"text": "Our experiments on a public benchmark dataset, FakeNewsChallenge, demonstrate the effectiveness of our approach.", "labels": [], "entities": [{"text": "FakeNewsChallenge", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.8772040009498596}]}], "introductionContent": [{"text": "Recently, an unprecedented amount of false information has been flooding the Internet with aims ranging from affecting individual people's beliefs and decisions to influencing major events such as political elections.", "labels": [], "entities": []}, {"text": "Consequently, manual fact checking has emerged with the promise to support accurate and unbiased analysis of rumors spreading in social medias, as well as of claims made by public figures or news media.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.7804256081581116}]}, {"text": "As manual fact checking is a very tedious task, automatic fact checking has been proposed as a possible alternative.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8283553421497345}, {"text": "fact checking", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.8244121372699738}]}, {"text": "This is often broken into intermediate steps in order to alleviate the task complexity.", "labels": [], "entities": []}, {"text": "One such step is stance detection, which is also useful for human experts as a stand-alone task.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.9565197229385376}]}, {"text": "The task aims to identify the relative perspective of apiece of text with respect to a claim, typically modeled using labels such as agree, disagree, discuss, and unrelated; gives some examples.", "labels": [], "entities": []}, {"text": "* Work conducted while these authors were at QCRI.", "labels": [], "entities": [{"text": "QCRI", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.9776056408882141}]}, {"text": "Here, we address the problem of stance detection using a novel model based on end-to-end memory networks (, which incorporates convolutional and recurrent neural networks, as well as a similarity matrix.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.9841490387916565}]}, {"text": "Our model jointly addresses the problems of predicting the stance of a text with respect to a given claim, and of extracting relevant text snippets as support for the prediction of the model.", "labels": [], "entities": [{"text": "predicting the stance of a text", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.8204215168952942}]}, {"text": "We further introduce a similarity matrix, which we use at inference time in order to improve the extraction of relevant snippets.", "labels": [], "entities": []}, {"text": "The experimental results on the Fake News Challenge benchmark dataset show that our model, which is very feature-light, performs close to the state of the art.", "labels": [], "entities": [{"text": "Fake News Challenge benchmark dataset", "start_pos": 32, "end_pos": 69, "type": "DATASET", "confidence": 0.8807132124900818}]}, {"text": "Our contributions can be summarized as follows: (i) We apply a novel memory network model enhanced with CNN and LSTM networks for stance detection.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 130, "end_pos": 146, "type": "TASK", "confidence": 0.9775515794754028}]}, {"text": "(ii) We further propose a novel extension of the general architecture based on a similarity matrix, which we use at inference time, and we show that this extension offers sizable performance gains.", "labels": [], "entities": []}, {"text": "(iii) Finally, we show that our model is capable of extracting meaningful snippets from a given text document, which is useful not only for stance detection, but more importantly can support human experts who need to decide on the factuality of a given claim.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 140, "end_pos": 156, "type": "TASK", "confidence": 0.9520441889762878}]}], "datasetContent": [{"text": "We use the following evaluation measures: Accuracy: The number of correctly classified examples divided by the total number of examples.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9992856383323669}]}, {"text": "It is equivalent to micro-averaged F 1 . Macro-F1: We calculate F 1 for each class, and then we average across all classes.", "labels": [], "entities": [{"text": "F 1", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.8944264054298401}, {"text": "F 1", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9856037199497223}]}, {"text": "Weighted Accuracy: This is a weighted, twolevel scoring scheme, which is applied to each test example.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.8746892213821411}]}, {"text": "First, if the example is from the unrelated class and the model correctly predicts it, the score is incremented by 0.25; otherwise, if the example is related and the model predicts agree, disagree, or discuss, the score is incremented by 0.25.", "labels": [], "entities": []}, {"text": "Second, there is a further increment by 0.75 for each related example if the model predicts the correct label: agree, disagree, or discuss.", "labels": [], "entities": []}, {"text": "1 Available at www.fakenewschallenge.org Finally, the score is normalized by dividing it by the total number of test examples.", "labels": [], "entities": []}, {"text": "The rationale behind this metric is that the binary related/unrelated classification task is expected to be much easier, while also being arguably less relevant to fake news detection than the stance detection task, which aims to further classify relevant instances as agree, disagree, or discuss.", "labels": [], "entities": [{"text": "fake news detection", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.6949922442436218}, {"text": "stance detection", "start_pos": 193, "end_pos": 209, "type": "TASK", "confidence": 0.8632483780384064}]}, {"text": "Therefore, the former task is given less weight and the latter task is given more weight through the weighted accuracy metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9263956546783447}]}], "tableCaptions": [{"text": " Table 1: Summary of our memory network algorithm  for stance detection.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.9819232821464539}]}, {"text": " Table 2: Evaluation results on the test data.", "labels": [], "entities": []}, {"text": " Table 3: Examples of highly ranked snippets of evidence for an input claim, which are automatically extracted by  our inference component. The P j  cnn column and the values in the top-right corner of the highlighted snippets show  the similarity between the claim and evidence, and between the claim and snippets of the evidence, respectively.", "labels": [], "entities": []}]}