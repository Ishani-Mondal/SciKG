{"title": [{"text": "An Encoder-decoder Approach to Predicting Causal Relations in Stories", "labels": [], "entities": [{"text": "Predicting Causal Relations", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.8348072171211243}]}], "abstractContent": [{"text": "We address the task of predicting causally related events in stories according to a standard evaluation framework, the Choice of Plausible Alternatives (COPA).", "labels": [], "entities": [{"text": "predicting causally related events in stories", "start_pos": 23, "end_pos": 68, "type": "TASK", "confidence": 0.8749118645985922}]}, {"text": "We present a neu-ral encoder-decoder model that learns to predict relations between adjacent sequences in stories as a means of modeling causality.", "labels": [], "entities": []}, {"text": "We explore this approach using different methods for extracting and representing sequence pairs as well as different model architectures.", "labels": [], "entities": []}, {"text": "We also compare the impact of different training datasets on our model.", "labels": [], "entities": []}, {"text": "In particular, we demonstrate the usefulness of a corpus not previously applied to COPA, the ROCStories corpus.", "labels": [], "entities": [{"text": "COPA", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.853530764579773}, {"text": "ROCStories corpus", "start_pos": 93, "end_pos": 110, "type": "DATASET", "confidence": 0.9291504323482513}]}, {"text": "While not state-of-the-art, our results establish anew reference point for systems evaluated on COPA, and one that is particularly informative for future neural-based approaches.", "labels": [], "entities": [{"text": "COPA", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.78841632604599}]}], "introductionContent": [{"text": "Automated story understanding is a long-pursued task in AI research.", "labels": [], "entities": [{"text": "Automated story understanding", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8093504905700684}]}, {"text": "It has been examined as a commonsense reasoning task, by which systems make inferences about events that prototypically occur in common experiences (e.g. going to a restaurant).", "labels": [], "entities": [{"text": "commonsense reasoning task", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8754627704620361}]}, {"text": "Early work often failed to scale beyond narrow domains of stories due to the difficulty of automatically inducing story knowledge.", "labels": [], "entities": []}, {"text": "The shift to data-driven AI established new opportunities to acquire this knowledge automatically from story corpora.", "labels": [], "entities": []}, {"text": "The field of NLP now recognizes that the type of commonsense reasoning used to predict what happens next in a story, for example, is as important for natural language understanding systems as linguistic knowledge itself.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 150, "end_pos": 180, "type": "TASK", "confidence": 0.6616631150245667}]}, {"text": "A barrier to this research has been the lack of standard evaluation schemes for benchmarking progress.", "labels": [], "entities": []}, {"text": "The Story Cloze Test  was recently developed to address this, with a focus on predicting events that are temporally and causally related within common real-world scenarios.", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.7191556493441263}]}, {"text": "The Story Cloze Test involves selecting which of two given sentences best completes a particular story.", "labels": [], "entities": []}, {"text": "Related to this is the Choice of Plausible Alternatives (COPA) task), which uses the same binary-choice format to elicit a prediction for either the cause or effect of a given story event.", "labels": [], "entities": []}, {"text": "While the Story Cloze Test involves predicting the ending of a story, COPA items focus specifically on commonsense knowledge related to identifying causal relations between sequences.", "labels": [], "entities": [{"text": "predicting the ending of a story", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.8591673374176025}]}, {"text": "The competitive approaches to narrative prediction evaluated by the Story Cloze Test largely involve neural networks trained to distinguish between correct and incorrect endings of stories, e.g.).", "labels": [], "entities": [{"text": "narrative prediction", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.7581271827220917}, {"text": "Story Cloze Test", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.7878564794858297}]}, {"text": "A neural network approach has yet to be applied to the related COPA task.", "labels": [], "entities": [{"text": "COPA task", "start_pos": 63, "end_pos": 72, "type": "TASK", "confidence": 0.6117244064807892}]}, {"text": "In the current paper, we initiate this investigation into these models for COPA.", "labels": [], "entities": []}, {"text": "In particular, we evaluate an encoder-decoder model that predicts the probability that a particular sequence follows another in a story.", "labels": [], "entities": []}, {"text": "Our experiments explore a few different variables for configuring this approach.", "labels": [], "entities": []}, {"text": "First, we examine how to extract temporally related sequence pairs provided as input to the model.", "labels": [], "entities": []}, {"text": "Second, we vary the use of feed-forward versus recurrent layers within the model.", "labels": [], "entities": []}, {"text": "Third, we assess different vector-based representations of the sequence pairs.", "labels": [], "entities": []}, {"text": "Finally, we compare our model using different narrative corpora for training, including the ROCStories corpus which was developed in conjunction with the Story Cloze Test.", "labels": [], "entities": [{"text": "ROCStories corpus", "start_pos": 92, "end_pos": 109, "type": "DATASET", "confidence": 0.8590152263641357}, {"text": "Story Cloze Test", "start_pos": 154, "end_pos": 170, "type": "DATASET", "confidence": 0.7889777421951294}]}, {"text": "Our results are presented in comparison to existing systems applied to COPA, which involve lexical cooccurrence statistics gathered from web corpora.", "labels": [], "entities": []}, {"text": "Our best-performing model achieves an accuracy of 66.2% on the COPA test set, which falls short of the current state-of-the-art of 71.2% (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9997099041938782}, {"text": "COPA test set", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9527917504310608}]}, {"text": "Interestingly, this best result utilizes the ROCStories for training, which is only a small fraction of the size of the datasets used in existing approaches.", "labels": [], "entities": [{"text": "ROCStories", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.6297534108161926}]}, {"text": "Applying our model to these larger datasets actually yields significantly worse performance, suggesting that the model is sensitive to the density of commonsense knowledge contained in its training set.", "labels": [], "entities": []}, {"text": "We conclude that this density is far more influential to COPA performance than just data quantity, and further success on the task will depend on methods for isolating implicit commonsense knowledge in text.", "labels": [], "entities": []}], "datasetContent": [{"text": "Gordon et al. found that the PMI approach trained on blog stories performed better on COPA than the same model trained on books in Project Gutenburg 6 , despite the much larger size of the latter.", "labels": [], "entities": [{"text": "COPA", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.8787444829940796}]}, {"text": "Beyond this, there has been limited exploration of the impact of different training datasets on COPA prediction, so we were motivated to examine this.", "labels": [], "entities": [{"text": "COPA prediction", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.9210624098777771}]}, {"text": "Thus, we applied the FFN encoderdecoder approach to the following datasets: Visual Storytelling (VIST): 50,200 fivesentence stories 7 authored through crowdsourcing in support of research on vision-to-language tasks.", "labels": [], "entities": [{"text": "Visual Storytelling (VIST)", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.6288845300674438}]}, {"text": "Participants were prompted to write a story from a sequence of photographs depicting salient \"storyable\" events.", "labels": [], "entities": []}, {"text": "CNN/DailyMail corpus: 312,085 bullet-item summaries 8 of news articles, which have been used for work on reading comprehension and summarization (.", "labels": [], "entities": [{"text": "CNN/DailyMail corpus", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.9435965269804001}, {"text": "summarization", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.9871037602424622}]}, {"text": "CMU Book/Movie Plot Summaries (CMU Plots): 58,862 plot summaries 9 from Wikipedia, which have been used for story modeling tasks like inferring relations between story characters.", "labels": [], "entities": [{"text": "CMU Book/Movie Plot Summaries (CMU Plots)", "start_pos": 0, "end_pos": 41, "type": "DATASET", "confidence": 0.8072442531585693}, {"text": "story modeling", "start_pos": 108, "end_pos": 122, "type": "TASK", "confidence": 0.7383658289909363}]}, {"text": "BookCorpus: 8,032 self-published fiction novels, a subset of the full corpus 10 of 11,000 books.", "labels": [], "entities": [{"text": "BookCorpus", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9570789933204651}]}, {"text": "We trained the FFN model with the bestperforming configuration from the ROCStories experiments (clause segments, N=4, bag-of-words input).", "labels": [], "entities": [{"text": "FFN", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.869086503982544}]}, {"text": "After determining that the lexicon used in the previous experiments included most of the words (93.5%) in the COPA development set, we re-used this same lexicon to avoid the inefficiency of assembling anew one for each separate corpus.", "labels": [], "entities": [{"text": "COPA development set", "start_pos": 110, "end_pos": 130, "type": "DATASET", "confidence": 0.880517840385437}]}, {"text": "We also trained a model on the initial 45,502 stories in the ROCStories (ROCStories-Half) to further analyze the impact of this dataset.", "labels": [], "entities": [{"text": "ROCStories (ROCStories-Half)", "start_pos": 61, "end_pos": 89, "type": "DATASET", "confidence": 0.8259181529283524}]}, {"text": "shows the results for these datasets compared alongside the ROCStories result from above (ROCStories-Full), listed in ascending order of the number of training pairs they contain.", "labels": [], "entities": [{"text": "ROCStories result", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.8263266682624817}]}, {"text": "As shown, none of the other datasets reach the level of accuracy of ROCStories-Full (66.2%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9995686411857605}, {"text": "ROCStories-Full", "start_pos": 68, "end_pos": 83, "type": "METRIC", "confidence": 0.47879984974861145}]}, {"text": "Even the model trained on only the initial half of this corpus outperforms the others (62.6%).", "labels": [], "entities": []}, {"text": "The next closest result is for the ClueWeb Pairs, which had 61.2% test accuracy despite containing 100 times more pairs than the ROCStories.", "labels": [], "entities": [{"text": "ClueWeb Pairs", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.909291684627533}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9797954559326172}, {"text": "ROCStories", "start_pos": 129, "end_pos": 139, "type": "DATASET", "confidence": 0.9338601231575012}]}, {"text": "The ClueWeb pairs obtained 71.2% accuracy when used in the CausalNet approach, so the encoder-decoder model is apparently not as effective in utilizing the causal knowledge contained in this dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9992130994796753}]}, {"text": "The larger Blog Stories and BookCorpus datasets also did not have much impact, despite that the Blog Stories obtained 65.2% accuracy in the PMI approach.", "labels": [], "entities": [{"text": "Blog Stories", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.9658731520175934}, {"text": "BookCorpus datasets", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.9227110147476196}, {"text": "Blog Stories", "start_pos": 96, "end_pos": 108, "type": "DATASET", "confidence": 0.9424215257167816}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9992458820343018}]}, {"text": "One speculative explanation for this is that our approach is highly dependent on the density of COPA-relevant knowledge contained in a dataset.", "labels": [], "entities": []}, {"text": "As mentioned above, authors of the ROCStories were instructed to emphasize the most obvious possibilities for 'what happens next' in prototypical scenarios.", "labels": [], "entities": [{"text": "ROCStories", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.6154171824455261}]}, {"text": "These expectations align with the correct COPA alternatives.", "labels": [], "entities": []}, {"text": "However, naturally occurring stories often focus on events that violate commonsense expectations, since these events make for more salient stories).", "labels": [], "entities": []}, {"text": "Thus, they may show greater diversity in 'what happens next' relative to the ROCStories.", "labels": [], "entities": [{"text": "ROCStories", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.9173259139060974}]}, {"text": "This diversity was seemingly more distracting for our encoder-decoder architecture than for the existing approaches.", "labels": [], "entities": []}, {"text": "Accordingly, despite all being related to narrative, the VIST, CNN/DailyMail, and CMU Plots datasets were also ineffective on the test set with regard to this model.", "labels": [], "entities": [{"text": "VIST", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.7197423577308655}, {"text": "CNN/DailyMail", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.8538419405619303}, {"text": "CMU Plots datasets", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.9038799007733663}]}], "tableCaptions": [{"text": " Table 2: Accuracy by segmentation unit and pair distance (N) for the FFN and RNN encoder-decoders trained on  ROCStories", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991051554679871}, {"text": "pair distance (N)", "start_pos": 44, "end_pos": 61, "type": "METRIC", "confidence": 0.9278364896774292}, {"text": "FFN", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.7664588689804077}, {"text": "ROCStories", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.5954841375350952}]}, {"text": " Table 3: Accuracy of FFN trained on ROCStories with  different input representations", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9955214262008667}, {"text": "FFN", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9142818450927734}]}, {"text": " Table 5: Accuracy of PMI and CausalNet trained on  ROCStories", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9982307553291321}, {"text": "PMI", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.7773939967155457}, {"text": "ROCStories", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.6147797703742981}]}, {"text": " Table 6: Accuracy of the FFN encoder-decoder on dif- ferent datasets", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9978231191635132}]}]}