{"title": [{"text": "ListOps: A Diagnostic Dataset for Latent Tree Learning", "labels": [], "entities": [{"text": "Latent Tree Learning", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.6642555991808573}]}], "abstractContent": [{"text": "Latent tree learning models learn to parse a sentence without syntactic supervision, and use that parse to build the sentence representation.", "labels": [], "entities": []}, {"text": "Existing work on such models has shown that, while they perform well on tasks like sentence classification, they do not learn grammars that conform to any plausible semantic or syntactic formalism (Williams et al., 2018a).", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.7274297773838043}]}, {"text": "Studying the parsing ability of such models in natural language can be challenging due to the inherent complexities of natural language, like having several valid parses fora single sentence.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9627330899238586}]}, {"text": "In this paper we introduce ListOps, a toy dataset created to study the parsing ability of latent tree models.", "labels": [], "entities": []}, {"text": "ListOps sequences are in the style of prefix arithmetic.", "labels": [], "entities": []}, {"text": "The dataset is designed to have a single correct parsing strategy that a system needs to learn to succeed at the task.", "labels": [], "entities": []}, {"text": "We show that the current leading latent tree models are unable to learn to parse and succeed at ListOps.", "labels": [], "entities": [{"text": "ListOps", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9042583107948303}]}, {"text": "These models achieve accuracies worse than purely sequential RNNs.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9637690186500549}]}], "introductionContent": [{"text": "Recent work on latent tree learning models) has introduced new methods of training tree-structured recurrent neural networks) without ground-truth parses.", "labels": [], "entities": []}, {"text": "These latent tree models learn to parse with indirect supervision from a downstream semantic task, like sentence classification.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.7429500669240952}]}, {"text": "They have been shown to perform well at sentence understanding tasks, like textual entailment and sentiment analysis, and they generally outperform their TreeRNN counterparts that use parses from conventional parsers.", "labels": [], "entities": [{"text": "sentence understanding", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7568454444408417}, {"text": "sentiment analysis", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.9018837511539459}]}, {"text": "Latent tree learning models lack direct syntactic supervision, so they are not being pushed to conform to expert-designed grammars, like the The parse is left-branching within each list, and each constituent is either a partial list, an integer, or the final closing bracket.", "labels": [], "entities": []}, {"text": "Penn Treebank (PTB;.", "labels": [], "entities": [{"text": "Penn Treebank (PTB;", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.9243331909179687}]}, {"text": "Theoretically then, they have the freedom to learn whichever grammar is best suited for the task at hand.", "labels": [], "entities": []}, {"text": "However, show that current latent tree learning models do not learn grammars that follow recognizable semantic or syntactic principles when trained on natural language inference.", "labels": [], "entities": []}, {"text": "Additionally, the learned grammars are not consistent across random restarts.", "labels": [], "entities": []}, {"text": "This begs the question, do these models fail to learn useful grammars because it is unnecessary for the task?", "labels": [], "entities": []}, {"text": "Or do they fail because they are incapable of learning to parse?", "labels": [], "entities": []}, {"text": "In this paper we introduce the ListOps datasets which is designed to address this second question.", "labels": [], "entities": [{"text": "ListOps datasets", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.9366495609283447}]}, {"text": "Since natural language is complex, there are often multiple valid parses fora single sentence.", "labels": [], "entities": []}, {"text": "Furthermore, as was shown in, using sensible grammars is not necessary to do well at some existing natural language datasets.", "labels": [], "entities": []}, {"text": "Since our primary objective is to study a system's ability to learn a correct parsing strategy, we build a toy dataset, ListOps, that primarily tests a system's parsing ability.", "labels": [], "entities": []}, {"text": "ListOps is in the style of prefix arithmetic; it is comprised of deeply nested lists of mathematical operations and a list of single-digit integers.", "labels": [], "entities": []}, {"text": "The ListOps sequences are generated with a reference parse, and this parse corresponds to the Truth: 7; Pred: 7 Truth: 7; Pred: 2", "labels": [], "entities": []}], "datasetContent": [{"text": "Description The ListOps examples are comprised of summary operations on lists of singledigit integers, written in prefix notation.", "labels": [], "entities": []}, {"text": "The full sequence has a corresponding solution which is also a single-digit integer, thus making it a tenway balanced classification problem.", "labels": [], "entities": []}, {"text": "For example, [MAX 2 9 [MIN 4 7 ] 0 ] has the solution 9.", "labels": [], "entities": [{"text": "MAX 2 9 [MIN 4 7 ] 0", "start_pos": 14, "end_pos": 34, "type": "DATASET", "confidence": 0.8686485290527344}]}, {"text": "Each operation has a corresponding closing square bracket that defines the list of numbers for the operation.", "labels": [], "entities": []}, {"text": "In this example, MIN operates on {4, 7}, while MAX operates on {2, 9, 4, 0}.", "labels": [], "entities": [{"text": "MAX", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8339528441429138}]}, {"text": "The correct parse for this example is shown in.", "labels": [], "entities": []}, {"text": "As with this example, the reference parses in ListOps are left-branching within each list.", "labels": [], "entities": []}, {"text": "If they were right-branching, the model would always have to maintain the entire list in memory.", "labels": [], "entities": []}, {"text": "This is because the summary statistic for each list is dependent on the type of operation, and the operation token appears first in prefix notation.", "labels": [], "entities": []}, {"text": "Furthermore, we select a small and easy operation space to lower output set difficulty.", "labels": [], "entities": []}, {"text": "The operations that appear in ListOps are: \u2022 MAX: the largest value of the given list.", "labels": [], "entities": [{"text": "MAX", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9589599967002869}]}, {"text": "For the list {8, 12, 6, 3}, 12 is the MAX.", "labels": [], "entities": [{"text": "MAX", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.6105074286460876}]}, {"text": "\u2022 MIN: the smallest value of the given list.", "labels": [], "entities": [{"text": "MIN", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9639600515365601}]}, {"text": "For the list {8, 12, 6, 3}, 3 is the MIN.", "labels": [], "entities": [{"text": "MIN", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.7470188140869141}]}, {"text": "\u2022 MED: the median value of the given list.", "labels": [], "entities": [{"text": "MED", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9985823631286621}]}, {"text": "For the list {8, 12, 6, 3}, 7 is the MED.", "labels": [], "entities": [{"text": "MED", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.8284991979598999}]}, {"text": "\u2022 SUM MOD (SM): the sum of the items in the list, constrained to a single digit by the use of the modulo-10 operator.", "labels": [], "entities": [{"text": "SUM MOD (SM)", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.8348507642745971}]}, {"text": "For the list {8, 12, 6, 3}, 9 is the SM.", "labels": [], "entities": []}, {"text": "ListOps is constructed such that it is trivially easy to solve if a model has access to the groundtruth parses.", "labels": [], "entities": []}, {"text": "However, if a model does not have the parses, or is unable to learn to parse correctly, it may have to maintain a large stack of information to arrive at the correct solution.", "labels": [], "entities": []}, {"text": "This is particularly true as the sequences become long and have many nested lists.", "labels": [], "entities": []}, {"text": "Efficacy We take an empirical approach to determine the efficacy of the ListOps dataset to test parsing capability.", "labels": [], "entities": [{"text": "Efficacy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.949397623538971}, {"text": "ListOps dataset", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9442689120769501}, {"text": "parsing", "start_pos": 96, "end_pos": 103, "type": "TASK", "confidence": 0.9654300212860107}]}, {"text": "ListOps should be trivial to solve if a model is given the ground-truth parses.", "labels": [], "entities": []}, {"text": "Therefore, a tree-structured model that is provided with the parses should be able to achieve near 100% accuracy on the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9989423155784607}]}, {"text": "So, to establish the upper-bound and solvability of the dataset, we use a TreeLSTM as one of our baselines.", "labels": [], "entities": []}, {"text": "Conversely, if the ListOps dataset is adequately difficult, then a strong sequential model should not perform well on the dataset.", "labels": [], "entities": [{"text": "ListOps dataset", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.952768474817276}]}, {"text": "We use an LSTM) as our sequential baseline.", "labels": [], "entities": []}, {"text": "We run extensive experiments on the ListOps dataset to ensure that the TreeLSTM does consistently succeed while the LSTM fails.", "labels": [], "entities": [{"text": "ListOps dataset", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.944816380739212}, {"text": "TreeLSTM", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9187109470367432}]}, {"text": "We tune the model size, learning rate, L2 regularization, and decay of learning rate (the learning rate is lowered at every epoch when there has been no gain).", "labels": [], "entities": [{"text": "decay of learning rate", "start_pos": 62, "end_pos": 84, "type": "METRIC", "confidence": 0.9000963717699051}]}, {"text": "We require that the TreeLSTM model does well at a relatively low model size.", "labels": [], "entities": []}, {"text": "We further ensure that the LSTM, at an order of magnitude greater model size, is still unable to solve ListOps.", "labels": [], "entities": [{"text": "ListOps", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9152638912200928}]}, {"text": "Therefore, we build the dataset and establish its effectiveness as a diagnostic task by maximizing this RNN-TreeRNN gap.", "labels": [], "entities": []}, {"text": "Theoretically, this RNN-TreeRNN gap arises because an RNN of fixed size does not have the capacity to store all the necessary information.", "labels": [], "entities": []}, {"text": "More concretely, we know that each of the operations in ListOps can be computed by passing over the list of integers with a constant amount of memory.", "labels": [], "entities": []}, {"text": "For example, to compute the MAX, the system only needs to remember the largest number it has seen in the operation's list.", "labels": [], "entities": [{"text": "MAX", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.3795492947101593}, {"text": "remember", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9730101823806763}]}, {"text": "As an RNN reads a sequence, if it is in the middle of the sequence, it will have read many operations without closed parentheses, i.e. without terminating the lists.", "labels": [], "entities": []}, {"text": "Therefore, it has to maintain the state of all the open operations it has read.", "labels": [], "entities": []}, {"text": "So the amount of information the RNN has to maintain grows linearly with tree depth.", "labels": [], "entities": []}, {"text": "As a result, once the trees are deep enough, an RNN with a fixed-size memory cannot effectively store and retrieve all the necessary information.", "labels": [], "entities": []}, {"text": "For a TreeRNN, every constituent in ListOps is either a partial list, an integer, or the final closing bracket.", "labels": [], "entities": []}, {"text": "For example, in, the first constituent, ([MAX, 2, 9), is a partial list.", "labels": [], "entities": []}, {"text": "So, the amount of information the TreeLSTM has to store at any given node is no greater than the small amount needed to process one list.", "labels": [], "entities": []}, {"text": "Unlike with an RNN, this small amount of information at each node does not grow with tree depth.", "labels": [], "entities": []}, {"text": "Consequently, TreeRNNs can achieve high accuracy at ListOps with very low model size, while RNNs require higher capacity to do well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9975899457931519}, {"text": "ListOps", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.9273288249969482}]}, {"text": "Generation The two primary variables that determine the difficulty of the ListOps dataset are tree depth and the function space of mathematical operations.", "labels": [], "entities": [{"text": "ListOps dataset", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.9081599116325378}]}, {"text": "We found tree depth to bean essential variable in stressing model performance, and in maximizing the RNN-TreeRNN gap.", "labels": [], "entities": []}, {"text": "While creating ListOps, we clearly observe that with increasing recursion in the dataset the performance of sequential models falls.", "labels": [], "entities": []}, {"text": "shows the distribution of tree depths in the ListOps dataset; the average tree depth is 9.6.", "labels": [], "entities": [{"text": "ListOps dataset", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.9823412895202637}]}, {"text": "As discussed previously, since we are concerned with a model's ability to learn to parse, and not its ability to approximate mathematical operations, we choose a minimal number of operations (MAX, MIN, MED, SM).", "labels": [], "entities": [{"text": "MED", "start_pos": 202, "end_pos": 205, "type": "METRIC", "confidence": 0.7642231583595276}]}, {"text": "In our explorations, we find that these easy-to-compute operations yield bigger RNN-TreeRNN gaps than operations like multiplication.", "labels": [], "entities": []}, {"text": "The ListOps dataset used in this paper has 90k training examples and 10k test examples.", "labels": [], "entities": [{"text": "ListOps dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9188727140426636}]}, {"text": "During data generation, the operations are selected at random, and their frequency is balanced in the final dataset.", "labels": [], "entities": []}, {"text": "We wrote a simple Python script to generate the ListOps data.", "labels": [], "entities": [{"text": "ListOps data", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9364685118198395}]}, {"text": "Variables such as maximum tree-depth, as well as number and kind of operations, can be changed to generate variations on ListOps.", "labels": [], "entities": []}, {"text": "One might want to increase the average tree depth if a model with much larger hidden states is being tested.", "labels": [], "entities": []}, {"text": "With a very large model size, an RNN, in principle, can succeed at the ListOps dataset presented in this paper.", "labels": [], "entities": [{"text": "ListOps dataset", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.9287624657154083}]}, {"text": "The dataset and data generation script are available on GitHub.", "labels": [], "entities": [{"text": "data generation", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.6958603262901306}]}], "tableCaptions": [{"text": " Table 1: SNLI shows test set results of models  on the Stanford Natural Language Inference Cor- pus, a sentence classification task. We see that the  latent tree learning models outperform the super- vised TreeLSTM model. However, on ListOps,  RL-SPINN and ST-Gumbel have worse perfor- mance accuracy than the LSTM baseline.", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 104, "end_pos": 132, "type": "TASK", "confidence": 0.7879903713862101}, {"text": "perfor- mance accuracy", "start_pos": 279, "end_pos": 301, "type": "METRIC", "confidence": 0.6826198101043701}]}, {"text": " Table 2: Accuracy shows accuracy across four runs  of the models (expressed as mean, standard devia- tion, and maximum). Self F1 shows how well each  of these four model runs agrees in its parsing de- cisions with the other three.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990756511688232}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9993724226951599}, {"text": "mean", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9640726447105408}, {"text": "Self F1", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.45294488966464996}]}, {"text": " Table 3: F1 wrt. shows F1 scores on ListOps with  respect to left-branching (LB), right-branching  (RB), and ground-truth (GT) trees. Avg. Depth  shows the average across sentences of the average  depth of each token in its tree.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9792598485946655}, {"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9969989061355591}]}]}