{"title": [{"text": "Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing", "labels": [], "entities": [{"text": "Knowledge Graph Augmented Neural Networks", "start_pos": 26, "end_pos": 67, "type": "TASK", "confidence": 0.78204345703125}]}], "abstractContent": [{"text": "Machine Learning has been the quintessen-tial solution for many AI problems, but learning models are heavily dependent on specific training data.", "labels": [], "entities": [{"text": "Machine Learning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8145214915275574}]}, {"text": "Some learning models can be incorporated with prior knowledge using a Bayesian setup, but these learning models do not have the ability to access any organized world knowledge on demand.", "labels": [], "entities": []}, {"text": "In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism.", "labels": [], "entities": []}, {"text": "We introduce a convolution-based model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space.", "labels": [], "entities": []}, {"text": "We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task.", "labels": [], "entities": []}, {"text": "Using this method we show significant improvement in performance for text classification with 20Newsgroups (News20) & DBPedia datasets, and natural language inference with Stanford Natural Language Inference (SNLI) dataset.", "labels": [], "entities": [{"text": "text classification", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7977655231952667}, {"text": "20Newsgroups (News20) & DBPedia datasets", "start_pos": 94, "end_pos": 134, "type": "DATASET", "confidence": 0.8583851626941136}, {"text": "Stanford Natural Language Inference (SNLI) dataset", "start_pos": 172, "end_pos": 222, "type": "DATASET", "confidence": 0.4804597422480583}]}, {"text": "We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a knowledge base.", "labels": [], "entities": []}], "introductionContent": [{"text": "Today, machine learning is centered around algorithms that can be trained on available taskspecific labeled and unlabeled training samples.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7895159125328064}]}, {"text": "Although learning paradigms like Transfer Learning attempt to incorporate * equal contribution \u2020 Main work done during internship at Accenture Technology Labs knowledge from one task into another, these techniques are limited in scalability and are specific to the task at hand.", "labels": [], "entities": [{"text": "Transfer Learning", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9437888860702515}, {"text": "Accenture Technology Labs", "start_pos": 133, "end_pos": 158, "type": "DATASET", "confidence": 0.9335048198699951}]}, {"text": "On the other hand, humans have the intrinsic ability to elicit required past knowledge from the world on demand and infuse it with newly learned concepts to solve problems.", "labels": [], "entities": []}, {"text": "The question that we address in this paper is the following: Is it possible to develop learning models that can be trained in away that it is able to infuse a general body of world knowledge for prediction apart from learning based on training data?", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were designed to analyze whether a deep learning model is being improved when it has access to KG facts from a relevant source.", "labels": [], "entities": []}, {"text": "The selection of knowledge graph has to be pertinent to the task at hand, as currently there is no single knowledge base that contains multiple kinds of information and can cater to all tasks.", "labels": [], "entities": []}, {"text": "We illustrate with results that the performance of a deep learning model improves when it has access to relevant facts.", "labels": [], "entities": []}, {"text": "We also illustrate that as the model learns faster with access to knowledge bases, we can train deep learning models with substantially less training data, without compromising on the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9980568289756775}]}, {"text": "In the subsequent section we briefly describe the datasets and associated Knowledge Bases used.", "labels": [], "entities": []}, {"text": "In our experiments, we have mainly used the popular text classification dataset 20Newsgroups and the Natural Language Inference dataset, Stanford Natural Language Inference (SNLI) corpus.", "labels": [], "entities": [{"text": "text classification dataset 20Newsgroups", "start_pos": 52, "end_pos": 92, "type": "TASK", "confidence": 0.7633393704891205}, {"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 137, "end_pos": 186, "type": "DATASET", "confidence": 0.7773851379752159}]}, {"text": "We have also done experiments on DBPedia ontology classification dataset 1 , with a very strong baseline.", "labels": [], "entities": [{"text": "DBPedia ontology classification dataset 1", "start_pos": 33, "end_pos": 74, "type": "DATASET", "confidence": 0.844118618965149}]}, {"text": "These datasets are chosen as they share domain knowledge with two most popular knowledge bases, Freebase (FB15k) and WordNet (WN18) (, which can help in inference tasks like SNLI.", "labels": [], "entities": [{"text": "Freebase (FB15k", "start_pos": 96, "end_pos": 111, "type": "DATASET", "confidence": 0.7844614783922831}, {"text": "WordNet (WN18)", "start_pos": 117, "end_pos": 131, "type": "DATASET", "confidence": 0.8867773413658142}]}, {"text": "Both the knowledge bases are directed graphs, due to fewer number of relations WN18 the entities are more likely to be connected using the same type of relations.", "labels": [], "entities": []}, {"text": "For experiments relating to both the datasets 20Newsgroups & SNLI we used the standard LSTM as the classification module.", "labels": [], "entities": [{"text": "20Newsgroups", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9057812690734863}, {"text": "SNLI", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.75168377161026}]}, {"text": "As iterated earlier, our KG based fact retrieval is independent of the base model used.", "labels": [], "entities": [{"text": "fact retrieval", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.6095567047595978}]}, {"text": "We show improvement in performance using the proposed models by KG fact retrieval.", "labels": [], "entities": [{"text": "KG fact retrieval", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.6458245317141215}]}, {"text": "We use classification accuracy of the test set as our evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.768286406993866}]}, {"text": "All experiments were carried on a Dell Precision Tower 7910 server with Quadro M5000 GPU with 8 GB of memory.", "labels": [], "entities": [{"text": "Dell Precision Tower 7910 server", "start_pos": 34, "end_pos": 66, "type": "DATASET", "confidence": 0.923446536064148}]}, {"text": "The models were trained using the Adam's Optimizer () in a stochastic gradient descent (Bottou, 2012) fashion.", "labels": [], "entities": []}, {"text": "The models were implemented using TensorFlow ().", "labels": [], "entities": []}, {"text": "The relevant hyperparameters are listed in.", "labels": [], "entities": []}, {"text": "The word embeddings for the experiments were obtained using the pre-trained GloVe () 2 vectors.", "labels": [], "entities": []}, {"text": "For words missing in the pre-trained vectors, the local GloVe vectors which was trained on the corresponding dataset was used.", "labels": [], "entities": []}, {"text": "shows the results of test accuracy of the various methods proposed on the datasets News20 & SNLI.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9951592087745667}, {"text": "News20 & SNLI", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.866370956103007}]}, {"text": "We observe that incorporation of KG facts using the basic vanilla model improves the performance slightly, as the retrieval model was  not getting trained effectively.", "labels": [], "entities": []}, {"text": "The convolutionbased model shows significant improvement over the normal LSTM classification.", "labels": [], "entities": []}, {"text": "While tuning the parameters of the convolution for clustered entities/relations it was observed that smaller stride length and longer max-pool window improved performance.", "labels": [], "entities": []}, {"text": "For News20 dataset we show an improvement of almost 3% and for SNLI an improvement of almost 5%.", "labels": [], "entities": [{"text": "News20 dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9924025535583496}, {"text": "SNLI", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8327631950378418}]}, {"text": "The work is motivated more from the perspective of whether incorporation of world knowledge will improve any deep learning model rather than beating the state-of-the-art performance.", "labels": [], "entities": []}, {"text": "Although LSTM is used to encode the input for the model as well as the retrieval vector, as mentioned earlier, these two modules need not be same.", "labels": [], "entities": []}, {"text": "For encoding the input any complex state-of-the-art model can be used.", "labels": [], "entities": []}, {"text": "LSTM has also been used to generate the retrieval vector.", "labels": [], "entities": []}, {"text": "For DBPedia ontology classification dataset, we have used a strong baseline of 98.6%, and after augmenting it with KG (Freebase) using convolution based model we saw an improvement of \u223c0.2%.", "labels": [], "entities": [{"text": "DBPedia ontology classification dataset", "start_pos": 4, "end_pos": 43, "type": "DATASET", "confidence": 0.7867897599935532}]}, {"text": "As the baseline is stronger, the improvement quantum has decreased.", "labels": [], "entities": [{"text": "improvement", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9678100943565369}]}, {"text": "This is quite intuitive as complex models are selfsufficient in learning from the data by itself and therefore the room available for further improvement is relatively less.", "labels": [], "entities": []}, {"text": "The improvement as observed in the experiments is significant in weaker learning models, however it is also capable of improving stronger baselines as is evident from the results of DBPedia dataset.", "labels": [], "entities": [{"text": "DBPedia dataset", "start_pos": 182, "end_pos": 197, "type": "DATASET", "confidence": 0.9604876637458801}]}, {"text": "We hypothesized that as Knowledge Graph is feeding more information to the model, we can achieve better performance with less training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Hyper-parameters which were used in experi- ments for News20 & SNLI datasets", "labels": [], "entities": [{"text": "News20 & SNLI datasets", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.8656602650880814}]}, {"text": " Table 3: Test accuracy of approaches in News20 using  FB15K & SNLI datasets using WN18", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9831647276878357}, {"text": "News20", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.9851146936416626}, {"text": "FB15K", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9650776982307434}, {"text": "SNLI datasets", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9199548065662384}, {"text": "WN18", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.8784735202789307}]}]}