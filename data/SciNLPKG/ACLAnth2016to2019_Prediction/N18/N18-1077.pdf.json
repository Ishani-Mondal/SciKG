{"title": [{"text": "Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource", "labels": [], "entities": [{"text": "Improving Temporal Relation Extraction", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.9167568534612656}]}], "abstractContent": [{"text": "Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language.", "labels": [], "entities": [{"text": "Extracting temporal relations (before, after, overlapping", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.8475012381871542}]}, {"text": "We argue that this task would gain from the availability of a resource that provides prior knowledge in the form of the temporal order that events usually follow.", "labels": [], "entities": []}, {"text": "This paper develops such a resource-a probabilistic knowledge base acquired in the news domain-by extracting temporal relations between events from the New York Times (NYT) articles over a 20-year span (1987-2007).", "labels": [], "entities": [{"text": "extracting temporal relations between events from the New York Times (NYT) articles", "start_pos": 98, "end_pos": 181, "type": "TASK", "confidence": 0.6972509665148598}]}, {"text": "We show that existing temporal extraction systems can be improved via this resource.", "labels": [], "entities": [{"text": "temporal extraction", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7262294292449951}]}, {"text": "As a byproduct, we also show that interesting statistics can be retrieved from this resource, which can potentially benefit other time-aware tasks.", "labels": [], "entities": []}, {"text": "The proposed system and resource are both publicly available 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Time is an important dimension of knowledge representation.", "labels": [], "entities": [{"text": "knowledge representation", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.7893320322036743}]}, {"text": "In natural language, temporal information is often expressed as relations between events.", "labels": [], "entities": []}, {"text": "Reasoning over these relations can help figuring out when things happened, estimating how long things take, and summarizing the timeline of a series of events.", "labels": [], "entities": []}, {"text": "Several recent SemEval workshops area good showcase of the importance of this topic.", "labels": [], "entities": [{"text": "SemEval", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.9525584578514099}]}, {"text": "One of the challenges in temporal relation extraction is that it requires high-level prior knowledge of the temporal order that events usually follow.", "labels": [], "entities": [{"text": "temporal relation extraction", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.7066380381584167}]}, {"text": "In Example 1, we have deleted events from several snippets from CNN, so that we cannot use our prior knowledge of those events.", "labels": [], "entities": [{"text": "CNN", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.919410228729248}]}, {"text": "We are also told that e1 and e2 have the same tense, and e3 and e4 have the same tense, so we cannot resort to their tenses to tell which one happens earlier.", "labels": [], "entities": []}, {"text": "As a result, it is very difficult even for humans to figure out the temporal relations (referred to as \"TempRels\" hereafter) between those events.", "labels": [], "entities": []}, {"text": "This is because rich temporal information is encoded in the events' names, and this often plays an indispensable role in making our decisions.", "labels": [], "entities": []}, {"text": "In the first paragraph of Example 1, it is difficult to understand what really happened without the actual event verbs; let alone the TempRels between them.", "labels": [], "entities": []}, {"text": "In the second paragraph, things are even more interesting: if we had e3:dislike and e4:stop, then we would know easily that \"I dislike\" occurs after \"they stop the column\".", "labels": [], "entities": [{"text": "stop", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9462571144104004}]}, {"text": "However, if we had e3:ask and e4:help, then the relation between e3 and e4 is now reversed and e3 is before e4.", "labels": [], "entities": []}, {"text": "We are in need of the event names to determine the TempRels; however, we do not have them in Example 1.", "labels": [], "entities": [{"text": "TempRels", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8558722734451294}]}, {"text": "In Example 2, where we show the complete sentences, the task has become much easier for humans due to our prior knowledge, namely, that explosion usually leads to casualties and that people usually ask before they get help.", "labels": [], "entities": []}, {"text": "Motivated by these examples (which are in fact very common), we believe in the importance of such a prior knowledge in determining TempRels between events.", "labels": [], "entities": [{"text": "TempRels between events", "start_pos": 131, "end_pos": 154, "type": "TASK", "confidence": 0.8148687283198038}]}, {"text": "Example 1: Difficulty in understanding TempRels when event content is missing.", "labels": [], "entities": []}, {"text": "Note that e1 and e2 have the same tense, and e3 and e4 have the same tense.", "labels": [], "entities": []}, {"text": "More than 10 people have (e1:died), police said.", "labels": [], "entities": []}, {"text": "A car (e2:exploded) on Friday in the middle of a group of men playing volleyball.", "labels": [], "entities": []}, {"text": "The first thing I (e3:ask) is that they (e4:help) writing this column.", "labels": [], "entities": []}, {"text": "However, most existing systems only make use of rather local features of these events, which cannot represent the prior knowledge humans have: TEMPROB is a unique source of information of the temporal order that events usually follow.", "labels": [], "entities": [{"text": "TEMPROB", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.8722251653671265}]}, {"text": "The probabilities below do not add up to 100% because less frequent relations are omitted.", "labels": [], "entities": []}, {"text": "The word sense numbers are not shown here for convenience.", "labels": [], "entities": []}, {"text": "about these events and their \"typical\" order.", "labels": [], "entities": []}, {"text": "As a result, existing systems almost always attempt to solve the situations shown in Example 1, even when they are actually presented with input as in Example 2.", "labels": [], "entities": []}, {"text": "The first contribution of this work is thus the construction of such a resource in the form of a probabilistic knowledge base, constructed from a large New York Times (NYT) corpus.", "labels": [], "entities": [{"text": "NYT) corpus", "start_pos": 168, "end_pos": 179, "type": "DATASET", "confidence": 0.7910078763961792}]}, {"text": "We hereafter name our resource TEMporal relation PRObabilistic knowledge Base (TEMPROB), which can potentially benefit many time-aware tasks.", "labels": [], "entities": []}, {"text": "A few example entries of TEMPROB are shown in.", "labels": [], "entities": [{"text": "TEMPROB", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.5503749847412109}]}, {"text": "Second, we show that existing TempRel extraction systems can be improved using TEMPROB, either in a local method or in a global method (explained later), by a significant margin in performance on the benchmark TimeBank-Dense dataset ).", "labels": [], "entities": [{"text": "TempRel extraction", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7498387694358826}, {"text": "TEMPROB", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9111157655715942}, {"text": "TimeBank-Dense dataset", "start_pos": 210, "end_pos": 232, "type": "DATASET", "confidence": 0.9584693610668182}]}, {"text": "Example 2: The original sentences in Example 1.", "labels": [], "entities": []}, {"text": "More than 10 people have (e1:died), police said.", "labels": [], "entities": []}, {"text": "A car (e2:exploded) on Friday in the middle of a group of men playing volleyball.", "labels": [], "entities": []}, {"text": "The first thing I (e3:ask) is that they (e4:help) writing this column.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides a literature review of TempRels extraction in NLP.", "labels": [], "entities": [{"text": "TempRels extraction", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.9246762692928314}]}, {"text": "Section 3 describes in detail the construction of TEMPROB.", "labels": [], "entities": [{"text": "TEMPROB", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.44646111130714417}]}, {"text": "4, we show that TEMPROB can be used in existing TempRels extraction systems and lead to significant improvement.", "labels": [], "entities": [{"text": "TEMPROB", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.974601686000824}, {"text": "TempRels extraction", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7374399304389954}]}, {"text": "Finally, we conclude in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the above, we have explained the construction of TEMPROB and shown some interesting examples from it, which were meant to visualize its correctness.", "labels": [], "entities": [{"text": "TEMPROB", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.6412177085876465}]}, {"text": "In this section, we first quantify the correctness of the prior obtained in TEMPROB, and Figure 1: Top events that most frequently precede or follow \"investigate\", \"bomb\", \"mourn\", or \"sentence\" in time, sorted by their conditional probabilities in . Word senses have been disambiguated and the \"bomb\" and \"sentence\" here are their verb meanings.", "labels": [], "entities": [{"text": "TEMPROB", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.5931976437568665}]}, {"text": "There are some possible errors (e.g., report is T-Before bomb) and some unclear pairs (e.g., know is T-Before investigate and play is T-After mourn), but overall the event sequences discovered here are reasonable.", "labels": [], "entities": []}, {"text": "More examples can be found in the appendix.", "labels": [], "entities": []}, {"text": "then show TEMPROB can be used to improve existing TempRel extraction systems.", "labels": [], "entities": [{"text": "TEMPROB", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.846243143081665}, {"text": "TempRel extraction", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7111418545246124}]}], "tableCaptions": [{"text": " Table 1: TEMPROB is a unique source of informa- tion of the temporal order that events usually fol- low. The probabilities below do not add up to 100%  because less frequent relations are omitted. The word  sense numbers are not shown here for convenience.", "labels": [], "entities": [{"text": "TEMPROB", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9311560392379761}]}, {"text": " Table 2: Several extreme cases from TEMPROB,  where some event is almost always labeled to be T- Before or T-After throughout the NYT corpus. By \"ex- treme\", we mean that either the probability of T-Before  or T-After is larger than 90%. The upper part of the  table shows the pairs that are both P-Before and T- Before, while the lower part shows the pairs that are  P-Before but T-After. In TEMPROB, there are about  7K event pairs being extreme cases.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 131, "end_pos": 141, "type": "DATASET", "confidence": 0.9670394361019135}]}, {"text": " Table 3: Validating \u03b7 b and \u03b7 a from TEMPROB based  on the T-Before and T-After examples in TBDense.  Performances are decomposed into same sentence ex- amples (Dist=0) and contiguous sentence examples  (Dist=1). A larger threshold leads to a higher precision,  so \u03b7 b and \u03b7 a indeed represent a notion of confidence.", "labels": [], "entities": [{"text": "TBDense", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.9083967804908752}, {"text": "precision", "start_pos": 251, "end_pos": 260, "type": "METRIC", "confidence": 0.9964631199836731}]}, {"text": " Table 4: Further justification of \u03b7 b and \u03b7 a from  TEMPROB on the EventCausality dataset. The  thresholding predictor from", "labels": [], "entities": [{"text": "EventCausality dataset", "start_pos": 68, "end_pos": 90, "type": "DATASET", "confidence": 0.8741770386695862}]}, {"text": " Table 5: Using prior distributions derived from  TEMPROB as features in an example local method.  Incorporating \u03b7 b to the original feature set already  yields better performance. By using the full set of prior  distributions, {f r } r\u2208R , the final system improves the  original in almost all metrics, and the improvement is  statistically significant with p<0.005 per the McNe- mar's test.", "labels": [], "entities": [{"text": "McNe- mar's test", "start_pos": 375, "end_pos": 391, "type": "DATASET", "confidence": 0.6711150407791138}]}, {"text": " Table 6: Regularizing global methods by the prior  distribution derived from TEMPROB. The \"+\"  means adding a component on top of its preceding line.  F aware is the temporal awareness F-score, another eval- uation metric used in TempEval3. The baseline sys- tem is to use (unregularized) ILP on top of the original  system in Table 5. System 3 is the proposed. Per the  McNemar's test, System 3 is significantly better than  System 1 with p<0.0005.", "labels": [], "entities": [{"text": "F aware", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.9770857989788055}, {"text": "temporal awareness F-score", "start_pos": 167, "end_pos": 193, "type": "METRIC", "confidence": 0.7085446317990621}, {"text": "McNemar's test", "start_pos": 372, "end_pos": 386, "type": "DATASET", "confidence": 0.8809983332951864}]}, {"text": " Table 7: Label-wise performance improvement of  System 3 over System 1 in", "labels": [], "entities": []}, {"text": " Table 6. We can see that  incorporating TEMPROB improves the recall of before  and after, and improves the precision of all labels, with  a slight drop in the recall of vague.", "labels": [], "entities": [{"text": "TEMPROB", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9713807106018066}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9991729855537415}, {"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9992280006408691}, {"text": "recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9994408488273621}]}, {"text": " Table 8: Comparison of the proposed TempRel ex- traction method with two best-so-far systems us- ing two metrics. Since TEMPROB is only on SRL  verb events, Partial TBDense is the focus of our work,  where we can see significant improvement brought by  simply using the prior knowledge from TEMPROB. Per  the McNemar's test, Line 3 is better than Line 2 with  p<0.0005. For interested readers, we also naively aug- mented the proposed method to the complete TBDense  and show state-of-the-art performance on it.", "labels": [], "entities": [{"text": "SRL  verb events", "start_pos": 140, "end_pos": 156, "type": "TASK", "confidence": 0.660825252532959}, {"text": "McNemar's test", "start_pos": 310, "end_pos": 324, "type": "DATASET", "confidence": 0.8335458238919576}]}]}