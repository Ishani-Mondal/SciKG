{"title": [], "abstractContent": [{"text": "We introduce a simple method for extracting non-arbitrary form-meaning representations from a collection of semantic vectors.", "labels": [], "entities": []}, {"text": "We treat the problem as one of feature selection fora model trained to predict word vectors from subword features.", "labels": [], "entities": []}, {"text": "We apply this model to the problem of automatically discovering phonesthemes, which are submorphemic sound clusters that appear in words with similar meaning.", "labels": [], "entities": []}, {"text": "Many of our model-predicted phonesthemes overlap with those proposed in the linguistics literature, and we validate our approach with human judgments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Linguists have long held that language is arbitrary, or that a word's phonetic and orthographic forms have no relation to its meaning (.", "labels": [], "entities": []}, {"text": "For example, there is nothing about an apple that suggests that apple is the proper word for it-this link between meaning and the representation in language is arbitrary.", "labels": [], "entities": []}, {"text": "Arbitrariness is a defining feature of human language, and it is a key component of the design features of language proposed by.", "labels": [], "entities": [{"text": "Arbitrariness", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9813517928123474}]}, {"text": "Despite this, work over the last decades has revealed several exceptions to the arbitrariness of language.", "labels": [], "entities": []}, {"text": "One such exception is iconicity, where the form of a word directly resembles its meaning.", "labels": [], "entities": []}, {"text": "For example, showed that speakers tend to associate vowels with high acoustic frequency with smaller objects, while vowels with low acoustic frequency are associated with larger objects.", "labels": [], "entities": []}, {"text": "In this case, speakers make a link between the phonetic form of a word and its perceived meaning because of an innate belief that smaller entities emit higher-frequency vowels while larger entities tend to emit low-frequency vowels.", "labels": [], "entities": []}, {"text": "Similarly, and observed a non-arbitrary connection between the shapes of objects and speech sounds.", "labels": [], "entities": []}, {"text": "American college undergraduates and Tamil speakers were presented with a jagged shape and a rounded shape and asked which is \"kiki\" and which is \"bouba\".", "labels": [], "entities": []}, {"text": "In both groups, 95% to 98% selected the jagged shape as \"kiki\" and the rounded shape as \"bouba\", demonstrating that the human brain connects sounds to shapes in a consistent way.", "labels": [], "entities": []}, {"text": "D' posits that the rounded shape is commonly named \"bouba\" since the mouth forms a rounded shape in producing the word, whereas pronouncing \"kiki\" requires a tighter, more angular mouth shape that seems more apt for the jagged object.", "labels": [], "entities": []}, {"text": "In this case, there is a strong, non-arbitrary link between the articulatory properties of the sound and their perceived meaning.", "labels": [], "entities": []}, {"text": "Phonesthemes are another exception to the arbitrariness of language.", "labels": [], "entities": []}, {"text": "Phonesthemes are noncompositional, submorphemic phonetic units that consistently occur in words with similar meanings.", "labels": [], "entities": []}, {"text": "For example, the word-initial gl-, occurs at the beginning of many English words relating to light or vision, like glint, glitter, gleam, glamour, etc.).", "labels": [], "entities": []}, {"text": "The work of includes a compilation of 46 phonesthemes proposed by linguists.", "labels": [], "entities": []}, {"text": "There is a body of previous work suggesting that phonesthemes are units in the mental lexicon of native speakers.", "labels": [], "entities": []}, {"text": "For example, the work of Hutchins (1998), uses priming experiments and other methods from psycholinguistics to demonstrate that phonesthemes significantly affect native speaker reaction times in a range of language processing tasks.", "labels": [], "entities": []}, {"text": "In another line of work, and verify phonesthemes by analyzing whether the words containing a given phonestheme are more semantically similar than expected by chance, where se-mantic similarity is derived from a distributional semantic model.", "labels": [], "entities": []}, {"text": "While there has been much work in verifying previously proposed phonesthemes, there has been little work on automatically discovering new ones.", "labels": [], "entities": []}, {"text": "In this work, our goal is to identify the likely phonesthemes of a language from a collection of semantic vectors.", "labels": [], "entities": []}, {"text": "We do this by identifying the character or phoneme sequences that are predictive of word meaning by training a model to predict word vectors from subword features.", "labels": [], "entities": [{"text": "predictive of word meaning", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.6776617094874382}]}, {"text": "Then, we use standard feature selection techniques to find a subset of features that best predict the vectors; this subset of features contains the model-predicted phonesthemes.", "labels": [], "entities": []}, {"text": "Lastly, we validate the model-predicted English phonesthemes with human judgments and also find that many of our predicted phonesthemes overlap with those documented in previous work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The candidate phonesthemes considered by the model are the word-initial phoneme bigram sequences that occur more than five times in our set of phonemicized vectors; we set a frequency threshold for feature inclusion since rare prefixes are unlikely to carry meaning.", "labels": [], "entities": []}, {"text": "Each word's feature vector is a one-hot encoding of its bigram phoneme prefix.", "labels": [], "entities": []}, {"text": "We choose to focus on wordinitial bigrams since the bulk of prior work in linguistics has also focused on phonesthemes in this position.", "labels": [], "entities": []}, {"text": "However, our method easily extends to larger subword units (e.g., trigrams), candidate phonesthemes within or at the end of a word, even other languages; we leave analysis of phonesthemes of other sizes, in different positions, and of different languages for future work.", "labels": [], "entities": []}, {"text": "We train our two-stage model on the phonemicized vectors; the features that are assigned a nonzero weight are our model-predicted phonesthemes.", "labels": [], "entities": []}, {"text": "The features of our morpheme-level model are binary indicator features corresponding to 181 different morphemes extracted from the CELEX2 database.", "labels": [], "entities": [{"text": "CELEX2 database", "start_pos": 131, "end_pos": 146, "type": "DATASET", "confidence": 0.9740680754184723}]}, {"text": "In total, our phonestheme extraction model considers 307 candidate phonesthemes; tuning the regularization strength on held-out error in 5-fold cross-validation results in a model that selects 123 candidate phonesthemes as predictive.", "labels": [], "entities": [{"text": "phonestheme extraction", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.7648114264011383}]}, {"text": "The phoneme bigrams corresponding to the 30 features with the highest absolute model weight are in.", "labels": [], "entities": []}, {"text": "Qualitatively, the words with the lowest error under the model containing each selected phonestheme candidate seem semantically coherent.", "labels": [], "entities": []}, {"text": "Many of the phonesthemes identified by our model have been proposed and validated by past work.", "labels": [], "entities": []}, {"text": "13 of the top 15 model-predicted phonesthemes were in Hutchins' set of 17 proposed word-  initial phoneme bigram phonesthemes.", "labels": [], "entities": []}, {"text": "This is an improvement over past work; identified 8 as statistically significant, with a hypothesis space restricted to 50 pre-specified word beginnings and endings.", "labels": [], "entities": []}, {"text": "also identified 8, but with a much larger hypothesis space of 225 candidates.", "labels": [], "entities": []}, {"text": "Our model considers an even larger hypothesis space of 307 candidate phonesthemes, which are all automatically extracted from the set of word vectors.", "labels": [], "entities": []}, {"text": "Validating Phonesthemes with Human Judgments Following the method of and, we empirically evaluate our phonesthemes by soliciting na\u00a8\u0131vena\u00a8\u0131ve human judgments about how well-suited a word's form is to its meaning.", "labels": [], "entities": []}, {"text": "We randomly selected 5 words containing each of the top 15 model-selected phonesthemes and 5 words containing 15 random phonestheme candidates that were not selected by the model, fora total of 150 words.", "labels": [], "entities": []}, {"text": "We recruited native English-speaking participants through Mechanical Turk, and asked them to judge how well each word fits its meaning on a Likert scale from 1 to 5. 150 words is too many judgments fora single HIT (annotators would become fatigued and words might start to lose meaning).", "labels": [], "entities": []}, {"text": "As a result, we randomly divided the task into 10 different HITs, each with 15 of the words to be tested.", "labels": [], "entities": []}, {"text": "We required Amazon Mechanical Turk Masters status for the crowdworkers and compensated them $0.20 per HIT; each word received 30 ratings.", "labels": [], "entities": []}, {"text": "Following, we compute ratings for each candidate phonestheme by averaging the rating of the words that contain it.", "labels": [], "entities": []}, {"text": "On average, model-predicted phonesthemes were rated 0.58 points higher than unselected phonestheme candidates (3.66 versus 3.08, respectively).", "labels": [], "entities": []}, {"text": "To assess whether this difference is statistically significant, we use the one-tailed Mann-Whitney U test) since the data is ordinal and unpaired.", "labels": [], "entities": [{"text": "one-tailed Mann-Whitney U test", "start_pos": 75, "end_pos": 105, "type": "METRIC", "confidence": 0.6678165197372437}]}, {"text": "Based on the results of the test, we reject the null hypothesis that the average rating of words containing model-selected phonesthemes is not greater than the average rating of words that contain phonesthemes not selected by the model (p < 10 \u22129 ).", "labels": [], "entities": []}, {"text": "plots the human ratings of the top 15 model-selected phonesthemes against their absolute weight under the model; there is a weak positive correlation (r = 0.081).", "labels": [], "entities": []}, {"text": "2 of the 15 model-predicted phonesthemes with the highest absolute weight were not previously proposed by: br-and wi-.", "labels": [], "entities": []}, {"text": "Both of these sound clusters seem like plausible phonesthemes.", "labels": [], "entities": []}, {"text": "To the authors, the br-cluster evokes the idea of a raw, almost uncultured force, with words like \"brags,\" \"brutish,\" and \"brusque\" appearing among the words with the lowest error under the model.", "labels": [], "entities": []}, {"text": "The types containing the word-initial wicluster with the lowest error under the model seem to convey fragility: \"wimpy,\" \"wince,\" and \"weak.\"", "labels": [], "entities": []}, {"text": "From, we can see that the br-phonestheme candidate received a very high model weight, but received lower ratings on average from human annotators.", "labels": [], "entities": []}, {"text": "On the other hand, the average human rating of the wi-phonestheme candidate seems inline with its assigned model weight.", "labels": [], "entities": []}, {"text": "Future work could further explore whether br-and wi-have psychological reality to native speakers.", "labels": [], "entities": []}], "tableCaptions": []}