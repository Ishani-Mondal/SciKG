{"title": [], "abstractContent": [{"text": "The Common European Framework of Reference (CEFR) guidelines describe language proficiency of learners on a scale of 6 levels.", "labels": [], "entities": [{"text": "Common European Framework of Reference (CEFR)", "start_pos": 4, "end_pos": 49, "type": "DATASET", "confidence": 0.7877848781645298}]}, {"text": "While the description of CEFR guidelines is generic across languages, the development of automated proficiency classification systems for different languages follow different approaches.", "labels": [], "entities": [{"text": "proficiency classification", "start_pos": 99, "end_pos": 125, "type": "TASK", "confidence": 0.6817494481801987}]}, {"text": "In this paper, we explore universal CEFR classification using domain-specific and domain-agnostic, theory-guided as well as data-driven features.", "labels": [], "entities": [{"text": "universal CEFR classification", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.624735822280248}]}, {"text": "We report the results of our preliminary experiments in monolingual, cross-lingual, and multilingual classification with three languages: German, Czech, and Italian.", "labels": [], "entities": []}, {"text": "Our results show that both monolingual and multilingual models achieve similar performance, and cross-lingual classification yields lower, but comparable results to monolingual classification.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated Essay Scoring (AES) refers to the task of automatically grading student essays written in response to some prompt.", "labels": [], "entities": [{"text": "Automated Essay Scoring (AES)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.724504311879476}]}, {"text": "Different approaches for AES have been proposed in literature, where it is modeled as a regression, ranking or a classification problem (cf..", "labels": [], "entities": [{"text": "AES", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9682025909423828}]}, {"text": "To our knowledge, all the previous work described approaches that work with a single language (mostly English).", "labels": [], "entities": []}, {"text": "Feature representations that can work for multiple languages and those that support cross-lingual AES have not been explored.", "labels": [], "entities": []}, {"text": "At first thought, using an essay scoring model developed for one language to test on another language seems unacceptable.", "labels": [], "entities": []}, {"text": "However, CEFR guidelines are not developed fora specific language.", "labels": [], "entities": []}, {"text": "This leads us to hypothesize about a common model of \"proficiency\" that can work across languages.", "labels": [], "entities": []}, {"text": "The existence of such a model would also be beneficial for quick prototyping of AES systems for languages that do not have readily available training data.", "labels": [], "entities": []}, {"text": "In this paper, we explore this hypothesis by exploring CEFR-classification for three languagesGerman, Italian, and Czech, for which CEFR graded data is publicly available.", "labels": [], "entities": [{"text": "CEFR-classification", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.904240071773529}]}, {"text": "Apart from constructing individual models using generic text classification and AES specific features, we also looked into cross-lingual (i.e., training a model on one language and testing on another) and multilingual classification approaches (i.e., building a single classification model trained on all the three languages at once).", "labels": [], "entities": [{"text": "generic text classification", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.6559379398822784}]}, {"text": "Testing our universal CEFR hypothesis would require a common feature representation across languages.", "labels": [], "entities": []}, {"text": "We developed such a representation, by employing features based on part-of-speech tags and dependency relations from the Universal Dependencies (UD)() project which provides treebanks for over 60 languages.", "labels": [], "entities": []}, {"text": "1 Therefore, this approach can be easily extended to other languages with available CEFR graded texts and UD treebanks.", "labels": [], "entities": [{"text": "CEFR graded texts", "start_pos": 84, "end_pos": 101, "type": "DATASET", "confidence": 0.8742298086484274}]}, {"text": "In short, the contributions of this paper are as follows: 1.", "labels": [], "entities": []}, {"text": "We study AES for multiple languages for the first time using CEFR scale.", "labels": [], "entities": [{"text": "AES", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.8493648767471313}, {"text": "CEFR scale", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.7664521634578705}]}, {"text": "2. We explore, for the first time, the possibility of a Universal CEFR classifier by training a single model consisting of three languages.", "labels": [], "entities": [{"text": "Universal CEFR classifier", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.4518146018187205}]}, {"text": "3. We also report first results on cross-lingual AES.", "labels": [], "entities": [{"text": "cross-lingual AES", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.5651232004165649}]}, {"text": "The rest of this paper is organized as follows: Section 2 describes related work.", "labels": [], "entities": []}, {"text": "Section 3 describes our data and methods.", "labels": [], "entities": []}, {"text": "Section 4 discuss 1 http://universaldependencies.org/ 147 our experiments and results in detail.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper with pointers to future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test our hypotheses, we need corpora graded with CEFR scale for multiple languages.", "labels": [], "entities": [{"text": "CEFR scale", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9082251191139221}]}, {"text": "One such multi-lingual corpus is the freely available MERLIN () corpus.", "labels": [], "entities": [{"text": "MERLIN () corpus", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.7906517187754313}]}, {"text": "This corpus consists of 2286 manually graded texts written by second language learners of German (DE), Italian (IT), and Czech (CZ) as apart of written examinations at authorized test institutions.", "labels": [], "entities": []}, {"text": "The aim of these examinations is to test the knowledge of the learners on the CEFR scale which consists of six categories -A1, A2, B1, B2, C1, C2 -which indicate improving language abilities.", "labels": [], "entities": [{"text": "CEFR scale", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9208107888698578}]}, {"text": "The writing tasks primarily consisted of writing formal/informal letters/emails and essays.", "labels": [], "entities": []}, {"text": "MER-LIN corpus has a multi-dimensional annotation of language proficiency covering aspects such as grammatical accuracy, vocabulary range, sociolinguistic awareness etc., and we used the \"Overall CEFR rating\" as the label for our experiments 2 http://merlin-platform.eu/ in this paper.", "labels": [], "entities": [{"text": "MER-LIN corpus", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.876332551240921}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.8386096358299255}]}, {"text": "Other information provided about the authors included-age, gender, and native language, and information about the task such as topic, and the CEFR level of the test itself.", "labels": [], "entities": [{"text": "CEFR", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.907336950302124}]}, {"text": "We did not use these information in the experiments reported in this paper.", "labels": [], "entities": []}, {"text": "Further, we removed all Language-CEFR Category combinations that had less than 10 examples in the corpus (German had 5 examples for level C2 and Italian had 2 examples for B2 which were removed from the data).", "labels": [], "entities": []}, {"text": "We also removed all the unrated texts from the original corpus.", "labels": [], "entities": []}, {"text": "The final corpus had 2266 documents covering three languages, and  We compared logistic regression, random forests, multi-layer perceptron, and support vector machines for experiments with non-embedding features and Neural Network models trained on taskspecific embedding representations for other experiments.", "labels": [], "entities": []}, {"text": "Word embeddings for each language were task-specific are trained only using the MER-LIN corpus.", "labels": [], "entities": [{"text": "MER-LIN corpus", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.8362340331077576}]}, {"text": "The embeddings are stacked with a softmax layer and trained with categorical crossentropy loss and Adadelta algorithm.", "labels": [], "entities": []}, {"text": "We also experimented by training a softmax classifier with character and word embeddings as input and found 3 https://languagetool.org/ that the combined model does not perform as well as a stand-alone word embeddings model.", "labels": [], "entities": []}, {"text": "Considering the space restrictions, we report only the best performing systems in this paper.", "labels": [], "entities": []}, {"text": "Due to the unbalanced class distribution across all the three languages in the data, we employed weighted-F1 score to evaluate the performance of our trained models.", "labels": [], "entities": []}, {"text": "Weighted F1 is computed as the weighted average of the F1 score for each label, taking label support (i.e., number of instances for each label in the data) into account.", "labels": [], "entities": [{"text": "F1", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.9485392570495605}, {"text": "F1 score", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9795965850353241}]}, {"text": "For both monolingual and multilingual settings, we report results with 10-fold cross validation.", "labels": [], "entities": []}, {"text": "For crosslingual evaluation, we report results on the test language's data.", "labels": [], "entities": []}, {"text": "All our neural network models are implemented using) with TensorFlow as the backend () and other models were implemented using scikit-learn).", "labels": [], "entities": []}, {"text": "While it is also possible to model AES as a regression task, we report classification results which is common in CEFR classification tasks.", "labels": [], "entities": [{"text": "CEFR classification tasks", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.7339046597480774}]}, {"text": "Our initial experiments with linear regression gave Pearson and Spearman correlation in the range of 0.7 \u2212 0.9 with gold standard scores, which is comparable with previous results on English AES task obtained using regression models ().", "labels": [], "entities": [{"text": "Pearson and Spearman correlation", "start_pos": 52, "end_pos": 84, "type": "METRIC", "confidence": 0.7273149788379669}]}, {"text": "For all the experiments, we considered a classifier using only document length (number of words per document) as the feature as the baseline.", "labels": [], "entities": []}, {"text": "Unless explicitly stated, all the reported results for nonembedding features are based on Random Forest classifier, which was the best performing classifier in our experiments.", "labels": [], "entities": [{"text": "Random Forest classifier", "start_pos": 90, "end_pos": 114, "type": "DATASET", "confidence": 0.8795618812243143}]}, {"text": "Numbers with superscript L indicate performance of results with a Logistic Regression model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Composition of MERLIN Corpus", "labels": [], "entities": [{"text": "Composition of MERLIN", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.5152765611807505}]}, {"text": " Table 2: Weighted F1 scores for Monolingual Clas- sification", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9592638611793518}]}, {"text": " Table 3: Weighted F1 scores for multilingual  classification with models trained on combined  datasets.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9590732157230377}, {"text": "multilingual  classification", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.7251318693161011}]}, {"text": " Table 4: Weighted F1 scores for cross-lingual clas- sification model trained on German.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9715175628662109}]}, {"text": " Table 5: Confusion matrices for cross-lingual scor- ing with Random Forests by training on German  data (DE-train).", "labels": [], "entities": [{"text": "cross-lingual scor- ing", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7623287886381149}, {"text": "German  data (DE-train)", "start_pos": 92, "end_pos": 115, "type": "DATASET", "confidence": 0.7251157879829406}]}]}