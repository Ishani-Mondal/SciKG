{"title": [{"text": "Accelerating NMT Batched Beam Decoding with LMBR Posteriors for Deployment", "labels": [], "entities": [{"text": "Accelerating", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9335563778877258}, {"text": "NMT Batched Beam Decoding", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.6654443442821503}, {"text": "LMBR Posteriors", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.8894385397434235}]}], "abstractContent": [{"text": "We describe a batched beam decoding algorithm for NMT with LMBR n-gram posteriors , showing that LMBR techniques still yield gains on top of the best recently reported results with Transformers.", "labels": [], "entities": []}, {"text": "We also discuss acceleration strategies for deployment, and the effect of the beam size and batching on memory and speed.", "labels": [], "entities": []}], "introductionContent": [{"text": "The advent of Neural Machine Translation (NMT) has revolutionized the market.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 14, "end_pos": 46, "type": "TASK", "confidence": 0.7974760631720225}]}, {"text": "Objective improvements () and a fair amount of neural hype have increased the pressure on companies offering Machine Translation services to shift as quickly as possible to this new paradigm.", "labels": [], "entities": [{"text": "Objective", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9229212999343872}, {"text": "Machine Translation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.8234268724918365}]}, {"text": "Such a radical change entails non-trivial challenges for deployment; consumers certainly look forward to better translation quality, but do not want to lose all the good features that have been developed over the years along with SMT technology.", "labels": [], "entities": [{"text": "SMT", "start_pos": 230, "end_pos": 233, "type": "TASK", "confidence": 0.985969066619873}]}, {"text": "With NMT, real time decoding is challenging without GPUs, and still an avenue for research.", "labels": [], "entities": []}, {"text": "Great speeds have been reported by on GPUs, for which batching queries to the neural model is essential.", "labels": [], "entities": [{"text": "batching queries", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.883918285369873}]}, {"text": "Disk usage and memory footprint of pure neural systems are certainly lower than that of SMT systems, but at the same time GPU memory is limited and high-end GPUs are expensive.", "labels": [], "entities": [{"text": "Disk usage", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9681442975997925}, {"text": "SMT", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9779495596885681}]}, {"text": "Further to that, consumers still need the ability to constrain translations; in particular, brandrelated information is often as important for companies as translation quality itself, and is currently under investigation (.", "labels": [], "entities": []}, {"text": "It is also well known that pure neural systems reach very high fluency, often sacrificing adequacy (, and have been reported to behave badly under noisy conditions.", "labels": [], "entities": []}, {"text": "show an effective way to counter these problems by taking advantage of the higher adequacy inherent to SMT systems via Lattice Minimum Bayes Risk (LMBR) decoding.", "labels": [], "entities": [{"text": "SMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9909268617630005}, {"text": "Lattice Minimum Bayes Risk (LMBR)", "start_pos": 119, "end_pos": 152, "type": "METRIC", "confidence": 0.7460360058716365}]}, {"text": "This makes the system more robust to pitfalls, such as over-and under-generation) which is important for commercial applications.", "labels": [], "entities": []}, {"text": "In this paper, we describe a batched beam decoding algorithm that uses NMT models with LMBR n-gram posterior probabilities ( . Batching in NMT beam decoding has been mentioned or assumed in the literature, e.g.), but to the best of our knowledge it has not been formally described, and there are interesting aspects for deployment worth taking into consideration.", "labels": [], "entities": [{"text": "NMT beam decoding", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.7332936326662699}]}, {"text": "We also report on the effect of LMBR posteriors on state-of-the-art neural systems, for five translation tasks.", "labels": [], "entities": []}, {"text": "Finally, we discuss how to prepare (LMBR-based) NMT systems for deployment, and how our batching algorithm performs in terms of memory and speed.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report experiments on English-German, German-English and Chinese-English language: Quality assessment of our NMT systems with and without LMBR posteriors for GRU-based (FNMT, LNMT) and Transformer models (TNMT, LTNMT).", "labels": [], "entities": []}, {"text": "Cased BLEU scores reported on 5 translation tasks.The exact PBMT systems used to compute n-gram posteriors for LNMT and LTNMT systems are also reported.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.995018482208252}, {"text": "translation tasks.The", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.892969936132431}]}, {"text": "The last row shows scores for the best official submissions to each task.", "labels": [], "entities": []}, {"text": "pairs for the WMT17 task, and Japanese-English and English-Japanese for the WAT task.", "labels": [], "entities": [{"text": "WMT17 task", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.5571922659873962}, {"text": "WAT task", "start_pos": 76, "end_pos": 84, "type": "TASK", "confidence": 0.9061338901519775}]}, {"text": "For the German tasks we use news-test2013 as a development set, and news-test2017 as a test set; for Chinese-English, we use news-dev2017 as a development set, and news-test2017 as a test set.", "labels": [], "entities": []}, {"text": "For Japanese tasks we use the ASPEC corpus ().", "labels": [], "entities": [{"text": "ASPEC corpus", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.7798752188682556}]}, {"text": "We use all available data in each task for training.", "labels": [], "entities": []}, {"text": "In addition, for German we use backtranslation data).", "labels": [], "entities": []}, {"text": "All training data for neural models is preprocessed with the byte pair encoding technique described by.", "labels": [], "entities": []}, {"text": "We use Blocks) with Theano () to train attention-based single GRU layer models (, henceforth called FNMT.", "labels": [], "entities": [{"text": "FNMT", "start_pos": 100, "end_pos": 104, "type": "DATASET", "confidence": 0.7463032007217407}]}, {"text": "The vocabulary size is 50K.", "labels": [], "entities": []}, {"text": "Transformer models (, called here TNMT, are trained using the Tensor2Tensor package 2 with a vocabulary size of 30K.", "labels": [], "entities": []}, {"text": "Our proprietary translation system is a modular homegrown tool that supports pure neural decoding (FNMT and TNMT) and with LMBR posteriors (henceforce called LNMT and LT-NMT respectively), and flexibly uses other components (phrase-based decoding, byte pair encoding, etcetera) to seamlessly deploy an end-to-end translation system.", "labels": [], "entities": [{"text": "LMBR posteriors", "start_pos": 123, "end_pos": 138, "type": "METRIC", "confidence": 0.871929794549942}, {"text": "byte pair encoding", "start_pos": 248, "end_pos": 266, "type": "TASK", "confidence": 0.6467933257420858}]}, {"text": "FNMT/LNMT systems use ensembles of 3 neural models unless specified otherwise; TNMT/LTNMT systems decode with 1 to 2 models, each averaging over the last 20 checkpoints.", "labels": [], "entities": []}, {"text": "The Phrase-based decoder (PBMT) uses standard features with one single 5-gram language 2 https://github.com/tensorflow/ tensor2tensor model (, and is tuned with standard MERT; n-gram posterior probabilities are computed on-the-fly over rich translation lattices, with size bounded by the PBMT stack and distortion limits.", "labels": [], "entities": []}, {"text": "The parameter \u03bb in Equation 2 is set as 0.5 divided by the number of models in the ensemble.", "labels": [], "entities": []}, {"text": "Empirically we have found this to be a good setting in many tasks.", "labels": [], "entities": []}, {"text": "Unless noted otherwise, the beam size is set to 12 and the NMT beam decoder always batches queries to the neural model.", "labels": [], "entities": []}, {"text": "The beam decoder relies on an early preview of ArrayFire 3.6 (Yalamanchili et al., 2015) 3 , compiled with CUDA 8.0 libraries.", "labels": [], "entities": [{"text": "ArrayFire 3.6 (Yalamanchili et al., 2015) 3", "start_pos": 47, "end_pos": 90, "type": "DATASET", "confidence": 0.8418229520320892}]}, {"text": "For speed measurements, the decoder uses one single CPU thread.", "labels": [], "entities": []}, {"text": "For hardware, we use an Intel Xeon CPU E5-2640 at 2.60GHz.", "labels": [], "entities": []}, {"text": "The GPU is a GeForce GTX 1080Ti.", "labels": [], "entities": []}, {"text": "We report cased BLEU scores (), strictly comparable to the official scores in each task 4 . shows contrastive experiments for all five language pair/tasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9210963845252991}]}, {"text": "We make the following observations:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Quality assessment of our NMT systems with and without LMBR posteriors for GRU-based (FNMT,  LNMT) and Transformer models (TNMT, LTNMT). Cased BLEU scores reported on 5 translation tasks.The exact  PBMT systems used to compute n-gram posteriors for LNMT and LTNMT systems are also reported. The last row  shows scores for the best official submissions to each task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9602035284042358}]}, {"text": " Table 2: Cased BLEU scores for research vs acceler- ated English-to-German WMT17 systems. Speed re- ported in words per minute.", "labels": [], "entities": [{"text": "Cased", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9791839122772217}, {"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9007498621940613}, {"text": "Speed", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.995267391204834}]}]}