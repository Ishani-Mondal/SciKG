{"title": [], "abstractContent": [{"text": "Dirichlet Multinomial Regression (DMR) and other supervised topic models can incorporate arbitrary document-level features to inform topic priors.", "labels": [], "entities": []}, {"text": "However, their ability to model corpora are limited by the representation and selection of these features-a choice the topic modeler must make.", "labels": [], "entities": []}, {"text": "Instead , we seek models that can learn the feature representations upon which to condition topic selection.", "labels": [], "entities": []}, {"text": "We present deep Dirichlet Multinomial Regression (dDMR), a generative topic model that simultaneously learns document feature representations and topics.", "labels": [], "entities": []}, {"text": "We evaluate dDMR on three datasets: New York Times articles with fine-grained tags, Amazon product reviews with product images, and Reddit posts with subreddit identity.", "labels": [], "entities": []}, {"text": "dDMR learns representations that outperform DMR and LDA according to heldout perplexity and are more effective at downstream predic-tive tasks as the number of topics grows.", "labels": [], "entities": []}, {"text": "Additionally, human subjects judge dDMR topics as being more representative of associated document features.", "labels": [], "entities": []}, {"text": "Finally, we find that supervision leads to faster convergence as compared to an LDA baseline and that dDMR's model fit is less sensitive to training parameters than DMR.", "labels": [], "entities": []}], "introductionContent": [{"text": "Fifteen years of research on topic models, starting from Latent Dirichlet Allocation (LDA) ( , have led to a variety of models for numerous data settings.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.6950728992621104}]}, {"text": "These models identify sets (distributions) of related words that reflect semantic topics in a large corpus of text data.", "labels": [], "entities": []}, {"text": "Topic models are now routinely used in the social sciences and humanities to analyze text collections.", "labels": [], "entities": []}, {"text": "Document collections are often accompanied by metadata and annotations, such as a book's author, an article's topic descriptor tags, images associated with a product review, or structured patient information associated with clinical records.", "labels": [], "entities": []}, {"text": "These document-level annotations can provide additional supervision for guiding topic model learning.", "labels": [], "entities": []}, {"text": "Additional information can be integrated into topic models using either downstream or upstream models.", "labels": [], "entities": []}, {"text": "Downstream models, such as supervised LDA, assume that these additional document features are generated from each document's topic distribution.", "labels": [], "entities": []}, {"text": "These models are most helpful when you desire topics that are predictive of the output, such as models for predicting the sentiment of product reviews.", "labels": [], "entities": [{"text": "predicting the sentiment of product reviews", "start_pos": 107, "end_pos": 150, "type": "TASK", "confidence": 0.8834307293097178}]}, {"text": "Upstream models, such as Dirichlet Multinomial Regression (DMR), condition each document's topic distribution on document features, such as author), social network (, or document labels (.", "labels": [], "entities": []}, {"text": "Previous work has demonstrated that upstream models tend to outperform downstream models in terms of model fit, as well as extracting topics that are useful in prediction of related tasks.", "labels": [], "entities": []}, {"text": "DMR is an upstream topic model with a particularly attractive method for incorporating arbitrary document features.", "labels": [], "entities": [{"text": "DMR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8801321387290955}]}, {"text": "Rather than defining specific random variables in the graphical model for each new document feature, DMR treats the document annotations as features in a log-linear model.", "labels": [], "entities": []}, {"text": "The log-linear model parameterizes the Dirichlet prior for the document's topic distribution, making the Dirichlet's hyperparameter (typically \u03b1) documentspecific.", "labels": [], "entities": []}, {"text": "By making no assumptions on model structure of new random variables, DMR is flexible to incorporating different types of features.", "labels": [], "entities": [{"text": "DMR", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9298794269561768}]}, {"text": "Despite this flexibility, DMR models are typically restricted to a small number of document features.", "labels": [], "entities": []}, {"text": "Several reasons account for this restriction: 1) Many text corpora only have a small number of documentlevel features; 2) Model hyperparameters become less interpretable as the dimensionality grows; and 3) DMR is liable to overfit the hyperparameters when the dimensionality of document features is high.", "labels": [], "entities": []}, {"text": "In practice, applications of DMR are limited to settings with a small number of features, or where the analyst selects a few meaningful features by hand.", "labels": [], "entities": [{"text": "DMR", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9745442867279053}]}, {"text": "A solution to this restriction is to learn lowdimensional representations of document features.", "labels": [], "entities": []}, {"text": "Neural networks have shown wide-spread success at learning generalizable representations, often obviating the need for hand designed features.", "labels": [], "entities": []}, {"text": "A prime example is word embedding features in natural language processing, which supplant traditional lexical features).", "labels": [], "entities": []}, {"text": "Jointly learning networks that construct feature representations along with the parameters of a standard NLP model has become a common approach.", "labels": [], "entities": []}, {"text": "For example, () used a tensor decomposition to jointly learn features from both word embeddings and traditional NLP features, along with the parameters of a relation extraction model.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.7121555805206299}]}, {"text": "Additionally, neural networks can handle a variety of data types, including text, images and general metadata features.", "labels": [], "entities": []}, {"text": "This makes them appropriate for addressing dimensionality reduction in DMR.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.6905536651611328}]}, {"text": "We propose deep Dirichlet Multinomial Regression (dDMR), a model that extends DMR by introducing a deep neural network that learns a transformation of the input metadata into features used to form the Dirichlet hyperparameter.", "labels": [], "entities": []}, {"text": "Whereas DMR parameterizes the document-topic priors as a log-linear function of document features, dDMR jointly learns a feature representation for each document along with a log-linear function that best captures the distribution over topics.", "labels": [], "entities": []}, {"text": "Since the function mapping document features to topic prior is a neural network, we can jointly optimize the topic model and the neural network parameters by gradient ascent and back-propagation.", "labels": [], "entities": []}, {"text": "We show that dDMR can use network architectures to better fit text corpora with high-dimensional document features as compared to other supervised topic models.", "labels": [], "entities": []}, {"text": "The topics learned by dDMR are judged as being more representative of document features by human subjects.", "labels": [], "entities": []}, {"text": "We also find that dDMR tends to converge in many fewer iterations than LDA, and also does not suffer from tuning difficulties that DMR encounters when applied to high-dimensional document features.", "labels": [], "entities": []}], "datasetContent": [{"text": "Model Estimation We used the same procedure for training topic models on each dataset.", "labels": [], "entities": [{"text": "Model Estimation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7651707828044891}]}, {"text": "Hyperparameter gradient updates were performed after a burnin period of 100 Gibbs sampling iterations.", "labels": [], "entities": []}, {"text": "Hyperparameters were updated with the adaptive learning rate algorithm Adadelta, with a tuned base learning rate and fixed \u03c1 = 0.95 3 . All models were trained fora maximum of 15,000 epochs, with early stopping if heldout perplexity showed no improvements after 200 epochs (evaluated once every 20 epochs).", "labels": [], "entities": [{"text": "Adadelta", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.8727346062660217}]}, {"text": "Hyperparameters were fit on every other token in the corpus, and (heldout) log-likelihood/perplexity was calculated on the remaining tokens.", "labels": [], "entities": []}, {"text": "For the architecture of the dDMR model we used single-hidden-layer multi-layer perceptrons (MLPs), with rectified linear unit (ReLU) activations on the hidden layer, and linear activation on the output layer.", "labels": [], "entities": []}, {"text": "We sampled three architectures for each dataset, by drawing layer widths independently at random from, and also included two architectures with (50, 10) and (100, 50), (hidden, output) layers 4 . We compare the performance of dDMR to DMR trained on the same feature set as well as LDA.", "labels": [], "entities": []}, {"text": "For the New York Times dataset, we also compare dDMR to DMR trained on features after applying principal components analysis (PCA) to reduce the dimensionality of descriptor feature supervision, sweeping over PCA projection width in {10, 50, 100, 250, 500, 1000}.", "labels": [], "entities": [{"text": "New York Times dataset", "start_pos": 8, "end_pos": 30, "type": "DATASET", "confidence": 0.6478603035211563}]}, {"text": "Comparing performance of dDMR to PCA-reduced DMR tests two modeling choices.", "labels": [], "entities": []}, {"text": "First, it tests the hypothesis that explicitly learning a representation for document annotations to maximize data likelihood produces a \"better-fit\" topic model than learning this annotation representation in unsupervised fashion -a two-step process.", "labels": [], "entities": []}, {"text": "It also lets us determine if a linear dimensionality reduction technique is sufficient to learning a good feature representation for topic modeling, as opposed to learning a non-linear transformation of the document supervision.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.7501756548881531}]}, {"text": "Note that we cannot apply PCA to reduce the dimensionality for subreddit id in Reddit since it is a one-hot feature.", "labels": [], "entities": [{"text": "PCA", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.8578046560287476}]}, {"text": "Documents in each dataset were partitioned into ten equally-sized folds.", "labels": [], "entities": []}, {"text": "Model training parameters of L1 and L2 regularization penalties on feature weights for DMR and dDMR and the base learning rate for each model class were tuned to minimize heldout perplexity on the first fold.", "labels": [], "entities": []}, {"text": "These were tuned independently for each model, with number of topics fixed to 10, and dDMR architecture fixed to narrow layer widths.", "labels": [], "entities": []}, {"text": "Model selection was based on the macro-averaged performance on the next eight folds, and we report performance on the remaining fold.", "labels": [], "entities": []}, {"text": "We selected models separately for each evaluation metric.", "labels": [], "entities": []}, {"text": "For dDMR, model selection amounts to selecting the document prior architecture, and for DMR with PCA-reduced feature supervision, model selection involved selecting the PCA projection width.", "labels": [], "entities": []}, {"text": "Evaluation Each model was evaluated according to heldout perplexity, topic coherence by normalized pointwise mutual information (NPMI) (, and a dataset-specific predictive task.", "labels": [], "entities": []}, {"text": "Heldout perplexity was computed by only aggregating document-topic and topic-word counts from every other token in the corpus, and evaluating perplexity on the remaining heldout tokens.", "labels": [], "entities": []}, {"text": "This corresponds to the \"document completion\" evaluation method as described in (, where instead of holding out the words in the second half of a document, every other word is held out.", "labels": [], "entities": [{"text": "document completion\" evaluation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.790067195892334}]}, {"text": "NPMI () computes a an automatic measure of topic quality, the sum of pointwise mutual information between pairs of m most likely words normalized by the negative log of each pair jointly occurring within a document (Eq. 2).", "labels": [], "entities": [{"text": "NPMI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7616122961044312}]}, {"text": "We calculated this topic quality metric on the top 20 most probable words in each topic, and averaged over the most coherent 1, 5, 10, and overall learned topics.", "labels": [], "entities": []}, {"text": "However, models were selected to only maximize average NPMI overall topics.", "labels": [], "entities": []}, {"text": "For prediction tasks, we used the sampled topic distribution associated with a document, averaged over the last 100 iterations, as features to predict a document-level label.", "labels": [], "entities": []}, {"text": "For New York Times articles we predicted 10 of the 200 most frequent descriptor tags restricting to articles with exactly one of these descriptors.", "labels": [], "entities": []}, {"text": "For Amazon, we predicted the product category a document belonged to (one of five), and for Reddit we predicted a heldout set of document subreddit IDs.", "labels": [], "entities": []}, {"text": "In the case of Reddit, these heldout subreddits were 10 out of the 100 most prevalent in our data, and were held out similar to the New York Times evaluation.", "labels": [], "entities": []}, {"text": "SVM models were fit on inferred topic distribution features and were then evaluated according to accuracy, F1-score, and area under the ROC curve.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9996355772018433}, {"text": "F1-score", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.999354898929596}, {"text": "ROC", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9809696674346924}]}, {"text": "The SVM slack parameter was tuned by 4-fold cross-validation on 60% of the documents, and evaluated on the remaining 40%.", "labels": [], "entities": [{"text": "SVM slack parameter", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.7239660024642944}]}, {"text": "We also collected human topic judgments using Amazon Mechanical Turk (Callison-Burch and  Dredze, 2010).", "labels": [], "entities": [{"text": "human topic judgments", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.6287245750427246}, {"text": "Amazon Mechanical Turk", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.8774276375770569}]}, {"text": "Each subject was presented with a human-readable version of the features used for supervision.", "labels": [], "entities": []}, {"text": "For New York Times articles we showed the descriptor tags, for Amazon the product image, and for Reddit the name, title, and public description of the subreddit.", "labels": [], "entities": []}, {"text": "We showed the top twenty words for the most probable topic sampled for the document with those features, as learned by two different models.", "labels": [], "entities": []}, {"text": "One topic was learned by dDMR and the other was either learned by LDA or DMR.", "labels": [], "entities": [{"text": "dDMR", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8614663481712341}, {"text": "DMR", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.6690368056297302}]}, {"text": "The topics presented were from the 200-topic model architecture that maximized NPMI on development folds.", "labels": [], "entities": []}, {"text": "Annotators were asked \"to choose which word list best describes a document . .", "labels": [], "entities": []}, {"text": "\" with the displayed features.", "labels": [], "entities": []}, {"text": "The topic learned by dDMR was shuffled to lie on either the right or left for each Human Intelligence Task (HIT).", "labels": [], "entities": []}, {"text": "We obtained judgments on 1,000 documents for each dataset and each model evaluation pair -6,000 documents in all.", "labels": [], "entities": []}, {"text": "This task can be difficult for many of the features, which maybe unclear (e.g. descriptor tags without context) or difficult to interpret (e.g. images of automotive parts).", "labels": [], "entities": []}, {"text": "We excluded the document text since we did not want subjects to evaluate topic quality based on token overlap with the actual document.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test fold heldout perplexity for each  dataset and model for number of topics Z. Stan- dard error of mean heldout perplexity over all cross- validation folds in parentheses.", "labels": [], "entities": [{"text": "Stan- dard error", "start_pos": 91, "end_pos": 107, "type": "METRIC", "confidence": 0.8530702292919159}]}, {"text": " Table 2: Top-1, 5, 10, and overall topic NPMI across all datasets. Models that maximized overall NPMI  across dev folds were chosen and the best-performing model is in bold.", "labels": [], "entities": []}, {"text": " Table 3: % HITs where humans preferred dDMR  topics as more representative of document supervi- sion than the competing model.  *  denotes statisti- cal significance according to a one-tailed binomial  test at the p = 0.05 level.", "labels": [], "entities": [{"text": "statisti- cal significance", "start_pos": 140, "end_pos": 166, "type": "METRIC", "confidence": 0.8665522933006287}]}, {"text": " Table 5: Top F-score, accuracy, and AUC on prediction tasks for all datasets.", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9985527396202087}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995853304862976}, {"text": "AUC", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9993477463722229}]}]}