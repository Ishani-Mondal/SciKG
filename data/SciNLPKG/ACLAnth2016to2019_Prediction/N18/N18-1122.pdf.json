{"title": [{"text": "Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets", "labels": [], "entities": [{"text": "Improving Neural Machine Translation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8934297859668732}]}], "abstractContent": [{"text": "This paper proposes an approach for applying GANs to NMT.", "labels": [], "entities": []}, {"text": "We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator.", "labels": [], "entities": []}, {"text": "The generator aims to generate sentences which are hard to be discriminated from human-translated sentences (i.e., the golden target sentences); And the discriminator makes efforts to discriminate the machine-generated sentences from human-translated ones.", "labels": [], "entities": []}, {"text": "The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium.", "labels": [], "entities": [{"text": "Nash Equilibrium", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.7206611335277557}]}, {"text": "Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9589444398880005}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9802145957946777}]}, {"text": "During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9965713024139404}]}, {"text": "Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks.", "labels": [], "entities": [{"text": "RNNSearch", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.8245418667793274}]}], "introductionContent": [{"text": "Neural machine translation) which directly leverages a single neural network to transform the source sentence into the target sentence, has drawn more and more attention in both academia and industry (.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7662306030591329}]}, {"text": "This end-to-end NMT typically consists of two sub neural networks.", "labels": [], "entities": []}, {"text": "The encoder network reads and encodes the source sentence into the context vector representation; and the decoder network generates the target sentence word byword based on the context vector.", "labels": [], "entities": []}, {"text": "To dynamically generate a context vector fora target word being generated, the attention mechanism which enables the model to focus on the relevant words in the sourceside sentence is usually deployed.", "labels": [], "entities": []}, {"text": "Under the encoder-decoder framework, many variants of the model structure, such as convolutional neural network (CNN) and recurrent neural network (RN-N) are proposed (.", "labels": [], "entities": []}, {"text": "Recently, ( propose the Transformer, the first sequence transduction model based entirely on attention, achieving state-of-the-art performance on the EnglishGerman and English-French translation tasks.", "labels": [], "entities": []}, {"text": "Despite its success, the Transformer, similar to traditional NMT models, is still optimized to maximize the likelihood estimation of the ground word (M-LE) at each time step.", "labels": [], "entities": [{"text": "likelihood estimation of the ground word (M-LE)", "start_pos": 108, "end_pos": 155, "type": "METRIC", "confidence": 0.8512052628729079}]}, {"text": "Such an objective poses a hidden danger to NMT models.", "labels": [], "entities": []}, {"text": "That is, the model may generate the best candidate word for the current time step yet a bad component of the whole sentence in the long run.", "labels": [], "entities": []}, {"text": "Minimum risk training (MRT)) is proposed to alleviate such a limitation by adopting the sequence level objective, i.e., the sentence-level BLEU, for traditional NMT models.", "labels": [], "entities": [{"text": "Minimum risk training (MRT))", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7227078278859457}, {"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9398758411407471}]}, {"text": "Yet somewhat improved, this objective still does not guarantee the translation results to be natural and sufficient.", "labels": [], "entities": [{"text": "translation", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.9543241858482361}]}, {"text": "Since the BLEU point is computed as the geometric mean of the modified n-gram precisions (), almost all of the existing objectives essentially train NMT models to generate sentences with n-gram precisions as high as possible (MLE can be viewed to generate sentences with high 1-gram precisions).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984061121940613}]}, {"text": "While n-gram precisions largely tell the good sentence apart from the bad one, it is widely acknowledged that higher n-gram precisions do not guarantee better sentences.", "labels": [], "entities": [{"text": "precisions", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.8490807414054871}]}, {"text": "Additionally, the manually defined objective, i.e., the n-gram precision, is unable to coverall crucial aspects of the data distribution and NMT models maybe trained to generate suboptimal sentences (.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.8663031458854675}]}, {"text": "In this paper, to address the limitation mentioned above, we borrow the idea of generative adversarial training from computer vision) to directly train the NMT model generating sentences which are hard to be discriminated from human translations.", "labels": [], "entities": []}, {"text": "The motivation behind is that while we cannot manually define the data distribution of golden sentences comprehensively, we are able to utilize a discriminative network to learn automatically what the golden sentences look like.", "labels": [], "entities": []}, {"text": "Following this motivation, we build a conditional sequence generative adversarial net where we jointly train two sub adversarial models: A generator generates the target-language sentence based on the input source-language sentence; And a discriminator, conditioned on the source-language sentence, predicts the probability of the target-language sentence being a human-generated one.", "labels": [], "entities": []}, {"text": "During the training process, the generator aims to fool the discriminator into believing that its output is a human-generated sentence, and the discriminator makes efforts not to be fooled by improving its ability to distinguish the machine-generated sentence from the human-generated one.", "labels": [], "entities": []}, {"text": "This kind of adversarial training achieves a win-win situation when the generator and discriminator reach a Nash Equilibrium (.", "labels": [], "entities": []}, {"text": "Besides generating the desired distribution, we also want to directly guide the generator with a static and specific objective, such as generating sentences with high BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9979825019836426}]}, {"text": "To this end, the smoothed sentence-level BLEU () is utilized as the reinforced objective for the generator.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9848757386207581}]}, {"text": "During training, we employ both the dynamic discriminator and the static BLEU objective to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9968430995941162}]}, {"text": "In summary, we mainly make the following contributions: \u2022 To the best of our knowledge, this work is among the first endeavors to introduce the generative adversarial training into NMT.", "labels": [], "entities": [{"text": "generative adversarial training", "start_pos": 144, "end_pos": 175, "type": "TASK", "confidence": 0.909710427125295}, {"text": "NMT", "start_pos": 181, "end_pos": 184, "type": "TASK", "confidence": 0.8917893171310425}]}, {"text": "We directly train the NMT model to generate sentences which are hard to be discriminated from human translations.", "labels": [], "entities": []}, {"text": "The proposed model can be applied to any end-to-end NMT systems.", "labels": [], "entities": []}, {"text": "\u2022 We conduct extensive experiments on English-German and Chinese-English translation tasks and we test two different NMT models, the traditional RNNSearch () and the state-of-the-art Transformer.", "labels": [], "entities": [{"text": "Chinese-English translation tasks", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.7651799122492472}]}, {"text": "Experimental results show that the proposed approach consistently achieves great success.", "labels": [], "entities": []}, {"text": "\u2022 Last but not least, we propose the smoothed sentence-level BLEU as the static and specific objective for the generator which biases the generation towards achieving high BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9654375910758972}, {"text": "BLEU", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9934428930282593}]}, {"text": "We show that the proposed approach is a weighted combination of the naive GAN and MRT.", "labels": [], "entities": [{"text": "MRT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.5043583512306213}]}], "datasetContent": [{"text": "We evaluate our BR-CSGAN on English-German and Chinese-English translation tasks and we test two different architectures for the generator, the traditional RNNSearch and the newly emerged state-of-the-art Transformer.", "labels": [], "entities": [{"text": "BR-CSGAN", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.8708653450012207}, {"text": "RNNSearch", "start_pos": 156, "end_pos": 165, "type": "DATASET", "confidence": 0.8054542541503906}]}, {"text": "For the Transformer, following the base model in (, we set the dimension of word embedding as 512, dropout rate as 0.1 and the head number as 8.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 99, "end_pos": 111, "type": "METRIC", "confidence": 0.9565519392490387}]}, {"text": "The encoder and decoder both have a stack of 6 layers.", "labels": [], "entities": []}, {"text": "We use beam search with abeam size of 4 and length penalty \u03b1 = 0.6.", "labels": [], "entities": [{"text": "beam search", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.8238837718963623}, {"text": "length penalty \u03b1", "start_pos": 44, "end_pos": 60, "type": "METRIC", "confidence": 0.9798505107561747}]}, {"text": "For the RNNSearch, following ( ), We set the hidden units for both encoders and decoders as 512.", "labels": [], "entities": [{"text": "RNNSearch", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.6944407224655151}]}, {"text": "The dimension of the word embedding is also set as 512.", "labels": [], "entities": []}, {"text": "We do not apply dropout for training the RNNSearch.", "labels": [], "entities": [{"text": "RNNSearch", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.9295166730880737}]}, {"text": "During testing, we use beam search with abeam size of 10 and length penalty is not applied.", "labels": [], "entities": [{"text": "length penalty", "start_pos": 61, "end_pos": 75, "type": "METRIC", "confidence": 0.9886136651039124}]}, {"text": "All models are implemented in TensorFlow ( and trained on up to four K80 GPUs synchronously in a multi-GPU setup on a 5 LDC2002L27, LDC2002T01, LDC2002E18, LD-C2003E07, LDC2004T08, LDC2004E12, LDC2005T10 When doing BPE for Chinese, we need to do word segmentation first and the following steps are the same with BPE for English.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 246, "end_pos": 263, "type": "TASK", "confidence": 0.7510746419429779}]}, {"text": "single machine . We stop training when the model achieves no improvement for the tenth evaluation on the development set.", "labels": [], "entities": []}, {"text": "BLEU () is utilized as the evaluation metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9813264608383179}]}, {"text": "We apply the script mteval-v11b.pl to evaluate the Chinese-English translation and utilize the script multi-belu.pl for English-German translation 8 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU score on Chinese-English and English-German translation tasks. The hyper-parameter \u03bb is selected  according to the development set. For the Transformer, following (Vaswani et al., 2017), we report the result of a  single model obtained by averaging the 5 checkpoints around the best model selected on the development set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982660412788391}]}, {"text": " Table 2: BLEU score on Chinese-English and English- German translation tasks for MRT and BR-CSGAN.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9635642766952515}, {"text": "English- German translation tasks", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.6877928376197815}, {"text": "MRT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.5185654163360596}, {"text": "BR-CSGAN", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.7096343040466309}]}]}