{"title": [{"text": "Tensor Product Generation Networks for Deep NLP Modeling", "labels": [], "entities": [{"text": "Tensor Product Generation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7441959381103516}, {"text": "Deep NLP Modeling", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.6276149650414785}]}], "abstractContent": [{"text": "We present anew approach to the design of deep networks for natural language processing (NLP), based on the general technique of Ten-sor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.813925047715505}]}, {"text": "A network architecture the Tensor Product Generation Network (TPGN)-is proposed which is capable in principle of carrying out TPR computation , but which uses unconstrained deep learning to design its internal representations.", "labels": [], "entities": []}, {"text": "Instantiated in a model for image-caption generation, TPGN outperforms LSTM base-lines when evaluated on the COCO dataset.", "labels": [], "entities": [{"text": "image-caption generation", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.7530201375484467}, {"text": "TPGN", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.7331603169441223}, {"text": "COCO dataset", "start_pos": 109, "end_pos": 121, "type": "DATASET", "confidence": 0.9684993326663971}]}, {"text": "The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content.", "labels": [], "entities": [{"text": "interpretation of internal representations and operations", "start_pos": 34, "end_pos": 91, "type": "TASK", "confidence": 0.8267427782217661}]}, {"text": "Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we introduce anew architecture for natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.7683379550774893}]}, {"text": "On what type of principles can a computational architecture be founded?", "labels": [], "entities": []}, {"text": "It would seem a sound principle to require that the hypothesis space for learning which an architecture provides include network hypotheses that are independently known to be suitable for performing the target task.", "labels": [], "entities": []}, {"text": "Our proposed architecture makes available to deep learning network configurations that perform natural language generation by use of Tensor Product Representations (TPRs) ().", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.7195562322934469}]}, {"text": "Whether learning will create TPRs is unknown in advance, but what we can say with certainty is that the hypothesis space being searched during learn- * LD is currently at ing includes TPRs as one appropriate solution to the problem.", "labels": [], "entities": []}, {"text": "TPRs area general method for generating vector-space embeddings of complex symbol structures.", "labels": [], "entities": []}, {"text": "Prior work has proved that TPRs enable powerful symbol processing to be carried out using neural network computation).", "labels": [], "entities": [{"text": "symbol processing", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.791131854057312}]}, {"text": "This includes generating parse trees that conform to a grammar (, although incorporating such capabilities into deep learning networks such as those developed here remains for future work.", "labels": [], "entities": []}, {"text": "The architecture presented here relies on simpler use of TPRs to generate sentences; grammars are not explicitly encoded here.", "labels": [], "entities": []}, {"text": "We test the proposed architecture by applying it to image-caption generation (on the MS-COCO dataset,).", "labels": [], "entities": [{"text": "image-caption generation", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7447120994329453}, {"text": "MS-COCO dataset", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9707301557064056}]}, {"text": "The results improve upon a baseline deploying a state-of-the-art LSTM architecture (, and the TPR foundations of the architecture provide greater interpretability.", "labels": [], "entities": []}, {"text": "Section 2 of the paper reviews TPR.", "labels": [], "entities": [{"text": "TPR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.7959769368171692}]}, {"text": "Section 3 presents the proposed architecture, the Tensor Product Generation Network (TPGN).", "labels": [], "entities": []}, {"text": "Section 4 describes the particular model we study for image captioning, and Section 5 presents the experimental results.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7584049105644226}]}, {"text": "Importantly, what the model has learned is interpreted in Section 5.3.", "labels": [], "entities": []}, {"text": "Section 6 discusses the relation of the new model to previous work and Section 7 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the performance of our proposed model, we use the COCO dataset.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.9668702483177185}]}, {"text": "The COCO dataset contains 123,287 images, each of which is annotated with at least 5 captions.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9534271657466888}]}, {"text": "We use the same pre-defined splits as in: 113,287 images for training, 5,000 images for validation, and 5,000 images for testing.", "labels": [], "entities": []}, {"text": "We use the same vocabulary as that employed in (, which consists of 8,791 words.", "labels": [], "entities": []}, {"text": "In our experiments, we choose d = 25 (where dis the dimension of vector pt ).", "labels": [], "entities": []}, {"text": "The dimension of St is 625 \u00d7 625 (whil\u00ea St is 25 \u00d7 25); the vocabulary size V = 8, 791; the dimension of u t and ft is d 2 = 625.", "labels": [], "entities": [{"text": "vocabulary size V", "start_pos": 60, "end_pos": 77, "type": "METRIC", "confidence": 0.692184050877889}]}, {"text": "The main evaluation results on the MS COCO dataset are reported in.2.", "labels": [], "entities": [{"text": "MS COCO dataset", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.8963815768559774}]}, {"text": "The widelyused BLEU (), METEOR (Banerjee and, and CIDEr (Vedantam et al., 2015) metrics are reported in our quantitative evaluation of the performance of the proposed model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9983416795730591}, {"text": "METEOR", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9813437461853027}, {"text": "CIDEr", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.7840668559074402}]}, {"text": "In evaluation, our baseline is the widely used CNN-LSTM captioning method originally proposed in ().", "labels": [], "entities": [{"text": "CNN-LSTM captioning", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.6964613795280457}]}, {"text": "For comparison, we include results in that paper in the first line of.2.", "labels": [], "entities": []}, {"text": "We also re-implemented the model using the latest ResNet features and report the results in the second line of.2.", "labels": [], "entities": []}, {"text": "Our re-implementation of the CNN-LSTM method matches the performance reported in (, showing that the baseline is a state-of-theart implementation.", "labels": [], "entities": []}, {"text": "For TPGN, we use parameter settings in a similar range to those in (.2, compared to the CNN-LSTM baseline, the proposed TPGN appreciably outperforms the benchmark schemes in all metrics across the board.", "labels": [], "entities": [{"text": "CNN-LSTM baseline", "start_pos": 88, "end_pos": 105, "type": "DATASET", "confidence": 0.9389322400093079}]}, {"text": "The improvement in BLEU-n is greater for greater n; TPGN particularly improves generation of longer subsequences.", "labels": [], "entities": [{"text": "BLEU-n", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9982388019561768}, {"text": "TPGN", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.7117637991905212}]}, {"text": "The results attest to the effectiveness of the TPGN architecture.", "labels": [], "entities": []}, {"text": "It is worth mentioning that this paper is aimed at developing a Tensor Product Representation (TPR) inspired network to replace the core layers in an LSTM; therefore, it is directly comparable to an LSTM baseline.", "labels": [], "entities": []}, {"text": "So in the experiments, we focus on comparison to a strong CNN-LSTM baseline.", "labels": [], "entities": [{"text": "CNN-LSTM baseline", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.853577733039856}]}, {"text": "We acknowledge that more recent papers () reported better performance on the task of image captioning.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7298141419887543}]}, {"text": "Performance improvements in these more recent models are mainly due to using better image features such as those obtained by Region-based Convolutional Neural Networks (R-CNN), or using reinforcement learning (RL) to directly optimize metrics such as CIDEr, or using more complex attention mechanisms ( to provide a better context vector for caption generation, or using an ensemble of multiple LSTMs, among others.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 342, "end_pos": 360, "type": "TASK", "confidence": 0.9256426692008972}]}, {"text": "However, the LSTM is still playing a core role in these works and we believe improvement over the core LSTM, in both performance and interpretability, is still very valuable; that is why we compare the proposed TPGN with a state-of-the-art native LSTM (the second line of.2).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the proposed TPGN model on the COCO dataset.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.974407434463501}]}, {"text": " Table 2: Conformity to N/V generalization (N u = 2).", "labels": [], "entities": []}, {"text": " Table 3: Interpretation of unbinding clusters (N u = 10)", "labels": [], "entities": []}]}