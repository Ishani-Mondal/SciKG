{"title": [{"text": "A Comparison of Two Paraphrase Models for Taxonomy Augmentation", "labels": [], "entities": [{"text": "Taxonomy Augmentation", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7949459850788116}]}], "abstractContent": [{"text": "Taxonomies are often used to lookup the concepts they contain in text documents (for instance , to classify a document).", "labels": [], "entities": [{"text": "classify a document)", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.8137844204902649}]}, {"text": "The more comprehensive the taxonomy, the higher recall the application has that uses the taxonomy.", "labels": [], "entities": [{"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9978609681129456}]}, {"text": "In this paper, we explore automatic taxonomy augmentation with paraphrases.", "labels": [], "entities": [{"text": "taxonomy augmentation", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.819129228591919}]}, {"text": "We compare two state-of-the-art paraphrase models based on Moses, a statistical Machine Translation system, and a sequence-to-sequence neural network, trained on a paraphrase datasets with respect to their abilities to add novel nodes to an existing taxonomy from the risk domain.", "labels": [], "entities": [{"text": "statistical Machine Translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.6119858920574188}]}, {"text": "We conduct component-based and task-based evaluations.", "labels": [], "entities": []}, {"text": "Our results show that paraphrasing is a viable method to enrich a taxonomy with more terms, and that Moses consistently outperforms the sequence-to-sequence neural model.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first approach to augment taxonomies with paraphrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Taxonomies are resources for organizing knowledge and are often used in a wide range of tasks such as document classification, search and natural language understanding, among others.", "labels": [], "entities": [{"text": "document classification", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.7895078659057617}, {"text": "natural language understanding", "start_pos": 138, "end_pos": 168, "type": "TASK", "confidence": 0.6627370019753774}]}, {"text": "Since developing taxonomies is a time consuming process, there has been a significant body of work on their automatic construction.", "labels": [], "entities": [{"text": "automatic construction", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.6955210119485855}]}, {"text": "However, even with the application of automatic methods, a taxonomy may not coverall concepts of interest due to issues in bootstrapping the automatic construction, for example the selection of seed terms, the coverage of the data used for mining the taxonomy, or balancing the trade-off between quality and recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 308, "end_pos": 314, "type": "METRIC", "confidence": 0.9957584738731384}]}, {"text": "In this work, we investigate the automatic augmentation of an existing taxonomy using generative paraphrasing.", "labels": [], "entities": [{"text": "generative paraphrasing", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.8951779901981354}]}, {"text": "We train a statistical machine translation model and a sequence-to-sequence neural network based model on a subset of the Paraphrase Database (PPDB 2.0).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.6307261983553568}, {"text": "Paraphrase Database (PPDB 2.0", "start_pos": 122, "end_pos": 151, "type": "DATASET", "confidence": 0.8512462139129638}]}, {"text": "We use the two models to augment an automatically mined taxonomy of risk terms based on (.", "labels": [], "entities": []}, {"text": "The research questions we address in this work are the following: \u2022 RQ1 Can the models generate high quality paraphrases for automatically augmenting a taxonomy?", "labels": [], "entities": [{"text": "RQ1", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.5494363903999329}]}, {"text": "\u2022 RQ2 How much does the coverage of the taxonomy increase?", "labels": [], "entities": [{"text": "RQ2", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.6618480682373047}, {"text": "coverage", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9806369543075562}]}, {"text": "\u2022 RQ3 Which model is best for generating paraphrases?", "labels": [], "entities": []}, {"text": "We answer these research questions by assessing the quality of the generated risk phrases and quantifying the number of additional sentences that the generated paraphrases match in a large corpus of news articles.", "labels": [], "entities": []}], "datasetContent": [{"text": "For training the paraphrase generation models, we use a subset of the Paraphrase Database (PPDB 2.0) corpus.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.9050078690052032}, {"text": "Paraphrase Database (PPDB 2.0) corpus", "start_pos": 70, "end_pos": 107, "type": "DATASET", "confidence": 0.8664055722100394}]}, {"text": "The PPDB 2.0 data set is a large-scale phrasal paraphrase data set that has been mined automatically based on (), and refined with machine learning based ranking of paraphrases based on human generated ground-truth and assignment of textual entailment labels for each paraphrase pair.", "labels": [], "entities": [{"text": "PPDB 2.0 data set", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8874845951795578}]}, {"text": "In this work, we used the large pack of lexical (single word to single word) and phrasal (multi-word to single or multi-word) paraphrases . Because the data set was automatically generated, some of the paraphrase pairs are not true paraphrases.", "labels": [], "entities": []}, {"text": "In our experiments, we kept only pairs that do not contain numeric digits.", "labels": [], "entities": []}, {"text": "We also use the textual entailment labels with which paraphrase pairs in the data set are annotated and keep the pairs labeled as equivalent.", "labels": [], "entities": []}, {"text": "We split the remaining data in 757,300 training data points and 39,325 test data points.", "labels": [], "entities": []}, {"text": "The splitting is performed by first creating a graph where phrases are nodes and edges exist between the two phrases in a paraphrase pair.", "labels": [], "entities": []}, {"text": "In this graph, we identify connected components and we assign all data points within each connected component to either the training or the test sets.", "labels": [], "entities": []}, {"text": "This process guarantees independence between the training and the test sets.", "labels": [], "entities": []}, {"text": "To train Moses, we precomputed a tri-gram language from the target phrases in the training data set and used the MERT optimizer.", "labels": [], "entities": []}, {"text": "To train the seq2seq model, we used a batch size of 256 training samples, 100-unit LSTM cells for both the encoder and the decoder, dropout with keep probability 0.8 at the output of cells, a bidirectional encoder, greedy 1-best search output generation criteria, and an additive attention function (.", "labels": [], "entities": [{"text": "keep probability 0.8", "start_pos": 145, "end_pos": 165, "type": "METRIC", "confidence": 0.9653478463490804}]}, {"text": "For representing words, we used 100 dimensional pre-trained GloVe embeddings).", "labels": [], "entities": []}, {"text": "We trained using the Adam optimizer and a learning rate of 0.001 and let the models train for 200,000 steps (a step is an iteration over a batch of training points).", "labels": [], "entities": []}, {"text": "For evaluation we used the BLEU score).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9570455253124237}]}, {"text": "BLEU is calculated on tokenized data using the implementation provided in the nltk framework 2 with NIST geometric sequence smoothing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9844687581062317}, {"text": "NIST geometric sequence smoothing", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.6493304073810577}]}, {"text": "Moses achieved a BLEU score of 0.4098 compared to 0.3156 obtained by the seq2seq model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9805445671081543}]}, {"text": "The difference in BLEU score shows that Moses is substantially better than the seq2seq model for the subset of PPDB 2.0 we used.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.999344527721405}]}, {"text": "After training the paraphrase generation models, we focus on augmenting the taxonomy of risks.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.8867745399475098}]}, {"text": "The risk taxonomy has been automatically mined based on the method described in and subsequently has been manually filtered to keep high quality risk terms, resulting in 2,824 terms.", "labels": [], "entities": []}, {"text": "For each term in the risk taxonomy, we apply the two paraphrase generation models to obtain a maximum of top 10 paraphrased risk terms.", "labels": [], "entities": []}, {"text": "shows the number of generated paraphrases that are also in the original list of risk phrases.", "labels": [], "entities": []}, {"text": "While our end goal is to generate phrases that are not in the original list of phrases, a large number of generations already appearing in the list of highquality and manually filtered list of risk phrases is an indication of the quality of the paraphrases.", "labels": [], "entities": []}, {"text": "As we can see from, Moses outperforms with a wide margin the seq2seq model in generating paraphrases already in the taxonomy.", "labels": [], "entities": []}, {"text": "Table 1 shows examples of generated paraphrases by Moses and seq2seq.", "labels": [], "entities": []}, {"text": "Furthermore, we have manually annotated the top-1 generated paraphrases that were not already in the original risk taxonomy.", "labels": [], "entities": []}, {"text": "Each paraphrased risk term was annotated as valid when it can directly replace the original risk term, noisy when the meaning of the paraphrase is close to the meaning of the original term or the paraphrase has additional terms, and invalid when the paraphrase is not suitable for substituting the original risk term.", "labels": [], "entities": []}, {"text": "shows for both models the number of paraphrases that were not in the original taxonomy and that were annotated with a given label.", "labels": [], "entities": []}, {"text": "Even though both the BLEU score and the number of paraphrases that were already in the original risk taxonomy demonstrate that Moses performs better than seq2seq in our setting, we have also looked how often a paraphrase generated with seq2seq was annotated as being better than the paraphrase generated by Moses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9992885589599609}]}, {"text": "For example, this is the case when one model generates a paraphrase that is annotated as valid and the other model generates for the input risk term a paraphrase that is annotated as noisy or invalid.", "labels": [], "entities": []}, {"text": "We have observed that in 1211 cases, the paraphrase generated by Moses was better than the paraphrase generated by seq2seq.", "labels": [], "entities": []}, {"text": "On the other hand, seq2seq was better only in 58 cases.", "labels": [], "entities": []}, {"text": "We have also looked at the lexical diversity of the generated paraphrases.", "labels": [], "entities": []}, {"text": "We define lexical diversity as the fraction of tokens in a paraphrase that were not in the original risk phrase.", "labels": [], "entities": []}, {"text": "shows that the seq2seq model results in higher lexical diversity than Moses for both the valid and noisy paraphrases.", "labels": [], "entities": []}, {"text": "Finally, we have looked at the number of sentences matched by the original risk phrases and the generated paraphrases in large corpus of approximately 14 million news articles.", "labels": [], "entities": []}, {"text": "The original list of risk phrases matches 23,110,506 sentences.", "labels": [], "entities": []}, {"text": "As    Overall, we have seen that the application of paraphrase generation can expand an existing taxonomy of risk terms with high quality phrases, where 67% of the added terms by Moses have been assessed as valid paraphrases (RQ1).", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.9811132252216339}]}, {"text": "This has led to an increase of the coverage of the taxonomy by 22% (RQ2).", "labels": [], "entities": [{"text": "coverage", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9816797971725464}, {"text": "RQ2)", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.7007739245891571}]}, {"text": "The experimental results also demonstrate that Moses outperforms the neural network-based model in this setting (RQ3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of paraphrases generated by Moses and seq2seq.", "labels": [], "entities": []}, {"text": " Table 2: Number of generated paraphrases annotated  as valid, noisy or invalid.", "labels": [], "entities": []}, {"text": " Table 3: Lexical diversity of generated valid and noisy  paraphrases in terms of fraction of tokens that are not  in the original risk phrase.", "labels": [], "entities": []}, {"text": " Table 4: The number of sentences from the news  archive matching at least one of the generated valid  or noisy risk paraphrases, which were not already  matched by a risk phrase in the original taxonomy.", "labels": [], "entities": []}]}