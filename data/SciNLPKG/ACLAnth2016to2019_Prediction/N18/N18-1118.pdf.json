{"title": [{"text": "Evaluating Discourse Phenomena in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.6927935083707174}]}], "abstractContent": [{"text": "For machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7621287703514099}]}, {"text": "There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena.", "labels": [], "entities": [{"text": "modelling context in neural machine translation (NMT)", "start_pos": 34, "end_pos": 87, "type": "TASK", "confidence": 0.7691710061497159}]}, {"text": "In this article, we present hand-crafted, discourse test sets, designed to test the models' ability to exploit previous source and target sentences.", "labels": [], "entities": []}, {"text": "We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French.", "labels": [], "entities": []}, {"text": "We also explore a novel way of exploiting context from the previous sentence.", "labels": [], "entities": []}, {"text": "Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50% accuracy on our coreference test set and 53.5% for coherence/cohesion (compared to a non-contextual baseline of 50%).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.876105546951294}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9995469450950623}]}, {"text": "A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5% for corefer-ence and 57% for coherence/cohesion), highlighting the importance of target-side context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation (MT) systems typically translate sentences independently of each other.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8773579716682434}]}, {"text": "However, certain textual elements cannot be correctly translated without linguistic context, which may appear outside the current sentence.", "labels": [], "entities": []}, {"text": "The most obvious examples of context-dependent phenomena problematic for MT are coreference, lexical cohesion and lexical disambiguation, an example for each of which is given in.", "labels": [], "entities": [{"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9950407147407532}, {"text": "coreference", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.9520904421806335}]}, {"text": "In each case, the English element in italic is ambiguous in terms of its French translation.", "labels": [], "entities": []}, {"text": "The correct translation choice (in bold) is determined by linguistic context (underlined), which can be outside the current sentence.", "labels": [], "entities": []}, {"text": "This disambiguating context can be source or target-side; the correct translation of anaphoric pronouns it and they depends on the gender of the translated antecedent (1).", "labels": [], "entities": []}, {"text": "In lexical cohesion, a translation may depend on target factors, but may also be triggered by source effects and linguistic mechanisms such as repetition or alignment.", "labels": [], "entities": []}, {"text": "In lexical disambiguation, source or target information may provide the appropriate context (3).", "labels": [], "entities": []}, {"text": "Recent work on multi-encoder neural machine translation (NMT) appears promising for the integration of linguistic context;).", "labels": [], "entities": [{"text": "multi-encoder neural machine translation (NMT)", "start_pos": 15, "end_pos": 61, "type": "TASK", "confidence": 0.7225428734506879}]}, {"text": "However models have almost only been evaluated using standard automatic metrics, which are poorly adapted to evaluating discourse phenomena.", "labels": [], "entities": []}, {"text": "Targeted evaluation, in particular of coreference in MT, has proved to be time-consuming and laborious.", "labels": [], "entities": [{"text": "coreference", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9654300212860107}, {"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9197413921356201}]}, {"text": "In this article, we address the evaluation of discourse phenomena for MT and propose a novel contextual model.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9937506318092346}]}, {"text": "We present two hand-crafted, discourse test sets designed to test models' capacity to exploit linguistic context for coreference and coherence/cohesion for English to French translation.", "labels": [], "entities": []}, {"text": "Using these sets, we review contextual NMT strategies trained on subtitles in a high-resource setting.", "labels": [], "entities": []}, {"text": "Our new combination of strategies outperforms previous methods according to our targeted evaluation and the standard metric BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9927133917808533}]}], "datasetContent": [{"text": "Each of the multi-encoder strategies is tested using the previous source and target sentences as an additional input (prefixed as S-and T-respectively) in order to test which is the most useful disambiguating context.", "labels": [], "entities": []}, {"text": "Two additional models tested are triple-encoder models, which use both the previous source and target (prefixed as S-T-).", "labels": [], "entities": []}, {"text": "All models are sequence-to-sequence models with attention (, implemented in Nematus ( . Training is performed using the Adam optimiser with a learning rate of 0.0001 until convergence.", "labels": [], "entities": []}, {"text": "We use embedding layers of dimension 512 and hidden layers of dimension 1024.", "labels": [], "entities": []}, {"text": "For training, the maximum sentence length is 50.", "labels": [], "entities": []}, {"text": "We use batch sizes of 80, tied decoder embeddings and layer normalisation.", "labels": [], "entities": []}, {"text": "The hyper-parameters are the same for all models and are the same as those used for the University of Edinburgh submissions to the news translation shared task at WMT16 and WMT17.", "labels": [], "entities": [{"text": "news translation shared task", "start_pos": 131, "end_pos": 159, "type": "TASK", "confidence": 0.7852797880768776}, {"text": "WMT16", "start_pos": 163, "end_pos": 168, "type": "DATASET", "confidence": 0.9504462480545044}, {"text": "WMT17", "start_pos": 173, "end_pos": 178, "type": "DATASET", "confidence": 0.9234771728515625}]}, {"text": "Final models are ensembled using the last three checkpointed models.", "labels": [], "entities": []}, {"text": "Models that use the previous target sentence are trained using the previous reference translation.", "labels": [], "entities": []}, {"text": "During translation, baseline translations are used.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.986234724521637}]}, {"text": "For the targeted evaluation, the problem does not apply since the translations that are being scored are given.", "labels": [], "entities": []}, {"text": "2 shows the results on the discourse test sets.", "labels": [], "entities": []}, {"text": "Coreference The multi-encoder models do not perform well on the coreference test set; all multiencoder models giving at best random accuracy, as with the baseline.", "labels": [], "entities": [{"text": "coreference test set", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.7718798120816549}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9380093216896057}]}, {"text": "This set is designed to test the Each of the test sets contains three films from that genre, with varying sizes and difficulty.", "labels": [], "entities": []}, {"text": "The number of sentences in each test set is as follows: comedy: 4,490, crime: 4,227, fantasy: 2,790 and horror: 2,158.", "labels": [], "entities": []}, {"text": "model's capacity to exploit previous target context.", "labels": [], "entities": []}, {"text": "It is therefore unsurprising that multi-encoder models using just the previous source sentence perform poorly.", "labels": [], "entities": []}, {"text": "It is possible that certain pronouns could be correctly predicted from the source antecedents, if the antecedent only has one possible translation.", "labels": [], "entities": []}, {"text": "However, this non-robust way of translating pronouns is not tested by the test set.", "labels": [], "entities": []}, {"text": "More surprisingly, the multi-encoder models using the previous target sentence also perform poorly on the test set.", "labels": [], "entities": []}, {"text": "An explanation could be that the target sentence is not being encoded sufficiently well in this framework, resulting in poor learning.", "labels": [], "entities": []}, {"text": "This hypothesis is supported by the low overall translation performance shown in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9080957174301147}]}, {"text": "1. Two models perform well on the test set: 2-TO-2 and our S-HIER-TO-2.", "labels": [], "entities": []}, {"text": "The high scores, particularly on the less common feminine pronouns, which can only be achieved through using contextual linguistic information, show that these models are capable of using previous linguistic context to disambiguate pronouns.", "labels": [], "entities": []}, {"text": "The progressively high performance of these models can be seen in, which illustrates the training progress of these models.", "labels": [], "entities": []}, {"text": "The S-T-HIER-TO-2 model (which uses the previous target sentence as a third auxiliary input) performs much worse than S-HIER-TO-2, showing that the addition of the previous target sentence is detrimental to performance.", "labels": [], "entities": []}, {"text": "Whilst the: Results on the discourse test sets (% correct).", "labels": [], "entities": []}, {"text": "Results on the coreference set are also given for each pronoun class.", "labels": [], "entities": []}, {"text": "CORR. and SEMI correspond respectively to the \"correct\" and \"semi-correct\" examples.", "labels": [], "entities": [{"text": "CORR.", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9343643188476562}, {"text": "SEMI", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9944490194320679}]}, {"text": "The best, second-and third-best results are highlighted by decreasingly dark shades of green.", "labels": [], "entities": []}, {"text": "results for the \"correct\" examples (CORR.) are almost always higher than the \"semi-correct\" examples (SEMI), for which the antecedent is strangely translated, the TO-2 models also give improved results on these examples, showing that the target context is necessarily being exploited during decoding.", "labels": [], "entities": [{"text": "CORR.", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9686216115951538}, {"text": "SEMI", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9474236965179443}]}, {"text": "These results show that the translation of the previous sentence is the most important factor in the efficient use of linguistic context.", "labels": [], "entities": [{"text": "translation of the previous sentence", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.8469537615776062}]}, {"text": "Combining the S-HIER model with decoding of the previous target sentence (S-HIER-TO-2) produces some of the best results across all pronoun types, and the 2-TO-2 model performs almost always second best.", "labels": [], "entities": []}, {"text": "Coherence and cohesion Much less variation in scores can be seen here, suggesting that these examples are more challenging and that there is room for improvement.", "labels": [], "entities": []}, {"text": "Unlike the coreference examples, the multi-encoder strategies exploiting the previous source sentences perform better than the baseline (up to 53.5% for S-CONCAT).", "labels": [], "entities": []}, {"text": "Yet again, using the previous target sentence achieves near random accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9820084571838379}]}, {"text": "2-TO-2 and 2-TO-1 achieve similarly low scores (52% and 53%), suggesting that if concatenated input is used, decoding the previous sentence does not add more information.", "labels": [], "entities": []}, {"text": "However, combining multi-encoding with the decoding of the previous and the current sentences (S-HIER-TO-2) greatly improves the handling of the ambiguous translations, improving the accu- racy to 57%.", "labels": [], "entities": [{"text": "accu- racy", "start_pos": 183, "end_pos": 193, "type": "METRIC", "confidence": 0.9526533087094625}]}, {"text": "Extending this same model to also exploit the previous target sentence (S-T-HIER-TO-2) degrades this result, giving very similar scores to T-HIER and is therefore not illustrated in.", "labels": [], "entities": []}, {"text": "This provides further support for the idea that the target sentence is not encoded efficiently as an auxiliary input and adds noise to the model, whereas exploiting the target context as a bias in the recurrent decoder is more effective.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results (de-tokenised, cased BLEU) of the ensembled models on four different test sets, each  containing three films from each film genre. The best, second-and third-best results are highlighted by  decreasingly dark shades of green.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9788486957550049}]}, {"text": " Table 2: Results on the discourse test sets (% correct). Results on the coreference set are also given  for each pronoun class. CORR. and SEMI correspond respectively to the \"correct\" and \"semi-correct\"  examples. The best, second-and third-best results are highlighted by decreasingly dark shades of green.", "labels": [], "entities": [{"text": "CORR.", "start_pos": 129, "end_pos": 134, "type": "METRIC", "confidence": 0.992624819278717}, {"text": "SEMI", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9948616623878479}]}]}