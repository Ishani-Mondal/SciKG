{"title": [{"text": "Deep Dungeons and Dragons: Learning Character-Action Interactions from Role-Playing Game Transcripts", "labels": [], "entities": []}], "abstractContent": [{"text": "An essential aspect to understanding narratives is to grasp the interaction between characters in a story and the actions they take.", "labels": [], "entities": []}, {"text": "We examine whether computational models can capture this interaction, when both character attributes and actions are expressed as complex natural language descriptions.", "labels": [], "entities": []}, {"text": "We propose role-playing games as a testbed for this problem, and introduce a large corpus 1 of game transcripts collected from online discussion forums.", "labels": [], "entities": []}, {"text": "Using neural language models which combine character and action descriptions from these stories, we show that we can learn the latent ties.", "labels": [], "entities": []}, {"text": "Action sequences are better predicted when the character performing the action is also taken into account, and vice versa for character attributes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Imagine a giant, a dwarf, and a fairy in a combat situation.", "labels": [], "entities": []}, {"text": "We would expect them to act differently, and conversely, if we are told of even a few actions taken by a character in a story, we naturally start to draw inferences about that character's personality.", "labels": [], "entities": []}, {"text": "Communicating narrative is a fundamental task of natural language, and understanding narrative requires modelling the interaction between events and characters.", "labels": [], "entities": []}, {"text": "In this paper, we propose that collaborativelytold stories that arise in certain types of games provide a natural test bed for the problem of inferring interactions between characters and actions in narratives.", "labels": [], "entities": []}, {"text": "We present a corpus of role-playing game (RPG) transcripts where characters and action sequences are described with complex natural language texts.", "labels": [], "entities": []}, {"text": "shows an example character Character description Name: Ana Blackclaw; Age: 27; Gender: Female Appearance: Standing at a mighty 6'5, she is a giant among her fellow humans.", "labels": [], "entities": []}, {"text": "Her face is light, though paler than the average manor woman's, and is marked by scars.", "labels": [], "entities": []}, {"text": "Her body is muscular, as it would have to be to carry both her armor and the hammer.", "labels": [], "entities": []}, {"text": "Her light grey eyes nearly always keep a bored expression.", "labels": [], "entities": []}, {"text": "Her canines seem a tad larger than the normal person's.", "labels": [], "entities": []}, {"text": "Darksign: No. Action description She stopped dead in her tracks as the hissing began.", "labels": [], "entities": []}, {"text": "A grumble escaped her as it did so, and she looked over to make sure the other woman was doing fine.", "labels": [], "entities": []}, {"text": "Seeing that all was not entirely well, she allowed herself to slide down, her hand gripping the slope side once more to slow herself.", "labels": [], "entities": []}, {"text": "Once that was accomplished, she reached out and grabbed the back of the girl's neck, pulling her back to steady herself.", "labels": [], "entities": []}, {"text": "The giant remained silent as she did so, and then glanced over to the nearby skeletons.", "labels": [], "entities": []}, {"text": "They would be upon them soon.", "labels": [], "entities": []}, {"text": "Her grip tightened on the hammer as she glanced from side to side.", "labels": [], "entities": []}, {"text": "It would not be a fun fight.: Example descriptions from our RPG corpus description, and an action text for the same character.", "labels": [], "entities": [{"text": "RPG corpus description", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.8224608898162842}]}, {"text": "This example shows how the ties between characters and their actions are subtly present in the text descriptions, and learning the latent ties between them is a difficult task.", "labels": [], "entities": []}, {"text": "Based on our corpus, and using neural language models, this work demonstrates an initial success on this problem.", "labels": [], "entities": []}, {"text": "The ability to understand and generate narratives is a useful skill for natural language systems, for example, to plan a coherent answer to a question, or to generate a summary of a document.", "labels": [], "entities": []}, {"text": "Prior work on narrative processing has focused on inducing disjoint sets of character and event types (as topic models), capturing the relationship between characters in the same story, or extracting character-action pairs as low level noun-verb tuples.", "labels": [], "entities": []}, {"text": "However, these models do not aim to match or infer characters and actions from each other.", "labels": [], "entities": []}, {"text": "We make two contributions towards closing this gap.", "labels": [], "entities": []}, {"text": "We introduce a corpus of thousands of RPG transcripts and demonstrate predictive cues between characters and actions by building neural language models with facility for adding side information.", "labels": [], "entities": []}, {"text": "We show that a language model over action text obtains lower perplexity when we also make available a representation of the character who produced each token.", "labels": [], "entities": []}, {"text": "Likewise, a language model for character descriptions benefits from information about the actions the character made.", "labels": [], "entities": [{"text": "character descriptions", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7075473368167877}]}, {"text": "Our findings open up new possibilities for making sophisticated inference over narrative texts.", "labels": [], "entities": []}], "datasetContent": [{"text": "We randomly divide our corpus into 100 stories for testing, 20 for development, and the rest, 1319 for training.", "labels": [], "entities": []}, {"text": "We compare the two ACTION language models, based on a vocabulary size of 20,000, and the CHAR models have a vocabulary of 10,000.", "labels": [], "entities": []}, {"text": "Some posts are long even after our filtering steps, and create a winding story line when concatenated.", "labels": [], "entities": []}, {"text": "So we also explore whether limits on description lengths is useful.", "labels": [], "entities": []}, {"text": "In ACTION models, a limit of g means that only the first g words of each post are concatenated to form X.", "labels": [], "entities": []}, {"text": "For CHAR models, only the first g words of the description Ci is used as the sequence for the LM.", "labels": [], "entities": []}, {"text": "The same limit g is given to both the models with and without side information.", "labels": [], "entities": []}, {"text": "When using side information, we can restrict the conditioning text as well, to a maximum of h words.", "labels": [], "entities": []}, {"text": "We tune these limit parameters, as well as the number of hidden layers, hidden unit sizes and dropout probability on a development set.", "labels": [], "entities": []}, {"text": "For the ACTION models, we set g to 100 words.", "labels": [], "entities": []}, {"text": "ACTION-LM uses 2 layers with 256 hidden units each.", "labels": [], "entities": [{"text": "ACTION-LM", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8739070892333984}]}, {"text": "ACTION-LMS has 1 layer with 256 hidden units for the feature network with h set to 25 words, and 1 layer with 50 units for the RNN part.", "labels": [], "entities": [{"text": "ACTION-LMS", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8563259243965149}]}, {"text": "For the CHAR models, g = 200 words.", "labels": [], "entities": [{"text": "CHAR", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.7720711827278137}]}, {"text": "CHAR-LM has one hidden layer with 100 units.", "labels": [], "entities": [{"text": "CHAR-LM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9101476669311523}]}, {"text": "For CHAR-LMS, the best network was the same as ACTION-LMS but with h = 100 (the first 100 words of all the action posts by that character are combined as the side information).", "labels": [], "entities": []}, {"text": "We apply a dropout probability of 0.65, clip gradients at 5.0, and use the Adam algorithm (Kingma and Ba, 2015) for optimization.", "labels": [], "entities": []}, {"text": "All our models can trained in an hour, ACTION-LM with 14 epochs, CHAR-LM 62, ACTION-LMS 60 and CHAR-LMS 91 epochs.", "labels": [], "entities": [{"text": "ACTION-LM", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.5730738639831543}]}, {"text": "We implemented the models in TensorFlow 5 .   pling from the models ().", "labels": [], "entities": []}, {"text": "For side information, we use simple words (taken from the descriptions in our test corpus) for closer examination.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Samples from our language models", "labels": [], "entities": []}, {"text": " Table 3: Perplexities of our models", "labels": [], "entities": []}]}