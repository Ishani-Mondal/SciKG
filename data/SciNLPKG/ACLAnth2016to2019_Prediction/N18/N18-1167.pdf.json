{"title": [{"text": "ELDEN: Improved Entity Linking using Densified Knowledge Graphs", "labels": [], "entities": [{"text": "Improved Entity Linking", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.671574592590332}]}], "abstractContent": [{"text": "Entity Linking (EL) systems aim to automatically map mentions of an entity in text to the corresponding entity in a Knowledge Graph (KG).", "labels": [], "entities": [{"text": "Entity Linking (EL)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7791772425174713}]}, {"text": "Degree of connectivity of an entity in the KG directly affects an EL system's ability to correctly link mentions in text to the entity in KG.", "labels": [], "entities": []}, {"text": "This causes many EL systems to perform well for entities well connected to other entities in KG, bringing into focus the role of KG density in EL.", "labels": [], "entities": []}, {"text": "In this paper, we propose Entity Linking using Densified Knowledge Graphs (ELDEN).", "labels": [], "entities": [{"text": "Entity Linking", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7813647389411926}]}, {"text": "ELDEN is an EL system which first densifies the KG with co-occurrence statistics from a large text corpus , and then uses the densified KG to train entity embeddings.", "labels": [], "entities": [{"text": "ELDEN", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8748620748519897}]}, {"text": "Entity similarity measured using these trained entity embeddings result in improved EL.", "labels": [], "entities": [{"text": "EL", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.953722357749939}]}, {"text": "ELDEN outperforms state-of-the-art EL system on benchmark datasets.", "labels": [], "entities": [{"text": "ELDEN", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.44046691060066223}]}, {"text": "Due to such densification, ELDEN performs well for sparsely connected entities in the KG too.", "labels": [], "entities": [{"text": "ELDEN", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9185749888420105}]}, {"text": "ELDEN's approach is simple, yet effective.", "labels": [], "entities": []}, {"text": "We have made ELDEN's code and data publicly available.", "labels": [], "entities": [{"text": "ELDEN's code and data", "start_pos": 13, "end_pos": 34, "type": "DATASET", "confidence": 0.7990891814231873}]}], "introductionContent": [{"text": "Entity Linking (EL) is the task of mapping mentions of an entity in text to the corresponding entity in Knowledge Graph (KG).", "labels": [], "entities": [{"text": "Entity Linking (EL)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8153717458248139}]}, {"text": "EL systems primarily exploit two types of information: (1) similarity of the mention to the candidate entity string, and (2) coherence between the candidate entity and other entities mentioned in the vicinity of the mention in text.", "labels": [], "entities": [{"text": "similarity", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9611567258834839}]}, {"text": "Coherence essentially measures how well the candidate entity is connected, either directly or indirectly, with other KG entities mentioned in the vicinity).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the following: \u2022 Is ELDEN's corpus co-occurrence statisticsbased densification helpful in disambiguating entities better?", "labels": [], "entities": [{"text": "ELDEN's corpus co-occurrence statisticsbased densification", "start_pos": 49, "end_pos": 107, "type": "DATASET", "confidence": 0.8741961220900217}]}, {"text": "(Sec. 6.1) \u2022 Where does ELDEN's selective densification of KG nodes link entities better?", "labels": [], "entities": []}, {"text": "(Sec. 6.3) Setup : ELDEN is implemented using Random Forest ensemble.", "labels": [], "entities": [{"text": "Random Forest ensemble", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.8257506489753723}]}, {"text": "Parameter values were set using CoNLL development set. was collected.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9819204211235046}, {"text": "CoNLL development set.", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.9692540367444357}]}, {"text": "We note that though some of the entities mentioned in this dataset are tenor more years old, we are able to collect, on an average more than 670 lines of web content.", "labels": [], "entities": []}, {"text": "Thus corpus proves to be a good source of additional links for densification, for both common and rare entities.", "labels": [], "entities": []}, {"text": "As Taneva and Weikum (2013) also note, it is not hard to find content about sparsely connected entities on the web.", "labels": [], "entities": []}, {"text": "The web corpus is analyzed for mentions and pseudo entities.", "labels": [], "entities": []}, {"text": "Co-occurrence matrix M is created 14 for mention and pseudo entities occurring within window of size 10 for PMI calculation . Edges are added from pseudo entities with positive PMI to mention of given entity.", "labels": [], "entities": [{"text": "PMI calculation", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.9284394085407257}]}, {"text": "In experiments we add edges from top 10 pseudo entities ordered by PMI values . Evaluation Dataset: In line with prior work on EL, we test the performance of ELDEN on CoNLL and TAC datasets.", "labels": [], "entities": [{"text": "TAC datasets", "start_pos": 177, "end_pos": 189, "type": "DATASET", "confidence": 0.7378631085157394}]}, {"text": "As this paper focuses on entity disambiguation, we tested ELDEN against datasets and baseline methods for disambiguation.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7232209891080856}]}, {"text": "We note that the entity disambiguation evaluation part of other recent datasets like ERD 2014 and TAC 2015 is exactly same as the TAC 2010 evaluation () . Training: ELDEN's parameters were tuned using training (development) sets of CoNLL and TAC datasets.", "labels": [], "entities": [{"text": "ERD 2014", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.8460009098052979}, {"text": "CoNLL and TAC datasets", "start_pos": 232, "end_pos": 254, "type": "DATASET", "confidence": 0.6911525726318359}]}, {"text": "CoNLL and TAC datasets consist of documents where mentions are marked and entity to which the mention links to, is specified.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8874939680099487}, {"text": "TAC datasets", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.6771668791770935}]}, {"text": "We use only mentions that link to a valid Wikipedia title (non NIL entities) and report performance on test set.", "labels": [], "entities": []}, {"text": "Some aspects of these datasets relevant to our experiments are provided below.", "labels": [], "entities": []}, {"text": "CoNLL: In CoNLL test set (5267 mentions), we report Precision of topmost candidate entity, aggregated overall mentions (P-micro) and aggregated overall documents (P-macro), i.e., if tp, fp and pare the individual true positives, false positives and precision for each document in a dataset of \u03b4 documents, then For CoNLL candidate entities, we use (Pershina et al., 2015) dataset . TAC: In TAC dataset, we report P-micro of topranked candidate entity on 1,020 mentions.", "labels": [], "entities": [{"text": "CoNLL test set", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.8644422888755798}, {"text": "Precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9970738887786865}, {"text": "precision", "start_pos": 249, "end_pos": 258, "type": "METRIC", "confidence": 0.9990205764770508}, {"text": "TAC dataset", "start_pos": 390, "end_pos": 401, "type": "DATASET", "confidence": 0.7230364233255386}]}, {"text": "Pmacro is not applicable to TAC as most documents have only one mention as query mention ( or 'mention to be linked').", "labels": [], "entities": [{"text": "Pmacro", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8898437023162842}, {"text": "TAC", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.7772766947746277}]}, {"text": "For TAC candidate entities, we index the Wikipedia word tokens and titles using solr . We index terms in (1) title of the entity, (2) title of another entity redirecting to the entity, and (3) names of anchors that point to the entity, inline with baselines.", "labels": [], "entities": [{"text": "TAC candidate entities", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8156598210334778}]}, {"text": "We are making this TAC candidate set publicly available.", "labels": [], "entities": [{"text": "TAC candidate set", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.8015478849411011}]}, {"text": "Baseline: Yamada16 Our baseline is the Yamada et al. system explained in Section 3.", "labels": [], "entities": []}, {"text": "Entity embedding distance measured using v e trained on the  input KG G is \u03c8 Yamada .  Train Test).", "labels": [], "entities": []}, {"text": "This is explained by analyzing distribution of densely-connected and sparsely connected entities in TAC and CoNLL datasets as presented in Table 6.", "labels": [], "entities": [{"text": "TAC", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.8492622971534729}, {"text": "CoNLL datasets", "start_pos": 108, "end_pos": 122, "type": "DATASET", "confidence": 0.8043850064277649}]}, {"text": "We see that CoNLL test set has almost half as densely-connected and half as sparsely connected entities, whereas in TAC test set, 63.6% are sparsely connected entities.", "labels": [], "entities": [{"text": "CoNLL test set", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.912782351175944}, {"text": "TAC test set", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.9085864027341207}]}, {"text": "This higher constitution of sparsely connected entities in TAC, explains ELDEN's better results in TAC relative to CoNLL dataset.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.927706241607666}]}, {"text": "As the number of sparsely connected entities is more than the number of denselyconnected entities inmost KGs (, our method is expected to be of significance for most KGs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Performance comparison with other recent EL  approaches. ELDEN matches best results in CoNLL  and outperforms the state-of-the-art in TAC dataset.  (Please see Section 6.1 for details and \u03c8 ELDEN++ row  of Table 5 for ELDEN results.)", "labels": [], "entities": [{"text": "ELDEN", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9355689883232117}, {"text": "CoNLL", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.7682790160179138}, {"text": "TAC dataset", "start_pos": 144, "end_pos": 155, "type": "DATASET", "confidence": 0.8448285162448883}]}, {"text": " Table 5: Ablation analysis involving various coher- ence measures (see", "labels": [], "entities": [{"text": "Ablation analysis", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.873374879360199}]}, {"text": " Table 6: Percentage of sparsely connected entities in  evaluation datasets. TAC has higher composition of  sparsely connected entities than CoNLL. Hence, EL- DEN results are better in TAC over CoNLL (Please see", "labels": [], "entities": [{"text": "EL- DEN", "start_pos": 155, "end_pos": 162, "type": "METRIC", "confidence": 0.8825337290763855}]}]}