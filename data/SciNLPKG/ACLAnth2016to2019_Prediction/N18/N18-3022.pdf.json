{"title": [], "abstractContent": [{"text": "This paper introduces a meaning representation for spoken language understanding.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.6832682291666666}]}, {"text": "The Alexa meaning representation language (AMRL), unlike previous approaches, which factor spoken utterances into domains, provides a common representation for how people communicate in spoken language.", "labels": [], "entities": [{"text": "Alexa meaning representation language (AMRL)", "start_pos": 4, "end_pos": 48, "type": "TASK", "confidence": 0.5891683357102531}]}, {"text": "AMRL is a rooted graph, links to a large-scale on-tology, supports cross-domain queries, fine-grained types, complex utterances and composition.", "labels": [], "entities": [{"text": "AMRL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8492886424064636}]}, {"text": "A spoken language dataset has been collected for Alexa, which contains \u223c 20k examples across eight domains.", "labels": [], "entities": [{"text": "Alexa", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9582267999649048}]}, {"text": "A version of this meaning representation was released to developers at a trade show in 2016.", "labels": [], "entities": []}], "introductionContent": [{"text": "Amazon has developed Alexa, a voice assistant that has been deployed across millions of devices and processes voice requests in multiple languages.", "labels": [], "entities": [{"text": "Amazon", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9089469909667969}]}, {"text": "This paper addresses improvements to the Alexa voice service, whose core capabilities (as measured by the number of supported intents and slots) has expanded more than four-fold over the last two years.", "labels": [], "entities": [{"text": "Alexa voice service", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.8986643155415853}]}, {"text": "In addition more than ten thousand voice skills have been created by third-party developers using the Alexa Skills Kit (ASK).", "labels": [], "entities": []}, {"text": "In order to continue this expansion, new voice experiences must be both accurate and capable of supporting complex interactions.", "labels": [], "entities": []}, {"text": "However, as the number of features has expanded, adding new features has become increasingly difficult for four primary reasons.", "labels": [], "entities": []}, {"text": "First, requests with a similar surface form may belong to different domains, which makes it challenging to add features without degrading the accuracy of existing domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9978355765342712}]}, {"text": "For example, similar linguistic phrases such as \"order mean echo dot\" (e.g., for Shopping) have a similar form to phrases used fora ride-hailing feature such as, \"Alexa, order me a taxi\".", "labels": [], "entities": []}, {"text": "The second challenge is that a fixed flat structure is unable to easily support certain features (), such as cross-domain queries or complex utterances, which cannot be clearly categorized into a given domain.", "labels": [], "entities": []}, {"text": "For example, \"Find me a restaurant near the sharks game\" contains both local businesses and sporting events and \"Play hunger games and turn the lights down to 3\" requires a representation that supports assigning an utterance to two intents.", "labels": [], "entities": []}, {"text": "The third challenge is that there is no mechanism to represent ambiguity, forcing the choice of a fixed interpretation for ambiguous utterances.", "labels": [], "entities": []}, {"text": "For example, \"Play Hunger Games\" could refer to an audiobook, a movie, or a soundtrack.", "labels": [], "entities": []}, {"text": "Finally, representations are not reused between skills, leading to the need for each developer to create a custom data and representations for their voice experiences.", "labels": [], "entities": []}, {"text": "In order to address these challenges and make Alexa more capable and accurate, we have developed two key components.", "labels": [], "entities": [{"text": "Alexa", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.8753288388252258}]}, {"text": "The first is the Alexa ontology, a large hierarchical ontology that contains fine-grained types, properties, actions and roles.", "labels": [], "entities": []}, {"text": "Actions represent a predicate that determines what the agent should do, roles express the arguments to an action, types categorize textual mentions and properties are relations between type mentions.", "labels": [], "entities": []}, {"text": "The second component is the Alexa Meaning Representation Language (AMRL), a graph-based domain and language independent meaning representation that can capture the meaning of spoken language utterances to intelligent assistants.", "labels": [], "entities": [{"text": "Alexa Meaning Representation Language (AMRL)", "start_pos": 28, "end_pos": 72, "type": "TASK", "confidence": 0.7284727650029319}]}, {"text": "AMRL is a rooted graph where action, operators, relations and classes are labeled vertices and properties and roles are labeled edges.", "labels": [], "entities": []}, {"text": "Unlike typical representations for spoken language understanding (SLU), which factors language understanding into the prediction of intents (nonoverlapping actions) and slots (e.g., named entities)), our representation is grounded in the Alexa ontology, which provides a common semantic representation for spoken language understanding and can directly represent ambiguity, complex nested utterances and crossdomain queries.", "labels": [], "entities": [{"text": "spoken language understanding (SLU)", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.839290847380956}]}, {"text": "Unlike similar meaning representations such as AMR (, AMRL is designed to be cross-lingual, explicitly represent fine-grained entity types, logical statements, spatial prepositions and relationships and support type mentions.", "labels": [], "entities": []}, {"text": "Examples of AMRL and the SLU representations can be seen in.", "labels": [], "entities": [{"text": "AMRL", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.774728536605835}]}, {"text": "The AMRL has been released via Alexa Skills Kit (ASK) built-in intents and slots in 2016 at a developers conference, offering coverage for eight of the \u223c20 SLU domains . In addition to these domains, we have demonstrated that the AMRL can cover a wide range of additional utterances by annotating a sample from all first and thirdparty applications.", "labels": [], "entities": []}, {"text": "We have manually annotated data for 20k examples using the Alexa ontology.", "labels": [], "entities": [{"text": "Alexa ontology", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.9739410281181335}]}, {"text": "This data includes the annotation of \u223c100 actions, \u223c500 types, \u223c20 roles and \u223c172 properties.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data has been collected for the AMRL across many spoken language use-cases.", "labels": [], "entities": [{"text": "AMRL", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.5960213541984558}]}, {"text": "The current domains that are supported include music, books, video, local search, weather and calendar.", "labels": [], "entities": []}, {"text": "We have prototyped mechanisms to speedup annotation via paraphrasing and conversion from our current SLU representation, in order to leverage the much larger data available.", "labels": [], "entities": []}, {"text": "The primary mechanism we have for data-acquisition is via manual annotation.", "labels": [], "entities": []}, {"text": "Tools have been developed in order to acquire the full graph annotated with all the properties, classes, actions and operators.", "labels": [], "entities": []}, {"text": "AMRL manual annotation is performed by data annotators in four stages.", "labels": [], "entities": [{"text": "AMRL manual annotation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7342493732770284}]}, {"text": "In the first stage an action is selected, for example ACTIVATEACTION in.", "labels": [], "entities": []}, {"text": "The second stage defines the text spans in an utterance that link to a class in the ontology (e.g., \"michael jackson\" is a Musician type and \"thriller\" and \"song\" are MusicRecording types, the first is a .name mention, while the latter is a .type mention.", "labels": [], "entities": []}, {"text": "The third stage creates connections between the classes and defines any missing nodes in the graph.", "labels": [], "entities": []}, {"text": "In the final stage a skilled annotator reviews the graph for mistakes and and re-annotates it if necessary.", "labels": [], "entities": []}, {"text": "There is a visualization of the semantic annotation available, enabling an annotator to verify that they have built the graph in a semantically accurate manner.", "labels": [], "entities": []}, {"text": "Manual annotation happens at the rate of 40 per hour.", "labels": [], "entities": []}, {"text": "The manually annotated dataset contains \u223c20k annotated utterances and contains 93 unique actions, (a) AMRL for \"when it is raining, turnoff the sprinklers\" (b) AMRL for \"when it is three p.m., turn on the lights.\"", "labels": [], "entities": [{"text": "AMRL", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9892455339431763}, {"text": "AMRL", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9324054718017578}]}, {"text": "448 types, 172 properties and 23 roles.", "labels": [], "entities": []}], "tableCaptions": []}