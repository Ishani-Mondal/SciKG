{"title": [{"text": "Using Morphological Knowledge in Open-Vocabulary Neural Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Languages with productive morphology pose problems for language models that generate words from a fixed vocabulary.", "labels": [], "entities": []}, {"text": "Although character-based models allow any possible word type to be generated, they are linguistically na\u00efve: they must discover that words exist and are delimited by spaces-basic linguistic facts that are builtin to the structure of word-based models.", "labels": [], "entities": []}, {"text": "We introduce an open-vocabulary language model that incorporates more sophisticated linguistic knowledge by predicting words using a mixture of three gen-erative processes: (1) by generating words as a sequence of characters, (2) by directly generating full word forms, and (3) by generating words as a sequence of morphemes that are combined using a handwritten morphological analyzer.", "labels": [], "entities": []}, {"text": "Experiments on Finnish, Turkish, and Russian show that our model outperforms character sequence models and other strong baselines on intrinsic and extrinsic measures.", "labels": [], "entities": []}, {"text": "Furthermore, we show that our model learns to exploit morphological knowledge encoded in the analyzer, and, as a byproduct, it can perform effective unsupervised morphological disambiguation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language modelling of morphologically rich languages is particularly challenging due to the vast set of potential word forms and the sparsity with which they appear in corpora.", "labels": [], "entities": []}, {"text": "Traditional closed vocabulary models are unable to produce word forms unseen in training data and unable to generalize sub-word patterns found in data.", "labels": [], "entities": []}, {"text": "The most straightforward solution is to treat language as a sequence of characters).", "labels": [], "entities": []}, {"text": "However, models that operate at two levels-a character level and a word levelhave better performance).", "labels": [], "entities": []}, {"text": "Another solution is to use morphological information, which has shown benefits in non-neural models (.", "labels": [], "entities": []}, {"text": "In this paper, we present a model that combines these approaches in a fully neural framework.", "labels": [], "entities": []}, {"text": "Our model incorporates explicit morphological knowledge (e.g. from a finite-state morphological analyzer/generator) into a neural language model, combining it with existing word-and characterlevel modelling techniques, in order to create a model capable of successfully modelling morphologically complex languages.", "labels": [], "entities": []}, {"text": "In particular, our model achieves three desirable properties.", "labels": [], "entities": []}, {"text": "First, it conditions on all available (intrasentential) context, allowing it, in principle, to capture long-range dependencies, such as that the verb agreement between \"students\" and \"are\" in the sentence \"The students who studied the hardest are getting the highest grades\".", "labels": [], "entities": []}, {"text": "While traditional n-gram based language models lack this property, RNN-based language models fulfill it.", "labels": [], "entities": []}, {"text": "Second, it explicitly captures morphological variation, allowing sharing of information between variants of the same word.", "labels": [], "entities": []}, {"text": "This allows faster, smoother training as well as improved predictive generalization.", "labels": [], "entities": [{"text": "predictive generalization", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.9568195044994354}]}, {"text": "For example, if the model sees the phrase \"gorped the ball\" in data, it is able to infer that \"gorping the ball\" is also likely to be valid.", "labels": [], "entities": []}, {"text": "Similarly, the model is capable of understanding that morphological consistency within noun phrases is important.", "labels": [], "entities": []}, {"text": "For example in Russian, one might say malen'kaya ch\u00ebrniya koshka (\"small black cat\", nominative), or malen'kuyu ch\u00ebrniyu koshku (accusative), but malen'kiy ch\u00ebr-nuyu koshke (mixing nominative, accusative and dative) would have much lower probability.", "labels": [], "entities": []}, {"text": "Third, the language model seamlessly handles out of vocabulary items and their morphological variants.", "labels": [], "entities": []}, {"text": "For example, even if the word Obama was never seen in a Russian corpus, we expect Ya dal eto prezidentu Obame (\"I gave it to presi-dent Obama\") to have higher probability using the dative Obame than Ya dal eto prezidentu Obama, which uses the nominative.", "labels": [], "entities": []}, {"text": "The model can also learn to decline proper nouns, including OOVs.", "labels": [], "entities": []}, {"text": "Here it can recognize that dal (\"gave\") requires a dative, and that nouns ending with \"a\" generally do not meet that requirement.", "labels": [], "entities": []}, {"text": "In order to capture these properties, our model combines two pieces: an alternative embedding module that uses sub-word information such as character and morpheme-level information, and a generation module that allows us to output words at the word, morpheme, or character-level.", "labels": [], "entities": []}, {"text": "The embedding module allows for the model to share information between morphological variants of surface forms, and produce sensible word embeddings for tokens never seen during training.", "labels": [], "entities": []}, {"text": "The generation model allows us to emit tokens never seen during training, either by combining a lemma and a sequence of affixes to create a novel surface form, or by directly spelling out the desired word character by character.", "labels": [], "entities": []}, {"text": "We then demonstrate the effectiveness both intrinsically, showing reduced perplexity on several morphologically rich languages, and extrinsically on machine translation and morphological disambiguation tasks.", "labels": [], "entities": [{"text": "machine translation and morphological disambiguation", "start_pos": 149, "end_pos": 201, "type": "TASK", "confidence": 0.6371369004249573}]}], "datasetContent": [{"text": "We demonstrate the effectiveness of our model by experimenting on three languages: Finnish, Turkish, and Russian.", "labels": [], "entities": []}, {"text": "For Finnish we use version 8 of the Europarl corpus, for Turkish we use the SE-TIMES2 corpus, and for Russian we use version 12 of the News Commentary corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9900104701519012}, {"text": "SE-TIMES2 corpus", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.7390310913324356}, {"text": "News Commentary corpus", "start_pos": 135, "end_pos": 157, "type": "DATASET", "confidence": 0.817749003569285}]}, {"text": "Statistics of our experimental corpora can be found in.", "labels": [], "entities": []}, {"text": "Each data set was pre-processed by UNKing all but the top \u224820k words and lemmas by frequency.", "labels": [], "entities": [{"text": "UNKing", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.7888460159301758}]}, {"text": "No characters or affixes were UNKed.", "labels": [], "entities": [{"text": "UNKed", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.6742026805877686}]}, {"text": "This step is not strictly required-our model is, after all, capable of producing arbitrary words-but it speeds up training immensely by reducing the size of the word and lemma softmaxes.", "labels": [], "entities": []}, {"text": "Since the morphology and/or character-level embeddings can still capture information about the original forms of these words, the degradation in modelling performance is minimal.", "labels": [], "entities": []}, {"text": "For morphological analysis we use Omorfi 2 for Finnish, the analyzer of for Turkish, and PyMorphy 3 for Russian.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.805914968252182}, {"text": "PyMorphy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9051722884178162}]}, {"text": "In addition to evaluating our model intrinsically using perplexity, we evaluate it on two downstream tasks.", "labels": [], "entities": []}, {"text": "The first is machine translation between English and Turkish.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.5907557606697083}]}, {"text": "The second is Turkish morphological analysis disambiguation.: Intrinsic evaluation of language models for three morphologically rich languages.", "labels": [], "entities": [{"text": "Turkish morphological analysis disambiguation.", "start_pos": 14, "end_pos": 60, "type": "TASK", "confidence": 0.691414013504982}]}, {"text": "Entropy for each test set is given in bits per character on the tokenized data.", "labels": [], "entities": [{"text": "Entropy", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9842701554298401}]}, {"text": "Lower is better, with 0.0 being perfect.", "labels": [], "entities": []}, {"text": "Kom\u00fcnizm pe\u00b8sindepe\u00b8sinde ko\u00b8sanko\u00b8san Arnavut pek yok Parlakl\u0131\u02d8 g\u0131n\u0131 kaybeden m\u00fccevher: Kirlilik Karadeniz'i esir al\u0131yor Olaylar\u0131n bask\u0131s\u0131yla kar\u00b8s\u0131la\u00b8sankar\u00b8s\u0131la\u00b8kar\u00b8s\u0131la\u00b8san rejim tutumunu yava\u00b8s\u00e7ayava\u00b8s\u00e7a yumu\u00b8satt\u0131yumu\u00b8satt\u0131, 1991 y\u0131l\u0131nda \u00e7ok partili \u2192 se\u00e7imleri d\u00fczenledi ve sonunda da ertesi y\u0131l t\u00fcmden iktidar\u0131 b\u0131rakt\u0131.", "labels": [], "entities": []}, {"text": "Southeast European Times i\u00e7in Belgrad'dan Dusan Kosanovi\u00e7'in haberi -24/06/04 23 Temmuz'dan bu yana Balkanlar'la ilgili i\u00b8si\u00b8s ve ekonomi haberlerine genel bak\u0131\u00b8sbak\u0131\u00b8s: AB'nin Geni\u00b8slemedenGeni\u00b8slemeden Sorumlu Komisyon \u00dcyesi Olli Rehn (solda) Arnavutluk Ba\u00b8sbakan\u0131Ba\u00b8sbakan\u0131 Sali \u2192 Beri\u00b8saBeri\u00b8sa ile 15 Mart Per\u00b8sembePer\u00b8sembe g\u00fcn\u00fc Tiran'da bir araya geldi.: Some examples of Turkish sentence on which our morphological model heavily outperforms the baseline RNNLM (top) and some examples of the opposite (bottom).", "labels": [], "entities": []}, {"text": "The sentences that our model performs well on have many particularly rare words, whereas the sentences the RNNLM performs well on were seen hundreds or thousands of times in the training corpus.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.8058175444602966}]}, {"text": "Words in bold were seen fewer than 25 times in the training corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of our data sets. Each cell indi- cates the number of sentences and the number of  words in each set.", "labels": [], "entities": []}, {"text": " Table 2: Intrinsic evaluation of language models for three morphologically rich languages. Entropy for  each test set is given in bits per character on the tokenized data. Lower is better, with 0.0 being perfect.", "labels": [], "entities": [{"text": "Entropy", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9867632985115051}]}, {"text": " Table 3: Some examples of Turkish sentence on which our morphological model heavily outperforms  the baseline RNNLM (top) and some examples of the opposite (bottom). The sentences that our model  performs well on have many particularly rare words, whereas the sentences the RNNLM performs well  on were seen hundreds or thousands of times in the training corpus. Words in bold were seen fewer than  25 times in the training corpus. Arrows indicate line wrapping.", "labels": [], "entities": [{"text": "line wrapping", "start_pos": 449, "end_pos": 462, "type": "TASK", "confidence": 0.7200034856796265}]}, {"text": " Table 4: Machine Translation Results", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8857421278953552}]}, {"text": " Table 5: Morphological disambiguation accuracy results for Turkish.", "labels": [], "entities": [{"text": "Morphological disambiguation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8900090754032135}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9605794548988342}]}]}