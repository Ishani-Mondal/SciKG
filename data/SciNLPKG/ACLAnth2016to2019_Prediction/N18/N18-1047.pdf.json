{"title": [{"text": "Neural Tensor Networks with Diagonal Slice Matrices", "labels": [], "entities": [{"text": "Neural Tensor Networks", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8372470537821451}]}], "abstractContent": [{"text": "Although neural tensor networks (NTNs) have been successful in many natural language processing tasks, they require a large number of parameters to be estimated, which often results in overfitting and long training times.", "labels": [], "entities": []}, {"text": "We address these issues by applying eigendecompo-sition to each slice matrix of a tensor to reduce the number of parameters.", "labels": [], "entities": []}, {"text": "We evaluate our proposed NTN models in two tasks.", "labels": [], "entities": []}, {"text": "First, the proposed models are evaluated in a knowledge graph completion task.", "labels": [], "entities": [{"text": "knowledge graph completion task", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.6944024637341499}]}, {"text": "Second, a recursive NTN (RNTN) extension of the proposed models is evaluated on a logical reasoning task.", "labels": [], "entities": []}, {"text": "The experimental results show that our proposed models learn better and faster than the original (R)NTNs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Alongside the nonlinear activation functions, linear mapping by matrix multiplication is an essential component of neural network (NN) models, as it determines the feature interaction and thus the expressiveness of models.", "labels": [], "entities": []}, {"text": "In addition to the matrix-based mapping, neural tensor networks (NTNs)) employ a 3-dimensional tensor to capture direct interactions among input features.", "labels": [], "entities": []}, {"text": "Due to the large expressive capacity of 3D tensors, NTNs have been successful in an array of natural language processing (NLP) and machine learning tasks, including knowledge graph completion (KGC)), sentiment analysis (, and reasoning with logical semantics.", "labels": [], "entities": [{"text": "knowledge graph completion (KGC))", "start_pos": 165, "end_pos": 198, "type": "TASK", "confidence": 0.7903570433457693}, {"text": "sentiment analysis", "start_pos": 200, "end_pos": 218, "type": "TASK", "confidence": 0.959762454032898}]}, {"text": "However, since a 3D tensor has a large number of parameters, NTNs need longer time to train than other NN models.", "labels": [], "entities": []}, {"text": "Moreover, the millions of parameters often make the model suffer from overfitting (.", "labels": [], "entities": []}, {"text": "To solve these problems, we propose two new parameter reduction techniques for NTNs.", "labels": [], "entities": [{"text": "parameter reduction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7821276485919952}]}, {"text": "These techniques drastically decrease the number of parameters in an NTN without diminishing its expressiveness.", "labels": [], "entities": []}, {"text": "We use the matrix decomposition techniques that are utilized for KGC in and.", "labels": [], "entities": []}, {"text": "imposed a constraint that a matrix in the bilinear term in their model had to be diagonal.", "labels": [], "entities": []}, {"text": "As mentioned in a subsequent section, this is essentially equal to assuming that the matrix be symmetric and performing eigendecomposition.", "labels": [], "entities": []}, {"text": "also applied eigendecomposition to a matrix by regarding it as the real part of a normal matrix.", "labels": [], "entities": []}, {"text": "Following these studies, we perform simultaneous diagonalization on all slice matrices of a NTN tensor.", "labels": [], "entities": []}, {"text": "As a result, mapping by a 3D (n \u00d7 n \u00d7 k) tensor is replaced with an array of k \"triple inner products\" of two input vectors and a weight vector.", "labels": [], "entities": []}, {"text": "Thus, we obtain two new NTN models where the number of parameters is reduced from O(n 2 k) to O(nk).", "labels": [], "entities": []}, {"text": "On a KGC task, these parameter-reduced NTNs (NTN-Diag and NTN-Comp) alleviate overfitting and outperform the original NTN.", "labels": [], "entities": []}, {"text": "Moreover, our proposed NTNs can learn faster than the original NTN.", "labels": [], "entities": []}, {"text": "We also show that our proposed models perform better and learn faster in a recursive setting by examining a logical reasoning task.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment, an example is a pair of propositional formulas, and its class label is the seven relation types between the pair.", "labels": [], "entities": []}, {"text": "We generated examples following the protocol described in, with the exception that the formulas are restricted to CNF or DNF, as mentioned above.", "labels": [], "entities": []}, {"text": "We obtained 62,589 training examples, 13,413 validation examples, and 55,150 test examples.", "labels": [], "entities": []}, {"text": "Each formula in the training and validation examples contains up to four logical operators, whereas those in the test examples have: Result of logical inference for Tests 1-12.", "labels": [], "entities": [{"text": "Result", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9027439951896667}]}, {"text": "Example in Test n has n logical operators in either or both left and right formulas.", "labels": [], "entities": []}, {"text": "Each score is the average accuracy of five trials of the \u03bb that achieved best performance on validation set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9979718327522278}]}, {"text": "\"Majority class\" denotes the ratio of the majority class (relation \"#\", i.e., Independence; see).", "labels": [], "entities": [{"text": "Independence", "start_pos": 78, "end_pos": 90, "type": "METRIC", "confidence": 0.9930272698402405}]}], "tableCaptions": [{"text": " Table 3: Mean Reciprocal Rank (MRR) and Hits@n for the models tested on WN18 and FB15k. MRR is reported  in the raw and filtered settings. Hits@n metrics are percentages of test examples that lie in the top n ranked results.  We report Hits@n in the filtered setting.  *  Results are those in (Trouillon et al., 2016)", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9618268807729086}, {"text": "WN18", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9689152836799622}, {"text": "FB15k", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.8503445982933044}, {"text": "MRR", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.5994808077812195}]}, {"text": " Table 7: Result of logical inference for Tests 1-12. Example in Test n has n logical operators in either or both  left and right formulas. Each score is the average accuracy of five trials of the \u03bb that achieved best performance  on validation set. \"Majority class\" denotes the ratio of the majority class (relation \"#\", i.e., Independence; see", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9862978458404541}, {"text": "Independence", "start_pos": 328, "end_pos": 340, "type": "METRIC", "confidence": 0.995652437210083}]}]}