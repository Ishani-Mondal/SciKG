{"title": [{"text": "Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose a novel end-to-end neural architecture for ranking candidate answers , that adapts a hierarchical recurrent neu-ral network and a latent topic clustering module.", "labels": [], "entities": []}, {"text": "With our proposed model, a text is encoded to a vector representation from an word-level to a chunk-level to effectively capture the entire meaning.", "labels": [], "entities": []}, {"text": "In particular, by adapting the hierarchical structure, our model shows very small performance degradations in longer text comprehension while other state-of-the-art recurrent neural network models suffer from it.", "labels": [], "entities": []}, {"text": "Additionally, the latent topic clustering module extracts semantic information from target samples.", "labels": [], "entities": [{"text": "latent topic clustering", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.5907464126745859}]}, {"text": "This clustering module is useful for any text related tasks by allowing each data sample to find its nearest topic cluster, thus helping the neural network model analyze the entire data.", "labels": [], "entities": []}, {"text": "We evaluate our models on the Ubuntu Dialogue Corpus and consumer electronic domain question answering dataset, which is related to Samsung products.", "labels": [], "entities": [{"text": "Ubuntu Dialogue Corpus", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.9242555697758993}, {"text": "consumer electronic domain question answering", "start_pos": 57, "end_pos": 102, "type": "TASK", "confidence": 0.5108422696590423}]}, {"text": "The proposed model shows state-of-the-art results for ranking question-answer pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently neural network architectures have shown great success in many machine learning fields such as image classification, speech recognition, machine translation, chat-bot, question answering, and other task-oriented areas.", "labels": [], "entities": [{"text": "image classification", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.7338337749242783}, {"text": "speech recognition", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.7307847738265991}, {"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.8159346580505371}, {"text": "question answering", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.848847359418869}]}, {"text": "Among these, the automatic question answering (QA) task has long been considered a primary objective of artificial intelligence.", "labels": [], "entities": [{"text": "question answering (QA) task", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.8565350224574407}]}, {"text": "In the commercial sphere, the QA task is usually tackled by using pre-organized knowledge bases and/or by using information retrieval (IR) based methods, which are applied in popular intelligent voice agents such as Siri, Alexa, and Google Assistant (from Apple, Amazon, and Google, respectively).", "labels": [], "entities": [{"text": "QA task", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9145215153694153}]}, {"text": "Another type of advanced QA systems is IBM's Watson who builds knowledge bases from unstructured data.", "labels": [], "entities": []}, {"text": "These raw data are also indexed in search clusters to support user queries.", "labels": [], "entities": []}, {"text": "In academic literature, researchers have intensely studied sentence pair ranking task which is core technique in QA system.", "labels": [], "entities": [{"text": "sentence pair ranking task", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.7737850248813629}]}, {"text": "The ranking task selects the best answer among candidates retrieved from knowledge bases or IR based modules.", "labels": [], "entities": []}, {"text": "Many neural network architectures with endto-end learning methods are proposed to address this task ().", "labels": [], "entities": []}, {"text": "These works focus on matching sentence-level text pair (.", "labels": [], "entities": []}, {"text": "Therefore, they have limitations in understanding longer text such as multi-turn dialogue and explanatory document, resulting in performance degradation on ranking as the length of the text become longer.", "labels": [], "entities": []}, {"text": "With the advent of the huge multi-turn dialogue corpus (, researchers have proposed neural network models to rank longer text pair ().", "labels": [], "entities": []}, {"text": "These techniques are essential for capturing context information in multi-turn conversation or understanding multiple sentences in explanatory text.", "labels": [], "entities": []}, {"text": "In this paper, we focus on investigating a novel neural network architecture with additional data clustering module to improve the performance in ranking answer candidates which are longer than a single sentence.", "labels": [], "entities": []}, {"text": "This work can be used not only for the QA ranking task, but also to evaluate the relevance of next utterance with given dialogue generated from the dialogue model.", "labels": [], "entities": [{"text": "QA ranking task", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.827780524889628}]}, {"text": "The key contributions of our work are as follows: First, we introduce a Hierarchical Recurrent Dual Encoder (HRDE) model to effectively calculate the affinity among question-answer pairs to determine the ranking.", "labels": [], "entities": []}, {"text": "By encoding texts from an word-level to a chunk-level with hierarchi-cal architecture, the HRDE prevents performance degradations in understanding longer texts while other state-of-the-art neural network models suffer.", "labels": [], "entities": []}, {"text": "Second, we propose a Latent Topic Clustering (LTC) module to extract latent information from the target dataset, and apply these additional information in end-to-end training.", "labels": [], "entities": []}, {"text": "This module allows each data sample to find its nearest topic cluster, thus helping the neural network model analyze the entire data.", "labels": [], "entities": []}, {"text": "The LTC module can be combined to any neural network as a source of additional information.", "labels": [], "entities": []}, {"text": "This is a novel approach using latent topic cluster information for the QA task, especially by applying the combined model of HRDE and LTC to the QA pair ranking task.", "labels": [], "entities": []}, {"text": "Extensive experiments are conducted to investigate efficacy and properties of the proposed model.", "labels": [], "entities": []}, {"text": "Our proposed model outperforms previous state-of-the-art methods in the Ubuntu Dialogue Corpus, which is one of the largest text pair scoring datasets.", "labels": [], "entities": [{"text": "Ubuntu Dialogue Corpus", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.931003193060557}]}, {"text": "We also evaluate the model on real world QA data crawled from crowd-QA web pages and from Samsung's official web pages.", "labels": [], "entities": [{"text": "QA data crawled from crowd-QA web pages", "start_pos": 41, "end_pos": 80, "type": "DATASET", "confidence": 0.8655915686062404}, {"text": "Samsung's official web pages", "start_pos": 90, "end_pos": 118, "type": "DATASET", "confidence": 0.9166787624359131}]}, {"text": "Our model also shows the best results for the QA data when compared to previous neural network based models.", "labels": [], "entities": [{"text": "QA data", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.7039934545755386}]}], "datasetContent": [{"text": "To implement the RDE model, we use two single layer Gated Recurrent Unit (GRU) () with 300 hidden units . Each GRU is used to encode {context} and {response}, respectively.", "labels": [], "entities": [{"text": "RDE", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9311744570732117}]}, {"text": "The weight for the two GRU are shared.", "labels": [], "entities": [{"text": "GRU", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.6474523544311523}]}, {"text": "The hidden units weight matrix of the GRU are initialized using orthogonal weights (, while input embedding weight matrix is initialized using a pre-trained embedding vector, the Glove (), with 300 dimension.", "labels": [], "entities": [{"text": "GRU", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.8277216553688049}]}, {"text": "The vocabulary size is 144,953 and 183,045 for the Ubuntu-v1/v2 case, respectively.", "labels": [], "entities": []}, {"text": "We use the Adam optimizer (, with gradients clipped with norm value 1.", "labels": [], "entities": []}, {"text": "The maximum time step for calculating gradient of the RNN is determined according to the input data statistics in For the HRDE model, we use two single layer GRU with 300 hidden units for word-level RNN part, and another two single layer GRU with 300 hidden units for chunk-level RNN part.", "labels": [], "entities": []}, {"text": "The weight of the GRU is shared within the same hierarchical part, word-level and chunk-level.", "labels": [], "entities": []}, {"text": "The other settings are the same with the RDE model case.", "labels": [], "entities": [{"text": "RDE model", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.6967542469501495}]}, {"text": "As for the combined model with the (H)RDE and the LTC, we choose the latent topic memory dimensions as 256 in both ubuntu-v1 and ubuntuv2.", "labels": [], "entities": [{"text": "LTC", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.9039686918258667}]}, {"text": "The number of the cluster in LTC module is decided to 3 for both the RDE-LTC and the HRDE-LTC cases.", "labels": [], "entities": [{"text": "RDE-LTC", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.7760157585144043}]}, {"text": "In HRDE-LTC case, we applied LTC module to the {context} part because we think it is longer having enough information to be clustered with.", "labels": [], "entities": []}, {"text": "All of these hyper-parameters are selected from additional parameter searching experiments.", "labels": [], "entities": []}, {"text": "The dropout () is applied for the purpose of regularization with the ratio of: 0.2 for the RNN in the RDE and the RDE-LTC, 0.3 for the word-level RNN part in the HRDE and the HRDE-LTC, 0.8 for the latent topic memory in the RDE-LTC and the HRDE-LTC.", "labels": [], "entities": [{"text": "regularization", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.9520785808563232}, {"text": "HRDE", "start_pos": 162, "end_pos": 166, "type": "DATASET", "confidence": 0.9222609400749207}, {"text": "HRDE-LTC", "start_pos": 240, "end_pos": 248, "type": "DATASET", "confidence": 0.9581307172775269}]}, {"text": "We need to mention that our implementation of the RDE module has the same architecture as the LSTM model () in ubuntuv1/v2 experiments case.", "labels": [], "entities": []}, {"text": "It is also the same architecture with the RNN model () in ubuntu-v2 experiment case.", "labels": [], "entities": []}, {"text": "We implement the same model ourselves, because we need a baseline model to compare with other proposed models such as the RDE-LTC, HRDE and HRDE-LTC.", "labels": [], "entities": [{"text": "RDE-LTC", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.8485952019691467}, {"text": "HRDE", "start_pos": 131, "end_pos": 135, "type": "DATASET", "confidence": 0.907528281211853}, {"text": "HRDE-LTC", "start_pos": 140, "end_pos": 148, "type": "DATASET", "confidence": 0.9400186538696289}]}, {"text": "To test the Samsung QA dataset, we use the same implementation of the model (RDE, RDE-LTC, HRDE and HRDE-LTC) used in testing the Ubuntu dataset.", "labels": [], "entities": [{"text": "Samsung QA dataset", "start_pos": 12, "end_pos": 30, "type": "DATASET", "confidence": 0.9305515885353088}, {"text": "HRDE", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.6946719288825989}, {"text": "HRDE-LTC", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.8520374894142151}, {"text": "Ubuntu dataset", "start_pos": 130, "end_pos": 144, "type": "DATASET", "confidence": 0.9018497467041016}]}, {"text": "Only the differences are, we use 100 hidden units for the RDE and the RDE-LTC, 300 hidden units for the HRDE and 200 hidden units for the HRDE-LTC, and the vocabulary size of 28,848.", "labels": [], "entities": [{"text": "RDE", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8634764552116394}, {"text": "RDE-LTC", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.8820984363555908}, {"text": "HRDE", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.9325785040855408}, {"text": "HRDE-LTC", "start_pos": 138, "end_pos": 146, "type": "DATASET", "confidence": 0.9142005443572998}]}, {"text": "As for the combined model with the (H)RDE and LTC, the dimensions of the latent topic memory is 64 and the number of latent cluster is 4.", "labels": [], "entities": [{"text": "LTC", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.853306770324707}]}, {"text": "We chose best performing hyperparameter of each model by additional extensive hyper-parameter search experiments.", "labels": [], "entities": []}, {"text": "All of the code developed for the empirical results are available via web repository 5 .  We regards all the tasks as selecting the best answer among text candidates for the given question.", "labels": [], "entities": []}, {"text": "Following the previous work (), we report model performance as recall at k (R@k) relevant texts among given 2 or 10 candidates (e.g., 1 in 2 R@1).", "labels": [], "entities": [{"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9972631931304932}]}, {"text": "Though this metric is useful for ranking task, R@1 metric is also meaningful for classifying the best relevant text.", "labels": [], "entities": [{"text": "R@1 metric", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9335145652294159}]}, {"text": "Each model we implement is trained multiple times (10 and 15 times for Ubuntu and the Samsung QA datasets in our experiments, respectively) with random weight initialization, which largely influences performance of neural network model.", "labels": [], "entities": [{"text": "Samsung QA datasets", "start_pos": 86, "end_pos": 105, "type": "DATASET", "confidence": 0.6902572810649872}]}, {"text": "Hence we report model performance as mean and standard derivation values (Mean\u00b1Std).", "labels": [], "entities": [{"text": "Mean\u00b1Std", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9344863692919413}]}], "tableCaptions": [{"text": " Table 1: Properties of the Ubuntu and Samsung QA dataset. The message and response are {context}, {response}  in Ubuntu and {question}, {answer} in the Samsung QA dataset.", "labels": [], "entities": [{"text": "Samsung QA dataset", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.9120890100797018}, {"text": "Samsung QA dataset", "start_pos": 153, "end_pos": 171, "type": "DATASET", "confidence": 0.932876706123352}]}, {"text": " Table 3: Model performance results for the Ubuntu- v1 dataset. Models [1-4] are from (Lowe et al., 2015;  Kadlec et al., 2015; Wang and Jiang, 2016; Wang et al.,  2017), respectively.", "labels": [], "entities": [{"text": "Ubuntu- v1 dataset", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.8479784727096558}]}, {"text": " Table 4: Model performance results for the Ubuntu-v2  dataset. Models [1,3-6] are from (Lowe et al., 2015;  Wang and Jiang, 2016; Wang et al., 2017; Baudi\u0161 et al.,  2016; Tan et al., 2015), respectively.", "labels": [], "entities": [{"text": "Ubuntu-v2  dataset", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.9226394891738892}]}, {"text": " Table 5: Model performance results for the Samsung  QA dataset.", "labels": [], "entities": [{"text": "Samsung  QA dataset", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.937507172425588}]}, {"text": " Table 6: The RDE-LTC model results with different  numbers of latent clusters. \"Cluster 1\" is the baseline  model, RDE.", "labels": [], "entities": []}]}