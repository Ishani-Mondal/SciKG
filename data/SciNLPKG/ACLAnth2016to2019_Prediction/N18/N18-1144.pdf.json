{"title": [{"text": "Tracking State Changes in Procedural Text: A Challenge Dataset and Models for Process Paragraph Comprehension", "labels": [], "entities": [{"text": "Tracking State Changes in Procedural Text", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8627387881278992}]}], "abstractContent": [{"text": "We present anew dataset and models for comprehending paragraphs about processes (e.g., photosynthesis), an important genre of text describing a dynamic world.", "labels": [], "entities": []}, {"text": "The new dataset, ProPara, is the first to contain natural (rather than machine-generated) text about a changing world along with a full annotation of entity states (location and existence) during those changes (81k datapoints).", "labels": [], "entities": [{"text": "ProPara", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.9624955654144287}, {"text": "81k datapoints", "start_pos": 211, "end_pos": 225, "type": "DATASET", "confidence": 0.7329711019992828}]}, {"text": "The end-task, tracking the location and existence of entities through the text, is challenging because the causal effects of actions are often implicit and need to be inferred.", "labels": [], "entities": [{"text": "tracking the location and existence of entities through the text", "start_pos": 14, "end_pos": 78, "type": "TASK", "confidence": 0.7411683797836304}]}, {"text": "We find that previous models that have worked well on synthetic data achieve only mediocre performance on ProPara, and introduce two new neural models that exploit alternative mechanisms for state prediction, in particular using LSTM input encoding and span prediction.", "labels": [], "entities": [{"text": "ProPara", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9708112478256226}, {"text": "state prediction", "start_pos": 191, "end_pos": 207, "type": "TASK", "confidence": 0.7129645496606827}, {"text": "LSTM input encoding", "start_pos": 229, "end_pos": 248, "type": "TASK", "confidence": 0.666394035021464}, {"text": "span prediction", "start_pos": 253, "end_pos": 268, "type": "TASK", "confidence": 0.7170859277248383}]}, {"text": "The new models improve accuracy by up to 19%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995731711387634}]}, {"text": "The dataset and models are available to the community at http://data.allenai.org/propara.", "labels": [], "entities": []}], "introductionContent": [{"text": "Building a reading comprehension (RC) system that is able to read a text document and to answer questions accordingly has been a long-standing goal in NLP and AI research.", "labels": [], "entities": []}, {"text": "Impressive progress has been made in factoid-style reading comprehension, e.g.,, enabled by well-designed datasets and modern neural network models.", "labels": [], "entities": []}, {"text": "However, these models still struggle with questions that require inference (.", "labels": [], "entities": []}, {"text": "Consider the paragraph in about photosynthesis.", "labels": [], "entities": []}, {"text": "While top systems on SQuAD () can reliably answer lookup questions such as: Q1: What do the roots absorb?", "labels": [], "entities": []}, {"text": "(A: water, minerals) they struggle when answers are not explicit, e.g., Q2: Where is sugar produced?", "labels": [], "entities": []}, {"text": "(A: in the leaf) 1 * * Bhavana Dalvi Mishra and Lifu Huang contributed equally to this work.", "labels": [], "entities": []}, {"text": "For example, the RC system BiDAF ( answers \"glucose\" to this question.", "labels": [], "entities": [{"text": "BiDAF", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9370384216308594}]}, {"text": "Chloroplasts in the leaf of the plant trap light from the sun.", "labels": [], "entities": []}, {"text": "The roots absorb water and minerals from the soil.", "labels": [], "entities": []}, {"text": "This combination of water and minerals flows from the stem into the leaf.", "labels": [], "entities": []}, {"text": "Carbon dioxide enters the leaf.", "labels": [], "entities": []}, {"text": "Light, water and minerals, and the carbon dioxide all combine into a mixture.", "labels": [], "entities": []}, {"text": "This mixture forms sugar (glucose) which is what the plant eats.", "labels": [], "entities": []}, {"text": "Q: Where is sugar produced?", "labels": [], "entities": []}, {"text": "A: in the leaf To answer Q2, it appears that a system needs knowledge of the world and the ability to reason with state transitions in multiple sentences: If carbon dioxide enters the leaf (stated), then it will beat the leaf (unstated), and as it is then used to produce sugar, the sugar production will beat the leaf too.", "labels": [], "entities": []}, {"text": "This challenge of modeling and reasoning with a changing world is particularly pertinent in text about processes, demonstrated by the paragraph in.", "labels": [], "entities": []}, {"text": "Understanding what is happening in such texts is important for many tasks, e.g., procedure execution and validation, effect prediction.", "labels": [], "entities": [{"text": "procedure execution and validation", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.6515843570232391}, {"text": "effect prediction", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.800798237323761}]}, {"text": "However, it is also difficult because the world state is changing, and the causal effects of actions on that state are often implicit.", "labels": [], "entities": []}, {"text": "To address this challenging style of reading comprehension problem, researchers have created several datasets.", "labels": [], "entities": []}, {"text": "The bAbI dataset) includes questions about objects moved throughout a paragraph, using machine-generated language over a deterministic domain with a small lexicon.", "labels": [], "entities": [{"text": "bAbI dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8536044955253601}]}, {"text": "The SCoNE dataset ( contains paragraphs describing a changing world state in three synthetic, deterministic domains, and: A (simplified) annotated paragraph from ProPara.", "labels": [], "entities": [{"text": "SCoNE dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7655972838401794}, {"text": "ProPara", "start_pos": 162, "end_pos": 169, "type": "DATASET", "confidence": 0.9788832068443298}]}, {"text": "Each filled row shows the existence and location of participants between each step (\"?\" denotes \"unknown\", \"-\" denotes \"does not exist\").", "labels": [], "entities": []}, {"text": "For example in state0, water is located at the soil.", "labels": [], "entities": []}, {"text": "assumes that a complete and correct model of the initial state is given for each task.", "labels": [], "entities": []}, {"text": "However, approaches developed using synthetic data often fail to handle the inherent complexity in language when applied to organic, real-world data (.", "labels": [], "entities": []}, {"text": "In this work, we create anew dataset, ProPara (Process Paragraphs), containing 488 humanauthored paragraphs of procedural text, along with 81k annotations about the changing states (existence and location) of entities in those paragraphs, with an end-task of predicting location and existence changes that occur.", "labels": [], "entities": []}, {"text": "This is the first dataset containing annotated, natural text for real-world processes, along with a simple representation of entity states during those processes.", "labels": [], "entities": []}, {"text": "A simplified example is shown in.", "labels": [], "entities": []}, {"text": "When applying existing state-of-the-art systems, such as Recurrent Entity Networks ( and Query-reduction Networks (, we find that they do not perform well on ProPara and the results are only slightly better than the majority baselines.", "labels": [], "entities": [{"text": "ProPara", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.9402737617492676}]}, {"text": "As a step forward, we propose two new neural models that use alternative mechanisms for state prediction and propagation, in particular using LSTM input encoding and span prediction.", "labels": [], "entities": [{"text": "state prediction and propagation", "start_pos": 88, "end_pos": 120, "type": "TASK", "confidence": 0.699268713593483}, {"text": "LSTM input encoding", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.5981488823890686}, {"text": "span prediction", "start_pos": 166, "end_pos": 181, "type": "TASK", "confidence": 0.6829163134098053}]}, {"text": "The new models improve accuracy by up to 19%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995731711387634}]}, {"text": "Our contributions in this work are twofold: (1) we create ProPara, anew dataset for process paragraph comprehension, containing annotated, natural language paragraphs about real-world processes, and (2) we propose two new models that learn to infer and propagate entity states in novel ways, and outperform existing methods on this dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "Task: Our dataset, ProPara, focuses on a particular genre of procedural text, namely simple scientific processes (e.g., photosynthesis, erosion).", "labels": [], "entities": [{"text": "ProPara", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9447475075721741}]}, {"text": "A system that understands a process paragraph should be able to answer questions such as: \"What are the inputs to the process?\", \"What is converted into what?\", and \"Where does the conversion take place?\"", "labels": [], "entities": []}, {"text": "3 Many of these questions reduce to understanding the basic dynamics of entities in the process, and we use this as our task: Given a process paragraph and an entity e mentioned in it, identify: (1) Is e created (destroyed, moved) in the process?", "labels": [], "entities": []}, {"text": "(2) When (step #) is e created (destroyed, moved)?", "labels": [], "entities": []}, {"text": "(3) Where is e created (destroyed, moved from/to)?", "labels": [], "entities": []}, {"text": "If we can track the entities' states through the process and answer such questions, many of the higherlevel questions can be answered too.", "labels": [], "entities": []}, {"text": "To do this, we now describe how these states are representated in ProPara, and how the dataset was built.", "labels": [], "entities": [{"text": "ProPara", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9734495282173157}]}, {"text": "Process State Representation: The states of the world throughout the whole process are represented as a grid.", "labels": [], "entities": [{"text": "Process State Representation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6162210802237192}]}, {"text": "Each column denotes a participant entity (a span in the paragraph, typically a noun phrase) that undergoes some creation, destruction, or movement in the process.", "labels": [], "entities": []}, {"text": "Each row denotes the states of all the participants after a step.", "labels": [], "entities": []}, {"text": "Each sentence is a step that may change the state of one or more participants.", "labels": [], "entities": []}, {"text": "Therefore, a process paragraph with m sentences and n participants will result in an (m + 1) \u00d7 n grid representation.", "labels": [], "entities": []}, {"text": "Each cell l i j in this grid records the location of the j-th participant after the i-th step, and l 0 j stores the location of j-th participant before the process.", "labels": [], "entities": []}, {"text": "4 shows one example of this representation.", "labels": [], "entities": []}, {"text": "Paragraph Authoring: To collect paragraphs, we first generated a list of 200 process-evoking prompts, such as \"What happens during photosynthesis?\", by instantiating five patterns , with nouns of the corresponding type from a science vocabulary, followed by manual rewording.", "labels": [], "entities": []}, {"text": "Then, crowdsourcing (MTurk) workers were shown one of the prompts and asked to write a sequence of event sentences describing the process.", "labels": [], "entities": []}, {"text": "Each prompt was given to five annotators to produce five (independent) paragraphs.", "labels": [], "entities": []}, {"text": "Short paragraphs (4 or less sentences) were then removed fora final total of 488 paragraphs describing 183 processes.", "labels": [], "entities": []}, {"text": "An example paragraph is the one shown earlier in.", "labels": [], "entities": []}, {"text": "Grid and Existence: Once the process paragraphs were authored, we asked expert annotators to create the initial grids.", "labels": [], "entities": []}, {"text": "First, for each paragraph, they listed the participant entities that underwent a state change during the process, thus creating the column headers.", "labels": [], "entities": []}, {"text": "They then marked the steps where a participant was created or destroyed.", "labels": [], "entities": []}, {"text": "Allstate cells before a Create or after a Destroy marker were labeled as \"not exists\".", "labels": [], "entities": [{"text": "Destroy marker", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.817167341709137}]}, {"text": "Each initial grid annotation was checked by a second expert annotator.", "labels": [], "entities": []}, {"text": "Locations: Finally, MTurk workers were asked to fill in all the location cells.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 20, "end_pos": 25, "type": "TASK", "confidence": 0.5938418507575989}]}, {"text": "A location can be \"unknown\" if it is not specified in the text, or a span of the original paragraph.", "labels": [], "entities": []}, {"text": "Five grids for the same paragraph were completed by five different Turkers, with average pairwise inter-annotator agreement of 0.67.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.8842058181762695}, {"text": "agreement", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.521659791469574}]}, {"text": "The end result was 81,345 annotations over 488 paragraphs about 183 processes.", "labels": [], "entities": []}, {"text": "The dataset  was then split 80/10/10 into train/dev/test by process prompt, ensuring that the test paragraphs were all about processes unseen in train and dev.", "labels": [], "entities": []}, {"text": "compares our dataset with bAbI and SCoNE.", "labels": [], "entities": []}, {"text": "As described in Section 3, the quality of a model is evaluated based on its ability to answer three categories of questions, with respect to a given participant e:  (Cat-1) Is e created (destroyed, moved) in the process?", "labels": [], "entities": []}, {"text": "(Cat-2) When (step#) is e created (destroyed, moved)?", "labels": [], "entities": []}, {"text": "(Cat-3) Where is e created (destroyed, moved from/to)?", "labels": [], "entities": []}, {"text": "These questions are answered by simple scans over the state predictions for the whole process.", "labels": [], "entities": []}, {"text": "(Cat-1) is asked overall participants, while (Cat-2) and (Cat-3) are asked over just those participants that were created (destroyed, moved).", "labels": [], "entities": []}, {"text": "The accuracy of the answers is used as the evaluation metric, except for questions that may have multiple answers (e.g., \"When is e moved?\").", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999106228351593}]}, {"text": "In this case, we compare the predicted and gold answers and use the F 1 score as the \"accuracy\" of the answer set prediction.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9884801109631857}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9991475343704224}]}, {"text": "For questions in category, an answer is considered correct if the predicted location is identical to, or a sub-phrase of, the labeled location (typically just one or two words), after stop-word removal and lemmatizing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ProPara vs. other procedural datasets.", "labels": [], "entities": [{"text": "ProPara", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.8204275965690613}]}, {"text": " Table 3: Model accuracy on the end task (test partition of ProPara). Questions are (Section 5.1): (Cat-1) Is e i  created (destroyed, moved)? (Cat-2) When is e i created (...)? (Cat-3) Where is e i created (...)?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9757835268974304}, {"text": "ProPara", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.933001697063446}]}]}