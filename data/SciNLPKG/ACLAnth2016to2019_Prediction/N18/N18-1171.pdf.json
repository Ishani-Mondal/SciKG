{"title": [{"text": "Sentiment Analysis: It's Complicated! 1*", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9715546071529388}]}], "abstractContent": [{"text": "Sentiment analysis is used as a proxy to measure human emotion, where the objective is to categorize text according to some prede-fined notion of sentiment.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9172138273715973}]}, {"text": "Sentiment analysis datasets are typically constructed with gold-standard sentiment labels, assigned based on the results of manual annotations.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8953216373920441}]}, {"text": "When working with such annotations, it is common for dataset constructors to discard \"noisy\" or \"controversial\" data where there is significant disagreement on the proper label.", "labels": [], "entities": []}, {"text": "In datasets constructed for the purpose of Twitter sentiment analysis (TSA), these controversial examples can compose over 30% of the originally annotated data.", "labels": [], "entities": [{"text": "Twitter sentiment analysis (TSA)", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.8270345330238342}]}, {"text": "We argue that the removal of such data is a problematic trend because , when performing real-time sentiment classification of short-text, an automated system cannot know a priori which samples would fall into this category of disputed sentiment.", "labels": [], "entities": [{"text": "real-time sentiment classification", "start_pos": 88, "end_pos": 122, "type": "TASK", "confidence": 0.6660047074158987}]}, {"text": "We therefore propose the notion of a \"complicated\" class of sentiment to categorize such text, and argue that its inclusion in the short-text sentiment analysis framework will improve the quality of automated sentiment analysis systems as they are implemented in real-world settings.", "labels": [], "entities": [{"text": "short-text sentiment analysis", "start_pos": 131, "end_pos": 160, "type": "TASK", "confidence": 0.6622739831606547}, {"text": "sentiment analysis", "start_pos": 209, "end_pos": 227, "type": "TASK", "confidence": 0.7413731813430786}]}, {"text": "We motivate this argument by building and analyzing anew publicly available TSA dataset of over 7,000 tweets annotated with 5x coverage, named MTSA.", "labels": [], "entities": [{"text": "TSA dataset", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.7719684839248657}, {"text": "MTSA", "start_pos": 143, "end_pos": 147, "type": "DATASET", "confidence": 0.8410657644271851}]}, {"text": "Our analysis of classifier performance over our dataset offers insights into sentiment analysis dataset and model design, how current techniques would perform in the real world, and how researchers should handle difficult data.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.9242148995399475}]}], "introductionContent": [{"text": "The goal of sentiment analysis is to determine the attitude or emotional state held by the author of * These authors contributed equally to this work.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.9548447132110596}]}, {"text": "\u2020 These authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "# These authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "Automatic sentiment classification that can quickly garner user sentiment is useful for applications ranging from product marketing to measuring public opinion.", "labels": [], "entities": [{"text": "Automatic sentiment classification", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7058678269386292}]}, {"text": "The volume and availability of short-text user content makes automated sentiment analysis systems highly attractive for companies and organizations, despite potential complications arising from their short length and specialized use of language.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8645468354225159}]}, {"text": "The popularity of Twitter as asocial media platform on which people can readily express their thoughts, feelings, and opinions, coupled with the openness of the platform, provides a large amount of publicly accessible data ripe for analysis, being a well established domain for sentiment analysis as reflecting realworld attitudes.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 278, "end_pos": 296, "type": "TASK", "confidence": 0.8983623087406158}]}, {"text": "In this paper, we look into Twitter sentiment analysis (TSA) as a suitable, core instance of general short-text sentiment analysis (, and encourage the methods and practices presented to be applied across other domains.", "labels": [], "entities": [{"text": "Twitter sentiment analysis (TSA)", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8644603788852692}, {"text": "general short-text sentiment analysis", "start_pos": 93, "end_pos": 130, "type": "TASK", "confidence": 0.7532211542129517}]}, {"text": "Building a TSA model that can automatically determine the sentiment of a tweet has received significant attention over the past several years.", "labels": [], "entities": []}, {"text": "However, since most state-of-the-art TSA models use machine learning to tune their parameters, their performance -and relevance to a real-world implementation setting -is highly dependent on the dataset on which they are trained.", "labels": [], "entities": []}, {"text": "TSA dataset construction has, unfortunately, received less attention than TSA model design.", "labels": [], "entities": [{"text": "TSA dataset", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.7526209056377411}, {"text": "TSA model design", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.6874093810717264}]}, {"text": "Many commonly used TSA datasets make assumptions that do not hold in a real-world implementation setting.", "labels": [], "entities": [{"text": "TSA datasets", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.8893781006336212}]}, {"text": "For example, it is a common practice for studies to discard tweets on which there is high annotator disagreement.", "labels": [], "entities": []}, {"text": "While some argue that this is done to remove noise resulting from poor annotator quality, this argument does not hold when considering that these datasets present high rates of unanimous annotator agreement . This suggests that the problem is not poor annotators, but, rather, difficult data that does not fall into the established categories of sentiment.", "labels": [], "entities": []}, {"text": "Consider the sample tweets in drawn from our dataset, one with unanimous agreement on an OBJECTIVE label, one with 60% agreement, and one with complete disagreement.", "labels": [], "entities": [{"text": "OBJECTIVE", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.7923940420150757}]}, {"text": "We observe that, as the amount of disagreement across annotations increases, so too does the clarity of what the tweet's gold standard label really should be.", "labels": [], "entities": [{"text": "clarity", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9973762035369873}]}, {"text": "Though the issues we raise may seem obvious, the absence of their proper treatment in the existing literature suggests the need to systematically consider their implications in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.9541875720024109}]}, {"text": "In this paper, we propose the inclusion of a COMPLICATED class of sentiment to indicate that the text does not fall into the established categories of sentiment.", "labels": [], "entities": []}, {"text": "We offer insights into the differences between tweets that receive different levels of inter-annotator-agreement, providing empirical evidence that tweets with differing levels of agreement are qualitatively different from each other.", "labels": [], "entities": []}, {"text": "Our claims are supported by empirical analysis of anew TSA dataset, the McGill Twitter Sentiment Analysis dataset (MTSA), which we release publicly with this work 2 . The dataset contains 7,026 tweets across five different topic-domains, annotated with 5x coverage.", "labels": [], "entities": [{"text": "TSA dataset", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.7504755258560181}, {"text": "McGill Twitter Sentiment Analysis dataset (MTSA)", "start_pos": 72, "end_pos": 120, "type": "DATASET", "confidence": 0.9073246568441391}]}, {"text": "We release this dataset with the raw annotation results, and hope that researchers and organizations will be able to Annotator disagreement information has proven useful in other areas of sentiment analysis analyze our dataset and build models that can be applied in real-world sentiment analysis settings.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.9201888740062714}]}], "datasetContent": [{"text": "In the), a thorough 5x coverage annotation scheme is used (each tweet is annotated by at least five people).", "labels": [], "entities": []}, {"text": "Annotations were made on a fivepoint scale, with categories STRONGLYNEGA-TIVE, WEAKLYNEGATIVE, NEUTRAL, WEAK-LYPOSITIVE, and STRONGLYPOSITIVE.", "labels": [], "entities": [{"text": "STRONGLYNEGA-TIVE", "start_pos": 60, "end_pos": 77, "type": "METRIC", "confidence": 0.7745273113250732}, {"text": "WEAKLYNEGATIVE", "start_pos": 79, "end_pos": 93, "type": "METRIC", "confidence": 0.7180856466293335}, {"text": "NEUTRAL", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.762778639793396}]}, {"text": "If at least three out of five of the annotators gave the same labelling, that was accepted as the final annotation.", "labels": [], "entities": []}, {"text": "Otherwise, the authors used an averaging scheme (mapping the labels to integers \u22122, \u22121, 0, 1, 2) to determine the final label, taking the average of the labellings and rounding according to a specific criterion.", "labels": [], "entities": []}, {"text": "For example, if a controversial tweet receives two STRONGLYNEGATIVE, two STRONGLYPOS-ITIVE, and one NEUTRAL labelling, it will have a resultant label of NEUTRAL.", "labels": [], "entities": [{"text": "STRONGLYPOS-ITIVE", "start_pos": 73, "end_pos": 90, "type": "METRIC", "confidence": 0.7813424468040466}]}, {"text": "Yet, the tweet would certainly not be \"neutral\", it would be qualitatively different from a tweet with unanimous agreement on a NEUTRAL labelling.", "labels": [], "entities": []}, {"text": "In Section 5, we provide empirical results supporting this claim, discovering that high-disagreement data is qualitatively different from high-agreement data.", "labels": [], "entities": []}, {"text": "provide a thorough exploration into the specific design decisions and considerations made during the construction of the 2013-2014 SemEval shared task for short-text sentiment analysis.", "labels": [], "entities": [{"text": "SemEval shared task", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.8649223248163859}, {"text": "short-text sentiment analysis", "start_pos": 155, "end_pos": 184, "type": "TASK", "confidence": 0.6844164729118347}]}, {"text": "In Subtask B, annotators de-termined the overall polarity of apiece of text, according to a ternary labelling scheme between POSITIVE, NEGATIVE, or NEUTRAL.", "labels": [], "entities": [{"text": "POSITIVE", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.8814128637313843}]}, {"text": "The final label of the sentence was \"determined based on the majority of the labels\" according to 5x coverage.", "labels": [], "entities": []}, {"text": "The designers thus discarded sentences where there was no majority annotator agreement, since such sentences \"are likely to be controversial cases\" (p. 40); they do not report how much data was discarded.", "labels": [], "entities": []}, {"text": "constructed anew dataset, the STS-Gold, by taking into account several limitations of the TSA datasets they reviewed.", "labels": [], "entities": [{"text": "TSA datasets", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.7619494199752808}]}, {"text": "In their study, 3,000 tweets were labelled with 3x coverage.", "labels": [], "entities": []}, {"text": "Any tweet without unanimous agreement on the label was discarded; this decision was justified by the argument that they did not want \"noisy data\" in their dataset.", "labels": [], "entities": []}, {"text": "Thus, they discarded 794 tweets, or 26.5% of their originally annotated data.", "labels": [], "entities": []}, {"text": "While we argue that this is a problematic design decision, we note that discarding data in this way successfully isolated unanimous-agreement from majority-agreement data, thus avoiding conflating tweets with different levels of agreement, unlike in the 2013-14 and 2017 SemEval tasks.", "labels": [], "entities": [{"text": "SemEval tasks", "start_pos": 271, "end_pos": 284, "type": "TASK", "confidence": 0.8354542255401611}]}, {"text": "The annotation scheme for the STS-Gold resolves one of the problems in the SemEval 2017 Task, as it provides an option for labelling a MIXED category, capturing tweets bearing multiple conflicting sentiments.", "labels": [], "entities": [{"text": "SemEval 2017 Task", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7950143615404764}]}, {"text": "It also provides the OTHER category for tweets where it is \"difficult to decide on a proper label\".", "labels": [], "entities": [{"text": "OTHER", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9564557671546936}]}, {"text": "Interestingly, the dichotomy between the high frequency of highdisagreement tweets (794 total) compared to the low frequency of tweets unanimously labelled as OTHER (4 total) is consistent with our findings on the COMPLICATED label (Section 3.3).", "labels": [], "entities": [{"text": "OTHER", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.8214497566223145}, {"text": "COMPLICATED label", "start_pos": 214, "end_pos": 231, "type": "DATASET", "confidence": 0.8734086751937866}]}, {"text": "The challenges and possible approaches to manual sentiment annotation have been previously discussed by, who offers important insights into how questions and problem descriptions should be posed to annotators.", "labels": [], "entities": [{"text": "manual sentiment annotation", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6767446796099345}]}, {"text": "The annotated tweets are categorized by four agreement levels: Unanimous (5 out of 5 agreed on the label), Consensus (exactly 4 out of 5 agreed), Majority (exactly 3 out of 5 agreed), or Disputed (maximum 2 out of 5 agreed).", "labels": [], "entities": []}, {"text": "The distribution of agreement rates was consistent across topics (see supplemental material), thus the entire dataset is merged for the remainder of the analysis. of the dataset (4505 tweets), and tweets with at least Majority agreement compose 92% of the dataset (6473 tweets; see).", "labels": [], "entities": []}, {"text": "The decision to discard tweets with significant annotator disagreement, as previously done in TSA research, would result in the loss of 8% to 34% of the annotated tweets in our dataset, depending on whether to filter to a minimum Majority or Consensus agreement, respectively.", "labels": [], "entities": []}, {"text": "Interestingly, these numbers are consistent with the proportion of discarded tweets in previous literature.", "labels": [], "entities": []}, {"text": "Tweets that caused more disagreement among the human annotators were found to be more sentimentladen (majority label of POSITIVE, NEGATIVE, or COMPLICATED;).", "labels": [], "entities": [{"text": "POSITIVE", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9224755764007568}]}, {"text": "Objective tweets composed 78% (1892 tweets), 63% (1311), and 50% (983) of the Unanimous, Consensus, and Majority subsets of annotated tweets, respectively.", "labels": [], "entities": []}, {"text": "Use of the COM-PLICATED label by annotators was infrequent, and of those tweets with high inter-annotator agreement, almost exclusively limited to tweets that expressed clear, mixed sentiment.", "labels": [], "entities": []}, {"text": "For example, the single tweet that received a unanimous COMPLI-CATED annotation had clear mixed sentiment: \"the iPhone 6s is so big and hard to use but I still like it\".", "labels": [], "entities": []}, {"text": "There were a total of 13 tweets with at least Consensus agreement for the COMPLICATED label (see supplemental material).", "labels": [], "entities": [{"text": "Consensus", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.992957353591919}, {"text": "COMPLICATED label", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.6542473882436752}]}, {"text": "These specific tweets largely corresponded to the MIXED label used in previous TSA datasets (  tweets that did not clearly fall clearly within OB-JECTIVE, POSITIVE, and NEGATIVE categories were not consistently identified as COMPLICATED by annotators.", "labels": [], "entities": [{"text": "MIXED", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.7644299864768982}, {"text": "TSA datasets", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.8490678668022156}]}, {"text": "Rather, those tweets were a source of significant disagreement.", "labels": [], "entities": []}, {"text": "We evaluate with weighted-and macro-F1-scores to assess classifier performance.", "labels": [], "entities": []}, {"text": "F1-score is a common way to measure classifier performance in sentiment analysis as it computes the harmonic mean between precision and recall.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9812817573547363}, {"text": "sentiment analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.9344775080680847}, {"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9972212314605713}, {"text": "recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9593358039855957}]}, {"text": "In multi-class classification, we obtain a one-versus-all F-score, F c , for each class c in our set of possible classes, C. Weighted F-score weights each F-score by its support in the test set; if there are n c samples in the test set belonging to class c, then the weighted F-score is expressed by F weighted in Equation 1.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.7682865858078003}]}, {"text": "Naturally, the weighted F-score is influenced by the frequency of samples in a class; so, in our case, it is biased toward the OBJECTIVE class due to its large frequency compared to the other classes (Table 5;).", "labels": [], "entities": [{"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9628349542617798}, {"text": "OBJECTIVE", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.8847590684890747}]}, {"text": "Thus, we also report the macro Fscore, which averages the F-scores for each class without considering their support, expressed by F macro in Equation 2.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9109743237495422}, {"text": "F-scores", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9753286838531494}]}, {"text": "This score evaluates model performance isolated from the class distribution, allowing us to determine if a change inaccuracy is the result of simply a change in distribution of classes or a change in model generalization ability.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example tweets from our dataset over varying  levels of annotator labellings; +, -, 0 stand for POSI- TIVE, NEGATIVE, OBJECTIVE.", "labels": [], "entities": [{"text": "POSI- TIVE", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.912846028804779}, {"text": "OBJECTIVE", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.7172889709472656}]}, {"text": " Table 2: Summary of some of the major TSA datasets used in recent work. Symbols +, -, 0 stand for POSITIVE,  NEGATIVE, and NEUTRAL, respectively; prefixes s, w stand for STRONGLY and WEAKLY.", "labels": [], "entities": [{"text": "TSA datasets", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.7464663237333298}, {"text": "POSITIVE", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9394413232803345}, {"text": "STRONGLY", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.8320074081420898}]}, {"text": " Table 4: Annotator agreement rates. Unanimous stands  for 100% annotator agreement, Consensus 80%, Ma- jority 60%, and Disputed <60%.", "labels": [], "entities": [{"text": "Unanimous", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9886478781700134}]}, {"text": " Table 6: Experiment I. Macro-F1 score results for precision, recall, and F1-score, as shown visually in", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9996451139450073}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9989493489265442}, {"text": "F1-score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.998813271522522}]}, {"text": " Table 7: Experiment II. Results across evaluation met- rics, as shown visually in", "labels": [], "entities": []}]}