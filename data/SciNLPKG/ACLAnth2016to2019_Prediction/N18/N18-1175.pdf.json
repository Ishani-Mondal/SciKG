{"title": [{"text": "The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants", "labels": [], "entities": [{"text": "Argument Reasoning Comprehension", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.7030914227167765}, {"text": "Identification and Reconstruction of Implicit Warrants", "start_pos": 43, "end_pos": 97, "type": "TASK", "confidence": 0.8096402188142141}]}], "abstractContent": [{"text": "Reasoning is a crucial part of natural language argumentation.", "labels": [], "entities": [{"text": "natural language argumentation", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.6484595239162445}]}, {"text": "To comprehend an argument , one must analyze its warrant, which explains why its claim follows from its premises.", "labels": [], "entities": []}, {"text": "As arguments are highly contextualized, warrants are usually presupposed and left implicit.", "labels": [], "entities": []}, {"text": "Thus, the comprehension does not only require language understanding and logic skills, but also depends on commonsense.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.6986570358276367}]}, {"text": "In this paper we develop a methodology for reconstructing warrants systematically.", "labels": [], "entities": [{"text": "reconstructing warrants", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.9254423677921295}]}, {"text": "We operationalize it in a scalable crowdsourcing process, resulting in a freely licensed dataset with warrants for 2k authentic arguments from news comments.", "labels": [], "entities": []}, {"text": "1 On this basis, we present anew challenging task, the argument reasoning comprehension task.", "labels": [], "entities": [{"text": "argument reasoning comprehension", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.8498503963152567}]}, {"text": "Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options.", "labels": [], "entities": []}, {"text": "Both warrants are plausible and lexically close, but lead to contradicting claims.", "labels": [], "entities": []}, {"text": "A solution to this task will define a substantial step towards automatic warrant reconstruction.", "labels": [], "entities": [{"text": "warrant reconstruction", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.825171709060669}]}, {"text": "However, experiments with several neural attention and language models reveal that current approaches do not suffice.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most house cats face enemies.", "labels": [], "entities": []}, {"text": "Russia has the opposite objectives of the US.", "labels": [], "entities": []}, {"text": "There is much innovation in 3-d printing and it is sustainable.", "labels": [], "entities": [{"text": "3-d printing", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.6666775643825531}]}, {"text": "What do the three propositions have in common?", "labels": [], "entities": []}, {"text": "They were never uttered but solely presupposed in arguments made by the participants of online discussions.", "labels": [], "entities": []}, {"text": "Presuppositions area fundamental pragmatic instrument of natural language argumentation in which parts of arguments are left unstated.", "labels": [], "entities": [{"text": "Presuppositions", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9205479025840759}, {"text": "natural language argumentation", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.6448264718055725}]}, {"text": "This phenomenon is also referred to as Title: Is Marijuana a Gateway Drug?", "labels": [], "entities": [{"text": "Title: Is Marijuana a Gateway Drug?", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.8385716453194618}]}, {"text": "Description: Does using marijuana lead to the use of more dangerous drugs, making it too dangerous to legalize?", "labels": [], "entities": []}, {"text": "Reason: Milk isn't a gateway drug even though most people drink it as children.", "labels": [], "entities": []}, {"text": "And since {Warrant 1 | Warrant 2}, Claim: Marijuana is not a gateway drug.", "labels": [], "entities": [{"text": "Claim", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9873842000961304}]}, {"text": "Warrant 1: milk is similar to marijuana Warrant 2: milk is not marijuana: Instance of the argument reasoning comprehension task.", "labels": [], "entities": [{"text": "argument reasoning comprehension task", "start_pos": 90, "end_pos": 127, "type": "TASK", "confidence": 0.760717511177063}]}, {"text": "The correct warrant has to be identified.", "labels": [], "entities": []}, {"text": "Notice the fallacious presupposed false analogy used by the author to make the argument.", "labels": [], "entities": []}, {"text": "treme case, there may exist an alternative warrant in which the same reason is connected to the opposite claim.", "labels": [], "entities": []}, {"text": "The intuition of alternative warrants is key to the systematic methodology that we develop in this paper for reconstructing a warrant for the original claim of an argument.", "labels": [], "entities": []}, {"text": "In particular, we first 'twist' the stance of a given argument, trying to plausibly explain its reasoning towards the opposite claim.", "labels": [], "entities": []}, {"text": "Then, we twist the stance back and use a similar reasoning chain to come up with a warrant for the original argument.", "labels": [], "entities": []}, {"text": "As we discuss further below, this works for real-world arguments with a missing piece of information that is taken for granted and considered as common knowledge, yet, would lead to the opposite stance if twisted.", "labels": [], "entities": []}, {"text": "We demonstrate the applicability of our methodology in a large crowdsourcing study.", "labels": [], "entities": []}, {"text": "The study results in 1,970 high-quality instances fora new task that we call argument reasoning comprehension: Given a reason and a claim, identify the correct warrant from two opposing options.", "labels": [], "entities": [{"text": "argument reasoning comprehension", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.7974330882231394}]}, {"text": "An example is given in.", "labels": [], "entities": []}, {"text": "A solution to this task will represent a substantial step towards automatic warrant reconstruction.", "labels": [], "entities": [{"text": "warrant reconstruction", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.8154945075511932}]}, {"text": "However, we present experiments with several neural attention and language models which reveal that current approaches based on the words and phrases in arguments and warrants do not suffice to solve the task.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are (1) a methodology for obtaining implicit warrants realized by means of scalable crowdsourcing and (2) anew task along with a high-quality dataset.", "labels": [], "entities": []}, {"text": "In addition, we provide (a) 2,884 user-generated arguments annotated for their stance, covering 50+ controversial topics, (b) 2,026 arguments with annotated reasons supporting the stance, (c) 4,235 rephrased reason gists, useful for argument summarization and sentence compression, and (d) a method for checking the reliability of crowdworkers in document and span labeling using traditional inter-annotator agreement measures.", "labels": [], "entities": [{"text": "argument summarization", "start_pos": 233, "end_pos": 255, "type": "TASK", "confidence": 0.6413663029670715}, {"text": "sentence compression", "start_pos": 260, "end_pos": 280, "type": "TASK", "confidence": 0.7212151437997818}, {"text": "document and span labeling", "start_pos": 347, "end_pos": 373, "type": "TASK", "confidence": 0.5743112042546272}]}], "datasetContent": [{"text": "To strictly assess quality in the entire crowdsourcing process, we propose an evaluation method that enables 'classic' inter-annotator agreement measures for crowdsourcing, such as Fleiss' \u03ba or Krippendorff's \u03b1.", "labels": [], "entities": []}, {"text": "Applying \u03ba and \u03b1 directly to crowdsourced data has been disputed ().", "labels": [], "entities": []}, {"text": "For estimating gold labels from the crowd, several models have been proposed; we rely on MACE (.", "labels": [], "entities": [{"text": "estimating gold labels", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7907372911771139}, {"text": "MACE", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.6340022087097168}]}, {"text": "Given a number of noisy workers, MACE outputs best estimates, outperforming simple majority votes.", "labels": [], "entities": [{"text": "MACE", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.6820760369300842}]}, {"text": "At least five workers are recommended fora crowdsourcing task, but how reliable is the output really?", "labels": [], "entities": []}, {"text": "We hence collected 18 assignments per item and split them into two groups (9+9) based on their submission time.", "labels": [], "entities": []}, {"text": "We then considered each group as an independent crowdsourcing experiment and estimated gold labels using MACE for each group, thus yielding two 'experts from the crowd.'", "labels": [], "entities": [{"text": "MACE", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.648900032043457}]}, {"text": "Having two independent 'experts' from the crowd allowed us to compute standard agreement scores.", "labels": [], "entities": []}, {"text": "We also varied the size of the sub-sample from each group from 1 to 9 by repeated random sampling of assignments.", "labels": [], "entities": []}, {"text": "This revealed how the score varies with respect to the crowd size per 'expert'.", "labels": [], "entities": []}, {"text": "shows the Cohen's \u03ba agreement for stance annotation with respect to the crowd size computed by our method.", "labels": [], "entities": [{"text": "stance annotation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.9382839798927307}]}, {"text": "As MACE also includes a threshold for keeping only the most confident predictions in order to benefit precision, we tuned this parameter, too.", "labels": [], "entities": [{"text": "MACE", "start_pos": 3, "end_pos": 7, "type": "TASK", "confidence": 0.46366608142852783}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9991437196731567}]}, {"text": "Deciding on the number of workers per task is a trade-off between the desired quality and the budget.", "labels": [], "entities": []}, {"text": "For example, reason span annotation is a harder task; however, the results for six workers are comparable to those for the expert annotations of . 9 The supplementary material contains a detailed figure; Error bars = std.", "labels": [], "entities": [{"text": "reason span annotation", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.6839929421742758}, {"text": "Error bars", "start_pos": 204, "end_pos": 214, "type": "METRIC", "confidence": 0.9812522530555725}]}, {"text": "dev; only shown for two thresholds: Cohen's \u03ba agreement for stance annotation on 98 comments.", "labels": [], "entities": [{"text": "stance annotation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8376677334308624}]}, {"text": "As a trade-off between reducing costs (i.e., discarding fewer instances) and increasing reliability, we chose 5 annotators and a threshold of 0.95 for this task, which resulted in \u03ba = 0.58 (moderate to substantial agreement).", "labels": [], "entities": [{"text": "reliability", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.9909819960594177}]}, {"text": "lists statistics of the entire crowdsourcing process carried out for our dataset, including tasks for which we created data as a by-product.", "labels": [], "entities": []}, {"text": "Given the dataset, we performed first experiments to assess the complexity of argument reasoning comprehension.", "labels": [], "entities": [{"text": "argument reasoning comprehension", "start_pos": 78, "end_pos": 110, "type": "TASK", "confidence": 0.799037237962087}]}, {"text": "To this end, we split the 1,970 instances into three sets based on the year of the denot to be confused with which refers to stance annotation.: Details and statistics of the datasets resulting from the eight steps of our methodology implemented in a crowdsourcing process.", "labels": [], "entities": []}, {"text": "*Input instances were filtered by their 'logic score' assigned in Step 6, such that the weakest 30% were discarded.", "labels": [], "entities": []}, {"text": "A more detailed description is available in the readme file of the source code.", "labels": [], "entities": [{"text": "readme file of the source code", "start_pos": 48, "end_pos": 78, "type": "DATASET", "confidence": 0.8078312575817108}]}, {"text": "bate they were taken from: 2011-2015 became the training set (1,210 instances), 2016 the development set (316 instances), and 2017 the test set (444 instances).", "labels": [], "entities": []}, {"text": "This follows the paradigm of learning on past data and predicting on new ones.", "labels": [], "entities": []}, {"text": "In addition, it removes much lexical and topical overlap.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details and statistics of the datasets resulting from the eight steps of our methodology implemented in  a crowdsourcing process. *Input instances were filtered by their 'logic score' assigned in Step 6, such that the  weakest 30% were discarded. A more detailed description is available in the readme file of the source code.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy of each approach (humans and sys- tems) on the development set and test set, respectively.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9958332180976868}]}]}