{"title": [{"text": "Specialising Word Vectors for Lexical Entailment", "labels": [], "entities": [{"text": "Lexical Entailment", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7974596321582794}]}], "abstractContent": [{"text": "We present LEAR (Lexical Entailment Attract-Repel), a novel post-processing method that transforms any input word vector space to emphasise the asymmetric relation of lexical entailment (LE), also known as the IS-A or hyponymy-hypernymy relation.", "labels": [], "entities": [{"text": "LEAR", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9846630096435547}]}, {"text": "By injecting external linguistic constraints (e.g., WordNet links) into the initial vector space, the LE spe-cialisation procedure brings true hyponymy-hypernymy pairs closer together in the transformed Euclidean space.", "labels": [], "entities": []}, {"text": "The proposed asym-metric distance measure adjusts the norms of word vectors to reflect the actual WordNet-style hierarchy of concepts.", "labels": [], "entities": []}, {"text": "Simultaneously, a joint objective enforces semantic similarity using the symmetric cosine distance, yielding a vector space specialised for both lexical relations at once.", "labels": [], "entities": []}, {"text": "LEAR specialisation achieves state-of-the-art performance in the tasks of hy-pernymy directionality, hypernymy detection, and graded lexical entailment, demonstrating the effectiveness and robustness of the proposed asymmetric specialisation model.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7304212152957916}]}], "introductionContent": [{"text": "Word representation learning has become a research area of central importance in NLP, with its usefulness demonstrated across application areas such as parsing), machine translation (, and many others ().", "labels": [], "entities": [{"text": "Word representation learning", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8104304273923238}, {"text": "parsing", "start_pos": 152, "end_pos": 159, "type": "TASK", "confidence": 0.9712991714477539}, {"text": "machine translation", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.8297046720981598}]}, {"text": "Standard techniques for inducing word embeddings rely on the distributional hypothesis, using co-occurrence information from large textual corpora to learn meaningful word representations (.", "labels": [], "entities": []}, {"text": "A major drawback of the distributional hypothesis is that it coalesces different relationships between words, such as synonymy and topical relatedness, into a single vector space.", "labels": [], "entities": []}, {"text": "A popular solution is to go beyond stand-alone unsupervised learning and fine-tune distributional vector spaces by using external knowledge from human-or automaticallyconstructed knowledge bases.", "labels": [], "entities": []}, {"text": "This is often done as a post-processing step, where distributional vectors are gradually refined to satisfy linguistic constraints extracted from lexical resources such as WordNet (, the Paraphrase Database (PPDB) (, or BabelNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 172, "end_pos": 179, "type": "DATASET", "confidence": 0.9702211618423462}]}, {"text": "One advantage of post-processing methods is that they treat the input vector space as a black box, making them applicable to any input space.", "labels": [], "entities": []}, {"text": "A key property of these methods is their ability to transform the vector space by specialising it fora particular relationship between words.", "labels": [], "entities": []}, {"text": "1 Prior work has predominantly focused on distinguishing between semantic similarity and conceptual relatedness.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a novel post-processing model which specialises vector spaces for the lexical entailment (LE) relation.", "labels": [], "entities": []}, {"text": "Word-level lexical entailment is an asymmetric semantic relation.", "labels": [], "entities": [{"text": "Word-level lexical entailment", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.607656200726827}]}, {"text": "It is a key principle determining the organisation of semantic networks into hierarchical structures such as semantic ontologies.", "labels": [], "entities": []}, {"text": "Automatic reasoning about LE supports tasks such as taxonomy creation), natural language inference (), text generation (, and metaphor detection (.", "labels": [], "entities": [{"text": "taxonomy creation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.822054535150528}, {"text": "text generation", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.8174287378787994}, {"text": "metaphor detection", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.8933409750461578}]}, {"text": "Our novel LE specialisation model, termed LEAR (Lexical Entailment Attract-Repel), is inspired by ATTRACT-REPEL, a state-of-the-art general spe- ; and 2) by imposing an LE ordering using vector norms, adjusting them so that higher-level concepts have larger norms (e.g., cialisation framework . The key idea of LEAR, illustrated by, is to pull desirable (ATTRACT) examples described by the constraints closer together, while at the same time pushing undesirable (REPEL) word pairs away from each other.", "labels": [], "entities": []}, {"text": "Concurrently, LEAR (re-)arranges vector norms so that norm values in the Euclidean space reflect the hierarchical organisation of concepts according to the given LE constraints: put simply, higher-level concepts are assigned larger norms.", "labels": [], "entities": [{"text": "LEAR", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.8573832511901855}]}, {"text": "Therefore, LEAR simultaneously captures the hierarchy of concepts (through vector norms) and their similarity (through their cosine distance).", "labels": [], "entities": []}, {"text": "The two pivotal pieces of information are combined into an asymmetric distance measure which quantifies the LE strength in the specialised space.", "labels": [], "entities": [{"text": "LE strength", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.963613897562027}]}, {"text": "After specialising four well-known input vector spaces with LEAR, we test them in three standard word-level LE tasks (): 1) hypernymy directionality; 2) hypernymy detection; and 3) combined hypernymy detection/directionality.", "labels": [], "entities": [{"text": "LEAR", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9439987540245056}, {"text": "hypernymy detection", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.6872584223747253}]}, {"text": "Our specialised vectors yield notable improvements over the strongest baselines for each task, with each input space, demonstrating the effectiveness and robustness of LEAR specialisation.", "labels": [], "entities": []}, {"text": "The employed asymmetric distance allows one to make graded assertions about hierarchical relationships between concepts in the specialised space.", "labels": [], "entities": []}, {"text": "This property is evaluated using HyperLex, a recent graded LE dataset . The LEAR-specialised vectors push state-of-the-art Spearman's correlation from 0.540 to 0.686 on the full dataset (2,616 word pairs), and from 0.512 to 0.705 on its noun subset (2,163 word pairs).", "labels": [], "entities": [{"text": "Spearman's correlation", "start_pos": 123, "end_pos": 145, "type": "METRIC", "confidence": 0.5155929525693258}]}, {"text": "The code for the LEAR model is available from: github.com/nmrksic/lear.", "labels": [], "entities": []}], "datasetContent": [{"text": "Starting Linguistic Constraints We use three groups of linguistic constraints in the LEAR specialisation model, covering three different relation types which are all beneficial to the specialisation process: directed 1) lexical entailment (LE) pairs; 2) synonymy pairs; and 3) antonymy pairs.", "labels": [], "entities": []}, {"text": "Synonyms are included as symmetric ATTRACT pairs (i.e., the BA pairs) since they can be seen as defining a trivial symmetric IS-A relation ().", "labels": [], "entities": [{"text": "ATTRACT", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.7820762991905212}, {"text": "BA", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9104986786842346}]}, {"text": "For a similar reason, All vectors are 300-dimensional except for the 600-dimensional CONTEXT2VEC vectors; for further details regarding the architectures and training setup of the used vector collections, we refer the reader to the original papers.", "labels": [], "entities": []}, {"text": "We also experimented with dependency-based SGNS vectors (, observing similar patterns in the results.", "labels": [], "entities": []}, {"text": "antonyms are clear REPEL constraints as they anticorrelate with the LE relation.", "labels": [], "entities": [{"text": "REPEL", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9071381092071533}]}, {"text": "Synonymy and antonymy constraints are taken from prior work (: they are extracted from WordNet and Roget.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9588208794593811}]}, {"text": "In total, we work with 1,023,082 synonymy pairs (11.7 synonyms per word on average) and 380,873 antonymy pairs (6.5 per word).", "labels": [], "entities": []}, {"text": "As in prior work (, LE constraints are extracted from the WordNet hierarchy, relying on the transitivity of the LE relation.", "labels": [], "entities": []}, {"text": "This means that we include both direct and indirect LE pairs in our set of constraints (e.g., (pangasius, fish), (fish, animal), and (pangasius, animal)).", "labels": [], "entities": []}, {"text": "We retained only noun-noun and verb-verb pairs, while the rest were discarded: the final number of LE constraints is 1,545,630.", "labels": [], "entities": []}, {"text": "6 Training Setup We adopt the original ATTRACT-REPEL model setup without any fine-tuning.", "labels": [], "entities": [{"text": "ATTRACT-REPEL", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.7347224950790405}]}, {"text": "Hyperparameter values are set to: \u03b4 att = 0.6, \u03b4 rep = 0.0, \u03bb reg = 10 \u22129 ).", "labels": [], "entities": [{"text": "\u03b4 rep", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.8263221681118011}]}, {"text": "The models are trained for 5 epochs with the AdaGrad algorithm (), with batch sizes set to k 1 = k 2 = k 3 = 128 for faster convergence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: L2 norms for selected concepts from the  WordNet hierarchy. Input: FASTTEXT; LEAR: D2.", "labels": [], "entities": [{"text": "WordNet hierarchy", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.9154644906520844}, {"text": "FASTTEXT", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9016695618629456}, {"text": "LEAR", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9908664226531982}]}, {"text": " Table 2: Analysing the importance of the synergy in  the FULL LEAR model on the final performance  on WBLESS, BLESS, HyperLex-All (HL-A) and  HyperLex-Nouns (HL-N). Input: FASTTEXT. D2.", "labels": [], "entities": [{"text": "FULL LEAR", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9162758588790894}, {"text": "WBLESS", "start_pos": 103, "end_pos": 109, "type": "DATASET", "confidence": 0.8758451342582703}, {"text": "BLESS", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.9977359771728516}, {"text": "FASTTEXT. D2", "start_pos": 173, "end_pos": 185, "type": "DATASET", "confidence": 0.8083485762278239}]}]}