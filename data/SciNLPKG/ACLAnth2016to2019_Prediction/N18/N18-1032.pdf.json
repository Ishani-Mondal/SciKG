{"title": [{"text": "Universal Neural Machine Translation for Extremely Low Resource Languages", "labels": [], "entities": [{"text": "Universal Neural Machine Translation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.643813356757164}]}], "abstractContent": [{"text": "In this paper, we propose anew universal machine translation approach focusing on languages with a limited amount of parallel data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7031207382678986}]}, {"text": "Our proposed approach utilizes a transfer-learning approach to share lexical and sentence level representations across multiple source languages into one target language.", "labels": [], "entities": []}, {"text": "The lexical part is shared through a Universal Lexical Representation to support multilingual word-level sharing.", "labels": [], "entities": []}, {"text": "The sentence-level sharing is represented by a model of experts from all source languages that share the source encoders with all other languages.", "labels": [], "entities": []}, {"text": "This enables the low-resource language to utilize the lexical and sentence representations of the higher resource languages.", "labels": [], "entities": []}, {"text": "Our approach is able to achieve 23 BLEU on Romanian-English WMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEU of strong baseline system which uses multilingual training and back-translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9983291029930115}, {"text": "WMT2016", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.5309669375419617}, {"text": "BLEU", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9925400614738464}]}, {"text": "Furthermore , we show that the proposed approach can achieve almost 20 BLEU on the same dataset through fine-tuning a pre-trained multilingual system in a zero-shot setting.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9983286261558533}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) () has achieved remarkable translation quality in various on-line large-scale systems ( as well as achieving state-of-the-art results on Chinese-English translation ().", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7789171834786733}, {"text": "Chinese-English translation", "start_pos": 170, "end_pos": 197, "type": "TASK", "confidence": 0.6902232319116592}]}, {"text": "With such large systems, NMT showed that it can scale up to immense amounts of parallel data in the order of tens of millions of sentences.", "labels": [], "entities": [{"text": "NMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.5325602293014526}]}, {"text": "However, such data is not widely available for all language pairs and domains.", "labels": [], "entities": []}, {"text": "* This work was done while the authors at In this paper, we propose a novel universal multilingual NMT approach focusing mainly on low resource languages to overcome the limitations of NMT and leverage the capabilities of multi-lingual NMT in such scenarios.", "labels": [], "entities": []}, {"text": "Our approach utilizes multi-lingual neural translation system to share lexical and sentence level representations across multiple source languages into one target language.", "labels": [], "entities": []}, {"text": "In this setup, some of the source languages maybe of extremely limited or even zero data.", "labels": [], "entities": []}, {"text": "The lexical sharing is represented by a universal word-level representation where various words from all source languages share the same underlaying representation.", "labels": [], "entities": []}, {"text": "The sharing module utilizes monolingual embeddings along with seed parallel data from all languages to build the universal representation.", "labels": [], "entities": []}, {"text": "The sentence-level sharing is represented by a model of language experts which enables low-resource languages to utilize the sentence representation of the higher resource languages.", "labels": [], "entities": []}, {"text": "This allows the system to translate from any language even with tiny amount of parallel resources.", "labels": [], "entities": []}, {"text": "We evaluate the proposed approach on 3 different languages with tiny or even zero parallel data.", "labels": [], "entities": []}, {"text": "We show that for the simulated \"zero-resource\" settings, our model can consistently outperform a strong multi-lingual NMT baseline with a tiny amount of parallel sentence pairs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extensively study the effectiveness of the proposed methods by evaluating on three \"almost-zeroresource\" language pairs with variant auxiliary languages.", "labels": [], "entities": []}, {"text": "The vanilla single-source NMT and the multi-lingual NMT models are used as baselines.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the available parallel resource in our experiments. All the languages are translated to English.", "labels": [], "entities": []}, {"text": " Table 2: Scores over variant source languages (6k sen- tences for Ro & Lv, and 10k for Ko). \"Multi\" means  the Multi-lingual NMT baseline.", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores evaluated on test set (6k), com- pared with ULR and MoLE. \"vanilla\" is the standard  NMT system trained only on Ro-En training set", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984533786773682}, {"text": "ULR", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.489965558052063}, {"text": "MoLE", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.8085492849349976}]}]}