{"title": [{"text": "Accurate Text-Enhanced Knowledge Graph Representation Learning", "labels": [], "entities": [{"text": "Accurate Text-Enhanced Knowledge Graph Representation Learning", "start_pos": 0, "end_pos": 62, "type": "TASK", "confidence": 0.7234206000963846}]}], "abstractContent": [{"text": "Previous representation learning techniques for knowledge graph representation usually represent the same entity or relation in different triples with the same representation, without considering the ambiguity of relations and entities.", "labels": [], "entities": [{"text": "knowledge graph representation", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.6422828535238901}]}, {"text": "To appropriately handle the semantic variety of entities/relations in distinct triples, we propose an accurate text-enhanced knowledge graph representation learning method, which can represent a relation/entity with different representations in different triples by exploiting additional textual information.", "labels": [], "entities": []}, {"text": "Specifically , our method enhances representations by exploiting the entity descriptions and triple-specific relation mention.", "labels": [], "entities": []}, {"text": "And a mutual attention mechanism between relation mention and entity description is proposed to learn more accurate textual representations for further improving knowledge graph representation.", "labels": [], "entities": [{"text": "knowledge graph representation", "start_pos": 162, "end_pos": 192, "type": "TASK", "confidence": 0.6277323464552561}]}, {"text": "Experimental results show that our method achieves the state-of-the-art performance on both link prediction and triple classification tasks, and significantly outperforms previous text-enhanced knowledge representation models .", "labels": [], "entities": [{"text": "link prediction", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.7736960351467133}, {"text": "triple classification tasks", "start_pos": 112, "end_pos": 139, "type": "TASK", "confidence": 0.7571201721827189}]}], "introductionContent": [{"text": "Knowledge graphs such as Freebase (, and WordNet are among the most widely used resources in NLP applications.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9639156460762024}, {"text": "WordNet", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9685714244842529}]}, {"text": "Typically, a knowledge graph consists of a set of triples {(h, r, t)}, where h, r, t stand for head entity, relation and tail entity respectively.", "labels": [], "entities": []}, {"text": "Learning distributional representation of knowledge graph has attracted many research attentions in recent years.", "labels": [], "entities": [{"text": "Learning distributional representation", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6647545198599497}]}, {"text": "By projecting all elements in a knowledge graph into a dense vector space, the semantic distance between all elements can be easily calculated, and thus enables many applications such as link prediction and triple classification.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 187, "end_pos": 202, "type": "TASK", "confidence": 0.8288190364837646}, {"text": "triple classification", "start_pos": 207, "end_pos": 228, "type": "TASK", "confidence": 0.839721143245697}]}, {"text": "Recently, translation-based models, including), TransH (), and), have achieved promising results in distributional representation learning of knowledge graph. has achieved the state-of-the-art performance on multiple tasks, such as triple classification and link prediction.", "labels": [], "entities": [{"text": "distributional representation learning", "start_pos": 100, "end_pos": 138, "type": "TASK", "confidence": 0.7239263852437338}, {"text": "triple classification", "start_pos": 232, "end_pos": 253, "type": "TASK", "confidence": 0.8276219069957733}, {"text": "link prediction", "start_pos": 258, "end_pos": 273, "type": "TASK", "confidence": 0.7835158109664917}]}, {"text": "Unfortunately, all of these methods only utilize the structure information of knowledge graph, which inevitably suffer from the sparseness and incompleteness of knowledge graph.", "labels": [], "entities": []}, {"text": "Even worse, structure information usually cannot distinguish the different meanings of relations and entities in different triples.", "labels": [], "entities": []}, {"text": "To address the above problem, additional information is introduced to enrich the knowledge representations, including entity types and logic rules.", "labels": [], "entities": []}, {"text": "However, most researches of this line are limited by manually constructed logic rules, which are knowledge graph sensitive and require the expert knowledge.", "labels": [], "entities": []}, {"text": "Another type of widely used resources is textual information, such as entity descriptions and words co-occurrence with entities).", "labels": [], "entities": []}, {"text": "The main drawback of the above methods is that they represent the same entity/relation in different triples with a unique representation.", "labels": [], "entities": []}, {"text": "Unfortunately, by detailed analyzing the triples in knowledge graph, we find two problems of the unique representation: (1) Relations are ambiguous, i.e., the accurate semantic meaning of a relation in a specific triple is related to the entities in the same triple.", "labels": [], "entities": []}, {"text": "For example, the relation \"parentOf\" may refer to two different meanings of (i.e., \"father\" and \"mother\"), depending on the entities in triples.", "labels": [], "entities": []}, {"text": "(2) Because different relations may concern different attributes of an entity, the same entity may express different aspects in different triples.", "labels": [], "entities": []}, {"text": "For example, different words in the description of \"Barack Obama\" should be emphasized by relations \"parentOf\" and \"professionOf\".", "labels": [], "entities": []}, {"text": "The ambiguity of entity/relation has been considered as one of the primary reasons why translation-based models cannot handle 1-to-N, N-to-1 and N-to-N categories of relations ().", "labels": [], "entities": []}, {"text": "tried to solve the two issues using words cooccurrence with the entities in the same sentences.", "labels": [], "entities": []}, {"text": "Despite its apparent success, there remains a major drawback: this method suffers from noisy text, which reduces the value of textual information.", "labels": [], "entities": []}, {"text": "To solve above problems, this paper proposes an accurate text-enhanced knowledge representation model, which can enhance the representations of entities and relations by incorporating accurate textual information for each triple.", "labels": [], "entities": []}, {"text": "To learn the representation of a given triple, we first extract its accurate relation mentions from text corpus, which reflect the specific relationship between its head entity and tail entity.", "labels": [], "entities": []}, {"text": "Then a mutual attention mechanism between relation mention and entity descriptions (extracted from knowledge graph), is introduced to enhance the representations of entities and relations.", "labels": [], "entities": []}, {"text": "For example, the two triples in have the same \"parentOf\" relationship, but have different underlying semantics \"was the father of \" and \"was the mother of \" respectively.", "labels": [], "entities": []}, {"text": "Besides, our mutual attention mechanism enables knowledge representation focusing more on related information from text information.", "labels": [], "entities": [{"text": "knowledge representation", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.7592266499996185}]}, {"text": "For example, the \"parentOf\" relation will concern more about the social relations and gender attributes of a person, rather than his/her jobs, which are also contained in its descriptions.", "labels": [], "entities": []}, {"text": "And such a relation-specific entity description will make an entity has more appropriate, relation-specific representations in different triples.", "labels": [], "entities": []}, {"text": "Concretely, we employ BiLSTM model) with mutual attention mechanism ( ) to learn representations for relation mentions and entity descriptions.", "labels": [], "entities": []}, {"text": "Specifically, in order to generate triple-specific textual representation of entities and relation, a mutual attention mechanism is proposed to model relation between entity descriptions and relation mention of one triple.", "labels": [], "entities": []}, {"text": "Then the learned textual representations are incorporated with previous traditional transitionbased representations, which are, learned from structural information of knowledge graph, directly to obtain enhanced triple specific representations of elements.", "labels": [], "entities": []}, {"text": "We evaluate our method on both link prediction task and triple classification task, using benchmark datasets from Freebase 1 and WordNet 2 with the text corpus.", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.8440001010894775}, {"text": "triple classification task", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7646620174249014}]}, {"text": "Experimental results show that, our model achieves the state-of-the-art performance, and significantly outperforms previous text-enhanced models.", "labels": [], "entities": []}, {"text": "The main contributions are threefold: (i) To the best of our knowledge, this is the first work which simultaneously exploits both relation mention and entity description to handle the ambiguity of relations and entities (Section 3).", "labels": [], "entities": []}, {"text": "(ii) We propose a mutual attention mechanism which exploits the textual representations of relation and entity to enhance each other (Section 3.2).", "labels": [], "entities": []}, {"text": "(iii) This paper achieves new state-of-the-art performances on triple classification tasks over two most widely used benchmarks (Section 4).", "labels": [], "entities": [{"text": "triple classification tasks", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.784747044245402}]}], "datasetContent": [{"text": "In this section, we first describe the settings in our experiments, and then we conduct experiments of link prediction and triple classification tasks and compare our method with base models and the state-of-the-art baselines.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.7594462931156158}, {"text": "triple classification tasks", "start_pos": 123, "end_pos": 150, "type": "TASK", "confidence": 0.7171480854352316}]}, {"text": "In this paper, we evaluate our model on four benchmark datasets: WN11, WN18, FB13 and FB15k ().", "labels": [], "entities": [{"text": "WN11", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.955705463886261}, {"text": "WN18", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.7757148146629333}, {"text": "FB13", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.6666756272315979}, {"text": "FB15k", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.543804943561554}]}, {"text": "For the text corpus, we use a snapshot of the English Wikipedia (Wiki) (Shaoul and Westbury, 2010) 3 dump in April 2016, which contains more than 1.2 billion tokens.", "labels": [], "entities": [{"text": "English Wikipedia (Wiki) (Shaoul and Westbury, 2010) 3 dump in April 2016", "start_pos": 46, "end_pos": 119, "type": "DATASET", "confidence": 0.8966212237582487}]}, {"text": "We link entities in the text corpus to entities in Freebase and synsets in WordNet as described above, and replace entities with HEAD TAG and TAIL TAG.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.908574640750885}]}, {"text": "The text descriptions of entities are freely available . In addition, we pre-process the word-entity corpus, including stemming, lowercasing and removing words with fewer than 5 occurrences.", "labels": [], "entities": []}, {"text": "The statistics of the datasets and linked-entities in text corpus are shown in  As introduced above, we implement our framework using TransE, TransH, TransR and ComplEx as base models, and evaluate on two classi-cal tasks: link prediction and triple classification.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 223, "end_pos": 238, "type": "TASK", "confidence": 0.7941060662269592}, {"text": "triple classification", "start_pos": 243, "end_pos": 264, "type": "TASK", "confidence": 0.7049442827701569}]}, {"text": "We refer AAT E E as the proposed model which enhances TransE with accurate textual informations and mutual attention mechanism, and refer AT E E as the proposed model without mutual attention mechanism to reveal the effect of our attention mechanism.", "labels": [], "entities": [{"text": "AAT E E", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.8643777171770731}, {"text": "AT E E", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.8977593382199606}]}, {"text": "To speedup training and reduce overfitting, we employ the SkipGram model of word2vec () to pre-train the word embeddings with the dimension of word embeddings is d w = 200, the windows size is 5, the number of iterations is 5, and the number of negative samples is 10.", "labels": [], "entities": []}, {"text": "And we pre-train the representations of entities and relations of knowledge graph using the mentioned base models, and the parameters are empirically tuned as follows: the dimension of vectors is d kg = 200, the number of epochs is 2000 and the margin is 1.0.", "labels": [], "entities": [{"text": "margin", "start_pos": 245, "end_pos": 251, "type": "METRIC", "confidence": 0.991885781288147}]}, {"text": "We implement our model based on the OpenKE 5 framework.", "labels": [], "entities": [{"text": "OpenKE 5 framework", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.934432844320933}]}, {"text": "In our experiments, the hyper-parameters of BiLSTM are empirically set as follows: the number of hidden units is d h = 200, the learning rates for SGD are among {0.1, 0.001, 0.0001}, the margin \u03bb values are among {0.5, 1.0, 2.0} and the batch sizes are among {100, 500, 2000}.", "labels": [], "entities": []}, {"text": "We employ two different BiLSTM networks with the same hyper-parameters to learn the representations of text mentions and entity descriptions.", "labels": [], "entities": []}, {"text": "And all the parameters are learned jointly, including BiLSTM networks and knowledge representations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of different datasets and the number  of entities linked in Wikipedia, #Linked represents the  number of entities linked in the text corpus. #N-to-1 is  the number of N-to-1 type of relations.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results of link prediction.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7878455519676208}]}, {"text": " Table 3: Hit@10 of link prediction on different type of relations on FB15k dataset.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.697558730840683}, {"text": "FB15k dataset", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.9947197437286377}]}, {"text": " Table 4: The triples whose tail entities were failed to be ranked in top 10 candidates, # is the number of occurrences  of the entity/relation in the training data.", "labels": [], "entities": []}, {"text": " Table 5: Evaluation results of triple classification.", "labels": [], "entities": [{"text": "triple classification", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.9352622926235199}]}]}