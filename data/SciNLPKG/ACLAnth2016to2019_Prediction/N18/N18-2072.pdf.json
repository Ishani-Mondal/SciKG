{"title": [{"text": "Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations", "labels": [], "entities": [{"text": "Contextual Augmentation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8477587699890137}, {"text": "Data Augmentation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.736175462603569}]}], "abstractContent": [{"text": "We propose a novel data augmentation for labeled sentences called contextual augmentation.", "labels": [], "entities": []}, {"text": "We assume an invariance that sentences are natural even if the words in the sentences are replaced with other words with paradigmatic relations.", "labels": [], "entities": []}, {"text": "We stochastically replace words with other words that are predicted by a bi-directional language model at the word positions.", "labels": [], "entities": []}, {"text": "Words predicted according to a context are numerous but appropriate for the augmentation of the original words.", "labels": [], "entities": []}, {"text": "Furthermore, we retrofit a language model with a label-conditional architecture, which allows the model to augment sentences without breaking the label-compatibility.", "labels": [], "entities": []}, {"text": "Through the experiments for six various different text classification tasks, we demonstrate that the proposed method improves classifiers based on the convolutional or recurrent neural networks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.8080889383951823}]}], "introductionContent": [{"text": "Neural network-based models for NLP have been growing with state-of-the-art results in various tasks, e.g., dependency parsing (), text classification (, machine translation ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.8388612866401672}, {"text": "text classification", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.8108509480953217}, {"text": "machine translation", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.8193854093551636}]}, {"text": "However, machine learning models often overfit the training data by losing their generalization.", "labels": [], "entities": []}, {"text": "Generalization performance highly depends on the size and quality of the training data and regularizations.", "labels": [], "entities": [{"text": "Generalization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9302443861961365}]}, {"text": "Preparing a large annotated dataset is very time-consuming.", "labels": [], "entities": []}, {"text": "Instead, automatic data augmentation is popular, particularly in the areas of vision () and speech (Jaitly and Hinton, 2015;.", "labels": [], "entities": []}, {"text": "Data augmentation is basically performed based on human knowledge on invariances, rules, or heuristics, e.g., \"even if a picture is flipped, the class of an object should be unchanged\".", "labels": [], "entities": [{"text": "Data augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7326025515794754}]}, {"text": "However, usage of data augmentation for NLP has been limited.", "labels": [], "entities": []}, {"text": "In natural languages, it is very difficult to obtain universal rules for transformations which assure the quality of the produced data and are easy to apply automatically in various domains.", "labels": [], "entities": []}, {"text": "A common approach for such a transformation is to replace words with their synonyms selected from a handcrafted ontology such as WordNet or word similarity calculation (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.9349938631057739}, {"text": "word similarity calculation", "start_pos": 140, "end_pos": 167, "type": "TASK", "confidence": 0.7198280890782675}]}, {"text": "Because words having exactly or nearly the same meanings are very few, synonym-based augmentation can be applied to only a small percentage of the vocabulary.", "labels": [], "entities": []}, {"text": "Other augmentation methods are known but are often developed for specific domains with handcrafted rules or pipelines, with the loss of generality.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel data aug-mentation method called contextual augmentation.", "labels": [], "entities": []}, {"text": "Our method offers a wider range of substitute words by using words predicted by a bidirectional language model (LM) according to the context, as shown in.", "labels": [], "entities": []}, {"text": "This contextual prediction suggests various words that have paradigmatic relations with the original words.", "labels": [], "entities": []}, {"text": "Such words can also be good substitutes for augmentation.", "labels": [], "entities": []}, {"text": "Furthermore, to prevent word replacement that is incompatible with the annotated labels of the original sentences, we retrofit the LM with a label-conditional architecture.", "labels": [], "entities": [{"text": "word replacement", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7155746519565582}]}, {"text": "Through the experiment, we demonstrate that the proposed conditional LM produces good words for augmentation, and contextual augmentation improves classifiers using recurrent or convolutional neural networks (RNN or CNN) in various classification tasks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Accuracies of the models for various bench- marks. The accuracies are averaged over eight models  trained from different seeds.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9973986148834229}, {"text": "accuracies", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9785460829734802}]}]}