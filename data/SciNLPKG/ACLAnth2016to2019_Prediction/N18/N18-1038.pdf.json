{"title": [], "abstractContent": [{"text": "We investigate grounded sentence representations , where we train a sentence encoder to predict the image features of a given caption-i.e., we try to \"imagine\" how a sentence would be depicted visually-and use the resultant features as sentence representations.", "labels": [], "entities": []}, {"text": "We examine the quality of the learned representations on a variety of standard sentence representation quality benchmarks, showing improved performance for grounded models over non-grounded ones.", "labels": [], "entities": []}, {"text": "In addition, we thoroughly analyze the extent to which grounding contributes to improved performance, and show that the system also learns improved word embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Following the word embedding upheaval of the past few years, one of NLP's next big challenges has become the hunt for universal sentence representations: generic representations of sentence meaning that can be \"plugged into\" any kind of system or pipeline.", "labels": [], "entities": []}, {"text": "Examples include Paragraph2Vec (), C-Phrase (), SkipThought ( and FastSent ().", "labels": [], "entities": [{"text": "SkipThought", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.7687076926231384}]}, {"text": "These representations tend to be learned from large corpora in an unsupervised setting, much like word embeddings, and effectively \"transferred\" to the task at hand.", "labels": [], "entities": []}, {"text": "Purely text-based semantic models, which represent word meaning as a distribution over other words, suffer from the grounding problem.", "labels": [], "entities": []}, {"text": "It has been shown that grounding leads to improved performance on a variety of word-level tasks.", "labels": [], "entities": []}, {"text": "Unsupervised sentence representation models are often doubly exposed to the grounding problem, especially if they represent sentence mean1Work done while at Facebook AI Research.", "labels": [], "entities": []}, {"text": "ings as a distribution over other sentences, as in SkipThought (.", "labels": [], "entities": [{"text": "SkipThought", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.936347484588623}]}, {"text": "Here, we examine whether grounding also leads to improved sentence representations.", "labels": [], "entities": []}, {"text": "In short, the grounding problem is characterized by the lack of an association between symbols and external information.", "labels": [], "entities": []}, {"text": "We address this problem by aligning text with paired visual data and hypothesize that sentence representations can be enriched with external information-i.e., grounded-by forcing them to capture visual semantics.", "labels": [], "entities": []}, {"text": "We investigate the performance of these representations and the effect of grounding on a variety of semantic benchmarks.", "labels": [], "entities": []}, {"text": "There has been much recent interest in generating actual images from text (.", "labels": [], "entities": []}, {"text": "Our method takes a slightly different approach: instead of predicting actual images, we train a deep recurrent neural network to predict the latent feature representation of images.", "labels": [], "entities": []}, {"text": "That is, we are specifically interested in the semantic content of visual representations and how useful that information is for learning sentence representations.", "labels": [], "entities": []}, {"text": "One can think of this as trying to imagine, or form a \"mental picture\", of a sentence's meaning (.", "labels": [], "entities": []}, {"text": "Much like a sentence's meaning in classical semantics is given by its model-theoretic ground truth, our ground truth is provided by images.", "labels": [], "entities": []}, {"text": "Grounding is likely to be more useful for concrete words and sentences: a sentence such as \"democracy is apolitical system\" does not yield any coherent mental picture.", "labels": [], "entities": []}, {"text": "In order to accommodate the fact that much of language is abstract, we take sentence representations obtained using textonly data (which are better for representing abstract meaning) and combine them with the grounded representations that our system learns (which are good for representing concrete meaning), leading to multi-modal sentence representations.", "labels": [], "entities": []}, {"text": "In what follows, we introduce a system for grounding sentence representations by learning to predict visual content.", "labels": [], "entities": []}, {"text": "Although it is not the primary aim of this work, it is important to first examine how well this system achieves what it is trained to do, by evaluating on the COCO5K image and caption retrieval task.", "labels": [], "entities": [{"text": "caption retrieval", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.7277660369873047}]}, {"text": "We then analyze the performance of grounded representations on a variety of sentence-level semantic transfer tasks, showing that grounding increases performance over textonly representations.", "labels": [], "entities": [{"text": "sentence-level semantic transfer tasks", "start_pos": 76, "end_pos": 114, "type": "TASK", "confidence": 0.6987467557191849}]}, {"text": "We then investigate an important open question in multi-modal semantics: to what extent are improvements in semantic performance due to grounding, rather than to having more data or data from a different distribution?", "labels": [], "entities": []}, {"text": "In the remainder, we analyze the role that concreteness plays in representation quality and show that our system learns grounded word embedding projections that outperform non-grounded ones.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work to comprehensively study grounding for distributed sentence representations on such a wide set of semantic benchmark tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the same COCO splits as for training (113,287 images), validation (5000 images) and testing (5000 images).", "labels": [], "entities": [{"text": "COCO splits", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.6014809161424637}, {"text": "validation", "start_pos": 62, "end_pos": 72, "type": "TASK", "confidence": 0.9315890073776245}]}, {"text": "Image features for COCO were obtained by transferring the final layer from a ResNet-101 () trained on ImageNet (ILSVRC 2015).", "labels": [], "entities": [{"text": "ImageNet (ILSVRC 2015)", "start_pos": 102, "end_pos": 124, "type": "DATASET", "confidence": 0.8561985850334167}]}], "tableCaptions": [{"text": " Table 1: Retrieval (higher is better) results on COCO, plus median rank (MEDR) and mean rank (MR) (lower is  better). Note that while this work underwent review, better methods have been published, most notably VSE++  (Faghri et al., 2017).", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9735996723175049}, {"text": "COCO", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.793738842010498}, {"text": "median rank (MEDR)", "start_pos": 61, "end_pos": 79, "type": "METRIC", "confidence": 0.8456564784049988}, {"text": "mean rank (MR)", "start_pos": 84, "end_pos": 98, "type": "METRIC", "confidence": 0.9608833432197571}, {"text": "VSE++", "start_pos": 212, "end_pos": 217, "type": "DATASET", "confidence": 0.8164749145507812}]}, {"text": " Table 2: Accuracy results on sentence classification and entailment tasks.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981181621551514}, {"text": "sentence classification", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.7833871841430664}]}, {"text": " Table 3: Thorough investigation of the contribution of grounding, ensuring equal number of components and  identical architectures, on the variety of sentence-level semantic benchmark tasks. STb=SkipThought-like model  with bidirectional LSTM+max. 2\u00d7STb-1024=ensemble of 2 different STb models with different initializations.  GroundSent is STb-1024+Cap2Cap/Img/Both. We find that performance improvements are sometimes due to  having more parameters, but in most cases due to grounding.", "labels": [], "entities": []}, {"text": " Table 5: Spearman \u03c1 s correlation on four standard se- mantic similarity evaluation benchmarks.", "labels": [], "entities": []}]}