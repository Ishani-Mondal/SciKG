{"title": [{"text": "Modeling Inter-Aspect Dependencies for Aspect-Based Sentiment Analysis", "labels": [], "entities": [{"text": "Aspect-Based Sentiment Analysis", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.6639831960201263}]}], "abstractContent": [{"text": "Aspect-based Sentiment Analysis is a fine-grained task of sentiment classification for multiple aspects in a sentence.", "labels": [], "entities": [{"text": "Aspect-based Sentiment Analysis", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8286508719126383}, {"text": "sentiment classification", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.8826308250427246}]}, {"text": "Present neural-based models exploit aspect and its contextual information in the sentence but largely ignore the inter-aspect dependencies.", "labels": [], "entities": []}, {"text": "In this paper, we incorporate this pattern by simultaneous classification of all aspects in a sentence along with temporal dependency processing of their corresponding sentence representations using recurrent networks.", "labels": [], "entities": []}, {"text": "Results on the benchmark SemEval 2014 dataset suggest the effectiveness of our proposed approach.", "labels": [], "entities": [{"text": "SemEval 2014 dataset", "start_pos": 25, "end_pos": 45, "type": "DATASET", "confidence": 0.7739653388659159}]}], "introductionContent": [{"text": "Aspect-based Sentiment Analysis (ABSA) is a fine-grained task of sentiment classification.", "labels": [], "entities": [{"text": "Aspect-based Sentiment Analysis (ABSA)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7693254699309667}, {"text": "sentiment classification", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.9146970808506012}]}, {"text": "Sentimentally involved sentences in reviews, debates, etc., often comprise of multiple aspects that have varied sentiment polarities.", "labels": [], "entities": []}, {"text": "An important subtask of ABSA is aspect or aspect-term classification which involves predicting sentiment of aspects embodied in a sentence (.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.9612785577774048}, {"text": "aspect or aspect-term classification", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.5719119384884834}, {"text": "predicting sentiment of aspects embodied in a sentence", "start_pos": 84, "end_pos": 138, "type": "TASK", "confidence": 0.8108460307121277}]}, {"text": "Present works in the literature approach this task by analyzing associations between aspects and their contexts provided in the sentence.", "labels": [], "entities": []}, {"text": "In this work, we argue that to classify an aspect into sentiment categories, knowledge of surrounding aspects, their sentiment orientation, and resulting inter-dependencies, is beneficial.", "labels": [], "entities": []}, {"text": "Inter-aspect dependencies abound in sentences with multiple aspects.", "labels": [], "entities": []}, {"text": "Largely ignored in present * means authors contributed equally.", "labels": [], "entities": []}, {"text": "literature, these dependencies may reveal themselves in many forms, such as a) Incomplete information, where a certain aspect does not contain enough contextual information to convey the sentiment.", "labels": [], "entities": []}, {"text": "In such cases, the surrounding aspects and their sentiment tone become crucial to fill the contextual gap.", "labels": [], "entities": []}, {"text": "As an example, in the sentence The menu is very limited -I think we counted 4 or 5 entries., the subsentence I think ...", "labels": [], "entities": []}, {"text": "entries containing aspect entries does not provide the required sentiment unless considered with the aspect menu.", "labels": [], "entities": []}, {"text": "Here, the negative sentiment of menu induces entries to have the same sentiment.", "labels": [], "entities": []}, {"text": "b) Sentiment influence in conjunctions, in which, the sentiment of an aspect in a sentence influences the succeeding aspects due to the presence of conjunctions.", "labels": [], "entities": []}, {"text": "In particular, for sentences containing conjunctions like and, not only, also, but, however, though, etc., aspects tend to share/contrast their sentiments.", "labels": [], "entities": []}, {"text": "In the sentence Food is usually very good, though I wonder about freshness of raw vegetables, the aspect raw vegetables does not have any sentiment marker linked to it.", "labels": [], "entities": []}, {"text": "However, the positive sentiment of food due to the word good and presence of conjunction though determines the sentiment of raw vegetables to be negative.", "labels": [], "entities": []}, {"text": "Thus, aspects when arranged as a sequence, reveal high correlation and interplay of sentiments.", "labels": [], "entities": []}, {"text": "In this paper, we facilitate such phenomena by proposing a neural network where the information is shared among the aspects by means of a Long Short-Term Memory (LSTM) network.", "labels": [], "entities": []}, {"text": "In other words, we model the sequential relationship between the aspects as per their occurrence in the sentence.", "labels": [], "entities": []}, {"text": "Specifically, our model first takes a sentence along with all of its aspect-terms and then generates the sentential representations relative to each aspect to get better aspect-oriented features).", "labels": [], "entities": []}, {"text": "This is done using an attentionbased LSTM network, where the attention mechanism enables the model to focus on key parts of the sentence that modulate the sentiment of the aspects.", "labels": [], "entities": []}, {"text": "To further guide the attention process the model incorporates aspect information at the word-level by concatenating aspect representations with each word (.", "labels": [], "entities": []}, {"text": "Finally, to capture the inter-aspect dependencies, the aspect-based sentential representations are ordered as a sequence and temporally modeled using another LSTM.", "labels": [], "entities": []}, {"text": "Each timestep of this LSTM corresponds to a particular aspect.", "labels": [], "entities": []}, {"text": "The hidden state output for each timestep is then projected to a dense layer and fed to a softmax classifier to predict the polarities of the corresponding aspect.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, use of inter-aspect dependencies in neural models is unprecedented and fills a significant gap in the literature.", "labels": [], "entities": []}, {"text": "In the remaining paper, Section 2 first provides a summary of existing works; Section 3 then describes the proposed approach in detail; Section 4 gives training and dataset details followed by results and a qualitative case study.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Training details: To perform experiments and subsequent hyperparameter tuning, we first split the training set randomly in the ratio 9 1 to get a held-out validation set.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.6949557811021805}]}, {"text": "For optimization, we use the Adam optimizer () having learning rate 0.01.", "labels": [], "entities": []}, {"text": "Embedding dimensions are set as follows, d a = 100, d sand dad = 300.", "labels": [], "entities": []}, {"text": "To facilitate batch processing, we attach dummy aspects in sentences with lesser aspects and also provide masking schemes.", "labels": [], "entities": []}, {"text": "For termination, we use the early-stopping procedure with a patience value of 10 that is monitored on the validation loss.", "labels": [], "entities": [{"text": "termination", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9691888689994812}, {"text": "patience", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.989599347114563}, {"text": "validation", "start_pos": 106, "end_pos": 116, "type": "TASK", "confidence": 0.9188022017478943}]}, {"text": "Dataset: We conduct our experiments using the dataset for SemEval 2014 Task 4 containing customer reviews on restaurants and laptops.", "labels": [], "entities": [{"text": "SemEval 2014 Task 4", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7412697374820709}]}, {"text": "Each review has one or more aspects with their corresponding polarities.", "labels": [], "entities": []}, {"text": "The polarity of an aspect can be positive, negative, neutral or conflict; however, we consider the first three labels for classification.", "labels": [], "entities": []}, {"text": "contains the statistics for the dataset.", "labels": [], "entities": []}, {"text": "presents the results of our proposed model along with state-of-the-art methods.", "labels": [], "entities": []}, {"text": "Our model significantly surpasses the performance of ATAE-LSTM (.", "labels": [], "entities": [{"text": "ATAE-LSTM", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.586357057094574}]}, {"text": "Given that ATAE's architecture has a strong correlation to our aspectbased sentential generator (see), their work can be categorized as a baseline to our model.", "labels": [], "entities": []}, {"text": "This reinforces our hypothesis that a model capable of capturing inter-aspect dependencies indeed performs better.", "labels": [], "entities": []}, {"text": "We also compare our model to the recently proposed IAN ().", "labels": [], "entities": []}, {"text": "On both datasets, our model performs competitively with IAN and produces nominal improvement.", "labels": [], "entities": [{"text": "IAN", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.6173298954963684}]}, {"text": "Given that IAN explores the inter-dependencies of aspects with their contexts, while we try to model inter-dependencies between aspects, an interesting direction would be to explore the IAN modeled in our proposed setting (Phase 2 of).", "labels": [], "entities": []}, {"text": "We set this path as an option for future research.  model.", "labels": [], "entities": []}, {"text": "Specifically, we tryout variants (a) Without attention: in this setting, we omit the attention mechanism while generating aspect-based sentential representation s i (Equation 1-3).", "labels": [], "entities": []}, {"text": "Instead, we define s i to be h n s , i.e., the last hidden state vector of LSTM s with input Sand A i . However, removing attention brings degradation in the performance of our model on the Restaurant and Laptop dataset by 4% and 3%, respectively.", "labels": [], "entities": [{"text": "Restaurant and Laptop dataset", "start_pos": 190, "end_pos": 219, "type": "DATASET", "confidence": 0.7060371935367584}]}, {"text": "This signifies the importance of an attention mechanism to derive the aspect-based sentential representations.", "labels": [], "entities": []}, {"text": "(b) With hadamard fusion: instead of concatenation of w j and ti , we use the hadamard product which is the element wise multiplication of the vectors.", "labels": [], "entities": []}, {"text": "Although this variation reduces the total parameter sizes of the network, it still does not benefit the model and gives a poorer performance to simple concatenation.", "labels": [], "entities": []}, {"text": "Numerous other fusion methods such as tensor fusion , compact bilinear pooling (, attention-based fusion (, etc. are applicable, whose analyses, however, is not the focus of this paper.", "labels": [], "entities": [{"text": "tensor fusion", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.8135402500629425}]}], "tableCaptions": [{"text": " Table 1 also presents variations of our proposed", "labels": [], "entities": []}, {"text": " Table 2: Accuracies for three-way classification on the  Restaurant and Laptop SemEval 2014 dataset.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9977638721466064}, {"text": "Restaurant and Laptop SemEval 2014 dataset", "start_pos": 58, "end_pos": 100, "type": "DATASET", "confidence": 0.705595001578331}]}]}