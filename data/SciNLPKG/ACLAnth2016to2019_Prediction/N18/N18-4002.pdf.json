{"title": [{"text": "Combining Abstractness and Language-specific Theoretical Indicators for Detecting Non-Literal Usage of Estonian Particle Verbs", "labels": [], "entities": [{"text": "Detecting Non-Literal Usage of Estonian Particle Verbs", "start_pos": 72, "end_pos": 126, "type": "TASK", "confidence": 0.7859600697244916}]}], "abstractContent": [{"text": "This paper presents two novel datasets and a random-forest classifier to automatically predict literal vs. non-literal language usage fora highly frequent type of multi-word expression in a low-resource language, i.e., Estonian.", "labels": [], "entities": []}, {"text": "We demonstrate the value of language-specific indicators induced from theoretical linguistic research, which outperform a high majority baseline when combined with language-independent features of non-literal language (such as abstractness).", "labels": [], "entities": []}], "introductionContent": [{"text": "Estonian particle verbs (PVs) are multi-word expressions combining an adverbial particle with abase verb (BV), cf..", "labels": [], "entities": [{"text": "Estonian particle verbs (PVs)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6212096909681956}]}, {"text": "They are challenging for automatic processing because their components do not always appear adjacent to each other, and the particles are homonymous with adpositions.", "labels": [], "entities": []}, {"text": "In addition, as illustrated in examples (1a) vs. (1b), the same PV type can be used in literal vs. non-literal language.", "labels": [], "entities": []}, {"text": "( Given that the automatic detection of nonliteral expressions (including metaphors and idioms) is critical for many NLP tasks, the last decade has seen an increase in research on distinguishing literal vs. non-literal meaning).", "labels": [], "entities": [{"text": "automatic detection of nonliteral expressions (including metaphors and idioms", "start_pos": 17, "end_pos": 94, "type": "TASK", "confidence": 0.7662276685237884}]}, {"text": "Most research up to date has, however, focused on resource-rich languages (mainly English and German), and elaborated on general indicators -such as contextual abstractness -to identify non-literal language.", "labels": [], "entities": []}, {"text": "As to our knowledge, only and  explored language-specific features.", "labels": [], "entities": []}, {"text": "The aim of this work is to automatically predict literal vs. non-literal language usage fora very frequent type of multi-word expression in a low-resource language, i.e., Estonian.", "labels": [], "entities": []}, {"text": "The predicate is the center of grammatical and usually semantic structure of the sentence, and it determines the meaning and the form of its arguments, cf..", "labels": [], "entities": []}, {"text": "Hence, the surrounding words (i.e., the context), their meanings and grammatical forms could help to decide whether the PV should be classified as compositional or noncompositional.", "labels": [], "entities": []}, {"text": "In addition to applying language-independent features of non-literal language, we demonstrate the value of indicators induced from theoretical linguistic research, that have so far not been explored in the context of compositionality.", "labels": [], "entities": []}, {"text": "For this purpose, this paper introduces two novel datasets and a random-forest classifier with standard and language-specific features.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "We give a brief overview of previous studies on Estonian PVs in Section 2, and Section 3 introduces the target dataset.", "labels": [], "entities": [{"text": "Estonian PVs", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8024173378944397}]}, {"text": "All features are described in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 lays out the experiments and evaluation of the model, and we conclude our work in Section 6.", "labels": [], "entities": []}, {"text": "decades but still lacks a comprehensive study.", "labels": [], "entities": []}, {"text": "studied six verbal particles and their aspectual meanings, and described how horizontal and vertical dimensions are represented.", "labels": [], "entities": []}, {"text": "investigated the prosody of Estonian PVs, finding PVs expressing perfectivity the most problematic to classify.", "labels": [], "entities": [{"text": "Estonian PVs", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.7492165267467499}]}, {"text": "Recent computational studies on Estonian PVs involve their automatic acquisition (, and predicting their degrees of compositionality.", "labels": [], "entities": [{"text": "Estonian PVs", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.8102348744869232}]}, {"text": "investigated the role of Estonian PVs in computational syntax, focusing on Constraint Grammar.", "labels": [], "entities": [{"text": "Constraint Grammar", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8197684288024902}]}, {"text": "Most research on automatically detecting non-literal language has been done on English and German (as mentioned above), and elaborated on general indicators to identify non-literal language.", "labels": [], "entities": [{"text": "automatically detecting non-literal language", "start_pos": 17, "end_pos": 61, "type": "TASK", "confidence": 0.7214261367917061}]}, {"text": "Our work is the first attempt to automatically distinguish literal and non-literal usage of Estonian PVs, and to specify on theory-and language-specific features.", "labels": [], "entities": [{"text": "Estonian PVs", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.6804516315460205}]}], "datasetContent": [{"text": "For creating a dataset of literal and non-literal language usage for Estonian PVs, we selected 210 PVs across 34 particles: we started with a list of 1,676 PVs that occurred at least once in a 170-million token newspaper subcorpus of the Estonian Reference Corpus 1 (ERC) and removed PVs with a frequency \u22649.", "labels": [], "entities": [{"text": "Estonian Reference Corpus 1 (ERC)", "start_pos": 238, "end_pos": 271, "type": "DATASET", "confidence": 0.8683562449046544}]}, {"text": "Then we sorted the PVs according to their frequency and selected PVs across different frequency ranges for the dataset.", "labels": [], "entities": []}, {"text": "In addition, we included the 20 most frequent PVs.", "labels": [], "entities": []}, {"text": "We plan to analyse the influence of frequency on the compositionality of PVs in future work, thus it was necessary to collect evaluations for PVs with different frequencies.", "labels": [], "entities": []}, {"text": "For each of the 210 target PVs, we then automatically extracted 16 sentences from the ERC.", "labels": [], "entities": [{"text": "ERC", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9338780045509338}]}, {"text": "The sentences were manually double-checked to make sure that verb and adverb formed a PV and did not appear as independent word units in a clause.", "labels": [], "entities": []}, {"text": "The choice of the numbers of PVs and sentences relied on the fact of limited time and other resources that allowed us to evaluate approximately 200 PVs and 2,000 sentences.", "labels": [], "entities": []}, {"text": "1 www.cl.ut.ee/korpused/segakorpus/ The resulting set of sentences was evaluated by three annotators with a linguistic background.", "labels": [], "entities": []}, {"text": "They were asked to assess each sentence by answering the question: \"What is the usage of the PV in the sentence on a 6-point scale ranging from clearly literal (0) to clearly non-literal (5) language usage?\"", "labels": [], "entities": []}, {"text": "In case of multiple PVs in the same sentence, the information of which PV to evaluate was provided for the annotators.", "labels": [], "entities": []}, {"text": "Although we use binary division of PVs in this study, it was reasonable to collect evaluations on a larger than binary scale because of the following reasons: first, it is a well-known fact that multi-word expressions do not fall into the binary classes of compositional vs. non-compositional expressions (, and second, it was important to create a dataset that would be applicable to multiple tasks.", "labels": [], "entities": []}, {"text": "Thus our dataset can be used to investigate the degrees of compositionality of PVs in the future.", "labels": [], "entities": []}, {"text": "The agreement among 3 annotators on all 6 categories is fair (Fleiss' \u03ba = 0.36).", "labels": [], "entities": []}, {"text": "A binary distinction based on the average sentence scores into literal (average \u2264 2.4) and non-literal (average \u2265 2.5) resulted in substantial agreement (\u03ba = 0.73).", "labels": [], "entities": []}, {"text": "Our experiments below use the binary-class setting, disregarding all cases of disagreement.", "labels": [], "entities": []}, {"text": "This final dataset 2 includes 1,490 sentences: 1,102 non-literal and 388 literal usages across 184 PVs with 120 different base verbs and 32 particle types.", "labels": [], "entities": []}, {"text": "63 PVs occur only in non-literal sentences, 15 only in literal sentences and 106 PVs in non-literal and literal sentences.", "labels": [], "entities": []}, {"text": "From 120 verbs 50 appear only in non-literal sentences, 15 only in literal sentences, and 55 verbs in both literal and non-literal sentences.", "labels": [], "entities": []}, {"text": "The distribution of (non-) literal sentences across particle types is shown in.", "labels": [], "entities": []}, {"text": "While many particles appear mostly in non-literal language (and esile, alt, \u00a8 uhte, \u00a8 ara are exclusively used in their non-literal meanings in our dataset), they all have literal correspondences.", "labels": [], "entities": []}, {"text": "No particle types appear only in literal sentences.", "labels": [], "entities": []}, {"text": "The classification experiments to distinguish between literal and non-literal language usage of Estonian PVs rely on the sentence features defined above.", "labels": [], "entities": []}, {"text": "They were carried out using a random forest classifier) that constructs a number of randomized decision trees during the training phase and makes prediction by averaging the results.", "labels": [], "entities": []}, {"text": "For our experiments, we used 100 random decision trees.", "labels": [], "entities": []}, {"text": "The random forest classifier performs better in comparison of other classification methods that we have applied in the Weka toolkit).", "labels": [], "entities": [{"text": "Weka toolkit", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.9688536524772644}]}, {"text": "For the evaluation we perform 10-fold cross validation, hence we use the previously described data for training and testing.", "labels": [], "entities": []}, {"text": "The classification results across features and combinations of features are presented in.", "labels": [], "entities": []}, {"text": "We report accuracy as well as F 1 for literal and non-literal sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997299313545227}, {"text": "F 1", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.99463951587677}]}, {"text": "shows that the best single feature types are the unigrams (acc: 82.3%) and the base verbs (81.2%).", "labels": [], "entities": [{"text": "acc", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9457831978797913}]}, {"text": "Combining the two, the accuracy reaches 84.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9997335076332092}]}, {"text": "No other single feature type goes beyond the high majority baseline (74.0%), but the combinations in the significantly outperform the baseline, according to \u03c7 2 with p<0.01.", "labels": [], "entities": []}, {"text": "Adding the particle type to the base verb information (1-2) correctly classifies 85.2% of the sentences.", "labels": [], "entities": []}, {"text": "Further adding unigrams (1-3), however, does not help.", "labels": [], "entities": []}, {"text": "Regarding abstractness, adding the ratings for all but objects to the particle-verb information (1-2, 4-6) is best and reaches an accuracy of 86.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9994722008705139}]}, {"text": "Subject case information, animacy and case government in combination with 1-2 reach similar values (85.3-86.3%).", "labels": [], "entities": []}, {"text": "The overall best result (87.9%) is reached when combining particle and base verb information with all-noun and subject abstractness ratings, subject case, subject animacy, and case government.", "labels": [], "entities": []}, {"text": "While only lists a selection of all possible combinations of features to present the most interesting cases, it illustrates that the combination of language-independent features and languagespecific features is able to outperform the high majority baseline.", "labels": [], "entities": []}, {"text": "Although the difference between the best combination without language-specific features (86.3%) and the best combination with language-specific features (87.9%) is not statistically significant, the best-performing combination provides F 1 =92.0 for non-literal sentences and F 1 =75.0 for literal sentences.", "labels": [], "entities": [{"text": "F 1", "start_pos": 236, "end_pos": 239, "type": "METRIC", "confidence": 0.9806768596172333}, {"text": "F 1", "start_pos": 276, "end_pos": 279, "type": "METRIC", "confidence": 0.9803792834281921}]}], "tableCaptions": [{"text": " Table 1: Overview of classification results.", "labels": [], "entities": []}]}