{"title": [{"text": "Sluice resolution without hand-crafted features over brittle syntax trees", "labels": [], "entities": [{"text": "Sluice resolution", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8915548920631409}]}], "abstractContent": [{"text": "Sluice resolution in English is the problem of finding antecedents of wh-fronted ellipses.", "labels": [], "entities": [{"text": "Sluice resolution", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8847851753234863}]}, {"text": "Previous work has relied on hand-crafted features over syntax trees that scale poorly to other languages and domains; in particular, to dialogue, which is one of the most interesting applications of sluice resolution.", "labels": [], "entities": [{"text": "sluice resolution", "start_pos": 199, "end_pos": 216, "type": "TASK", "confidence": 0.8776500523090363}]}, {"text": "Syntactic information is arguably important for sluice resolution, but we show that multi-task learning with partial parsing as auxiliary tasks effectively closes the gap and buys us an additional 9% error reduction over previous work.", "labels": [], "entities": [{"text": "sluice resolution", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.9634422063827515}]}, {"text": "Since we are not directly relying on features from partial parsers, our system is more robust to domain shifts, giving a 26% error reduction on embedded sluices in dialogue.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sluices, also known as wh-fronted ellipses, are questions where the specification of what is asked for (beyond the wh-word), is elided (and thus needs to be retrieved from context).", "labels": [], "entities": []}, {"text": "Below we distinguish two types of sluices: (i) embedded sluices, and (ii) root sluices.", "labels": [], "entities": []}, {"text": "Embedded sluices occur in both single-authored texts and dialogue, while root sluices are particularly frequent in dialogue.", "labels": [], "entities": []}, {"text": "(1) If [this is not practical], explain why.", "labels": [], "entities": []}, {"text": "Example 1 is an embedded sluice.", "labels": [], "entities": []}, {"text": "In it, why is the remnant of the embedded question, which we understand to mean 'why this is not practical'.", "labels": [], "entities": []}, {"text": "Example 2 is a root sluice.", "labels": [], "entities": []}, {"text": "Again, why is the remnant of the question; however, the wh-word is not embedded in a larger structure.", "labels": [], "entities": []}, {"text": "In both cases, we consider the antecedent of a wh-fronted ellipsis to be the content in the prior discourse that most intuitively provides the elided material, i.e., [this is not practical] in Example 1, and [Jennifer is looking for you/me] in Example 2.", "labels": [], "entities": []}, {"text": "Contributions This paper presents a more robust, neural model for sluice resolution in English based on multi-task learning.", "labels": [], "entities": [{"text": "sluice resolution", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.9246372580528259}]}, {"text": "Our model significantly outperforms the only previous work on sluice resolution on available newswire corpora, but also has a number of advantages over this work.", "labels": [], "entities": [{"text": "sluice resolution", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7904327511787415}]}, {"text": "In particular, our model (a) does not require full syntactic parsing as a pre-processing step, (b) does not require manual feature engineering, and (c) is more robust when evaluated on speech corpora, because it is not dependent on full syntactic parsers (a).", "labels": [], "entities": []}, {"text": "The lack of dependence on full syntactic parsers should also make it easier to transfer our model to new languages.", "labels": [], "entities": []}, {"text": "In addition to the implementation of our architecture, which we make publicly available, we also make anew benchmark available for sluice resolution in English dialogue.", "labels": [], "entities": [{"text": "sluice resolution", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.862010657787323}]}], "datasetContent": [{"text": "Corpora We evaluate our models on two datasets, the newswire corpus introduced in Anand in treating the first annotator in each example as the goldstandard.", "labels": [], "entities": [{"text": "newswire corpus", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9160796701908112}]}, {"text": "To measure the sensitivity of our systems to domain shifts, we annotate a total of 2000 examples from the OpenSubtitles corpus.", "labels": [], "entities": [{"text": "OpenSubtitles corpus", "start_pos": 106, "end_pos": 126, "type": "DATASET", "confidence": 0.8877566456794739}]}, {"text": "1000 examples are root sluices, and 1000 are embedded sluices.", "labels": [], "entities": []}, {"text": "Each example is annotated by two annotators.", "labels": [], "entities": []}, {"text": "Interannotator scores were 0.77 for embedded sluices, and 0.83 for root sluices.", "labels": [], "entities": []}, {"text": "Auxiliary Tasks We use four auxiliary tasks in our experiments below: POS tagging is the task of determining the syntactic category (part of speech) of a word in context.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.7457973659038544}, {"text": "determining the syntactic category (part of speech) of a word in context", "start_pos": 97, "end_pos": 169, "type": "TASK", "confidence": 0.6652940894876208}]}, {"text": "Our data is from the Wall Street Journal section of the English Penn Treebank, using the splits in the CONLL 2007 shared task (.", "labels": [], "entities": [{"text": "Wall Street Journal section of the English Penn Treebank", "start_pos": 21, "end_pos": 77, "type": "DATASET", "confidence": 0.9489171372519599}, {"text": "CONLL 2007 shared task", "start_pos": 103, "end_pos": 125, "type": "DATASET", "confidence": 0.9120720624923706}]}, {"text": "Chunk-ing is a partial parsing task in which we need to identify the boundary of the main phrases in a sentence.", "labels": [], "entities": [{"text": "partial parsing task", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7295815944671631}]}, {"text": "Our data is from the 2000 CoNLL shared task).", "labels": [], "entities": [{"text": "2000 CoNLL shared task", "start_pos": 21, "end_pos": 43, "type": "DATASET", "confidence": 0.7382581755518913}]}, {"text": "Com Sentence compression is the task of sentence parts that can be dropped without loosing coherence nor salient information.", "labels": [], "entities": [{"text": "Com Sentence compression", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7482817967732748}]}, {"text": "We use the dataset also used in).", "labels": [], "entities": []}, {"text": "CCG super-tagging is another form of partial parsing, using a more fine-grained tagset.", "labels": [], "entities": []}, {"text": "We use the CCGBank with standard splits.", "labels": [], "entities": [{"text": "CCGBank", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.979188859462738}]}, {"text": "The S\u00f8gaard and Goldberg (2016) model uses sentence compression at the lowest layer, then chunking, and finally antecedent tagging at the highest.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7222498059272766}]}, {"text": "We observed a detrimental effect when including compression in the same stack as the other auxiliaries for the model presented here.", "labels": [], "entities": []}, {"text": "This effect vanished when compression is placed in a separate stack.", "labels": [], "entities": []}, {"text": "Evaluation metrics We evaluate predicted antecedents using (token-level) F1 scores.", "labels": [], "entities": [{"text": "F1", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9767045974731445}]}, {"text": "This metric is motivated by the observation that annotated spans vary in length, and that annotators often disagree about the exact bracketing; it differs from the one used in, however, and we stress that our results are therefore not directly comparable to those reported in their paper.", "labels": [], "entities": []}, {"text": "Moreover, Anand and Hardt (2016) used cross-validation; we compare systems and baselines on a fixed split.", "labels": [], "entities": []}, {"text": "Baselines In addition to comparing to, the only previous work on sluice resolution, we compare our performance to two baseline neural network architectures: a singletask architecture and a multi-task architecture similar to . Our first baseline is a single-task, two-layered long-short-term memory (LSTM) network, with: F1 scores on embedded sluices from ESC (Newswire) and embedded and root sluices from OpenSubtitles (Dialogue).", "labels": [], "entities": [{"text": "sluice resolution", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.9231677055358887}, {"text": "F1", "start_pos": 320, "end_pos": 322, "type": "METRIC", "confidence": 0.9992864727973938}]}, {"text": "a projection layer and a softmax layer.", "labels": [], "entities": []}, {"text": "Our second baseline is a cascading, three-layered LSTM, as described by.", "labels": [], "entities": []}, {"text": "See \u00a73 for hyper-parameters.", "labels": [], "entities": []}, {"text": "Replicability We make our corpus splits, our annotations, our final models, and our source code available at https://github.com/OlaRonning/ sluice_antecedent_selection.", "labels": [], "entities": [{"text": "OlaRonning", "start_pos": 128, "end_pos": 138, "type": "DATASET", "confidence": 0.8416894674301147}]}], "tableCaptions": [{"text": " Table 1: F1 scores on embedded sluices from ESC (Newswire) and embedded and root sluices from  OpenSubtitles (Dialogue).", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.999478280544281}, {"text": "ESC (Newswire)", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.889940619468689}]}]}