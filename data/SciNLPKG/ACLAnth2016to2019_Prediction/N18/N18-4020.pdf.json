{"title": [{"text": "Generating Image Captions in Arabic using Root-Word Based Recurrent Neural Networks and Deep Neural Networks", "labels": [], "entities": [{"text": "Generating Image Captions", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7166534463564554}]}], "abstractContent": [{"text": "Image caption generation has gathered widespread interest in the artificial intelligence community.", "labels": [], "entities": [{"text": "Image caption generation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8947203755378723}]}, {"text": "Automatic generation of an image description requires both computer vision and natural language processing techniques.", "labels": [], "entities": [{"text": "Automatic generation of an image description", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8074258267879486}]}, {"text": "While, there has been advanced research in English caption generation, research on generating Arabic descriptions of an image is extremely limited.", "labels": [], "entities": [{"text": "English caption generation", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.7631364663441976}]}, {"text": "Semitic languages like Arabic are heavily influenced by root-words.", "labels": [], "entities": []}, {"text": "We leverage this critical dependency of Arabic to generate captions of an image directly in Arabic using root-word based Recurrent Neural Network and Deep Neural Networks.", "labels": [], "entities": []}, {"text": "Experimental results on datasets from various Middle Eastern newspaper websites allow us to report the first BLEU score for direct Arabic caption generation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9703534841537476}, {"text": "direct Arabic caption generation", "start_pos": 124, "end_pos": 156, "type": "TASK", "confidence": 0.6665449365973473}]}, {"text": "We also compare the results of our approach with BLEU score captions generated in English and translated into Arabic.", "labels": [], "entities": [{"text": "BLEU score captions", "start_pos": 49, "end_pos": 68, "type": "METRIC", "confidence": 0.9414501190185547}]}, {"text": "Experimental results confirm that generating image captions using root-words directly in Arabic significantly outperforms the English-Arabic translated captions using state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the increase in the number of devices with cameras, there is a widespread interest in generating automatic captions from images and videos.", "labels": [], "entities": [{"text": "generating automatic captions from images and videos", "start_pos": 91, "end_pos": 143, "type": "TASK", "confidence": 0.7723713346890041}]}, {"text": "Automatic generation of image descriptions is a widely researched problem.", "labels": [], "entities": [{"text": "Automatic generation of image descriptions", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8466976642608642}]}, {"text": "However, this problem is significantly more challenging that the image classification or image recognition tasks which gained popularity with ImageNet recognition challenge (.", "labels": [], "entities": [{"text": "image classification", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.79142165184021}, {"text": "image recognition", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7285356819629669}, {"text": "ImageNet recognition", "start_pos": 142, "end_pos": 162, "type": "TASK", "confidence": 0.7869992554187775}]}, {"text": "Automatic generation of image captions have a huge impact in the fields of information retrieval, accessibility for the vision impaired, categorization of images etc.", "labels": [], "entities": [{"text": "image captions", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7336368560791016}, {"text": "information retrieval", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.7847068011760712}]}, {"text": "Additionally, the automatic generation of the descriptions of images can be used as Recent works which utilize large image datasets and deep neural networks have obtained strong results in the field of image recognition (.", "labels": [], "entities": [{"text": "image recognition", "start_pos": 202, "end_pos": 219, "type": "TASK", "confidence": 0.7635778486728668}]}, {"text": "To generate more natural descriptive sentences in English,) introduced a model that generates natural language descriptions of image regions based on weak labels inform of a dataset of images and sentences.", "labels": [], "entities": []}, {"text": "However, most visual recognition models and approaches in the image caption generation community are focused on Western languages, ignoring Semitic and Middle-Eastern languages like Arabic, Hebrew, Urdu and Persian.", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7422922253608704}, {"text": "image caption generation", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.8185238242149353}]}, {"text": "As discussed further in related works, almost all major caption generation models have validated their approaches using English.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.921800822019577}]}, {"text": "This is primarily due to two major reasons: i) lack of existing image corpora in languages other than English ii) the significant di-", "labels": [], "entities": []}], "datasetContent": [{"text": "Figure 3, 5-8 gives a sample of our approach inaction.", "labels": [], "entities": []}, {"text": "For the convenience of our readers who are not familiar with Arabic,, 6, 7, 8 have the English caption generated using () denoted as \"Arabic-English\" and \"Ours\" denote a professional English translation of the Arabic caption generated from our approach.", "labels": [], "entities": []}, {"text": "We evaluate our technique using two datasets: Flickr8k dataset with manually written captions in Arabic by professional Arabic translators and 405,000 im- We also compare the results of our approach with generating English captions using previously proposed approaches and translating them to Arabic using Google translate.", "labels": [], "entities": [{"text": "Flickr8k dataset", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.7988190352916718}]}, {"text": "To evaluate the performance, automatic metrics are computed using human generated ground-truth captions.", "labels": [], "entities": []}, {"text": "All our images in the dataset were translated using professional Arabic translations as ground-truth.", "labels": [], "entities": []}, {"text": "The most commonly used metric to compare generated image captions is BLEU score ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9890211820602417}]}, {"text": "BLEU is the precision of word n-grams between generated and reference sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9894558787345886}, {"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9965984225273132}]}, {"text": "Additionally, scores like METEOR () which capture perplexity of models fora given transcription have gained widespread attention.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9744607210159302}]}, {"text": "Perplexity is the geometric mean of the inverse probability for each predicted word.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9769335389137268}]}, {"text": "We report both the BLEU and METEOR score for Arabic captions using root-words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9995131492614746}, {"text": "METEOR score", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.9783632457256317}, {"text": "Arabic captions", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.5428346246480942}]}, {"text": "Additionally, this opens anew field of research to use root-words to generate captions from images in Semitic languages and may also be applied to English for words originating from Latin.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, our scores are the first reported score for Arabic captions.", "labels": [], "entities": [{"text": "Arabic captions", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.6202449202537537}]}, {"text": "Furthermore, the results also show that generating captions directly in Arabic attains a much better BLEU scores compared to generating captions in English and translating them to Arabic.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9994885921478271}]}, {"text": "All results shown in are captions generated using the corresponding approaches in English and translating them to Arabic using Google Translate.", "labels": [], "entities": []}, {"text": "According to, we can see that our root-word based approach outperforms all current English based approaches and translated to Arabic using Google Translate.", "labels": [], "entities": []}, {"text": "An interesting observation is in.", "labels": [], "entities": []}, {"text": "While all current approaches fail to describe the actual \"Palm Jumeriah Island\" which is a man-made island in shape of Palm tree in Dubai, our approach learns the context of \"sea\", \"island\" and \"palm\" and produces the correct result.", "labels": [], "entities": [{"text": "Palm Jumeriah Island\"", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.8431326001882553}, {"text": "Palm tree in Dubai", "start_pos": 119, "end_pos": 137, "type": "DATASET", "confidence": 0.9235083311796188}]}, {"text": "Most inefficient cases in our algorithm are due to random outliers like some recent words which are not influenced by root-words.", "labels": [], "entities": []}, {"text": "This can be further improved by using a larger dataset and using new dialectal captions in the training phase.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU-1,2,3,4/METEOR metrics compared to other methods, (-) indicates an unknown metric", "labels": [], "entities": [{"text": "BLEU-1,2,3,4", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.999146580696106}, {"text": "METEOR", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9586968421936035}]}]}