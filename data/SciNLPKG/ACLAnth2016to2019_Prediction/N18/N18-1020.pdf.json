{"title": [{"text": "Zero-Shot Question Generation from Knowledge Graphs for Unseen Predicates and Entity Types", "labels": [], "entities": [{"text": "Zero-Shot Question Generation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5145466725031534}]}], "abstractContent": [{"text": "We present a neural model for question generation from knowledge base triples in a \"Zero-Shot\" setup, that is generating questions for triples containing predicates, subject types or object types that were not seen at training time.", "labels": [], "entities": [{"text": "question generation from knowledge base triples", "start_pos": 30, "end_pos": 77, "type": "TASK", "confidence": 0.7953440646330515}]}, {"text": "Our model leverages triples occurrences in the natural language corpus in an encoder-decoder architecture, paired with an original part-of-speech copy action mechanism to generate questions.", "labels": [], "entities": []}, {"text": "Benchmark and human evaluation show that our model sets anew state-of-the-art for zero-shot QG.", "labels": [], "entities": []}], "introductionContent": [{"text": "Questions Generation (QG) from Knowledge Graphs is the task consisting in generating natural language questions given an input knowledge base (KB) triple.", "labels": [], "entities": [{"text": "Questions Generation (QG)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8264300286769867}]}, {"text": "QG from knowledge graphs has shown to improve the performance of existing factoid question answering (QA) systems either by dual training or by augmenting existing training datasets (.", "labels": [], "entities": [{"text": "factoid question answering (QA)", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.7621791362762451}]}, {"text": "Those methods rely on large-scale annotated datasets such as SimpleQuestions (.", "labels": [], "entities": []}, {"text": "Building such datasets is a tedious task in practice, especially to obtain an unbiased dataset -i.e. a dataset that covers equally a large amount of triples in the KB.", "labels": [], "entities": []}, {"text": "In practice many of the predicates and entity types in KB are not covered by those annotated datasets.", "labels": [], "entities": []}, {"text": "For example 75.6% of Freebase predicates are not covered by the SimpleQuestions dataset . Among those we can find important missing predicates such as: fb:food/beer/country, fb:location/country/national anthem, fb:astronomy/star system/stars.", "labels": [], "entities": [{"text": "SimpleQuestions dataset", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.9485344290733337}]}, {"text": "One challenge for QG from knowledge graphs is to adapt to predicates and entity types that replicate the observation http://bit.ly/2GvVHae were not seen at training time (Zero-Shot Question Generation).", "labels": [], "entities": [{"text": "Zero-Shot Question Generation", "start_pos": 171, "end_pos": 200, "type": "TASK", "confidence": 0.5237759649753571}]}, {"text": "Since state-of-the-art systems in factoid QA rely on the tremendous efforts made to create SimpleQuestions, these systems can only process questions on the subset of 24.4% of freebase predicates defined in SimpleQuestions.", "labels": [], "entities": []}, {"text": "Previous works for factoid QG ( claims to solve the issue of small size QA datasets.", "labels": [], "entities": []}, {"text": "However encountering an unseen predicate / entity type will generate questions made out of random text generation for those out-of-vocabulary predicates a QG system had never seen.", "labels": [], "entities": []}, {"text": "We go beyond this state-of-the-art by providing an original and non-trivial solution for creating a much broader set of questions for unseen predicates and entity types.", "labels": [], "entities": []}, {"text": "Ultimately, generating questions to predicates and entity types unseen at training time will allow QA systems to cover predicates and entity types that would not have been used for QA otherwise.", "labels": [], "entities": []}, {"text": "Intuitively, a human who is given the task to write a question on a fact offered by a KB, would read natural language sentences where the entity or the predicate of the fact occur, and buildup questions that are aligned with what he reads from both a lexical and grammatical standpoint.", "labels": [], "entities": []}, {"text": "In this paper, we propose a model for Zero-Shot Question Generation that follows this intuitive process.", "labels": [], "entities": [{"text": "Zero-Shot Question Generation", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.5811293224493662}]}, {"text": "In addition to the input KB triple, we feed our model with a set of textual contexts paired with the input KB triple through distant supervision.", "labels": [], "entities": []}, {"text": "Our model derives an encoder-decoder architecture, in which the encoder encodes the input KB triple, along with a set of textual contexts into hidden representations.", "labels": [], "entities": []}, {"text": "Those hidden representations are fed to a decoder equipped with an attention mechanism to generate an output question.", "labels": [], "entities": []}, {"text": "In the Zero-Shot setup, the emergence of new predicates and new class types during test time requires new lexicalizations to express these pred-icates and classes in the output question.", "labels": [], "entities": []}, {"text": "These lexicalizations might not be encountered by the model during training time and hence do not exist in the model vocabulary, or have been seen only few times not enough to learn a good representation for them by the model.", "labels": [], "entities": []}, {"text": "Recent works on Text Generation tackle the rare words/unknown words problem using copy actions (: words with a specific position are copied from the source text to the output text -although this process is blind to the role and nature of the word in the source text.", "labels": [], "entities": [{"text": "Text Generation", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.8158232271671295}]}, {"text": "Inspired by research in open information extraction) and structure-content neural language models (, in which part-of-speech tags represent a distinctive feature when representing relations in text, we extend these positional copy actions.", "labels": [], "entities": [{"text": "open information extraction", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6390542685985565}]}, {"text": "Instead of copying a word in a specific position in the source text, our model copies a word with a specific part-of-speech tag from the input text -we refer to those as partof-speech copy actions.", "labels": [], "entities": []}, {"text": "Experiments show that our model using contexts through distant supervision significantly outperforms the strongest baseline among six (+2.04 BLEU-4 score).", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.999284565448761}]}, {"text": "Adding our copy action mechanism further increases this improvement (+2.39).", "labels": [], "entities": []}, {"text": "Additionally, a human evaluation complements the comprehension of our model for edge cases; it supports the claim that the improvement brought by our copy action mechanism is even more significant than what the BLEU score suggests.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 211, "end_pos": 221, "type": "METRIC", "confidence": 0.9645838439464569}]}], "datasetContent": [{"text": "As a source of question paired with KB triples we use the SimpleQuestions dataset (.", "labels": [], "entities": [{"text": "SimpleQuestions dataset", "start_pos": 58, "end_pos": 81, "type": "DATASET", "confidence": 0.9269313812255859}]}, {"text": "It consists of 100K questions with their corresponding triples from Freebase, and was created manually through crowdsourcing.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9672920107841492}]}, {"text": "When asked to form a question from an input triple, human annotators usually tend to mainly focus on expressing the predicate of the input triple.", "labels": [], "entities": []}, {"text": "For example, given a triple with the predicate fb:spacecraft/manufacturer the user may ask \"What is the manufacturer of ?\".", "labels": [], "entities": []}, {"text": "Annotators may specify the entity type of the subject or the object of the triple: \"What is the manufacturer of the spacecraft [S]?\" or \"Which company manufactures [S]?\".", "labels": [], "entities": []}, {"text": "Motivated by this example we chose to associate each input triple with three textual contexts of three different types.", "labels": [], "entities": []}, {"text": "The first is a phrase containing lexicalization of the predicate of the triple.", "labels": [], "entities": []}, {"text": "The second and the third are two phrases containing the entity type of the subject and the object of the triple.", "labels": [], "entities": []}, {"text": "In what follows we show the process of collection and preprocessing of those textual contexts.", "labels": [], "entities": []}, {"text": "To evaluate the quality of the generated question, we compare the original labeled questions by human annotators to the ones generated by each variation of our model and the baselines.", "labels": [], "entities": []}, {"text": "We rely on a set of well established evaluation metrics for text generation: BLEU-1, BLEU-2, BLEU-3, BLEU-4 (), METEOR) and ROUGE L).", "labels": [], "entities": [{"text": "text generation", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.741719514131546}, {"text": "BLEU-1", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9972959160804749}, {"text": "BLEU-2", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9944879412651062}, {"text": "BLEU-3", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.994878888130188}, {"text": "BLEU-4", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9947275519371033}, {"text": "METEOR", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9769232869148254}, {"text": "ROUGE L", "start_pos": 124, "end_pos": 131, "type": "METRIC", "confidence": 0.9801859557628632}]}, {"text": "Automatic Metrics for evaluating text generation such as BLEU and METEOR give an measure of how close the generated questions are to the target correct labels.", "labels": [], "entities": [{"text": "text generation", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7462068200111389}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9967795014381409}, {"text": "METEOR", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.925845742225647}]}, {"text": "However, they still suffer from many limitations ().", "labels": [], "entities": []}, {"text": "Automatic metrics might not be able to evaluate directly whether a specific predicate was explicitly mentioned in the generated text or not.", "labels": [], "entities": []}, {"text": "As an example, taking a target question and two corresponding generated questions A and B: We can find that the sentence A having a better BLEU score than B although it is notable to express the correct target predicate (film genre).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 139, "end_pos": 149, "type": "METRIC", "confidence": 0.9838795363903046}]}, {"text": "For that reason we decide to run two further human evaluations to directly measure the following: Predicate identification: annotators were asked to indicate whether the generated question contains the given predicate in the factor not, either directly or implicitly.", "labels": [], "entities": [{"text": "Predicate identification", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.9011635482311249}]}, {"text": "Naturalness: following (Ngomo et al., 2013), we measure the comprehensibility and readability of the generated questions.", "labels": [], "entities": []}, {"text": "Each annotator was asked to rate each generated question using a scale from 1 to 5, where: (5) perfectly clear and natural, (3) artificial but understandable, and (1) completely not understandable.", "labels": [], "entities": []}, {"text": "We run our studies on 100 randomly sampled input facts alongside with their corresponding generated questions by each of the systems using the help of 4 annotators.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Dataset statistics across 10 folds for each ex- periment", "labels": [], "entities": []}, {"text": " Table 4: Evaluation results of our model and all other baselines for the unseen predicate evaluation setup", "labels": [], "entities": []}, {"text": " Table 5: Automatic evaluation of our model against se- lected baselines for unseen sub-types and obj-types", "labels": [], "entities": []}, {"text": " Table 6: results of Human evaluation on % of predi- cates identified and naturalness 0-5", "labels": [], "entities": []}]}