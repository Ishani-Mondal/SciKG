{"title": [{"text": "A Structured Syntax-Semantics Interface for English-AMR Alignment", "labels": [], "entities": [{"text": "English-AMR Alignment", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.6582561284303665}]}], "abstractContent": [{"text": "Meaning Representation (AMR) annotations are often assumed to closely mirror dependency syntax, but AMR explicitly does not require this, and the assumption has never been tested.", "labels": [], "entities": [{"text": "Meaning Representation (AMR) annotations", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8304556608200073}]}, {"text": "To test it, we devise an expressive framework to align AMR graphs to dependency graphs, which we use to annotate 200 AMRs.", "labels": [], "entities": []}, {"text": "Our annotation explains how 97% of AMR edges are evoked by words or syntax.", "labels": [], "entities": []}, {"text": "Previously existing AMR alignment frameworks did not allow for mapping AMR onto syntax, and as a consequence they explained at most 23%.", "labels": [], "entities": [{"text": "AMR alignment", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.9189443290233612}]}, {"text": "While we find that there are indeed many cases where AMR annotations closely mirror syntax, there are also pervasive differences.", "labels": [], "entities": [{"text": "AMR annotations", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.8636663258075714}]}, {"text": "We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMR-to-string alignment; and to pinpoint errors in an AMR parser.", "labels": [], "entities": [{"text": "AMR-to-string alignment", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.8456941545009613}]}, {"text": "We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax.", "labels": [], "entities": [{"text": "AMR parsing and generation", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.8538483083248138}]}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR;) is a popular framework for annotating whole sentence meaning.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7985204577445983}]}, {"text": "An AMR annotation is a directed, usually acyclic graph in which nodes represent entities and events, and edges represent relations between them, as on the right in.", "labels": [], "entities": []}, {"text": "AMR annotations include no explicit mapping between elements of an AMR and the corresponding elements of the sentence that evoke them, and this presents a challenge to developers of machine learning systems that parse sentences to AMR or generate sentences from AMR, since they must first infer this mapping in the training data (e.g., inter alia).", "labels": [], "entities": []}, {"text": "This AMR alignment problem was first formalized by, who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping-we call this JAMR alignment.", "labels": [], "entities": [{"text": "AMR alignment", "start_pos": 5, "end_pos": 18, "type": "TASK", "confidence": 0.9637541770935059}, {"text": "JAMR alignment", "start_pos": 184, "end_pos": 198, "type": "TASK", "confidence": 0.7018852829933167}]}, {"text": "then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption-we call this ISI alignment.", "labels": [], "entities": []}, {"text": "In ISI alignments, edges often align to syntactic function words: for example, :location aligns to in in figure 1.", "labels": [], "entities": [{"text": "ISI alignments", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.9353195428848267}]}, {"text": "So edge alignments allow ISI to explain more of the AMR structure than JAMR, but in a limited way: only 23% of AMR edges are aligned in the ISI corpus.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.835260272026062}, {"text": "ISI corpus", "start_pos": 140, "end_pos": 150, "type": "DATASET", "confidence": 0.7172643691301346}]}, {"text": "This maybe be-cause edges are often evoked by syntactic structure rather than words: for instance, the :ARG1 edge in is evoked by the fact that cat is the subject of lies and not by any particular word.", "labels": [], "entities": []}, {"text": "Although it seems sensible to assume that all of the nodes and edges of an AMR are evoked by the words and syntax of a sentence, the existing alignment schemes do not allow for expressing that relationship.", "labels": [], "entities": []}, {"text": "We therefore propose a framework expressive enough to align AMR to syntax ( \u00a72) and use it to align a corpus of 200 AMRs to dependency parses.", "labels": [], "entities": []}, {"text": "We analyse our corpus and show that the addition of syntactic alignments allows us account for 97% of the AMR content.", "labels": [], "entities": []}, {"text": "Syntactic-semantic mappings are often assumed by AMR parsing models (e.g., which is understandable since these mappings are wellstudied in linguistic theory.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.8374351263046265}]}, {"text": "But AMR explicitly avoids theoretical commitment to a syntaxsemantics mapping: state that \"AMR is agnostic about how we might want to derive meanings from strings.\"", "labels": [], "entities": []}, {"text": "If we are going to build such an assumption into our models, we should test it empirically, which we can do by analysing our corpus.", "labels": [], "entities": []}, {"text": "We observe some pervasive structural differences between AMR and dependency syntax ( \u00a73), despite the fact that a majority of AMR edges map easily onto dependency edges.", "labels": [], "entities": []}, {"text": "Since syntactic alignment can largely explain AMRs, we also develop a baseline rule-based aligner for it, and show that this new task is much more difficult than lexical alignment ( \u00a74).", "labels": [], "entities": [{"text": "syntactic alignment", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.7484517693519592}, {"text": "AMRs", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.895411491394043}, {"text": "lexical alignment", "start_pos": 162, "end_pos": 179, "type": "TASK", "confidence": 0.7311705648899078}]}, {"text": "We also show how our data can be used to analyze errors made by an AMR parser ( \u00a75).", "labels": [], "entities": []}, {"text": "We make our annotated data and aligner freely available for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use our annotations to measure the accuracy of AMR aligners on specific phenomena that were inexpressible in previous annotation schemes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9992807507514954}]}, {"text": "Our experiments evaluate the JAMR heuristic aligner (), the ISI statistical aligner (, and a heuristic rulebased aligner that we developed specifically for An AMR concept evoked by a preposition usually dominates the structure (after structural alignment.", "labels": [], "entities": []}, {"text": "We evaluate JAMR, ISI, and our aligner on two distinct tasks.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.516708254814148}]}, {"text": "Lexical alignment involves aligning AMR nodes to words, a task all three systems can perform.", "labels": [], "entities": [{"text": "Lexical alignment", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8985223472118378}]}, {"text": "We evaluate against three datasets: our own, the JAMR dataset (, and the ISI dataset ().", "labels": [], "entities": [{"text": "JAMR dataset", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.9727546870708466}, {"text": "ISI dataset", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.9320910274982452}]}, {"text": "17 Results (table 3) suggest that this task is already well-addressed, but also that there exist marked differences between how lexical alignment is defined in each dataset and that aligners are: Structural alignment ( \u00a74.1) scores, with different sources of input lexical alignments.", "labels": [], "entities": [{"text": "Structural alignment", "start_pos": 196, "end_pos": 216, "type": "TASK", "confidence": 0.8469478189945221}]}, {"text": "Scores are shown for gold standard and automatic UD trees.", "labels": [], "entities": []}, {"text": "For our aligner, errors are due to faulty morphological analysis, duplicated words, and both accidental string similarity between AMR concepts and words and occasional lack of similarity between concepts and words that should be aligned.", "labels": [], "entities": []}, {"text": "An important goal of our experiments is to establish baselines for the structural alignment task.", "labels": [], "entities": [{"text": "structural alignment task", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.8843211730321249}]}, {"text": "While we cannot evaluate the JAMR and ISI aligners directly on this task, we can use the lexical alignments they output in place of the first pass of our aligner.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8266662955284119}]}, {"text": "The only dataset for this task is our own.", "labels": [], "entities": []}, {"text": "The results (table 4) evaluate accuracy of structural alignments only and do not count lexical alignments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9993289709091187}]}, {"text": "The automatic alignments have lower coverage of AMRs than the gold alignments do: our best aligner leaves 13.3% of AMR nodes and 30.0% of AMR edges unaligned, compared to 0.07% and 2.8% in the gold standard.", "labels": [], "entities": []}, {"text": "The aligner also leaves 39.2% of DG nodes and 47.7% of DG edges unaligned, compared to 28.6% and 34.8% in the gold standard.", "labels": [], "entities": []}, {"text": "The relatively low F-score for the gold standard lexical alignments and DGs condition suggests that substantial improvements to our structural alignment algorithm are possible.", "labels": [], "entities": [{"text": "F-score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.998921275138855}, {"text": "DGs", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.8353537321090698}]}, {"text": "The two most common reasons for low recall were missing one of the conjuncts in a coordinate structure and aligning structures that violate the principle of minimality.", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.998272180557251}]}, {"text": "Our corpus gives alignments between AMRs and gold standard dependency parses.", "labels": [], "entities": [{"text": "AMRs", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.6883161664009094}]}, {"text": "To see how much performance degrades when such parses are not available we also evaluate on automatic parses.", "labels": [], "entities": []}, {"text": "Both precision and recall are substantially worse when the aligner relies on automatic syntax.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9995670914649963}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9995282888412476}]}], "tableCaptions": [{"text": " Table 1: Number of sentences whose highest alignment  configurations is max config.", "labels": [], "entities": []}, {"text": " Table 2: Frequency of alignment configurations for named entities, coordination, semantically decomposed words,  quantities and dates, and other phenomena.", "labels": [], "entities": []}, {"text": " Table 3: Lexical alignment (precision, recall, F 1 -score).  Our lexical alignment algorithm does not use syntax.", "labels": [], "entities": [{"text": "Lexical alignment", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8258770704269409}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9988406300544739}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.988595187664032}, {"text": "F 1 -score", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9814140349626541}]}]}