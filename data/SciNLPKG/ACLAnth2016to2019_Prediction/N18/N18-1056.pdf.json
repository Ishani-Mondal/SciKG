{"title": [{"text": "Robust Cross-lingual Hypernymy Detection using Dependency Context", "labels": [], "entities": [{"text": "Robust Cross-lingual Hypernymy Detection", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.5718826279044151}]}], "abstractContent": [{"text": "Cross-lingual Hypernymy Detection involves determining if a word in one language (\"fruit\") is a hypernym of a word in another language (\"pomme\" i.e. apple in French).", "labels": [], "entities": [{"text": "Cross-lingual Hypernymy Detection", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7247850894927979}]}, {"text": "The ability to detect hypernymy cross-lingually can aid in solving cross-lingual versions of tasks such as textual entailment and event coreference.", "labels": [], "entities": []}, {"text": "We propose BISPARSE-DEP, a family of un-supervised approaches for cross-lingual hyper-nymy detection, which learns sparse, bilingual word embeddings based on dependency contexts.", "labels": [], "entities": [{"text": "BISPARSE-DEP", "start_pos": 11, "end_pos": 23, "type": "METRIC", "confidence": 0.9459896087646484}, {"text": "cross-lingual hyper-nymy detection", "start_pos": 66, "end_pos": 100, "type": "TASK", "confidence": 0.6634436448415121}]}, {"text": "We show that BISPARSE-DEP can significantly improve performance on this task, compared to approaches based only on lexical context.", "labels": [], "entities": [{"text": "BISPARSE-DEP", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.9621063470840454}]}, {"text": "Our approach is also robust, showing promise for low-resource settings: our dependency-based embeddings can be learned using a parser trained on related languages, with negligible loss in performance.", "labels": [], "entities": []}, {"text": "We also crowd-source a challenging dataset for this task on four languages-Russian, French, Arabic, and Chinese.", "labels": [], "entities": []}, {"text": "Our embeddings and datasets are publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translation helps identify correspondences in bilingual texts, but other asymmetric semantic relationships can improve language understanding when translations are not exactly equivalent.", "labels": [], "entities": []}, {"text": "One such relationship is cross-lingual hypernymyidentifying that\u00e9cureuilthat\u00b4that\u00e9cureuil (\"squirrel\" in French) is a kind of rodent, or \u0432\u043e\u0440\u043e\u043d\u0430 (\"crow\" in Russian) is a kind of bird.", "labels": [], "entities": []}, {"text": "The ability to detect hypernyms across languages serves as a building block in a range of cross-lingual tasks, including Recognizing Textual Entailment (RTE) (, * These authors contributed equally.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 121, "end_pos": 157, "type": "TASK", "confidence": 0.7183554371198019}]}, {"text": "1 https://github.com/yogarshi/ bisparse-dep/ 2013), constructing multilingual taxonomies (), event coreference across multilingual news sources, and evaluating Machine Translation output).", "labels": [], "entities": [{"text": "event coreference across multilingual news sources", "start_pos": 93, "end_pos": 143, "type": "TASK", "confidence": 0.8427151143550873}, {"text": "Machine Translation", "start_pos": 160, "end_pos": 179, "type": "TASK", "confidence": 0.7937847673892975}]}, {"text": "Building models that can robustly identify hypernymy across the spectrum of human languages is a challenging problem, that is further compounded in low resource settings.", "labels": [], "entities": []}, {"text": "At first glance, translating words to English and then identifying hypernyms in a monolingual setting may appear to be a sufficient solution.", "labels": [], "entities": []}, {"text": "However, this approach cannot capture many phenomena.", "labels": [], "entities": []}, {"text": "For instance, the English words cook, leader and supervisor can all be hypernyms of the French word chef, as the French word does not have a exact translation in English covering its possible usages.", "labels": [], "entities": []}, {"text": "However, translating chef to cook and then determining hypernymy monolingually precludes identifying leader or supervisor as a hypernyms of chef.", "labels": [], "entities": []}, {"text": "Similarly, language-specific usage patterns can also influence hypernymy decisions.", "labels": [], "entities": []}, {"text": "For instance, the French word chroniqueur translates to chronicler in English, but is more frequently used in French to refer to journalists (making journalist its hypernym).", "labels": [], "entities": []}, {"text": "This motivates approaches that directly detect hypernymy in the cross-lingual setting by extending distributional methods for detecting monolingual hypernymy, as in our prior work.", "labels": [], "entities": []}, {"text": "State-of-the-art distributional approaches) for detecting monolingual hypernymy require syntactic analysis (eg. dependency parsing), which may not available for many languages.", "labels": [], "entities": [{"text": "dependency parsing)", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7852004766464233}]}, {"text": "Additionally, limited training resources make unsupervised methods more desirable than supervised hypernymy detection approaches.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7745034098625183}]}, {"text": "Furthermore, monolingual distributional approaches cannot be applied directly to the crosslingual task, because the vector spaces of two languages need to be aligned using a cross-lingual resource (a bilingual dictionary, for instance).", "labels": [], "entities": []}, {"text": "We tackle these challenges by proposing BISPARSE-DEP -a family of robust, unsupervised approaches for identifying cross-lingual hypernymy.", "labels": [], "entities": [{"text": "BISPARSE-DEP", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.9435498118400574}]}, {"text": "BISPARSE-DEP uses a cross-lingual word embedding model learned from a small bilingual dictionary and a variety of monolingual syntactic context extracted from a dependency parsed corpus.", "labels": [], "entities": [{"text": "BISPARSE-DEP", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.6715129613876343}]}, {"text": "BISPARSE-DEP exhibits robust behavior along multiple dimensions.", "labels": [], "entities": [{"text": "BISPARSE-DEP", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.6944999694824219}]}, {"text": "In the absence of a dependency treebank fora language, it can learn embeddings using a parser trained on related languages.", "labels": [], "entities": []}, {"text": "When exposed to less monolingual data, or a lower quality bilingual dictionary, BISPARSE-DEP degrades only marginally.", "labels": [], "entities": [{"text": "BISPARSE-DEP", "start_pos": 80, "end_pos": 92, "type": "METRIC", "confidence": 0.9884520173072815}]}, {"text": "In all these cases, it compares favorably with models that have been supplied with all necessary resources, showing promise for low-resource settings.", "labels": [], "entities": []}, {"text": "We extensively evaluate BISPARSE-DEP on anew crowd-sourced cross-lingual dataset, with over 2900 hypernym pairs, spanning four languages from distinct families -French, Russian, Arabic and Chinese -and release the datasets for future evaluations.", "labels": [], "entities": [{"text": "BISPARSE-DEP", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.6286287903785706}]}], "datasetContent": [{"text": "To verify if the crowdsourced hyponyms are challenging negative examples we create two evaluation sets.", "labels": [], "entities": []}, {"text": "Both share the (crowdsourced) positive examples, but differ in their negatives: \u2022 HYPER-HYPO -negative examples are the crowdsourced hyponyms.", "labels": [], "entities": [{"text": "HYPER-HYPO", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9489609599113464}]}, {"text": "\u2022 HYPER-COHYPO -negative examples are cohyponyms drawn from OMW.", "labels": [], "entities": [{"text": "HYPER-COHYPO -negative", "start_pos": 2, "end_pos": 24, "type": "METRIC", "confidence": 0.9432397484779358}, {"text": "OMW", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9007394313812256}]}, {"text": "Cohyponyms are words sharing a common hypernym.", "labels": [], "entities": []}, {"text": "For instance,b\u00ec ere (\"beer\" in French) and vodka are cohyponyms since they share a common hypernym in alcool/alcohol.", "labels": [], "entities": []}, {"text": "We choose cohyponyms for the second test set because: (a) They require differentiating between similarity (a symmetric relation) and hypernymy (an asymmetric relation).", "labels": [], "entities": []}, {"text": "For instance,b\u00ec ere and vodka are highly similar yet, they do not have a hypernymy relationship.", "labels": [], "entities": []}, {"text": "(b) Cohyponyms area popular choice of negative examples in many entailment datasets (Baroni and Lenci, 2011).", "labels": [], "entities": []}, {"text": "Training BISPARSE-DEP requires a dependency parsed monolingual corpus, and a translation matrix for jointly aligning the monolingual vectors.", "labels": [], "entities": [{"text": "BISPARSE-DEP", "start_pos": 9, "end_pos": 21, "type": "METRIC", "confidence": 0.9524874091148376}]}, {"text": "We compute the translation matrix using word alignments derived from parallel corpora (see corpus statistics in?).", "labels": [], "entities": [{"text": "word alignments", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.6957285106182098}]}, {"text": "While we use parallel corpora to generate the translation matrix to be comparable to baselines ( \u00a75.2), we can obtain the matrix from any bilingual dictionary.", "labels": [], "entities": []}, {"text": "The monolingual corpora are parsed using Yara Parser (, trained on the corresponding treebank from the Universal Dependency Treebank () (UDT-v1.4).", "labels": [], "entities": [{"text": "Universal Dependency Treebank", "start_pos": 103, "end_pos": 132, "type": "DATASET", "confidence": 0.6728054881095886}, {"text": "UDT-v1.4", "start_pos": 137, "end_pos": 145, "type": "DATASET", "confidence": 0.880852997303009}]}, {"text": "Yara Parser was chosen as it is fast, and competitive with stateof-the-art parsers ().", "labels": [], "entities": []}, {"text": "The monolingual corpora was POS-tagged using TurboTagger ().", "labels": [], "entities": []}, {"text": "We induce dependency contexts for words by first thresholding the language vocabulary to the top 50,000 nouns, verbs and adjectives.", "labels": [], "entities": []}, {"text": "A co-occurrence matrix is computed over this vocabulary using the context types in \u00a73.1.", "labels": [], "entities": []}, {"text": "We aim to answer the following questions -(a) Are dependency based embeddings superior to window based embeddings for identifying crosslingual hypernymy?", "labels": [], "entities": []}, {"text": "( \u00a76.1) (b) Does directionality in the dependency context help cross-lingual hypernymy identification?", "labels": [], "entities": [{"text": "cross-lingual hypernymy identification", "start_pos": 63, "end_pos": 101, "type": "TASK", "confidence": 0.7349278728167216}]}, {"text": "( \u00a76.2) (c) Are our models robust in data scarce settings ( \u00a76.3)?", "labels": [], "entities": []}, {"text": "(d) Is the answer to (a) predicated on the choice of entailment scorer?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Crowd-sourced dataset statistics. #pos (#neg)  denote positives (negatives) in the evaluation set. We  deliberately under-sample negatives to have a balanced  evaluation set.", "labels": [], "entities": []}, {"text": " Table 3: Comparing the different approaches from  \u00a75.2 with our BISPARSE-DEP approach on HYPER-HYPO  and HYPER-COHYPO (random baseline= 0.5). Bold denotes the best score for each language, and the * on the  best score indicates a statistically significant (p < 0.05) improvement over the next best score, using McNemar's  test", "labels": [], "entities": [{"text": "BISPARSE-DEP", "start_pos": 65, "end_pos": 77, "type": "METRIC", "confidence": 0.9764131903648376}, {"text": "McNemar's  test", "start_pos": 312, "end_pos": 327, "type": "DATASET", "confidence": 0.8776342074076334}]}, {"text": " Table 4: Robustness in absence of a treebank: The  delexicalized model is competitive to the best depen- dency based and the best window based models on both  test sets. For each dataset, * indicates a statistically  significant (p < 0.05) improvement over the next best  model in that column, using McNemar's test", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 301, "end_pos": 315, "type": "DATASET", "confidence": 0.8906387885411581}]}]}