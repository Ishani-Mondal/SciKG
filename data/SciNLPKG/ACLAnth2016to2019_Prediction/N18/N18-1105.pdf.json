{"title": [{"text": "Sentences with Gapping: Parsing and Reconstructing Elided Predicates", "labels": [], "entities": [{"text": "Parsing and Reconstructing Elided Predicates", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.8772768497467041}]}], "abstractContent": [{"text": "Sentences with gapping, such as Paul likes coffee and Mary tea, lack an overt predicate to indicate the relation between two or more arguments.", "labels": [], "entities": []}, {"text": "Surface syntax representations of such sentences are often produced poorly by parsers, and even if correct, not well suited to downstream natural language understanding tasks such as relation extraction that are typically designed to extract information from sentences with canonical clause structure.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.8120980560779572}]}, {"text": "In this paper, we present two methods for parsing to a Universal Dependencies graph representation that explicitly encodes the elided material with additional nodes and edges.", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9643170237541199}]}, {"text": "We find that both methods can reconstruct elided material from dependency trees with high accuracy when the parser correctly predicts the existence of a gap.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9962255954742432}]}, {"text": "We further demonstrate that one of our methods can be applied to other languages based on a case study on Swedish.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentences with gapping such as Paul likes coffee and Mary tea are characterized by having one or more conjuncts that contain multiple arguments or modifiers of an elided predicate.", "labels": [], "entities": []}, {"text": "In this example, the predicate likes is elided for the relation Mary likes tea.", "labels": [], "entities": []}, {"text": "While these sentences appear relatively infrequently inmost written texts, they are often used to convey a lot of factual information that is highly relevant for language understanding (NLU) tasks such as open information extraction and semantic parsing.", "labels": [], "entities": [{"text": "open information extraction", "start_pos": 205, "end_pos": 232, "type": "TASK", "confidence": 0.6490135192871094}, {"text": "semantic parsing", "start_pos": 237, "end_pos": 253, "type": "TASK", "confidence": 0.7166784852743149}]}, {"text": "For example, consider the following sentence from the WSJ portion of the Penn Treebank (: Overview of our two approaches.", "labels": [], "entities": [{"text": "WSJ portion of the Penn Treebank", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.9325538476308187}]}, {"text": "Both methods first parse a sentence with gapping to one of two different dependency tree representations and then reconstruct the elided predicate from this tree.", "labels": [], "entities": []}, {"text": "To extract the information about unemployment rates in the various countries, an NLU system has to identify that the percentages indicate unemployment rates and the locational modifiers indicate the corresponding country.", "labels": [], "entities": []}, {"text": "Given only this sentence, or this sentence and a strict surface syntax representation that does not indicate elided predicates, this is a challenging task.", "labels": [], "entities": []}, {"text": "However, given a dependency graph that reconstructs the elided predicate for each conjunct, the problem becomes much easier and methods developed to extract information from dependency trees of clauses with canonical structures are much more likely to extract the correct information from a gapped clause.", "labels": [], "entities": []}, {"text": "While gapping constructions receive a lot of attention in the theoretical syntax literature (e.g.,, they have been almost entirely neglected by the NLP community so far.", "labels": [], "entities": []}, {"text": "The Penn Treebank explicitly annotates gapping constructions, by coindexing arguments in the clause with a predicate and the clause with the gap, but these co-indices are not included in the standard parsing metrics and almost all parsers ignore them.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9924512803554535}]}, {"text": "Despite the sophisticated analysis of gapping within CCG, sentences with gapping were deemed too difficult to represent within the CCGBank.", "labels": [], "entities": [{"text": "CCG", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9101017713546753}, {"text": "CCGBank", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.9592133164405823}]}, {"text": "Similarly the treebanks for the Semantic Dependencies Shared Task () exclude all sentences from the Wall Street Journal that contain gapping.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 100, "end_pos": 119, "type": "DATASET", "confidence": 0.9500820438067118}]}, {"text": "Finally, while the tectogrammatical layer of the Prague Dependency Treebank) as well as the enhanced Universal Dependencies (UD) representation provide an analysis with reconstructed nodes for gapping constructions, there exist no methods to automatically parse to these representations.", "labels": [], "entities": [{"text": "Prague Dependency Treebank", "start_pos": 49, "end_pos": 75, "type": "DATASET", "confidence": 0.9813950061798096}]}, {"text": "Here, we provide the first careful analysis of parsing of gapping constructions, and we present two methods for reconstructing elided predicates in sentences with gapping within the UD framework.", "labels": [], "entities": [{"text": "parsing of gapping constructions", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.8981043249368668}, {"text": "UD framework", "start_pos": 182, "end_pos": 194, "type": "DATASET", "confidence": 0.750631719827652}]}, {"text": "As illustrated in, we first parse to a dependency tree and then reconstruct the elided material.", "labels": [], "entities": []}, {"text": "The methods differ in how much information is encoded in the dependency tree.", "labels": [], "entities": []}, {"text": "The first method adapts an existing procedure for parsing sentences with elided function words, which uses composite labels that can be deterministically turned into dependency graphs inmost cases.", "labels": [], "entities": [{"text": "parsing sentences with elided function words", "start_pos": 50, "end_pos": 94, "type": "TASK", "confidence": 0.8391357560952505}]}, {"text": "The second method is a novel procedure that relies on the parser only to identify a gap, and then employs an unsupervised method to reconstruct the elided predicates and reattach the arguments to the reconstructed predicate.", "labels": [], "entities": []}, {"text": "We find that both methods can reconstruct elided predicates with very high accuracy from gold standard dependency trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.997860848903656}]}, {"text": "When applied to the output of a parser, which often fails to identify gapping, our methods achieve a sentence-level accuracy of 32% and 34%, significantly outperforming the recently proposed constituent parser by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.927729070186615}]}], "datasetContent": [{"text": "Both methods rely on a dependency parser followed by a post-processing step.", "labels": [], "entities": []}, {"text": "We evaluated the individual steps and the end-to-end performance.", "labels": [], "entities": []}, {"text": "Parser We used the parser by Dozat and Manning (2017) for parsing to the two different intermediate dependency representations.", "labels": [], "entities": []}, {"text": "This parser is a graph-based parser) that uses a biLSTM to compute token representations and then uses a multi-layer perceptron with biaffine attention to compute arc and label scores.", "labels": [], "entities": []}, {"text": "Setup We trained the parser on the COMBINED training corpus with gold tokenization, and predicted fine-grained and universal part-of-speech tags, for which we used the tagger by: Labeled (LAS g ) and unlabeled attachment score (UAS g ) of head tokens of remnants for parsers trained and evaluated on the UD representation (ORPHAN) and the composite relations representation (COMPOSITE) on the development and test sets of the COMBINED treebank.", "labels": [], "entities": [{"text": "COMBINED training corpus", "start_pos": 35, "end_pos": 59, "type": "DATASET", "confidence": 0.7410264213879904}, {"text": "Labeled (LAS g ) and unlabeled attachment score (UAS g )", "start_pos": 179, "end_pos": 235, "type": "METRIC", "confidence": 0.7784940760869247}, {"text": "COMBINED treebank", "start_pos": 426, "end_pos": 443, "type": "DATASET", "confidence": 0.866801917552948}]}, {"text": "Results that differ significantly are marked with * (p < 0.05) or *** (p < 0.001).", "labels": [], "entities": []}, {"text": "tistical significance of pairwise comparisons, we performed two-tailed approximate randomization tests) with an adapted version of the sigf package ().", "labels": [], "entities": []}, {"text": "shows the overall parsing results on the development and test sets of the two treebanks.", "labels": [], "entities": []}, {"text": "There was no significant difference between the parser that was trained on the UD representation (ORPHAN) and the parser trained on the composite representation (COMPOSITE) when tested on the EWT data sets, which is not surprising considering that there is just one sentence with gapping each in the development and the test split.", "labels": [], "entities": [{"text": "ORPHAN", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.95458984375}, {"text": "EWT data sets", "start_pos": 192, "end_pos": 205, "type": "DATASET", "confidence": 0.9804415305455526}]}, {"text": "When evaluated on the GAPPING datasets, the OR-PHAN parser performs significantly better (p < 0.01) in terms of labeled attachment score, which suggests that the parser trained on the COMPOS-ITE representation is indeed struggling with the greatly increased label space.", "labels": [], "entities": [{"text": "GAPPING datasets", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.7549947500228882}, {"text": "labeled attachment score", "start_pos": 112, "end_pos": 136, "type": "METRIC", "confidence": 0.6810484925905863}]}, {"text": "This is further confirmed by the attachment scores of the head tokens of remnants.", "labels": [], "entities": []}, {"text": "The labeled attachment score of remnants is significantly higher for the ORPHAN parser than for the COMPOSITE parser.", "labels": [], "entities": [{"text": "labeled attachment score", "start_pos": 4, "end_pos": 28, "type": "METRIC", "confidence": 0.798850437005361}, {"text": "ORPHAN", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9100707173347473}]}, {"text": "Further, the unlabeled attachment score on the test set is also higher for the ORPHAN parser, which suggests that the COMPOSITE parser is sometimes struggling with finding the right attachment for the multiple long-distance composite dependencies.", "labels": [], "entities": [{"text": "ORPHAN", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9792554974555969}]}, {"text": "Our second set of experiments concerns the recovery of the elided material and the reattachment of the orphans.", "labels": [], "entities": []}, {"text": "We conducted two experiments: an oracle experiment that used gold standard dependency trees and an end-to-end experiment that used the output of the parser as input.", "labels": [], "entities": []}, {"text": "For all experiments, we used the COMBINED treebank.", "labels": [], "entities": [{"text": "COMBINED treebank", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.809544026851654}]}, {"text": "Evaluation Here, we evaluated dependency graphs and therefore used the labeled and unlabeled precision and recall metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9877027869224548}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9939776659011841}]}, {"text": "However, as our two procedures are only changing the attachment of orphans, we only computed these metrics for copy nodes and their dependents.", "labels": [], "entities": []}, {"text": "Further, we excluded punctuation and coordinating conjunctions as their attachment is usually trivial and including them would inflate scores.", "labels": [], "entities": []}, {"text": "Lastly, we computed the sentence-level accuracy for all sentences with gapping.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9464505314826965}]}, {"text": "For this metric, we considered a sentence to be correct if all copy nodes and their dependents of a sentence were attached to the correct head with the correct label.", "labels": [], "entities": []}, {"text": "Oracle results The top part of shows the results for the oracle experiment.", "labels": [], "entities": []}, {"text": "Both methods are able to reconstruct the elided material and the canonical clause structure from gold dependency trees with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9935857653617859}]}, {"text": "This was expected for the COMPOSITE procedure, which can make use of the composite relations in the dependency trees, but less so for the ORPHAN procedure which has to recover the structure and the types of relations.", "labels": [], "entities": []}, {"text": "The two methods work equally well in terms of all metrics except for the sentence-level accuracy, which is significantly higher for the COMPOSITE procedure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9374107122421265}]}, {"text": "This difference is caused by a difference in the types of mistakes.", "labels": [], "entities": []}, {"text": "All errors of the COMPOSITE procedure are of a structural nature and stem from copying the wrong number of nodes while the dependency labels are always correct because they are part of the dependency tree.", "labels": [], "entities": []}, {"text": "The majority of errors of the ORPHAN procedure stem from incorrect dependency labels, and these mistakes are scattered across more examples, which leads to the lower sentence-level accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9730758666992188}]}, {"text": "End-to-end results The middle part of shows the results for the end-to-end experiment.", "labels": [], "entities": []}, {"text": "The performance of both methods is considerably lower than in the oracle experiment, which is pri-: Labeled and unlabeled precision and recall as well as sentence-level accuracy of the two gapping reconstructions methods and the K&K parser on the development and test set of the COMBINED treebank.", "labels": [], "entities": [{"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.996833860874176}, {"text": "recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9990506768226624}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9517658352851868}, {"text": "COMBINED treebank", "start_pos": 279, "end_pos": 296, "type": "DATASET", "confidence": 0.894704669713974}]}, {"text": "Results that differ significantly from the other result within the same section are marked with * (p < 0.05) or ** (p < 0.01).", "labels": [], "entities": []}, {"text": "marily driven by the much lower recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9994747042655945}]}, {"text": "Both methods assume that the parser detects the existence of a gap and if the parser fails to do so, neither method attempts to reconstruct the elided material.", "labels": [], "entities": []}, {"text": "In general, precision tends to be a bit higher for the ORPHAN procedure whereas recall tends to be a bit higher for the COMPOSITE method but overall and in terms of sentence-level accuracy both methods seem to perform equally well.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9995724558830261}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9991030693054199}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9385566115379333}]}, {"text": "Error analysis For both methods, the primary issue is low recall, which is a result of parsing errors.", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9287872910499573}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.998195230960846}]}, {"text": "When the parser correctly predicts the orphan relation, the main sources of error for the ORPHAN procedure are missing correspondents for remnants (e.g.,for good] has no correspondent in They had left the company, many for good) or that the types of argument of the remnant and its correspondent differ (e.g., in She was convicted of selling unregistered securities in Florida and of unlawful phone calls in Ohio, [of selling unregistered securities] is an adverbial clause whereas [of unlawful phone calls] is an oblique modifier).", "labels": [], "entities": [{"text": "ORPHAN", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9487733244895935}]}, {"text": "Apart from the cases where the COMPOSITE procedure leads to an incorrect structure, the remaining errors are all caused by the parser predicting the wrong composite relation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Treebank statistics. The copy nodes row lists the number of copy nodes and the unique composite rela- tions row lists the number of unique composite relations in the treebanks annotated according to the COMPOSITE  analysis. The percentages are relative to the total number of sentences.", "labels": [], "entities": []}, {"text": " Table 2: Distribution of gap types in our corpus. The  classification is according to the four types of gaps that  we discussed in Section 2.1.", "labels": [], "entities": []}, {"text": " Table 3: Labeled (LAS) and unlabeled attachment  score (UAS) of parsers trained and evaluated on the UD  representation (ORPHAN) and the composite relations  representation (COMPOSITE) on the development and  test sets of the EWT and the GAPPING treebank. ** in- dicates that results differ significantly at p < 0.01.", "labels": [], "entities": [{"text": "Labeled (LAS) and unlabeled attachment  score (UAS)", "start_pos": 10, "end_pos": 61, "type": "METRIC", "confidence": 0.7409897229888223}, {"text": "ORPHAN", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.6584356427192688}, {"text": "EWT", "start_pos": 227, "end_pos": 230, "type": "DATASET", "confidence": 0.8957139849662781}, {"text": "GAPPING treebank", "start_pos": 239, "end_pos": 255, "type": "DATASET", "confidence": 0.8630535006523132}]}, {"text": " Table 4:  Labeled (LAS g ) and unlabeled attach- ment score (UAS g ) of head tokens of remnants for  parsers trained and evaluated on the UD representation  (ORPHAN) and the composite relations representation  (COMPOSITE) on the development and test sets of the  COMBINED treebank. Results that differ significantly  are marked with * (p < 0.05) or *** (p < 0.001).", "labels": [], "entities": [{"text": "Labeled (LAS g ) and unlabeled attach- ment score (UAS g )", "start_pos": 11, "end_pos": 69, "type": "METRIC", "confidence": 0.7742645601431529}, {"text": "COMBINED treebank", "start_pos": 264, "end_pos": 281, "type": "DATASET", "confidence": 0.8538223206996918}]}, {"text": " Table 5: Labeled and unlabeled precision and recall as well as sentence-level accuracy of the two gapping recon- structions methods and the K&K parser on the development and test set of the COMBINED treebank. Results that  differ significantly from the other result within the same section are marked with * (p < 0.05) or ** (p < 0.01).", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9959361553192139}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9988521337509155}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9857847094535828}, {"text": "COMBINED treebank", "start_pos": 191, "end_pos": 208, "type": "DATASET", "confidence": 0.9127176403999329}]}]}