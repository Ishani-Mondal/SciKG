{"title": [{"text": "Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-based Sentiment Analysis", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8047609329223633}]}], "abstractContent": [{"text": "While neural networks have been shown to achieve impressive results for sentence-level sentiment analysis, targeted aspect-based sentiment analysis (TABSA)-extraction of fine-grained opinion polarity w.r.t. a pre-defined set of aspects-remains a difficult task.", "labels": [], "entities": [{"text": "sentence-level sentiment analysis", "start_pos": 72, "end_pos": 105, "type": "TASK", "confidence": 0.7786158124605814}, {"text": "targeted aspect-based sentiment analysis (TABSA)-extraction of fine-grained opinion polarity w.r.t. a pre-defined set of aspects-remains", "start_pos": 107, "end_pos": 243, "type": "TASK", "confidence": 0.8146180344952477}]}, {"text": "Motivated by recent advances in memory-augmented models for machine reading, we propose a novel architecture, utilising external \"memory chains\" with a delayed memory update mechanism to track entities.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.7654429078102112}]}, {"text": "On a TABSA task, the proposed model demonstrates substantial improvements over state-of-the-art approaches, including those using external knowledge bases.", "labels": [], "entities": [{"text": "TABSA task", "start_pos": 5, "end_pos": 15, "type": "TASK", "confidence": 0.531397670507431}]}], "introductionContent": [{"text": "Targeted aspect-based sentiment analysis (TABSA) is the task of identifying fine-grained opinion polarity towards a specific aspect associated with a given target.", "labels": [], "entities": [{"text": "Targeted aspect-based sentiment analysis (TABSA)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8064810633659363}]}, {"text": "The task requires classification of opinions on different entities across a range of different attributes, with the expectation that there will be no overt opinion expressed on a given entity for many attributes.", "labels": [], "entities": []}, {"text": "This can be seen in Example (1), e.g., where opinions on the aspects SAFETY and PRICE are expressed for entity LOC1 but not entity LOC2: (1) LOC1 is your best bet for secure although expensive and LOC2 is too far.", "labels": [], "entities": [{"text": "SAFETY", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.6627351641654968}]}, {"text": "Note that in our dataset, all entity mentions have been pre-nomalised to LOCn, where n is an index.", "labels": [], "entities": [{"text": "LOCn", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9293735027313232}]}, {"text": "The earliest work on (T)ABSA relied heavily on feature engineering (), but more recent work based on deep learning has used models such as LSTMs to automatically learn aspect-specific word and sentence representations (.", "labels": [], "entities": [{"text": "T)ABSA", "start_pos": 22, "end_pos": 28, "type": "TASK", "confidence": 0.5359620153903961}]}, {"text": "Despite these successes, keeping track of multiple entity-aspect pairs remains a difficult task, even for an LSTM.", "labels": [], "entities": []}, {"text": "As reported in, a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features.", "labels": [], "entities": [{"text": "aspect detection", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.700459435582161}, {"text": "sentiment classification", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.9396313726902008}]}, {"text": "Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research.", "labels": [], "entities": []}, {"text": "More recently, successful works in (T)ABSA have explored the idea of leveraging external memory ().", "labels": [], "entities": [{"text": "T)ABSA", "start_pos": 36, "end_pos": 42, "type": "TASK", "confidence": 0.46949154138565063}]}, {"text": "Their models are largely based on memory networks ( , originally developed for reasoning-focused machine reading comprehension tasks.", "labels": [], "entities": []}, {"text": "In contrast to memory networks, where each input sentence/word occupies a memory slot and is then accessed via attention independently, recent advances in machine reading suggest that processing inputs sequentially is beneficial to overall performance (.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 155, "end_pos": 170, "type": "TASK", "confidence": 0.7584157884120941}]}, {"text": "However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of inputs between the two tasks: on the Children's Book Test corpus (CBT), for example, competitive models take as input a window of text, centred around candidate entities, with crucial information contained within that window (.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7029601782560349}, {"text": "Children's Book Test corpus (CBT)", "start_pos": 168, "end_pos": 201, "type": "DATASET", "confidence": 0.7945582792162895}]}, {"text": "In TABSA, given the fine-grained nature of the task, it is common practice for models to operate at the word-rather than chunk/sentencelevel.", "labels": [], "entities": []}, {"text": "It is not uncommon to see examples like Example (1), where the sentence starts with LOC1, but the negative PRICE sentiment towards the entity is not expressed until much later.", "labels": [], "entities": [{"text": "LOC1", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9507918953895569}]}, {"text": "Moreover, phrases such as best bet and although play the role of triggers, indicating that succeeding tokens bear aspect/sentiment signal.", "labels": [], "entities": [{"text": "although", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9528458118438721}]}, {"text": "This key difference necessitates the ability to model the delayed activation of memory updates.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel model architecture for TABSA, augmented with multiple \"memory chains\", and equipped with a delayed memory update mechanism, to keep track of numerous entities independently.", "labels": [], "entities": [{"text": "TABSA", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.6027260422706604}]}, {"text": "We evaluate the effectiveness of the proposed model over the task of TABSA, and achieve substantial improvements over a number of baselines, including one incorporating external knowledge bases, setting anew state of the art in both sentiment classification and aspect detection.", "labels": [], "entities": [{"text": "TABSA", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.49895310401916504}, {"text": "sentiment classification", "start_pos": 233, "end_pos": 257, "type": "TASK", "confidence": 0.9519014060497284}, {"text": "aspect detection", "start_pos": 262, "end_pos": 278, "type": "TASK", "confidence": 0.8764482736587524}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance on Sentihood. We take the results reported in Saeidi et al. (2016) and Ma et al.", "labels": [], "entities": [{"text": "Sentihood", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.9453086256980896}]}]}