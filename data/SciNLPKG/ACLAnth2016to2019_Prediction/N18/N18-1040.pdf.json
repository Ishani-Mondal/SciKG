{"title": [{"text": "Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7333057820796967}]}], "abstractContent": [{"text": "Visual question answering (Visual QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve.", "labels": [], "entities": [{"text": "Visual question answering (Visual QA)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.789120227098465}]}, {"text": "In this paper, we study a crucial component of this task: how can we design good datasets for the task?", "labels": [], "entities": []}, {"text": "We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (i.e. the correct one) and the decoys (i.e. the incorrect ones).", "labels": [], "entities": []}, {"text": "Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show that the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets.", "labels": [], "entities": []}, {"text": "In particular , the resulting learner can ignore the visual information, the question, or both while still doing well on the task.", "labels": [], "entities": []}, {"text": "Inspired by this, we propose automatic procedures to remedy such design deficiencies.", "labels": [], "entities": []}, {"text": "We apply the procedures to reconstruct decoy answers for two popular Visual QA datasets as well as to create anew Visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task.", "labels": [], "entities": [{"text": "Visual QA datasets", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.7877204020818075}, {"text": "Visual QA dataset", "start_pos": 114, "end_pos": 131, "type": "DATASET", "confidence": 0.8206276297569275}]}, {"text": "Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models.", "labels": [], "entities": []}, {"text": "The datasets are released and publicly available via http://www.teds.", "labels": [], "entities": []}, {"text": "usc.edu/website_vqa/.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimodal information processing tasks such as image captioning and visual question answering (Visual QA) () have * Equal contributions In the original dataset, the correct answer \"A train\" is easily selected by a machine as it is far often used as the correct answer than the other decoy (negative) answers.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7512732446193695}, {"text": "visual question answering", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7109628915786743}]}, {"text": "(The numbers in the brackets are probability scores computed using eq. (2)).", "labels": [], "entities": []}, {"text": "Our two proceduresQoU and IoU (cf. Sect. 4) -create alternative decoys such that both the correct answer and the decoys are highly likely by examining either the image or the question alone.", "labels": [], "entities": []}, {"text": "In these cases, machines make mistakes unless they consider all information together.", "labels": [], "entities": []}, {"text": "Thus, the alternative decoys suggested our procedures are better designed to gauge how well a learning algorithm can understand all information equally well.", "labels": [], "entities": []}, {"text": "gained a lot of attention recently.", "labels": [], "entities": []}, {"text": "A number of significant advances in learning algorithms have been made, along with the development of nearly two dozens of datasets in this very active research domain.", "labels": [], "entities": []}, {"text": "Among those datasets, popular ones include MSCOCO (, Visual Genome (), VQA (, and several others.", "labels": [], "entities": [{"text": "VQA", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.8547412157058716}]}, {"text": "The overarching objective is that a learning machine needs to go beyond understanding different modalities of information separately (such as image recognition alone) and to learn how to correlate them in order to perform well on those tasks.", "labels": [], "entities": [{"text": "image recognition", "start_pos": 142, "end_pos": 159, "type": "TASK", "confidence": 0.7182996273040771}]}], "datasetContent": [{"text": "In this section, we describe our approaches of remedying design deficiencies in the existing datasets for the Visual QA task.", "labels": [], "entities": [{"text": "Visual QA task", "start_pos": 110, "end_pos": 124, "type": "TASK", "confidence": 0.5000834266344706}]}, {"text": "We introduce two automatic and widely-applicable procedures to create new decoys that can prevent learning models from exploiting incident statistics in the datasets.", "labels": [], "entities": []}, {"text": "Several authors have noticed the design deficiencies in the existing databases and have proposed \"fixes\" (.", "labels": [], "entities": []}, {"text": "No dataset has used a procedure to generate IoU-decoys.", "labels": [], "entities": []}, {"text": "We empirically show that how the IoU-decoys significantly remedy the design deficiencies in the datasets.", "labels": [], "entities": []}, {"text": "Several previous efforts have generated decoys that are similar in spirit to our QoU-decoys.,, and automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and GLOVE embeddings (, and paragraph vectors () and linguistic surface similarity, respectively.", "labels": [], "entities": []}, {"text": "The later two are for different tasks from Visual QA, and only consider removing semantically ambiguous decoys like ours. and ask humans to create decoys, given the questions and targets.", "labels": [], "entities": []}, {"text": "As shown earlier, such decoys may disobey the rule of Neutrality.", "labels": [], "entities": [{"text": "Neutrality", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.7291933298110962}]}, {"text": "augment the VQA dataset () (by human efforts) with additional IQT triplets to eliminate the shortcuts (language prior) in the open-ended setting.", "labels": [], "entities": [{"text": "VQA dataset", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.9398332238197327}]}, {"text": "Their effort is complementary to ours on the multiplechoice setting.", "labels": [], "entities": []}, {"text": "Note that an extended task of Visual QA, visual dialog (, also adopts the latter setting.", "labels": [], "entities": []}, {"text": "We examine our automatic procedures for creating decoys on five datasets.", "labels": [], "entities": []}, {"text": "summarizes the characteristics of the three datasets we focus on.", "labels": [], "entities": []}, {"text": "As the test set does not indicate the targets, our studies focus on the training and validation sets.", "labels": [], "entities": []}, {"text": "We divide the dataset into non-overlapping 50%/20%/30% for training/validation/testing.", "labels": [], "entities": []}, {"text": "Additionally, we partition such that each portion is a \"superset\" of the corresponding one in Visual7W, respectively.", "labels": [], "entities": [{"text": "Visual7W", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.9404035806655884}]}, {"text": "Creating decoys We create 3 QoU-decoys and 3 IoU-decoys for every IQT triplet in each dataset, following the steps in Sect.", "labels": [], "entities": []}, {"text": "In the cases that we cannot find 3 decoys, we include random ones from the original set of decoys for VQA and Visual7W; for other datasets, we randomly include those from the top 10 frequently-occurring targets.", "labels": [], "entities": [{"text": "VQA", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9404751062393188}, {"text": "Visual7W", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.8675035834312439}]}], "tableCaptions": [{"text": " Table 1: Accuracy of selecting the right answers out of  4 choices (%) on the Visual QA task on Visual7W.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9923230409622192}]}, {"text": " Table 2: Summary of Visual QA datasets.", "labels": [], "entities": [{"text": "Visual QA datasets", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.7840085029602051}]}, {"text": " Table 3: Test accuracy (%) on Visual7W.", "labels": [], "entities": [{"text": "Test", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9288461208343506}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9201042652130127}, {"text": "Visual7W", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9652086496353149}]}, {"text": " Table 4: Accuracy (%) on the validation set in VQA.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993106126785278}, {"text": "VQA", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9021722674369812}]}, {"text": " Table 5: Test accuracy (%) on qaVG.", "labels": [], "entities": [{"text": "Test", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9454517960548401}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9489257335662842}]}, {"text": " Table 6: Using models trained on qaVG to improve Vi- sual7W and VQA (Accuracy in %).", "labels": [], "entities": [{"text": "VQA", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.954673707485199}, {"text": "Accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9983501434326172}]}]}