{"title": [{"text": "Visual Referring Expression Recognition: What Do Systems Actually Learn?", "labels": [], "entities": [{"text": "Visual Referring Expression Recognition", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8407382965087891}]}], "abstractContent": [{"text": "We present an empirical analysis of state-of-the-art systems for referring expression recognition the task of identifying the object in an image referred to by a natural language expression-with the goal of gaining insight into how these systems reason about language and vision.", "labels": [], "entities": [{"text": "referring expression recognition", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.8116251230239868}]}, {"text": "Surprisingly, we find strong evidence that even sophisticated and linguistically-motivated models for this task may ignore linguistic structure, instead relying on shallow correlations introduced by unin-tended biases in the data selection and annotation process.", "labels": [], "entities": []}, {"text": "For example, we show that a system trained and tested on the input image without the input referring expression can achieve a precision of 71.2% in top-2 predictions.", "labels": [], "entities": [{"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.998935878276825}]}, {"text": "Furthermore , a system that predicts only the object category given the input can achieve a precision of 84.2% in top-2 predictions.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9990679621696472}]}, {"text": "These surprisingly positive results for what should be deficient prediction scenarios suggest that careful analysis of what our models are learning and further, how our data is constructed-is critical as we seek to make substantive progress on grounded language tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been increasing interest in modeling natural language in the context of a visual grounding.", "labels": [], "entities": []}, {"text": "Several benchmark datasets have recently been introduced for describing a visual scene with natural language, describing or localizing specific objects in a scene (, answering natural language questions about the scenes (, and performing visually grounded dialogue (.", "labels": [], "entities": []}, {"text": "Here, we focus on referring expression recognition (RER) -the task of identifying the object in an image that is referred to by a natural language expression produced by a human (.", "labels": [], "entities": [{"text": "referring expression recognition (RER)", "start_pos": 18, "end_pos": 56, "type": "TASK", "confidence": 0.854328989982605}]}, {"text": "Recent work on RER has sought to make progress by introducing models that are better capable of reasoning about linguistic structure () -however, since most of the state-of-the-arts systems involve complex neural parameterizations, what these models actually learn has been difficult to interpret.", "labels": [], "entities": [{"text": "RER", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9336808323860168}]}, {"text": "This is concerning because several post-hoc analyses of related tasks ( have revealed that some positive results are actually driven by superficial biases in datasets or shallow correlations without deeper visual or linguistic understanding.", "labels": [], "entities": []}, {"text": "Evidently, it is hard to be completely sure if a model is performing well for the right reasons.", "labels": [], "entities": []}, {"text": "To increase our understanding of how RER systems function, we present several analyses inspired by approaches that probe systems with perturbed inputs ( and employ simple models to exploit and reveal biases in datasets).", "labels": [], "entities": [{"text": "RER", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9561927318572998}]}, {"text": "First, we investigate whether systems that were designed to incorporate linguistic structure actually require it and make use of it.", "labels": [], "entities": []}, {"text": "To test this, we perform perturbation experiments on the input referring expressions.", "labels": [], "entities": []}, {"text": "Surprisingly, we find that models are robust to shuffling the word order and limiting the word categories to nouns and adjectives.", "labels": [], "entities": []}, {"text": "Second, we attempt to reveal shallower correlations that systems might instead be leveraging to do well on this task.", "labels": [], "entities": []}, {"text": "We build two simple systems called Neural Sieves: one that completely ignores the input referring expression and another that only predicts the category of the referred object from the input expression.", "labels": [], "entities": []}, {"text": "Again, surprisingly, both sieves are able to identify the correct object with surprising precision in top-2 and top-3 predictions.", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9950371384620667}]}, {"text": "When these two simple systems are com-bined, the resulting system achieves precisions of 84.2% and 95.3% for top-2 and top-3 predictions, respectively.", "labels": [], "entities": [{"text": "precisions", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9993553757667542}]}, {"text": "These results suggest that to make meaningful progress on grounded language tasks, we need to pay careful attention to what and how our models are learning, and whether our datasets contain exploitable bias.", "labels": [], "entities": []}], "datasetContent": [{"text": "We are interested in determining how accurate these simple nueral sieves can be.", "labels": [], "entities": []}, {"text": "High accuracy here would give a possible explanation for the high performance of more complex models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9993374943733215}]}, {"text": "For our experiments, we use GoogleRef ( which is one of the standard benchmarks for referring expression recognition.", "labels": [], "entities": [{"text": "referring expression recognition", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.8292444149653116}]}, {"text": "It consists of around 26K images with 104K annotations.", "labels": [], "entities": []}, {"text": "We use their Ground-Truth evaluation setup where the ground truth bounding box annotations from MSCOCO () are provided to the system as apart of the input.", "labels": [], "entities": []}, {"text": "We used the split provided by where splits have disjoint sets of images.", "labels": [], "entities": []}, {"text": "We use precision@k for evaluating the performance of models.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9974004030227661}]}, {"text": "To train our models, we used stochastic gradient descent for 6 epochs with an initial learning rate of 0.01 and multiplied by 0.4 after each epoch.", "labels": [], "entities": []}, {"text": "Word embeddings were initialized using GloVe () and finetuned during training.", "labels": [], "entities": []}, {"text": "We extracted features for bounding boxes using the fc7 layer output of Faster-RCNN VGG-16 network pre-trained on MSCOCO dataset ().", "labels": [], "entities": [{"text": "Faster-RCNN VGG-16 network pre-trained on MSCOCO dataset", "start_pos": 71, "end_pos": 127, "type": "DATASET", "confidence": 0.7344954822744642}]}, {"text": "Hyperparameters such as hidden layer size of LSTM networks were picked based on the best validation score.", "labels": [], "entities": []}, {"text": "For perturbation experiments, we did not perform any grid search for hyperparameters.", "labels": [], "entities": []}, {"text": "We used hyperparameters of the previously reported best performing model in the literature.", "labels": [], "entities": []}, {"text": "We released our code for public use 1 . Baseline Models.", "labels": [], "entities": []}, {"text": "We compare Neural Sieves to the state-of-the-art models from the literature.", "labels": [], "entities": [{"text": "Neural Sieves", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.8134185969829559}]}, {"text": "LSTM + CNN -MIL score target object-context object pairs using LSTMs for processing the referring expression and CNN features for bounding boxes.", "labels": [], "entities": []}, {"text": "The pair with the highest score is predicted as the referred object.", "labels": [], "entities": []}, {"text": "They use Multi-Instance Learning for training the model.", "labels": [], "entities": []}, {"text": "CMN () is a neural module network with a tuple of object-relationship-subject nodes.", "labels": [], "entities": []}, {"text": "The text encoding of tuples is calculated with a two-layer bi-directional LSTM and an attention mechanism () over the referring expression.", "labels": [], "entities": []}, {"text": "The referred object is in the top-2 candidates selected by Sieve I 71.2% of the time and in the top-3 predictions 86.6% of the time.", "labels": [], "entities": [{"text": "Sieve I", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.8243151307106018}]}, {"text": "Combining both sieves into a pipeline, these numbers further increase to 84.2% for top-2 predictions and to 95.3% for top-3 predictions.", "labels": [], "entities": []}, {"text": "Considering the simplicity of Neural Sieve approach, these are surprising results: two simple neural network systems, the first one ignoring the referring expression, the second predicting only object type, are able to reduce the number of candidate boxes down to 2 on 84.2% of instances.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for Shuffling Word Order for Referring", "labels": [], "entities": [{"text": "Shuffling Word Order", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.9007324973742167}, {"text": "Referring", "start_pos": 47, "end_pos": 56, "type": "TASK", "confidence": 0.8321195244789124}]}, {"text": " Table 3: Results with discarded referring expressions. Sur-", "labels": [], "entities": [{"text": "Sur-", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.8730537593364716}]}, {"text": " Table 4: Precision@k accuracies for Neural Sieves and state-", "labels": [], "entities": [{"text": "Precision@k accuracies", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.848169282078743}, {"text": "Neural Sieves", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.7350610494613647}]}]}