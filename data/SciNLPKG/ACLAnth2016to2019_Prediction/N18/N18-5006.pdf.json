{"title": [], "abstractContent": [{"text": "We present ClaimRank, an online system for detecting check-worthy claims.", "labels": [], "entities": []}, {"text": "While originally trained on political debates, the system can work for any kind of text, e.g., interviews or regular news articles.", "labels": [], "entities": []}, {"text": "Its aim is to facilitate manual fact-checking efforts by prioritizing the claims that fact-checkers should consider first.", "labels": [], "entities": []}, {"text": "ClaimRank supports both Arabic and English, it is trained on actual annotations from nine reputable fact-checking organizations (PolitiFact, FactCheck, ABC, CNN, NPR, NYT, Chicago Tribune, The Guardian, and Washington Post), and thus it can mimic the claim selection strategies for each and any of them, as well as for the union of them all.", "labels": [], "entities": [{"text": "NYT", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.9112849831581116}]}], "introductionContent": [{"text": "The proliferation of fake news demands the attention of both investigative journalists and scientists.", "labels": [], "entities": []}, {"text": "The need for automated fact-checking systems rises from the fact that manual fact-checking is both effort-and time-consuming.", "labels": [], "entities": []}, {"text": "The first step towards building an automated fact-checking system is to identify the claims that are worth factchecking.", "labels": [], "entities": []}, {"text": "We introduce ClaimRank, an automatic system to detect check-worthy claims in a given text.", "labels": [], "entities": []}, {"text": "ClaimRank is multilingual and at the moment it is available for both English and Arabic.", "labels": [], "entities": [{"text": "ClaimRank", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.891293466091156}]}, {"text": "To the best of our knowledge, it is the only such system available for Arabic.", "labels": [], "entities": []}, {"text": "ClaimRank is trained on actual annotations from nine reputable fact-checking organizations (PolitiFact, FactCheck, ABC, CNN, NPR, NYT, Chicago Tribune, The Guardian, and Washington Post), and thus it can be used to predict the claims by each of the individual sources, as well as their union.", "labels": [], "entities": [{"text": "NYT", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.893791139125824}]}, {"text": "This is the only system we are aware of that offers such a capability.", "labels": [], "entities": []}, {"text": "* Work conducted while this author was at QCRI.", "labels": [], "entities": [{"text": "QCRI", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9771754741668701}]}], "datasetContent": [{"text": "We train the system on five English political debates, and we test on two debates: either English or their Arabic translations.", "labels": [], "entities": []}, {"text": "Note that, compared to our original model), here we use more debates: seven instead of four.", "labels": [], "entities": []}, {"text": "Moreover, here we exclude some of the features, namely some debate-specific information (e.g., speaker, system messages), in order to be able to process any free text, and also discourse parse features, as we do not have a discourse parser for Arabic.", "labels": [], "entities": [{"text": "discourse parse", "start_pos": 177, "end_pos": 192, "type": "TASK", "confidence": 0.7210595309734344}]}, {"text": "One of the most important components of the system that we had to port across languages were the word embeddings.", "labels": [], "entities": []}, {"text": "We experimented with the following cross-language embeddings: -VecMap: we used a parallel English-Arabic corpus of TED talks 1 () to generate monolingual embeddings (Arabic and English) using word2vec ().", "labels": [], "entities": [{"text": "English-Arabic corpus of TED talks 1", "start_pos": 90, "end_pos": 126, "type": "DATASET", "confidence": 0.6888348261515299}]}, {"text": "Then we projected these embeddings into a joint vector space using VecMap ().", "labels": [], "entities": []}, {"text": "-MUSE embeddings: Ina similar fashion, we generated cross-language embeddings from the same TED talks using Facebook's supervised MUSE model ( to project the Arabic and the English monolingual embeddings into a joint vector space.", "labels": [], "entities": []}, {"text": "-Attract-Repel embeddings: we used the pretrained English-Arabic embeddings from AttractRepel . shows the system performance when predicting claims by any of the sources, using word2vec and the cross-language embeddings.", "labels": [], "entities": []}, {"text": "All results are well above a random baseline.", "labels": [], "entities": []}, {"text": "We can see some drop in MAP for English when using VecMap or MUSE, which is to be expected as the model needs to balance between preserving the original embeddings and projecting them into a joint space.", "labels": [], "entities": []}, {"text": "The Attract-Repel vectors perform better for English, which is probably due to the monolingual synonymy/antonymy constraints that they impose , thus yielding better vectors, even for English.", "labels": [], "entities": []}, {"text": "The overall MAP results for Arabic are competitive, compared to English.", "labels": [], "entities": [{"text": "MAP", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.675147533416748}]}, {"text": "The best model is MUSE, while Attract-Repel is way behind, probably because, unlike VecMap and MUSE, its word embeddings are trained on unsegmented Arabic, which causes severe data sparseness issues.: Performance when using different cross-language embeddings.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.6838783621788025}]}, {"text": "In the final system, we use MUSE vectors for both languages, which perform best overall: not only for MAP, but also P@20, and P@50, which are very important measures assuming that manual fact-checking can be done for up to 20 or up to 50 claims only (in fact, statistics show that eight out of our nine fact-checking organizations had no more than 50 claims checked per debate).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance when using different cross-language embeddings.", "labels": [], "entities": []}]}