{"title": [{"text": "Multinomial Adversarial Networks for Multi-Domain Text Classification", "labels": [], "entities": [{"text": "Multi-Domain Text Classification", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.677718311548233}]}], "abstractContent": [{"text": "Many text classification tasks are known to be highly domain-dependent.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 5, "end_pos": 30, "type": "TASK", "confidence": 0.8401890198389689}]}, {"text": "Unfortunately, the availability of training data can vary drastically across domains.", "labels": [], "entities": []}, {"text": "Worse still, for some domains there may not be any annotated data at all.", "labels": [], "entities": []}, {"text": "In this work, we propose a multino-mial adversarial network 1 (MAN) to tackle this real-world problem of multi-domain text classification (MDTC) in which labeled data may exist for multiple domains, but in insufficient amounts to train effective classifiers for one or more of the domains.", "labels": [], "entities": [{"text": "multi-domain text classification (MDTC)", "start_pos": 105, "end_pos": 144, "type": "TASK", "confidence": 0.8086627423763275}]}, {"text": "We provide theoretical justifications for the MAN framework, proving that different instances of MANs are essentially minimizers of various f-divergence metrics (Ali and Silvey, 1966) among multiple probability distributions.", "labels": [], "entities": []}, {"text": "MANs are thus a theoretically sound generalization of traditional adversarial networks that discriminate over two distributions.", "labels": [], "entities": [{"text": "MANs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9116811752319336}]}, {"text": "More specifically, for the MDTC task, MAN learns features that are invariant across multiple domains by resorting to its ability to reduce the divergence among the feature distributions of each domain.", "labels": [], "entities": [{"text": "MDTC task", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.8234145939350128}]}, {"text": "We present experimental results showing that MANs significantly outperform the prior art on the MDTC task.", "labels": [], "entities": [{"text": "MANs", "start_pos": 45, "end_pos": 49, "type": "TASK", "confidence": 0.946388840675354}]}, {"text": "We also show that MANs achieve state-of-the-art performance for domains with no labeled data.", "labels": [], "entities": [{"text": "MANs", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.9639833569526672}]}], "introductionContent": [{"text": "Text classification is one of the most fundamental tasks in Natural Language Processing, and has found its way into a wide spectrum of NLP applications, ranging from email spam detection and social media analytics to sentiment analysis and data mining.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7898397147655487}, {"text": "email spam detection", "start_pos": 166, "end_pos": 186, "type": "TASK", "confidence": 0.6285496552785238}, {"text": "sentiment analysis", "start_pos": 217, "end_pos": 235, "type": "TASK", "confidence": 0.955475240945816}, {"text": "data mining", "start_pos": 240, "end_pos": 251, "type": "TASK", "confidence": 0.81526780128479}]}, {"text": "Over the past couple of decades, supervised statistical learning methods have become the dominant approach for text classification (e.g.;;).", "labels": [], "entities": [{"text": "text classification", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.8256249725818634}]}, {"text": "Unfortunately, many text classification tasks are highly domain-dependent in that a text classifier trained using labeled data from one domain is likely to perform poorly on another.", "labels": [], "entities": [{"text": "text classification", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.772856742143631}]}, {"text": "In the task of sentiment classification, for example, the phrase \"runs fast\" is usually associated with positive sentiment in the sports domain; not so when a user is reviewing the battery of an electronic device.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.9095086753368378}]}, {"text": "In real applications, therefore, an adequate amount of training data from each domain of interest is typically required, and this is expensive to obtain.", "labels": [], "entities": []}, {"text": "Two major lines of work attempt to tackle this challenge: domain adaptation ( and multi-domain text classification (MDTC) (.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.8571873903274536}, {"text": "multi-domain text classification (MDTC)", "start_pos": 82, "end_pos": 121, "type": "TASK", "confidence": 0.7628898719946543}]}, {"text": "In domain adaptation, the assumption is that there is some domain with abundant training data (the source domain), and the goal is to utilize knowledge learned from the source domain to help perform classifications on another lower-resourced target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7553013265132904}]}, {"text": "The focus of this work, MDTC, instead simulates an arguably more realistic scenario, where labeled data may exist for multiple domains, but in insufficient amounts to train an effective classifier for one or more of the domains.", "labels": [], "entities": []}, {"text": "Worse still, some domains may have no labeled data at all.", "labels": [], "entities": []}, {"text": "The objective of MDTC is to leverage all the available resources in order to improve the system performance overall domains simultaneously.", "labels": [], "entities": []}, {"text": "One state-of-the-art system for MDTC, the CMSC system of, combines a classifier that is shared across all domains (for learning domain-invariant knowledge) with a set of classifiers, one per domain, each of which captures domain-specific text classification knowledge.", "labels": [], "entities": [{"text": "text classification knowledge", "start_pos": 238, "end_pos": 267, "type": "TASK", "confidence": 0.7853992382685343}]}, {"text": "This paradigm is sometimes known as the Shared-Private model.", "labels": [], "entities": []}, {"text": "CMSC, however, lacks an explicit mechanism to ensure that the shared classifier captures only domain-independent knowledge: the shared classifier may well also acquire some domainspecific features that are useful fora subset of the domains.", "labels": [], "entities": [{"text": "CMSC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9334456324577332}]}, {"text": "We hypothesize that better performance can be obtained if this constraint were explicitly enforced.", "labels": [], "entities": []}, {"text": "In this paper, we thus propose Multinomial Adversarial Networks (henceforth, MANs) for the task of multi-domain text classification.", "labels": [], "entities": [{"text": "multi-domain text classification", "start_pos": 99, "end_pos": 131, "type": "TASK", "confidence": 0.6690261960029602}]}, {"text": "In contrast to standard adversarial networks (, which serve as a tool for minimizing the divergence between two distributions (, MANs represent a family of theoretically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple probability distributions.", "labels": [], "entities": []}, {"text": "And just as binomial adversarial networks have been applied to numerous tasks (e.g. image generation (), domain adaptation (, crosslingual text classification), we anticipate that MANs will make a versatile machine learning framework with applications beyond the MDTC task studied in this work.", "labels": [], "entities": [{"text": "image generation", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.7583709359169006}, {"text": "domain adaptation", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7581530809402466}, {"text": "crosslingual text classification", "start_pos": 126, "end_pos": 158, "type": "TASK", "confidence": 0.6142281194527944}]}, {"text": "We introduce the MAN architecture in \u00a72 and prove in \u00a73 that it directly minimizes the (generalized) f-divergence among multiple distributions so that they are indistinguishable upon successful training.", "labels": [], "entities": [{"text": "MAN", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9033160209655762}]}, {"text": "Specifically for MDTC, MAN is used to overcome the aforementioned limitation in prior art where domain-specific features may sneak into the shared model.", "labels": [], "entities": [{"text": "MAN", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8538066148757935}]}, {"text": "This is accomplished by relying on MAN's power of minimizing the divergence among the feature distributions of each domain.", "labels": [], "entities": []}, {"text": "The high-level idea is that MAN will make the extracted feature distributions of each domain indistinguishable from one another, thus learning general features that are invariant across domains.", "labels": [], "entities": [{"text": "MAN", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.845798134803772}]}, {"text": "We then validate the effectiveness of MAN in experiments on two MDTC data sets.", "labels": [], "entities": [{"text": "MAN", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.96115642786026}, {"text": "MDTC data sets", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.9489627679189047}]}, {"text": "We find first that MAN significantly outperforms the stateof-the-art CMSC method ( on the widely used multi-domain Amazon review dataset, and does so without relying on external resources such as sentiment lexica ( \u00a74.1).", "labels": [], "entities": [{"text": "Amazon review dataset", "start_pos": 115, "end_pos": 136, "type": "DATASET", "confidence": 0.6106701592604319}]}, {"text": "When applied to the second dataset, FDU-MTL ( \u00a74.3), we obtain similar results: MAN achieves substantially higher accuracy than the previous top-performing method, ASP-MTL (.", "labels": [], "entities": [{"text": "FDU-MTL", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.6724085807800293}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9987990856170654}]}, {"text": "ASP-MTL is the first empirical attempt to use a multinomial adversarial network for multi-task learning, but is more restricted and can be viewed as a special case of MAN.", "labels": [], "entities": []}, {"text": "In addition, we provide the first theoretical guarantees for multinomial adversarial networks ( \u00a73).", "labels": [], "entities": []}, {"text": "Finally, while many MDTC methods such as CMSC require labeled data for each domain, MANs can be applied in cases where no labeled data exists fora subset of domains.", "labels": [], "entities": []}, {"text": "To evaluate MAN in this semi-supervised setting, we compare MAN to a method that can accommodate unlabeled data for (only) one domain (, and show that MAN achieves performance comparable to the state of the art ( \u00a74.2).", "labels": [], "entities": [{"text": "MAN", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.94012451171875}]}], "datasetContent": [{"text": "As CMSC requires labeled data for each domain, their experiments were naturally designed this way.", "labels": [], "entities": []}, {"text": "In reality, however, many domains may not  have any annotated corpora available.", "labels": [], "entities": []}, {"text": "It is therefore also important to look at the performance in these unlabeled domains fora MDTC system.", "labels": [], "entities": []}, {"text": "Fortunately, as depicted before, MAN's adversarial training only utilizes unlabeled data from each domain to learn the domain-invariant features, and can thus be used on unlabeled domains as well.", "labels": [], "entities": []}, {"text": "During testing, only the shared feature vector is fed into C, while the domain feature vector is set to 0.", "labels": [], "entities": []}, {"text": "In order to validate MAN's effectiveness, we compare to state-of-the-art multi-source domain adaptation (MS-DA) methods (see \u00a76).", "labels": [], "entities": [{"text": "MAN", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9847477078437805}, {"text": "multi-source domain adaptation", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.6686840454737345}]}, {"text": "Compared to standard domain adaptation methods with one source and one target domain, MS-DA allows the adaptation from multiple source domains to a single target domain.", "labels": [], "entities": []}, {"text": "Analogically, MDTC can be viewed as multi-source multi-target domain adaptation, which is superior when multiple target domains exist.", "labels": [], "entities": [{"text": "multi-source multi-target domain adaptation", "start_pos": 36, "end_pos": 79, "type": "TASK", "confidence": 0.6988773196935654}]}, {"text": "With multiple target domains, MS-DA will need to treat each one as an independent task, which is more expensive and cannot utilize the unlabeled data in other target domains.", "labels": [], "entities": []}, {"text": "In this work, we compare MAN with one recent MS-DA method, MDAN (.", "labels": [], "entities": [{"text": "MAN", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8463550806045532}]}, {"text": "Their experiments only have one target domain to suit their approach, and we follow this setting for fair comparison.", "labels": [], "entities": []}, {"text": "However, it is worth noting that MAN is designed for the MDTC setting, and can deal with multiple target domains at the same time, which can potentially improve the performance by taking advantage of more unlabeled data from multiple target domains during adversarial training.", "labels": [], "entities": [{"text": "MAN", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8337123990058899}]}, {"text": "We adopt the same setting as, which is based on the same multidomain Amazon review dataset.", "labels": [], "entities": [{"text": "Amazon review dataset", "start_pos": 69, "end_pos": 90, "type": "DATASET", "confidence": 0.7238928278287252}]}, {"text": "Each of the four domains in the dataset is treated as the target domain in four separate experiments, while the re-books elec.", "labels": [], "entities": []}, {"text": "dvd kitchen apparel camera health music toys video baby magaz. softw.", "labels": [], "entities": [{"text": "dvd kitchen apparel camera health music toys video baby magaz. softw", "start_pos": 0, "end_pos": 68, "type": "DATASET", "confidence": 0.7905891587336858}]}, {"text": "maining three are used as source domains.", "labels": [], "entities": []}, {"text": "To make fair comparisons, the previous experiments follow the standard settings in the literature, where the widely adopted Amazon review dataset is used.", "labels": [], "entities": [{"text": "Amazon review dataset", "start_pos": 124, "end_pos": 145, "type": "DATASET", "confidence": 0.8990762035051981}]}, {"text": "However, this dataset has a few limitations.", "labels": [], "entities": []}, {"text": "First, it has only four domains.", "labels": [], "entities": []}, {"text": "In addition, the reviews are already tokenized and converted to a bag of features consisting of unigrams and bigrams.", "labels": [], "entities": []}, {"text": "Raw review texts are hence not available in this dataset, making it impossible to use certain modern neural architectures such as CNNs and RNNs.", "labels": [], "entities": []}, {"text": "To provide more insights on how well MAN works with other feature extractor architectures, we provide a third set of experiments on the FDU-MTL dataset (.", "labels": [], "entities": [{"text": "MAN", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9025453925132751}, {"text": "FDU-MTL dataset", "start_pos": 136, "end_pos": 151, "type": "DATASET", "confidence": 0.9824346005916595}]}, {"text": "This dataset is created as a multi-task learning dataset with 16 tasks, where each task is essentially a different domain of reviews.", "labels": [], "entities": []}, {"text": "It has 14 Amazon domains: books, electronics, DVD, kitchen, apparel, camera, health, music, toys, video, baby, magazine, software, and sports, in addition to two movie review domains from the IMDb and the MR datasets.", "labels": [], "entities": [{"text": "MR datasets", "start_pos": 205, "end_pos": 216, "type": "DATASET", "confidence": 0.9498460590839386}]}, {"text": "Each domain has a development set of 200 samples, and a test set of 400 samples.", "labels": [], "entities": []}, {"text": "The amount of training and unlabeled data vary across domains but are roughly 1400 and 2000, respectively.", "labels": [], "entities": []}, {"text": "We compare MAN with ASP-MTL ( on this FDU-MTL dataset.", "labels": [], "entities": [{"text": "FDU-MTL dataset", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9304239749908447}]}, {"text": "ASP-MTL also adopts adversarial training for learning a shared feature space, and can be viewed as a special case of MAN that adopts the NLL loss (MAN-NLL) and chooses LSTM as their feature extractor.", "labels": [], "entities": []}, {"text": "In contrast, we found a CNN-based feature extractor) achieves much better accuracy while being \u21e0 10 times faster.", "labels": [], "entities": [{"text": "CNN-based feature extractor", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6950380802154541}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9991559982299805}]}, {"text": "Indeed, as shown in, with or without adversarial training, our CNN models outperform LSTM ones by a large margin.", "labels": [], "entities": []}, {"text": "When used in our MAN framework, we attain the state-of-the-art performance on every domain with a 88.4% overall accuracy, surpassing ASP-MTL by a significant margin of 2.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9988445043563843}]}, {"text": "We hypothesize the reason a LSTM performs much worse than a CNN is its lack of an attention mechanism.", "labels": [], "entities": []}, {"text": "In ASP-MTL, only the last hidden unit is taken as the extracted features.", "labels": [], "entities": []}, {"text": "While LSTMs are effective for representing the context for each token, it might not be powerful enough for directly encoding the entire document (.", "labels": [], "entities": []}, {"text": "Therefore, various attention mechanisms have been introduced on top of the vanilla LSTM to select words (and contexts) most relevant for making the predictions.", "labels": [], "entities": []}, {"text": "In our preliminary experiments, we find that a Bi-directional LSTM with the dot-product attention () yields better performance than the vanilla LSTM in ASP-MTL.", "labels": [], "entities": []}, {"text": "However, it still does not outperform CNN and is much slower.", "labels": [], "entities": [{"text": "CNN", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.7991381883621216}]}, {"text": "As a result, we conclude that, for text classification tasks, CNN is both effective and efficient in extracting local and higher-level features for making a single categorization.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.8469201326370239}]}, {"text": "Finally, we observe that MAN-NLL achieves slightly higher overall performance compared to MAN-L2, providing evidence for the claim in a recent study () that the original GAN loss (NLL) may not be inherently inferior to the L2 loss.", "labels": [], "entities": []}, {"text": "Moreover, the two variants excel in different domains, suggesting the possibility of further performance gain when using ensemble.", "labels": [], "entities": []}, {"text": "For our first two experiments on the Amazon review dataset, the MLP feature extractor is used.", "labels": [], "entities": [{"text": "Amazon review dataset", "start_pos": 37, "end_pos": 58, "type": "DATASET", "confidence": 0.9635564287503561}, {"text": "MLP feature extractor", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.5507959326108297}]}, {"text": "As described in \u00a74.1, it has an input size of 5000.", "labels": [], "entities": []}, {"text": "Two hidden layers are used, with size 1000 and 500, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MDTC results on the Amazon dataset. Mod- els in bold are ours while the performance of the rest is  taken from Wu and Huang (2015). Numbers in paren- theses indicate standard errors, calculated based on 5  runs. Bold numbers indicate the highest performance  in each domain, and \u21e4 shows statistical significance  (p < 0.05) over CMSC under a one-sample T-Test.", "labels": [], "entities": [{"text": "MDTC", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.6977338194847107}, {"text": "Amazon dataset", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.953548014163971}, {"text": "CMSC", "start_pos": 339, "end_pos": 343, "type": "DATASET", "confidence": 0.930497407913208}]}, {"text": " Table 2: Results on unlabeled domains. Models in bold  are our models while the rest is taken from Zhao et al.  (2017). Highest domain performance is shown in bold.", "labels": [], "entities": []}, {"text": " Table 3: Results on the FDU-MTL dataset. Bolded models are ours, while the rest is from Liu et al. (2017). High- est performance is each domain is highlighted. For our full MAN models, standard errors are shown in parenthese  and statistical significance (p < 0.01) over ASP-MTL is indicated by *.", "labels": [], "entities": [{"text": "FDU-MTL dataset", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.9749166965484619}]}]}