{"title": [{"text": "Modeling Noisiness to Recognize Named Entities using Multitask Neural Networks on Social Media", "labels": [], "entities": []}], "abstractContent": [{"text": "Recognizing named entities in a document is a key task in many NLP applications.", "labels": [], "entities": [{"text": "Recognizing named entities in a document", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.9105014204978943}]}, {"text": "Although current state-of-the-art approaches to this task reach a high performance on clean text (e.g. newswire genres), those algorithms dramatically degrade when they are moved to noisy environments such as social media domains.", "labels": [], "entities": []}, {"text": "We present two systems that address the challenges of processing social media data using character-level phonetics and phonology, word embeddings, and Part-of-Speech tags as features.", "labels": [], "entities": []}, {"text": "The first model is a multitask end-to-end Bidirectional Long Short-Term Memory (BLSTM)-Conditional Random Field (CRF) network whose output layer contains two CRF classifiers.", "labels": [], "entities": []}, {"text": "The second model uses a multi-task BLSTM network as feature extractor that transfers the learning to a CRF classifier for the final prediction.", "labels": [], "entities": []}, {"text": "Our systems outperform the current F1 scores of the state of the art on the Workshop on Noisy User-generated Text 2017 dataset by 2.45% and 3.69%, establishing a more suitable approach for social media environments.", "labels": [], "entities": [{"text": "F1", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9976116418838501}, {"text": "Noisy User-generated Text 2017 dataset", "start_pos": 88, "end_pos": 126, "type": "DATASET", "confidence": 0.6255734741687775}]}], "introductionContent": [{"text": "One of the core tasks in Natural Language Processing (NLP) is Named Entity Recognition (NER).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.7645493050416311}, {"text": "Named Entity Recognition (NER)", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.7613559067249298}]}, {"text": "NER is a sequence tagging task that consists in selecting the words that describe entities and recognizing their types (e.g., a person, location, company, etc.).", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8466123938560486}, {"text": "sequence tagging task", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.7603746851285299}]}, {"text": "shows examples of sentences from different domains that contain named entities.", "labels": [], "entities": []}, {"text": "Recognizing entities in running text is typically one of the first tasks in the pipeline of many NLP applications, including machine translation, summarization, sentiment analysis, and question answering.", "labels": [], "entities": [{"text": "Recognizing entities in running text", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8700643539428711}, {"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.8067974150180817}, {"text": "summarization", "start_pos": 146, "end_pos": 159, "type": "TASK", "confidence": 0.9787948131561279}, {"text": "sentiment analysis", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.9518329203128815}, {"text": "question answering", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.9069046378135681}]}, {"text": "Traditional machine learning systems have proven to be effective informal text, where grammatical errors are minimal and writers stick to the rules of the written language (;).", "labels": [], "entities": []}, {"text": "However, those traditional systems dramatically fail on informal text, where improper grammatical structures, spelling inconsistencies, and slang vocabulary prevail).", "labels": [], "entities": []}, {"text": "For instance, shows a snapshot of NER systems' performance during the last years, where the results drop from 96.49% to 41.86% on the F1 metric as we move from formal to informal text.", "labels": [], "entities": [{"text": "F1", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.9903606176376343}]}, {"text": "Although the results are not directly comparable because they consider different conditions and challenges, they serve as strong evidence that the NER task in social media is far from being solved.", "labels": [], "entities": [{"text": "NER task", "start_pos": 147, "end_pos": 155, "type": "TASK", "confidence": 0.9208315312862396}]}, {"text": "Recently, researchers have approached NER using different neural network architectures.", "labels": [], "entities": [{"text": "NER", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9555290937423706}]}, {"text": "For instance, proposed a neural model using Convolutional Neural Networks (CNN) for characters and a bidirectional Long Short Term Memory (LSTM) for words.", "labels": [], "entities": []}, {"text": "Their model learned from word embeddings, capitalization, and lexicon features.", "labels": [], "entities": []}, {"text": "On a slightly different approach,  moving the dependencies on external resources.", "labels": [], "entities": []}, {"text": "Moreover, proposed an end-to-end BLSTM-CNN-CRF network, whose loss function is based on the maximum loglikelihood estimation of the CRF.", "labels": [], "entities": []}, {"text": "These architectures were benchmarked on the standard CoNLL 2003 dataset.", "labels": [], "entities": [{"text": "CoNLL 2003 dataset", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.9698352813720703}]}, {"text": "Although most of the work has focused on formal datasets, similar approaches have been evaluated on SM domains ().", "labels": [], "entities": []}, {"text": "In the Workshop on Noisy User-generated Text (WNUT) 2016,, the winners of the NER shared task, used a BLSTM-CRF model that induced features from an orthographic representation of the text.", "labels": [], "entities": [{"text": "NER shared task", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.6240010261535645}, {"text": "BLSTM-CRF", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9241043329238892}]}, {"text": "Later, in the WNUT 2017 shared task, the best performing system used a multitask network that transferred the learning to a CRF classifier for the final prediction (.", "labels": [], "entities": [{"text": "WNUT 2017 shared task", "start_pos": 14, "end_pos": 35, "type": "DATASET", "confidence": 0.8411432057619095}]}, {"text": "In this work we focus on addressing the challenges of the NER task found in social media environments.", "labels": [], "entities": [{"text": "NER task", "start_pos": 58, "end_pos": 66, "type": "TASK", "confidence": 0.9329478442668915}]}, {"text": "We propose that what is traditionally categorized as noise (i.e., misspellings, inconsistent orthography, emerging abbreviations, and slang) should be modeled as is since it is an inherent characteristic of SM text.", "labels": [], "entities": [{"text": "SM text", "start_pos": 207, "end_pos": 214, "type": "TASK", "confidence": 0.8954345881938934}]}, {"text": "Specifically, the proposed models attempt to address i) misspellings using subword level representations, ii) grammatical mistakes with SM-oriented Part-ofSpeech tags (), iii) sounddriven text with phonetic and phonological features ( , and iv) the intrinsic skewness of NER datasets by applying class weights.", "labels": [], "entities": [{"text": "NER datasets", "start_pos": 271, "end_pos": 283, "type": "DATASET", "confidence": 0.7361653745174408}]}, {"text": "It is worth noting that our models do not rely on capitalization or any external resources such as gazetteers.", "labels": [], "entities": []}, {"text": "The reasons are that capitalization is arbitrarily used on SM environments, and gazetteers are expensive resources to develop fora scenario where novel entities constantly and rapidly emerge).", "labels": [], "entities": []}, {"text": "Based on our experiments, we have seen that a multitask variation of the proposed networks improves the results over a single-task network.", "labels": [], "entities": []}, {"text": "Additionally, this multitask version, paired with phonetic and phonological features, outperforms previous state-of-the-art results on the WNUT 2017 dataset, and the same models obtain reasonable results with respect to the state of the art on the CoNLL 2003 dataset.", "labels": [], "entities": [{"text": "WNUT 2017 dataset", "start_pos": 139, "end_pos": 156, "type": "DATASET", "confidence": 0.9628487825393677}, {"text": "CoNLL 2003 dataset", "start_pos": 248, "end_pos": 266, "type": "DATASET", "confidence": 0.9757412473360697}]}, {"text": "The rest of the paper is organized as follows: \u00a72 presents the proposed features, the formal description of the models, and the implementation details.", "labels": [], "entities": []}, {"text": "\u00a73 describes the datasets and their challenges.", "labels": [], "entities": []}, {"text": "On \u00a74, we show the evaluation process of our models and the results.", "labels": [], "entities": []}, {"text": "We explain the performance of the models on \u00a75.", "labels": [], "entities": []}, {"text": "\u00a76 describes related work and, finally, we draw conclusions on \u00a77.", "labels": [], "entities": []}], "datasetContent": [{"text": "Social media (SM) captures the fast evolving behavior of the language, and, as its influence in society grows, SM platforms play an important role in language understanding.", "labels": [], "entities": [{"text": "Social media (SM)", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.644071239233017}, {"text": "language understanding", "start_pos": 150, "end_pos": 172, "type": "TASK", "confidence": 0.7107262462377548}]}, {"text": "We focus this work on the WNUT 2017 dataset for NER . This dataset covers multiple SM platforms and suits perfectly the purpose of this work.", "labels": [], "entities": [{"text": "WNUT 2017 dataset", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.93616251150767}, {"text": "NER", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.47538459300994873}]}, {"text": "shows the distribution of the dataset and its classes.", "labels": [], "entities": []}, {"text": "The training set uses tweets, whereas the development set is based on YouTube comments.", "labels": [], "entities": []}, {"text": "year ago), whereas rare depicts the entities that appear less than certain number of times.", "labels": [], "entities": []}, {"text": "It is worth noting that this dataset presents a great challenge to systems that rely on external resources due to the rare and emerging properties.", "labels": [], "entities": []}, {"text": "We also consider the CoNLL 2003 dataset as it has been used as the standard dataset for NER benchmarks.", "labels": [], "entities": [{"text": "CoNLL 2003 dataset", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.9693801999092102}, {"text": "NER benchmarks", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.7917134165763855}]}, {"text": "However, we emphasize that both datasets present significantly different challenges and, thus, some relevant aspects in CoNLL 2003 may not be that relevant in the WNUT 2017 dataset.", "labels": [], "entities": [{"text": "CoNLL 2003", "start_pos": 120, "end_pos": 130, "type": "DATASET", "confidence": 0.9415948688983917}, {"text": "WNUT 2017 dataset", "start_pos": 163, "end_pos": 180, "type": "DATASET", "confidence": 0.9714761773745219}]}, {"text": "For example, capitalization is a crucial feature in newswire text, but it is less important in SM data since users tend to arbitrarily alter the character casing.", "labels": [], "entities": [{"text": "SM", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.966738224029541}]}, {"text": "Moreover, the target classes on the WNUT 2017 dataset cover the CoNLL 2003 classes plus fine-grained classes such as creativework (e.g., movie titles, T.V. shows, etc.), group (e.g., sports teams, music bands, etc.), and product.", "labels": [], "entities": [{"text": "WNUT 2017 dataset", "start_pos": 36, "end_pos": 53, "type": "DATASET", "confidence": 0.9709330797195435}, {"text": "CoNLL 2003 classes", "start_pos": 64, "end_pos": 82, "type": "DATASET", "confidence": 0.932679295539856}]}, {"text": "The additional classes are more heterogeneous, and thus, it makes the task more difficult to generalize.", "labels": [], "entities": []}, {"text": "Furthermore, shows the percentage of unique tokens of the WNUT 2017 dataset, which certainly shows a great diversity compared to the CoNLL 2003 dataset.", "labels": [], "entities": [{"text": "WNUT 2017 dataset", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.9734509189923605}, {"text": "CoNLL 2003 dataset", "start_pos": 133, "end_pos": 151, "type": "DATASET", "confidence": 0.9705423712730408}]}, {"text": "We mainly focus our experiments on the WNUT 2017 dataset.", "labels": [], "entities": [{"text": "WNUT 2017 dataset", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.9681811134020487}]}, {"text": "However, we consider relevant to compare our approach to the standard CoNLL 2003 dataset where current state-of-the-art systems are benchmarked.", "labels": [], "entities": [{"text": "CoNLL 2003 dataset", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.9722605148951212}]}, {"text": "This section addresses the experiments and results of both datasets.", "labels": [], "entities": []}, {"text": "In this section we discuss the experiments of the proposed approaches.", "labels": [], "entities": []}, {"text": "We compare our models and describe the contribution of each component of the stacked system.", "labels": [], "entities": []}, {"text": "Additionally, we compare our results against the state of the art in the WNUT 2017 dataset.", "labels": [], "entities": [{"text": "WNUT 2017 dataset", "start_pos": 73, "end_pos": 90, "type": "DATASET", "confidence": 0.9602718949317932}]}, {"text": "shows that the stacked system has a lower precision than the end-to-end model, but its recall is the highest.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.999146580696106}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9998199343681335}]}, {"text": "This means that the stacked model is slightly better at generalizing than the other models since it can detect a more diverse set of entities.", "labels": [], "entities": []}, {"text": "The surface form F1 metric  supports that intuition as well.", "labels": [], "entities": [{"text": "F1", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9807182550430298}]}, {"text": "It assigns a better F1 score to the stacked system (43.90%) than to the end-to-end model (42.79%) because the former finds more rare and emerging entities than the latter.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9874551296234131}]}, {"text": "Moreover, also shows that the precision of the end-to-end model is higher than the rest of the systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9996331930160522}]}, {"text": "This tends to capture the most frequent entities and leave behind the rare ones, which explains the different behaviors between the precision and recall of both models.", "labels": [], "entities": [{"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9989776611328125}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9856433868408203}]}, {"text": "The feature extractor contains a category task that can produce predictions of the test set.", "labels": [], "entities": []}, {"text": "We explored predicting the final labels with the feature extractor and compared the results against the predictions of the CRF classifier.", "labels": [], "entities": []}, {"text": "We noticed that the CRF always outperformed the network.", "labels": [], "entities": []}, {"text": "For the best scores the feature extractor achieved 40.64% whereas the CRF reached 45.55%.", "labels": [], "entities": [{"text": "CRF", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.5197360515594482}]}, {"text": "This is consistent with previous research) in that the individual output probabilities of the network do not consider the whole sequence, and thus, a sequential algorithm such as a CRF can improve the results by learning global constraints (i.e., the B-person cannot be followed by I-corporation).", "labels": [], "entities": []}, {"text": "We explored the contribution of the features and different aspects of our models.", "labels": [], "entities": []}, {"text": "For instance, we tried a BLSTM network using pretrained word embeddings only.", "labels": [], "entities": []}, {"text": "The re-: The class-level and overall results of our systems on the WNUT 2017 dataset.", "labels": [], "entities": [{"text": "WNUT 2017 dataset", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.9645307660102844}]}, {"text": "WNUT represents the winning system of the shared task (UH-RiTUAL), E2E is the end-to-end model, and Stacked shows the results of the stacked model.", "labels": [], "entities": [{"text": "WNUT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6929593086242676}]}, {"text": "Both systems considerably outperform the state-of-the-art results.", "labels": [], "entities": []}, {"text": "Between the end-to-end and the stacked models, the former gets better overall precision while the latter stands out on recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9988011121749878}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9980048537254333}]}, {"text": "sults of this model set our baseline on a 39.78% F1-score (see).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9980230331420898}]}, {"text": "This score is considerably close to the state-of-the-art performance, but improvements beyond that are small.", "labels": [], "entities": []}, {"text": "For instance, shows an ablation experiment using the stacked model.", "labels": [], "entities": []}, {"text": "The ablation reveals that weighting the classes is the most influential factor, which accounts fora 2.58% of F1 score improvement.", "labels": [], "entities": [{"text": "ablation", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9752160906791687}, {"text": "F1 score improvement", "start_pos": 109, "end_pos": 129, "type": "METRIC", "confidence": 0.9768079916636149}]}, {"text": "This aligns with the fact that the data is highly skewed, and thus, the model should pay more attention to the less frequent classes.", "labels": [], "entities": []}, {"text": "The second most important aspect is the POS tags, which enhance the results by 1.10%.", "labels": [], "entities": [{"text": "POS tags", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.5096603333950043}]}, {"text": "This improvement suggests that POS tags are important whether the dataset is from a noisy environment or not since other researchers have found positive effects by using this feature on formal text (.", "labels": [], "entities": []}, {"text": "Almost equally influential are the phonetic and phonological features that push the F1 score by 0.93%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9894490540027618}]}, {"text": "According to the ablation experiment, using phonetic and phonology along with the pretrained word embeddings and POS tags can reach an F1 measure of 41.81%, which is a very similar result to the state-of-the-art score, but with a simpler and more suitable model for SM environments (i.e., without gazetteers or capitalization).", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.9928153455257416}]}, {"text": "We also benchmarked our approach on a standard CoNLL 2003 dataset for the NER task.", "labels": [], "entities": [{"text": "CoNLL 2003 dataset", "start_pos": 47, "end_pos": 65, "type": "DATASET", "confidence": 0.9520796537399292}, {"text": "NER task", "start_pos": 74, "end_pos": 82, "type": "TASK", "confidence": 0.9368292987346649}]}, {"text": "The stacked model reached 89.01% while the end-toend model achieved 88.98% on the F1 metric.", "labels": [], "entities": [{"text": "F1", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9830575585365295}]}, {"text": "Although the state-of-the-art performance is 91.21% (Ma and Hovy, 2016), our approach targets SM domains and, consequently, our models disregard some of the important aspects on formal text while still getting reasonable results.", "labels": [], "entities": []}, {"text": "For instance, input the text to their model as is, which indirectly introduce capitalization to the morphological analysis at the character level.", "labels": [], "entities": []}, {"text": "This aspect becomes relevant in this dataset because entities are usually capitalized on formal text.", "labels": [], "entities": []}, {"text": "As explained before, our models do not rely on capitalization because the characters are represented by the International Phonetic Alphabet, which does not differentiate between lower and upper cases.", "labels": [], "entities": [{"text": "International Phonetic Alphabet", "start_pos": 108, "end_pos": 139, "type": "DATASET", "confidence": 0.9020072221755981}]}, {"text": "shows some predictions of our stacked model on the WNUT 2017 test set.", "labels": [], "entities": [{"text": "WNUT 2017 test set", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.9760151952505112}]}, {"text": "In example number 1, the model is able to correctly label Srinagar as person, even though the model does not rely on gazetteers or capitalization.", "labels": [], "entities": []}, {"text": "It is also important to mention that the word was not in the training or development set, which means that the network had to infer the entity purely from the context.", "labels": [], "entities": []}, {"text": "Moreover, the second example shows that the model has problems to determine whether the article the belongs to an NE or not.", "labels": [], "entities": []}, {"text": "This is an ambiguous problem that even humans struggle with.", "labels": [], "entities": []}, {"text": "This example also has a variation on spelling for the words Defence and Organisation.", "labels": [], "entities": [{"text": "spelling", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9498889446258545}, {"text": "Defence", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.8044752478599548}]}, {"text": "We suspect that the mitigation of OOV words using the FastText library helped in this case.", "labels": [], "entities": [{"text": "FastText library", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.913293182849884}]}, {"text": "Also, from the phonetic perspective, the model treated the word Defence as if it was the word Defense because both words map to the same IPA sequence, /dIfEns/.", "labels": [], "entities": [{"text": "Defense", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.7978790998458862}]}, {"text": "In the third case, the model is notable to identify the NE Scout, even though the context makes it fairly easy.", "labels": [], "entities": [{"text": "NE Scout", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.8275093734264374}]}], "tableCaptions": [{"text": " Table 1: Results on different NER shared tasks. The performance degrades as the systems are moved to social  media (SM) environments. The last row considers multiple SM domains, such as Twitter, YouTube, Reddit, and  StackExchange.", "labels": [], "entities": [{"text": "NER shared tasks", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.8761367599169413}]}, {"text": " Table 3: Percentage of unique NEs in two benchmark  datasets, the one from CoNLL 2003 and the one used  in the 2017 shared task held by the WNUT workshop.", "labels": [], "entities": [{"text": "CoNLL 2003", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.8959123194217682}, {"text": "WNUT workshop", "start_pos": 141, "end_pos": 154, "type": "DATASET", "confidence": 0.8059377372264862}]}, {"text": " Table 4: General statistics of the WNUT 2017 dataset.  It is worth noting that the NE tokens account for less  than 10% on any dataset, which shows the inherent  skewness of the task.", "labels": [], "entities": [{"text": "WNUT 2017 dataset", "start_pos": 36, "end_pos": 53, "type": "DATASET", "confidence": 0.9214345216751099}]}, {"text": " Table 5: Classes and their frequency distribution on the  WNUT 2017 dataset.", "labels": [], "entities": [{"text": "WNUT 2017 dataset", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.9880704283714294}]}, {"text": " Table 6: The class-level and overall results of our systems on the WNUT 2017 dataset. WNUT represents the  winning system of the shared task (UH-RiTUAL), E2E is the end-to-end model, and Stacked shows the results of  the stacked model. Both systems considerably outperform the state-of-the-art results. Between the end-to-end and  the stacked models, the former gets better overall precision while the latter stands out on recall.", "labels": [], "entities": [{"text": "WNUT 2017 dataset", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.9626308083534241}, {"text": "precision", "start_pos": 383, "end_pos": 392, "type": "METRIC", "confidence": 0.9984721541404724}, {"text": "recall", "start_pos": 424, "end_pos": 430, "type": "METRIC", "confidence": 0.9972810745239258}]}]}