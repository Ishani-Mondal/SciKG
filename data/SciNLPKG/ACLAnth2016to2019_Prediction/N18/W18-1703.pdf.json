{"title": [{"text": "Multi-hop Inference for Sentence-level TextGraphs: How Challenging is Meaningfully Combining Information for Science Question Answering?", "labels": [], "entities": [{"text": "Science Question Answering", "start_pos": 109, "end_pos": 135, "type": "TASK", "confidence": 0.6775286396344503}]}], "abstractContent": [{"text": "Question Answering for complex questions is often modelled as a graph construction or traversal task, where a solver must build or traverse a graph of facts that answer and explain a given question.", "labels": [], "entities": [{"text": "Question Answering for complex questions", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8574203491210938}]}, {"text": "This \"multi-hop\" inference has been shown to be extremely challenging, with few models able to aggregate more than two facts before being overwhelmed by \"se-mantic drift\", or the tendency for long chains of facts to quickly drift off topic.", "labels": [], "entities": []}, {"text": "This is a major barrier to current inference models, as even elementary science questions require an average of 4 to 6 facts to answer and explain.", "labels": [], "entities": []}, {"text": "In this work we empirically characterize the difficulty of building or traversing a graph of sentences connected by lexical overlap, by evaluating chance sentence aggregation quality through 9,784 manually-annotated judgements across knowledge graphs built from three free-text corpora (including study guides and Simple Wikipedia).", "labels": [], "entities": [{"text": "Simple Wikipedia", "start_pos": 314, "end_pos": 330, "type": "DATASET", "confidence": 0.8358244597911835}]}, {"text": "We demonstrate semantic drift tends to be high and aggregation quality low, at between 0.04% and 3%, and highlight scenarios that maximize the likelihood of meaningfully combining information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering (QA) is a task where models must find answers to natural language questions, either by retrieving these answers from a corpus, or inferring them by some inference process.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9398497819900513}]}, {"text": "Retrieval methods model QA as an answer sentence selection task, where a solver must find a sentence or short continuous passage of text in a corpus that answers the question (, inter alia).", "labels": [], "entities": [{"text": "answer sentence selection task", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.7332715094089508}]}, {"text": "These methods often fall short for questions requiring complex inference, such as those in the science domain, where nearly 80% of even 4 th grade science exam questions require some form of causal, model-based, or otherwise com-\"a girl means a human girl\" \"humans are living organisms\" Girl \"a girl means a human girl\" \"humans are living organisms\" Girl AND \"eating is when an organism takes in nutrients in the form of food\" plex inference to answer and explain, and a single continuous passage of text rarely describes the reasoning required to move from question to correct answer.", "labels": [], "entities": []}, {"text": "In these cases, multiple sentences, often from different parts of a text, different documents, or different knowledge bases must be aggregated together to build a complete answer and explanation.", "labels": [], "entities": []}, {"text": "Aggregating knowledge to support inference and complex question answering is often framed as a graph construction or traversal problem (e.g., where the solver must find paths that link sentences that contain question terms with sentences that contain answer terms through some number of intermediate sentences (see).", "labels": [], "entities": [{"text": "question answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.778324693441391}]}, {"text": "In these knowledge graphs, nodes represent facts or single sentences, and edges between nodes represent some signal that the facts are interrelated, such as having lexical overlap.", "labels": [], "entities": []}, {"text": "Information aggregation or \"multi-hop\" graph traversal has been shown to be extremely challenging, with QA solvers generally showing only modest performance benefits when aggregating information, and diminishing returns as the amount of aggregation increases.", "labels": [], "entities": [{"text": "Information aggregation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7993118166923523}, {"text": "multi-hop\" graph traversal", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.6596755683422089}, {"text": "QA solvers", "start_pos": 104, "end_pos": 114, "type": "TASK", "confidence": 0.766597330570221}]}, {"text": "In the elementary sci-ence domain, current estimates suggest that an average of 4 to 6 sentences are required to answer and explain a given question, while recent QA solvers generally struggle to meaningfully aggregate more than two freetext sentences, even when using alternate representations including semistructured tables () or graphs of words or syntactic dependencies traversed using monolingual alignment or PageRank variants in open-domain QA (.", "labels": [], "entities": [{"text": "QA solvers", "start_pos": 163, "end_pos": 173, "type": "TASK", "confidence": 0.7830173969268799}]}, {"text": "suggest these performance limitations are due to \"semantic drift\", whereas the number of sentences being aggregated increases, so do the chances of making a misstep in the aggregation -for example, aggregating a sentence about seed funding fora company when making an inference about the stages of plant growth.", "labels": [], "entities": []}, {"text": "This appears to occur across a variety of solvers, representations, and methods for aggregation, and is leading to both the development of datasets specifically designed for multi-hop QA (, as well as methods of controlling for semantic drift in knowledge graphs constructed from (for example) OpenIE triples using either support graphs ( or drift-sensitive random walks (.", "labels": [], "entities": []}, {"text": "In an effort to better understand the challenges of inference and explanation construction for QA, here we characterize the difficultly of the information aggregation task in the context of science exams.", "labels": [], "entities": [{"text": "explanation construction", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.7379313707351685}, {"text": "information aggregation task", "start_pos": 143, "end_pos": 171, "type": "TASK", "confidence": 0.7440582712491354}]}, {"text": "The contributions of this work are: 1.", "labels": [], "entities": []}, {"text": "We provide the first empirical characterization of the difficulty of information aggregation by manually evaluating sentence aggregation quality using 9,784 annotated judgements across 14 representative exam questions, highlighting specific patterns of lexical overlap between question, answer, and candidate sentence that maximize the chances of successful aggregation.", "labels": [], "entities": [{"text": "information aggregation", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.7131670117378235}]}, {"text": "2. We evaluate aggregation difficulty across three knowledge resources, and empirically demonstrate that while moving to open domain resources increases knowledge coverage, it also increases the difficulty of the information aggregation task by more than an order of magnitude.", "labels": [], "entities": []}, {"text": "3. We evaluate aggregating up to three sentences that connect terms in the question to terms in the answer, and show that this suffers both from sparsity (even on Wikipediascale corpora), as well as a very low probability of producing meaningful aggregations (0.04% to 3%) through lexical overlap alone.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Observed frequencies for sentences with given utility ratings for the three categories of direct (lexical  overlap) connections: Q \u2194 S Q , S A \u2194 A, and Q \u2194 S QA \u2194 A, and various degrees of lexical overlap.", "labels": [], "entities": []}, {"text": " Table 3: Observed frequencies for aggregating two  sentences together with specific utility ratings in the  Q \u2194 S Q \u2194 S A \u2194 A condition across each corpus.", "labels": [], "entities": []}]}