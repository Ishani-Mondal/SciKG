{"title": [{"text": "Exploiting Dynamic Oracles to Train Projective Dependency Parsers on Non-Projective Trees", "labels": [], "entities": []}], "abstractContent": [{"text": "Because the most common transition systems are projective, training a transition-based dependency parser often implies to either ignore or rewrite the non-projective training examples , which has an adverse impact on accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.9970356225967407}]}, {"text": "In this work, we propose a simple modification of dynamic oracles, which enables the use of non-projective data when training projective parsers.", "labels": [], "entities": []}, {"text": "Evaluation on 73 tree-banks shows that our method achieves significant gains (+2 to +7 UAS for the most non-projective languages) and consistently outper-forms traditional projectivization and pseudo-projectivization approaches.", "labels": [], "entities": [{"text": "UAS", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9584178924560547}]}], "introductionContent": [{"text": "Because of their efficiency and ease of implementation, transition-based parsers are the most common systems for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8448010683059692}]}, {"text": "However, efficiency comes at a price, namely a loss in expressivity: while graph-based parsers are able to produce any tree spanning the input sentence, many transition-based systems are restricted to projective trees.", "labels": [], "entities": []}, {"text": "Informally, a dependency tree is non-projective if at least one dependency crosses another arc (see).", "labels": [], "entities": []}, {"text": "The inability to generate non-projective trees is an obvious issue for accuracy: attest time, a projective parser is guaranteed to be wrong for all the non-projective dependencies, a limitation already pointed out several times.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9956514239311218}]}, {"text": "In this paper, we show that the impact can also be severe at training time.", "labels": [], "entities": []}, {"text": "This is because the standard training procedure assumes that the reference tree is within reach of the parser, which is not the case for non-projective examples.", "labels": [], "entities": []}, {"text": "Therefore, projective parsers cannot make any use of such samples and common practice is to filter them out, thereby wasting potentially valuable training material.", "labels": [], "entities": []}, {"text": "Depending on the annotation schemes and languages, between 5 and 10% of the training set are typically discarded.", "labels": [], "entities": []}, {"text": "Several strategies have been proposed to overcome the projectivity constraint.", "labels": [], "entities": []}, {"text": "One line of research is to sacrifice parsing efficiency and introduce special transition systems capable to build non-projective dependencies.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9709039330482483}]}, {"text": "Another approach introduces nonprojective dependencies by post-processing the output projective trees.", "labels": [], "entities": []}, {"text": "This is the case of the pseudo-projectivization method), which encodes crossings in augmented relation labels and makes all examples projective.", "labels": [], "entities": []}, {"text": "The accuracy on projective dependencies alone can also be maximized by projectivizing all training examples prior to training, using Eisner (1996)'s decoder.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999110996723175}]}, {"text": "In this work, we propose an alternative strategy: we show ( \u00a73) that it is possible, with a small modification of the dynamic oracle of, to directly train a projective parser with non-projective examples.", "labels": [], "entities": []}, {"text": "While our approach remains unable to produce non-projective trees, it still results in significant improvements on the overall UAS ( \u00a74), and consistently outperforms the (pseudo-)projectivization approaches.", "labels": [], "entities": [{"text": "UAS", "start_pos": 127, "end_pos": 130, "type": "DATASET", "confidence": 0.8832550644874573}]}], "datasetContent": [{"text": "The benefits of non-projective examples for training projective parsers are evaluated on the 73 treebanks of the UD 2.0 (Nivre et al., 2017b,a).", "labels": [], "entities": [{"text": "UD 2.0", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.9165966212749481}]}, {"text": "Three methods to exploit non-projective trees (instead of discarding them) are contrasted: learning on the trees projectivized using Eisner (1996)'s algorithm, learning on pseudo-projectivized examples) and learning on the nonprojective trees, with the minimum-cost oracle described in \u00a73.", "labels": [], "entities": []}, {"text": "Projectivization is based on Yoav Goldberg's code.", "labels": [], "entities": []}, {"text": "For pseudo-projectivization, the MALTPARSER 1.9 implementation is used, with the head encoding scheme.", "labels": [], "entities": [{"text": "MALTPARSER 1.9", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.7729148268699646}]}, {"text": "For parsing, we use PANPARSER, our own open source 5 implementation of a greedy ARCEAGER parser (using an averaged perceptron and a dynamic oracle).", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9843336939811707}, {"text": "PANPARSER", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.8686205148696899}]}, {"text": "As shown in, it is empirically better to handle non-projective sentences with minimumcost dynamic oracles than to discard them all; but this strategy also outperforms projectivization and pseudo-projectivization.", "labels": [], "entities": []}, {"text": "As expected, the gains of all methods increase when the proportion of nonprojectivity increases, i.e. when more examples would have been discarded.", "labels": [], "entities": []}, {"text": "The minimum-cost technique is notably effective on the least projective treebanks, which correspond to ancient languages like Ancient Greek (63% of non-projective examples, with again of +2.4 UAS, compared to +1.1 and +2.0 for projectivization and pseudo-projectivization) and Latin (41%, +2.0 vs +0.4/+2.8); but it also achieves large improvements for modern languages with less non-projectivity, such as Dutch-LassySmall (30%, +7.0 vs +5.7/+3.3), Belarusian (17%, +5.2 vs +2.2/+4.4) and Turkish (11%, +2.3 vs +1.9/+1.3).", "labels": [], "entities": []}, {"text": "Apart from higher gains on average, the advantage of the minimum-cost strategy is that it is consistently beneficial, whereas pseudoprojectivization is detrimental for small treebanks.", "labels": [], "entities": []}, {"text": "A plausible explanation is that arbitrarily rewriting the trees introduces inconsistencies in the training material, which are only alleviated when data is large enough.", "labels": [], "entities": []}, {"text": "In that regard, the opposite effects of projectivization (detrimental with a static oracle, beneficial with a dynamic one) highlight the limited reliability of such transformations.", "labels": [], "entities": []}, {"text": "The minimum-cost strategy is also applied to an improved version of PANPARSER, using beam search and a dynamic oracle extended to global training (, with abeam of size 8 and the max-violation strategy.", "labels": [], "entities": [{"text": "PANPARSER", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.6175039410591125}]}, {"text": "The minimum-cost criterion appears particularly fit for that setting, with even larger gains (+0.63 UAS on average) despite a higher baseline.", "labels": [], "entities": [{"text": "UAS", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9951636791229248}]}, {"text": "Comparison with other parsers For illustrative purposes, similar experiments are conducted with other parsing systems: the ARCHYBRID version of PANPARSER, MALTPARSER and UDPIPE.", "labels": [], "entities": [{"text": "UDPIPE", "start_pos": 170, "end_pos": 176, "type": "DATASET", "confidence": 0.8532605171203613}]}, {"text": "MALTPARSER is the original implementation of the ARCEAGER system, but differs from ours in several ways, notably feature templates and the oracle (which is not dynamic, but precomputed statically); to help comparison, additional results are reported for PANPARSER without dynamic oracles.", "labels": [], "entities": []}, {"text": "UDPIPE is a state-of-the-art neural parser including both projective and non-projective parsing systems; we use version 1.1 () with Straka (2017)'s set of tuned hyperparameters, but without their pre-trained word embeddings, for fair comparison.", "labels": [], "entities": [{"text": "UDPIPE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.931829035282135}]}, {"text": "The ARCHYBRID results show that the gains achieved by the minimum-cost criterion are not specific to the ARCEAGER system: despite different baseline scores, the proposed strategy yields similar improvements.", "labels": [], "entities": [{"text": "ARCHYBRID", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.5648869872093201}]}, {"text": "Compared to MALTPARSER, our ARCEAGER baseline appears much stronger (+5.4 UAS) on the downsized datasets; but the gains achieved when exploiting the non-projective trees (with pseudoprojectivization) are similar in both implementations.", "labels": [], "entities": [{"text": "ARCEAGER baseline", "start_pos": 28, "end_pos": 45, "type": "METRIC", "confidence": 0.8999221324920654}, {"text": "UAS", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.995019793510437}]}, {"text": "There is one exception, Ancient Greek (the only treebank with more than 50% non-projective sentences), for which the MALTPARSER gains are way larger than those of PANPARSER; but this treebank seems particular in several regards and consequently does not question the superiority of the minimum-cost oracle over the pseudoprojectivization strategy, measured even in Ancient Greek for PANPARSER.", "labels": [], "entities": [{"text": "MALTPARSER", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.9800717830657959}]}, {"text": "also reports the gains achieved by MALTPARSER when pseudo-projectivization is followed by deprojectivization of the output.", "labels": [], "entities": [{"text": "MALTPARSER", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.53394615650177}]}, {"text": "Plain comparison of this line with the minimum-cost strategy is delicate, because it does not result from better training only, but also from again in expressivity: it is able to retrieve even nonprojective dependencies.", "labels": [], "entities": []}, {"text": "But it is interesting to see that deprojectivization only marginally improves over pseudo-projectivization alone: most of the gain actually resides in the treebank augmentation rather than in retrieving non-projective dependencies.", "labels": [], "entities": []}, {"text": "Besides, the minimum-cost strategy outperforms even the deprojectivized results.", "labels": [], "entities": []}, {"text": "Finally, measures with UDPIPE reveal that, even though it benefits a lot from its higher expressivity (as it uses non-projective systems for the most non-projective treebanks), it achieves low accuracies on small treebanks and is thus outperformed on average by the beam version of PAN-PARSER (+0.30 UAS) -and the minimum-cost criterion significantly widens that gap (+0.97 UAS).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison on Universal Dependencies 2.0 of various strategies to handle non-projective training exam- ples, depending on the non-projectivity rate and on treebank size. We report the average UAS over the correspond- ing sets of languages. All UAS gains are computed with respect to their 'only projective snt.' baseline.", "labels": [], "entities": []}]}