{"title": [{"text": "Recurrent Neural Networks as Weighted Language Recognizers", "labels": [], "entities": [{"text": "Recurrent Neural Networks as Weighted Language Recognizers", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.7598303811890739}]}], "abstractContent": [{"text": "We investigate the computational complexity of various problems for simple recurrent neu-ral networks (RNNs) as formal models for recognizing weighted languages.", "labels": [], "entities": []}, {"text": "We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications.", "labels": [], "entities": []}, {"text": "We show that most problems for such RNNs are undecidable, including consistency , equivalence, minimization, and the determination of the highest-weighted string.", "labels": [], "entities": [{"text": "consistency", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.9847645163536072}]}, {"text": "However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds.", "labels": [], "entities": []}, {"text": "If additionally the string is limited to polynomial length, the problem becomes NP-complete.", "labels": [], "entities": []}, {"text": "In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent neural networks (RNNs) are an attractive apparatus for probabilistic language modeling.", "labels": [], "entities": [{"text": "probabilistic language modeling", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.6206688582897186}]}, {"text": "Recent experiments show that RNNs significantly outperform other methods in assigning high probability to held-out English text (.", "labels": [], "entities": []}, {"text": "Roughly speaking, an RNN works as follows.", "labels": [], "entities": []}, {"text": "At each time step, it consumes one input token, updates its hidden state vector, and predicts the next token by generating a probability distribution overall permissible tokens.", "labels": [], "entities": []}, {"text": "The probability of an input string is simply obtained as the product of the predictions of the tokens constituting the string followed by a terminating token.", "labels": [], "entities": []}, {"text": "In this manner, each RNN defines a weighted language; i.e. a total function from strings to weights.", "labels": [], "entities": []}, {"text": "showed that single-layer rational-weight RNNs with saturated linear activation can compute any computable function.", "labels": [], "entities": []}, {"text": "To this end, a specific architecture with 886 hidden units can simulate any Turing machine in real-time (i.e., each Turing machine step is simulated in a single time step).", "labels": [], "entities": []}, {"text": "However, their RNN encodes the whole input in its internal state, performs the actual computation of the Turing machine when reading the terminating token, and then encodes the output (provided an output is produced) in a particular hidden unit.", "labels": [], "entities": []}, {"text": "In this way, their RNN allows \"thinking\" time (equivalent to the computation time of the Turing machine) after the input has been encoded.", "labels": [], "entities": []}, {"text": "We consider a different variant of RNNs that is commonly used in natural language processing applications.", "labels": [], "entities": []}, {"text": "It uses ReLU activations, consumes an input token at each time step, and produces softmax predictions for the next token.", "labels": [], "entities": []}, {"text": "It thus immediately halts after reading the last input token and the weight assigned to the input is simply the product of the input token predictions in each step.", "labels": [], "entities": []}, {"text": "Other formal models that are currently used to implement probabilistic language models such as finite-state automata and context-free grammars are by now well-understood.", "labels": [], "entities": []}, {"text": "A fair share of their utility directly derives from their nice algorithmic properties.", "labels": [], "entities": []}, {"text": "For example, the weighted languages computed by weighted finite-state automata are closed under intersection (pointwise product) and union (pointwise sum), and the corresponding unweighted languages are closed under intersection, union, difference, and complementation (.", "labels": [], "entities": []}, {"text": "Moreover, toolkits like OpenFST ( and Carmel 1 implement efficient algorithms on automata like minimization, intersection, finding the highestweighted path and the highest-weighted string.", "labels": [], "entities": [{"text": "OpenFST", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9226534962654114}]}, {"text": "RNN practitioners naturally face many of these same problems.", "labels": [], "entities": []}, {"text": "For example, an RNN-based machine translation system should extract the highest-weighted output string (i.e., the most likely translation) generated by an RNN, (.", "labels": [], "entities": [{"text": "RNN-based machine translation", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.6110123097896576}]}, {"text": "Currently this task is solved by approximation techniques like heuristic greedy and beam searches.", "labels": [], "entities": [{"text": "beam searches", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.7790704369544983}]}, {"text": "To facilitate the deployment of large RNNs onto limited memory devices (like mobile phones) minimization techniques would be beneficial.", "labels": [], "entities": []}, {"text": "Again currently only heuristic approaches like knowledge distillation are available.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.7987905442714691}]}, {"text": "Meanwhile, it is unclear whether we can determine if the computed weighted language is consistent; i.e., if it is a probability distribution on the set of all strings.", "labels": [], "entities": []}, {"text": "Without a determination of the overall probability mass assigned to all finite strings, a fair comparison of language models with regard to perplexity is simply impossible.", "labels": [], "entities": []}, {"text": "The goal of this paper is to study the above problems for the mentioned ReLU-variant of RNNs.", "labels": [], "entities": []}, {"text": "More specifically, we ask and answer the following questions: \u2022 Consistency: Do RNNs compute consistent weighted languages?", "labels": [], "entities": []}, {"text": "Is the consistency of the computed weighted language decidable?", "labels": [], "entities": [{"text": "consistency", "start_pos": 7, "end_pos": 18, "type": "METRIC", "confidence": 0.9573850631713867}]}, {"text": "\u2022 Highest-weighted string: Can we (efficiently) determine the highest-weighted string in a computed weighted language?", "labels": [], "entities": []}, {"text": "\u2022 Equivalence: Can we decide whether two given RNNs compute the same weighted language?", "labels": [], "entities": []}, {"text": "\u2022 Minimization: Can we minimize the number of neurons fora given RNN?", "labels": [], "entities": [{"text": "Minimization", "start_pos": 2, "end_pos": 14, "type": "TASK", "confidence": 0.9681193232536316}]}], "datasetContent": [], "tableCaptions": []}