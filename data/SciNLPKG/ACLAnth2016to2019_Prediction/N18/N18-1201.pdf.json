{"title": [{"text": "Stacking With Auxiliary Features for Visual Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.6949972808361053}]}], "abstractContent": [{"text": "Visual Question Answering (VQA) is a well-known and challenging task that requires systems to jointly reason about natural language and vision.", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7341744850079218}]}, {"text": "Deep learning models in various forms have been the standard for solving VQA.", "labels": [], "entities": [{"text": "solving VQA", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.6325135082006454}]}, {"text": "However, some of these VQA models are better at certain types of image-question pairs than other models.", "labels": [], "entities": []}, {"text": "Ensembling VQA models intelligently to leverage their diverse expertise is, therefore, advantageous.", "labels": [], "entities": []}, {"text": "Stacking With Auxiliary Features (SWAF) is an intelligent ensembling technique which learns to combine the results of multiple models using features of the current problem as context.", "labels": [], "entities": [{"text": "Stacking With Auxiliary Features (SWAF)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.628245873110635}]}, {"text": "We propose four categories of auxiliary features for ensembling for VQA.", "labels": [], "entities": [{"text": "VQA", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.7604936361312866}]}, {"text": "Three out of the four categories of features can be inferred from an image-question pair and do not require querying the component models.", "labels": [], "entities": []}, {"text": "The fourth category of auxiliary features uses model-specific explanations.", "labels": [], "entities": []}, {"text": "In this paper, we describe how we use these various categories of auxiliary features to improve performance for VQA.", "labels": [], "entities": []}, {"text": "Using SWAF to effectively ensemble three recent systems, we obtain anew state-of-the-art.", "labels": [], "entities": []}, {"text": "Our work also highlights the advantages of explainable AI models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Visual Question Answering (VQA), the task of addressing open-ended questions about images, has attracted significant attention in recent years.", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.75056325395902}]}, {"text": "Given an image and a natural language question about the image, the task is to provide an accurate natural language answer.", "labels": [], "entities": []}, {"text": "VQA requires visual and linguistic comprehension, language grounding as well as common-sense knowledge.", "labels": [], "entities": [{"text": "VQA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.771623432636261}]}, {"text": "A variety of methods to address these challenges have been developed in recent years ().", "labels": [], "entities": []}, {"text": "The vision component of atypical VQA system extracts visual features using a deep convolutional neural network (CNN), and the linguistic component encodes the question into a semantic vector using a recurrent neural network (RNN).", "labels": [], "entities": []}, {"text": "An answer is then generated conditioned on the visual features and the question vector.", "labels": [], "entities": []}, {"text": "Most VQA systems have a single underlying method that optimizes a specific loss function and do not leverage the advantage of using multiple diverse models.", "labels": [], "entities": []}, {"text": "One recent ensembling approach to VQA) combined multiple models that use multimodal compact bilinear pooling with attention and achieved state-of-the-art accuracy on the VQA 2016 challenge.", "labels": [], "entities": [{"text": "VQA", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7738658785820007}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9990228414535522}, {"text": "VQA 2016 challenge", "start_pos": 170, "end_pos": 188, "type": "DATASET", "confidence": 0.9446288545926412}]}, {"text": "However, their ensemble uses simple softmax averaging to combine outputs from multiple systems.", "labels": [], "entities": []}, {"text": "Also, their model is pre-trained on the Visual Genome dataset ( and they concatenate learned word embeddings with pre-trained GloVe vectors ().", "labels": [], "entities": [{"text": "Visual Genome dataset", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.7993906537691752}]}, {"text": "Several other deep and non-deep learning approaches for solving VQA have also been proposed (.", "labels": [], "entities": [{"text": "solving VQA", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.7072078883647919}]}, {"text": "Although these models perform fairly well on certain image-question (IQ) pairs, they fail spectacularly on certain other IQ pairs.", "labels": [], "entities": []}, {"text": "This led us to conclude that the various VQA models have learned to perform well on specific types of questions and images.", "labels": [], "entities": []}, {"text": "Therefore, there is an opportunity to combine these models intelligently so as to leverage their diverse strengths.", "labels": [], "entities": []}, {"text": "Ensembling multiple systems is a well known standard approach to improving accuracy in machine learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9974138140678406}]}, {"text": "Stacking with Auxiliary Features (SWAF)) is a recent ensembling algorithm that learns to combine outputs of multiple systems using fea- tures of the current problem as context.", "labels": [], "entities": []}, {"text": "In this paper, we use SWAF to more effectively combine several VQA models.", "labels": [], "entities": []}, {"text": "Traditional stacking trains a supervised meta-classifier to appropriately combine multiple system outputs.", "labels": [], "entities": []}, {"text": "SWAF further enables the stacker to exploit additional relevant knowledge of both the component systems and the problem by providing auxiliary features to the meta-classifier.", "labels": [], "entities": []}, {"text": "Our approach extracts features from the IQ pair under consideration, as well as the component models and provides this information to the classifier.", "labels": [], "entities": []}, {"text": "The metaclassifier then learns to predict whether a specific generated answer is corrector not.", "labels": [], "entities": []}, {"text": "Explanations attempt to justify a system's predicted output and provide context for their decision that may also help SWAF.", "labels": [], "entities": [{"text": "SWAF", "start_pos": 118, "end_pos": 122, "type": "TASK", "confidence": 0.9778746962547302}]}, {"text": "We extract visual explanations from various deep learning models and use those as auxiliary features for SWAF.", "labels": [], "entities": [{"text": "SWAF", "start_pos": 105, "end_pos": 109, "type": "TASK", "confidence": 0.9439638257026672}]}, {"text": "Our contributions can be summarized as follows: (a) developing novel auxiliary features that can be inferred from VQA questions and images; (b) extracting visual explanations from several component models for each IQ pair and using those to also generate auxiliary features; and (c) using SWAF to ensemble various VQA models and evaluating ablations of features while comparing our approach extensively to several individual as well as ensemble systems.", "labels": [], "entities": []}, {"text": "By effectively ensembling three leading VQA systems with SWAF, we demonstrate state-of-the-art performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present experimental results on the VQA challenge using the SWAF approach and compare it to various baselines, individual and ensemble VQA models, as well as ablations of our SWAF algorithm on the standard VQA test set.", "labels": [], "entities": [{"text": "VQA test set", "start_pos": 209, "end_pos": 221, "type": "DATASET", "confidence": 0.9479840795199076}]}, {"text": "In addition to the three data splits given in, the VQA challenge divides the test set into test-dev and test-standard.", "labels": [], "entities": [{"text": "VQA", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.8358932733535767}]}, {"text": "Evaluation on either split requires submitting the output to the competition's online server.", "labels": [], "entities": []}, {"text": "1 However, there are fewer restrictions on the number of submissions that can be made to the test-dev compared to the test-standard.", "labels": [], "entities": []}, {"text": "The test-dev is a subset of the standard test set consisting of randomly selected 60, 864 (25%) questions.", "labels": [], "entities": []}, {"text": "We use the test-dev set to tune the parameters of the meta-classifier.", "labels": [], "entities": []}, {"text": "All the individual VQA models that we ensemble are trained only on the VQA training set and the SWAF meta-classifier is trained on the VQA validation set.", "labels": [], "entities": [{"text": "VQA training set", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.9505030115445455}, {"text": "VQA validation set", "start_pos": 135, "end_pos": 153, "type": "DATASET", "confidence": 0.9205780029296875}]}, {"text": "For the meta-classifier, we use a L1-regularized SVM classifier for generic stacking and stacking with only question/answer types as auxiliary features.", "labels": [], "entities": []}, {"text": "For the question, image, and explanation features, we found that a neural network with two hidden layers works best.", "labels": [], "entities": []}, {"text": "The first hidden layer is fully connected and the second has approximately half the number of neurons as the first layer.", "labels": [], "entities": []}, {"text": "The question and image features are high-dimensional and therefore a neural network classifier worked: Accuracy results on the VQA test-standard set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9748932123184204}, {"text": "VQA test-standard set", "start_pos": 127, "end_pos": 148, "type": "DATASET", "confidence": 0.9488896330197653}]}, {"text": "The first block shows performance of a VQA model that use external data for pre-training, the second block shows single system VQA models, the third block shows an ensemble VQA model that also uses external data for pre-training, and the fourth block shows ensemble VQA models. well.", "labels": [], "entities": []}, {"text": "We found that using late fusion) to combine the auxiliary features for the neural network classifier worked slightly better.", "labels": [], "entities": []}, {"text": "We used Keras with Tensorflow back-end (Chollet, 2015) for implementing the network.", "labels": [], "entities": []}, {"text": "We compare our approach to a voting baseline that returns the answer with maximum agreement, with ties broken in the favor of systems with higher confidence scores.", "labels": [], "entities": []}, {"text": "We also compare against other state-of-the-art VQA systems not used in our en The iBowIMG concatenates the image features with the bag-of-word question embedding and feeds them into a softmax classifier to predict the answer, resulting in performance comparable to other models that use deep or recursive neural networks.", "labels": [], "entities": []}, {"text": "The iBowIMG beats most VQA models considered in their paper.", "labels": [], "entities": []}, {"text": "The DPPNet, on the other hand, learns a CNN with some parameters predicted from a separate parameter prediction network.", "labels": [], "entities": [{"text": "DPPNet", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.9441030025482178}]}, {"text": "Their parameter prediction network uses a Gated Recurrent Unit (GRU) to generate a question representation and maps the predicted weights to a CNN via hashing.", "labels": [], "entities": [{"text": "parameter prediction", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.738804966211319}]}, {"text": "The DPPNet uses external data (data-augmentation) in addition to the VQA dataset to pre-train the GRU.", "labels": [], "entities": [{"text": "DPPNet", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.9380218982696533}, {"text": "VQA dataset", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.9628517925739288}, {"text": "GRU", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.850037693977356}]}, {"text": "Another well-known VQA model is the Neural Module Network (NMN) that generates a neural network on the fly for each individual image and question.", "labels": [], "entities": []}, {"text": "This is done through choosing from various sub-modules based on the question and composing these to generate the neural network, e.g., the find[x] module outputs an attention map for detecting x.", "labels": [], "entities": []}, {"text": "To arrange the modules, the question is first parsed into a symbolic expression and using these expressions, modules are composed into a sequence to answer the query.", "labels": [], "entities": []}, {"text": "The whole system is trained end-to-end through backpropagation.", "labels": [], "entities": []}, {"text": "The VQA evaluation server, along with reporting accuracies on the full question set, also reports a break-down of accuracy across three answer categories.", "labels": [], "entities": [{"text": "VQA evaluation server", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.9252779682477316}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9982327222824097}]}, {"text": "The image-question (IQ) pairs that have answer type as \"yes/no\", those that have \"number\" as their answer type and finally those that do not belong to either of the first two categories are classified as \"other\".", "labels": [], "entities": []}, {"text": "shows the full and category-wise accuracies.", "labels": [], "entities": []}, {"text": "All scores for the stacking models were obtained using the VQA test-standard server.", "labels": [], "entities": [{"text": "stacking", "start_pos": 19, "end_pos": 27, "type": "TASK", "confidence": 0.9668547511100769}, {"text": "VQA test-standard server", "start_pos": 59, "end_pos": 83, "type": "DATASET", "confidence": 0.980059822400411}]}, {"text": "The table shows results for both single system and ensemble MCB models.", "labels": [], "entities": []}, {"text": "We used the single system MCB model as a component in our ensemble.", "labels": [], "entities": []}, {"text": "The ensemble MCB system, however, was the top-ranked system in the VQA 2016 challenge and it is pre-trained on the Visual Genome dataset ( as well as uses pre-trained GloVe vectors).", "labels": [], "entities": [{"text": "VQA 2016 challenge", "start_pos": 67, "end_pos": 85, "type": "DATASET", "confidence": 0.8839713533719381}, {"text": "Visual Genome dataset", "start_pos": 115, "end_pos": 136, "type": "DATASET", "confidence": 0.8373379707336426}]}, {"text": "On the other hand, our ensemble system does not use any external data and consists of only three component models.", "labels": [], "entities": []}, {"text": "The SWAF approach obtains anew state-of-theart result on the VQA task.", "labels": [], "entities": [{"text": "VQA task", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.7825455069541931}]}, {"text": "The vanilla stacking approach itself beats the best individual model and adding the auxiliary features further boosts the performance.", "labels": [], "entities": [{"text": "vanilla stacking", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7600011229515076}]}, {"text": "Our SWAF model that uses all three sets of auxiliary features related to IQ pairs does particularly well on the more difficult \"other\" answer category, indicating that the auxiliary features provide crucial information at classification time.", "labels": [], "entities": []}, {"text": "To further analyze the SWAF results, we performed experiments with ablations of the auxiliary features.", "labels": [], "entities": [{"text": "SWAF", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.7662404179573059}]}, {"text": "shows the results on the test-dev set obtained when ablating each of the auxiliary feature sets.", "labels": [], "entities": []}, {"text": "We observe that deleting the Q/A type decreased performance the most and deleting the explanation features decreased performance the least.", "labels": [], "entities": []}, {"text": "This indicates that the Q/A type features are the most informative and the explanation features are the least informative for deciding the correct answer.", "labels": [], "entities": [{"text": "Q/A type", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.7180353701114655}]}, {"text": "The voting baseline does not perform very well even though it is able to beat one of the component models.", "labels": [], "entities": []}, {"text": "The SWAF ablation results clearly indicate that there is an advantage to using each type of auxiliary feature.", "labels": [], "entities": [{"text": "SWAF ablation", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.6254296898841858}]}, {"text": "Each of the auxiliary feature sets contributes to the final ensemble's performance, which is clear from.", "labels": [], "entities": []}, {"text": "The voting and the \"vanilla stacking\" ensembles do not perform as well as SWAF.", "labels": [], "entities": [{"text": "vanilla stacking\"", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8622636000315348}, {"text": "SWAF", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.7648665308952332}]}, {"text": "This leads us to conclude that the performance gain is actually obtained from using the auxiliary features.", "labels": [], "entities": []}, {"text": "In particular, using explanations generated by various deep learning models as auxiliary features improved performance.", "labels": [], "entities": []}, {"text": "We observed that the localization-maps generated were fairly noisy, as is evident from.", "labels": [], "entities": []}, {"text": "Although the individual component systems agreed on an answer for many of the IQ pairs, the regions of the image they attend to varied significantly.", "labels": [], "entities": []}, {"text": "However, the rank correlation metric in the auxiliary features made the localization-maps useful for ensembling.", "labels": [], "entities": []}, {"text": "This is because, when training on the validation set, the stacker learns how to weight the auxiliary features, including those obtained using localization-maps.", "labels": [], "entities": []}, {"text": "In this way, it learns to trust only the localization-maps that are actually useful.", "labels": [], "entities": []}, {"text": "We also observed that there was a high positive correlation between the localization-maps generated by the HieCoAtt and MCB models, followed by the LSTM and MCB models, and then the LSTM and HieCoAtt models with several of the maps even negatively correlated between the last two models.", "labels": [], "entities": [{"text": "HieCoAtt", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.9216413497924805}]}, {"text": "We also experimented with using Earth Mover's Distance (EMD) to compare heat-maps and found that it worked even better than rank-order correlation; however, it came at a cost of high computational complexity (O(n 3 ) vs. O(n)).", "labels": [], "entities": []}, {"text": "shows the difference in performance obtained when explanation features calculated using either EMD or rank-order correlation are ablated from the final ensemble.", "labels": [], "entities": [{"text": "EMD", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.8292608857154846}]}, {"text": "Clearly, using EMD to compare explanation maps has more impact on the system's accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9966828227043152}]}, {"text": "Consistent with previous findings (, our results confirm that EMD provides a finer-grained comparison between localization maps.", "labels": [], "entities": []}, {"text": "Overall, our work shows that the utility of explanations is not limited to just developing human trust and making models more transparent.", "labels": [], "entities": []}, {"text": "Explanations can also be used to improve performance on a challenging task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: VQA dataset splits.", "labels": [], "entities": [{"text": "VQA dataset", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.8918847441673279}]}, {"text": " Table 2: Accuracy results on the VQA test-standard set. The first block shows performance of a VQA model that  use external data for pre-training, the second block shows single system VQA models, the third block shows an  ensemble VQA model that also uses external data for pre-training, and the fourth block shows ensemble VQA  models.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.98441481590271}, {"text": "VQA test-standard set", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.9311744372049967}]}]}