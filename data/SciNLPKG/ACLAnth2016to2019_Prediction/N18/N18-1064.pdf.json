{"title": [{"text": "Entity Commonsense Representation for Neural Abstractive Summarization", "labels": [], "entities": [{"text": "Neural Abstractive Summarization", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.6118819216887156}]}], "abstractContent": [{"text": "A major proportion of a text summary includes important entities found in the original text.", "labels": [], "entities": []}, {"text": "These entities buildup the topic of the summary.", "labels": [], "entities": []}, {"text": "Moreover, they hold commonsense information once they are linked to a knowledge base.", "labels": [], "entities": []}, {"text": "Based on these observations, this paper investigates the usage of linked entities to guide the decoder of a neural text summarizer to generate concise and better summaries.", "labels": [], "entities": []}, {"text": "To this end, we leverage on an off-the-shelf entity linking system (ELS) to extract linked entities and propose Entity2Topic (E2T), a module easily attachable to a sequence-to-sequence model that transforms a list of entities into a vector representation of the topic of the summary.", "labels": [], "entities": []}, {"text": "Current available ELS's are still not sufficiently effective, possibly introducing unre-solved ambiguities and irrelevant entities.", "labels": [], "entities": []}, {"text": "We resolve the imperfections of the ELS by (a) encoding entities with selective disambiguation, and (b) pooling entity vectors using firm attention.", "labels": [], "entities": [{"text": "ELS", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.8664478659629822}]}, {"text": "By applying E2T to a simple sequence-to-sequence model with attention mechanism as base model, we see significant improvements of the performance in the Gigaword (sentence to title) and CNN (long document to multi-sentence highlights) summarization datasets by at least 2 ROUGE points.", "labels": [], "entities": [{"text": "CNN (long document to multi-sentence highlights) summarization datasets", "start_pos": 186, "end_pos": 257, "type": "DATASET", "confidence": 0.4844845920801163}, {"text": "ROUGE", "start_pos": 272, "end_pos": 277, "type": "METRIC", "confidence": 0.9762297868728638}]}], "introductionContent": [{"text": "Text summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text.", "labels": [], "entities": [{"text": "Text summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.767945259809494}]}, {"text": "The task can be divided into two subtask based on the approach: extractive and abstractive summarization.", "labels": [], "entities": []}, {"text": "Extractive summarization is a task to create summaries by pulling out snippets of text form the original text and combining them to form a summary.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8555354177951813}]}, {"text": "Abstractive summarization asks to generate summaries from scratch without the restriction to use * Amplayo and Lim are co-first authors with equal contribution.", "labels": [], "entities": [{"text": "summarization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9762753844261169}]}, {"text": "the available words from the original text.", "labels": [], "entities": []}, {"text": "Due to the limitations of extractive summarization on incoherent texts and unnatural methodology (, the research trend has shifted towards abstractive summarization.", "labels": [], "entities": []}, {"text": "Sequence-to-sequence models ) with attention mechanism ( have found great success in generating abstractive summaries, both from a single sentence ( and from along document with multiple sentences.", "labels": [], "entities": []}, {"text": "However, when generating summaries, it is necessary to determine the main topic and to sift out unnecessary information that can be omitted.", "labels": [], "entities": [{"text": "summaries", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.9000855088233948}]}, {"text": "Sequenceto-sequence models have the tendency to include all the information, relevant or not, that are found in the original text.", "labels": [], "entities": []}, {"text": "This may result to unconcise summaries that concentrates wrongly on irrelevant topics.", "labels": [], "entities": []}, {"text": "The problem is especially severe when summarizing longer texts.", "labels": [], "entities": [{"text": "summarizing longer texts", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.9196207920710245}]}, {"text": "In this paper, we propose to use entities found in the original text to infer the summary topic, miti-gating the aforementioned problem.", "labels": [], "entities": []}, {"text": "Specifically, we leverage on linked entities extracted by employing a readily available entity linking system.", "labels": [], "entities": []}, {"text": "The importance of using linked entities in summarization is intuitive and can be explained by looking at as an example.", "labels": [], "entities": [{"text": "summarization", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9716805815696716}]}, {"text": "First (O1 in the, aside from auxiliary words to construct a sentence, a summary is mainly composed of linked entities extracted from the original text.", "labels": [], "entities": []}, {"text": "Second (O2), we can depict the main topic of the summary as a probability distribution of relevant entities from the list of entities.", "labels": [], "entities": []}, {"text": "Finally (O3), we can leverage on entity commonsense learned from a separate large knowledge base such as Wikipedia.", "labels": [], "entities": []}, {"text": "To this end, we present a method to effectively apply linked entities in sequence-tosequence models, called Entity2Topic (E2T).", "labels": [], "entities": []}, {"text": "E2T is a module that can be easily attached to any sequence-to-sequence based summarization model.", "labels": [], "entities": [{"text": "E2T", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8662341237068176}]}, {"text": "The module encodes the entities extracted from the original text by an entity linking system (ELS), constructs a vector representing the topic of the summary to be generated, and informs the decoder about the constructed topic vector.", "labels": [], "entities": []}, {"text": "Due to the imperfections of current ELS's, the extracted linked entities maybe too ambiguous and coarse to be considered relevant to the summary.", "labels": [], "entities": []}, {"text": "We solve this issue by using entity encoders with selective disambiguation and by constructing topic vectors using firm attention.", "labels": [], "entities": []}, {"text": "We experiment on two datasets, Gigaword and CNN, with varying lengths.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9164133667945862}, {"text": "CNN", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.8953434228897095}]}, {"text": "We show that applying our module to a sequence-to-sequence model with attention mechanism significantly increases its performance on both datasets.", "labels": [], "entities": []}, {"text": "Moreover, when compared with the state-of-the-art models for each dataset, the model obtains a comparable performance on the Gigaword dataset where the texts are short, and outperforms all competing models on the CNN dataset where the texts are longer.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 125, "end_pos": 141, "type": "DATASET", "confidence": 0.97981396317482}, {"text": "CNN dataset", "start_pos": 213, "end_pos": 224, "type": "DATASET", "confidence": 0.9684480428695679}]}, {"text": "Furthermore, we provide analysis on how our model effectively uses the extracted linked entities to produce concise and better summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 127, "end_pos": 136, "type": "TASK", "confidence": 0.9600427746772766}]}], "datasetContent": [{"text": "Gigaword Previous studies on the summarization tasks have only used entities in the preprocessing stage to anonymize the dataset ( and to mitigate out-of-vocabulary problems (.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.9287423193454742}]}, {"text": "Linked entities for summarization are still not properly explored and we are the first to use linked entities to improve the performance of the summarizer.", "labels": [], "entities": [{"text": "summarization", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.9865785241127014}]}, {"text": "Datasets We use two widely used summarization datasets with different text lengths.", "labels": [], "entities": []}, {"text": "First, we use the Annotated English Gigaword dataset as used in.", "labels": [], "entities": [{"text": "Annotated English Gigaword dataset", "start_pos": 18, "end_pos": 52, "type": "DATASET", "confidence": 0.8847122937440872}]}, {"text": "This dataset receives the first sentence of a news article as input and use the headline title as the gold standard summary.", "labels": [], "entities": []}, {"text": "Since the development dataset is large, we randomly selected 2000 pairs as our development dataset.", "labels": [], "entities": []}, {"text": "We use the same held-out test dataset used in () for comparison.", "labels": [], "entities": []}, {"text": "Second, we use the CNN dataset released in (.", "labels": [], "entities": [{"text": "CNN dataset", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.9427354633808136}]}, {"text": "This dataset receives the full news article as input and use the human-generated multiple sentence highlight as the gold standard summary.", "labels": [], "entities": []}, {"text": "The original dataset has been modified and preprocessed specifically for the document summarization task (.", "labels": [], "entities": [{"text": "document summarization task", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.7566524744033813}]}, {"text": "In addition to the previously provided datasets, we extract linked entities using Dexter, an open source ELS that links text snippets found in a given text to entities contained in Wikipedia.", "labels": [], "entities": []}, {"text": "We use the default recommended parameters stated in the website.", "labels": [], "entities": []}, {"text": "We summarize the statistics of both datasets in.", "labels": [], "entities": []}, {"text": "Implementation For both datasets, we further reduce the size of the input, output, and entity vocabularies to at most 50K as suggested in and replace less frequent words to \"<unk>\".", "labels": [], "entities": []}, {"text": "We use 300D Glove) and 1000D wiki2vec 7 pre-trained vectors to initialize our word and entity vectors.", "labels": [], "entities": []}, {"text": "For GRUs, we set the state size to 500.", "labels": [], "entities": []}, {"text": "For CNN, we set h = 3, 4, 5 with 400, 300, 300 feature maps, respectively.", "labels": [], "entities": []}, {"text": "For firm attention, k is tuned by calculating the perplexity of the model starting with smaller values (i.e. k = 1, 2, 5, 10, 20, ...) and stopping when the perplexity of the model becomes worse than the previous model.", "labels": [], "entities": []}, {"text": "Our preliminary tuning showed that k = 5 for Gigaword dataset and k = 10 for CNN dataset are the best choices.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9669836461544037}, {"text": "CNN dataset", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.9852118194103241}]}, {"text": "We use dropout () on all non-linear connections with a dropout rate of 0.5.", "labels": [], "entities": []}, {"text": "We set the batch sizes of Gigaword and CNN datasets to 80 and 10, respectively.", "labels": [], "entities": [{"text": "CNN datasets", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.8546300232410431}]}, {"text": "Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule, with l 2 constraint (Hinton et al., 2012) of 3.", "labels": [], "entities": [{"text": "Adadelta update rule", "start_pos": 85, "end_pos": 105, "type": "METRIC", "confidence": 0.6316004395484924}]}, {"text": "We perform early stopping using a subset of the given development dataset.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7285885214805603}]}, {"text": "We use beam search of size 10 to generate the summary.", "labels": [], "entities": []}, {"text": "Baselines For the Gigaword dataset, we compare our models with the following abstractive baselines: ABS+ () is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder, Feat2s) is an RNN sequence-to-sequence model with lexical and statistical features in the encoder, Luong-NMT () is a two-layer LSTM encoder-decoder model, RASElman () uses an attentive CNN encoder and an Elman RNN decoder, and SEASS ( uses BiGRU encoders and GRU decoders with selective encoding.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.9124996066093445}, {"text": "SEASS", "start_pos": 423, "end_pos": 428, "type": "METRIC", "confidence": 0.751380443572998}]}, {"text": "For the CNN dataset, we compare our models with the following extractive and abstractive baselines: Lead-3 is a strong baseline that extracts the first three sentences of the document as summary, LexRank extracts texts using), Bi-GRU is a non-hierarchical one-layer sequence-to-sequence abstractive baseline, Distraction-M3 () uses a sequence-to-sequence abstractive model with distraction-based networks, and GBA () is a graph-based attentional neural abstractive model.", "labels": [], "entities": [{"text": "CNN dataset", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.9447330236434937}, {"text": "Bi-GRU", "start_pos": 227, "end_pos": 233, "type": "METRIC", "confidence": 0.9047957062721252}, {"text": "GBA", "start_pos": 410, "end_pos": 413, "type": "METRIC", "confidence": 0.8150963187217712}]}, {"text": "All baseline results used beam search and are gathered from previous papers.", "labels": [], "entities": []}, {"text": "Also,   we compare our final model BASE+E2T with the base model BASE and some variants of our model (without selective disambiguation, using soft attention).", "labels": [], "entities": [{"text": "BASE", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.8667835593223572}, {"text": "BASE", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9364870190620422}]}, {"text": "Original western mexico @state @jalisco will host the first edition of the @UNK dollar @lorena ochoa invitation @golf tournament on nov.", "labels": [], "entities": [{"text": "UNK dollar @lorena ochoa invitation @golf tournament on nov", "start_pos": 76, "end_pos": 135, "type": "DATASET", "confidence": 0.8937961730090055}]}, {"text": "##-## #### , in @guadalajara @country club , the @lorena ochoa foundation said in a statement on wednesday . Baseline netanyahu says he is a country of \" UNK cheating \" and that it is a country of \" UNK cheating \" netanyahu says he is a country of \" UNK cheating \" and that \" is a very bad deal \" he says he says he says the plan is a country of \" UNK cheating \" and that it is a country of \" UNK cheating \" he says the u.s. is a country of \" UNK cheating \" and that is a country of \" UNK cheating \" Soft benjamin netanyahu : \" i think there 's a third alternative , and that is standing firm , \" netanyahu tells cnn . he says he does not rollback iran 's nuclear ambitions . \" it does not rollback iran 's nuclear program . \" Firm new : netanyahu : \" i think there 's a third alternative , and that is standing firm , \" netanyahu says . obama 's comments come as democrats and republicans spar over the framework announced last week to lift western sanctions on iran .: Examples from Gigaword and CNN datasets and corresponding summaries generated by competing models.", "labels": [], "entities": [{"text": "Gigaword and CNN datasets", "start_pos": 985, "end_pos": 1010, "type": "DATASET", "confidence": 0.7714471369981766}]}, {"text": "The tagged part of text is marked bold and preceded with at sign (@).", "labels": [], "entities": []}, {"text": "The red color fill represents the attention scores given to each entity.", "labels": [], "entities": []}, {"text": "We only report the attention scores of entities in the Gigaword example for conciseness since there are 80 linked entities in the CNN example.", "labels": [], "entities": []}, {"text": "Text d Linked entity: https://en.wikipedia.org/wiki/United_States E1.1: andy roddick got the better of dmitry tursunov in straight sets on friday , assuring the @united states a #-# lead over defending champions russia in the #### davis cup final .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.941478043794632}, {"text": "ROUGE", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.9026854634284973}]}, {"text": " Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.", "labels": [], "entities": [{"text": "CNN dataset", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.9715778529644012}, {"text": "F1 ROUGE metric", "start_pos": 60, "end_pos": 75, "type": "METRIC", "confidence": 0.8544570008913676}]}, {"text": " Table 4: Human evaluations on the Gigaword dataset.  Bold-faced values are the best while red-colored values  are the worst among the values in the evaluation metric.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.9732671976089478}]}]}