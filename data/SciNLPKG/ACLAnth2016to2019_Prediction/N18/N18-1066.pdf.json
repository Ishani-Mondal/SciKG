{"title": [], "abstractContent": [{"text": "Traditional approaches to semantic parsing (SP) work by training individual models for each available parallel dataset of text-meaning pairs.", "labels": [], "entities": [{"text": "semantic parsing (SP)", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.8689898133277894}]}, {"text": "In this paper, we explore the idea of polyglot semantic translation, or learning semantic parsing models that are trained on multiple datasets and natural languages.", "labels": [], "entities": [{"text": "polyglot semantic translation", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.6305046677589417}, {"text": "learning semantic parsing", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.7037395437558492}]}, {"text": "In particular , we focus on translating text to code signature representations using the software component datasets of Richardson and Kuhn (2017a,b).", "labels": [], "entities": []}, {"text": "The advantage of such models is that they can be used for parsing a wide variety of input natural languages and output programming languages, or mixed input languages , using a single unified model.", "labels": [], "entities": []}, {"text": "To facilitate modeling of this type, we develop a novel graph-based decoding framework that achieves state-of-the-art performance on the above datasets, and apply this method to two other benchmark SP tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work by; Miceli considers the problem of translating source code documentation to lower-level code template representations as part of an effort to model the meaning of such documentation.", "labels": [], "entities": []}, {"text": "Example documentation fora number of programming languages is shown in, where each docstring description in red describes a given function (blue) in the library.", "labels": [], "entities": []}, {"text": "While capturing the semantics of docstrings is in general a difficult task, learning the translation from descriptions to formal code representations (e.g., formal representations of functions) is proposed as a reasonable first step towards learning more general natural language understanding models in the software domain.", "labels": [], "entities": []}, {"text": "Under this approach, one can view a software library, or API, as a kind of parallel translation corpus for studying text \u2192 code or code \u2192 text translation.", "labels": [], "entities": [{"text": "code \u2192 text translation", "start_pos": 131, "end_pos": 154, "type": "TASK", "confidence": 0.6518005505204201}]}, {"text": "Richardson and Kuhn (2017b) extracted the standard library documentation for 10 popular programming languages across a number of natural languages to study the problem of text to function signature translation.", "labels": [], "entities": [{"text": "text to function signature translation", "start_pos": 171, "end_pos": 209, "type": "TASK", "confidence": 0.6606107652187347}]}, {"text": "Initially, these datasets were proposed as a resource for studying semantic parser induction, or for building models that learn to translate text to formal meaning representations from parallel data.", "labels": [], "entities": [{"text": "semantic parser induction", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.7953580220540365}]}, {"text": "In followup work (, they proposed using the resulting models to do automated question-answering (QA) and code retrieval on target APIs, and experimented with an additional set of software datasets built from 27 open-source Python projects.", "labels": [], "entities": [{"text": "code retrieval", "start_pos": 105, "end_pos": 119, "type": "TASK", "confidence": 0.6963112056255341}]}, {"text": "As traditionally done in SP, their approach involves learning individual models for each parallel dataset or language pair, e.g., (en, Java), (de, PHP), and.", "labels": [], "entities": []}, {"text": "Looking again at, we notice that while programming languages differ in terms of representation conventions, there is often overlap between the functionality implemented and naming in these different languages (e.g., the max function), and redundancy in the associated linguistic descriptions.", "labels": [], "entities": []}, {"text": "In addition, each English description (.1-1.3) describes max differently using the synonyms greater, maximum, largest.", "labels": [], "entities": []}, {"text": "In this case, it would seem that training models on multiple datasets, as opposed to single language pairs, might make learning more robust, and help to capture various linguistic alternatives.", "labels": [], "entities": []}, {"text": "With the software QA application in mind, an additional limitation is that their approach does not allow one to freely translate a given description to multiple output languages, which would be useful for comparing how different programming languages represent the same functionality.", "labels": [], "entities": []}, {"text": "The model also cannot translate between natural languages and programming languages that are not observed during training.", "labels": [], "entities": []}, {"text": "While software documentation is easy to find in bulk, if a particular API is not already documented in a language other than English (e.g., Haskell in de), it is unlikely that such a translation will appear without considerable effort by experienced translators.", "labels": [], "entities": []}, {"text": "Similarly, many individual APIs maybe too small or poorly documented to build individual models or QA applications, and will in someway need to bootstrap off of more general models or resources.", "labels": [], "entities": []}, {"text": "To deal with these issues, we aim to learn more general text-to-code translation models that are trained on multiple datasets simultaneously.", "labels": [], "entities": [{"text": "text-to-code translation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.7095987498760223}]}, {"text": "Our ultimate goal is to build polyglot translation models (cf.), or models with shared representations that can translate any input text to any output programming language, regardless of whether such language pairs were encountered explicitly during training.", "labels": [], "entities": []}, {"text": "Inherent in this task is the challenge of building an efficient polyglot decoder, or a translation mechanism that allows such crossing between input and output languages.", "labels": [], "entities": []}, {"text": "A key challenge is ensuring that such a decoder generates well-formed code representations, which is not guaranteed when one simply applies standard decoding strategies from SMT and neural MT (cf.).", "labels": [], "entities": [{"text": "SMT", "start_pos": 174, "end_pos": 177, "type": "TASK", "confidence": 0.8870449066162109}]}, {"text": "Given our ultimate interest in API QA, such a decoder must also facilitate monolingual translation, or being able to translate to specific output languages as needed.", "labels": [], "entities": [{"text": "monolingual translation", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.6215281784534454}]}, {"text": "To solve the decoding problem, we introduce anew graph-based decoding and representation framework that reduces to solving shortest path problems in directed graphs.", "labels": [], "entities": []}, {"text": "We investigate several translation models that work within this framework, including traditional SMT models and models based on neural networks, and report stateof-the-art results on the technical documentation task of.", "labels": [], "entities": [{"text": "SMT", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9889318943023682}]}, {"text": "To show the applicability of our approach to more conventional SP tasks, we apply our methods to the GeoQuery domain) and the Sportscaster corpus.", "labels": [], "entities": [{"text": "Sportscaster corpus", "start_pos": 126, "end_pos": 145, "type": "DATASET", "confidence": 0.9614926278591156}]}, {"text": "These experiments also provide insight into the main technical documentation task and highlight the strengths and weaknesses of the various translation models being investigated.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with two main types of resources: 45 API documentation datasets and two multilingual benchmark SP datasets.", "labels": [], "entities": []}, {"text": "In the former case, our main objective is to test whether training polyglot models (shown as polyglot in Tables 1-2) on multiple datasets leads to an improvement when compared to training individual monolingual models (shown as monolingual in.", "labels": [], "entities": []}, {"text": "Experiments involving the latter datasets are meant to test the applicability of our general graph and polyglot method to related SP tasks, and are also used for comparison against our main technical documentation task.", "labels": [], "entities": []}, {"text": "Technical API Docs The first dataset includes the Stdlib and Py27 datasets of, which are publicly available via.", "labels": [], "entities": [{"text": "Stdlib", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.9459561705589294}, {"text": "Py27 datasets", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.836110383272171}]}, {"text": "Stdlib consists of short description and function signature pairs for 10 programming languages in 7 languages, and Py27 contains the same type of data for 27 popular Python projects in English mined from Github.", "labels": [], "entities": [{"text": "Stdlib", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8716373443603516}]}, {"text": "We also built new datasets from the Japanese translation of the Python 2.7 standard library, as well as the Lua stdlib documentation in a mixture of Russian, Portuguese, German, Spanish and English.", "labels": [], "entities": [{"text": "Lua stdlib documentation", "start_pos": 108, "end_pos": 132, "type": "DATASET", "confidence": 0.7583822210629781}]}, {"text": "Taken together, these resources consist of 79,885 training pairs, and we experiment with training models on Stdlib and Py27 separately as well as together (shown as + more in).", "labels": [], "entities": []}, {"text": "We use a BPE subword encoding ( of both input and output words to make the representations more similar and transliterated all datasets (excluding Japanese datasets) to an 8-bit latin encoding.", "labels": [], "entities": []}, {"text": "Graphs were built by concatenating all function representations into a single word list and compiling this list into a minimized DAFSA.", "labels": [], "entities": []}, {"text": "For our global polyglot dataset, this resulted in a graph with 218,505 nodes, 313,288 edges, and 112,107 paths or component representations over an output vocabulary of 9,324 words.", "labels": [], "entities": []}, {"text": "Mixed GeoQuery and Sportscaster We run experiments on the GeoQuery 880 corpus using the splits from, which includes geography queries for English, Greek, Thai, and German paired with formal database queries, as well as a seed lexicon or NP list for each language.", "labels": [], "entities": [{"text": "GeoQuery 880 corpus", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.8801423112551371}]}, {"text": "In addition to training models on each individual dataset, we also learn polyglot models trained on all datasets concatenated together.", "labels": [], "entities": []}, {"text": "We also created anew mixed language test set that was built by replacing NPs in 803 test examples with one or more NPs from a different language using the NP lists mentioned above (see examples in).", "labels": [], "entities": []}, {"text": "The goal in the last case is to test our model's ability to handle mixed language input.", "labels": [], "entities": []}, {"text": "We also ran monolingual experiments on the English Sportscaster corpus, which contains human generated soccer commentary paired with symbolic meaning representation produced by a simulation of four games.", "labels": [], "entities": [{"text": "English Sportscaster corpus", "start_pos": 43, "end_pos": 70, "type": "DATASET", "confidence": 0.9671932260195414}]}, {"text": "For GeoQuery graph construction, we built a single graph for all languages by extracting general rule templates from all representations in the dataset, and exploited additional information and patterns using the Geobase database and the semantic grammars used in).", "labels": [], "entities": [{"text": "GeoQuery graph construction", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.7636705438296}, {"text": "Geobase database", "start_pos": 213, "end_pos": 229, "type": "DATASET", "confidence": 0.9419642984867096}]}, {"text": "This resulted in a graph with 2,419 nodes, 4,936 edges and 39,482 paths over an output vocabulary of 164.", "labels": [], "entities": []}, {"text": "For Sportscaster, we directly translated the semantic grammar provided in to a DAFSA, which resulted in a graph with 98 nodes, 86 edges and 830 paths.", "labels": [], "entities": [{"text": "DAFSA", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.8317933082580566}]}, {"text": "For the technical datasets, the goal is to see if our model generates correct signature representations from unobserved descriptions using exact match.", "labels": [], "entities": []}, {"text": "We follow exactly the experimental setup and data splits from, and measure the accuracy at 1 (Acc@1), accuracy in top 10 (Acc@10), and MRR.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9996544122695923}, {"text": "Acc@1)", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9370584487915039}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9995334148406982}, {"text": "Acc@10)", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9328413605690002}, {"text": "MRR", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9939674139022827}]}, {"text": "For the GeoQuery and Sportscaster experiments, the goal is to see if our models can generate correct meaning representations for unseen input.", "labels": [], "entities": []}, {"text": "For GeoQuery, we follow in evaluating extrinsically by checking that each representation evaluates to the same answer as the gold representation when executed against the Geobase database.", "labels": [], "entities": [{"text": "Geobase database", "start_pos": 171, "end_pos": 187, "type": "DATASET", "confidence": 0.9099601209163666}]}, {"text": "For Sportscaster, we evaluate by exact match to a gold representation.", "labels": [], "entities": [{"text": "Sportscaster", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9299802780151367}]}], "tableCaptions": [{"text": " Table 1: Test results on the Stdlib and Py27 tasks  averaged over all datasets and compared against the  best monolingual results from Richardson and Kuhn  (2017b,a), or RK", "labels": [], "entities": [{"text": "RK", "start_pos": 171, "end_pos": 173, "type": "DATASET", "confidence": 0.5404799580574036}]}, {"text": " Table 2: Test results for the standard (above) and mixed  (middle) GeoQuery tasks averaged over all languages,  and results for the English Sportscaster task (below).", "labels": [], "entities": [{"text": "English Sportscaster task", "start_pos": 133, "end_pos": 158, "type": "DATASET", "confidence": 0.8479668100674947}]}]}