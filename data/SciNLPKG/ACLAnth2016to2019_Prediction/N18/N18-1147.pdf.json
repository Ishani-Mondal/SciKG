{"title": [{"text": "Detecting Denial-of-Service Attacks from Social Media Text: Applying NLP to Computer Security", "labels": [], "entities": [{"text": "Detecting Denial-of-Service Attacks from Social Media Text", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.8841638905661446}]}], "abstractContent": [{"text": "This paper describes a novel application of NLP models to detect denial of service attacks using only social media as evidence.", "labels": [], "entities": []}, {"text": "Individual networks are often slow in reporting attacks , so a detection system from public data could better assist a response to abroad attack across multiple services.", "labels": [], "entities": []}, {"text": "We explore NLP methods to use social media as an indirect measure of network service status.", "labels": [], "entities": []}, {"text": "We describe two learning frameworks for this task: a feed-forward neural network and a partially labeled LDA model.", "labels": [], "entities": []}, {"text": "Both models outperform previous work by significant margins (20% F1 score).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9903071820735931}]}, {"text": "We further show that the topic-based model enables the first fine-grained analysis of how the public reacts to ongoing network attacks , discovering multiple \"stages\" of observation.", "labels": [], "entities": []}, {"text": "This is the first model that both detects network attacks (with best performance) and provides an analysis of when and how the public interprets service outages.", "labels": [], "entities": []}, {"text": "We describe the models, present experiments on the largest twitter DDoS corpus to date, and conclude with an analysis of public reactions based on the learned model's output.", "labels": [], "entities": [{"text": "twitter DDoS corpus", "start_pos": 59, "end_pos": 78, "type": "DATASET", "confidence": 0.6173598567644755}]}], "introductionContent": [{"text": "Distributed Denial of Service (DDoS) attacks have become more frequent and more severe in their impact.", "labels": [], "entities": [{"text": "Distributed Denial of Service (DDoS)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7868628289018359}]}, {"text": "Coordinated attacks across several services are now common, yet there are fewer methods to detect multi-network events.", "labels": [], "entities": []}, {"text": "Research into detecting and preventing single attacks focuses on direct evidence based on characteristics of a network itself, such as monitoring abnormal traffic.", "labels": [], "entities": []}, {"text": "This paper instead investigates an aytpical source for multiple attacks with indirect evidence: social media text.", "labels": [], "entities": []}, {"text": "Do users of attacked systems post on social media?", "labels": [], "entities": []}, {"text": "What can be learned from comments?", "labels": [], "entities": []}, {"text": "Can NLP learning models extract enough information from user posts to detect attacks?", "labels": [], "entities": []}, {"text": "Previous work on attack detection with social media is sparse, and focused on detecting trending words.", "labels": [], "entities": [{"text": "attack detection", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7145597189664841}]}, {"text": "This paper is the first to learn models of language without 'attack' dictionaries and seed words.", "labels": [], "entities": []}, {"text": "The goal is the real-time detection of attacks without network data.", "labels": [], "entities": [{"text": "real-time detection", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.5937083661556244}]}, {"text": "Our secondary goal is to illustrate NLP applications to computer security topics.", "labels": [], "entities": []}, {"text": "Research on information extraction from social media has shown that many types of events in the world can be reliably detected from the language that users post.", "labels": [], "entities": [{"text": "information extraction from social media", "start_pos": 12, "end_pos": 52, "type": "TASK", "confidence": 0.8607768774032593}]}, {"text": "Several approaches have been shown effective in identifying events like earthquakes (), concerts and product releases (, and other natural disasters).", "labels": [], "entities": []}, {"text": "Detecting DDoS attacks is not too dissimilar from these goals.", "labels": [], "entities": []}, {"text": "An attack is areal event in the world, and it takes a community by surprise.", "labels": [], "entities": []}, {"text": "This paper thus adopts ideas from NLP, but applies them to the unique application of DDoS detection.", "labels": [], "entities": [{"text": "DDoS detection", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.83332559466362}]}, {"text": "Social media is obviously not the only way (nor the most direct) to monitor network services and attacks.", "labels": [], "entities": []}, {"text": "There are several commercial services that directly measure outages, such as norsecorp . These perform direct monitoring of network response.", "labels": [], "entities": [{"text": "norsecorp", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.9322596192359924}]}, {"text": "We do not propose social media as a better alternative, but rather as an alternative that enhances direct monitoring.", "labels": [], "entities": []}, {"text": "Social media also brings its own unique benefits.", "labels": [], "entities": []}, {"text": "For instance, social media does not require a priori knowledge of which networks should be monitored.", "labels": [], "entities": []}, {"text": "It can also help detect \"soft\" outages like slowdowns and account blockages, things that direct monitoring cannot always detect.", "labels": [], "entities": []}, {"text": "Therefore, this paper is not suggesting a replacement, but rather anew source of valuable information.", "labels": [], "entities": []}, {"text": "It is a monitoring architecture that is not constrained by a predefined list of services.", "labels": [], "entities": []}, {"text": "Our goal in using social media is driven by the hypothesis that as a network attack unfolds, its users go through a series of observational stages that can be automatically learned and detected.", "labels": [], "entities": []}, {"text": "The first stage is a state of confusion and basic symptom observation, as seen in the following real tweets from Twitter: hey linode what's happening?", "labels": [], "entities": []}, {"text": "I can't login, my servers are down and you don't reply on mails? is xbox live experiencing some issues?", "labels": [], "entities": []}, {"text": "These tweets don't discuss an attack even though that is what was occurring.", "labels": [], "entities": []}, {"text": "Later stages then develop into direct commentary as the community coelesces to a belief that an attack is the cause: Breaking: Band of America website rumored to be under DDoS attack.", "labels": [], "entities": [{"text": "Breaking: Band of America website", "start_pos": 117, "end_pos": 150, "type": "DATASET", "confidence": 0.9041041930516561}]}], "datasetContent": [{"text": "We manually created a dataset of historical DDoS attacks that include the entity attacked and the date of attack.", "labels": [], "entities": []}, {"text": "Most past attacks are difficult to identify hour ranges, so we used a full 24-hour day as our granularity.", "labels": [], "entities": []}, {"text": "We included 6 attacks with sufficient volume from previous work (), but we grew this set to 50 attacks based on our own investigations into recent years, mostly through web search results for 'DDoS attacks'.", "labels": [], "entities": []}, {"text": "lists some of these for illustration of its diversity.", "labels": [], "entities": []}, {"text": "The full list is at www.usna.edu/Users/cs/ nchamber/data/ddos/ For each of these known attacks, we collected tweets that contained the attacked entity's name in a 20-day period: 17 days prior to the attack, 1 day on the attack, and 2 days following.", "labels": [], "entities": []}, {"text": "We wanted a sufficient lead up to the attack to include previous work's trending model (, and to provide non-attack days for evaluation.", "labels": [], "entities": []}, {"text": "The days surrounding the known attack date are labeled NOT-ATTACK, and the attack day itself as ATTACK.", "labels": [], "entities": [{"text": "NOT-ATTACK", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.5360050797462463}, {"text": "ATTACK", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.4837188124656677}]}, {"text": "Sometimes an attack lasted longer than a single day, in which case the days following were also labeled as ATTACK, as appropriate.", "labels": [], "entities": [{"text": "ATTACK", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.955160915851593}]}, {"text": "The historical attacks span the years 2012-2016.", "labels": [], "entities": []}, {"text": "We split the data so that years 2012, 2015, and 2016 comprise the training set, 2013 is the development set, and the year 2014 is the test set only used for computing final experiment numbers.", "labels": [], "entities": []}, {"text": "Splitting on years (rather than months or entities) guards against test set pollution into our training set.", "labels": [], "entities": [{"text": "Splitting", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9660633206367493}]}, {"text": "The evaluation on the 2014 test set is thus an unbiased experiment because nothing from the entire year is included in training.", "labels": [], "entities": [{"text": "2014 test set", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.8709632953008016}]}, {"text": "For the experiments, we use the union of training and development to train the final models that are then used to evaluate on the test year 2014.", "labels": [], "entities": []}, {"text": "There are 200 test days in 2014, 50 in dev, and\u02dc500and\u02dc500 in training.", "labels": [], "entities": []}, {"text": "The full dataset consists of 50 attack days over approximately 800 days and 2 million tweets.", "labels": [], "entities": []}, {"text": "The only previous work on this area used seed words to pullout around 9-10 thousand tweets.", "labels": [], "entities": []}, {"text": "Our dataset is more than 2 orders of magnitude larger.", "labels": [], "entities": []}, {"text": "The reason is due to the larger number of attacks we collected, but notably our tweets are more diverse and varied because we don't require hard-coded target words and phrases to match.", "labels": [], "entities": []}, {"text": "Formally, the dataset is 800 labeled datums: where d i \u2208 D and Dis the set of all days.", "labels": [], "entities": []}, {"text": "Entity is the attacked network service, Date is the calendar date, and T weets are all tweets on that date mentioning that entity.", "labels": [], "entities": [{"text": "Date", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9278429746627808}]}, {"text": "Label is a binary variable: ATTACK or NOT-ATTACK.", "labels": [], "entities": [{"text": "ATTACK", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.989835798740387}]}, {"text": "Even though the day following an attack often includes attack discussion, it is still labeled NOT-ATTACK.", "labels": [], "entities": [{"text": "NOT-ATTACK", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.7138209939002991}]}, {"text": "Only if the attack was ongoing is the next day labeled ATTACK.", "labels": [], "entities": [{"text": "ATTACK", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.5723764300346375}]}, {"text": "All experiments are conducted on the dataset described in Datasets.", "labels": [], "entities": []}, {"text": "The task is a binary classification of ATTACK or NOTATTACK given a day of tweets.", "labels": [], "entities": [{"text": "ATTACK", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9944814443588257}, {"text": "NOTATTACK", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.485135942697525}]}, {"text": "All parameters are optimized on the development set: we treat attack days as known on training days, but hidden from the development and test days.", "labels": [], "entities": []}, {"text": "We calculate F1 score on the development attack days, and optimize parameters using a basic grid search.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9829823672771454}]}, {"text": "For the final reported results, we combine train+dev into one observed training set, and the test set is now included in sampling, but with unobserved attack days.", "labels": [], "entities": []}, {"text": "Since the PLDAttack model is probabilistic, all reported numbers are an average of 10 independent runs.", "labels": [], "entities": []}, {"text": "We use ATTACK F1 as the main evaluation target; the harmonic mean between precision and recall.", "labels": [], "entities": [{"text": "ATTACK F1", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.7607035636901855}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.999184787273407}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9944338202476501}]}, {"text": "Applications overly concerned with missing attacks would optimize to recall R.", "labels": [], "entities": [{"text": "recall R", "start_pos": 69, "end_pos": 77, "type": "TASK", "confidence": 0.6703601032495499}]}, {"text": "We chose F 1 as a happy balance between a quality classifier (good precision P ) and a useful classifier (good recall R).", "labels": [], "entities": [{"text": "F 1", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.931672990322113}, {"text": "precision P", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.9593245387077332}, {"text": "recall R)", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.963182290395101}]}, {"text": "We report all three scores for both the AT-TACK and NOTATTACK labels, but optimize to F1 during parameter search on the development set.", "labels": [], "entities": [{"text": "AT-TACK", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9671837687492371}, {"text": "NOTATTACK", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.5316453576087952}, {"text": "F1", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9988434314727783}]}, {"text": "The test set results for baselines and models are shown in.", "labels": [], "entities": []}, {"text": "All improvements are statistically significant as indicated using McNemar's two-tailed test.", "labels": [], "entities": []}, {"text": "The trending baselines have high recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9994383454322815}]}, {"text": "When an attack is happening, the network does indeed trend on social media.", "labels": [], "entities": []}, {"text": "Precision is low, however, because non-security events also cause discussions.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9595435261726379}]}, {"text": "The neural models outperform the baselines, and a hidden layer (Neural-2) is definitely needed for increased detection.", "labels": [], "entities": []}, {"text": "The training set of 500 documents is still small for neural training, though.", "labels": [], "entities": [{"text": "neural training", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7275827825069427}]}, {"text": "Neural models have many parameters, and they overfit to our training set despite regularization with dropout, reducing dimensions, and removing hidden layers.", "labels": [], "entities": []}, {"text": "Even still, we improved over the Motoyama baseline by 20% relative F1.", "labels": [], "entities": [{"text": "Motoyama baseline", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.9082142114639282}, {"text": "F1", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9995935559272766}]}, {"text": "PLDA (PLDAttack) showed the highest precision when classifying an ATTACK.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.999388575553894}]}, {"text": "Since its recall was similar to the neural models, it produced the best F1 score.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9994294047355652}, {"text": "F1 score", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9864069521427155}]}, {"text": "This is a 25% relative improvement over previous work.", "labels": [], "entities": []}, {"text": "PLDAttack generalizes to the dataset slightly better than the neural models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on the held-out test set of 200 test  datums. Motoyama'10 is a trending phrase baseline.", "labels": [], "entities": []}]}