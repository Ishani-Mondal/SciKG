{"title": [{"text": "Which Scores to Predict in Sentence Regression for Text Summarization?", "labels": [], "entities": [{"text": "Text Summarization", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7547589540481567}]}], "abstractContent": [{"text": "The task of automatic text summarization is to generate a short text that summarizes the most important information in a given set of documents.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.6456473171710968}]}, {"text": "Sentence regression is an emerging branch in automatic text summarizations.", "labels": [], "entities": [{"text": "Sentence regression", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9530774652957916}, {"text": "automatic text summarizations", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6089748640855154}]}, {"text": "Its key idea is to estimate the importance of information via learned utility scores for individual sentences.", "labels": [], "entities": []}, {"text": "These scores are then used for selecting sentences from the source documents , typically according to a greedy selection strategy.", "labels": [], "entities": []}, {"text": "Recently proposed state-of-the-art models learn to predict ROUGE recall scores of individual sentences, which seems reasonable since the final summaries are evaluated according to ROUGE recall.", "labels": [], "entities": [{"text": "ROUGE recall scores", "start_pos": 59, "end_pos": 78, "type": "METRIC", "confidence": 0.7438292105992635}]}, {"text": "In this paper, we show in extensive experiments that following this intuition leads to suboptimal results and that learning to predict ROUGE precision scores leads to better results.", "labels": [], "entities": [{"text": "ROUGE precision scores", "start_pos": 135, "end_pos": 157, "type": "METRIC", "confidence": 0.7948585351308187}]}, {"text": "The crucial difference is to aim not at covering as much information as possible but at wasting as little space as possible in every greedy step.", "labels": [], "entities": []}], "introductionContent": [{"text": "More and more data is generated in textual form in newspapers, social media platforms, and microblogging services and it has become impossible for humans to read, comprehend, and filter all the available data.", "labels": [], "entities": []}, {"text": "Automatic summarization aims at mitigating these problems by \"taking an information source, extracting content from it, and presenting the most important content to the user in a condensed form and in a manner sensitive to the users or applications needs\".", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6707127094268799}]}, {"text": "Very prominent in automatic text summarization is the idea of extractive summarization.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.5678117970625559}, {"text": "extractive summarization", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.6517878770828247}]}, {"text": "In extractive summarization, summaries are not generated from scratch.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.7052077949047089}, {"text": "summaries", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.9654842615127563}]}, {"text": "Instead, sentences in the source documents, which are supposed to be summarized, are extracted and concatenated to form a summary.", "labels": [], "entities": []}, {"text": "To be able to select sentences in a meaningful manner, it is crucial for the extractive systems to be able to estimate the utility of individual sentences.", "labels": [], "entities": []}, {"text": "Supervised extractive methods are usually modeled in a regression framework.", "labels": [], "entities": []}, {"text": "Hence, this subfield of automatic summarization is called sentence regression.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.7635555267333984}, {"text": "sentence regression", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7393591105937958}]}, {"text": "The predicted scores are used to generate a ranking of the sentences, and a greedy strategy is often used in combination with additional redundancy avoidance to select sentences which will be added to the iteratively generated summary).", "labels": [], "entities": []}, {"text": "Another method for the selection is solving an integer linear programming (ILP) problem) which is, however, an NP-hard problem).", "labels": [], "entities": []}, {"text": "Even though it can be argued that the complexity is not an issue since there are good solvers for ILPs, it remains a problem when large document collections with many sentences have to be summarized or the system should be used on a large scale for many users.", "labels": [], "entities": []}, {"text": "The greedy approach is due its simplicity and efficiency very appealing.", "labels": [], "entities": []}, {"text": "Crucial for building sentence regression models is the choice of the regressands which has to be predicted by the models.", "labels": [], "entities": []}, {"text": "Most of the recent works try to predict ROUGE recall scores of individual sentences, which seems to bean obvious choice since the final summaries are also evaluated with ROUGE recall metrics).", "labels": [], "entities": [{"text": "ROUGE recall scores", "start_pos": 40, "end_pos": 59, "type": "METRIC", "confidence": 0.781558612982432}]}, {"text": "We show in this paper that following this intuition leads to suboptimal results.", "labels": [], "entities": []}, {"text": "In extensive experiments, we investigate sentence regression models with perfect and noisy prediction of different regressand candidates with and without redundancy avoidance.", "labels": [], "entities": []}, {"text": "In all experiments, we observe the very same result: learning to predict ROUGE precision scores of sentences leads to better results than learning to predict ROUGE recall scores if the scores are selected with a greedy algorithm afterwards.", "labels": [], "entities": [{"text": "ROUGE precision scores", "start_pos": 73, "end_pos": 95, "type": "METRIC", "confidence": 0.7079097827275594}, {"text": "ROUGE recall scores", "start_pos": 158, "end_pos": 177, "type": "METRIC", "confidence": 0.6868569254875183}]}, {"text": "Our findings are in particular important for automatic summarization research since the best models currently available are sentence regression models trained to predict ROUGE recall scores.", "labels": [], "entities": [{"text": "automatic summarization", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.6434223651885986}, {"text": "ROUGE recall scores", "start_pos": 170, "end_pos": 189, "type": "METRIC", "confidence": 0.7571364641189575}]}, {"text": "We expect that simply replacing ROUGE recall scores as regressand with ROUGE precision scores can potentially improve these state-of-the-art models further.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.7708104848861694}, {"text": "ROUGE precision scores", "start_pos": 71, "end_pos": 93, "type": "METRIC", "confidence": 0.8564647237459818}]}, {"text": "We note in passing that the problem is reminiscent of defining heuristics in inductive rule learning: Individual rules are typically evaluated according to their consistency (minimizing the amount of false positives) and completeness (maximizing the amount of true positives), which loosely correspond to precision and recall).", "labels": [], "entities": [{"text": "consistency", "start_pos": 162, "end_pos": 173, "type": "METRIC", "confidence": 0.9892998933792114}, {"text": "precision", "start_pos": 305, "end_pos": 314, "type": "METRIC", "confidence": 0.9989758729934692}, {"text": "recall", "start_pos": 319, "end_pos": 325, "type": "METRIC", "confidence": 0.989572286605835}]}, {"text": "Heuristics such as weighted relative accuracy, which give equal importance to both dimensions, are successfully used for evaluating single rules in subgroup discovery), but tend to over-generalize when being used for selecting rules for inclusion into a predictive rule set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.718502402305603}, {"text": "subgroup discovery", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.7660680413246155}]}, {"text": "The reason for this is that alack of completeness can be repaired by adding more rules, whereas alack of consistency cannot, so that consistency or precision of individual rules should receive a higher weight in the selection task.", "labels": [], "entities": [{"text": "consistency", "start_pos": 105, "end_pos": 116, "type": "METRIC", "confidence": 0.8986024856567383}, {"text": "consistency", "start_pos": 133, "end_pos": 144, "type": "METRIC", "confidence": 0.9695865511894226}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9738166928291321}]}, {"text": "Transferred to summarization, this means that space wasted by recall-oriented selection cannot be used anymore whereas a low recall in a partial summary can be repaired by adding more sentences.", "labels": [], "entities": [{"text": "summarization", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9795175790786743}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9812045097351074}]}, {"text": "In the following, we will first formalize the problem of extractive summarization and outline the greedy selection strategy (Section 2).", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.6283872127532959}]}, {"text": "Previously extractive summarization systems, in particularly sentence regression models, are summarized in Section 3.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.6199473142623901}]}, {"text": "We then present an intuition why predicting ROUGE precision scores can potentially give better results in Section 4.", "labels": [], "entities": [{"text": "ROUGE precision scores", "start_pos": 44, "end_pos": 66, "type": "METRIC", "confidence": 0.7955596446990967}]}, {"text": "In extensive experiments (Section 5), we actually show the previously stated hypothesis which says that selecting sentence according to ROUGE precision instead of ROUGE recall leads to better results if sentence are selected greedily.", "labels": [], "entities": [{"text": "ROUGE precision", "start_pos": 136, "end_pos": 151, "type": "METRIC", "confidence": 0.8497802913188934}, {"text": "ROUGE recall", "start_pos": 163, "end_pos": 175, "type": "METRIC", "confidence": 0.7867684066295624}]}], "datasetContent": [{"text": "We now present the experimental setups in which we test different regressand candidates for sentence regression in three different, well-known multi-document summarization (MDS) corpora.", "labels": [], "entities": [{"text": "sentence regression", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.6917870491743088}, {"text": "multi-document summarization (MDS)", "start_pos": 143, "end_pos": 177, "type": "TASK", "confidence": 0.7535715699195862}]}, {"text": "We used the MDS corpora from the DUC 2004 1 , TAC 2008, and TAC 2009 2 summarization shared tasks.", "labels": [], "entities": [{"text": "DUC 2004 1", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9370181957880656}, {"text": "TAC 2008", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.8711593151092529}]}, {"text": "All corpora contain 10 input documents and 4 reference summaries for each topic.", "labels": [], "entities": []}, {"text": "The number of topics are 50, 46, and 44, respectively.", "labels": [], "entities": []}, {"text": "We simulate in the experiments the outcomes of regression models which use different regressands.", "labels": [], "entities": []}, {"text": "This will provide us with theoretical insights on which regressand candidates should be considered in regression models and will answer the main question of this paper: Which scores to predict in sentence regression for text summarization?", "labels": [], "entities": [{"text": "text summarization", "start_pos": 220, "end_pos": 238, "type": "TASK", "confidence": 0.7367159426212311}]}, {"text": "For our experiments, we produce summaries containing 665 characters for DUC2004 and summaries containing 100 words for TAC2008 and TAC2009.", "labels": [], "entities": [{"text": "DUC2004", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9483925700187683}, {"text": "TAC2008", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9208581447601318}, {"text": "TAC2009", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.9333425164222717}]}], "tableCaptions": [{"text": " Table 1: Summarization results in three different  multi-document summarization corpora without  redundancy avoidance. Columns R-1 and R-2 dis- play the summary quality according to ROUGE-1  recall and ROUGE-2 recall scores, respectively.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 183, "end_pos": 190, "type": "METRIC", "confidence": 0.8656929731369019}, {"text": "recall", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.5971992611885071}, {"text": "ROUGE-2", "start_pos": 203, "end_pos": 210, "type": "METRIC", "confidence": 0.8129575848579407}, {"text": "recall", "start_pos": 211, "end_pos": 217, "type": "METRIC", "confidence": 0.6129096746444702}]}, {"text": " Table 2: Averaged lengths of resulting summaries  measured in number of stems (avg. stems) and  number of sentences (avg. sentences). D04 refers  to DUC2004 and T08 and T09 refer to TAC2008  and TAC2009, respectively. We count also par- tially contained sentences which have been cut by  the ROUGE length limitation.", "labels": [], "entities": [{"text": "DUC2004", "start_pos": 150, "end_pos": 157, "type": "DATASET", "confidence": 0.8349334597587585}, {"text": "ROUGE length", "start_pos": 293, "end_pos": 305, "type": "METRIC", "confidence": 0.8848254978656769}]}, {"text": " Table 3: Results as in", "labels": [], "entities": []}, {"text": " Table 4: Summarization results in three differ- ent multi-document summarization corpora with  noisy score prediction with uniform noise (top)  and Gaussian noise (bottom).", "labels": [], "entities": [{"text": "Summarization", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9762364029884338}]}]}