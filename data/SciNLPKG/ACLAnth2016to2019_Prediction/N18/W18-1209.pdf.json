{"title": [{"text": "Incorporating Subword Information into Matrix Factorization Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "The positive effect of adding subword information to word embeddings has been demonstrated for predictive models.", "labels": [], "entities": []}, {"text": "In this paper we investigate whether similar benefits can also be derived from incorporating subwords into counting models.", "labels": [], "entities": []}, {"text": "We evaluate the impact of different types of subwords (n-grams and un-supervised morphemes), with results confirming the importance of subword information in learning representations of rare and out-of-vocabulary words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Low dimensional word representations (embeddings) have become a key component in modern NLP systems for language modeling, parsing, sentiment classification, and many others.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7098052799701691}, {"text": "parsing", "start_pos": 123, "end_pos": 130, "type": "TASK", "confidence": 0.9519827961921692}, {"text": "sentiment classification", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.9253704845905304}]}, {"text": "These embeddings are usually derived by employing the distributional hypothesis: that similar words appear in similar contexts.", "labels": [], "entities": []}, {"text": "The models that perform the word embedding can be divided into two classes: predictive, which learn a target or context word distribution, and counting, which use a raw, weighted, or factored word-context co-occurrence matrix ().", "labels": [], "entities": []}, {"text": "The most well-known predictive model, which has become eponymous with word embedding, is word2vec ().", "labels": [], "entities": []}, {"text": "Popular counting models include PPMI-SVD (), GloVe (), and).", "labels": [], "entities": [{"text": "GloVe", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9044325351715088}]}, {"text": "These models all learn word-level representations, which presents two main problems: 1) Learned information is not explicitly shared among the representations as each word has an independent vector.", "labels": [], "entities": []}, {"text": "2) There is no clearway to represent out-of-vocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "fastText) addresses these issues in the Skip-gram word2vec model by representing a word by the sum of a unique vector and a set of shared character n-grams (from hereon simply referred to as n-grams) vectors.", "labels": [], "entities": []}, {"text": "This addresses both issues above as learned information is shared through the n-gram vectors and from these OOV word representations can be constructed.", "labels": [], "entities": []}, {"text": "In this paper we propose incorporating subword information into counting models using a strategy similar to fastText.", "labels": [], "entities": []}, {"text": "We use LexVec as the counting model as it generally outperforms PPMI-SVD and GloVe on intrinsic and extrinsic evaluations, but the method proposed here should transfer to GloVe unchanged.", "labels": [], "entities": [{"text": "LexVec", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.9073112607002258}, {"text": "GloVe", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9343847036361694}]}, {"text": "The LexVec objective is modified 1 such that a word's vector is the sum of all its subword vectors.", "labels": [], "entities": [{"text": "LexVec objective", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8511064648628235}]}, {"text": "We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor to learn whether more linguistically motivated subwords offer any advantage oversimple n-grams.", "labels": [], "entities": []}, {"text": "To evaluate the impact subword information has on in-vocabulary (IV) word representations, we run intrinsic evaluations consisting of word similarity and word analogy tasks.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 154, "end_pos": 166, "type": "TASK", "confidence": 0.7434532940387726}]}, {"text": "The incorporation of subword information results in similar gains (and losses) to that of fastText over Skip-gram.", "labels": [], "entities": []}, {"text": "Whereas incorporating n-gram subwords tends to capture more syntactic information, unsupervised morphemes better preserve semantics while also improving syntactic results.", "labels": [], "entities": []}, {"text": "Given that intrinsic performance can correlate poorly with performance on downstream tasks (, we also conduct evaluation using the VecEval suite of tasks (, in which all subword models, including fastText, show no significant improvement over word-level models.", "labels": [], "entities": []}, {"text": "We verify the model's ability to represent OOV words by quantitatively evaluating nearestneighbors.", "labels": [], "entities": []}, {"text": "Results show that, like fastText, both LexVec n-gram and (to a lesser degree) unsupervised morpheme models give coherent answers.", "labels": [], "entities": [{"text": "LexVec", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.9362763166427612}]}, {"text": "This paper discusses related word ( \u00a72), introduces the subword LexVec model ( \u00a73), describes experiments ( \u00a74), analyzes results ( \u00a75), and concludes with ideas for future works ( \u00a76).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Word similarity (Spearman's rho), analogy  (% accuracy), and downstream task (% accuracy) re- sults. In downstream tasks, for the same model accu- racy varies over different runs, so we report the mean  over 20 runs, in which the only significantly (p < .05  under a random permutation test) different result is in  chunking.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9683552980422974}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9046695232391357}]}]}