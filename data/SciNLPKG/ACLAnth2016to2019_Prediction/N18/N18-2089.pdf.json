{"title": [], "abstractContent": [{"text": "We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs.", "labels": [], "entities": [{"text": "Question-Answer Meaning Representations (QAMRs)", "start_pos": 13, "end_pos": 60, "type": "TASK", "confidence": 0.7514826705058416}]}, {"text": "We develop a crowdsourcing scheme to show that QAMRs can be labeled with very little training, and gather a dataset with over 5,000 sentences and 100,000 questions.", "labels": [], "entities": []}, {"text": "A qualitative analysis demonstrates that the crowd-generated question-answer pairs cover the vast majority of predicate-argument relationships in existing datasets (including PropBank, Nom-Bank, and QA-SRL) along with many previously under-resourced ones, including implicit arguments and relations.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 175, "end_pos": 183, "type": "DATASET", "confidence": 0.9632493257522583}]}, {"text": "We also report baseline models for question generation and answering, and summarize a recent approach for using QAMR labels to improve an Open IE system.", "labels": [], "entities": [{"text": "question generation and answering", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.814209908246994}]}, {"text": "These results suggest the freely available 1 QAMR data and annotation scheme should support significant future work.", "labels": [], "entities": [{"text": "QAMR data", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.6675675213336945}]}], "introductionContent": [{"text": "Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom.", "labels": [], "entities": [{"text": "sentential meaning representations", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.7513372103373209}]}, {"text": "Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank () and there have been efforts to study other parts of speech (e.g. NomBank () and FrameNet () and introduce whole-sentence structures (e.g. AMR ().", "labels": [], "entities": [{"text": "predicate-argument structure", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.7244763970375061}]}, {"text": "However, highly skilled and trained annotators are re- quired to label data within these formulations for each new domain, and it takes significant effort to model each new type of relationship (e.g., noun arguments in NomBank).", "labels": [], "entities": []}, {"text": "We propose anew method to annotate relatively complete representations of the predicate-argument structure of a sentence, which can be done easily by non-experts.", "labels": [], "entities": []}, {"text": "We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs (see).", "labels": [], "entities": [{"text": "Question-Answer Meaning Representations (QAMRs)", "start_pos": 13, "end_pos": 60, "type": "TASK", "confidence": 0.760119711359342}]}, {"text": "Following the QA-SRL formalism, each question-answer pair corresponds to a predicateargument relationship.", "labels": [], "entities": []}, {"text": "There is no need fora carefully curated ontology and the labels are highly interpretable.", "labels": [], "entities": []}, {"text": "However, we differ from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates.", "labels": [], "entities": []}, {"text": "The QAMR formulation provides anew way of thinking about predicate-argument structure.", "labels": [], "entities": []}, {"text": "Any form of sentence meaning-from a vector of real numbers to a logical form-should support the challenge of determining which questions are answerable by the sentence, and what the answers are.", "labels": [], "entities": []}, {"text": "A QAMR sidesteps intermediate formal representations by surfacing those questions and an-swers as the representation.", "labels": [], "entities": []}, {"text": "As with any other representation, this can then be reprocessed for downstream tasks.", "labels": [], "entities": []}, {"text": "Indeed, the question-answer format facilitates reprocessing for tasks that are similar inform, for example Open IE (see Section 4).", "labels": [], "entities": [{"text": "Open IE", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.7163617312908173}]}, {"text": "A key advantage of QAMRs is that they can be annotated with crowdsourcing.", "labels": [], "entities": []}, {"text": "The main challenge is coverage, as it can be difficult fora single annotator to write all possible QA pairs fora sentence.", "labels": [], "entities": [{"text": "coverage", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9486979246139526}]}, {"text": "Instead, we distribute the work between multiple annotators in a novel crowdsourcing scheme, which we use to gather a dataset of over 100,000 QA pairs for 5,000 sentences in Newswire and Wikipedia domains.", "labels": [], "entities": [{"text": "Newswire", "start_pos": 174, "end_pos": 182, "type": "DATASET", "confidence": 0.9474701285362244}]}, {"text": "Although QAMR questions' free-form nature is crucial for our approach, it means that predicates are not explicitly marked.", "labels": [], "entities": []}, {"text": "However, with a simple predicate-finding heuristic, we can align QAMR to PropBank, NomBank, and QA-SRL and show high coverage of predicate-argument structure, including more than 90% of non-discourse relationships.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.925635814666748}]}, {"text": "Further analysis reveals that QAMRs also capture many phenomena that are not modeled in traditional representations of predicate-argument structure, including coreference, implicit and inferred arguments, and implicit relations (for example, with noun adjuncts).", "labels": [], "entities": []}, {"text": "Finally, we report simple neural baselines for QAMR question generation and answering.", "labels": [], "entities": [{"text": "QAMR question generation and answering", "start_pos": 47, "end_pos": 85, "type": "TASK", "confidence": 0.8068792462348938}]}, {"text": "We also highlight a recent result ( showing that QAMR data can be used to improve performance on a challenging task: Open Information Extraction.", "labels": [], "entities": [{"text": "Open Information Extraction", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.7243791619936625}]}, {"text": "Together, these results show that there is significant potential for followup work on developing innovative uses of QAMR and modeling their relatively comprehensive and complex predicate-argument relationships.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Summary of the data gathered.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7626451849937439}]}]}