{"title": [{"text": "CNNs for NLP in the Browser: Client-Side Deployment and Visualization Opportunities", "labels": [], "entities": []}], "abstractContent": [{"text": "We demonstrate a JavaScript implementation of a convolutional neural network that performs feedforward inference completely in the browser.", "labels": [], "entities": []}, {"text": "Such a deployment means that models can run completely on the client, on a wide range of devices, without making backend server requests.", "labels": [], "entities": []}, {"text": "This design is useful for applications with stringent latency requirements or low connectivity.", "labels": [], "entities": []}, {"text": "Our evaluations show the feasibility of JavaScript as a deployment target.", "labels": [], "entities": []}, {"text": "Furthermore, an in-browser implementation enables seamless integration with the JavaScript ecosystem for information visual-ization, providing opportunities to visually inspect neural networks and better understand their inner workings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Once trained, feedforward inference using neural networks (NNs) is straightforward: just a series of matrix multiplications, application of nonlinearities, and other simple operations.", "labels": [], "entities": []}, {"text": "With the rise of model interchange formats such as ONNX, we now have clean abstractions that separate model training from model inference.", "labels": [], "entities": [{"text": "ONNX", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.9287853240966797}]}, {"text": "In this context, we explore JavaScript as a deployment target of neural networks for NLP applications.", "labels": [], "entities": []}, {"text": "To be clear, we are not concerned with training, and simply assume the existence of a pre-trained model that we wish to deploy for inference.", "labels": [], "entities": []}, {"text": "We provide two compelling reasons.", "labels": [], "entities": []}, {"text": "First, JavaScript is the most widely deployed platform in the world since it resides in every web browser.", "labels": [], "entities": []}, {"text": "An implementation in JavaScript means that a NN can be embedded in any web page for client-side execution on any device that has a browser-from laptops to tablets to mobile phones to even potentially \"smart home\" gadgets.", "labels": [], "entities": []}, {"text": "Performing inference on the client also obviates the need for server requests and the associated latencies.", "labels": [], "entities": []}, {"text": "With such a deployment, NLP applications that have high demands on responsiveness (e.g., typeahead prediction, grammar correction) or suffer from low connectivity (e.g., remote locations or developing countries) can take advantage of NN models.", "labels": [], "entities": [{"text": "typeahead prediction", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.8570992350578308}, {"text": "grammar correction", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7435685843229294}]}, {"text": "Such a deployment also protects user privacy, since user data does not leave the client.", "labels": [], "entities": []}, {"text": "Second, the browser has emerged as the dominant platform for information visualization, and JavaScript-based implementations support seamless integration with modern techniques and existing toolkits (e.g., D3.js).", "labels": [], "entities": [{"text": "information visualization", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.8889627754688263}]}, {"text": "This provides opportunities to visually inspect neural networks.", "labels": [], "entities": []}, {"text": "We demonstrate a prototype implementation of a convolutional neural network for sentence classification, applied to sentiment analysis-the model of-in JavaScript, running completely inside a web browser.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7283298224210739}, {"text": "sentiment analysis-the model", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.9259925683339437}]}, {"text": "Not surprisingly, we find that inference performance is significantly slower compared to code running natively, but the browser is nevertheless able to take advantage of GPUs and hardware acceleration on a variety of platforms.", "labels": [], "entities": []}, {"text": "Our implementation enables simple visualizations that allow us to gain insights into what semantic n-gram features the model is extracting.", "labels": [], "entities": []}, {"text": "This is useful for pedagogy (teaching students about neural networks) as well as research, since understanding a model is critical to improving it.", "labels": [], "entities": []}, {"text": "Overall, our visualizations contribute to an emerging thread of research on interpretable machine learning.", "labels": [], "entities": [{"text": "interpretable machine learning", "start_pos": 76, "end_pos": 106, "type": "TASK", "confidence": 0.7326381603876749}]}], "datasetContent": [{"text": "The first obvious question we tackle is the performance of our JavaScript implementation.", "labels": [], "entities": []}, {"text": "How much slower is it than model inferencing performed natively?", "labels": [], "entities": []}, {"text": "We evaluated in-browser inference on a 2015 MacBook Pro laptop equipped with an Intel Core i5-5257U processor (2 cores), running MacOS 10.13.", "labels": [], "entities": []}, {"text": "We compared performance with the desktop machine used to train the model, which has an Intel Core i7-6800K processor (6 cores) and an NVIDIA GeForce GTX 1080.", "labels": [], "entities": []}, {"text": "The first block of the table shows the performance of PyTorch running on the desktop, with and without GPU acceleration.", "labels": [], "entities": []}, {"text": "As expected, the GPU is able to exploit parallelism for batch inferencing, but on individual sentences, the CPU is only slightly slower.", "labels": [], "entities": []}, {"text": "In the bottom block of the table, we report results of running our JavaScript implementation in Google Chrome (v64).", "labels": [], "entities": []}, {"text": "We compared the desktop and the laptop, with and without GPU acceleration.", "labels": [], "entities": []}, {"text": "For the most common case (inference on a single sentence), the browser is about an order of magnitude slower with the GPU.", "labels": [], "entities": []}, {"text": "Without the GPU, performance drops by another \u223c25\u00d7.", "labels": [], "entities": []}, {"text": "The above figures include only inference time.", "labels": [], "entities": []}, {"text": "Loading the word vectors takes 7.4, 214, 459, and 1184 ms, for batch sizes of 1, 32, 64, and 128, respectively, on the MacBook Pro.", "labels": [], "entities": [{"text": "MacBook Pro", "start_pos": 119, "end_pos": 130, "type": "DATASET", "confidence": 0.9013579785823822}]}, {"text": "As explained previously, using IndexedDB requires a one-time download of the word vectors and the model weights.", "labels": [], "entities": []}, {"text": "This step takes approximately 16s on our MacBook Pro for 16,271 word vectors (for simplicity, we only download the vocabulary needed for our experiments).", "labels": [], "entities": [{"text": "MacBook Pro", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9000773429870605}]}, {"text": "Loading the model itself takes approximately one second.", "labels": [], "entities": []}, {"text": "Because our implementation is in JavaScript, our model runs in any device that has a web browser.", "labels": [], "entities": []}, {"text": "To demonstrate this, we evaluated performance on a number of other devices we had convenient access to: an iPad Pro with an Apple A10X Fusion chip, a Nexus 6P with a Qualcomm Snapdragon 810 octa-core CPU, and an iPhone 6 with an Apple A8 chip.", "labels": [], "entities": []}, {"text": "These results are also shown in.", "labels": [], "entities": []}, {"text": "As expected, performance on these devices is lower than our laptop, but interestingly, batch inference on these devices is faster than batch inference in the browser without GPU acceleration.", "labels": [], "entities": []}, {"text": "This indicates that hardware acceleration is a standard feature on many devices today.", "labels": [], "entities": [{"text": "hardware acceleration", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.6861231923103333}]}, {"text": "These experiments illustrate the feasibility of deploying neural networks on a wide range of devices, exploiting the ubiquity of JavaScript.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Latency of our CNN running in Chrome on  different devices for a batch of N sentences.", "labels": [], "entities": [{"text": "Latency", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9953858256340027}]}]}