{"title": [{"text": "A Generalized Knowledge Hunting Framework for the Winograd Schema Challenge", "labels": [], "entities": [{"text": "Generalized Knowledge Hunting", "start_pos": 2, "end_pos": 31, "type": "TASK", "confidence": 0.8216525713602701}, {"text": "Winograd Schema", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.8926099240779877}]}], "abstractContent": [{"text": "The Winograd Schema Challenge is a popular alternative Turing test, comprising a binary-choice coreference-resolution task that requires significant common-sense and world knowledge to solve.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel framework that successfully resolves many Winograd questions while imposing minimal restrictions on their form and difficulty.", "labels": [], "entities": []}, {"text": "Our method works by (i) generating queries from a parsed representation of a Winograd question, (ii) acquiring relevant knowledge using Information Retrieval, and (iii) reasoning on the gathered knowledge.", "labels": [], "entities": []}, {"text": "Our approach improves the F1 performance by 0.16 over previous works, without task-specific supervised training.", "labels": [], "entities": [{"text": "F1", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9665501713752747}]}], "introductionContent": [{"text": "The Winograd Schema Challenge (WSC) has emerged as a popular alternative to the Turing test as a means to measure progress towards humanlike artificial intelligence ().", "labels": [], "entities": [{"text": "Winograd Schema Challenge (WSC)", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.6017472793658575}]}, {"text": "WSC problems are short passages containing a target pronoun that must be correctly resolved to one of two possible antecedents.", "labels": [], "entities": [{"text": "WSC", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.917444109916687}]}, {"text": "They come in pairs which differ slightly and result in different correct resolutions.", "labels": [], "entities": [{"text": "correct resolutions", "start_pos": 65, "end_pos": 84, "type": "METRIC", "confidence": 0.8506799638271332}]}, {"text": "As an example: (1) a.", "labels": [], "entities": []}, {"text": "Jim yelled at Kevin because he was so upset.", "labels": [], "entities": []}, {"text": "(Answer: Jim) b.", "labels": [], "entities": []}, {"text": "Jim comforted Kevin because he was so upset.", "labels": [], "entities": []}, {"text": "(Answer: Kevin) WSC problem pairs (\"twins,\" using the terminology of) are carefully controlled such that heuristics involving syntactic salience, the number and gender of the antecedent, or other simple syntactic and semantic cues are ineffective.", "labels": [], "entities": [{"text": "WSC problem pairs", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.8877686858177185}]}, {"text": "This distinguishes the task from the standard coreference resolution problem.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.8970894515514374}]}, {"text": "Performant systems must make common-sense inferences; i.e., that someone who yells is likely to be upset, and that someone who is upset tends to be comforted.", "labels": [], "entities": []}, {"text": "Additional examples are shown in.", "labels": [], "entities": []}, {"text": "WSC problems are simple for people to solve but difficult for automatic systems because common-sense reasoning encompasses many types of reasoning (causal, spatio-temporal, etc.) and requires a wide breadth of knowledge.", "labels": [], "entities": [{"text": "WSC", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8953178524971008}]}, {"text": "There have been efforts to encode such knowledge directly, using logical formalisms ( or by using deep learning models (; however, these approaches have so far solved only restricted subsets of WSC questions with high precision, and show limited ability to generalize to new instances.", "labels": [], "entities": [{"text": "WSC questions", "start_pos": 194, "end_pos": 207, "type": "TASK", "confidence": 0.837579995393753}, {"text": "precision", "start_pos": 218, "end_pos": 227, "type": "METRIC", "confidence": 0.9620198011398315}]}, {"text": "Other work aims to develop a repository of common-sense knowledge (e.g.,, ConceptNet ()) using semi-automatic methods.", "labels": [], "entities": []}, {"text": "These knowledge bases are necessarily incomplete and further processing is required to retrieve the entries relevant to a given WSC context.", "labels": [], "entities": [{"text": "WSC context", "start_pos": 128, "end_pos": 139, "type": "TASK", "confidence": 0.7230812311172485}]}, {"text": "Even given the appropriate entries, further reasoning operations must usually be performed as in;.", "labels": [], "entities": []}, {"text": "In this work we propose a three-stage knowledge hunting method for solving the WSC.", "labels": [], "entities": [{"text": "knowledge hunting", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7695827186107635}, {"text": "solving the WSC", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7664687633514404}]}, {"text": "We hypothesize that on-the-fly, large-scale processing of textual data can complement knowledge engineering efforts to automate common-sense reasoning.", "labels": [], "entities": []}, {"text": "In this view, information that appears in natural text can act as implicit or explicit evidence for the truth of candidate WSC resolutions.", "labels": [], "entities": [{"text": "WSC resolutions", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.8049770891666412}]}, {"text": "There are several challenges inherent to such an approach.", "labels": [], "entities": []}, {"text": "First, WSC instances are explicitly designed to be robust to the type of statistical correlations that underpin modern distributional lexical semantics.", "labels": [], "entities": [{"text": "WSC", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9351744055747986}]}, {"text": "In the example above, yelled at and comforted are both similar to upset, so it is difficult to distinguish the two cases by lexical similarity.", "labels": [], "entities": []}, {"text": "Also, commonsense involves background 1 a) The man couldn't lift his son because he was so weak.", "labels": [], "entities": [{"text": "commonsense", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.7505363821983337}]}, {"text": "(Answer: the man) 1 b) The man couldn't lift his son because he was so heavy.", "labels": [], "entities": []}, {"text": "(Answer: son) 2 a) The older students were bullying the younger ones, so we punished them.", "labels": [], "entities": []}, {"text": "(Answer: the older students) 2 a) The older students were bullying the younger ones, so we rescued them.", "labels": [], "entities": []}, {"text": "(Answer: the younger ones) 3 a) Sam tried to paint a picture of shepherds with sheep, but they ended up looking more like golfers.", "labels": [], "entities": []}, {"text": "3 b) Sam tried to paint a picture of shepherds with sheep, but they ended up looking more like dogs.", "labels": [], "entities": []}, {"text": "(Answer: sheep) knowledge that is, by definition, shared by most readers.", "labels": [], "entities": []}, {"text": "Common sense is thus assumed knowledge that is rarely stated explicitly in naturally occurring text.", "labels": [], "entities": []}, {"text": "As such, even modern NLP corpora composed of billions of word tokens, like Gigaword ( and Google News (http://news.google.com), are unlikely to offer good coverage -or if they do, instances of specific knowledge are likely to be diffuse and rare (\"long tail\").", "labels": [], "entities": [{"text": "Google News", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.8897633850574493}]}, {"text": "Information Retrieval (IR) techniques can sidestep some of these issues by using the entire indexed Internet as an input corpus.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8388587415218354}]}, {"text": "In particular, our method of knowledge hunting aims to retrieve scenarios that are similar to a given WSC question but where the ambiguities built into the question are absent.", "labels": [], "entities": [{"text": "knowledge hunting", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8245652914047241}]}, {"text": "For example, to solve (1a), the following search result contains the relevant knowledge without the matching ambiguity: (2) I got really upset with her and I started to yell at her because...", "labels": [], "entities": []}, {"text": "Here, the same entity I is the subject of both upset and yell at, which is strong evidence for resolving the original ambiguity.", "labels": [], "entities": []}, {"text": "This information can be extracted from a syntactic parse of the passage using standard NLP tools.", "labels": [], "entities": []}, {"text": "Previous work on end-to-end knowledgehunting mechanisms for the WSC includes a recent framework that compares query counts of evidence retrieved online for the competing antecedents (.", "labels": [], "entities": [{"text": "WSC", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.4807130992412567}]}, {"text": "That framework's coverage is restricted to a small subset of the Winograd instances based on knowledge constraints.", "labels": [], "entities": [{"text": "Winograd instances", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.9409647881984711}]}, {"text": "In contrast, our approach covers a much larger subset of WSC passages and is impartial to knowledge constraints.", "labels": [], "entities": [{"text": "WSC passages", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.6176833808422089}]}, {"text": "Our framework adopts a novel representation schema that achieves significant coverage on Winograd instances, as well as an antecedent selection process that considers the evidence strength of the knowledge retrieved to make a more precise coreference decision.", "labels": [], "entities": []}, {"text": "Our method achieves a balanced F1 of 0.46 on the WSC, which significantly improves over the previous state-of-the-art of 0.3.", "labels": [], "entities": [{"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.999575674533844}, {"text": "WSC", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9338551759719849}]}, {"text": "We will also discuss the importance of F1 as a basis for comparing systems on the WSC, since it prevents overspecifying systems to perform well on certain WSC instances (boosting precision at the cost of recall).", "labels": [], "entities": [{"text": "F1", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9992620348930359}, {"text": "WSC", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.8602129220962524}, {"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.998835027217865}, {"text": "recall", "start_pos": 204, "end_pos": 210, "type": "METRIC", "confidence": 0.9984073042869568}]}], "datasetContent": [{"text": "We tested three versions of our framework (varying in the method of query generation: automatic vs. automatic with synonyms vs. manual) on the original 273 Winograd sentences (135 pairs and one triple).", "labels": [], "entities": [{"text": "query generation", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.7137814313173294}]}, {"text": "We compared these systems with previous work on the basis of Precision (P), Recall (R), and F1, where precision is the fraction of correctly answered instances among answered instances, recall is the fraction of correctly answered instances among all instances, and We used Stanford CoreNLP's coreference resolver () during query generation to identify the predicates from the syntactic parse, as well as during antecedent selection to retrieve the coreference chain of a candidate evidence sentence.", "labels": [], "entities": [{"text": "Precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.90464848279953}, {"text": "Recall (R)", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9345593601465225}, {"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9994046688079834}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9987553358078003}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9988767504692078}, {"text": "query generation", "start_pos": 324, "end_pos": 340, "type": "TASK", "confidence": 0.7247768342494965}]}, {"text": "Python's Selenium package was used for web-scraping and Bing-USA and Google (top two pages per result) were the search engines (we unioned all results).", "labels": [], "entities": []}, {"text": "The search results comprise a list of document snippets that contain the queries (for example, \"yelled at\" and \"upset\").", "labels": [], "entities": []}, {"text": "We then extract the sentence/s within each snippet that contain the query terms (with the added restriction that the terms should be within 70 characters of each other to ensure relevance).", "labels": [], "entities": []}, {"text": "For example, for the queries \"yelled at\" and \"upset\", one snippet is: \"Once the football players left the car, she testified that she yelled at the girl because she was upset with her actions from the night before.\"", "labels": [], "entities": []}, {"text": "In the next section we compare the performance of our framework with the most recent automatic system that tackles the original WSC (Sharma et al., 2015) (S2015).", "labels": [], "entities": [{"text": "WSC (Sharma et al., 2015) (S2015)", "start_pos": 128, "end_pos": 161, "type": "DATASET", "confidence": 0.5324805974960327}]}, {"text": "In addition to P/R/F1, we also compare systems' evidence coverage, by which we mean the number of Winograd questions for which evidence sentences are retrieved by the search engine.", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.6700733304023743}]}, {"text": "This should not be conflated with the schemal coverage of our system, by which we mean the number of Winograd questions that syntactically obey Class A (85% of the Winograd questions).", "labels": [], "entities": []}, {"text": "Our system is designed specifically to resolve these Class A questions.", "labels": [], "entities": []}, {"text": "We nevertheless test on the remaining 15% in our experiments.", "labels": [], "entities": []}, {"text": "Although other systems for the WSC exist outside of S2015, their results are not directly comparable to ours for one or more of the following reasons: a) they are directed towards solving the larger, easier dataset; b) they are not entirely automatic; or c) they are designed fora much smaller, author-selected subset of the WSC.", "labels": [], "entities": [{"text": "WSC", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.8473058342933655}]}, {"text": "We elaborate on this point in Section 5.", "labels": [], "entities": []}, {"text": "Our results show that the framework using manually generated queries (MGQ) performs best, with an F1 of 0.50.", "labels": [], "entities": [{"text": "F1", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9991282820701599}]}, {"text": "We emphasize here that the promise of our approach lies mainly in its generality, shown in its improved coverage of the original problem set: it produces an answer for 70% of the instances.", "labels": [], "entities": []}, {"text": "This coverage surpasses previous methods, which only admit specific instance types, by nearly 50%.", "labels": [], "entities": []}, {"text": "The random baseline on this task achieves a P/R/F1 of .5.", "labels": [], "entities": [{"text": "P/R/F1", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.8823320031166076}]}, {"text": "We could artificially raise the F1 performance of all systems to be above .5 by randomly guessing an answer in cases where the system makes no decision.", "labels": [], "entities": [{"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9995315074920654}]}, {"text": "We chose not to do this so that automatic systems are compared transparently based on when they decide to make a prediction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Coverage and performance on the  Winograd Challenge (273 sentences). The best  system on each measure is shown in bold.", "labels": [], "entities": [{"text": "Winograd Challenge", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.9007004499435425}]}]}