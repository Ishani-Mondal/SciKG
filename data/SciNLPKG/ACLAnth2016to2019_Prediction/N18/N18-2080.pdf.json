{"title": [{"text": "Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models", "labels": [], "entities": [{"text": "Adapting Neural Machine Translation", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.8589398711919785}]}], "abstractContent": [{"text": "In this paper we explore the use of Learning Hidden Unit Contribution for the task of neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.6455952425797781}]}, {"text": "The method was initially proposed in the context of speech recognition for adapting a general system to the specific acoustic characteristics of each speaker.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7594090700149536}]}, {"text": "Similar in spirit, in a machine translation framework we want to adapt a general system to a specific domain.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.6971689760684967}]}, {"text": "We show that the proposed method achieves improvements of up to 2.6 BLEU points over a general system , and up to 6 BLEU points if the initial system has only been trained on out-of-domain data, a situation which may easily happen in practice.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9984859824180603}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9983465671539307}]}, {"text": "The good performance together with its short training time and small memory footprint make it a very attractive solution for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.8608995378017426}]}], "introductionContent": [{"text": "Domain adaptation for neural machine translation (NMT) is starting to get more attention from the scientific community.", "labels": [], "entities": [{"text": "Domain adaptation for neural machine translation (NMT)", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8143363462554084}]}, {"text": "Often researchers and machine translation practitioners want to improve the performance of their systems on a domain for which they were not explicitly optimized.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7270345389842987}]}, {"text": "Due to the high training times needed to develop NMT systems, often up to several weeks, efficient methods for improving an existing system fora specific domain can have important practical applications.", "labels": [], "entities": []}, {"text": "In this paper we review \"Learning Hidden Unit Contribution\" ( \u00a7 3), a method developed initially for speaker adaptation in speech recognition systems, and apply it to the task of neural machine translation ( \u00a7 \u00a7 4 and 5).", "labels": [], "entities": [{"text": "speaker adaptation in speech recognition", "start_pos": 101, "end_pos": 141, "type": "TASK", "confidence": 0.6602332115173339}, {"text": "neural machine translation", "start_pos": 179, "end_pos": 205, "type": "TASK", "confidence": 0.7128183444341024}]}, {"text": "We show that it improves translation quality on in-domain data, while at the same timekeeping the translation quality of outof-domain data intact).", "labels": [], "entities": [{"text": "translation", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.9532022476196289}]}, {"text": "Due to its small memory footprint and short training time it can be realistically applied to adapt large, general domain systems in order to improve their performance on specific domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "Similar to, we present results on the IWSLT 2016 2 German to English TED dataset (, consisting of transcribed and translated TED talks.", "labels": [], "entities": [{"text": "IWSLT 2016 2 German to English TED dataset", "start_pos": 38, "end_pos": 80, "type": "DATASET", "confidence": 0.9112719371914864}]}, {"text": "As out-of-domain data we use the same year's WMT data (.", "labels": [], "entities": [{"text": "WMT data", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9318625628948212}]}, {"text": "We report results on the TED 2013 and TED 2014 (the newest ones with provided references) and additionally on the newstest 2016 dataset for measuring the performance on out-of-domain data.", "labels": [], "entities": [{"text": "TED 2013", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9378548860549927}, {"text": "TED 2014", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.722878098487854}, {"text": "newstest 2016 dataset", "start_pos": 114, "end_pos": 135, "type": "DATASET", "confidence": 0.939921498298645}]}, {"text": "Statistics for the training corpora are given in.", "labels": [], "entities": []}, {"text": "It can be seen that the WMT data (out-of-domain) is an order of magnitude bigger that the in-domain IWSLT data.", "labels": [], "entities": [{"text": "WMT data", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.7079052627086639}, {"text": "IWSLT data", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.8945279121398926}]}, {"text": "Our system is a recurrent encoder-decoder NMT model, with one bidirectional LSTM layer with 1024 units in the encoder and one layer with 1024 units in the decoder.", "labels": [], "entities": []}, {"text": "The data has been BPEencoded using 32K merge operations, and the em-bedding layer has a dimension of 512.", "labels": [], "entities": [{"text": "BPEencoded", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.7604901790618896}]}, {"text": "Training has been performed with the Adam algorithm ().", "labels": [], "entities": []}, {"text": "The provided TED dev set was used as stopping criterion (or newstest14 for the case of a WMT-only system).", "labels": [], "entities": [{"text": "TED dev set", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.6323821246623993}]}, {"text": "Experiments have been carried out using Sockeye (, and the LHUC code has been open sourced as part of it.", "labels": [], "entities": [{"text": "Sockeye (", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.9615432322025299}, {"text": "LHUC code", "start_pos": 59, "end_pos": 68, "type": "DATASET", "confidence": 0.8118628561496735}]}, {"text": "For LHUC experiments, both the encoder and decoder hidden units have been expanded with the additional scaling.", "labels": [], "entities": []}, {"text": "As discussed in Section 5 we will differentiate two conditions: in the \"full training data\" condition, both the out-of-domain and in-domain data are available for training the initial system.", "labels": [], "entities": []}, {"text": "In the \"growing training data\" condition, the initial system is trained only on out-of-domain data.", "labels": [], "entities": []}, {"text": "We will compare the performance of the LHUC method with the \"continuation of training\" proposed by.", "labels": [], "entities": []}, {"text": "Both methods can start from an already trained system and refine the training on the in-domain data.", "labels": [], "entities": []}, {"text": "For the full training data condition we also explore the tagging technique similar to the one proposed by.", "labels": [], "entities": [{"text": "tagging", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.9738735556602478}]}], "tableCaptions": [{"text": " Table 2: BLEU scores [%] for the Full Training Data condition. \"OOD\" denotes out-of-domain data, \"ID\" denotes  in-domain data. For LHUC results, the number in parenthesis shows the result of applying the adapted system to  the out-of-domain data (which would not be applied in practice).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994243383407593}, {"text": "Full Training Data", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.5751323401927948}, {"text": "OOD", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9914878606796265}]}, {"text": " Table 3: BLEU scores [%] for the Growing Training Data condition.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990596175193787}, {"text": "Growing Training Data condition", "start_pos": 34, "end_pos": 65, "type": "DATASET", "confidence": 0.722676619887352}]}]}