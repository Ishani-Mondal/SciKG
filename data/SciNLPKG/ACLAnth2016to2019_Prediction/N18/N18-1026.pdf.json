{"title": [{"text": "Tempo-Lexical Context driven Word Embedding for Cross-Session Search Task Extraction", "labels": [], "entities": [{"text": "Cross-Session Search Task Extraction", "start_pos": 48, "end_pos": 84, "type": "TASK", "confidence": 0.7947403192520142}]}], "abstractContent": [{"text": "Search task extraction in information retrieval is the process of identifying search intents over a set of queries relating to the same topical information need.", "labels": [], "entities": [{"text": "Search task extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6415191690127054}, {"text": "information retrieval", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.6839966177940369}]}, {"text": "Search tasks may potentially span across multiple search sessions.", "labels": [], "entities": []}, {"text": "Most existing research on search task extraction has focused on identifying tasks within a single session, where the notion of a session is defined by a fixed length time window.", "labels": [], "entities": [{"text": "search task extraction", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.6442111531893412}]}, {"text": "By contrast , in this work we seek to identify tasks that span across multiple sessions.", "labels": [], "entities": []}, {"text": "To identify tasks, we conduct a global analysis of a query login its entirety without restricting analysis to individual temporal windows.", "labels": [], "entities": []}, {"text": "To capture inherent task semantics, we represent queries as vectors in an abstract space.", "labels": [], "entities": []}, {"text": "We learn the embedding of query words in this space by leveraging the temporal and lexical contexts of queries.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of the proposed query embedding, we conduct experiments of clustering queries into tasks with a particular interest of measuring the cross-session search task recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 189, "end_pos": 195, "type": "METRIC", "confidence": 0.8520857095718384}]}, {"text": "Results of our experiments demonstrate that task extraction effectiveness, including cross-session recall , is improved significantly with the help of our proposed method of embedding the query terms by leveraging the temporal and temp-lexical contexts of queries.", "labels": [], "entities": [{"text": "task extraction", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.7071207165718079}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.964449942111969}]}], "introductionContent": [{"text": "A complex search task is defined as a \"a multiaspect or a multi-step information need consisting of a set of related subtasks, each of which might recursively be complex\").", "labels": [], "entities": []}, {"text": "For example, a task of making arrangements for travel to a conference qualifies as a complex search task because there are several choices that a user needs to make in order to plan his entire trip, e.g. selecting flight, hotel, making arrangements for local transport, finding the conference venue, finding good places to eat around, finding local sight-seeing options after the conference etc.", "labels": [], "entities": []}, {"text": "All these sub-tasks are likely to take place within their own search sessions, where a session is defined as a set comprised of queries executed during a time period of a specific length, usually about half-an-hour (.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of automatically predicting whether search sessions, focused on specific activities, area part of a broader complex search task, which we refer to as the cross-session search task extraction problem.", "labels": [], "entities": [{"text": "cross-session search task extraction", "start_pos": 192, "end_pos": 228, "type": "TASK", "confidence": 0.7305258810520172}]}, {"text": "Cross-session search task extraction can potentially find applications in designing more proactive search engines, which may suggest relevant information about specific subsequent subtasks along a timeline, e.g. suggesting places to eat around a conference venue without the user needing to execute these queries.", "labels": [], "entities": [{"text": "Cross-session search task extraction", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6756129935383797}]}, {"text": "To see why cross-session search task extraction is a challenging problem, firstly, note that it is likely that a query session for flight booking and one for local sightseeing around a conference venue maybe far apart in time, as a result of which simple approaches of grouping queries by their timestamps, e.g. (, are not likely to yield satisfactory outcomes.", "labels": [], "entities": [{"text": "cross-session search task extraction", "start_pos": 11, "end_pos": 47, "type": "TASK", "confidence": 0.8355287164449692}]}, {"text": "Secondly, the term overlap between the queries of these two sessions is also likely to below, indicating that using lexical similarity for clustering cross-session queries into a single group, e.g. (, is unlikely to be effective.", "labels": [], "entities": [{"text": "overlap", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9465181827545166}]}, {"text": "As an illustrative example of term mismatch, consider the two queries 'Eric Harris', 'Reb Vodka' from the AOL query log . Although these two queries do not share any common terms between them, they refer to the task of finding infor-mation on the Columbine high school massacre, the first query referring to the name of the first murderer while the second one refers to their nickname.", "labels": [], "entities": [{"text": "AOL query log", "start_pos": 106, "end_pos": 119, "type": "DATASET", "confidence": 0.8897533814112345}, {"text": "finding infor-mation on the Columbine high school massacre", "start_pos": 219, "end_pos": 277, "type": "TASK", "confidence": 0.5876860730350018}]}, {"text": "To alleviate the identified problems with attempting to group queries by their timestamps or lexical similarities, we propose to embed queries in a task-based semantic space in a manner that will give two similar queries in this space a high likelihood of pertaining to the same underlying task.", "labels": [], "entities": []}, {"text": "Word embedding algorithms, such as 'word2vec' ( , make use of the lexical context in learning vector representations of words.", "labels": [], "entities": []}, {"text": "We propose to transform these word vectors into a task-oriented semantic space with the objective of making two words that are likely to be apart of the same search task closer to each other.", "labels": [], "entities": []}, {"text": "To learn the transformation function, we make use of average session duration and lexical similarities between within-session queries.", "labels": [], "entities": []}, {"text": "Our method thus provides a unifying framework for addressing tempo-lexical similarity, in contrast to previous approaches that treat these two separately.", "labels": [], "entities": []}, {"text": "Another important contribution of our proposed method is that we are able to empirically demonstrate that our proposed method is more effective than existing algorithms () in extracting crosssession search tasks without the application of any external information for estimating task relatedness.", "labels": [], "entities": [{"text": "extracting crosssession search tasks", "start_pos": 175, "end_pos": 211, "type": "TASK", "confidence": 0.8202580958604813}]}, {"text": "For instance, the work in () relies on Wikipedia to contextualize queries, while the one in) uses Wikipedia-based entity recognition to estimate task relatedness.", "labels": [], "entities": [{"text": "Wikipedia-based entity recognition", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.5488957464694977}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we overview previous work in task extraction and query embedding.", "labels": [], "entities": [{"text": "task extraction", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7415070533752441}]}, {"text": "In Section 3, we introduce our semantic context driven transformation-based word vector embedding algorithm to enhance cross-session query similarity matching.", "labels": [], "entities": [{"text": "cross-session query similarity matching", "start_pos": 119, "end_pos": 158, "type": "TASK", "confidence": 0.7001583948731422}]}, {"text": "Section 4 then describes how the transformed query vectors are clustered into search tasks.", "labels": [], "entities": []}, {"text": "Section 5 describes our experimental setup.", "labels": [], "entities": []}, {"text": "Section 6 presents the results of our experiments.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper with suggestions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the setup for our experimental study.", "labels": [], "entities": []}, {"text": "We begin with an overview of our datasets, then introduce the experimental baselines used and the objectives of our experiments, finally we set out the parameter settings used in our experiments.", "labels": [], "entities": []}, {"text": "Similar to previous reported studies (, we use the AOL query log for our experiments.", "labels": [], "entities": [{"text": "AOL query log", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.655946284532547}]}, {"text": "In order to compare our results with these studies, we use the same subset of 1424 queries from the AOL query log for the evaluation of task extraction effectiveness as used in these earlier studies.", "labels": [], "entities": [{"text": "AOL query log", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.8708128531773885}, {"text": "task extraction", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.6967874467372894}]}, {"text": "However, since the purpose of these studies was only to extract tasks from a single session, in its annotation scheme two queries only qualified as part of the same task if they appeared within the same session.", "labels": [], "entities": []}, {"text": "In contrast, since we investigate cross-session task extraction, the time length threshold is not applicable to our annotation scheme, as a result of which, we re-annotated the task labelled dataset of ().", "labels": [], "entities": [{"text": "cross-session task extraction", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.6556685368220011}, {"text": "time length threshold", "start_pos": 69, "end_pos": 90, "type": "METRIC", "confidence": 0.824269692103068}]}, {"text": "In particular, our annotation scheme was solely based on the underlying search intent of the query.", "labels": [], "entities": []}, {"text": "While re-annotating the dataset of (Lucchese et al., 2013), the annotators were instructed not to change the task labels within each session.", "labels": [], "entities": []}, {"text": "Instead, the annotators were asked to re-label task identifiers spanning across different query sessions.", "labels": [], "entities": []}, {"text": "For example, the annotation of () considered 'robert f kennedy jr' and 'robert francis kennedy' to belong to two different tasks since these queries were executed during different sessions.", "labels": [], "entities": []}, {"text": "However, our annotation scheme considers them to be apart of the same task.", "labels": [], "entities": []}, {"text": "Two persons were employed to carryout our annotation step of the set of 1424 queries in two different batches.", "labels": [], "entities": []}, {"text": "They were asked to come to a consensus when trying to merge the task labels across their individual batches.", "labels": [], "entities": []}, {"text": "The annotators were instructed to use a commercial search engine (e.g. Google), if required, to determine if two queries from different search sessions could potentially relate to the same underlying task.", "labels": [], "entities": []}, {"text": "provides an overview of our annotated task labels; this shows that there area considerable number of sessions that contain queries spanning across session boundaries.", "labels": [], "entities": []}, {"text": "It can be seen from that after post-processing the single session task labels, the total number of distinct tasks is reduced.", "labels": [], "entities": []}, {"text": "This is indicative of the fact that the modified dataset: Dataset statistics of task annotated queries from the AOL query log.", "labels": [], "entities": [{"text": "AOL query log", "start_pos": 112, "end_pos": 125, "type": "DATASET", "confidence": 0.8537559906641642}]}, {"text": "Cross-session task labels are post-processed annotations of the dataset prepared by. is able to consider queries from different search sessions as apart of the same search task (there are 36, 768 of them as shown in).", "labels": [], "entities": []}, {"text": "The post-processed dataset with cross-session task labels that we use for our experiments is publicly available 3 .  Since our proposed task extraction method is unsupervised, fora fair comparison we only employ unsupervised approaches as baselines.", "labels": [], "entities": [{"text": "task extraction", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.7586371004581451}]}, {"text": "More specifically, we did not consider the supervised approaches reported in) as our baselines.", "labels": [], "entities": []}, {"text": "As our first baseline, we re-implemented QC-W CC , the best performing approach (Lucchese et al., 2013) (briefly described in Section 4.2).", "labels": [], "entities": []}, {"text": "This study investigated a wide range of features, clustering methods and parameter settings.", "labels": [], "entities": []}, {"text": "We adopt the same linear combination of similarities in our study as shown in Equation 5.", "labels": [], "entities": []}, {"text": "Our re-implementation of this work involves a slight change to the original one.", "labels": [], "entities": []}, {"text": "Instead of using a Wikipedia document collection, we employ a much larger collection of crawled web documents, namely the ClueWeb12B collection, comprising of nearly 52M documents 4 . Our reasons for using the ClueWeb collection are as follows.", "labels": [], "entities": [{"text": "ClueWeb12B collection", "start_pos": 122, "end_pos": 143, "type": "DATASET", "confidence": 0.8825578391551971}]}, {"text": "Firstly, our study is carried out using queries from a Web search log and hence it is reasonable to expect that a web collection will provide better estimates of semantic similarities between the queries.", "labels": [], "entities": []}, {"text": "Secondly, a number of our queries in our dataset are not of expository type, and hence the num-ber of matching Wikipedia articles is expected to below for them due to vocabulary mismatch.", "labels": [], "entities": []}, {"text": "On the other hand, the web collection, being diverse, is expected to retrieve more matching articles for these types of queries.", "labels": [], "entities": []}, {"text": "To compare the performance of our implementation of QC-W CC with ClueWeb12B with the original one, we adopted an experimental and evaluation setup identical to that of (), the only difference being in the collection used for deriving the semantic similarities.", "labels": [], "entities": []}, {"text": "For the retrieval model we used the LM-JM (Language Model with Jelineck-Mercer smoothing) with the smoothing parameter set to 0.6 as suggested in).", "labels": [], "entities": []}, {"text": "To demonstrate the potential benefits of our proposed tempo and tempo-lexical context-driven word embedding based approaches for the query terms, we employed the following three baselines.", "labels": [], "entities": []}, {"text": "1. Qry vec skip-gram: In this approach, query vectors were obtained by summing over the constituent word vectors obtained using the standard skip-gram ( ).", "labels": [], "entities": []}, {"text": "2. Qry vec (All-in-one Session Context): We hypothesized that additional context is likely to capture task-specific semantics of the query terms.", "labels": [], "entities": []}, {"text": "A boundary condition arises when the entire query log is assumed to belong to one session.", "labels": [], "entities": []}, {"text": "To show that the temporal context needs to be focused, in this approach, we investigate the effect of setting the context set S to the entire vocabulary of query terms.", "labels": [], "entities": []}, {"text": "3. Qry vec (Pre-trained Google news vectors) We hypothesized that additional context is likely to be useful to learn the vector representations of constituent words of short documents (in this case, queries).", "labels": [], "entities": []}, {"text": "To see if pretrained word vectors from an external generic corpus can be useful to alleviate the problem of short documents, we employ pretrained word vectors from the Google news corpus to obtain the vector representation of the queries.", "labels": [], "entities": [{"text": "Google news corpus", "start_pos": 168, "end_pos": 186, "type": "DATASET", "confidence": 0.735500305891037}]}, {"text": "The objective of the experiments is to show that our proposed query term embedding method can outperform the above mentioned baselines, thus indicating that within-session adjacency information can be useful to learn task specific semantics.", "labels": [], "entities": []}, {"text": "In our method, we use the cosine similarity between the embedded query vectors instead of using character 3-grams and Levenshtein similarity, as used in (), to compute Sim c (q i , q j ) between any two query pairs q i and q j . We employ three different embedding strategies for our experiments: i) standard word2vec, ii) transformed vectors with temporal context, and iii) transformed vectors with tempolexical contexts.", "labels": [], "entities": []}, {"text": "In all our experiments, we tune \u03b1 from 0 to 1 in steps of 0.1.", "labels": [], "entities": []}, {"text": "The second parameter common to all the methods, the threshold \u03b7, which is used in QC-W CC clustering to prune off edges from the weighted similarity graph between query pairs.", "labels": [], "entities": [{"text": "QC-W CC clustering", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.6566311120986938}]}, {"text": "We tuned \u03b7 in the range 0.1 to 1 in steps of 0.1 for each method separately.", "labels": [], "entities": []}, {"text": "For our word vector based experiments, we used the skip-gram model to train the word vectors using the entire AOL query log comprising over 6M queries.", "labels": [], "entities": [{"text": "AOL query log", "start_pos": 110, "end_pos": 123, "type": "DATASET", "confidence": 0.746907631556193}]}, {"text": "The dimensionality of the word vectors was set to 200.", "labels": [], "entities": []}, {"text": "The initially obtained word vectors were used as starting inputs to learn the temporal and tempo-lexical transformations.", "labels": [], "entities": []}, {"text": "For the tempolexical based transformation method, we used the optimal value of \u03b7 as obtained from the QC-W CC baseline, to cluster the queries in each temporal window of 26 minutes.", "labels": [], "entities": [{"text": "QC-W CC baseline", "start_pos": 102, "end_pos": 118, "type": "DATASET", "confidence": 0.9228599866231283}]}, {"text": "Since we use weighted clustering to extract cross-session search tasks, we used standard clustering evaluation metrics to evaluate the effectiveness of the task extraction.", "labels": [], "entities": []}, {"text": "Clustering is typically evaluated with the effectiveness of the pair-wise decisions of assigning data points to the same or different clusters.", "labels": [], "entities": []}, {"text": "In our case, the number of true positives was given by the number of query pairs in the ground-truth that were judged to belong to the same task and were also predicted by the system to be apart of the same task.", "labels": [], "entities": []}, {"text": "Similarly, we computed the false positives and the true negatives.", "labels": [], "entities": []}, {"text": "Based on these counts, we computed the standard metrics of precision, recall, and F-score (similar to ().", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9997324347496033}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9996151924133301}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9994631409645081}]}, {"text": "Additionally, to measure how many of the total number of cross-session queries that were part of the same search tasks were discovered by these approaches, we computed the cross-session recall (denoted as 'CS-Recall').", "labels": [], "entities": [{"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9378575682640076}]}, {"text": "This metric was computed as the ratio of the number of correctly identified cross-session similar-task query pairs against   the total number of them (36, 768 as reported in).", "labels": [], "entities": []}, {"text": "In order to extend evaluation of our proposed approach to within-session task extraction, for comparison with existing studies, we computed the clustering metrics for each individual session and then computed the weighted average of these values over each session as reported in.", "labels": [], "entities": [{"text": "within-session task extraction", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.6360864440600077}]}, {"text": "Although these earlier studies refer to this weighted measure as F-score, we refer to this version of Fscore as 'Session-F-score'.", "labels": [], "entities": [{"text": "F-score", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9833794236183167}, {"text": "Fscore", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9414835572242737}]}], "tableCaptions": [{"text": " Table 1: Dataset statistics of task annotated queries  from the AOL query log. Cross-session task labels are  post-processed annotations of the dataset prepared by", "labels": [], "entities": [{"text": "AOL query log", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.8553840716679891}]}, {"text": " Table 2: Comparison between the best results obtained after parameter tuning on different unsupervised approaches", "labels": [], "entities": []}, {"text": " Table 3: Within-session Task Extraction effectiveness.", "labels": [], "entities": [{"text": "Within-session Task Extraction", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.5404472847779592}]}]}