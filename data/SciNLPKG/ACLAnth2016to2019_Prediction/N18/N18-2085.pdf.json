{"title": [{"text": "Are All Languages Equally Hard to Language-Model?", "labels": [], "entities": []}], "abstractContent": [{"text": "For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles?", "labels": [], "entities": []}, {"text": "In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information.", "labels": [], "entities": []}, {"text": "We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models.", "labels": [], "entities": []}, {"text": "We show complex inflectional morphology to be a cause of performance differences among languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern natural language processing practitioners strive to create modeling techniques that work well on all of the world's languages.", "labels": [], "entities": []}, {"text": "Indeed, most methods are portable in the following sense: Given appropriately annotated data, they should, in principle, be trainable on any language.", "labels": [], "entities": []}, {"text": "However, despite this crude cross-linguistic compatibility, it is unlikely that all languages are equally easy, or that our methods are equally good at all languages.", "labels": [], "entities": []}, {"text": "In this work, we probe the issue, focusing on language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.8032238185405731}]}, {"text": "A fair comparison is tricky.", "labels": [], "entities": []}, {"text": "Training corpora in different languages have different sizes, and reflect the disparate topics of discussion in different linguistic communities, some of which maybe harder to predict than others.", "labels": [], "entities": []}, {"text": "Moreover, bits per character, a standard metric for language modeling, depends on the vagaries of a given orthographic system.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.6973549723625183}]}, {"text": "We argue fora fairer metric based on the bits per utterance using utterance-aligned multi-text.", "labels": [], "entities": []}, {"text": "That is, we train and test on \"the same\" set of utterances in each language, modulo translation.", "labels": [], "entities": []}, {"text": "To avoid discrepancies in out-of-vocabulary handling, we evaluate open-vocabulary models.", "labels": [], "entities": []}, {"text": "We find that under standard approaches, text tends to be harder to predict in languages with finegrained inflectional morphology.", "labels": [], "entities": []}, {"text": "Specifically, language models perform worse on these languages, in our controlled comparison.", "labels": [], "entities": []}, {"text": "Furthermore, this performance difference essentially vanishes when we remove the inflectional markings.", "labels": [], "entities": []}, {"text": "Thus, in highly inflected languages, either the utterances have more content or the models are worse.", "labels": [], "entities": []}, {"text": "(1) Text in highly inflected languages maybe inherently harder to predict (higher entropy per utterance) if its extra morphemes carry additional, unpredictable information.", "labels": [], "entities": []}, {"text": "Alternatively, perhaps the extra morphemes are predictable in principlefor example, redundant marking of grammatical number on both subjects and verbs, or marking of object case even when it is predictable from semantics or word order-and yet our current language modeling technology fails to predict them.", "labels": [], "entities": []}, {"text": "This might happen because (2a) the technology is biased toward modeling words or characters and fails to discover intermediate morphemes, or because (2b) it fails to capture the syntactic and semantic predictors that govern the appearance of the extra morphemes.", "labels": [], "entities": []}, {"text": "We leave it to future work to tease apart these hypotheses.", "labels": [], "entities": []}], "datasetContent": [{"text": "Effecting a cross-linguistic study on LMs is complicated because different models could be trained and tested on incomparable corpora.", "labels": [], "entities": []}, {"text": "To avoid this problem, we use multi-text: k-way translations of the same semantic content.", "labels": [], "entities": []}, {"text": "As indicate, increasing the number of parameters may allow us to achieve better performance.", "labels": [], "entities": []}, {"text": "What's wrong with bits per character?", "labels": [], "entities": []}, {"text": "Openvocabulary language modeling is most commonly evaluated under bits per character (BPC) = 1 |c|+1 |c|+1 i=1 log p(c i | c <i ).", "labels": [], "entities": [{"text": "Openvocabulary language modeling", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6194386382897695}, {"text": "BPC)", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.8402979075908661}]}, {"text": "Even with multitext, comparing BPC is not straightforward, as it relies on the vagaries of individual writing systems.", "labels": [], "entities": [{"text": "BPC", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.7944297790527344}]}, {"text": "Consider, for example, the difference in how Czech and German express the phoneme /tS /: Czech use\u0161use\u0161 c, whereas German tsch.", "labels": [], "entities": []}, {"text": "Now, consider the Czech word pu\u010d and its German equivalent Putsch.", "labels": [], "entities": []}, {"text": "Even if these words are both predicted with the same probability in a given context, German will end up with a lower BPC.", "labels": [], "entities": [{"text": "BPC", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9863873720169067}]}, {"text": "Multi-text allows us to compute a fair metric that is invariant to the orthographic (or phonological) changes discussed above: bits per English character (BPEC).", "labels": [], "entities": [{"text": "BPEC)", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.8984333276748657}]}, {"text": "A Potential Confound: Translationese.", "labels": [], "entities": []}, {"text": "Working with multi-text, however, does introduce anew bias: all of the utterances in the corpus have a source language and 20 translations of that source utterance into target languages.", "labels": [], "entities": []}, {"text": "The characteristics of translated language has been widely studied and exploited, with one prominent characteristic of translations being simplification.", "labels": [], "entities": []}, {"text": "Note that a significant fraction of the original utterances in the corpus are English.", "labels": [], "entities": []}, {"text": "Our analysis may then have underestimated the BPEC for other languages, to the extent that their sentences consist of simplified \"translationese.\"", "labels": [], "entities": [{"text": "BPEC", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.6730033159255981}]}, {"text": "Even so, English had the lowest BPEC from among the set of languages.", "labels": [], "entities": [{"text": "BPEC", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9990177154541016}]}, {"text": "Our experiments are conducted on the 21 languages of the Europarl corpus ().", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.9873092174530029}]}, {"text": "The corpus consists of utterances made in the European parliament and are aligned cross-linguistically by a unique utterance id.", "labels": [], "entities": []}, {"text": "With the exceptions (noted in) of Finnish, Hungarian and Estonian, which are Uralic, the languages are Indo-European.", "labels": [], "entities": []}, {"text": "While Europarl does not contain quite our desired breadth of typological diversity, it serves our purpose by providing large collections of aligned data across many languages.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 6, "end_pos": 14, "type": "DATASET", "confidence": 0.9701050519943237}]}, {"text": "To create our experimental data, we extract all utterances and randomly sort them into train-development-test splits such that roughly 80% of the data are in train and 10% in development and test, respectively.", "labels": [], "entities": []}, {"text": "We also perform experiments on lemmatized text, where we replace every word with its lemma using the UDPipe toolkit (, stripping away its inflectional morphology.", "labels": [], "entities": []}, {"text": "We report two evaluation metrics: BPC and BPEC (see \u00a73).", "labels": [], "entities": [{"text": "BPC", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.762292206287384}, {"text": "BPEC", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.6355348825454712}]}, {"text": "Our BPEC measure always normalizes by the length of the original, not lemmatized, English.", "labels": [], "entities": [{"text": "BPEC measure", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.8857155442237854}]}, {"text": "Experimentally, we want to show: (i) When evaluating models in a controlled environment (multitext under BPEC), the models achieve lower performance on certain languages and (ii) inflectional morphology is the primary culprit for the performance differences.", "labels": [], "entities": [{"text": "BPEC", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.7993137836456299}]}, {"text": "However, we repeat that we do not in this paper tease apart whether the models are at fault, or that certain languages inherently encode more information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for all configurations and the typological", "labels": [], "entities": []}]}