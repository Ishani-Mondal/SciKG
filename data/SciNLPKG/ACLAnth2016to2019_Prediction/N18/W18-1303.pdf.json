{"title": [{"text": "Detecting Sarcasm is Extremely Easy ;-)", "labels": [], "entities": [{"text": "Detecting Sarcasm", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9114383459091187}]}], "abstractContent": [{"text": "Detecting sarcasm in text is a particularly challenging problem in computational semantics , and its solution may vary across different types of text.", "labels": [], "entities": [{"text": "Detecting sarcasm in text", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9171647429466248}]}, {"text": "We analyze the performance of a domain-general sarcasm detection system on datasets from two very different domains: Twitter, and Amazon product reviews.", "labels": [], "entities": [{"text": "domain-general sarcasm detection", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.6231362620989481}]}, {"text": "We categorize the errors that we identify with each, and make recommendations for addressing these issues in NLP systems in the future.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sarcasm detection is a tricky problem, even for humans.", "labels": [], "entities": [{"text": "Sarcasm detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9641053676605225}]}, {"text": "The definition of sarcasm is hazy, sarcasm can be heavily context-dependent, and it is often marked more by prosodic cues than syntactic characteristics, all of which make its computational detection particularly complex.", "labels": [], "entities": [{"text": "computational detection", "start_pos": 176, "end_pos": 199, "type": "TASK", "confidence": 0.711556687951088}]}, {"text": "Nonetheless, some researchers have achieved success in predicting whether or not instances of text contain sarcasm based on domain-specific features), sentiment (), text patterns (, and other semantic features (.", "labels": [], "entities": []}, {"text": "Since most prior work in this area has been domain-specific, the findings resulting from these models may not be broadly applicable.", "labels": [], "entities": []}, {"text": "For example, Twitter, a popular domain for sarcasm researchers, constrains posts to 140 (or as of very recently, 280) characters; this means that the type of sarcasm found in tweets maybe quite different from that found in a domain that allows lengthy posts, such as Amazon product reviews.", "labels": [], "entities": []}, {"text": "Previously, we explored this phenomenon by experimenting with various models to identify an approach better capable of learning domain-general sarcasm detection . In this paper, we build upon that work by conducting a performance analysis of our best-performing approach on two different text domains, and identifying common types of errors made by the system.", "labels": [], "entities": [{"text": "learning domain-general sarcasm detection", "start_pos": 119, "end_pos": 160, "type": "TASK", "confidence": 0.6307682618498802}]}, {"text": "We follow this with recommendations for improvement in future sarcasm detection systems.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.9799693822860718}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance of our sarcasm detection model  relative to prior work on the same datasets.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.9344715476036072}]}, {"text": " Table 3: Errors included in the analysis.", "labels": [], "entities": [{"text": "Errors", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9948620200157166}]}, {"text": " Table 5: Errors: Instances incorrectly predicted as sarcastic.", "labels": [], "entities": [{"text": "Errors", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9531723856925964}]}]}