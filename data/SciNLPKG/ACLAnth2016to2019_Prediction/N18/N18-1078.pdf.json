{"title": [{"text": "Multimodal Named Entity Recognition for Short Social Media Posts", "labels": [], "entities": [{"text": "Multimodal Named Entity Recognition", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5831547603011131}, {"text": "Short Social Media Posts", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6264828369021416}]}], "abstractContent": [{"text": "We introduce anew task called Multimodal Named Entity Recognition (MNER) for noisy user-generated data such as tweets or Snapchat captions, which comprise short text with accompanying images.", "labels": [], "entities": [{"text": "Multimodal Named Entity Recognition (MNER)", "start_pos": 30, "end_pos": 72, "type": "TASK", "confidence": 0.7079029934746879}]}, {"text": "These social media posts often come in inconsistent or incomplete syntax and lexical notations with very limited surrounding textual contexts, bringing significant challenges for NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 179, "end_pos": 182, "type": "TASK", "confidence": 0.8994832634925842}]}, {"text": "To this end, we create anew dataset for MNER called SnapCap-tions (Snapchat image-caption pairs submitted to public and crowd-sourced stories with fully annotated named entities).", "labels": [], "entities": []}, {"text": "We then build upon the state-of-the-art Bi-LSTM word/character based NER models with 1) a deep image network which incorporates relevant visual context to augment textual information, and 2) a generic modality-attention module which learns to attenuate irrelevant modalities while amplifying the most informative ones to extract contexts from, adaptive to each sample and token.", "labels": [], "entities": []}, {"text": "The proposed MNER model with modality attention significantly outper-forms the state-of-the-art text-only NER models by successfully leveraging provided visual contexts, opening up potential applications of MNER on myriads of social media platforms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social media with abundant user-generated posts provide a rich platform for understanding events, opinions and preferences of groups and individuals.", "labels": [], "entities": []}, {"text": "These insights are primarily hidden in unstructured forms of social media posts, such as in free-form text or images without tags.", "labels": [], "entities": []}, {"text": "Named entity recognition (NER), the task of recognizing named entities from free-form text, is thus a critical step for building structural information, allowing for its use in personalized assistance, recommendations, advertisement, etc.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8039381504058838}]}, {"text": "While many previous approaches; Chiu and , and focuses on character-based context (e.g. capitalized first letter, and lexical similarity to a known named entity ('Marshmello', a music producer)) for correct prediction.) on NER have shown success for wellformed text in recognizing named entities via word context resolution (e.g. LSTM with word embeddings) combined with character-level features (e.g. CharLSTM/CNN), several additional challenges remain for recognizing named entities from extremely short and coarse text found in social media posts.", "labels": [], "entities": [{"text": "word context resolution", "start_pos": 300, "end_pos": 323, "type": "TASK", "confidence": 0.6202282011508942}]}, {"text": "For instance, short social media posts often do not provide enough textual contexts to resolve polysemous entities (e.g. \"monopoly is da best \", where 'monopoly' may refer to aboard game (named entity) or a term in economics).", "labels": [], "entities": []}, {"text": "In addition, noisy text includes a huge number of unknown tokens due to inconsistent lexical notations and frequent mentions of various newly trending entities (e.g. \"xoxo Marshmelloooo \", where 'Marshmelloooo' is a mis-spelling of a known entity 'Marshmello', a music producer), making word embeddings based neural networks NER models vulnerable.", "labels": [], "entities": []}, {"text": "To address the challenges above for social media posts, we build upon the state-of-the-art neural architecture for NER with the following two novel approaches).", "labels": [], "entities": [{"text": "NER", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9034152626991272}]}, {"text": "First, we propose to leverage auxiliary modalities for additional context resolution of entities.", "labels": [], "entities": [{"text": "context resolution of entities", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.8056638389825821}]}, {"text": "For example, many popular social media platforms now provide ways to compose a post in multiple modalities -specifically image and text (e.g. Snapchat captions, Twitter posts with image URLs), from which we can obtain additional context for understanding posts.", "labels": [], "entities": []}, {"text": "While \"monopoly\" in the previous example is ambiguous in its textual form, an accompanying snap image of aboard game can help disambiguate among polysemous entities, thereby correctly recognizing it as a named entity.", "labels": [], "entities": []}, {"text": "Second, we also propose a general modality attention module which chooses per decoding step the most informative modality among available ones (in our case, word embeddings, character embeddings, or visual features) to extract context from.", "labels": [], "entities": []}, {"text": "For example, the modality attention module lets the decoder attenuate the word-level signals for unknown word tokens (e.g. \"Marshmellooooo\" with trailing 'o's) and amplifies character-level features intsead (e.g. capitalized first letter, lexical similarity to other known named entity token 'Marshmello', etc.), thereby suppressing noise information (\"UNK\" token embedding) in decoding steps.", "labels": [], "entities": []}, {"text": "Note that most of the previous literature in NER or other NLP tasks combine word and character-level information with naive concatenation, which is vulnerable to noisy social media posts.", "labels": [], "entities": [{"text": "NER", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9229381680488586}]}, {"text": "When an auxiliary image is available, the modality attention module determines to amplify this visual context e.g. in disambiguating polysemous entities, or to attenuate visual contexts when they are irrelevant to target named entities, e.g. selfies, etc.", "labels": [], "entities": []}, {"text": "Note that the proposed modality attention module is distinct from how attention is used in other sequence-tosequence literature (e.g. attending to a specific token within an input sequence).", "labels": [], "entities": []}, {"text": "Section 2 provides the detailed literature review.", "labels": [], "entities": []}, {"text": "Our contributions are three-fold: we propose (1) an LSTM-CNN hybrid multimodal NER network that takes as input both image and text for recognition of a named entity in text input.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, our approach is the first work to incorporate visual contexts for named entity recognition tasks.", "labels": [], "entities": [{"text": "named entity recognition tasks", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.7303192317485809}]}, {"text": "(2) We propose a general modality attention module that selectively chooses modalities to extract primary context from, maximizing information gain and suppressing irrelevant contexts from each modality (we treat words, characters, and images as separate modalities).", "labels": [], "entities": []}, {"text": "We show that the proposed approaches outperform the state-of-the-art NER models (both with and without using additional visual contexts) on our new MNER dataset SnapCaptions, a large collection of informal and extremely short social media posts paired with unique images.", "labels": [], "entities": [{"text": "MNER dataset SnapCaptions", "start_pos": 148, "end_pos": 173, "type": "DATASET", "confidence": 0.8766239086786906}]}], "datasetContent": [{"text": "The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC).", "labels": [], "entities": [{"text": "SnapCaptions dataset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7300609648227692}, {"text": "PER", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.8795331120491028}, {"text": "ORG", "start_pos": 204, "end_pos": 207, "type": "METRIC", "confidence": 0.7419228553771973}, {"text": "MISC", "start_pos": 209, "end_pos": 213, "type": "METRIC", "confidence": 0.8284900188446045}]}, {"text": "These captions are collected exclusively from snaps submitted to public and crowd-sourced stories (aka Snapchat Live Stories or Our Stories).", "labels": [], "entities": [{"text": "Snapchat Live Stories or Our Stories)", "start_pos": 103, "end_pos": 140, "type": "DATASET", "confidence": 0.8764051198959351}]}, {"text": "Examples of such public crowd-sourced stories are \"New York Story\" or \"Thanksgiving Story\", which comprise snaps that are aggregated for various public events, venues, etc.", "labels": [], "entities": [{"text": "New York Story\" or \"Thanksgiving Story\"", "start_pos": 51, "end_pos": 90, "type": "DATASET", "confidence": 0.7259536749786801}]}, {"text": "All snaps were posted between year 2016 and 2017, and do not contain raw images or other associated information (only textual captions and obfuscated visual descriptor features extracted from the pre-trained InceptionNet are available).", "labels": [], "entities": []}, {"text": "We split the dataset into train (70%), validation (15%), and test sets (15%).", "labels": [], "entities": []}, {"text": "The captions data have average length of 30.7 characters (5.81 words) with vocabulary size 15,733, where 6,612 are considered unknown tokens from Stanford GloVE embeddings ().", "labels": [], "entities": []}, {"text": "Named entities annotated in the SnapCaptions dataset include many of new and emerging entities, and they are found in various surface forms (various nicknames, typos, etc.)", "labels": [], "entities": [{"text": "SnapCaptions dataset", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.8915671110153198}]}, {"text": "To the best of our knowledge, SnapCaptions is the only dataset that contains natural image-caption pairs with expert-annotated named entities.", "labels": [], "entities": []}, {"text": "Parameters: We tune the parameters of each model with the following search space (bold indicate the choice for our final model): character embeddings dimension: {25, 50, 100, 150, 200, 300}, word embeddings size: {25, 50, 100, 150, 200, 300}, LSTM hidden states: {25, 50, 100, 150, 200, 300}, and x dimension: {25, 50, 100, 150, 200, 300}.", "labels": [], "entities": []}, {"text": "We optimize the parameters with Adagrad () with batch size 10, learning rate 0.02, epsilon 10 \u22128 , and decay 0.0.", "labels": [], "entities": [{"text": "learning rate 0.02", "start_pos": 63, "end_pos": 81, "type": "METRIC", "confidence": 0.9656638105710348}, {"text": "decay", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9648323059082031}]}, {"text": "Main Results: When visual context is available (W+C+V), we see that the model performance greatly improves over the textual models (W+C), showing that visual contexts are complimentary to textual information in named entity recognition tasks.", "labels": [], "entities": [{"text": "entity recognition tasks", "start_pos": 217, "end_pos": 241, "type": "TASK", "confidence": 0.798563818136851}]}, {"text": "In addition, it can be seen that the modality attention module further improves the entity type recognition performance for (W+C+V).", "labels": [], "entities": [{"text": "entity type recognition", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.6982480684916178}]}, {"text": "This result indicates that the modality attention is able to focus on the most effective modality (visual, words, or characters) adaptive to each sample to maximize information gain.", "labels": [], "entities": []}, {"text": "Note that our textonly model (W+C) with the modality attention module also significantly outperform the state-ofthe-art baselines) that use the same textual modalities (W+C), showing the effectiveness of the modality attention module for textual models as well.", "labels": [], "entities": []}, {"text": "Error Analysis: shows example cases where incorporation of visual contexts affects prediction of named entities.", "labels": [], "entities": [{"text": "Error Analysis", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6949509233236313}, {"text": "prediction of named entities", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.8129648268222809}]}, {"text": "For example, the token 'curry' in the caption \"The curry's \" is polysemous and may refer to either a type of food or a famous basketball player 'Stephen Curry', and the surrounding textual contexts do not provide: Error analysis: when do images help NER?", "labels": [], "entities": [{"text": "Error", "start_pos": 214, "end_pos": 219, "type": "METRIC", "confidence": 0.9264786243438721}, {"text": "NER", "start_pos": 250, "end_pos": 253, "type": "TASK", "confidence": 0.9246470332145691}]}, {"text": "Ground-truth labels (GT) and predictions of our model with vision input (W+C+V) and the one without (W+C) for the underlined named entities (or false positives) are shown.", "labels": [], "entities": []}, {"text": "For interpretability, visual tags (label output of InceptionNet) are presented instead of actual feature vectors used.", "labels": [], "entities": []}, {"text": "enough information to disambiguate it.", "labels": [], "entities": [{"text": "disambiguate it", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.9199520945549011}]}, {"text": "On the other hand, visual contexts (visual tags: 'parade', 'urban area', ...) provide similarities to the token's distributional semantics from other training examples (e.g. snaps from \"NBA Championship Parade Story\"), and thus the model successfully predicts the token as a named entity.", "labels": [], "entities": [{"text": "NBA Championship Parade Story\")", "start_pos": 186, "end_pos": 217, "type": "DATASET", "confidence": 0.7487182199954987}]}, {"text": "Similarly, while the text-only model erroneously predicts 'Apple' in the caption \"Grandma w dat lit Apple Crisp\" as an organization (e.g. Apple Inc.), the visual contexts (describing objects related to food) help disambiguate the token, making the model predict it correctly as a non-named entity (a fruit).", "labels": [], "entities": []}, {"text": "Trending entities (musicians or DJs such as 'CID', 'Duke Dumont', 'Marshmello', etc.) are also recognized correctly with strengthened contexts from visual information (describing concert scenes) despite lack of surrounding textual contexts.", "labels": [], "entities": []}, {"text": "A few cases where visual contexts harmed the performance mostly include visual tags that are unrelated to a token or its surrounding textual contexts.", "labels": [], "entities": []}, {"text": "Visualization of Modality Attention: visualizes the modality attention module at each decoding step (each column), where amplified modality is represented with darker color, and attenuated modality is represented with lighter color.", "labels": [], "entities": []}, {"text": "For the image-aided model (W+C+V; upper row in), we confirm that the modality attention successfully attenuates irrelevant signals (e.g. selfies, etc.) and amplifies relevant modalitybased contexts in prediction of a given token.", "labels": [], "entities": []}, {"text": "In the example of \"disney word essential = coffee\" with visual tags selfie, phone, person, the modality attention successfully attenuates distracting visual signals and focuses on textual modalities, consequently making correct predictions.", "labels": [], "entities": []}, {"text": "The named entities in the examples of \"Beautiful night atop The Space Needle\" and \"Splash Mountain\" are challenging to predict because they are composed of common nouns (space, needle, splash, mountain), and thus they often need additional contexts to correctly predict.", "labels": [], "entities": []}, {"text": "In the training data, visual contexts make stronger indicators for these named entities (space needle, splash mountain), and the modality attention module successfully attends more to stronger signals.", "labels": [], "entities": []}, {"text": "For text-only model (W+C), we observe that performance gains mostly come from the modality attention module better handling tokens unseen during training or unknown tokens from the pretrained word embeddings matrix.", "labels": [], "entities": []}, {"text": "For example, while WaRriOoOrs and Kooler Matic are missing tokens in the word embeddings matrix, it successfully amplifies character-based contexts (e.g. capitalized first letters, similarity to known entities 'Golden State Warriors') and suppresses wordbased contexts (word embeddings for unknown tokens e.g. 'WaRriOoOrs'), leading to correct predictions.", "labels": [], "entities": [{"text": "Golden State Warriors", "start_pos": 211, "end_pos": 232, "type": "DATASET", "confidence": 0.8844479322433472}]}, {"text": "This result is significant because it shows performance of the model, with an almost identical architecture, can still improve without having to scale the word embeddings matrix indefinitely.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: NER performance on the SnapCaptions dataset with varying modalities (W: word, C: char, V: visual).  We report precision, recall, and F1 score for both entity types recognition (PER, LOC, ORG, MISC) and entity  segmentation (untyped recognition -named entity or not) tasks.", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9734072089195251}, {"text": "SnapCaptions dataset", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.7905729413032532}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9996509552001953}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9987788796424866}, {"text": "F1 score", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9842546284198761}, {"text": "entity types recognition", "start_pos": 161, "end_pos": 185, "type": "TASK", "confidence": 0.7106760541598002}, {"text": "entity  segmentation", "start_pos": 212, "end_pos": 232, "type": "TASK", "confidence": 0.7061545997858047}]}, {"text": " Table 3: NER performance (F1) on SnapCaptions with  varying word embeddings vocabulary size. Mod- els being compared: (W+C) Bi-LSTM/CRF + Bi- CharLSTM w/ and w/o modality attention (M.A.)", "labels": [], "entities": [{"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9893783926963806}]}, {"text": " Table 3. By increasing the number of  missing tokens artificially by randomly removing  words from the word embeddings matrix (original  vocab size: 400K), we observe that while the over- all performance degrades, the modality attention  module is able to suppress the peformance degra- dation. Note also that the performance gap gen- erally gets bigger as we decrease the vocabulary  size of the word embeddings matrix. This result is", "labels": [], "entities": []}]}