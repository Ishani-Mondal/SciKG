{"title": [{"text": "Scene Graph Parsing as Dependency Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we study the problem of parsing structured knowledge graphs from textual descriptions.", "labels": [], "entities": [{"text": "parsing structured knowledge graphs from textual descriptions", "start_pos": 39, "end_pos": 100, "type": "TASK", "confidence": 0.8462880253791809}]}, {"text": "In particular, we consider the scene graph representation (Johnson et al., 2015) that considers objects together with their attributes and relations: this representation has been proved useful across a variety of vision and language applications.", "labels": [], "entities": []}, {"text": "We begin by introducing an alternative but equivalent edge-centric view of scene graphs that connect to dependency parses.", "labels": [], "entities": []}, {"text": "Together with a careful redesign of label and action space, we combine the two-stage pipeline used in prior work (generic dependency parsing followed by simple post-processing) into one, enabling end-to-end training.", "labels": [], "entities": [{"text": "generic dependency parsing", "start_pos": 114, "end_pos": 140, "type": "TASK", "confidence": 0.590069423119227}]}, {"text": "The scene graphs generated by our learned neural dependency parser achieve an F-score similarity of 49.67% to ground truth graphs on our evaluation set, surpassing best previous approaches by 5%.", "labels": [], "entities": [{"text": "F-score similarity", "start_pos": 78, "end_pos": 96, "type": "METRIC", "confidence": 0.9125877618789673}]}, {"text": "We further demonstrate the effectiveness of our learned parser on image retrieval applications.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.8003522157669067}]}], "introductionContent": [{"text": "Recent years have witnessed the rise of interest in many tasks at the intersection of computer vision and natural language processing, including semantic image retrieval, image captioning (), visual question answering (, and referring expressions ().", "labels": [], "entities": [{"text": "semantic image retrieval", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.6189477741718292}, {"text": "image captioning", "start_pos": 171, "end_pos": 187, "type": "TASK", "confidence": 0.7016710489988327}, {"text": "question answering", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.6909632831811905}]}, {"text": "The pursuit for these tasks is inline with people's desire for high level understanding of visual content, in particular, using textual descriptions or questions to help understand or express images and scenes.", "labels": [], "entities": []}, {"text": "What is shared among all these tasks is the need fora common representation to establish connection between the two different modalities.", "labels": [], "entities": []}, {"text": "The majority of recent works handle the vision side with convolutional neural networks, and the language side with recurrent neural networks) or word embeddings ().", "labels": [], "entities": []}, {"text": "In either case, neural networks map original sources into a semantically meaningful ( vector representation that can be aligned through end-toend training.", "labels": [], "entities": []}, {"text": "This suggests that the vector embedding space is an appropriate choice as the common representation connecting different modalities (see e.g.).", "labels": [], "entities": []}, {"text": "While the dense vector representation yields impressive performance, it has an unfortunate limitation of being less intuitive and hard to interpret.", "labels": [], "entities": []}, {"text": "Scene graphs), on the other hand, proposed a type of directed graph to encode information in terms of objects, attributes of objects, and relationships between objects (see Figure 1 for visualization).", "labels": [], "entities": []}, {"text": "This is a more structured and explainable way of expressing the knowledge from either modality, and is able to serve as an alternative form of common representation.", "labels": [], "entities": []}, {"text": "In fact, the value of scene graph representation has already been proven in a wide range of visual tasks, including semantic image retrieval), caption quality evaluation (, etc.", "labels": [], "entities": [{"text": "scene graph representation", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.6565691431363424}, {"text": "semantic image retrieval", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.6207815706729889}, {"text": "caption quality evaluation", "start_pos": 143, "end_pos": 169, "type": "TASK", "confidence": 0.8153170148531595}]}, {"text": "In this paper, we focus on scene graph generation from textual descriptions.", "labels": [], "entities": [{"text": "scene graph generation", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.6856937905152639}]}, {"text": "Previous attempts at this problem ( follow the same spirit.", "labels": [], "entities": []}, {"text": "They first use a dependency parser to obtain the dependency relationship for all words in a sentence, and then use either a rule-based or a learned classifier as post-processing to generate the scene graph.", "labels": [], "entities": []}, {"text": "However, the rule-based classifier cannot a young boy in front of a soccer goal a soccer ball in the air a man standing with hands behind back a woman wearing a purple shirt a young boy wearing a black uniform the roof is brown the ball is white a soccer ball on the ground a man wearing a red and white shirt people behind the net goalkeeper watching the ball a white ball on the ground goalkeeper is wearing gloves a kid is sitting on the ground the man is standing the uniform is black a red and black backpack sitting on the ground trees outside the fence blue and white soccer ball) dataset contains tens of region descriptions and the region scene graphs associated with them.", "labels": [], "entities": []}, {"text": "In this paper, we study how to generate high quality scene graphs (two such examples are shown in the figure) from textual descriptions, without using image information.", "labels": [], "entities": []}, {"text": "learn from data, and the learned classifier is rather simple with hand-engineered features.", "labels": [], "entities": []}, {"text": "In addition, the dependency parser was trained on linguistics data to produce complete dependency trees, some parts of which maybe redundant and hence confuse the scene graph generation process.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8028730154037476}, {"text": "scene graph generation", "start_pos": 163, "end_pos": 185, "type": "TASK", "confidence": 0.7053168416023254}]}, {"text": "Therefore, our model abandons the two-stage pipeline, and uses a single, customized dependency parser instead.", "labels": [], "entities": []}, {"text": "The customization is necessary for two reasons.", "labels": [], "entities": []}, {"text": "First is the difference in label space.", "labels": [], "entities": []}, {"text": "Standard dependency parsing has tens of edge labels to represent rich relationships between words in a sentence, but in scene graphs we are only interested in three types, namely objects, attributes, and relations.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7385394871234894}]}, {"text": "Second is whether every word needs ahead.", "labels": [], "entities": []}, {"text": "In some sense, the scene graph represents the \"skeleton\" of the sentence, which suggests that empty words are unlikely to be included in the scene graph.", "labels": [], "entities": []}, {"text": "We argue that in scene graph generation, it is unnecessary to require a parent word for every single word.", "labels": [], "entities": [{"text": "scene graph generation", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.6485510170459747}]}, {"text": "We build our model on top of a neural dependency parser implementation) that is among the state-of-theart.", "labels": [], "entities": []}, {"text": "We show that our carefully customized dependency parser is able to generate high quality scene graphs by learning from data.", "labels": [], "entities": []}, {"text": "Specifically, we use the Visual Genome dataset (, which provides rich amounts of region description -region graph pairs.", "labels": [], "entities": [{"text": "Visual Genome dataset", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.7943278551101685}]}, {"text": "We first align nodes in region graphs with words in the region descriptions using simple rules, and then use this alignment to train our customized dependency parser.", "labels": [], "entities": []}, {"text": "We evaluate our parser by computing the F-score between the parsed scene graphs and ground truth scene graphs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9982600808143616}]}, {"text": "We also apply our approach to image retrieval to show its effectiveness.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.788556694984436}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The F-scores (i.e. SPICE metric) between  scene graphs parsed from region descriptions and  ground truth region graphs on the intersection of Vi- sual Genome (Krishna et al., 2017) and MS COCO (Lin  et al., 2014) validation set.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9848350286483765}, {"text": "MS COCO (Lin  et al., 2014) validation set", "start_pos": 195, "end_pos": 237, "type": "DATASET", "confidence": 0.9091019034385681}]}, {"text": " Table 3: Image retrieval results. We follow the same experiment setup as Schuster et al. (2015), except using a  different scoring function when ranking images. Our parser consistently outperforms the Stanford Scene Graph  Parser across evaluation metrics.", "labels": [], "entities": [{"text": "Image retrieval", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8085799515247345}]}]}