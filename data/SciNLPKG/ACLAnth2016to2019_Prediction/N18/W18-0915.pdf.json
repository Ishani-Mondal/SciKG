{"title": [{"text": "Conditional Random Fields for Metaphor Detection", "labels": [], "entities": [{"text": "Metaphor Detection", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7103355824947357}]}], "abstractContent": [{"text": "We present an algorithm for detecting metaphor in sentences which was used in Shared Task on Metaphor Detection by First Workshop on Figurative Language Processing.", "labels": [], "entities": [{"text": "detecting metaphor in sentences", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.8788693398237228}, {"text": "Shared Task on Metaphor Detection", "start_pos": 78, "end_pos": 111, "type": "TASK", "confidence": 0.6979682683944702}]}, {"text": "The algorithm is based on different features and Conditional Random Fields.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we present a system which predicts metaphoricity of the word depending on its neighbors.", "labels": [], "entities": []}, {"text": "We used VU Amsterdam corpus given by competition's organizers, 10 features which were also given by competition's organizers and algorithm of Conditional Random Fields for predictions that are depending on previous ones.", "labels": [], "entities": [{"text": "VU Amsterdam corpus", "start_pos": 8, "end_pos": 27, "type": "DATASET", "confidence": 0.9513276616732279}]}], "datasetContent": [{"text": "As a dataset was used VU Amsterdam corpus.", "labels": [], "entities": [{"text": "VU Amsterdam corpus", "start_pos": 22, "end_pos": 41, "type": "DATASET", "confidence": 0.9026897152264913}]}, {"text": "It consists of 117 texts divided into 4 parts (academic, news, fiction, conversation).", "labels": [], "entities": []}, {"text": "It was divided into two parts: train and test.", "labels": [], "entities": []}, {"text": "The model was trained on the train set and evaluated on the test set.", "labels": [], "entities": []}, {"text": "We tried different parameters that were provided in the crfsuite.", "labels": [], "entities": []}, {"text": "There were five training algorithms such as lbfgs (gradient descending using the L-BFGS method), l2sgd (stochastic gradient descend with L2 regularization term), Averaged Perceptron, Passive Aggressive, Adaptive Regularization Of Weight Vector.", "labels": [], "entities": []}, {"text": "The best training algorithm was lbfgs.", "labels": [], "entities": []}, {"text": "Moreover, we used a different amount of iterations, and its amount affects the loss because there is no limit to the number of iterations in the lbfgsalgorithm.", "labels": [], "entities": []}, {"text": "Furthermore, some experiments with regularization were conducted.", "labels": [], "entities": [{"text": "regularization", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.957793653011322}]}, {"text": "Regularization was used for reducing the generalization error and it is important in CRF.", "labels": [], "entities": [{"text": "CRF", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.6197028756141663}]}, {"text": "For the selection of the most appropriate parameters for regularization, we used RandomizedSearchCV from scikit-learn (http://scikit-learn.org).", "labels": [], "entities": [{"text": "regularization", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.9733040928840637}]}, {"text": "We used sklearn-crfsuite that is the special wrapper of crfsuite written in C for Python (https://github.com/TeamHG-Memex/sklearncrfsuite) for computing the algorithm.", "labels": [], "entities": []}, {"text": "As a metric for evaluating the score was taken F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9964467883110046}]}, {"text": "The best F-score had the algorithm with 200 iterations, lbfgs-algorithm, c1 regularization and c2 regularization that equal to 0.1.", "labels": [], "entities": [{"text": "F-score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9863384366035461}]}, {"text": "The result obtained with these parameters was evaluated using a held-out set from the train set.", "labels": [], "entities": []}, {"text": "F-score of this model and other experiments are presented in table 1 for All-POS track and for Verb track.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9923424124717712}]}], "tableCaptions": []}