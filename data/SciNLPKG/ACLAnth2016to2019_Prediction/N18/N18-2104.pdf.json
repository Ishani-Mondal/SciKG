{"title": [{"text": "Pruning Basic Elements for Better Automatic Evaluation of Summaries", "labels": [], "entities": [{"text": "Evaluation of Summaries", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.6897425651550293}]}], "abstractContent": [{"text": "We propose a simple but highly effective automatic evaluation measure of summarization, pruned Basic Elements (pBE).", "labels": [], "entities": [{"text": "summarization", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.9576479196548462}, {"text": "pruned Basic Elements (pBE)", "start_pos": 88, "end_pos": 115, "type": "METRIC", "confidence": 0.7983052333196005}]}, {"text": "Although the BE concept is widely used for the automated evaluation of summaries, its weakness is that it redundantly matches basic elements.", "labels": [], "entities": [{"text": "BE", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.8630269169807434}, {"text": "evaluation of summaries", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.6075421571731567}]}, {"text": "To avoid this redundancy, pBE prunes basic elements by (1) disregarding frequency count of basic elements and (2) reducing semantically overlapped basic elements based on word similarity.", "labels": [], "entities": []}, {"text": "Even though it is simple, pBE outper-forms ROUGE in DUC datasets inmost cases and achieves the highest rank correlation coefficient in TAC 2011 AESOP task.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9794177412986755}, {"text": "DUC datasets", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9393008351325989}, {"text": "rank correlation coefficient", "start_pos": 103, "end_pos": 131, "type": "METRIC", "confidence": 0.8002921938896179}, {"text": "TAC 2011 AESOP task", "start_pos": 135, "end_pos": 154, "type": "DATASET", "confidence": 0.7369492799043655}]}], "introductionContent": [{"text": "Automatic evaluation measures have a significant impact on the research on summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.9858899116516113}]}, {"text": "Since there is no other practical way to quickly evaluate the quality of system summaries, summarization studies work on raising the scores that are given by automatic evaluation measures.", "labels": [], "entities": [{"text": "summarization", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.9826081991195679}]}, {"text": "Among the automatic evaluation measures, the most popular ones are ROUGE) and BE ().", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9972544312477112}, {"text": "BE", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9985275268554688}]}, {"text": "ROUGE/BE counts the number of ngrams/basic elements 1 that match those in manual reference summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9738285541534424}, {"text": "BE", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.7461540102958679}]}, {"text": "ROUGE normally employs unigrams or bigrams while BE uses dependency triples (head|modifier|relation) as their units.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.618668258190155}, {"text": "BE", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.926857054233551}]}, {"text": "It is known that both ROUGE and BE are well correlated with human judgment.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9982730150222778}, {"text": "BE", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9983856678009033}]}, {"text": "Their evaluation approach, however, is quite different from humans' in two ways: they score low-information units higher and ignore the semantic overlap of units.", "labels": [], "entities": []}, {"text": "The first problem is caused by scoring units according to their frequencies.", "labels": [], "entities": []}, {"text": "We found that the units that occur multiple times in a summary are highly likely to be function-word bigrams (e.g., \"of the\") or basic elements that represent only single nouns (e.g., (house|the|det)); such units are less informative than units connected with verbs (e.g., \"John went\" and (went|John|nsubj)).", "labels": [], "entities": []}, {"text": "The second problem is that ROUGE/BE sometimes gives scores twice or more to the units that are semantically overlapped but spelled differently.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9801862239837646}, {"text": "BE", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.6674227118492126}]}, {"text": "This is due to the fact that ROUGE/BE only considers the surface level of unit matching, which also yields inaccurate scoring of paraphrased units.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9620401263237}, {"text": "BE", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.7623661756515503}]}, {"text": "Our method is aimed at solving these problems by cutting back redundant units.", "labels": [], "entities": []}, {"text": "We use BE, but with Universal Dependencies (UD) (, a more ideal form of annotation that is available for multiple languages, and introduce two steps to prune basic elements.", "labels": [], "entities": [{"text": "BE", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9929595589637756}]}, {"text": "The first step is to disregard the frequency count of basic elements, and the other one is to reduce semantically overlapped basic elements using word embeddings.", "labels": [], "entities": []}, {"text": "We call this new measure pruned BE (pBE).", "labels": [], "entities": [{"text": "BE (pBE)", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8769055753946304}]}, {"text": "Our experiments show that pBE outperforms ROUGE inmost DUC datasets and achieves the highest rank correlation coefficient in TAC 2011 AESOP task.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9954082369804382}, {"text": "DUC datasets", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.9023480117321014}, {"text": "rank correlation coefficient", "start_pos": 93, "end_pos": 121, "type": "METRIC", "confidence": 0.8560793399810791}, {"text": "TAC 2011 AESOP task", "start_pos": 125, "end_pos": 144, "type": "DATASET", "confidence": 0.7340341061353683}]}], "datasetContent": [{"text": "To assess the effectiveness of pBE, we computed the correlation coefficient between pBE scores and human judgments, as well as between the scores of other automatic evaluation measures and manual scores for comparison.", "labels": [], "entities": []}, {"text": "We used multi-document summarization datasets.", "labels": [], "entities": []}, {"text": "The correlation was computed between all system summaries, excluding reference summaries.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9861987829208374}]}, {"text": "Our first experiment compared the performance of pBE and ROUGE on DUC datasets.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9879756569862366}, {"text": "DUC datasets", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.8730306625366211}]}, {"text": "Since a dependency triple is a type of bigram/skip-bigram, we chose ROUGE-2 and ROUGE-S4 for comparison.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.8928123116493225}]}, {"text": "We also examined ROUGE-SU4 9 because it is known as a strong baseline that outperforms most of other measures in).", "labels": [], "entities": [{"text": "ROUGE-SU4 9", "start_pos": 17, "end_pos": 28, "type": "METRIC", "confidence": 0.9638431966304779}]}, {"text": "The second experiment was designed to see how well pBE worked compared with our related All three ROUGE here were run with stemming but with no removal of stopwords.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.9284756183624268}]}, {"text": "We chose the latest AE-SOP dataset, TAC 2011, for which ROUGE-WE achieved the highest Spearman coefficient.", "labels": [], "entities": [{"text": "AE-SOP dataset, TAC 2011", "start_pos": 20, "end_pos": 44, "type": "DATASET", "confidence": 0.8164494037628174}, {"text": "ROUGE-WE", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.986162543296814}, {"text": "Spearman coefficient", "start_pos": 86, "end_pos": 106, "type": "METRIC", "confidence": 0.9704402685165405}]}, {"text": "The details of our experimental setup are given in and below.", "labels": [], "entities": []}, {"text": "Parser: We used the neural-network dependency parser of Stanford CoreNLP ().", "labels": [], "entities": [{"text": "Parser", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8157840371131897}, {"text": "Stanford CoreNLP", "start_pos": 56, "end_pos": 72, "type": "DATASET", "confidence": 0.9279411733150482}]}, {"text": "Dependencies were set to enhanced++ Universal Dependencies (.", "labels": [], "entities": []}, {"text": "Clustering: We employed hierarchical clustering, maximum distance method.", "labels": [], "entities": []}, {"text": "The number of clusters, N , was set to 0.975 * Q.", "labels": [], "entities": []}, {"text": "Word Embeddings: A set of pre-trained Google-News word embeddings . It contains 3 million words, each of which has a word embedding of 300 dimensions.: The number of basic elements which returned more than 1 in min{N (f km , R k ), N (f km , S)}, before clustering (BE) and after clustering (BE +cls ), and the difference of the numbers, BE +cls \u2212 BE (Increased).", "labels": [], "entities": [{"text": "BE", "start_pos": 266, "end_pos": 268, "type": "METRIC", "confidence": 0.9411344528198242}, {"text": "BE", "start_pos": 338, "end_pos": 340, "type": "METRIC", "confidence": 0.9848085641860962}, {"text": "BE", "start_pos": 348, "end_pos": 350, "type": "METRIC", "confidence": 0.6554201245307922}, {"text": "Increased", "start_pos": 352, "end_pos": 361, "type": "METRIC", "confidence": 0.9425016641616821}]}, {"text": "The relation \"subj & obj\" includes nsubj, nsubjpass, csubj, csubjpass, iobj and dobj.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Correlation coefficients of pBE and ROUGE. The coefficients are written in the order of \"Pear- son/Spearman/Kendall\".", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.993999719619751}, {"text": "Pear- son/Spearman/Kendall", "start_pos": 99, "end_pos": 125, "type": "DATASET", "confidence": 0.7560103833675385}]}, {"text": " Table 3: The details of the datasets. \"Evaluation\" rep- resents manual evaluation methods and \"Limit\" repre- sents word limits of summarization.", "labels": [], "entities": []}, {"text": " Table 4: The number of basic elements which returned  more than 1 in min{N (f k  m , R k ), N (f k  m , S)}, before  clustering (BE) and after clustering (BE +cls ), and the  difference of the numbers, BE +cls \u2212 BE (Increased).  The relation \"subj & obj\" includes nsubj, nsubjpass,  csubj, csubjpass, iobj and dobj.", "labels": [], "entities": [{"text": "BE", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.9571349024772644}, {"text": "BE", "start_pos": 203, "end_pos": 205, "type": "METRIC", "confidence": 0.9719762802124023}, {"text": "BE", "start_pos": 213, "end_pos": 215, "type": "METRIC", "confidence": 0.7173723578453064}, {"text": "Increased", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9349818825721741}]}]}