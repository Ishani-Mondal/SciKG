{"title": [{"text": "Automated Paraphrase Lattice Creation for HyTER Machine Translation Evaluation", "labels": [], "entities": [{"text": "Paraphrase Lattice Creation", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6130635738372803}, {"text": "HyTER Machine Translation Evaluation", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.7569273635745049}]}], "abstractContent": [{"text": "We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER (Dreyer and Marcu, 2012), which exploits reference translations enriched with meaning equivalent expressions.", "labels": [], "entities": [{"text": "machine translation (MT) evaluation", "start_pos": 37, "end_pos": 72, "type": "TASK", "confidence": 0.8643589814503988}, {"text": "HyTER", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.8934547305107117}]}, {"text": "The original HyTER metric relied on hand-crafted paraphrase networks which restricted its applicability to new data.", "labels": [], "entities": []}, {"text": "We test, for the first time, HyTER with automatically built paraphrase lattices.", "labels": [], "entities": []}, {"text": "We show that although the metric obtains good results on small and carefully curated data with both manually and automatically selected substitutes , it achieves medium performance on much larger and noisier datasets, demonstrating the limits of the metric for tuning and evaluation of current MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 294, "end_pos": 296, "type": "TASK", "confidence": 0.9850226044654846}]}], "introductionContent": [{"text": "Human translators and MT systems can produce multiple plausible translations for input texts.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9780387282371521}]}, {"text": "To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.897471159696579}]}, {"text": "The HyTER metric) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators.", "labels": [], "entities": []}, {"text": "The manually built networks attempt to encode the set of all correct translations fora sentence, and HyTER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations.", "labels": [], "entities": []}, {"text": "HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data.", "labels": [], "entities": [{"text": "HyTER", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6949570178985596}]}, {"text": "We propose to use an embedding-based lexical substitution model ( for building this type of reference networks and test, for the first time, the metric with automatically generated lattices (hereafter HyTERA).", "labels": [], "entities": [{"text": "HyTERA", "start_pos": 201, "end_pos": 207, "type": "METRIC", "confidence": 0.845464825630188}]}, {"text": "We show that HyTERA strongly correlates with HyTER with hand-crafted lattices, and approximates the hTER score () as measured using post-edits made by human annotators.", "labels": [], "entities": []}, {"text": "Furthermore, we generate lattices for standard datasets from a recent WMT Metrics Shared Task and perform the first evaluation of HyTER on large and noisier datasets.", "labels": [], "entities": [{"text": "WMT Metrics Shared Task", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.5646863132715225}, {"text": "HyTER", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.9472315907478333}]}, {"text": "The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9761260449886322}, {"text": "MT", "start_pos": 136, "end_pos": 138, "type": "TASK", "confidence": 0.9711645841598511}]}], "datasetContent": [{"text": "To evaluate the performance of HyTER, Dreyer and Marcu (2012) examine whether it can approximate the hTER score () that measures the number of edits required to change a system output into its post-edition.", "labels": [], "entities": []}, {"text": "hTER scores area good estimate of translation quality and usefulness, but require each translation hypothesis to be corrected by a human annotator.", "labels": [], "entities": []}, {"text": "show that it can be closely approximated by HyTER scores.", "labels": [], "entities": [{"text": "HyTER", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9921808242797852}]}, {"text": "In this section, we reproduce their experiments with HyTERA to see whether it is possible to use automatically-built rather than hand-crafted references to approximate hTER scores.", "labels": [], "entities": [{"text": "HyTERA", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.7606602907180786}]}, {"text": "Data Experimental Setting We build meaningequivalent lattices by applying the lexical substitution method described in Section 3 to each of the four references associated with a sentence, and considering the union of the resulting lattices.", "labels": [], "entities": []}, {"text": "We report results for two kinds of lattices: lattices encoding all lexical substitutes available fora word in PPDB (allPars) and lattices of substitutes with PPDBSc>2.3 (allParsFiltered) and AddCosSc\u22650.", "labels": [], "entities": []}, {"text": "As expected, the allPars lattices are much larger than the manual and the filtered lattices (cf.).", "labels": [], "entities": []}, {"text": "In all our experiments, all corpora are down-cased and tokenized using standard Moses scripts.", "labels": [], "entities": []}, {"text": "hTER scores are computed using).", "labels": [], "entities": []}, {"text": "score, estimated by the arithmetic mean of 1 to 4-gram precisions.", "labels": [], "entities": []}, {"text": "In all cases, there is a high correlation between HyTER, HyTERA and hTER, significantly higher than the correlation between BLEU and hTER.", "labels": [], "entities": [{"text": "HyTER", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.8855366110801697}, {"text": "HyTERA", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9446213841438293}, {"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9983239769935608}]}, {"text": "This observation shows that replacing the handcrafted lattices with automatically built ones has only a moderate impact on the HyTER metric quality: automatic lattices result in a small drop of the correlation when evaluating hypotheses translated from Chinese, and slightly improve it for the Arabic to English condition.", "labels": [], "entities": [{"text": "HyTER metric quality", "start_pos": 127, "end_pos": 147, "type": "METRIC", "confidence": 0.6976630687713623}]}, {"text": "Overall HyTERA scores are highly correlated with HyTER scores (\u03c1 = 0.766 for Arabic and \u03c1 = 0.756 for Chinese).", "labels": [], "entities": [{"text": "HyTERA", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9953206181526184}, {"text": "HyTER", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.956813395023346}]}, {"text": "More importantly, considering the filtered lattices allows to significantly reduce computation time compared to the allPars ones without hurting the quality estimation capacity of the metric.", "labels": [], "entities": []}, {"text": "shows how the five MT systems are ranked by the different metrics we consider, when translating from Arabic to English.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9792200326919556}]}, {"text": "All metrics rank the systems in the same order, except from HyTER with allParsFiltered that only inverts two systems.", "labels": [], "entities": [{"text": "HyTER", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.8476681709289551}]}, {"text": "Note that the tested systems were selected by NIST to cover a variety of system architectures (statistical, rule-based, hybrid) and performances, which makes distinction between them an easy task for all metrics.", "labels": [], "entities": [{"text": "NIST", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9156626462936401}]}, {"text": "The benefits of using a metric like HyTER, which focuses on the word level, are much clearer in the sentence-based evaluation.", "labels": [], "entities": [{"text": "HyTER", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.7361166477203369}]}, {"text": "In our second set of experiments, we explore the ability of HyTERA to predict direct human judgments at the sentence level using the setting of the WMT16 Metrics Shared Task (.", "labels": [], "entities": [{"text": "predict direct human judgments", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.7940578460693359}, {"text": "WMT16 Metrics Shared Task", "start_pos": 148, "end_pos": 173, "type": "DATASET", "confidence": 0.7399068474769592}]}, {"text": "We measure the correlation between ad-9 More precisely: SBLEU = 1 4 \u00b7 \u2211 4 i=1 pi where pi is the number of i-grams that appears both in the reference and in the hypothesis divided by the number of i-grams in the reference.", "labels": [], "entities": [{"text": "SBLEU", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9939951300621033}]}, {"text": "equacy scores collected on Amazon Mechanical Turk following the method advocated by and the translation quality estimated by applying HyTERA to the official WMT reference.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 27, "end_pos": 49, "type": "DATASET", "confidence": 0.9535102844238281}, {"text": "HyTERA", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9711153507232666}, {"text": "WMT reference", "start_pos": 157, "end_pos": 170, "type": "DATASET", "confidence": 0.8705329298973083}]}, {"text": "reports the results achieved by HyTERA on the six language pairs of the WMT16 Shared Task and its rank among the other metrics tested in the competition.", "labels": [], "entities": [{"text": "WMT16 Shared Task", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.7159448464711508}]}, {"text": "HyTERA obtains medium performance on the WMT16 dataset, which is much larger and noisier than the dataset used for evaluation in: it is made, for each language, of 560 translations sampled from outputs of all systems taking part in the WMT15 campaign.", "labels": [], "entities": [{"text": "HyTERA", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7584559321403503}, {"text": "WMT16 dataset", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9766649901866913}, {"text": "WMT15 campaign", "start_pos": 236, "end_pos": 250, "type": "DATASET", "confidence": 0.7822762131690979}]}, {"text": "It is important to note that the hTER scores used in the initial HyTER evaluation were produced by experienced LDC annotators, while the WMT16 Direct Assessment (DA) adequacy judgments were collected from non-experts through crowd-sourcing (.", "labels": [], "entities": [{"text": "WMT16 Direct Assessment (DA)", "start_pos": 137, "end_pos": 165, "type": "DATASET", "confidence": 0.6684954265753428}]}, {"text": "HyTERA achieves higher performance than the SENTBLEU baseline in four language pairs (cs/de/ru/tr-en).", "labels": [], "entities": [{"text": "SENTBLEU baseline", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.6635366082191467}]}, {"text": "It obtains slightly lower correlation than SENTBLEU for fi-en and ro-en, the language pairs in which correlation was lower for all metrics.", "labels": [], "entities": [{"text": "correlation", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.986816942691803}, {"text": "SENTBLEU", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9455681443214417}, {"text": "correlation", "start_pos": 101, "end_pos": 112, "type": "METRIC", "confidence": 0.9722926020622253}]}, {"text": "Among the metrics tested at the WMT16 shared task we find combination metrics, and metrics that have been tuned on a development dataset.", "labels": [], "entities": [{"text": "WMT16 shared task", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.6323582530021667}]}, {"text": "The metric that performs best for most languages in the segment-level WMT16 evaluation, DPMFCOMB, combines 57 individual metrics (.", "labels": [], "entities": []}, {"text": "Similarly, the second highest ranked metric, METRICSF, combines BLEU, METEOR, the alignment-based metric UPF-COMBALT (, and fluency features.", "labels": [], "entities": [{"text": "METRICSF", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.7972406148910522}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9990054965019226}, {"text": "METEOR", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.8479844331741333}, {"text": "UPF-COMBALT", "start_pos": 105, "end_pos": 116, "type": "METRIC", "confidence": 0.372763991355896}]}, {"text": "The BEER metric, found in fifth position, is a trained evaluation metric with a linear model that combines features capturing character n-grams and permutation trees.", "labels": [], "entities": [{"text": "BEER metric", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9690208435058594}]}, {"text": "We report the rank of HyTERA among all metrics (single and combined), and among the single ones.", "labels": [], "entities": [{"text": "HyTERA", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9641099572181702}]}, {"text": "It is important to note that HyTERA needs no tuning, is straightforward to use and very fast to compute, especially with filtered lattices (on average 6s).", "labels": [], "entities": []}, {"text": "The lower performance of the metric on this dataset is also due to the different nature of the MT systems tested.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9395461082458496}]}, {"text": "While in the (Dreyer and Marcu, 2012) evaluation, the systems came from the 2010 Open MT NIST evaluation and were selected to cover a variety of architectures and performances, the systems that participated in WMT15 are, for the large part, neural MT systems (.", "labels": [], "entities": [{"text": "Open MT NIST evaluation", "start_pos": 81, "end_pos": 104, "type": "DATASET", "confidence": 0.5573934242129326}, {"text": "WMT15", "start_pos": 210, "end_pos": 215, "type": "DATASET", "confidence": 0.7677679657936096}]}, {"text": "As reported by, Neural MT systems make at least 17% fewer lexical choice errors than phrase-based systems, which limits the potential of HyTERA, primarily focused on capturing correct lexical choice.", "labels": [], "entities": [{"text": "Neural MT", "start_pos": 16, "end_pos": 25, "type": "TASK", "confidence": 0.7269188463687897}]}], "tableCaptions": [{"text": " Table 2 reports the  correlation between HyTER, HyTERA and hTER at  the sentence level. We also include as a base- line the correlation with the sentence-level BLEU", "labels": [], "entities": [{"text": "HyTER", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.931598424911499}, {"text": "HyTERA", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9104865789413452}, {"text": "BLEU", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.96872878074646}]}, {"text": " Table 2: Correlation, calculated by Spearman's \u03c1, between the scores of translation hypotheses estimated by  HyTER and hTER.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9627960324287415}, {"text": "Spearman's \u03c1", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.6160688400268555}, {"text": "HyTER", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.8126052618026733}]}, {"text": " Table 3: Pearson correlation between HyTERA and human judgments at the segment level on WMT16 Metrics  Shared Task data on different language pairs. We compare to the scores of the SENTBLEU baseline. We report  the best correlation achieved by the participating metrics and the rank of HyTERA among all 15 participants, and  among the single 13 metrics left after excluding combined ones.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9863706827163696}, {"text": "WMT16", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.9455716609954834}, {"text": "SENTBLEU baseline", "start_pos": 182, "end_pos": 199, "type": "DATASET", "confidence": 0.7177810072898865}, {"text": "HyTERA", "start_pos": 287, "end_pos": 293, "type": "METRIC", "confidence": 0.8934754729270935}]}]}