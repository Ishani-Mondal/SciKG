{"title": [{"text": "A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications", "labels": [], "entities": []}], "abstractContent": [{"text": "Peer reviewing is a central component in the scientific publishing process.", "labels": [], "entities": [{"text": "Peer reviewing", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8956066370010376}]}, {"text": "We present the first public dataset of scientific peer reviews available for research purposes (PeerRead v1), 1 providing an opportunity to study this important artifact.", "labels": [], "entities": [{"text": "PeerRead v1)", "start_pos": 96, "end_pos": 108, "type": "DATASET", "confidence": 0.8291920820871989}]}, {"text": "The dataset consists of 14.7K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR.", "labels": [], "entities": [{"text": "ACL", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.9203642010688782}, {"text": "NIPS", "start_pos": 123, "end_pos": 127, "type": "DATASET", "confidence": 0.7531439661979675}, {"text": "ICLR", "start_pos": 132, "end_pos": 136, "type": "DATASET", "confidence": 0.859599232673645}]}, {"text": "The dataset also includes 10.7K textual peer reviews written by experts fora subset of the papers.", "labels": [], "entities": []}, {"text": "We describe the data collection process and report interesting observed phenomena in the peer reviews.", "labels": [], "entities": []}, {"text": "We also propose two novel NLP tasks based on this dataset and provide simple base-line models.", "labels": [], "entities": []}, {"text": "In the first task, we show that simple models can predict whether a paper is accepted with up to 21% error reduction compared to the majority baseline.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 101, "end_pos": 116, "type": "METRIC", "confidence": 0.9784682989120483}]}, {"text": "In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean base-line for aspects with high variance such as 'originality' and 'impact'.", "labels": [], "entities": []}], "introductionContent": [{"text": "Prestigious scientific venues use peer reviewing to decide which papers to include in their journals or proceedings.", "labels": [], "entities": []}, {"text": "While this process seems essential to scientific publication, it is often a subject of debate.", "labels": [], "entities": [{"text": "scientific publication", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.68545863032341}]}, {"text": "Recognizing the important consequences of peer reviewing, several researchers studied various aspects of the process, including consistency, bias, author response and general review quality (e.g.,.", "labels": [], "entities": [{"text": "consistency", "start_pos": 128, "end_pos": 139, "type": "METRIC", "confidence": 0.9877402186393738}]}, {"text": "For example, the organizers of 1 https://github.com/allenai/PeerRead the NIPS 2014 conference assigned 10% of conference submissions to two different sets of reviewers to measure the consistency of the peer reviewing process, and observed that the two committees disagreed on the accept/reject decision for more than a quarter of the papers (.", "labels": [], "entities": [{"text": "PeerRead the NIPS 2014 conference", "start_pos": 60, "end_pos": 93, "type": "DATASET", "confidence": 0.7048723459243774}]}, {"text": "Despite these efforts, quantitative studies of peer reviews had been limited, for the most part, to the few individuals who had access to peer reviews of a given venue (e.g., journal editors and program chairs).", "labels": [], "entities": []}, {"text": "The goal of this paper is to lower the barrier to studying peer reviews for the scientific community by introducing the first public dataset of peer reviews for research purposes: PeerRead.", "labels": [], "entities": [{"text": "PeerRead", "start_pos": 180, "end_pos": 188, "type": "DATASET", "confidence": 0.8513732552528381}]}, {"text": "We use three strategies to construct the dataset: (i) We collaborate with conference chairs and conference management systems to allow authors and reviewers to opt-in their paper drafts and peer reviews, respectively.", "labels": [], "entities": []}, {"text": "(ii) We crawl publicly available peer reviews and annotate textual reviews with numerical scores for aspects such as 'clarity' and 'impact'.", "labels": [], "entities": [{"text": "clarity", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.990170419216156}]}, {"text": "(iii) We crawl arXiv submissions which coincide with important conference submission dates and check whether a similar paper appears in proceedings of these conferences at a later date.", "labels": [], "entities": []}, {"text": "In total, the dataset consists of 14.7K paper drafts and the corresponding accept/reject decisions, including a subset of 3K papers for which we have 10.7K textual reviews written by experts.", "labels": [], "entities": []}, {"text": "We plan to make periodic releases of PeerRead, adding more sections for new venues every year.", "labels": [], "entities": [{"text": "PeerRead", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.8298937678337097}]}, {"text": "We provide more details on data collection in \u00a72.", "labels": [], "entities": [{"text": "data collection", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.67521932721138}]}, {"text": "The PeerRead dataset can be used in a variety of ways.", "labels": [], "entities": [{"text": "PeerRead dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9392364621162415}]}, {"text": "A quantitative analysis of the peer reviews can provide insights to help better understand (and potentially improve) various nuances of the review process.", "labels": [], "entities": []}, {"text": "For example, in \u00a73, we analyze correlations between the overall recommendation score and individual aspect scores (e.g., clarity, impact and originality) and quantify how reviews recom-: The PeerRead dataset.", "labels": [], "entities": [{"text": "clarity", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9971832633018494}, {"text": "PeerRead dataset", "start_pos": 191, "end_pos": 207, "type": "DATASET", "confidence": 0.9323133528232574}]}, {"text": "indicates whether the reviews have aspect specific scores (e.g., clarity).", "labels": [], "entities": [{"text": "clarity", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.999413013458252}]}, {"text": "Note that ICLR contains the aspect scores assigned by our annotators (see Section 2.4).", "labels": [], "entities": [{"text": "ICLR", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.45267146825790405}]}, {"text": "Acc/Rej is the distribution of accepted/rejected papers.", "labels": [], "entities": [{"text": "Acc/Rej", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8746379017829895}]}, {"text": "Note that NIPS provide reviews only for accepted papers.", "labels": [], "entities": [{"text": "NIPS", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8751170635223389}]}, {"text": "mending an oral presentation differ from those recommending a poster.", "labels": [], "entities": [{"text": "mending an oral presentation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.810099646449089}]}, {"text": "Other examples might include aligning review scores with authors to reveal gender or nationality biases.", "labels": [], "entities": []}, {"text": "From a pedagogical perspective, the PeerRead dataset also provides inexperienced authors and first-time reviewers with diverse examples of peer reviews.", "labels": [], "entities": [{"text": "PeerRead dataset", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.9257967174053192}]}, {"text": "As an NLP resource, peer reviews raise interesting challenges, both from the realm of sentiment analysis-predicting various properties of the reviewed paper, e.g., clarity and novelty, as well as that of text generation-given a paper, automatically generate its review.", "labels": [], "entities": [{"text": "sentiment analysis-predicting", "start_pos": 86, "end_pos": 115, "type": "TASK", "confidence": 0.9201538860797882}, {"text": "clarity", "start_pos": 164, "end_pos": 171, "type": "METRIC", "confidence": 0.9944509863853455}, {"text": "novelty", "start_pos": 176, "end_pos": 183, "type": "METRIC", "confidence": 0.7373236417770386}, {"text": "text generation-given a paper", "start_pos": 204, "end_pos": 233, "type": "TASK", "confidence": 0.7885065078735352}]}, {"text": "Such NLP tasks, when solved with sufficiently high quality, might help reviewers, area chairs and program chairs in the reviewing process, e.g., by lowering the number of reviewers needed for some paper submission.", "labels": [], "entities": []}, {"text": "In \u00a74, we introduce two new NLP tasks based on this dataset: (i) predicting whether a given paper would be accepted to some venue, and (ii) predicting the numerical score of certain aspects of a paper.", "labels": [], "entities": []}, {"text": "Our results show that we can predict the accept/reject decisions with 6-21% error reduction compared to the majority reject-all baseline, in four different sections of PeerRead.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 76, "end_pos": 91, "type": "METRIC", "confidence": 0.9781581163406372}, {"text": "PeerRead", "start_pos": 168, "end_pos": 176, "type": "DATASET", "confidence": 0.931644082069397}]}, {"text": "Since the baseline models we use are fairly simple, there is plenty of room to develop stronger models to make better predictions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we describe the collection and compilation of PeerRead, our scientific peer-review dataset.", "labels": [], "entities": [{"text": "PeerRead", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8750410079956055}]}, {"text": "For an overview of the dataset, see.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The PeerRead dataset. Asp. indicates  whether the reviews have aspect specific scores  (e.g., clarity). Note that ICLR contains the aspect  scores assigned by our annotators (see Section 2.4).  Acc/Rej is the distribution of accepted/rejected pa- pers. Note that NIPS provide reviews only for  accepted papers.", "labels": [], "entities": [{"text": "PeerRead dataset", "start_pos": 14, "end_pos": 30, "type": "DATASET", "confidence": 0.8310303092002869}, {"text": "clarity", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9989495873451233}, {"text": "ICLR", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.6139111518859863}, {"text": "Acc/Rej", "start_pos": 204, "end_pos": 211, "type": "METRIC", "confidence": 0.9409847060839335}, {"text": "NIPS", "start_pos": 273, "end_pos": 277, "type": "DATASET", "confidence": 0.897531270980835}]}, {"text": " Table 3. No- tably, the average 'overall recommendation' score  in reviews recommending an oral presentation is  0.9 higher than in reviews recommending a poster  presentation, suggesting that reviewers tend to rec- ommend oral presentation for submissions which  are holistically stronger.", "labels": [], "entities": []}, {"text": " Table 3: Mean review scores for each presenta- tion format (oral vs. poster). Raw scores range  between 1-5. For reference, the last column shows  the sample standard deviation based on all reviews.", "labels": [], "entities": []}, {"text": " Table 4: Mean \u00b1 standard deviation of various  measurements on reviews in the ACL 2017 and  ICLR 2017 sections of PeerRead. Note that ACL  aspects were written by the reviewers themselves,  while ICLR aspects were predicted by our annota- tors based on the review.", "labels": [], "entities": [{"text": "Mean \u00b1 standard deviation", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.9459097683429718}, {"text": "ACL 2017 and  ICLR 2017 sections of PeerRead", "start_pos": 79, "end_pos": 123, "type": "DATASET", "confidence": 0.824210487306118}]}, {"text": " Table 5: Test accuracies (%) for acceptance classi- fication. Our best model outperforms the majority  classifiers in all cases.", "labels": [], "entities": [{"text": "Test accuracies", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7562571465969086}]}]}