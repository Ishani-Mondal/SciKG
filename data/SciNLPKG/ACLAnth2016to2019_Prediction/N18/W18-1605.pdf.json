{"title": [{"text": "Cross-corpus Native Language Identification via Statistical Embedding", "labels": [], "entities": [{"text": "Cross-corpus Native Language Identification", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5904560834169388}]}], "abstractContent": [{"text": "In this paper, we approach the task of native language identification in a realistic cross-corpus scenario where a model is trained with available data and has to predict the native language from data of a different corpus.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6470024486382803}]}, {"text": "We have proposed a statistical embedding representation reporting a significant improvement over common single-layer approaches of the state of the art, identifying Chinese, Arabic, and Indonesian in a cross-corpus scenario.", "labels": [], "entities": []}, {"text": "The proposed approach was shown to be competitive even when the data is scarce and imbalanced.", "labels": [], "entities": []}], "introductionContent": [{"text": "Native Language Identification (NLI) is the task of identifying the native language (L1) of a writer based solely on a textual sample of their writing in a second language (L2), for example, essays in English by students from China, Indonesia or Arabic-speaking countries.", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7765243103106817}]}, {"text": "NLI is very important for education, since it can lead to the provision of more targeted feedback to language learners about their most common errors.", "labels": [], "entities": []}, {"text": "It is also of interest for forensics, security and marketing.", "labels": [], "entities": []}, {"text": "For example, knowing the possible native language of the user who wrote a potentially threatening message may help to better profile that user and the potential scope of the threat.", "labels": [], "entities": []}, {"text": "The first Native Language Identification shared task was organised in ).", "labels": [], "entities": [{"text": "Native Language Identification shared task", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.7650757730007172}]}, {"text": "The twenty-nine teams had to classify essays written in English (L2) in one of the eleven possible native languages (L1).", "labels": [], "entities": []}, {"text": "The most common features were word, character and POS n-grams, and the reported accuracies rose to 83.6%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9949579834938049}]}, {"text": "The Support Vector Machine (SVM) has been the most prevalent classification approach.", "labels": [], "entities": []}, {"text": "Furthermore, participants were allowed to train their models with external data, specifically i) any kind of external data, excluding TOEFL 1 ( ; or ii) any kind of external data, including TOEFL.", "labels": [], "entities": [{"text": "TOEFL", "start_pos": 190, "end_pos": 195, "type": "DATASET", "confidence": 0.8908031582832336}]}, {"text": "The reported accuracies show that, when training only with external data, the results fall to 56.5%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9939071536064148}]}, {"text": "Recently, the 2017 Native Language Identification Shared Task () has been organised with the aim of identifying the native language of written texts, alongside a second task on spoken transcripts and low dimensional audio file representations as data (although original audio files were not shared).", "labels": [], "entities": [{"text": "Native Language Identification Shared Task", "start_pos": 19, "end_pos": 61, "type": "TASK", "confidence": 0.7145464658737183}, {"text": "identifying the native language of written texts", "start_pos": 100, "end_pos": 148, "type": "TASK", "confidence": 0.7304788657597133}]}, {"text": "The organisers included the macro-averaged F1-score (Yang and Liu, 1999) since it evaluates the performance across classes more consistently.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9566619396209717}]}, {"text": "Although deep learning approaches were widely used, the best results (up to 88.18%) were achieved with classical methods such as SVM and n-grams.", "labels": [], "entities": []}, {"text": "Despite participants being allowed to use external data, there were no such submissions, possibly also due to the poor results obtained in the previous edition (56.5% of accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9987622499465942}]}, {"text": "We are interested in the following cross-corpus scenario: a model trained with data from external sources (e.g. social media).", "labels": [], "entities": []}, {"text": "The authors in) used the EF Cambridge Open Language Database (EFCamDat)) for training and TOEFL for evaluation, and vice versa.", "labels": [], "entities": [{"text": "EF Cambridge Open Language Database (EFCamDat))", "start_pos": 25, "end_pos": 72, "type": "DATASET", "confidence": 0.9374997019767761}, {"text": "TOEFL", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.7754068970680237}]}, {"text": "They trained a linear-SVM with several features such as function word unigrams and bigrams, production rules and part-of-speech unigrams, bigrams and trigrams, and the combination of all of them.", "labels": [], "entities": []}, {"text": "The authors reported an accuracy of 33.45% when training with EFCamDat and evaluated on TOEFL, and an accuracy of 28.42% when training on TOEFL, and evaluated on EFCamDat, in contrast to the accuracy of 64.95% obtained when evaluating intra-corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9995104074478149}, {"text": "EFCamDat", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9485037326812744}, {"text": "TOEFL", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.8618901371955872}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9993950128555298}, {"text": "TOEFL", "start_pos": 138, "end_pos": 143, "type": "DATASET", "confidence": 0.8791037201881409}, {"text": "EFCamDat", "start_pos": 162, "end_pos": 170, "type": "DATASET", "confidence": 0.9600891470909119}, {"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9985583424568176}]}, {"text": "The authors in () evaluated String Kernels in a cross-corpus scenario (TOEFL11 for training and TOEFL11-Big () for evaluation).", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.6407307386398315}, {"text": "TOEFL11-Big", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.7519832849502563}]}, {"text": "They reported significant improvements over the state of the art with accuracies up to 67.7%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9973512887954712}]}, {"text": "The authors explain these results by arguing \"that string kernels are language independent, and for the same reasons they can also be topic independent\".", "labels": [], "entities": []}, {"text": "In this work, we propose to follow the methodology represented in.", "labels": [], "entities": []}, {"text": "Given a set of corpora C, we learn a model with all the corpora together except c, which is used to evaluate the model.", "labels": [], "entities": []}, {"text": "To evaluate the task, we have proposed a statistical embedding representation that we have compared with common single-layer approaches based on n-grams, obtaining encouraging results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report and discuss the obtained results.", "labels": [], "entities": []}, {"text": "Firstly, we focus on the described corpora for the languages of interest.", "labels": [], "entities": []}, {"text": "Then, we analyse as a case study the Australian academic scenario.", "labels": [], "entities": [{"text": "Australian academic scenario", "start_pos": 37, "end_pos": 65, "type": "DATASET", "confidence": 0.8620752096176147}]}, {"text": "Due to the imbalance of the data, we use a macroaveraged F1-score which gives the same importance to the different classes no matter their size.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9526338577270508}]}], "tableCaptions": [{"text": " Table 1: Number of documents in each corpus. L1 cor- responds to the documents written by authors of the  native language to be identified. Others comprise all  the documents written by authors of the other native  languages in the corpus.", "labels": [], "entities": []}, {"text": " Table 2. The second and  third columns show respectively the corpus used  for training and test: Lang8 and TOEFL include  Arabic and Chinese, whereas Indonesian is in- cluded in Lang8 and ICNALE. The fourth column  shows the best result obtained by the baseline: 8 ,  whereas the fifth column shows the result obtained  with LDSE.", "labels": [], "entities": [{"text": "TOEFL", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.5714015364646912}, {"text": "ICNALE", "start_pos": 189, "end_pos": 195, "type": "DATASET", "confidence": 0.903971791267395}, {"text": "LDSE", "start_pos": 326, "end_pos": 330, "type": "METRIC", "confidence": 0.4926288425922394}]}, {"text": " Table 2: Results in macro-averaged F1-score. The  baseline corresponds to the best result obtained with  character or word n-grams. The last column shows the  improvement percentage achieved by LDSE over the  baseline.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9453787803649902}]}]}