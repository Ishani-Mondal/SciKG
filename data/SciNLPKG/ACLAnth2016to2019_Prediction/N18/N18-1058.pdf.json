{"title": [{"text": "Self-Training for Jointly Learning to Ask and Answer Questions", "labels": [], "entities": [{"text": "Jointly Learning to Ask and Answer Questions", "start_pos": 18, "end_pos": 62, "type": "TASK", "confidence": 0.8243162631988525}]}], "abstractContent": [{"text": "Building curious machines that can answer as well as ask questions is an important challenge for AI.", "labels": [], "entities": []}, {"text": "The two tasks of question answering and question generation are usually tackled separately in the NLP literature.", "labels": [], "entities": [{"text": "question answering", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.9141474366188049}, {"text": "question generation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8447641730308533}]}, {"text": "At the same time, both require significant amounts of supervised data which is hard to obtain in many domains.", "labels": [], "entities": []}, {"text": "To alleviate these issues, we propose a self-training method for jointly learning to ask as well as answer questions, leveraging unlabeled text along with labeled question answer pairs for learning.", "labels": [], "entities": []}, {"text": "We evaluate our approach on four benchmark datasets: SQUAD, MS MARCO, WikiQA and TrecQA, and show significant improvements over a number of established baselines on both question answering and question generation tasks.", "labels": [], "entities": [{"text": "SQUAD", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.8504393696784973}, {"text": "MS", "start_pos": 60, "end_pos": 62, "type": "DATASET", "confidence": 0.90171879529953}, {"text": "MARCO", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.45647117495536804}, {"text": "WikiQA", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.8422406315803528}, {"text": "TrecQA", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.6671810746192932}, {"text": "question answering and question generation tasks", "start_pos": 170, "end_pos": 218, "type": "TASK", "confidence": 0.80716406305631}]}, {"text": "We also achieved new state-of-the-art results on two competitive answer sentence selection tasks: WikiQA and TrecQA.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question Answering (QA) is a well-studied problem in NLP which focuses on answering questions using some structured or unstructured sources of knowledge.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9039630770683289}]}, {"text": "Alongside question answering, there has also been some work on generating questions (QG)) which focuses on generating questions based on given sources of knowledge.", "labels": [], "entities": [{"text": "question answering", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8957095742225647}]}, {"text": "QA and QG are closely related 1 tasks.", "labels": [], "entities": [{"text": "QA", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.6514233350753784}]}, {"text": "However, NLP literature views the two as entirely separate tasks.", "labels": [], "entities": []}, {"text": "In this paper, we explore this relationship between the two tasks by jointly learning to generate as well as answer questions.", "labels": [], "entities": []}, {"text": "An improved ability to generate as well as answer questions will help us build curious machines that can interact with humans in a better manner.", "labels": [], "entities": []}, {"text": "Joint modeling of We can think of QA and QG as inverse of each other.", "labels": [], "entities": []}, {"text": "QA and QG is useful as the two can be used in conjunction to generate novel questions from free text and then answers for the generated questions.", "labels": [], "entities": [{"text": "QA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.5400692224502563}]}, {"text": "We use this idea to perform self-training) and leverage free text to augment the training of QA and QG models.", "labels": [], "entities": []}, {"text": "QA and QG models are typically trained on question answer pairs which are expensive to obtain in many domains.", "labels": [], "entities": []}, {"text": "However, it is cheaper to obtain large quantities of free text.", "labels": [], "entities": []}, {"text": "Our selftraining procedure leverages unlabeled text to boost the quality of our QA and QG models.", "labels": [], "entities": []}, {"text": "This is achieved by a careful data augmentation procedure which uses pre-trained QA and QG models to generate additional labeled question answer pairs.", "labels": [], "entities": []}, {"text": "This additional data is then used to retrain our QA and QG models and the procedure is repeated.", "labels": [], "entities": []}, {"text": "This addition of synthetic labeled data needs to be performed carefully.", "labels": [], "entities": []}, {"text": "During self-training, typically the most confident samples are added to the training set () in each iteration.", "labels": [], "entities": []}, {"text": "We use the performance of our QA and QG models as a proxy for estimating the confidence value of the questions.", "labels": [], "entities": []}, {"text": "We describe a suite of heuristics inspired from curriculum learning ( to select the questions to be generated and added to the training set at each epoch.", "labels": [], "entities": []}, {"text": "Curriculum learning is inspired from the incremental nature of human learning and orders training samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively.", "labels": [], "entities": []}, {"text": "We show that introducing questions in increasing order of hardness leads to improvements over a baseline that introduces questions randomly.", "labels": [], "entities": []}, {"text": "We use a seq2seq model with soft attention () for QG and a neural model inspired from Attentive Reader ( for QA. However, these can be any QA and QG models.", "labels": [], "entities": []}, {"text": "We evaluate our approach on four datasets: SQUAD, MS MARCO, WikiQA and TrecQA.", "labels": [], "entities": [{"text": "SQUAD", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.8972734808921814}, {"text": "MS", "start_pos": 50, "end_pos": 52, "type": "DATASET", "confidence": 0.9396100044250488}, {"text": "MARCO", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.4829059839248657}, {"text": "WikiQA", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.8993744254112244}, {"text": "TrecQA", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.8138911724090576}]}, {"text": "We use a corpus of English Wikipedia as unlabeled text.", "labels": [], "entities": []}, {"text": "Our experiments show that the self-training approach leads to significant improvements over a number of established approaches in QA and QG on these benchmarks.", "labels": [], "entities": []}, {"text": "On the two answer sentence selection QA tasks: (WikiQA and TrecQA), we obtain state-of-the-art.", "labels": [], "entities": [{"text": "answer sentence selection QA", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.6451353803277016}, {"text": "WikiQA", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.947584331035614}]}], "datasetContent": [{"text": "Implementation Details: We perform the same preprocessing on all the text.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8666957020759583}]}, {"text": "We lower-case all the text.", "labels": [], "entities": []}, {"text": "We use NLTK for word tokenization.", "labels": [], "entities": [{"text": "word tokenization", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7131334841251373}]}, {"text": "For training our neural networks, we only keep the most frequent 50k words (including entity and placeholder markers), and map all other words to a special <UNK> token.", "labels": [], "entities": []}, {"text": "We choose word embedding size d = 100, and use the 100-dimensional pretrained GloVe word embeddings () for initialization.", "labels": [], "entities": []}, {"text": "We set k, m, k 1 and k 2 (hyperparameters for self-training) by grid search on a held-out development set. workers.", "labels": [], "entities": []}, {"text": "Finally, TrecQA is a QA answer sentence selection dataset from the TREC QA track.", "labels": [], "entities": [{"text": "TrecQA", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9103868007659912}, {"text": "QA answer sentence selection", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.6152988746762276}, {"text": "TREC QA track", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.829306165377299}]}, {"text": "While WikiQA and TrecQA are directly answer sentence selection tasks, the other two are not.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 6, "end_pos": 12, "type": "DATASET", "confidence": 0.8981951475143433}, {"text": "TrecQA", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.6034857630729675}, {"text": "answer sentence selection tasks", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6787460595369339}]}, {"text": "Hence, we treat the SQUAD and MS MARCO tasks as the answer sentence selection task assuming a one to one correspondence between answer sentences and annotated correct answer spans.", "labels": [], "entities": [{"text": "SQUAD", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.6594879031181335}, {"text": "MS", "start_pos": 30, "end_pos": 32, "type": "DATASET", "confidence": 0.7279836535453796}, {"text": "MARCO", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.6208813190460205}, {"text": "answer sentence selection", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.7137517929077148}]}, {"text": "Note that only a very small proportion of answers (< 0.2% in training set) span two or more sentences.", "labels": [], "entities": []}, {"text": "Since SQUAD and MS MARCO have a hidden test set, we only use the training and development sets for our evaluation purposes and we further split the provided development set into a dev and test set.", "labels": [], "entities": [{"text": "SQUAD", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.8623108863830566}, {"text": "MS MARCO", "start_pos": 16, "end_pos": 24, "type": "DATASET", "confidence": 0.742818146944046}]}, {"text": "This is also the data analysis setting used in previous works (.", "labels": [], "entities": []}, {"text": "In fact, we use the same setting as in  for comparison.", "labels": [], "entities": []}, {"text": "The statistics of the four datasets and the respective train, dev and test splits are given in.", "labels": [], "entities": []}, {"text": "For WikiQA and TrecQA datasets, we use the standard data splits.", "labels": [], "entities": [{"text": "TrecQA datasets", "start_pos": 15, "end_pos": 30, "type": "DATASET", "confidence": 0.9128853380680084}]}, {"text": "We use a large randomly subsampled corpus of English Wikipedia and use the first paragraph of each document as unlabeled text for self-training.", "labels": [], "entities": []}, {"text": "For QG, we compare our model against the following four baselines used in previous work (.", "labels": [], "entities": []}, {"text": "The first baseline is a simple IR baselines taken from which generates questions by memorizing them from the training set and uses edit distance to calculate distance between a question and the input sentence.", "labels": [], "entities": []}, {"text": "The second baseline is a MT system -MOSES ( which models question generation as a translation task where raw sentences are treated as source texts and questions are treated as target texts.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9679931402206421}, {"text": "question generation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7217170000076294}]}, {"text": "The third baseline, DirectIn, uses the longest sub-sentence of the input sentence (using a set of simple sentence splitters) as the question.", "labels": [], "entities": []}, {"text": "The fourth baseline, H&S is a rule-based overgenerate-and-rank system proposed by.", "labels": [], "entities": []}, {"text": "The Question Selection Oracle: The first question we wish to answer is: Is careful question selection even necessary?", "labels": [], "entities": [{"text": "Question Selection Oracle", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7422749300797781}]}, {"text": "To answer this, we plot MAP scores for our best QA model (QA+QG, Ensemble+E&E) when we do not have a curriculum learning based oracle (i.e. an oracle which picks questions to be added to the dataset randomly) in as a function of epochs.", "labels": [], "entities": []}, {"text": "We observe that   the MAP score degrades instead of improving with time.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9602933526039124}]}, {"text": "This supports our claim that we need to augment the training set by a more careful procedure.", "labels": [], "entities": []}, {"text": "We also plot MAP scores for our best QA model (Ensemble+E&E) when we use various question selection oracles as a function of the amount of unlabeled data in.", "labels": [], "entities": [{"text": "MAP", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.5074605345726013}]}, {"text": "We can observe that when we do not have a curriculum learning based oracle, the MAP score degrades by having more and more unlabeled data.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.8412306606769562}]}, {"text": "We also observe that the QA+QG oracle performs better than QA and QG which confirms that the best oracle is one that selects questions in increasing degree of hardness in terms of both question answering and question generation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.738280713558197}, {"text": "question generation", "start_pos": 208, "end_pos": 227, "type": "TASK", "confidence": 0.7703339159488678}]}, {"text": "This holds for all the experimental settings.", "labels": [], "entities": []}, {"text": "Thus we only show results for the QA+QG strategies in our future experiments.", "labels": [], "entities": []}, {"text": "Evaluating Question Answering: First, we evaluate our models on the question answering task.", "labels": [], "entities": [{"text": "Evaluating Question Answering", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6641866763432821}, {"text": "question answering task", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.779483030239741}]}, {"text": "Ensemble+E&E(K) is the variant where we perform self-training using K Wikipedia paragraphs.", "labels": [], "entities": []}, {"text": "Hence, Ensemble+E&E(0) is the variant of our MAP MRR CNN ( 0.665 0.652 APCNN ( 0.696 0.689 NASM ( 0.707 0.689 ABCNN ( 0.702 0.692 KVMN ( 0.707 0.727 0.706 0.723 0.734 0.742 0.743 0.755  0.700 0.754 0.753  model without any self-training.", "labels": [], "entities": [{"text": "MAP MRR CNN", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.8062289953231812}]}, {"text": "We vary K to seethe impact of the size of unlabeled Wikipedia paragraphs on the self-training model.", "labels": [], "entities": []}, {"text": "shows the results of the QA evaluations on the SQUAD and MS MARCO datasets.", "labels": [], "entities": [{"text": "QA", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.851534903049469}, {"text": "SQUAD", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.9413860440254211}, {"text": "MS MARCO datasets", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.8468245267868042}]}, {"text": "We can observe that our QA model has competetive or better performance overall the baselines on both datasets in terms of all the three evaluation metrics.", "labels": [], "entities": []}, {"text": "When we incorporate ensembling or diversity, we see a further improvement in the result.", "labels": [], "entities": []}, {"text": "show results of QA evaluations on the WikiQA and TrecQA datasets, respectively.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.9668900966644287}, {"text": "TrecQA datasets", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.9118304252624512}]}, {"text": "We can again observe that our QA model is competitive to all the baselines.", "labels": [], "entities": []}, {"text": "When we introduce ensembling and diversity while jointly learning the QA and QG models, we see incremental improvements.", "labels": [], "entities": []}, {"text": "In both these answer sentence selection tasks, our approach achieves new state-of-the-art.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.7507633368174235}]}, {"text": "Evaluating Question Generation: shows the results for QG on the four datasets on each of the three evaluation metrics on all the four datasets.", "labels": [], "entities": [{"text": "Evaluating Question Generation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5864025155703226}]}, {"text": "We can observe that the QG model described in our paper performs much better than all the baselines.", "labels": [], "entities": []}, {"text": "We again observe that self-training while jointly training the QA and QG models leads to even better performance.", "labels": [], "entities": []}, {"text": "These results show that self-training and leveraging the relationship between QA and QG is very useful for boosting the performance of the QA and QG models, while additionally only using cheap unlabeled data.", "labels": [], "entities": []}, {"text": "Human Evaluations: We asked two people not involved with this research to evaluate 1000 (randomly selected) questions generated by our best QG model and our best performing baseline ( on SQUAD for fluency and correctness on a scale of 1 to 5.", "labels": [], "entities": []}, {"text": "The raters were also shown the passage sentence used to generate the question.", "labels": [], "entities": []}, {"text": "The raters were blind to which system produced which question.", "labels": [], "entities": []}, {"text": "The Pearson correlation between the raters' judgments was r = 0.89 for fluency and r = 0.78 for correctness.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9718082845211029}]}, {"text": "In our analyses, we used the averages of the two raters' judgments.", "labels": [], "entities": []}, {"text": "The evaluation showed that our system generates questions that are more fluent and correct than those by the baseline.", "labels": [], "entities": []}, {"text": "The mean fluency rating of our best system was 4.15 compared to 3.35 for the baseline, a difference which is statistically significant (t-test, p < 0.001).", "labels": [], "entities": []}, {"text": "Evaluating the Question Selection Oracle: As discussed earlier, the choice of which subset of questions to add to our labeled dataset while selftraining is important.", "labels": [], "entities": [{"text": "Question Selection Oracle", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.7316874464352926}]}, {"text": "To evaluate the various heuristics proposed in our paper, we show the effect of the question selection oracle on the final QA and QG performance in, 4 and 5.", "labels": [], "entities": []}, {"text": "These comparisions are shown in the shaded grey portions of the tables for self-training with 10,000 Wikipedia paragraphs as unlabeled data.", "labels": [], "entities": []}, {"text": "We can observe that all the proposed heuristics (and ensembling and diversity strategies) lead to improvements in the final performance of both QA and QG.", "labels": [], "entities": []}, {"text": "The heuristics arranged in increasing order of performance are: M 2 , ECiO, GO, CiO and CiO-ECiO.", "labels": [], "entities": [{"text": "GO", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9433025121688843}]}, {"text": "While the choice of which heuristic to pick seems to make a lesser impact on the final performance, we do see a much more significant performance gain by ensembling to combine the various heuristics and using E&E to incorporate diversity.", "labels": [], "entities": []}, {"text": "The incorporatation of diversity is important because the neural network models which learnt latent representions of data usually find it hard to adjust to new level of difficulty of questions as the current representation may not be appropriate for the new level of difficulty.", "labels": [], "entities": []}, {"text": "Low data scenarios: A key advantage of our selftraining approach is that it can leverage unlabeled text, and thus requires less labeled data.", "labels": [], "entities": []}, {"text": "To test this, we plot MAP for our best self-training model and various QA baselines as we vary the proportion of labeled training set in.", "labels": [], "entities": [{"text": "MAP", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.7555810213088989}]}, {"text": "However, we keep the unlabeled text fixed (10K Wikipedia paragraphs).", "labels": [], "entities": []}, {"text": "We observe that all the baselines significantly drop in performance as we reduce the proportion of labeled training set.", "labels": [], "entities": []}, {"text": "However, the drop happens at a much slower rate for our selftrained model.", "labels": [], "entities": []}, {"text": "Thus, we can conclude that our approach requires less labeled data as compared to the baselines.", "labels": [], "entities": []}, {"text": "text always improve our models?", "labels": [], "entities": []}, {"text": "Will the performance improve if we add more and more unsupervised data during self-training.", "labels": [], "entities": []}, {"text": "According to our results in, 4 and 5, the answer is \"probably yes\".", "labels": [], "entities": []}, {"text": "As we can observe from these tables, the performance of the QA and QG models improves as we increase K, the size of the unsupervised data during training of the various Ensemble+E&E(K) models.", "labels": [], "entities": []}, {"text": "Having said that, we do see a tapering effect on the performance results, so it is clear that the performance will be capped by some upper-bound and we will need better ways of modeling language and meaning to make progress.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the four datasets used in evaluating our QA and QG models.", "labels": [], "entities": []}, {"text": " Table 1. For WikiQA and TrecQA  datasets, we use the standard data splits. We use  a large randomly subsampled corpus of English  Wikipedia and use the first paragraph of each doc- ument as unlabeled text for self-training.", "labels": [], "entities": [{"text": "TrecQA  datasets", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.8761575520038605}]}, {"text": " Table 2: Performance of our models and QA baselines  on SQUAD and MS MARCO datasets. Shaded part of  the table shows results of various question selection  heuristics when 10000 Wiki paragraphs are used as un- labeled data.", "labels": [], "entities": [{"text": "MS MARCO datasets", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.8154421250025431}]}, {"text": " Table 3: Performance of our models and the QA base- lines on the WikiQA dataset. Shaded part of the table  shows the effect of various question selection heuris- tics when 10000 Wikipedia paragraphs are used as un- labeled data. Our model achieves the state-of-the-art.", "labels": [], "entities": [{"text": "WikiQA dataset", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.9756916761398315}]}, {"text": " Table 4: Performance of our models and the QA base- lines on the TrecQA dataset. Shaded part of the table  shows the effect of various question selection heuris- tics when 10000 Wikipedia paragraphs are used as un- labeled data. Our model achieves the state-of-the-art.", "labels": [], "entities": [{"text": "TrecQA dataset", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.9726696610450745}]}, {"text": " Table 5: Performance (B: BLEU4, M: METEOR, and R: ROUGE) of our model variants and various QG baselines  on SQUAD, MS MARCO and WikiQA datasets. The shaded part of the table shows the effect of various question  selection heuristics when 10000 Wikipedia paragraphs are used as unlabeled data. The performance numbers for  Tang et al. (2017) and Du et al. (2017) were not reported for all the settings.", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.5509856343269348}, {"text": "METEOR", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9242656230926514}, {"text": "ROUGE", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.5083829760551453}, {"text": "SQUAD", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.9324620962142944}, {"text": "MS MARCO", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.770327627658844}, {"text": "WikiQA datasets", "start_pos": 129, "end_pos": 144, "type": "DATASET", "confidence": 0.8289615511894226}]}]}