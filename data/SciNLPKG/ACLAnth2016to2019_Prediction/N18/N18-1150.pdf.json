{"title": [{"text": "Deep Communicating Agents for Abstractive Summarization", "labels": [], "entities": [{"text": "Abstractive Summarization", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.632107600569725}]}], "abstractContent": [{"text": "We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing along document for abstractive summariza-tion.", "labels": [], "entities": []}, {"text": "With deep communicating agents, the task of encoding along text is divided across multiple collaborating agents, each in charge of a subsection of the input text.", "labels": [], "entities": []}, {"text": "These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused and coherent summary.", "labels": [], "entities": []}, {"text": "Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single encoder or multiple non-communicating encoders.", "labels": [], "entities": []}], "introductionContent": [{"text": "We focus on the task of abstractive summarization of along document.", "labels": [], "entities": []}, {"text": "In contrast to extractive summarization, where a summary is composed of a subset of sentences or words lifted from the input text as is, abstractive summarization requires the generative ability to rephrase and restructure sentences to compose a coherent and concise summary.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.5604342222213745}]}, {"text": "As recurrent neural networks (RNNs) are capable of generating fluent language, variants of encoder-decoder RNNs () have shown promising results on the abstractive summarization task (.", "labels": [], "entities": [{"text": "abstractive summarization task", "start_pos": 151, "end_pos": 181, "type": "TASK", "confidence": 0.6871566772460938}]}, {"text": "The fundamental challenge, however, is that the strong performance of neural models at encoding short text does not generalize well to long text.", "labels": [], "entities": []}, {"text": "The motivation behind our approach is to be able to dynamically attend to different parts of the input to capture salient facts.", "labels": [], "entities": []}, {"text": "While recent work in sum- marization addresses these issues using improved attention models (), pointer networks with coverage mechanisms (, and coherence-focused training objectives (, an effective mechanism for representing along document remains a challenge.", "labels": [], "entities": [{"text": "sum- marization", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.9303321838378906}]}, {"text": "Simultaneous work has investigated the use of deep communicating agents () for collaborative tasks such as logic puzzles, visual dialog (, and reference games (.", "labels": [], "entities": []}, {"text": "Our work builds on these approaches to propose the first study on using communicating agents to encode long text for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 117, "end_pos": 130, "type": "TASK", "confidence": 0.9862886667251587}]}, {"text": "The key idea of our model is to divide the hard task of encoding along text across multiple collaborating encoder agents, each in charge of a different subsection of the text).", "labels": [], "entities": []}, {"text": "Each of these agents encodes their assigned text independently, and broadcasts their encoding to others, allowing agents to share global context information with one another about different sections of the document.", "labels": [], "entities": []}, {"text": "All agents then adapt the encoding of their assigned text in light of the global context and re- Figure 2: Multi-agent-encoder-decoder overview.", "labels": [], "entities": []}, {"text": "Each agent a encodes a paragraph using a local encoder followed by multiple contextual layers with agent communication through concentrated messages z (k) a at each layer k.", "labels": [], "entities": []}, {"text": "The word context vectors ct a are condensed into agent context c * t . Agent specific generation probabilities, pt a , enable voting for the suitable out-of-vocabulary words (e.g., 'yen') in the final distribution.", "labels": [], "entities": []}, {"text": "peat the process across multiple layers, generating new messages at each layer.", "labels": [], "entities": []}, {"text": "Once each agent completes encoding, they deliver their information to the decoder with a novel contextual agent attention ().", "labels": [], "entities": []}, {"text": "Contextual agent attention enables the decoder to integrate information from multiple agents smoothly at each decoding step.", "labels": [], "entities": []}, {"text": "The network is trained end-to-end using self-critical reinforcement learning) to generate focused and coherent summaries.", "labels": [], "entities": []}, {"text": "Empirical results on the CNN/DailyMail and New York Times datasets demonstrate that multiple communicating encoders lead to higher quality summaries compared to strong baselines, including those based on a single encoder or multiple non-communicating encoders.", "labels": [], "entities": [{"text": "CNN/DailyMail and New York Times datasets", "start_pos": 25, "end_pos": 66, "type": "DATASET", "confidence": 0.8664139360189438}]}, {"text": "Human evaluations indicate that our model is able to produce more focused summaries.", "labels": [], "entities": []}, {"text": "The agents gather salient information from multiple areas of the document, and communicate their information with one another, thus reducing common mistakes such as missing key facts, repeating the same content, or including unnecessary details.", "labels": [], "entities": []}, {"text": "Further analysis reveals that our model attains better performance when the decoder interacts with multiple agents in a more balanced way, confirming the benefit of representing along document with multiple encoding agents.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets We conducted experiments on two summarization datasets: CNN/DailyMail ( and New York Times (NYT).", "labels": [], "entities": [{"text": "CNN/DailyMail", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.8857933282852173}, {"text": "New York Times (NYT)", "start_pos": 85, "end_pos": 105, "type": "DATASET", "confidence": 0.7485364278157552}]}, {"text": "We replicate the preprocessing steps of to obtain the same data splits, except that we do not anonymize named entities.", "labels": [], "entities": []}, {"text": "For our DCA models, we initialize the number of agents before training, and partition the document among the agents (i.e., three agent \u2192 three paragraphs).", "labels": [], "entities": []}, {"text": "Additional details can be found in Appendix A.1.", "labels": [], "entities": [{"text": "Appendix A.1", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.7825082242488861}]}, {"text": "Training Details During training and testing we truncate the article to 800 tokens and limit the length of the summary to 100 tokens for training and 110 tokens attest time.", "labels": [], "entities": [{"text": "length", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.948466956615448}]}, {"text": "We distribute the truncated articles among agents for multi-agent models, preserving the paragraph and sentences as possible.", "labels": [], "entities": []}, {"text": "For both datasets, we limit the input and output vocabulary size to the 50,000 most frequent tokens in the training set.", "labels": [], "entities": []}, {"text": "We train with up to two contextual layers in all the DCA models as more layers did not provide additional performance gains.", "labels": [], "entities": []}, {"text": "We fix \u03b3 = 0.97 for the RL term in Equation    The rest of our models use 3 agents and incrementally add one component.", "labels": [], "entities": [{"text": "RL", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9334046244621277}, {"text": "Equation", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.6857803463935852}]}, {"text": "First, we add the semantic cohesion loss (m4).", "labels": [], "entities": []}, {"text": "Then, we add multi-agent pointer networks (mpgen) and agent communication (m5).", "labels": [], "entities": []}, {"text": "Finally, we add contextual agent attention (caa) (m6), and train with the mixed MLE+RL+SEM loss (m7).", "labels": [], "entities": [{"text": "MLE+RL+SEM loss", "start_pos": 80, "end_pos": 95, "type": "METRIC", "confidence": 0.7884952127933502}]}, {"text": "All DCA models use pointer networks.", "labels": [], "entities": []}, {"text": "We perform human evaluations to establish that our model's ROUGE improvements are correlated with human judgments.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9395265579223633}]}, {"text": "We measure the communicative multi-agent network with contextual agent attention in comparison to a single-agent network with no communication.", "labels": [], "entities": []}, {"text": "We use the following as evaluation criteria for generated summaries: (1) non-redundancy, fewer of the same ideas are repeated, (2) coherence, ideas are expressed clearly; (3) focus, the main ideas of the document are shared while avoiding superfluous details, and (4) overall, the summary effectively communicates the article's content.", "labels": [], "entities": []}, {"text": "The focus and non-redundancy dimensions help quantify the impact of multi-agent communication in our model, while coherence helps to evaluate the impact of the reward based learning and repetition penalty of the proposed models.", "labels": [], "entities": [{"text": "repetition penalty", "start_pos": 186, "end_pos": 204, "type": "METRIC", "confidence": 0.9612013399600983}]}, {"text": "Evaluation Procedure We randomly selected 100 samples from the CNN/DailyMail test set and use workers from Amazon Mechanical Turk as judges to evaluate them on the four criteria defined above.", "labels": [], "entities": [{"text": "CNN/DailyMail test set", "start_pos": 63, "end_pos": 85, "type": "DATASET", "confidence": 0.9545760869979858}, {"text": "Amazon Mechanical Turk", "start_pos": 107, "end_pos": 129, "type": "DATASET", "confidence": 0.921326756477356}]}, {"text": "Judges are shown the original document, the ground truth summary, and two model summaries and are asked to evaluate each summary on the four criteria using a Likert scale from 1 (worst) to 5 (best).", "labels": [], "entities": []}, {"text": "The ground truth and model summaries are presented to the judges in random order.", "labels": [], "entities": []}, {"text": "Each summary is rated by 5 judges and the results are averaged across all examples and judges.", "labels": [], "entities": []}, {"text": "We also performed a head-to-head evaluation (more common in DUC style evaluations) and randomly show two model generated summaries.", "labels": [], "entities": []}, {"text": "We ask the human annotators to rate each summary on the same metrics as before without seeing the source document or ground truth summaries.", "labels": [], "entities": []}, {"text": "Results Human evaluators significantly prefer summaries generated by the communicating encoders.", "labels": [], "entities": [{"text": "summaries generated", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8902764618396759}]}, {"text": "In the rating task, evaluators preferred the multi-agent summaries to the single-agent cases for all metrics.", "labels": [], "entities": []}, {"text": "In the head-to-head evaluation, humans consistently preferred the DCA summaries to those generated by a single agent.", "labels": [], "entities": []}, {"text": "In both the head-to-head and the rating evaluation, the largest improvement for the DCA model was on the focus question, indicating that the model learns to generate summaries with more pertinent details by capturing salient information from later portions of the document.", "labels": [], "entities": []}, {"text": "Human Mr Turnbull was interviewed about his childhood and his political stance.", "labels": [], "entities": [{"text": "Human Mr Turnbull", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.8722610473632812}]}, {"text": "He also admitted he planned to run for prime minister if Tony Abbott had been successfully toppled in February's leadership spill.", "labels": [], "entities": []}, {"text": "The words 'primed minister' were controversially also printed on the cover.", "labels": [], "entities": []}, {"text": "Single Malcolm Turnbull is set to feature on the front cover of the GQ Australia in a bold move that will no doubt set senators' tongues wagging.", "labels": [], "entities": [{"text": "front cover of the GQ Australia", "start_pos": 49, "end_pos": 80, "type": "DATASET", "confidence": 0.7241450697183609}]}, {"text": "Posing in a suave blue suit with a pinstriped shirt and a contrasting red tie , Mr Turnbull's confident demeanour is complimented by the bold, confronting words printed across the page: 'primed minister'.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.", "labels": [], "entities": [{"text": "CNN/Daily Mail test set", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.893330862124761}]}, {"text": " Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.", "labels": [], "entities": [{"text": "New York Times test set", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.7450935244560242}, {"text": "F1", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.8396773934364319}]}, {"text": " Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9755145907402039}, {"text": "CNN/Daily Maily Dataset", "start_pos": 124, "end_pos": 147, "type": "DATASET", "confidence": 0.9370165348052979}]}, {"text": " Table 4: Comparison of a human summary to best single-and multi-agent model summaries, (m3) and (m7)  from CNN/DailyMail dataset. Although single-agent model generates a coherent summary, it is less focused and  contains more unnecessary details ( highlighed red ) and misses keys facts that the multi-agent model successfully  captures (bolded).", "labels": [], "entities": [{"text": "CNN/DailyMail dataset", "start_pos": 108, "end_pos": 129, "type": "DATASET", "confidence": 0.9016764611005783}]}, {"text": " Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.", "labels": [], "entities": [{"text": "CNN/DM  dataset", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.9140548706054688}, {"text": "MA", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.9777808785438538}, {"text": "sta- tistical significance", "start_pos": 150, "end_pos": 176, "type": "METRIC", "confidence": 0.7801369428634644}]}, {"text": " Table 6: Summary statistics of CNN/DailyMail (DM)  and New York Times (NYT) Datasets.", "labels": [], "entities": [{"text": "CNN/DailyMail (DM)  and New York Times (NYT) Datasets", "start_pos": 32, "end_pos": 85, "type": "DATASET", "confidence": 0.9153603826250348}]}]}