{"title": [{"text": "Large-scale spectral clustering using diffusion coordinates on landmark-based bipartite graphs", "labels": [], "entities": []}], "abstractContent": [{"text": "Spectral clustering has received a lot of attention due to its ability to separate non-convex, non-intersecting manifolds, but its high computational complexity has significantly limited its applicability.", "labels": [], "entities": [{"text": "Spectral clustering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9537692666053772}]}, {"text": "Motivated by the document-term co-clustering framework by Dhillon (2001), we propose a landmark-based scalable spectral clustering approach in which we first use the selected landmark set and the given data to form a bipartite graph and then run a diffusion process on it to obtain a family of diffusion coordinates for clustering.", "labels": [], "entities": []}, {"text": "We show that our proposed algorithm can be implemented based on very efficient operations on the affinity matrix between the given data and selected landmarks, thus capable of handling large data.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate the excellent performance of our method by comparing with the state-of-the-art scalable algorithms on several benchmark data sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given a data set X = {x 1 , . .", "labels": [], "entities": []}, {"text": ", x n } \u2282 Rd and a similarity function \u03b4(\u00b7, \u00b7) such as the Gaussian radial basis function (RBF), spectral clustering first constructs a pairwise similarity matrix W = (w ij ) \u2208 R n\u00d7n , w ij = \u03b4(x i , x j ) and then uses the top eigenvectors of W (after certain kind of normalization) to embed X into a low-dimensional space where k-means is employed to group the data into k clusters.", "labels": [], "entities": [{"text": "spectral clustering", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7832076549530029}]}, {"text": "Though mathematically quite simple, spectral clustering can easily adapt to nonconvex geometries and accurately separate various non-intersecting shapes.", "labels": [], "entities": [{"text": "spectral clustering", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7237876355648041}]}, {"text": "As a result, it has been successfully applied to many practical tasks, e.g., image segmentation) and document clustering, often significantly outperforming traditional methods (such as k-means).", "labels": [], "entities": [{"text": "image segmentation)", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8525912960370382}, {"text": "document clustering", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.8014552593231201}]}, {"text": "Furthermore, spectral clustering has a very rich theory, with interesting connections to kernel k-means (), random walk (), graph cut) (and the underlying spectral graph theory), and matrix perturbation analysis ().", "labels": [], "entities": [{"text": "spectral clustering", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.8118840456008911}, {"text": "matrix perturbation analysis", "start_pos": 183, "end_pos": 211, "type": "TASK", "confidence": 0.6865097979704539}]}, {"text": "However, spectral clustering is known to suffer from a high computational cost associated with then \u00d7 n matrix W , especially when n is large.", "labels": [], "entities": [{"text": "spectral clustering", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.8393678665161133}]}, {"text": "Consequently, there has been considerable effort to develop fast, approximate algorithms that can handle large data sets.", "labels": [], "entities": []}, {"text": "Interestingly, a considerable fraction of them use a landmark set to help reduce the computational complexity of spectral clustering.", "labels": [], "entities": [{"text": "spectral clustering", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7582612931728363}]}, {"text": "Specifically, they first find a small set of data representatives (called landmarks), Y = {y 1 , . .", "labels": [], "entities": []}, {"text": ", y m } \u2282 Rd (with m n), from the given data in X and then form an affinity matrix between X and Y (see): A = (a ij ) \u2208 R n\u00d7m , a ij = \u03b4(x i , y j ).", "labels": [], "entities": []}, {"text": "Afterwards, different scalable methods use the matrix A in different ways to cluster the given data.", "labels": [], "entities": []}, {"text": "For example, the column-sampling spectral clustering (cSPEC) algorithm () regards A as a column-reduced version of W and correspondingly use the left singular vectors of A to approximate the eigenvectors of W . However, they seem to consider only unnormalized spectral clustering, and it is unclear how they extend their technique to normalized spectral clustering).", "labels": [], "entities": [{"text": "column-sampling spectral clustering (cSPEC)", "start_pos": 17, "end_pos": 60, "type": "TASK", "confidence": 0.7822827299435934}]}, {"text": "Another example is the landmark-based spectral", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we conduct extensive experiments to evaluate the practical performance of LBDM with \u03b1 = 1, 2.", "labels": [], "entities": []}, {"text": "For the odd value \u03b1 = 1, for which the co-clustering method has to be used, we denote the corresponding implementation by LBDM . For the even value \u03b1 = 2, we use both the direct clustering and landmark clustering methods and denote them as LBDM and LBDM , respectively.", "labels": [], "entities": [{"text": "LBDM", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.7938686013221741}, {"text": "LBDM", "start_pos": 249, "end_pos": 253, "type": "METRIC", "confidence": 0.5563524961471558}]}, {"text": "We compare the different LBDM versions with the following algorithms: KASP (), LSC), cSPEC (, and the co-clustering algorithm by (used similarly as LBDM ), in the setting of Gaussian similarity.", "labels": [], "entities": []}, {"text": "We also include the plain Ncut algorithm) in our study (as a baseline).", "labels": [], "entities": []}, {"text": "We implement all the methods in MATLAB 2016b (except LSC 2 ) and conduct all the experiments on a compute server with 48GB of RAM and 2 CPUs with 12 total cores.", "labels": [], "entities": [{"text": "MATLAB 2016b", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.7892486751079559}]}, {"text": "We choose six benchmark data sets -usps, pendigits, letter, protein, shuttle, mnist -from the LIBSVM website 3 (see for their summary statistics).", "labels": [], "entities": [{"text": "LIBSVM website 3", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.9690348903338114}]}, {"text": "They are originally partitioned into training and test parts for classification purposes, but for each data set we have merged the two parts together for our unsupervised setting.", "labels": [], "entities": []}, {"text": "Also, we provide the true number of clusters k to all algorithms to focus on the clustering task.", "labels": [], "entities": []}, {"text": "4 In order to have a fair comparison between the different algorithms, we use the same values for the shared parameters.", "labels": [], "entities": []}, {"text": "In particular, we fix m = 500 (for all methods) and s = 5 (for LSC, Dhillon and LBDM with \u03b1 = 1, 2).", "labels": [], "entities": [{"text": "LBDM", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.7748652100563049}]}, {"text": "Also, we feed all the algorithms with the same landmark set found by k-means (with only 10 iterations), which is initialized with the centroids obtained by preliminary k-means clustering on 10% of the data (with 100 iterations, 10 restarts).", "labels": [], "entities": []}, {"text": "In the last step of each algorithm (where k-means is applied to cluster data in the respective embedding space), we use 100 iterations and 10 restarts.", "labels": [], "entities": []}, {"text": "We evaluate the different methods in terms of clustering accuracy and CPU time (averaged over 50 replications), with the former being calculated by first finding the best match between the output cluster labels and the ground truth and then computing the fraction of correctly assigned labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9636707901954651}]}], "tableCaptions": [{"text": " Table 1: Data sets used in our experiments.", "labels": [], "entities": []}, {"text": " Table 2: Average clustering accuracy (%) of the various methods obtained on the data sets in Table 1. (Due to  memory issue, we could not run the plain Ncut algorithm on the last two data sets.)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.906049370765686}]}, {"text": " Table 3: Average CPU time (in seconds) used by the various methods on the data sets in Table 1. The CPU time  needed by the initial k-means to sample landmark points from each data set has been separately reported in the  third column of the table (as it is common to all the methods).", "labels": [], "entities": [{"text": "Average CPU time", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8564000527064005}]}]}