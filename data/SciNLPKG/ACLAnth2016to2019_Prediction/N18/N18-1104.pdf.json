{"title": [], "abstractContent": [{"text": "Meaning Representation (AMR) research has mostly focused on English.", "labels": [], "entities": [{"text": "Meaning Representation (AMR)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9142809510231018}]}, {"text": "We show that it is possible to use AMR annotations for English as a semantic representation for sentences written in other languages.", "labels": [], "entities": []}, {"text": "We exploit an AMR parser for English and parallel corpora to learn AMR parsers for Italian, Spanish, German and Chinese.", "labels": [], "entities": []}, {"text": "Qualitative analysis show that the new parsers overcome structural differences between the languages.", "labels": [], "entities": []}, {"text": "We further propose a method to evaluate the parsers that does not require gold standard data in the target languages.", "labels": [], "entities": []}, {"text": "This method highly correlates with the gold standard evaluation, obtaining a (Pearson) correlation of 0.95.", "labels": [], "entities": [{"text": "Pearson) correlation", "start_pos": 78, "end_pos": 98, "type": "METRIC", "confidence": 0.9378321965535482}]}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) parsing is the process of converting natural language sentences into their corresponding AMR representations ().", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR) parsing", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8542776703834534}]}, {"text": "An AMR is a graph with nodes representing the concepts of the sentence and edges representing the semantic relations between them.", "labels": [], "entities": []}, {"text": "Most available AMR datasets large enough to train statistical models consist of pairs of English sentences and AMR graphs.", "labels": [], "entities": []}, {"text": "The cross-lingual properties of AMR across languages has been the subject of preliminary discussions.", "labels": [], "entities": []}, {"text": "The AMR guidelines state that AMR is not an interlingua ( and categorizes different kinds of divergences in the annotation between English AMRs and Czech AMRs.", "labels": [], "entities": []}, {"text": "show that structurally aligning English AMRs with Czech and Chinese AMRs is not always possible but that refined annotation guidelines suffice to resolve some of these cases.", "labels": [], "entities": []}, {"text": "We extend this line of research by exploring whether divergences among languages can be overcome, i.e., we investigate This is the sovereignty of each country We implement AMR parsers for Italian, Spanish, German and Chinese using annotation projection, where existing annotations are projected from a source language (English) to a target language through a parallel corpus (e.g.,.", "labels": [], "entities": []}, {"text": "By evaluating the parsers and manually analyzing their output, we show that the parsers are able to recover the AMR structures even when there exist structural differences between the languages, i.e., although AMR is not an interlingua it can act as one.", "labels": [], "entities": []}, {"text": "This method also provides a quick way to prototype multilingual AMR parsers, assuming that Part-of-speech (POS) taggers, Named Entity Recognition (NER) taggers and dependency parsers are available for the target languages.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.8970228731632233}, {"text": "Named Entity Recognition (NER) taggers", "start_pos": 121, "end_pos": 159, "type": "TASK", "confidence": 0.71638742515019}]}, {"text": "We also propose an alternative approach, where Machine Translation (MT) is used to translate the input sentences into English so that an available English AMR parser can be employed.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.8287230491638183}]}, {"text": "This method is an even quicker solution which only requires translation models between the target languages and English.", "labels": [], "entities": []}, {"text": "Due to the lack of gold standard in the target languages, we exploit the English data to evaluate the parsers for the target languages.", "labels": [], "entities": []}, {"text": "Henceforth, we will use the term target parser to indicate a parser fora target language.", "labels": [], "entities": []}, {"text": "We achieve this by first learning the target parser from the gold standard English parser, and then inverting this process to learn anew English parser from the target parser.", "labels": [], "entities": []}, {"text": "We then evaluate the resulting English parser against the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.8560708463191986}]}, {"text": "We call this \"fullcycle\" evaluation.", "labels": [], "entities": []}, {"text": "Similarly to, we also directly evaluate the target parser on \"silver\" data, obtained by parsing the English side of a parallel corpus.", "labels": [], "entities": []}, {"text": "In order to assess the reliability of these evaluation methods, we collected gold standard datasets for Italian, Spanish, German and Chinese by acquiring professional translations of the AMR gold standard data to these languages.", "labels": [], "entities": [{"text": "gold standard datasets", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.6176597972710928}, {"text": "AMR gold standard data", "start_pos": 187, "end_pos": 209, "type": "DATASET", "confidence": 0.9276884198188782}]}, {"text": "We hypothesize that the full-cycle score can be used as a more reliable proxy than the silver score for evaluating the target parser.", "labels": [], "entities": []}, {"text": "We provide evidence to this claim by comparing the three evaluation procedures (silver, full-cycle, and gold) across languages and parsers.", "labels": [], "entities": []}, {"text": "Our main contributions are: \u2022 We provide evidence that AMR annotations can be successfully shared across languages.", "labels": [], "entities": [{"text": "AMR annotations", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.9010233581066132}]}, {"text": "\u2022 We propose two ways to rapidly implement non-English AMR parsers.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.8533275723457336}]}, {"text": "\u2022 We propose a novel method to evaluate nonEnglish AMR parsers when gold annotations in the target languages are missing.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.7973141670227051}]}, {"text": "This method highly correlates with gold standard evaluation, obtaining a Pearson correlation coefficient of 0.95.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 73, "end_pos": 104, "type": "METRIC", "confidence": 0.9329331119855245}]}, {"text": "\u2022 We release human translations of an AMR dataset (LDC2015E86) to Italian, Spanish, German and Chinese.", "labels": [], "entities": [{"text": "AMR dataset (LDC2015E86)", "start_pos": 38, "end_pos": 62, "type": "DATASET", "confidence": 0.8479659080505371}]}, {"text": "2 Cross-lingual AMR parsing AMR is a semantic representation heavily biased towards English, where labels for nodes and edges are either English words or Propbank frames).", "labels": [], "entities": [{"text": "AMR parsing AMR", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.8565751512845358}]}, {"text": "The goal of AMR is to abstract away from the syntactic realization of the original sentences while maintaining its underlying meaning.", "labels": [], "entities": [{"text": "AMR", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9602351784706116}]}, {"text": "As a consequence, different phrasings of one sentence are expected to provide identical AMR representations.", "labels": [], "entities": [{"text": "AMR", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9052759408950806}]}, {"text": "This canonicalization does not always hold across languages: two sentences that express the same meaning in two different languages are not guaranteed to produce identical AMR structures).", "labels": [], "entities": []}, {"text": "However, show that in many cases the unlabeled AMRs are in fact shared across languages.", "labels": [], "entities": []}, {"text": "We are encouraged by this finding and argue that it should be possible to develop algorithms that account for some of these differences when they arise.", "labels": [], "entities": []}, {"text": "We therefore introduce anew problem, which we call cross-lingual AMR parsing: given a sentence in any language, the goal is to recover the AMR graph that was originally devised for its English translation.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.8786590397357941}]}, {"text": "This task is harder than traditional AMR parsing as it requires to recover English labels as well as to deal with structural differences between languages, usually referred as translation divergence.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9759175181388855}, {"text": "translation divergence", "start_pos": 176, "end_pos": 198, "type": "TASK", "confidence": 0.8857289254665375}]}, {"text": "We propose two initial solutions to this problem: by annotation projection and by machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7279830276966095}]}], "datasetContent": [{"text": "We now turn to the problem of evaluation.", "labels": [], "entities": []}, {"text": "Let us assume that we trained a parser fora target language, for example using the annotation projection method discussed in Section 2.1.", "labels": [], "entities": []}, {"text": "In line with rapid development of new parsers, we assume that the only gold AMR dataset available is the one released for English.", "labels": [], "entities": [{"text": "AMR dataset", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.7253232151269913}]}, {"text": "SILVER We can generate a silver test set by running an automatic (English) AMR parser on the English side of a parallel corpus and use the output AMRs as references.", "labels": [], "entities": []}, {"text": "However, the silver test set is affected by mistakes made by the English AMR parser, therefore it may not be reliable.", "labels": [], "entities": [{"text": "silver test set", "start_pos": 13, "end_pos": 28, "type": "DATASET", "confidence": 0.8112125595410665}, {"text": "English AMR parser", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.7137481768925985}]}, {"text": "FULL-CYCLE In order to perform the evaluation on a gold test set, we propose full-cycle evaluation: after learning the target parser from the English parser, we invert this process to learn anew English parser from the target parser, in the same way that we learned the target parser from the English parser.", "labels": [], "entities": [{"text": "FULL-CYCLE", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.6883355975151062}]}, {"text": "The resulting English parser is then evaluated against the (English) AMR gold standard.", "labels": [], "entities": [{"text": "AMR gold standard", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.8723172346750895}]}, {"text": "We hypothesize that the score of the new English parser can be used as a proxy to the score of the target parser.", "labels": [], "entities": []}, {"text": "GOLD To show whether the evaluation methods proposed can be used reliably, we also generated gold test AMR datasets for four target languages (Italian, Spanish, German and Chinese).", "labels": [], "entities": [{"text": "GOLD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9207334518432617}, {"text": "AMR datasets", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.7848668098449707}]}, {"text": "In order to do so, we collected professional translations for the English sentences in the AMR test set.", "labels": [], "entities": [{"text": "AMR test set", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.8896456360816956}]}, {"text": "We were then able to create pairs of human-produced sentences with human-produced AMR graphs.", "labels": [], "entities": []}, {"text": "A diagram summarizing the different evaluation stages is shown in.", "labels": [], "entities": []}, {"text": "In the case of MTbased systems, the full-cycle corresponds to first translating from English to the target language and then back to English (back-translation), and only then parsing the sentences with the English AMR parser.", "labels": [], "entities": []}, {"text": "At the end of this process, a noisy version of the original sentence will be returned and its parsed graph will be a noisy version of the graph parsed from the original sentence.", "labels": [], "entities": []}, {"text": "We run experiments on four languages: Italian, Spanish, German and Chinese.", "labels": [], "entities": []}, {"text": "We use Europarl () as the parallel corpus for Italian, Spanish and German, containing around 1.9M sentences for each language pair.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.9400631189346313}]}, {"text": "For Chinese, we use the first 2M sentences from the United Nations Parallel Corpus (.", "labels": [], "entities": [{"text": "United Nations Parallel Corpus", "start_pos": 52, "end_pos": 82, "type": "DATASET", "confidence": 0.9086699336767197}]}, {"text": "For each target language we extract two parallel datasets of 20,000/2,000/2,000 (train/dev/test) sentences for the two step of the annotation projection (English \u2192 target and target \u2192 English).", "labels": [], "entities": []}, {"text": "These are used to train the AMR parsers.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.6111142337322235}]}, {"text": "The projection approach also requires training the word alignments, for which we use all the remaining sentences from the parallel corpora (Europarl for Spanish/German/Italian and UN Parallel Corpus for Chinese).", "labels": [], "entities": [{"text": "word alignments", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7057522237300873}, {"text": "Europarl", "start_pos": 140, "end_pos": 148, "type": "DATASET", "confidence": 0.8672013878822327}, {"text": "UN Parallel Corpus", "start_pos": 180, "end_pos": 198, "type": "DATASET", "confidence": 0.7780043482780457}]}, {"text": "These are also the sentences we use to train the MT models.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9540033936500549}]}, {"text": "The gold AMR dataset is LDC2015E86, containing 16,833 training sentences, 1,368 development sentences, and 1,371 testing sentences.", "labels": [], "entities": [{"text": "AMR dataset", "start_pos": 9, "end_pos": 20, "type": "DATASET", "confidence": 0.7861918807029724}, {"text": "LDC2015E86", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.5873052477836609}]}, {"text": "Word alignments were generated using fast align, while AMR alignments were generated with JAMR ().", "labels": [], "entities": [{"text": "JAMR", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.656780481338501}]}, {"text": "AMREager () was chosen as the pre-existing English AMR parser.", "labels": [], "entities": [{"text": "AMREager", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7844545841217041}]}, {"text": "AMREager is an open-source AMR parser that needs only minor modifications for re-use with other languages.", "labels": [], "entities": [{"text": "AMREager", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9581080675125122}, {"text": "AMR parser", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.760022759437561}]}, {"text": "It requires tokenization, POS tagging, NER tagging and dependency parsing, which for English, German and Chinese are provided by).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.7857162952423096}, {"text": "NER tagging", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.8327880501747131}, {"text": "dependency parsing", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.6916632056236267}]}, {"text": "We use Freeling () for Spanish, as CoreNLP does not provide dependency parsing for this language.", "labels": [], "entities": []}, {"text": "Italian is not supported in CoreNLP: we use Tint (, a CoreNLP-compatible NLP pipeline for Italian.", "labels": [], "entities": [{"text": "CoreNLP-compatible NLP pipeline", "start_pos": 54, "end_pos": 85, "type": "DATASET", "confidence": 0.8554642796516418}]}, {"text": "In order to experiment with the approach of Section 2.2, we experimented with translations from Google Translate.", "labels": [], "entities": []}, {"text": "As Google Translate has access to a much larger training corpus, we also trained baseline MT models using Moses () and Nematus (, with the same training data we use for the projection method and default hyper-parameters. Smatch) is used to evaluate AMR parsers.", "labels": [], "entities": [{"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.936323881149292}, {"text": "AMR parsers", "start_pos": 249, "end_pos": 260, "type": "TASK", "confidence": 0.8889844417572021}]}, {"text": "It looks for the best alignment between the predicted AMR and the reference AMR and it then computes precision, recall and F 1 of their edges.", "labels": [], "entities": [{"text": "AMR", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.81182861328125}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9983939528465271}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9995978474617004}, {"text": "F 1", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9915567934513092}]}, {"text": "The original English parser achieves 65% Smatch score on the test split of LDC2015E86.", "labels": [], "entities": [{"text": "Smatch score", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9635856449604034}, {"text": "LDC2015E86", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.8112937808036804}]}, {"text": "Full-cycle and gold evaluations use the same dataset, while silver evaluation is performed on the split of the parallel corpora we reserved for testing.", "labels": [], "entities": []}, {"text": "Results are shown in Table 1.", "labels": [], "entities": []}, {"text": "The Google Translate system outperforms all other systems, but is not directly comparable to them, as it has the unfair advantage of being The multilingual adaptation of AMREager is avail- trained on a much larger dataset.", "labels": [], "entities": [{"text": "AMREager", "start_pos": 170, "end_pos": 178, "type": "DATASET", "confidence": 0.852769672870636}]}, {"text": "Due to noisy JAMR alignments and silver training data involved in the annotation projection approach, the MTbased systems give in general better parsing results.", "labels": [], "entities": [{"text": "JAMR alignments", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.5704393684864044}]}, {"text": "The BLEU scores of all translation systems are shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9987574815750122}]}, {"text": "There are several sources of noise in the annotation projection method, which affect the parsing results: 1) the parsers are trained on silver data obtained by an automatic parser for English; 2) the projection uses noisy word alignments; 3) the AMR alignments on the source side are also noisy; 4) translation divergences exist between the languages, making it sometimes difficult to project the annotation without loss of information.", "labels": [], "entities": [{"text": "parsing", "start_pos": 89, "end_pos": 96, "type": "TASK", "confidence": 0.9647143483161926}]}, {"text": "shows examples of output parses 4 for all languages, including the AMR alignments byproduct of the parsing process, that we use to discuss the mistakes made by the parsers.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.8835935294628143}]}], "tableCaptions": [{"text": " Table 1: Silver, gold and full-cycle Smatch scores for  projection-based and MT-based systems.", "labels": [], "entities": [{"text": "MT-based", "start_pos": 78, "end_pos": 86, "type": "TASK", "confidence": 0.9504790902137756}]}, {"text": " Table 2: BLEU scores for Moses, Nematus and Google  Translate (GT) on the (out-of-domain) LDC2015E86  test set", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993151426315308}, {"text": "LDC2015E86  test set", "start_pos": 91, "end_pos": 111, "type": "DATASET", "confidence": 0.916419267654419}]}]}