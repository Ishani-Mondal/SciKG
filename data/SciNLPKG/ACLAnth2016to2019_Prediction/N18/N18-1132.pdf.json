{"title": [{"text": "DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference *", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel deep learning architecture to address the natural language inference (NLI) task.", "labels": [], "entities": []}, {"text": "Existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis.", "labels": [], "entities": []}, {"text": "Instead, we propose a novel dependent reading bidirec-tional LSTM network (DR-BiLSTM) to efficiently model the relationship between a premise and a hypothesis during encoding and inference.", "labels": [], "entities": []}, {"text": "We also introduce a sophisticated ensemble strategy to combine our proposed models, which noticeably improves final predictions.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate how the results can be improved further with an additional pre-processing step.", "labels": [], "entities": []}, {"text": "Our evaluation shows that DR-BiLSTM obtains the best single model and ensemble model results achieving the new state-of-the-art scores on the Stanford NLI dataset.", "labels": [], "entities": [{"text": "Stanford NLI dataset", "start_pos": 142, "end_pos": 162, "type": "DATASET", "confidence": 0.9435345331827799}]}], "introductionContent": [{"text": "Natural Language Inference (NLI; a.k.a. Recognizing Textual Entailment, or RTE) is an important and challenging task for natural language understanding.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI; a.k.a. Recognizing Textual Entailment", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.811670970916748}, {"text": "RTE)", "start_pos": 75, "end_pos": 79, "type": "TASK", "confidence": 0.7216050028800964}, {"text": "natural language understanding", "start_pos": 121, "end_pos": 151, "type": "TASK", "confidence": 0.6590606967608134}]}, {"text": "The goal of NLI is to identify the logical relationship (entailment, neutral, or contradiction) between a premise and a corresponding hypothesis.", "labels": [], "entities": []}, {"text": "shows few example relationships from the Stanford Natural Language Inference (SNLI) dataset.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) dataset", "start_pos": 41, "end_pos": 91, "type": "DATASET", "confidence": 0.6116755791008472}]}, {"text": "Recently, NLI has received a lot of attention from the researchers, especially due to the avail- * ArXiv version of this work can be found here (arxiv.org/pdf/1802.05577.pdf).", "labels": [], "entities": [{"text": "ArXiv", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.9951222538948059}]}, {"text": "\u2020 This work was conducted as part of an internship program at Philips Research.", "labels": [], "entities": [{"text": "Philips Research", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.9401526749134064}]}, {"text": "ability of large annotated datasets like SNLI.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.8054065704345703}]}, {"text": "Various deep learning models have been proposed that achieve successful results for this task ().", "labels": [], "entities": []}, {"text": "Most of these existing NLI models use attention mechanism to jointly interpret and align the premise and hypothesis.", "labels": [], "entities": []}, {"text": "Such models use simple reading mechanisms to encode the premise and hypothesis independently.", "labels": [], "entities": []}, {"text": "However, such a complex task require explicit modeling of dependency relationships between the premise and the hypothesis during the encoding and inference processes to prevent the network from the loss of relevant, contextual information.", "labels": [], "entities": []}, {"text": "In this paper, we refer to such strategies as dependent reading.", "labels": [], "entities": []}, {"text": "There are some alternative reading mechanisms available in the literature () that consider dependency aspects of the premise-hypothesis relationships.", "labels": [], "entities": []}, {"text": "However, these mechanisms have two major limitations: \u2022 So far, they have only explored dependency aspects during the encoding stage, while ignoring its benefit during inference.", "labels": [], "entities": []}, {"text": "\u2022 Such models only consider encoding a hy-pothesis depending on the premise, disregarding the dependency aspects in the opposite direction.", "labels": [], "entities": []}, {"text": "We propose a dependent reading bidirectional LSTM (DR-BiLSTM) model to address these limitations.", "labels": [], "entities": []}, {"text": "Given a premise u and a hypothesis v, our model first encodes them considering dependency on each other (u|v and v|u).", "labels": [], "entities": []}, {"text": "Next, the model employs a soft attention mechanism to extract relevant information from these encodings.", "labels": [], "entities": []}, {"text": "The augmented sentence representations are then passed to the inference stage, which uses a similar dependent reading strategy in both directions, i.e. u \u2192 v and v \u2192 u.", "labels": [], "entities": []}, {"text": "Finally, a decision is made through a multi-layer perceptron (MLP) based on the aggregated information.", "labels": [], "entities": []}, {"text": "Our experiments on the SNLI dataset show that DR-BiLSTM achieves the best single model and ensemble model performance obtaining improvements of a considerable margin of 0.4% and 0.3% over the previous state-of-the-art single and ensemble models, respectively.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.9448428153991699}, {"text": "DR-BiLSTM", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.858792245388031}]}, {"text": "Furthermore, we demonstrate the importance of a simple preprocessing step performed on the SNLI dataset.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.8779946565628052}]}, {"text": "Evaluation results show that such preprocessing allows our single model to achieve the same accuracy as the state-of-the-art ensemble model and improves our ensemble model to outperform the state-of-the-art ensemble model by a remarkable margin of 0.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.998714804649353}]}, {"text": "Finally, we perform an extensive analysis to clarify the strengths and weaknesses of our models.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Stanford Natural Language Inference (SNLI) dataset contains 570K human annotated sentence pairs.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) dataset", "start_pos": 4, "end_pos": 54, "type": "DATASET", "confidence": 0.6310454979538918}]}, {"text": "The premises are drawn from the Flickr30k (Plummer et al., 2015) corpus, and then the hypotheses are manually composed for each relationship class (entailment, neutral, contradiction, and -).", "labels": [], "entities": [{"text": "Flickr30k (Plummer et al., 2015) corpus", "start_pos": 32, "end_pos": 71, "type": "DATASET", "confidence": 0.9044380320443047}]}, {"text": "The \"-\" class indicates that there is no consensus decision among the annotators, consequently, we remove them during the training and evaluation following the literature.", "labels": [], "entities": []}, {"text": "We use the same data split as provided in to report comparable results with other models.", "labels": [], "entities": []}, {"text": "We use pre-trained 300-D Glove 840B vectors () to initialize our word embedding vectors.", "labels": [], "entities": []}, {"text": "All hidden states of BiLSTMs during input encoding and inference have 450 dimensions (r = 300 and d = 450).", "labels": [], "entities": []}, {"text": "The weights are learned by minimizing the log-loss on the training data via the Adam optimizer ().", "labels": [], "entities": []}, {"text": "The initial learning rate is 0.0004.", "labels": [], "entities": []}, {"text": "To avoid overfitting, we use dropout () with the rate of 0.4 for regularization, which is applied to all feedforward connections.", "labels": [], "entities": []}, {"text": "During training, the word embeddings are updated to learn effective representations for the NLI task.", "labels": [], "entities": []}, {"text": "We use a fairly small batch size of 32 to provide more exploration power to the model.", "labels": [], "entities": []}, {"text": "Our observation indicates that using larger batch sizes hurts the performance of our model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracies of the models on the training  set and test set of SNLI. DR-BiLSTM (Ensemble)  achieves the accuracy of 89.3%, the best result ob- served on SNLI, while DR-BiLSTM (Single) ob- tains the accuracy of 88.5%, which considerably  outperforms the previous non-ensemble models.  Also, utilizing a trivial preprocessing step yields to  further improvements of 0.4% and 0.3% for single  and ensemble DR-BiLSTM models respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9994470477104187}, {"text": "SNLI", "start_pos": 162, "end_pos": 166, "type": "DATASET", "confidence": 0.9024177193641663}, {"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9987481832504272}]}, {"text": " Table 4: Categorical performance analyses (accu- racy) of ESIM (", "labels": [], "entities": []}]}