{"title": [{"text": "DeepAlignment: Unsupervised Ontology Matching With Refined Word Vectors", "labels": [], "entities": [{"text": "Unsupervised Ontology Matching", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.6148479183514913}]}], "abstractContent": [{"text": "Ontologies compartmentalize types and relations in a target domain and provide the semantic backbone needed fora plethora of practical applications.", "labels": [], "entities": [{"text": "Ontologies compartmentalize types and relations", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8252864241600036}]}, {"text": "Very often different on-tologies are developed independently for the same domain.", "labels": [], "entities": []}, {"text": "Such \"parallel\" ontologies raise the need fora process that will establish alignments between their entities in order to unify and extend the existing knowledge.", "labels": [], "entities": []}, {"text": "In this work, we present a novel entity alignment method which we dub DeepAlignment.", "labels": [], "entities": [{"text": "entity alignment", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.757556289434433}]}, {"text": "DeepAlignment refines pre-trained word vectors aiming at deriving ontological entity descriptions which are tailored to the ontol-ogy matching task.", "labels": [], "entities": []}, {"text": "The absence of explicit information relevant to the ontology matching task during the refinement process makes DeepAlignment completely unsupervised.", "labels": [], "entities": [{"text": "ontology matching task", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.837882936000824}]}, {"text": "We empirically evaluate our method using standard ontology matching benchmarks.", "labels": [], "entities": []}, {"text": "We present significant performance improvements over the current state-of-the-art, demonstrating the advantages that representation learning techniques bring to ontology matching.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.8527875542640686}]}], "introductionContent": [{"text": "Translation across heterogeneous conceptual systems is an important challenge for cognitive science (.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9441125392913818}]}, {"text": "Ontology Matching constitutes the task of establishing correspondences between semantically related entities (i.e. classes and properties) from different ontologies, as illustrated in.", "labels": [], "entities": [{"text": "Ontology Matching", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8453818559646606}]}, {"text": "Similarly, ontology matching is crucial for accomplishing a mutual understanding across heterogeneous artificial cognitive agents.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.889457643032074}]}, {"text": "However, despite the many proposed solutions, it is widely accepted that there is no solution robust enough to deal with the high ontological linguistic variability 2013); hampering, thus, the discovery of shared meanings.", "labels": [], "entities": []}, {"text": "Research in automatic ontology matching has focused on engineering features from terminological, structural, extensional (ontology instances) and semantic model information extracted from the ontological model.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7236958891153336}]}, {"text": "These features are then used to compute ontological entity similarities that will guide the ontology matching.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7123517990112305}]}, {"text": "Deriving such features fora given problem is an extremely time consuming task.", "labels": [], "entities": []}, {"text": "To make matters worse, these features do not transfer in other domains.", "labels": [], "entities": []}, {"text": "As have recently shown, the performance of ontology matching based on different textual features varies greatly with the type of ontologies under consideration.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.9225707650184631}]}, {"text": "At the same time, machine learning research is characterised by a shift from feature engineering based approaches to feature and representation learning as a result of the performance improvements brought by deep learning methods.", "labels": [], "entities": []}, {"text": "A by now classical example is the unsupervised learning of semantic word representations based on the distributional hypothesis, i.e. the assumption that semantically similar or related words appear in similar contexts.", "labels": [], "entities": []}, {"text": "Word vectors have the potential to bring significant value to on-tology matching given the fact that a great deal of ontological information comes in textual form.", "labels": [], "entities": []}, {"text": "One drawback of these semantic word embeddings is that they tend to coalesce the notions of semantic similarity and conceptual association).", "labels": [], "entities": []}, {"text": "For instance, the word \"harness\" is highly related to the word \"horse\", as they share strong associations, i.e. a harness is often used on horses.", "labels": [], "entities": []}, {"text": "From an ontological point of view, however, these types should not be similar.", "labels": [], "entities": []}, {"text": "Moreover, as unsupervised learning requires even larger text corpora, the learned vectors tend to bring closer words with similar frequency instead of similar meaning.", "labels": [], "entities": []}, {"text": "Clearly, word representations that reflect frequency instead of meaning is an undesired feature if we seek to exploit word vectors for ontology matching; alignment based on such representations will reflect similar frequency instead of similar meaning.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.7686712443828583}]}, {"text": "A number of lightweight vector space representation refining techniques were introduced recently in an effort to correct these biases.", "labels": [], "entities": [{"text": "vector space representation refining", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.7006122246384621}]}, {"text": "They use synonymy and antonymy constraints extracted from semantic lexicons to refine the learned word representations and make them better suited for semantic similarity tasks.", "labels": [], "entities": []}, {"text": "Such methods area way to inject domain-specific knowledge to tailor the learned word representations to a given task.", "labels": [], "entities": []}, {"text": "As a result, we can exploit the synonymy/antonymy constraints to learn semantic word representations that are better candidates for ontology matching.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.788556694984436}]}, {"text": "In this paper we learn representations of ontological entities instead of feature engineering them.", "labels": [], "entities": []}, {"text": "We use the learned representations to compute the entities' semantic distances and to subsequently perform the ontology matching task.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.7835740745067596}]}, {"text": "In order to represent the ontological entities, we exploit the textual information that accompanies them.", "labels": [], "entities": []}, {"text": "We represent words by learning their representations using synonymy and antonymy constraints extracted from general lexical resources and information captured implicitly in ontologies.", "labels": [], "entities": []}, {"text": "We cast the problem of ontology matching as an instance of the Stable Marriage problem () using the entities semantic distances.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8448826372623444}, {"text": "Stable Marriage problem", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.8009104530016581}]}, {"text": "Our approach has a number of advantages.", "labels": [], "entities": []}, {"text": "The word embeddings we establish are tailored to the domains and ontologies we want to match.", "labels": [], "entities": []}, {"text": "The method relies on a generic unsupervised representation learning solution which is important given the small size of training sets in ontology matching problems.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the Conference dataset provided by the Ontology Alignment Evaluation Initiative (OAEI) campaign and on areal world alignment scenario between the Schema.org and the DBpedia Ontologies.", "labels": [], "entities": [{"text": "Conference dataset", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.895479679107666}]}, {"text": "We compare our method to state-of-the-art ontology matching systems and show significant performance gains on both benchmarks.", "labels": [], "entities": []}, {"text": "Our approach demonstrates the advantages that representation learning can bring to the task of ontology matching and shows a novel way to study the problem in the setting of recent advances in NLP.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.8900417387485504}, {"text": "ontology matching", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.818929523229599}]}], "datasetContent": [{"text": "In this section, we present the experiments we performed on the OAEI conference dataset and in one real word alignment scenario between the Schema.org and DBpedia ontologies.", "labels": [], "entities": [{"text": "OAEI conference dataset", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.9544754227002462}]}, {"text": "One of the main problems that we have encountered with the comparative evaluation of our algorithm is that even though numerous ontology matching algorithms exist, for only a very small portion of them either the respective software or the system's out-put is publicly available.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.713941752910614}]}, {"text": "To the best of our knowledge, among all the systems tested in the conference dataset only AML () and LogMap (Jim\u00e9nez-Ruiz and Grau, 2011) are publicly available.", "labels": [], "entities": [{"text": "conference dataset", "start_pos": 66, "end_pos": 84, "type": "DATASET", "confidence": 0.7508361339569092}, {"text": "AML", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.6030807495117188}]}, {"text": "As it happens these are two of the state-of-the-art systems.", "labels": [], "entities": []}, {"text": "Moreover, AML offers solutions to detect many-to-many alignments) and, thus, constitutes a competitive baseline against which we will compare the performance of extendMap which also provides many-to-many alignments.", "labels": [], "entities": [{"text": "AML", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6618555188179016}, {"text": "extendMap", "start_pos": 161, "end_pos": 170, "type": "DATASET", "confidence": 0.9082015156745911}]}, {"text": "When training to refine the vector representations an unbalanced proportion of synonymy and antonymy constraints sets can cause problems; the set with the lower cardinality will have limited impact on the final word representations.", "labels": [], "entities": []}, {"text": "To overcome this problem, we run an additional step of the counter-fitting procedure, using only a small random subset of the supernumerary constraints and all constraints of the minority set.", "labels": [], "entities": []}, {"text": "We randomly undersample the larger set and reduce its cardinality to that of the smaller set.", "labels": [], "entities": []}, {"text": "We call this additional step the recounter-fitting process.", "labels": [], "entities": []}, {"text": "To demonstrate the importance of the recounterfitting process and test the behavior of the pretrained word vectors in the absence of synonymy and/or antonymy relations, we have conducted additional experiments which we also present.", "labels": [], "entities": []}, {"text": "In all of our experiments we have applied the counter-fitting process upon the Paragram-SL999 word vectors provided by.", "labels": [], "entities": []}, {"text": "With respect to the textual information extracted for each entity, we have only used the entity's ID (rdf:ID).", "labels": [], "entities": []}, {"text": "To estimate the precision, recall and F 1 measure of all the systems, that we consider for testing, and check for the statistical significance of the results we use an approximate randomization test with 1048576 shuffles, as described in Yeh (2000).", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9996271133422852}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9995831847190857}, {"text": "F 1 measure", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9914782643318176}]}, {"text": "One of our evaluation benchmarks comes from the Ontology Alignment Evaluation Initiative (OAEI), which organizes annual campaigns for evaluating ontology matching systems.", "labels": [], "entities": [{"text": "Ontology Alignment Evaluation Initiative (OAEI)", "start_pos": 48, "end_pos": 95, "type": "TASK", "confidence": 0.7972010714667184}]}, {"text": "The external to OAEI evaluation benchmark comes from the provided alignments between the Schema.org and the DBpedia ontologies.", "labels": [], "entities": []}, {"text": "We provide some further details for each dataset below: OAEI Conference Dataset: It contains 7 ontologies addressing the same domain, namely the conference organization.", "labels": [], "entities": [{"text": "OAEI Conference Dataset", "start_pos": 56, "end_pos": 79, "type": "DATASET", "confidence": 0.9307358066240946}]}, {"text": "These ontologies are suitable for ontology matching task because of their heterogeneous character of origin.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.8684791922569275}]}, {"text": "The overall performance (micro-precision, micro-recall, micro-F1) of the systems is tested upon 21 different test cases.", "labels": [], "entities": []}, {"text": "Specifically, we summed up the individual true positives, false positives and false negatives based on the system results for the different ontology matching tasks and, in the next step, we computed the performance metrics.", "labels": [], "entities": []}, {"text": "The original reference alignment is not closed under the alignment relation, so the transitive closure should be computed before proceeding on the evaluation of the systems.", "labels": [], "entities": []}, {"text": "Schema.org -DBpedia Alignment: It corresponds to the incomplete mapping of the Schema.org and DBpedia ontologies.", "labels": [], "entities": []}, {"text": "Schema.org is a collaborative, community activity with a mission to create, maintain, and promote schemas for structured data on the Internet, on web pages, in email messages, and beyond.", "labels": [], "entities": []}, {"text": "On the other hand, DBpedia is a crowd-sourced community effort to extract structured information from Wikipedia and make this information available on the Web.", "labels": [], "entities": []}, {"text": "This alignment corresponds to areal case scenario between two of the most widely used ontologies in the web today.", "labels": [], "entities": []}, {"text": "All the systems presented in the Conference dataset experiments fall into the category of feature engineering.", "labels": [], "entities": [{"text": "Conference dataset experiments", "start_pos": 33, "end_pos": 63, "type": "DATASET", "confidence": 0.8243133227030436}, {"text": "feature engineering", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7643685936927795}]}, {"text": "CroMatch), AML (,) perform ontology matching based on heuristic methods that rely on aggregation functions.) use logic-based reasoning over the extracted features and cast the ontology matching to a satisfiability problem.", "labels": [], "entities": [{"text": "AML", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.8410160541534424}, {"text": "ontology matching", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.8586454093456268}]}, {"text": "shows the performance of our algorithm compared to the five top performing systems on the Conference 2016 benchmark, according to the results published in OAEI 3 . DeepAligment achieves the highest micro-F 1 measure and the highest recall.", "labels": [], "entities": [{"text": "Conference 2016 benchmark", "start_pos": 90, "end_pos": 115, "type": "DATASET", "confidence": 0.8188713391621908}, {"text": "OAEI 3", "start_pos": 155, "end_pos": 161, "type": "DATASET", "confidence": 0.865717351436615}, {"text": "micro-F 1 measure", "start_pos": 198, "end_pos": 215, "type": "METRIC", "confidence": 0.8533040881156921}, {"text": "recall", "start_pos": 232, "end_pos": 238, "type": "METRIC", "confidence": 0.9997432827949524}]}, {"text": "We were able to perform statistical significance test only for the two systems that were publicly available.", "labels": [], "entities": [{"text": "significance", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.6223399043083191}]}, {"text": "DeepAlignment is significantly better than both of them with a p-value \u2264 0.05.", "labels": [], "entities": [{"text": "DeepAlignment", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9385565519332886}]}, {"text": "In order to explore the performance effect of the many-to-many mappings that DeepAlignment produces we also did experiments where our extendMap algorithm was not used, thus generating only one-to-one alignments.", "labels": [], "entities": []}, {"text": "We give these results under the DeepAlignment * listing.", "labels": [], "entities": [{"text": "DeepAlignment * listing", "start_pos": 32, "end_pos": 55, "type": "DATASET", "confidence": 0.9461383620897929}]}, {"text": "It can be seen that DeepAlignment * achieves the same level of recall as the state-of-the-art systems and this with no feature engineering.", "labels": [], "entities": [{"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9985027313232422}]}, {"text": "When we compare the performance of DeepAlignment * and DeepAlignment we see that the use of extendMap generates correct many-to-many alignments and thus it does not produce large numbers of false positives.", "labels": [], "entities": [{"text": "DeepAlignment", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9450158476829529}]}, {"text": "In any case, however, we retain a small precision which indicates a semantic similarity and conceptual association coalescence.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9955295920372009}]}], "tableCaptions": [{"text": " Table 1: Results on Conference OAEI dataset. StringE- quiv corresponds to ontology matching by simple  string equivalence check.", "labels": [], "entities": [{"text": "OAEI dataset", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.849166989326477}, {"text": "ontology matching", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7832913100719452}]}, {"text": " Table 2. In all of these  experiments, we have applied the extendMap al- gorithm. The last row of", "labels": [], "entities": [{"text": "extendMap al- gorithm", "start_pos": 60, "end_pos": 81, "type": "DATASET", "confidence": 0.8589075356721878}]}, {"text": " Table 2: Experiments on Conference OAEI dataset.", "labels": [], "entities": [{"text": "Conference OAEI dataset", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.8094695210456848}]}, {"text": " Table 3: Results on aligning Schema.org and DBpedia  ontologies.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.906468391418457}]}, {"text": " Table 5. In the absence of counter-fitting,", "labels": [], "entities": []}, {"text": " Table 5: Dependency of DeepAlignment's perfor- mance on the choice of the initial word vectors 6 .", "labels": [], "entities": []}, {"text": " Table 6: Dependency of DeepAlignment's perfor- mance on the external resources' coverage 6 .", "labels": [], "entities": []}]}