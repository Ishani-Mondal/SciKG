{"title": [{"text": "Neural Syntactic Generative Models with Exact Marginalization", "labels": [], "entities": []}], "abstractContent": [{"text": "We present neural syntactic generative models with exact marginalization that support both dependency parsing and language mod-eling.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.8076793551445007}]}, {"text": "Exact marginalization is made tractable through dynamic programming over shift-reduce parsing and minimal RNN-based feature sets.", "labels": [], "entities": []}, {"text": "Our algorithms complement previous approaches by supporting batched training and enabling online computation of next word probabilities.", "labels": [], "entities": []}, {"text": "For supervised dependency parsing, our model achieves a state-of-the-art result among generative approaches.", "labels": [], "entities": [{"text": "supervised dependency parsing", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.5824932754039764}]}, {"text": "We also report empirical results on unsuper-vised syntactic models and their role in language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7268456965684891}]}, {"text": "We find that our model formulation of latent dependencies with exact marginalization do not lead to better intrinsic language modeling performance than vanilla RNNs, and that parsing accuracy is not correlated with language modeling perplexity in stack-based models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.7401047348976135}]}], "introductionContent": [{"text": "We investigate the feasibility of neural syntactic generative models with structured latent variables in which exact inference is tractable.", "labels": [], "entities": [{"text": "neural syntactic generative", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6805155078570048}]}, {"text": "Recent models have added structure to recurrent neural networks at the cost of giving up exact inference, or through using soft structure instead of latent variables (.", "labels": [], "entities": []}, {"text": "We propose generative models in which syntactic structure is modelled with a discrete stack which can be marginalized as a latent variable through dynamic programming.", "labels": [], "entities": []}, {"text": "This enables us to investigate the trade-off between model expressivity and exact marginalization in probabilistic models based on recurrent neural networks (RNNs).", "labels": [], "entities": []}, {"text": "While Long Short-term Memory) (LSTM) RNNs have driven strong improvements in intrinsic language modelling performance, they fail at capturing certain long-distance dependencies, such as those required for modelling subject-verb agreement ( or performing synthetic transduction tasks based on context-free grammars (.", "labels": [], "entities": []}, {"text": "We propose generative models, based on transition-based dependency parsing, a widely used framework for incremental syntactic parsing, that are able to capture desirable dependencies.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.6542452772458395}]}, {"text": "Our generative approach to dependency parsing encodes sentences with an RNN and estimate transition and next word probability distributions by conditioning on a small number of features represented by RNN encoder vectors.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7886308431625366}]}, {"text": "In contrast to previous syntactic language models such as RNNG ( , marginal word probabilities can be computed both online and exactly.", "labels": [], "entities": []}, {"text": "A GPU implementation which exploits parallelization enables unsupervised learning and fast training and decoding.", "labels": [], "entities": []}, {"text": "The price of exact inference is that our models are less expressive than RNNG, as the recurrence is not syntax-dependent.", "labels": [], "entities": []}, {"text": "Our generative models are based on the arceager and arc-hybrid transition systems, with O(n 3 ) dynamic programs based on.", "labels": [], "entities": []}, {"text": "Previous work on dynamic programming for transition-based parsing either required approximate inference due to a too high polynomial order run-time complexity, or had too restrictive feature spaces to be used as accurate models ().", "labels": [], "entities": [{"text": "transition-based parsing", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.5392112731933594}]}, {"text": "Recent work showed that bidirectional RNNs enable accurate graphbased and transition-based dependency parsing using minimal feature spaces.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 74, "end_pos": 109, "type": "TASK", "confidence": 0.6847104430198669}]}, {"text": "further showed that under this approach exact decoding and globally-normalized discriminative training is tractable with dynamic programming.", "labels": [], "entities": []}, {"text": "While discriminative neural network-based models obtain state-of-the-art parsing accuracies, generative models for structured prediction have a number of advantages: They do not suffer from label bias or explaining away effects (), have lower sample complexity (, are amenable to unsupervised learning and can model uncertainty and incorporate prior knowledge through latent variables.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.6819397062063217}]}, {"text": "As a supervised parser our model obtains state of the art performance in transition-based generative dependency parsing.", "labels": [], "entities": [{"text": "generative dependency parsing", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.7892287572224935}]}, {"text": "While its intrinsic language modelling performance is worse than that of a well-tuned vanilla RNN, we see that the formulation of the generative model has a large impact on both the informedness of the syntactic structure and the parsing accuracy of the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.9197514653205872}]}, {"text": "Furthermore, there is a discrepancy between the model structure most suitable for parsing and for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7536891996860504}]}, {"text": "Our analysis shows that there exist informative syntactically-motivated dependencies which LSTMs are not capturing, even though our syntactic models are notable to predict them accurately enough during online processing to improve language modelling performance.", "labels": [], "entities": []}, {"text": "Our implementation is available at https://github.", "labels": [], "entities": []}, {"text": "com/janmbuys/ndp-parser.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: PTB development set parsing results (SD  dependencies), reporting unlabelled and labelled at- tachment scores (UAS/LAS). Discriminative models  (above the line) use either unidirectional or bidirec- tional RNNs.", "labels": [], "entities": [{"text": "PTB development set parsing", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.7124396562576294}, {"text": "at- tachment scores (UAS/LAS)", "start_pos": 100, "end_pos": 129, "type": "METRIC", "confidence": 0.7352702518304189}]}, {"text": " Table 4: PTB test set parsing results with supervised  generative models, on the Stanford (SD) and Yamada  and Matsumoto (2003) (YM) dependencies. The mod- els from", "labels": [], "entities": [{"text": "PTB test set", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.8772635459899902}, {"text": "Stanford (SD) and Yamada  and Matsumoto (2003) (YM) dependencies", "start_pos": 82, "end_pos": 146, "type": "DATASET", "confidence": 0.8755279819170634}]}, {"text": " Table 5: Language modelling perplexity results on the  PTB parsing test set.", "labels": [], "entities": [{"text": "Language modelling perplexity", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.8000163833300272}, {"text": "PTB parsing test set", "start_pos": 56, "end_pos": 76, "type": "DATASET", "confidence": 0.8988145440816879}]}, {"text": " Table 6: Language modelling perplexity analysis on the PTB test set.", "labels": [], "entities": [{"text": "Language modelling perplexity analysis", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.7922748178243637}, {"text": "PTB test set", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9798492987950643}]}]}