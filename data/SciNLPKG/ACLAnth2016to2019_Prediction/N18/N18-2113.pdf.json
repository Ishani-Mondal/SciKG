{"title": [{"text": "Evaluating historical text normalization systems: How well do they generalize?", "labels": [], "entities": [{"text": "Evaluating historical text normalization", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7743807137012482}]}], "abstractContent": [{"text": "We highlight several issues in the evaluation of historical text normalization systems that make it hard to tell how well these systems would actually work in practice-i.e., for new datasets or languages; in comparison to more na\u00efve systems; or as a preprocessing step for downstream NLP tools.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7006803900003433}]}, {"text": "We illustrate these issues and exemplify our proposed evaluation practices by comparing two neural models against a na\u00efve baseline system.", "labels": [], "entities": []}, {"text": "We show that the neural models generalize well to unseen words in tests on five languages; nevertheless, they provide no clear benefit over the na\u00efve baseline for downstream POS tagging of an English historical collection.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 174, "end_pos": 185, "type": "TASK", "confidence": 0.8244228661060333}]}, {"text": "We conclude that future work should include more rigorous evaluation , including both intrinsic and extrinsic measures where possible.", "labels": [], "entities": []}], "introductionContent": [{"text": "Historical text normalization systems aim to convert historical wordforms to their modern equivalents, in order to make historical documents more searchable or to improve the performance of downstream NLP tools.", "labels": [], "entities": [{"text": "Historical text normalization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6996892690658569}]}, {"text": "In historical texts, a single word type maybe realized with several different orthographic forms, which may not correspond to the modern form.", "labels": [], "entities": []}, {"text": "For example, the modern English word said might be realized as sayed, seyd, said, sayd, etc.", "labels": [], "entities": []}, {"text": "Spellings changeover time, but also vary within a single time period and even within a single author, since orthography only became standardized in many languages fairly recently.", "labels": [], "entities": []}, {"text": "Over the years, researchers have proposed normalization methods based on rules and/or edit distances (), statistical machine translation (, and most recently neural network models (.", "labels": [], "entities": [{"text": "normalization", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9660345911979675}, {"text": "statistical machine translation", "start_pos": 105, "end_pos": 136, "type": "TASK", "confidence": 0.6865718166033427}]}, {"text": "However, most of these systems have been developed and tested on a single language (or even a single corpus), and many have not been compared to the na\u00efve but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.", "labels": [], "entities": []}, {"text": "1 These issues make it hard to tell which methods generalize across languages and corpora, and how they compare to each other.", "labels": [], "entities": []}, {"text": "Moreover, researchers have rarely examined whether their systems actually improve performance on downstream tasks.", "labels": [], "entities": []}, {"text": "This paper brings together best practices for evaluating historical text normalization systems, highlighting in particular the need to report results on unseen tokens and to consider the na\u00efve baseline.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.603574792544047}]}, {"text": "We focus our evaluation on two recent neural models: one that has been previously tested only on a German collection that is not widely available (, and one that is adapted from work on morphological re-inflection, but has not been used for historical text normalization (.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 241, "end_pos": 270, "type": "TASK", "confidence": 0.6511835952599844}]}, {"text": "Both are encoderdecoder models; the former with soft attention, and the latter with hard monotonic attention.", "labels": [], "entities": []}, {"text": "We present results on five languages, for both seen and unseen words and for various amounts of training data.", "labels": [], "entities": []}, {"text": "The soft attention model performs surprisingly poorly on seen words, so that its overall performance is worse than the na\u00efve baseline and several earlier models).", "labels": [], "entities": []}, {"text": "However, on unseen words (which we argue are what matters), both neural models do well.", "labels": [], "entities": []}, {"text": "Unfortunately, these positive results did not translate into improvements when we tested the English-trained models on a downstream POS tagging task using a different historical collection spanning a similar time range.", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 132, "end_pos": 148, "type": "TASK", "confidence": 0.8749843041102091}]}, {"text": "Normalizing the text gave better tag accuracy than not normalizing, but neither neural model convincingly outperformed the na\u00efve normalizer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9484342932701111}]}, {"text": "Although these results are disappointing, the clear evaluation standards laid out here should benefit future work in this area.", "labels": [], "entities": []}], "datasetContent": [{"text": "We follow previous work in training our systems on pairs (h, m) of historical tokens and their gold standard modern forms.", "labels": [], "entities": []}, {"text": "Note that attest time, most of the h tokens will have been seen before in the training data (due to Zipf's law), and for these tokens it is very difficult to beat a baseline that normalizes each h to the most common m seen for it in training.", "labels": [], "entities": []}, {"text": "Thus, in practice, normalization systems should typically only be applied to unseen tokens.", "labels": [], "entities": []}, {"text": "It is therefore critical to report both dataset statistics and experimental results for unseen tokens.", "labels": [], "entities": []}, {"text": "Unfortunately, some recent papers have only reported accuracy on all tokens, and only in comparison to other (non-baseline) systems (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9994602799415588}]}, {"text": "These figures can be misleading if systems underperform the na\u00efve baseline on seen tokens (which we show does happen in practice).", "labels": [], "entities": []}, {"text": "To see why, suppose 80% of test tokens were seen in training, and the baseline gets 90% of them right, while system A gets 80% and system B gets only 70%.", "labels": [], "entities": []}, {"text": "Meanwhile the baseline gets only 50% of unseen tokens right, whereas systems A and B get 70% and 90%, respectively.", "labels": [], "entities": []}, {"text": "A's accuracy is higher overall than B's (78% vs 74%), but both systems underperform the baseline (82%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.8839734196662903}]}, {"text": "More importantly, the best system (90% accuracy overall) is achieved by applying the baseline to seen tokens, and the system that generalizes best (B) to unseen tokens; it is irrelevant that A scores higher overall than B.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9942588806152344}]}, {"text": "Stemming from the reasoning above, we argue that a full evaluation of any spelling normalization system requires more complete dataset statistics and experimental results.", "labels": [], "entities": [{"text": "spelling normalization", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.8742703795433044}]}, {"text": "In describing the training and test sets, researchers should not only report the number of types and tokens, but also the per-centage of unseen tokens in the test (or dev) set and the percentage of training items (h, m) where h = m.", "labels": [], "entities": []}, {"text": "This last statistic measures the degree of spelling variation, which varies considerably between corpora.", "labels": [], "entities": []}, {"text": "As for reporting results, we have argued that accuracy should be reported separately for seen vs unseen tokens, and overall results compared to the na\u00efve memorization baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9993283748626709}]}, {"text": "Since historical spelling normalization is typically a low-resource task, systems should also ideally be tested with varying amounts of training data to assess how much annotation might be required fora new corpus (.", "labels": [], "entities": [{"text": "historical spelling normalization", "start_pos": 6, "end_pos": 39, "type": "TASK", "confidence": 0.7270181775093079}]}, {"text": "Finally, since these systems maybe deployed on corpora other than those they were trained on, and used as preprocessing for other tasks, we advocate reporting performance on a downstream task and/or different corpus.", "labels": [], "entities": []}, {"text": "To our knowledge the only previous supervised learning system to do so is.", "labels": [], "entities": []}, {"text": "We use the same datasets as, with data from five languages over a range of historical periods.", "labels": [], "entities": []}, {"text": "We use the same train/dev/test splits as Pettersson; dataset statistics are shown in.", "labels": [], "entities": []}, {"text": "Because we do no hyperparameter tuning, we do not use the development sets, and all results are reported on the test sets.", "labels": [], "entities": []}, {"text": "Each system was tested as recommended above, with accuracy reported separately on seen and unseen items, and for different training data sizes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9995629191398621}]}, {"text": "To evaluate the downstream effects of normalization, we applied the models to a collection of unseen documents and then tagged them with the Stan-ford POS tagger, which comes pre-trained on modern English.", "labels": [], "entities": [{"text": "normalization", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.9609084725379944}]}, {"text": "The documents are from the Parsed Corpus of Early English Correspondence (PCEEC) (), comprised of 84 letter collections from the 15th-17th centuries.", "labels": [], "entities": [{"text": "Parsed Corpus of Early English Correspondence (PCEEC)", "start_pos": 27, "end_pos": 80, "type": "DATASET", "confidence": 0.8838531672954559}]}, {"text": "(Our English normalization training data is from the 14th-17th centuries.)", "labels": [], "entities": [{"text": "English normalization training data", "start_pos": 5, "end_pos": 40, "type": "DATASET", "confidence": 0.7087549343705177}]}, {"text": "PCEEC contains roughly 2.2m manually POS-tagged tokens but no spelling annotation.", "labels": [], "entities": [{"text": "PCEEC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9228664636611938}]}, {"text": "Because it uses a large and somewhat idiosyncratic set of POS tags, we converted these to better match the Stanford tags before evaluating (though the match still isn't perfect; accuracy would be higher in all cases if the tag sets were identical).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9992427825927734}]}, {"text": "Baselines are provided by tagging the unnormalized text and the output of the na\u00efve normalization baseline.", "labels": [], "entities": []}, {"text": "gives test set results for all models, broken down into seen and unseen items where possible.", "labels": [], "entities": []}, {"text": "The split into seen/unseen highlights the fact that neither of the neural models does as well on seen items as the baseline; indeed the soft attention model is considerably worse in English and Hungarian, the two largest datasets.", "labels": [], "entities": []}, {"text": "The result is that this model actually underperforms the baseline when applied to all tokens, although a hybrid model (baseline for seen, soft attention for unseen) would outperform the baseline.", "labels": [], "entities": []}, {"text": "Nevertheless, the hard attention model performs best on unseen tokens in all cases, often by a wide margin, and also yields competitive overall performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics: the number of tokens in  train/dev/test sets;", "labels": [], "entities": []}, {"text": " Table 2: Tokens normalized correctly (%) for each dataset. Upper half: results on (A)ll tokens reported by", "labels": [], "entities": []}]}