{"title": [{"text": "Unified Pragmatic Models for Generating and Following Instructions", "labels": [], "entities": [{"text": "Generating and Following Instructions", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.7164997532963753}]}], "abstractContent": [{"text": "We show that explicit pragmatic inference aids in correctly generating and following natural language instructions for complex, sequential tasks.", "labels": [], "entities": []}, {"text": "Our pragmatics-enabled models reason about why speakers produce certain instructions , and about how listeners will react upon hearing them.", "labels": [], "entities": []}, {"text": "Like previous pragmatic models , we use learned base listener and speaker models to build a pragmatic speaker that uses the base listener to simulate the interpretation of candidate descriptions, and a pragmatic listener that reasons counterfactually about alternative descriptions.", "labels": [], "entities": []}, {"text": "We extend these models to tasks with sequential structure.", "labels": [], "entities": []}, {"text": "Evaluation of language generation and interpretation shows that pragmatic inference improves state-of-the-art listener models (at correctly interpreting human instructions) and speaker models (at producing instructions correctly interpreted by humans) in diverse settings.", "labels": [], "entities": [{"text": "language generation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7342788279056549}]}], "introductionContent": [{"text": "How should speakers and listeners reason about each other when they communicate?", "labels": [], "entities": []}, {"text": "A core insight of computational pragmatics is that speaker and listener agents operate within a cooperative game-theoretic context, and that each agent benefits from reasoning about others' intents and actions within that context.", "labels": [], "entities": []}, {"text": "Pragmatic inference has been studied by along line of work in linguistics, natural language processing, and cognitive science.", "labels": [], "entities": [{"text": "Pragmatic inference", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.895973801612854}, {"text": "natural language processing", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.6744701862335205}]}, {"text": "In this paper, we present a technique for layering explicit pragmatic inference on top of models for complex, sequential instruction-following and instruction-generation tasks.", "labels": [], "entities": []}, {"text": "We investigate a range of current data sets for both tasks, showing that pragmatic behavior arises naturally from this inference procedure, and gives rise to state-of-theart results in a variety of domains.", "labels": [], "entities": []}, {"text": "Consider the example shown in, in which a speaker agent must describe a route to The base listener moves to an unintended position (even though it correctly passes two objects).", "labels": [], "entities": []}, {"text": "The rational listener, which reasons about the speaker, infers that a route ending at the sofa would have been described differently, and stops earlier.", "labels": [], "entities": []}, {"text": "a target position in a hallway.", "labels": [], "entities": []}, {"text": "A conventional learned instruction-generating model produces a truthful description of the route (walk forward four times).", "labels": [], "entities": []}, {"text": "But the pragmatic speaker in this paper, which is capable of reasoning about the listener, chooses to also include additional information (the intersection with the bare concrete hall), to reduce potential ambiguity and increase the odds that the listener reaches the correct destination.", "labels": [], "entities": []}, {"text": "This same reasoning procedure also allows a listener agent to overcome ambiguity in instructions by reasoning counterfactually about the speaker.", "labels": [], "entities": []}, {"text": "Given the command walk along the blue carpet and you pass two objects, a conven-tional learned instruction-following model is willing to consider all paths that pass two objects, and ultimately arrives at an unintended final position.", "labels": [], "entities": []}, {"text": "But a pragmatic listener that reasons about the speaker can infer that the long path would have been more easily described as go to the sofa, and thus that the shorter path is probably intended.", "labels": [], "entities": []}, {"text": "In these two examples, which are produced by the system we describe in this paper, a unified reasoning process (choose the output sequence which is most preferred by an embedded model of the other agent) produces pragmatic behavior for both speakers and listeners.", "labels": [], "entities": []}, {"text": "The application of models with explicit pragmatic reasoning abilities has so far been largely restricted to simple reference games, in which the listener's only task is to select the right item from among a small set of candidate referents given a single short utterance from the speaker.", "labels": [], "entities": []}, {"text": "But as the example shows, there are real-world instruction following and generation tasks with rich action spaces that might also benefit from pragmatic modeling.", "labels": [], "entities": []}, {"text": "Moreover, approaches that learn to map directly between human-annotated instructions and action sequences are ultimately limited by the effectiveness of the humans themselves.", "labels": [], "entities": []}, {"text": "The promise of pragmatic modeling is that we can use these same annotations to build a model with a different (and perhaps even better) mechanism for interpreting and generating instructions.", "labels": [], "entities": []}, {"text": "The primary contribution of this work is to show how existing models of pragmatic reasoning can be extended to support instruction following and generation for challenging, multi-step, interactive tasks.", "labels": [], "entities": [{"text": "instruction following", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.7161156088113785}]}, {"text": "Our experimental evaluation focuses on four instruction-following domains which have been studied using both semantic parsers and attentional neural models.", "labels": [], "entities": []}, {"text": "We investigate the interrelated tasks of instruction following and instruction generation, and show that incorporating an explicit model of pragmatics helps in both cases.", "labels": [], "entities": [{"text": "instruction following", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7117979228496552}, {"text": "instruction generation", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.7131156921386719}]}, {"text": "Reasoning about the human listener allows a speaker model to produce instructions that are easier for humans to interpret correctly in all domains (with absolute gains inaccuracy ranging from 12% to 46%).", "labels": [], "entities": []}, {"text": "Similarly, reasoning about the human speaker improves the accuracy of the listener models in interpreting instructions inmost domains (with gains inaccuracy of up to 10%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9988449811935425}]}, {"text": "In all cases, the resulting systems are competitive with, and in many cases exceed, results from past state-of-the-art systems for these tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate speaker and listener agents on both the instruction following and instruction generation tasks in the SAIL domain and three SCONE domains (Section 2).", "labels": [], "entities": [{"text": "instruction following and instruction generation", "start_pos": 52, "end_pos": 100, "type": "TASK", "confidence": 0.6665241062641144}]}, {"text": "For all domains, we compare the rational listener and speaker against the base listener and speaker, as well as against past state-of-the-art results for each task and domain.", "labels": [], "entities": []}, {"text": "Finally, we examine pragmatic inference from a model combination perspective, comparing the pragmatic reranking procedure to ensembles of a larger number of base speakers or listeners.", "labels": [], "entities": []}, {"text": "For all experiments, we use beam search both to generate candidate lists for the rational systems (section 4.2) and to generate the base model's output.", "labels": [], "entities": []}, {"text": "We fix the beam size n to be the same in both the base and rational systems, using n = 20 for the speakers and n = 40 for the listeners.", "labels": [], "entities": []}, {"text": "We tune the weight \u03bb in the combined rational agents (L 0 \u00b7 L 1 or S 0 \u00b7 S 1 ) to maximize accuracy (for listener models) or BLEU (for speaker models) on each domain's development data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9993543028831482}, {"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9992406368255615}]}], "tableCaptions": [{"text": " Table 1: Instruction-following results on the SAIL  dataset. The table shows cross-validation test accu- racy for the base listener (L 0 ) and pragmatic listen- ers (L 0 \u00b7 L 1 ), along with the gain given by prag- matics. We report results for the single-and multi- sentence conditions, under the relative and absolute  starting conditions 5 , comparing to the best-performing  prior work by Artzi and Zettlemoyer (2013) (AZ), Artzi  et al. (2014) (ADP), and Mei et al. (2016) (MBW).  Bold numbers show new state-of-the-art results.", "labels": [], "entities": [{"text": "SAIL  dataset", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9176954329013824}, {"text": "accu- racy", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.8594817320505778}, {"text": "pragmatic listen- ers (L 0 \u00b7 L 1 )", "start_pos": 144, "end_pos": 178, "type": "METRIC", "confidence": 0.6845615208148956}]}, {"text": " Table 2: Instruction-following results in the SCONE  domains. The table shows accuracy on the test set. For  reference, we also show prior results from Guu et al.  (2017) (GPLL), although our models use more super- vision at training time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.999560534954071}]}, {"text": " Table 3: Instruction generation results. We report the  accuracies of human evaluators at following the outputs  of the speaker systems (as well as other humans) on 50- instance samples from the SAIL dataset and SCONE  domains. DBW is the system of Daniele et al. (2017).  Bold numbers are new state-of-the-art results.", "labels": [], "entities": [{"text": "Instruction generation", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8864214420318604}, {"text": "accuracies", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9681913256645203}, {"text": "SAIL dataset", "start_pos": 196, "end_pos": 208, "type": "DATASET", "confidence": 0.9479585886001587}]}, {"text": " Table 4: Gains in how easy the directions are to fol- low are not always associated with a gain in BLEU.  This table shows corpus-level 4-gram BLEU compar- ing outputs of the speaker systems to human-produced  directions on the SAIL dataset and SCONE domains,  compared to gains in accuracy when asking humans to  carry out a sample of the systems' directions (see", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9971394538879395}, {"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9269110560417175}, {"text": "SAIL dataset", "start_pos": 229, "end_pos": 241, "type": "DATASET", "confidence": 0.9187453985214233}, {"text": "accuracy", "start_pos": 283, "end_pos": 291, "type": "METRIC", "confidence": 0.9981707334518433}]}, {"text": " Table 6: Hyperparameters for the base listener (L 0 ) and  speaker (S 0 ) models. The SCONE speakers do not use  an attention mechanism.", "labels": [], "entities": []}]}