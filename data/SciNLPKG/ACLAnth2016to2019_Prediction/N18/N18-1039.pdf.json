{"title": [{"text": "Comparatives, Quantifiers, Proportions: A Multi-Task Model for the Learning of Quantities from Vision", "labels": [], "entities": []}], "abstractContent": [{"text": "The present work investigates whether different quantification mechanisms (set comparison , vague quantification, and proportional estimation) can be jointly learned from visual scenes by a multi-task computational model.", "labels": [], "entities": []}, {"text": "The motivation is that, in humans, these processes underlie the same cognitive, non-symbolic ability, which allows an automatic estimation and comparison of set magnitudes.", "labels": [], "entities": []}, {"text": "We show that when information about lower-complexity tasks is available, the higher-level proportional task becomes more accurate than when performed in isolation.", "labels": [], "entities": []}, {"text": "Moreover, the multi-task model is able to generalize to unseen combinations of target/non-target objects.", "labels": [], "entities": []}, {"text": "Consistently with behavioral evidence showing the interference of absolute number in the proportional task, the multi-task model no longer works when asked to provide the number of target objects in the scene.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding and producing sentences like 'There are more cars than parking lots', 'Most of the supporters wear blue t-shirts', 'Twenty percent of the trees have been planted last year', or 'Seven students passed the exam', is a fundamental competence which allows speakers to communicate information about quantities.", "labels": [], "entities": []}, {"text": "Crucially, the type of information conveyed by these expressions, as well as their underlying cognitive mechanisms, are not equivalent, as suggested by evidence from linguistics, language acquisition, and cognition.", "labels": [], "entities": []}, {"text": "First, comparatives ('more', 'less'), quantifiers ('some', 'most', 'all'), and proportions ('20%', 'two thirds') express a comparison or relation between sets (e.g., between the set of cars and the set of parking lots).", "labels": [], "entities": []}, {"text": "Such relational information is rather coarse when expressed by comparatives and vague quantifiers, more precise when denoted by proportions.", "labels": [], "entities": []}, {"text": "In contrast, numbers 'twenty-two') denote the exact, absolute cardinality of the items belonging to one set (e.g., the set of students who passed the exam).", "labels": [], "entities": []}, {"text": "Second, during language acquisition, these expressions are neither learned at the same time nor governed by the same rules.", "labels": [], "entities": []}, {"text": "Recent evidence showed that children can understand comparatives at around 3.3 years (, with quantifiers being learned a few months later, at around 3.4-3.6 years (.", "labels": [], "entities": []}, {"text": "Crucially, knowing the meaning of numbers, an ability that starts not before the age of 3.5 years, is not required to understand and use these expressions.", "labels": [], "entities": []}, {"text": "As for proportions, they are acquired significantly later, being fully mastered only at the age of 9 or 10 (.", "labels": [], "entities": []}, {"text": "Third, converging evidence from cognition and neuroscience supports the hypothesis that some important components of these expressions of quantity are grounded on a preverbal, nonsymbolic system representing magnitudes.", "labels": [], "entities": []}, {"text": "This system, often referred to as, is invariant to the sensory modality and almost universal in the animal domain, and consists in the ability of holistically extracting and comparing approximate numerosities (.", "labels": [], "entities": []}, {"text": "In humans, it is present since the youngest age, with 6-monthold infants being able to automatically compare sets and combine them by means of protoarithmetical operations).", "labels": [], "entities": []}, {"text": "Since it obeys Weber's law, according to which highly differing sets (e.g. 2:8) are easier to discriminate than highly similar sets (e.g. 7:8), ANS has been recently claimed to be a ratio-based mechanism (.", "labels": [], "entities": [{"text": "ANS", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.951104998588562}]}, {"text": "In support of this, behavioral findings indicate that, in non-symbolic contexts (e.g. visual scenes), proportional values are extracted holistically, i.e. without relying on the pre-computed cardinalities of the sets.", "labels": [], "entities": []}, {"text": "Indeed, people are fairly accurate in providing the proportion of targets in a scene, even in high-speed settings ().", "labels": [], "entities": []}, {"text": "Similarly, in briefly-presented scenes, the interpretation of quantifiers is shown to be best described by proportional information (Pezzelle et al., under review).", "labels": [], "entities": []}, {"text": "Altogether, this suggests that performing (1) set comparison, (2) vague quantification, and proportional estimation, which all rely on information regarding relations among sets, underlies increasingly-complex steps of the same mechanism.", "labels": [], "entities": [{"text": "proportional estimation", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.6327112019062042}]}, {"text": "Notably, such complexity would range from 'more/less' judgements to proportional estimation, as suggested by the increasing precision of ANS through years (, the reported boundary role of 'half' in early proportional reasoning, and the different age of acquisition of the corresponding linguistic expressions.", "labels": [], "entities": [{"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.998611569404602}, {"text": "ANS", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.7363907098770142}]}, {"text": "Finally, the ratio-based operation underlying these task would be different from (and possibly conflicting with) that of estimating the absolute numerosity of one set.", "labels": [], "entities": []}, {"text": "Indeed, absolute numbers are found to interfere with the access to proportions.", "labels": [], "entities": []}, {"text": "Inspired by this converging evidence, the present work proposes a computational framework to explore various quantification tasks in the visual domain (see).", "labels": [], "entities": []}, {"text": "In particular, we investigate whether ratio-based quantification tasks can be modeled by a single, multi-task learning neural network.", "labels": [], "entities": []}, {"text": "Given a synthetic scene depicting animals (in our setting, the 'target' objects) and artifacts ('non-target'), our model is designed to jointly perform all the tasks by means of an architecture that reflects their increasing complexity.", "labels": [], "entities": []}, {"text": "To perform proportional estimation (the most complex), the model builds on the representations learned to perform vague quantification and, in turn, set comparison (the least complex).", "labels": [], "entities": []}, {"text": "We show that the multi-task model achieves both higher accuracy and higher generalization power compared to the one-task models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9989907145500183}]}, {"text": "In contrast, we prove that introducing the absolute number task in the loop is not beneficial and indeed hurts the performance.", "labels": [], "entities": []}, {"text": "Our main contribution lies in the novel application and evaluation of a multi-task learning architecture on the task of jointly modeling 3 different quantification operations.", "labels": [], "entities": []}, {"text": "On the one hand, our results confirm the interdependency of the mechanisms underlying the tasks of set comparison, vague quantification, and proportional estimation.", "labels": [], "entities": [{"text": "set comparison", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.6476006209850311}, {"text": "vague quantification", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.6715927720069885}]}, {"text": "On the other, we provide further evidence on the effectiveness of these computational architectures.", "labels": [], "entities": []}], "datasetContent": [{"text": "We built a large dataset of synthetic visual scenes depicting a variable number of animals and artifacts on the top of a neutral, grey background We also experimented with Mean Average Error and dot product and found the same patterns of results (not reported).", "labels": [], "entities": [{"text": "Mean Average Error", "start_pos": 172, "end_pos": 190, "type": "METRIC", "confidence": 0.9615235328674316}]}, {"text": "In doing so, we employed the same methodology and materials used in Pezzelle et al.", "labels": [], "entities": []}, {"text": "(under review), where the use of quantifiers in grounded contexts was explored by asking participants to select the most suitable quantifier fora given scene.", "labels": [], "entities": []}, {"text": "Since the category of animals was always treated as the 'target', and that of artifacts as the 'non-target', we will henceforth use this terminology throughout the paper.", "labels": [], "entities": []}, {"text": "The scenes were automatically generated by an in-house script using the following pipeline: (a) Two natural images, one depicting a target object (e.g. a butterfly) and one depicting a non-target (e.g. a mug) were randomly picked up from a sample of the dataset by.", "labels": [], "entities": []}, {"text": "The sample was obtained by Pezzelle et al.", "labels": [], "entities": []}, {"text": "(under review), who manually selected pictures depicting whole items (not just parts) and whose color, orientation and shape were not deceptive.", "labels": [], "entities": []}, {"text": "In total, 100 unique instances of animals and 145 unique instances of artifacts were included; (b) The proportion of targets in the scene (e.g. 20%) was chosen by selecting one among 17 pre-defined ratios between targets:non-targets (e.g. 1:4, 'four non-targets to one target').", "labels": [], "entities": []}, {"text": "Out of 17 ratios, 8 were positive (targets > 50%), 8 negative (targets < 50%), and 1 equal (targets = 50%); (c) The absolute number of targets/non-targets was chosen to equally represent the various combinations available fora given ratio (e.g., for ratio 1:4: 1-4, 2-8, 3-12, 4-16), with the constraint of having a number of total objects in the scene (targets+non-targets) ranging from 3 to 20.", "labels": [], "entities": []}, {"text": "In total, 97 combinations were represented in the dataset, with an average of 5.7 combinations/ratio (min 2, max 18); (d) To inject some variability, the instances of target/non-target objects were randomly resized according to one of three possible sizes (i.e. medium, big, and small) and flipped on the vertical axis before being randomly inserted onto a 5*5-cell virtual grid.", "labels": [], "entities": []}, {"text": "As reported in, 17K scenes balanced per ratio (1K scenes/ratio) were generated and further split into train (70%), validation (10%), and test (20%).", "labels": [], "entities": [{"text": "validation", "start_pos": 115, "end_pos": 125, "type": "TASK", "confidence": 0.6668984293937683}]}, {"text": "Ground-truth classes for the tasks of setComp and propTarg were automatically assigned to each scene while generating the data.", "labels": [], "entities": [{"text": "propTarg", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.8561113476753235}]}, {"text": "For vagueQ, we took the probability distributions obtained on a dataset of 340 scenes by Pezzelle et al.", "labels": [], "entities": []}, {"text": "(under review) and we applied them to our datapoints, which were builtin the exact same way.", "labels": [], "entities": []}, {"text": "These probability distributions had been collected by asking participants to select, from a list of 9 quantifiers (reported in \u00a7 3.1), the most suitable one to describe the target objects in a visual scene presented for 1 second.", "labels": [], "entities": []}, {"text": "In particular, they were computed against the proportion of targets in the scene, which in that study was shown to be the overall best predictor for quantifiers.", "labels": [], "entities": []}, {"text": "To illustrate, given a scene containing 20% of targets (cf. leftmost panel in), the probability of choosing 'few' (ranging from 0 to 1) is 0.38, 'almost none' 0.27, 'the smaller part' 0.25, etc.", "labels": [], "entities": []}, {"text": "It is worth mentioning that, for scenes containing either 100% or 0% targets the probability of choosing 'all' and 'none', respectively, is around 1.", "labels": [], "entities": []}, {"text": "In all other cases, the distribution of probabilities is fuzzier and reflects the largely overlapping use of quantifiers, as in the example above.", "labels": [], "entities": []}, {"text": "On average, the probability of the most-chosen quantifier across ratios is 0.53.", "labels": [], "entities": []}, {"text": "Though this number cannot be seen as a genuine inter-annotator agreement score, it suggests that, on average, there is one quantifier which is preferred over the others.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number and partitioning of the datapoints.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9341663122177124}]}, {"text": " Table 2: Performance of the models in the tasks of set comparison (setComp), vague quantification (vagueQ),  proportional estimation (propTarg), and absolute number of targets (nTarg). Values in bold are the highest.", "labels": [], "entities": [{"text": "proportional estimation (propTarg)", "start_pos": 110, "end_pos": 144, "type": "METRIC", "confidence": 0.7140337944030761}, {"text": "absolute number of targets (nTarg)", "start_pos": 150, "end_pos": 184, "type": "METRIC", "confidence": 0.8505805134773254}]}, {"text": " Table 3: Unseen dataset. Performance of the models in  each task. Values in bold are the highest.", "labels": [], "entities": []}]}