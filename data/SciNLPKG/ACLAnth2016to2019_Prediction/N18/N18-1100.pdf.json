{"title": [{"text": "Explainable Prediction of Medical Codes from Clinical Text", "labels": [], "entities": [{"text": "Explainable Prediction of Medical Codes from Clinical Text", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.8322510495781898}]}], "abstractContent": [{"text": "Clinical notes are text documents that are created by clinicians for each patient encounter.", "labels": [], "entities": []}, {"text": "They are typically accompanied by medical codes, which describe the diagnosis and treatment.", "labels": [], "entities": []}, {"text": "Annotating these codes is labor intensive and error prone; furthermore, the connection between the codes and the text is not annotated , obscuring the reasons and details behind specific diagnoses and treatments.", "labels": [], "entities": []}, {"text": "We present an attentional convolutional network that predicts medical codes from clinical text.", "labels": [], "entities": []}, {"text": "Our method aggregates information across the document using a convolutional neural network, and uses an attention mechanism to select the most relevant segments for each of the thousands of possible codes.", "labels": [], "entities": []}, {"text": "The method is accurate , achieving precision@8 of 0.71 and a Micro-F1 of 0.54, which are both better than the prior state of the art.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.999667763710022}, {"text": "Micro-F1", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.6589152812957764}]}, {"text": "Furthermore, through an interpretability evaluation by a physician, we show that the attention mechanism identifies meaningful explanations for each code assignment .", "labels": [], "entities": []}], "introductionContent": [{"text": "Clinical notes are free text narratives generated by clinicians during patient encounters.", "labels": [], "entities": []}, {"text": "They are typically accompanied by a set of metadata codes from the International Classification of Diseases (ICD), which present a standardized way of indicating diagnoses and procedures that were performed during the encounter.", "labels": [], "entities": [{"text": "International Classification of Diseases (ICD)", "start_pos": 67, "end_pos": 113, "type": "DATASET", "confidence": 0.8962673715182713}]}, {"text": "ICD codes have a variety of uses, ranging from billing to predictive modeling of patient state (.", "labels": [], "entities": [{"text": "billing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.9616406559944153}]}, {"text": "Because manual coding is timeconsuming and error-prone, automatic coding has been studied since at least the 1990s (de).", "labels": [], "entities": [{"text": "automatic coding", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.6915818005800247}]}, {"text": "The task is difficult for two main reasons.", "labels": [], "entities": []}, {"text": "First, the label space is very high-dimensional, with over 15,000 codes in the ICD-9 taxonomy, and over 140,000 codes combined in the newer ICD-10-CM and ICD-10-PCS taxonomies.", "labels": [], "entities": [{"text": "ICD-9 taxonomy", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.9493526220321655}, {"text": "ICD-10-CM", "start_pos": 140, "end_pos": 149, "type": "DATASET", "confidence": 0.9183315634727478}, {"text": "ICD-10-PCS taxonomies", "start_pos": 154, "end_pos": 175, "type": "DATASET", "confidence": 0.918411523103714}]}, {"text": "Second, clinical text includes irrelevant information, misspellings and non-standard abbreviations, and a large medical vocabulary.", "labels": [], "entities": []}, {"text": "These features combine to make the prediction of ICD codes from clinical notes an especially difficult task, for computers and human coders alike).", "labels": [], "entities": [{"text": "prediction of ICD codes from clinical notes", "start_pos": 35, "end_pos": 78, "type": "TASK", "confidence": 0.8826215096882412}]}, {"text": "In this application paper, we develop convolutional neural network (CNN)-based methods for automatic ICD code assignment based on text discharge summaries from intensive care unit (ICU) stays.", "labels": [], "entities": [{"text": "ICD code assignment", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.8633202711741129}, {"text": "text discharge summaries from intensive care unit (ICU) stays", "start_pos": 130, "end_pos": 191, "type": "TASK", "confidence": 0.7629367167299445}]}, {"text": "To better adapt to the multi-label setting, we employ a per-label attention mechanism, which allows our model to learn distinct document representations for each label.", "labels": [], "entities": []}, {"text": "We call our method Convolutional Attention for Multi-Label classification (CAML).", "labels": [], "entities": [{"text": "Multi-Label classification (CAML)", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.8247281193733216}]}, {"text": "Our model design is motivated by the conjecture that important information correlated with a code's presence maybe contained in short snippets of text which could be anywhere in the document, and that these snippets likely differ for different labels.", "labels": [], "entities": []}, {"text": "To cope with the large label space, we exploit the textual descriptions of each code to guide our model towards appropriate parameters: in the absence of many labeled examples fora given code, its parameters should be similar to those of codes with similar textual descriptions.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two versions of MIMIC (, an open dataset of ICU medical records.", "labels": [], "entities": [{"text": "MIMIC", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.4806462228298187}]}, {"text": "Each record includes a variety of narrative notes describing a patient's stay, including diagnoses and procedures.", "labels": [], "entities": []}, {"text": "Our approach substantially outperforms previous results on medical code prediction on both MIMIC-II and MIMIC-III datasets.", "labels": [], "entities": [{"text": "medical code prediction", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.5687322417894999}, {"text": "MIMIC-II", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.9177061915397644}, {"text": "MIMIC-III datasets", "start_pos": 104, "end_pos": 122, "type": "DATASET", "confidence": 0.8682197630405426}]}, {"text": "We consider applications of this work in a decision support setting.", "labels": [], "entities": [{"text": "decision support", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.9187966585159302}]}, {"text": "Interpretability is important for any decision support system, especially in the 934.1: \"Foreign body in main bronchus\" CAML .  medical domain.", "labels": [], "entities": [{"text": "CAML .  medical domain", "start_pos": 120, "end_pos": 142, "type": "DATASET", "confidence": 0.8606008440256119}]}, {"text": "The system should be able to explain why it predicted each code; even if the codes are manually annotated, it is desirable to explain what parts of the text are most relevant to each code.", "labels": [], "entities": []}, {"text": "These considerations further motivate our per-label attention mechanism, which assigns importance values to -grams in the input document, and which can therefore provide explanations for each code, in the form of extracted snippets of text from the input document.", "labels": [], "entities": []}, {"text": "We perform a human evaluation of the quality of the explanations provided by the attention mechanism, asking a physician to rate the informativeness of a set of automatically generated explanations.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section evaluates the accuracy of code prediction, comparing our models against several competitive baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9988930821418762}, {"text": "code prediction", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7677760422229767}]}, {"text": "MIMIC-III () is an open-access dataset of text and structured records from a hospital ICU.", "labels": [], "entities": []}, {"text": "Following previous work, we focus on discharge summaries, which condense information about a stay into a single document.", "labels": [], "entities": [{"text": "discharge summaries", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7615450918674469}]}, {"text": "In MIMIC-III, some admissions have addenda to their summary, which we concatenate to form one document.", "labels": [], "entities": [{"text": "MIMIC-III", "start_pos": 3, "end_pos": 12, "type": "DATASET", "confidence": 0.7787336707115173}]}, {"text": "Each admission is tagged by human coders with a set of ICD-9 codes, describing both diagnoses and procedures which occurred during the patient's stay.", "labels": [], "entities": []}, {"text": "There are 8,921 unique ICD-9 codes present in our datasets, including 6,918 diagnosis codes and 2,003 procedure codes.", "labels": [], "entities": []}, {"text": "Some patients have multiple admissions and therefore multiple discharge summaries; we split the data by patient ID, so that no patient appears in both the training and test sets.", "labels": [], "entities": []}, {"text": "In this full-label setting, we use a set of 47,724 discharge summaries from 36,998 patients for training, with 1,632 summaries and 3,372 summaries for validation and testing, respectively.", "labels": [], "entities": []}, {"text": "Secondary evaluations For comparison with prior work, we also follow and train and evaluate on a label set consisting of the 50 most frequent labels.", "labels": [], "entities": []}, {"text": "In this setting, we filter each dataset down to the instances that have at least one of the top 50 most frequent codes, and subset the training data to equal the size of the training set of, resulting in 8,067 summaries for training, 1,574 for validation, and 1,730 for testing.", "labels": [], "entities": []}, {"text": "We also run experiments with the MIMIC-II dataset, to compare with prior work by and.", "labels": [], "entities": [{"text": "MIMIC-II dataset", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.9251058399677277}]}, {"text": "We use the train/test split of, which consists of 20,533 training examples and 2,282 testing examples.", "labels": [], "entities": []}, {"text": "Detailed statistics for the three settings are summarized in.", "labels": [], "entities": []}, {"text": "Preprocessing We remove tokens that contain no alphabetic characters (e.g., removing \"500\" but keeping \"250mg\"), lowercase all tokens, and replace tokens that appear in fewer than three training documents with an 'UNK' token.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8574188351631165}]}, {"text": "We pretrain word embeddings of size = 100 using the word2vec CBOW method ( on the preprocessed text from all discharge summaries.", "labels": [], "entities": []}, {"text": "All documents are truncated to a maximum length of 2500 tokens.", "labels": [], "entities": []}, {"text": "To facilitate comparison with both future and prior work, we report a variety of metrics, focusing on the micro-averaged and macro-averaged F1 and area under the ROC curve (AUC).", "labels": [], "entities": [{"text": "F1", "start_pos": 140, "end_pos": 142, "type": "METRIC", "confidence": 0.9502066969871521}, {"text": "ROC curve (AUC)", "start_pos": 162, "end_pos": 177, "type": "METRIC", "confidence": 0.953747570514679}]}, {"text": "Micro-averaged values are calculated by treating each (text, code) pair as a separate prediction.", "labels": [], "entities": []}, {"text": "Macro-averaged values, while less frequently reported in the multilabel classification literature, are calculated by averaging metrics computed per-label.", "labels": [], "entities": [{"text": "multilabel classification", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.7051878273487091}]}, {"text": "For recall, the metrics are distinguished as follows: where TP denotes true positive examples and FN denotes false negative examples.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9683659076690674}, {"text": "TP", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9731062650680542}, {"text": "FN", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9848530292510986}]}, {"text": "The macro-averaged metrics place much more emphasis on rare label prediction.", "labels": [], "entities": [{"text": "rare label prediction", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.6886350909868876}]}, {"text": "We also report precision at (denoted as 'P@n'), which is the fraction of the highestscored labels that are present in the ground truth.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9947190284729004}]}, {"text": "This is motivated by the potential use case as a decision support application, in which a user is presented with a fixed number of predicted codes to review.", "labels": [], "entities": []}, {"text": "In such a case, it is more suitable to select a model with high precision than high recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9979817867279053}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9985802173614502}]}, {"text": "We choose = 5 and = 8 to compare with prior work ().", "labels": [], "entities": []}, {"text": "For the MIMIC-III full label setting, we also compute precision@15, which roughly corresponds to the average number of codes in MIMIC-III discharge summaries.", "labels": [], "entities": [{"text": "MIMIC-III", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.6635425090789795}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9986699819564819}, {"text": "MIMIC-III discharge summaries", "start_pos": 128, "end_pos": 157, "type": "TASK", "confidence": 0.5413415630658468}]}, {"text": "To compare with prior published work, we also evaluate on the 50 most common codes in MIMIC-III, and on MIMIC-II).", "labels": [], "entities": [{"text": "MIMIC-III", "start_pos": 86, "end_pos": 95, "type": "DATASET", "confidence": 0.8688139319419861}, {"text": "MIMIC-II", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.850397527217865}]}, {"text": "We report DR-CAML results on the 50-label setting of MIMIC-III with = 10, and on MIMIC-II with = 0.1, which were determined by grid search on a validation set.", "labels": [], "entities": [{"text": "MIMIC-III", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.8830493092536926}, {"text": "MIMIC-II", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.8344188332557678}]}, {"text": "The other hyperparameters were left at the settings for the main MIMIC-III evaluation, as described in.", "labels": [], "entities": [{"text": "MIMIC-III", "start_pos": 65, "end_pos": 74, "type": "TASK", "confidence": 0.46515339612960815}]}, {"text": "In the 50-label setting of MIMIC-III, we see strong improvement over prior work in all reported metrics, as well as against the baselines, with the exception of precision@5, on which the CNN baseline performs best.", "labels": [], "entities": [{"text": "MIMIC-III", "start_pos": 27, "end_pos": 36, "type": "DATASET", "confidence": 0.643539309501648}, {"text": "precision", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9969111084938049}, {"text": "CNN baseline", "start_pos": 187, "end_pos": 199, "type": "DATASET", "confidence": 0.8799726963043213}]}, {"text": "We hypothesize that this is because the relatively large value of = 10 for CAML leads to a larger network that is more suited to larger datasets; tuning CAML's hyperparameters on this dataset would be expected to improve performance on all metrics.", "labels": [], "entities": []}, {"text": "additionally report a micro-F1 score of 0.407 by training on MIMIC-III, and evaluating on MIMIC-II.", "labels": [], "entities": [{"text": "MIMIC-III", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.9221523404121399}, {"text": "MIMIC-II", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.9518751502037048}]}, {"text": "Our model achieves better performance using only the (smaller) MIMIC-II training set, leaving this alternative training protocol for future work.", "labels": [], "entities": [{"text": "MIMIC-II training set", "start_pos": 63, "end_pos": 84, "type": "DATASET", "confidence": 0.8537864287694296}]}, {"text": "We now evaluate the explanations generated by CAML's attention mechanism, in comparison with three alternative heuristics.", "labels": [], "entities": []}, {"text": "A physician was presented with explanations from four methods, using a random sample of 100 predicted codes from the MIMIC-III full-label test set.", "labels": [], "entities": [{"text": "MIMIC-III full-label test set", "start_pos": 117, "end_pos": 146, "type": "DATASET", "confidence": 0.8513436913490295}]}, {"text": "The most important -gram from each method was extracted, along with a window of five words on either side for context.", "labels": [], "entities": []}, {"text": "We select = 4 in this setting to emulate a span of attention over words likely to be given by a human reader.", "labels": [], "entities": []}, {"text": "Examples can be found in.", "labels": [], "entities": []}, {"text": "Observe that the snippets may overlap in multiple words.", "labels": [], "entities": []}, {"text": "We prompted the evaluator to select all text snippets which he felt adequately explained the presence of a given code, provided the code and its description, with the option to distinguish snippets as \"highly informative\" should they be found particularly informative over others.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Descriptive statistics for MIMIC discharge summary training sets.", "labels": [], "entities": [{"text": "MIMIC discharge summary training sets", "start_pos": 37, "end_pos": 74, "type": "DATASET", "confidence": 0.6028005838394165}]}, {"text": " Table 3: Hyperparameter ranges and optimal values for  each neural model selected by Spearmint.", "labels": [], "entities": []}, {"text": " Table 4: Results on MIMIC-III full, 8922 labels. Here, \"Diag\" denotes Micro-F1 performance on diagnosis codes  only, and \"Proc\" denotes Micro-F1 performance on procedure codes only. Here and in all tables, (*) by the bold  (best) result indicates significantly improved results compared to the next best result, < 0.001.", "labels": [], "entities": [{"text": "MIMIC-III full", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.8479202091693878}]}, {"text": " Table 5: Results on MIMIC-III, 50 labels.", "labels": [], "entities": [{"text": "MIMIC-III", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.6904420852661133}]}, {"text": " Table 6: Results on MIMIC-II full, 5031 labels.", "labels": [], "entities": [{"text": "MIMIC-II full", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.8492383658885956}]}, {"text": " Table 7: Qualitative evaluation results. The columns", "labels": [], "entities": []}]}