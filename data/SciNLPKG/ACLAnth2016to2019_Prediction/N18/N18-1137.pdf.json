{"title": [], "abstractContent": [{"text": "A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and associated texts.", "labels": [], "entities": [{"text": "statistical data-to-text generation", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.6666388114293417}]}, {"text": "In this paper we aim to bootstrap generators from large scale datasets where the data (e.g., DBPedia facts) and related texts (e.g., Wikipedia abstracts) are loosely aligned.", "labels": [], "entities": []}, {"text": "We tackle this challenging task by introducing a special-purpose content selection mechanism.", "labels": [], "entities": []}, {"text": "1 We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention.", "labels": [], "entities": []}], "introductionContent": [{"text": "A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and paired texts ().", "labels": [], "entities": [{"text": "statistical data-to-text generation", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.6710340281327566}]}, {"text": "These correspondences describe how data representations are expressed in natural language (content realisation) but also indicate which subset of the data is verbalised in the text (content selection).", "labels": [], "entities": []}, {"text": "Although content selection is traditionally performed by domain experts, recent advances in generation using neural networks () have led to the use of large scale datasets containing loosely related data and text pairs.", "labels": [], "entities": [{"text": "content selection", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7558760344982147}]}, {"text": "A prime example are online data sources like and Wikipedia and their associated texts which Our code and data are available at https://github.com/EdinburghNLP/wikigen. are often independently edited.", "labels": [], "entities": [{"text": "EdinburghNLP", "start_pos": 146, "end_pos": 158, "type": "DATASET", "confidence": 0.957233190536499}]}, {"text": "Another example are sports databases and related textual resources.", "labels": [], "entities": []}, {"text": "recently define a generation task relating statistics of basketball games with commentaries and a blog written by fans.", "labels": [], "entities": []}, {"text": "In this paper, we focus on short text generation from such loosely aligned data-text resources.", "labels": [], "entities": [{"text": "short text generation", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.649482657512029}]}, {"text": "We work with the biographical subset of the DBPedia and Wikipedia resources where the data corresponds to DBPedia facts and texts are Wikipedia abstracts about people.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.959250271320343}, {"text": "DBPedia facts", "start_pos": 106, "end_pos": 119, "type": "DATASET", "confidence": 0.9202422499656677}]}, {"text": "shows an example for the film-maker Robert Flaherty, the Wikipedia infobox, and the corresponding abstract.", "labels": [], "entities": [{"text": "Wikipedia infobox", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.984631359577179}]}, {"text": "We wish to bootstrap a data-to-text generator that learns to verbalise properties about an entity from a loosely related example text.", "labels": [], "entities": []}, {"text": "Given the set of properties in and the related text in, we want to learn verbalisations for those properties that are mentioned in the text and produce a short description like the one in.", "labels": [], "entities": []}, {"text": "In common with previous work our model draws on insights from neural machine translation () using an encoder-decoder architecture as its backbone.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.7131298383076986}]}, {"text": "introduce the task of generating biographies from Wikipedia data, however they focus on single sentence generation.", "labels": [], "entities": [{"text": "single sentence generation", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.6881518363952637}]}, {"text": "We generalize the task to multi-sentence text, and highlight the limitations of the standard attention mechanism which is often used as a proxy for content selection.", "labels": [], "entities": [{"text": "content selection", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.6825560033321381}]}, {"text": "When exposed to sub-sequences that do not correspond to any facts in the input, the soft attention mechanism will still try to justify the sequence and somehow distribute the attention weights over the input representation.", "labels": [], "entities": []}, {"text": "The decoder will still memorise high frequency sub-sequences in spite of these not being supported by any facts in the input.", "labels": [], "entities": []}, {"text": "We propose to alleviate these shortcom-(a) (b) Robert Joseph Flaherty,) was an American film-maker who directed and produced the first commercially successful feature-length documentary film, Nanook of the North.", "labels": [], "entities": []}, {"text": "The film made his reputation and nothing in his later life fully equalled its success, although he continued the development of this new genre of narrative documentary, e.g., with, set in the South Seas, and Man of, filmed in Ireland's Aran Islands.", "labels": [], "entities": [{"text": "Man of, filmed in Ireland's Aran Islands", "start_pos": 208, "end_pos": 248, "type": "DATASET", "confidence": 0.697588817940818}]}, {"text": "He is considered the \"father\" of both the documentary and the ethnographic film.", "labels": [], "entities": []}, {"text": "ings via a specific content selection mechanism based on multi-instance learning (MIL;) which automatically discovers correspondences, namely alignments, between data and text pairs.", "labels": [], "entities": []}, {"text": "These alignments are then used to modify the generation function during training.", "labels": [], "entities": []}, {"text": "We experiment with two frameworks that allow to incorporate alignment information, namely multi-task learning (MTL; and reinforcement learning (RL; Williams, 1992).", "labels": [], "entities": []}, {"text": "In both cases we define novel objective functions using the learnt alignments.", "labels": [], "entities": []}, {"text": "Experimental results using automatic and human-based evaluation show that models trained with content-specific objectives improve upon vanilla encoder-decoder architectures which rely solely on soft attention.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "We discuss related work in Section 2 and describe the MIL-based content selection approach in Section 3.", "labels": [], "entities": [{"text": "MIL-based content selection", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.922844390074412}]}, {"text": "We explain how the generator is trained in Section 4 and present evaluation experiments in Section 5.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We evaluated our model on a dataset collated from WIKIBIO (), a corpus of 728,321 biography articles (their first paragraph) and their infoboxes sampled from the English Wikipedia.", "labels": [], "entities": [{"text": "WIKIBIO", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.9522976279258728}, {"text": "English Wikipedia", "start_pos": 167, "end_pos": 184, "type": "DATASET", "confidence": 0.9271711707115173}]}, {"text": "We adapted the original dataset in three ways.", "labels": [], "entities": []}, {"text": "Firstly, we make use of the entire abstract rather than first sentence.", "labels": [], "entities": []}, {"text": "Secondly, we reduced the dataset to examples with a rich set of properties and multi-sentential text.", "labels": [], "entities": []}, {"text": "We eliminated examples with less than six property-value pairs and abstracts consisting of one sentence.", "labels": [], "entities": []}, {"text": "We also placed a minimum restriction of 23 words in the length of the abstract.", "labels": [], "entities": []}, {"text": "We considered abstracts up to a maximum of 12 sentences and property sets with a maximum of 50 property-value pairs.", "labels": [], "entities": []}, {"text": "Finally, we associated each abstract with the set of DBPedia properties p : v corresponding to the abstract's main entity.", "labels": [], "entities": []}, {"text": "As entity classification is available in DBPedia for most entities, we concatenate class information c (whenever available) with the property value, i.e., p : v c.", "labels": [], "entities": [{"text": "entity classification", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.7186492681503296}]}, {"text": "In, the property value spouse : Frances H. Flaherty is extended with class information from the DBPedia ontology to spouse : Frances H. Flaherty Person.", "labels": [], "entities": [{"text": "DBPedia ontology", "start_pos": 96, "end_pos": 112, "type": "DATASET", "confidence": 0.9472352564334869}]}, {"text": "Pre-processing Numeric date formats were converted to a surface form with month names.", "labels": [], "entities": []}, {"text": "Numerical expressions were delexicalised using different tokens created with the property name and position of the delexicalised token on the value sequence.", "labels": [], "entities": []}, {"text": "For instance, given the property-value for birth date in, the first sentence in the abstract) becomes \" Robert Joseph Flaherty, (February DLX birth date 2, DLX birth date 4 -July . .", "labels": [], "entities": []}, {"text": "\". Years and numbers in the text not found in the values of the property set were replaced with tokens YEAR and NUMERIC.", "labels": [], "entities": [{"text": "YEAR", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9787538051605225}, {"text": "NUMERIC", "start_pos": 112, "end_pos": 119, "type": "METRIC", "confidence": 0.963988184928894}]}, {"text": "Ina second phase, when creating the input and output vocabularies, VI and V O respectively, we delexicalised words w which were absent from the output vocabulary but were attested in the input vocabulary.", "labels": [], "entities": []}, {"text": "Again, we created tokens based on the property name and the position of the word in the value sequence.", "labels": [], "entities": []}, {"text": "Words not in V O or VI were replaced with the symbol UNK.", "labels": [], "entities": [{"text": "UNK", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.8305323123931885}]}, {"text": "Vocabulary sizes were limited to |V I | = 50k and |V O | = 50k for the alignment model and |V O | = 20k for the generator.", "labels": [], "entities": []}, {"text": "We discarded examples where the text contained more than three UNKs (for the content aligner) and five UNKs (for the generator); or more than two UNKs in the property-value (for generation).", "labels": [], "entities": []}, {"text": "Finally, we added the empty relation to the property sets.", "labels": [], "entities": []}, {"text": "For the content aligner (cf. Section 3), each sentence constitutes a training instance, and as a result the sizes of the train and development sets are 796,446 and 153,096, respectively.", "labels": [], "entities": []}, {"text": "Training Configuration We adjusted all models' hyperparameters according to their performance on the development set.", "labels": [], "entities": []}, {"text": "The encoders for both content selection and generation models were initialised with GloVe () pre-trained vectors.", "labels": [], "entities": [{"text": "content selection and generation", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.7298762053251266}]}, {"text": "The input and hidden unit dimension was set to 200 for content selection and 100 for generation.", "labels": [], "entities": []}, {"text": "In all models, we used encoder biLSTMs and decoder LSTM (regularised with a dropout rate of 0.3 () with one layer.", "labels": [], "entities": []}, {"text": "Content selection and generation models (base encoder-decoder and MTL) were trained for 20 epochs with the ADAM optimiser () using a learning rate of 0.001.", "labels": [], "entities": [{"text": "Content selection and generation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7454842031002045}]}, {"text": "The reinforcement learning model was initialised with the base encoder-decoder model and trained for 35 additional epochs with stochastic gradient descent and a fixed learning rate of 0.001.", "labels": [], "entities": []}, {"text": "Block sizes were set to 40 (base), 60 (MTL) and 50 (RL).", "labels": [], "entities": [{"text": "RL", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9269024729728699}]}, {"text": "Weights for the MTL objective were also tuned experimentally; we set \u03bb = 0.1 for the first four epochs (training focuses on alignment prediction) and switched to \u03bb = 0.9 for the remaining epochs.", "labels": [], "entities": [{"text": "MTL", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9709148406982422}, {"text": "alignment prediction", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.9732573628425598}]}, {"text": "Content Alignment We optimized content alignment on the development set against manual alignments.", "labels": [], "entities": [{"text": "Content Alignment", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6603606790304184}, {"text": "content alignment", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.6983654350042343}]}, {"text": "Specifically, two annotators aligned 132 sentences to their infoboxes.", "labels": [], "entities": []}, {"text": "We used the Yawat annotation tool and followed the alignment guidelines (and evaluation metrics) used in.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement using macro-averaged f-score was 0.72 (we treated one annotator as the reference and the other one as hypothetical system output).", "labels": [], "entities": []}, {"text": "Alignment sets were extracted from the model's output (cf. Section 3) by optimizing the threshold avg(sim) + a * std(sim) where sim denotes the similarity between the set of property values and words, and a is empirically set to 0.75; avg and std are the mean and standard deviation of sim scores across the development set.", "labels": [], "entities": []}, {"text": "Each word was aligned to a property-value if their similarity exceeded a threshold of 0.22.", "labels": [], "entities": []}, {"text": "Our best content alignment model (Content-Aligner) obtained an fscore of 0.36 on the development set.", "labels": [], "entities": [{"text": "content alignment", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.6931070238351822}, {"text": "fscore", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.996134877204895}]}, {"text": "We also compared our Content-Aligner against a baseline based on pre-trained word embeddings (EmbeddingsBL).", "labels": [], "entities": []}, {"text": "For each pair (P , s) we computed the dot product between words in sand properties in P (properties were represented by the the averaged sum of their words' vectors).", "labels": [], "entities": []}, {"text": "Words were aligned to property-values if their similarity exceeded a threshold of 0.4.", "labels": [], "entities": []}, {"text": "EmbeddingsBL obtained an f-score of 0.057 against the manual alignments.", "labels": [], "entities": [{"text": "f-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9956861734390259}]}, {"text": "Finally, we compared the performance of the Content-Aligner at the level of property set P and sentence s similarity by comparing the average ranking position of correct pairs among 14 distractors, namely rank@15.", "labels": [], "entities": []}, {"text": "The Content-Aligner obtained a rank of 1.31, while the EmbeddingsBL model had a rank of 7.99 (lower is better).", "labels": [], "entities": []}, {"text": "() against the noisy Wikipedia abstracts.", "labels": [], "entities": []}, {"text": "Considering these as a gold standard is, however, not entirely satisfactory for two reasons.", "labels": [], "entities": []}, {"text": "Firstly, our models generate considerably shorter text and will be penalized for not generating text they were not supposed to generate in the first place.", "labels": [], "entities": []}, {"text": "Secondly, the model might try to reproduce what is in the imperfect reference but not supported by the input properties and as a result will be rewarded when it should not.", "labels": [], "entities": []}, {"text": "To alleviate this, we crowd-sourced using AMT a revised version of 200 randomly selected abstracts from the test set.", "labels": [], "entities": [{"text": "AMT", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.8861975073814392}]}, {"text": "Crowdworkers were shown a Wikipedia infobox with the accompanying abstract and were asked to adjust the text to the content present in the infobox.", "labels": [], "entities": [{"text": "Wikipedia infobox", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.9369853436946869}]}, {"text": "Annotators were instructed to delete spans which did not have supporting facts and rewrite the remaining parts into a well-formed text.", "labels": [], "entities": []}, {"text": "We collected three revised versions for each abstract.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement was 81.64 measured as the mean pairwise BLEU-4 amongst AMT workers.", "labels": [], "entities": [{"text": "Inter-annotator agreement", "start_pos": 0, "end_pos": 25, "type": "METRIC", "confidence": 0.6746053397655487}, {"text": "BLEU-4", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9580380320549011}]}, {"text": "Automatic evaluation results against the revised abstracts are also shown in.", "labels": [], "entities": []}, {"text": "As can be seen, all encoder-decoder based models have a significant advantage over Templ when evaluating against both types of abstracts.", "labels": [], "entities": []}, {"text": "The model enabled with the multi-task learning content selection mechanism brings an improvement of 1.29 BLEU-4 over a vanilla encoder-decoder model.", "labels": [], "entities": [{"text": "multi-task learning content selection", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.6179650351405144}, {"text": "BLEU-4", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9961918592453003}]}, {"text": "Performance of the RL trained model is inferior and close to the ED model.", "labels": [], "entities": [{"text": "ED", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.6707726716995239}]}, {"text": "We discuss the reasons for this discrepancy shortly.", "labels": [], "entities": []}, {"text": "To provide a rough comparison with the results reported in, we also computed BLEU-4 on the first sentence of the text generated by our system.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9987781643867493}]}, {"text": "3 Recall that their model generates the first sentence of the abstract, whereas we out- We post-processed system output with Stanford CoreNLP ( ) to extract the first sentence.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 125, "end_pos": 141, "type": "DATASET", "confidence": 0.8779885768890381}]}, {"text": "Human-Based Evaluation We further examined differences among systems in a human-based evaluation study.", "labels": [], "entities": []}, {"text": "Using AMT, we elicited 3 judgements for the same 200 infobox-abstract pairs we used in the abstract revision study.", "labels": [], "entities": [{"text": "AMT", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.7667006254196167}, {"text": "abstract revision", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.6944359540939331}]}, {"text": "We compared the output of the templates, the three neural generators and also included one of the human edited abstracts as a gold standard (reference).", "labels": [], "entities": []}, {"text": "For each test case, we showed crowdworkers the Wikipedia infobox and five short texts in random order.", "labels": [], "entities": [{"text": "Wikipedia infobox", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9470303356647491}]}, {"text": "The annotators were asked to rank each of the texts according to the following criteria: (1) Is the text faithful to the content of the table? and (2) Is the text overall comprehensible and fluent?", "labels": [], "entities": []}, {"text": "Ties were allowed only when texts were identical strings.", "labels": [], "entities": [{"text": "Ties", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9596472978591919}]}, {"text": "Table 5 presents examples of the texts (and properties) crowdworkers saw.", "labels": [], "entities": []}, {"text": "shows, proportionally, how often crowdworkers ranked each system, first, second, and soon.", "labels": [], "entities": []}, {"text": "Unsurprisingly, the human authored gold text is considered best (and ranked first 47% of the time).", "labels": [], "entities": []}, {"text": "ED MTL is mostly ranked second and third best, followed closely by ED RL . The vanilla encoder-decoder system ED is mostly forth and Templ is fifth.", "labels": [], "entities": [{"text": "ED MTL", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.903927356004715}, {"text": "ED RL", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.7109842896461487}, {"text": "ED", "start_pos": 110, "end_pos": 112, "type": "DATASET", "confidence": 0.8615682721138}]}, {"text": "As shown in the last column of the table (Rank), the ranking of ED MTL is overall slightly better than ED RL . We further converted the ranks to ratings on a scale of 1 to 5 (assigning ratings 5.", "labels": [], "entities": [{"text": "ED MTL", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.6249915957450867}, {"text": "ED RL", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.6907596290111542}]}, {"text": "1 to rank placements 1.", "labels": [], "entities": []}, {"text": "This allowed us to perform Analysis of Variance (ANOVA) which revealed a reliable effect of system type.", "labels": [], "entities": [{"text": "Analysis of Variance (ANOVA)", "start_pos": 27, "end_pos": 55, "type": "METRIC", "confidence": 0.7224457710981369}]}, {"text": "Post-hoc Tukey tests showed that all systems were significantly worse than RevAbs and significantly better than Templ (p < 0.05).", "labels": [], "entities": [{"text": "RevAbs", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.9577654600143433}]}, {"text": "ED MTL is not significantly better than ED RL but is significantly (p < 0.05) different from ED.", "labels": [], "entities": [{"text": "ED MTL", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.563659593462944}]}], "tableCaptions": [{"text": " Table 1: Example of word-property alignments for the  Wikipedia abstract and facts in", "labels": [], "entities": [{"text": "word-property alignments", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.725194126367569}]}, {"text": " Table 3: BLEU-4 results using the original Wikipedia  abstract (Abstract) as reference and crowd-sourced re- vised abstracts (RevAbs) for template baseline (Templ),  standard encoder-decoder model (ED), and our content- based models trained with multi-task learning (ED MTL )  and reinforcement learning (ED RL ).", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.994310736656189}]}, {"text": " Table 4: Rankings shown as proportions and mean  ranks given to systems by human subjects.", "labels": [], "entities": []}]}