{"title": [{"text": "Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work, we present a hybrid learning method for training task-oriented dialogue systems through online user interactions.", "labels": [], "entities": []}, {"text": "Popular methods for learning task-oriented dialogues include applying reinforcement learning with user feedback on supervised pre-training models.", "labels": [], "entities": []}, {"text": "Efficiency of such learning method may suffer from the mismatch of dialogue state distribution between offline training and online interactive learning stages.", "labels": [], "entities": []}, {"text": "To address this challenge, we propose a hybrid imitation and reinforcement learning method, with which a dialogue agent can effectively learn from its interaction with users by learning from human teaching and feedback.", "labels": [], "entities": []}, {"text": "We design a neural network based task-oriented dialogue agent that can be optimized end-to-end with the proposed learning method.", "labels": [], "entities": []}, {"text": "Experimental results show that our end-to-end dialogue agent can learn effectively from the mistake it makes via imitation learning from user teaching.", "labels": [], "entities": []}, {"text": "Applying reinforcement learning with user feedback after the imitation learning stage further improves the agent's capability in successfully completing a task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Task-oriented dialogue systems assist users to complete tasks in specific domains by understanding user's request and aggregate useful information from external resources within several dialogue turns.", "labels": [], "entities": []}, {"text": "Conventional task-oriented dialogue systems have a complex pipeline () consisting of independently developed and modularly connected components for natural language understanding (NLU);, dialogue state tracking (DST) (; * Work done while the author was an intern at Google.", "labels": [], "entities": [{"text": "natural language understanding (NLU)", "start_pos": 148, "end_pos": 184, "type": "TASK", "confidence": 0.8159883320331573}, {"text": "dialogue state tracking (DST", "start_pos": 187, "end_pos": 215, "type": "TASK", "confidence": 0.6934433043003082}]}, {"text": "\u2020 Work done while at Google Research.", "labels": [], "entities": []}, {"text": "Mrk\u0161i\u00b4, and dialogue policy learning (.", "labels": [], "entities": [{"text": "Mrk\u0161i\u00b4", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8482522368431091}, {"text": "dialogue policy learning", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8893914620081583}]}, {"text": "These system components are usually trained independently, and their optimization targets may not fully align with the overall system evaluation criteria (e.g. task success rate and user satisfaction).", "labels": [], "entities": []}, {"text": "Moreover, errors made in the upper stream modules of the pipeline propagate to downstream components and get amplified, making it hard to track the source of errors.", "labels": [], "entities": []}, {"text": "To address these limitations with the conventional task-oriented dialogue systems, recent efforts have been made in designing endto-end learning solutions with neural network based methods.", "labels": [], "entities": []}, {"text": "Both supervised learning (SL) based) and deep reinforcement learning (RL) based systems ( have been studied in the literature.", "labels": [], "entities": []}, {"text": "Comparing to chit-chat dialogue models that are usually trained offline using single-turn context-response pairs, task-oriented dialogue model involves reasoning and planning over multiple dialogue turns.", "labels": [], "entities": []}, {"text": "This makes it especially important fora system to be able to learn from users in an interactive manner.", "labels": [], "entities": []}, {"text": "Comparing to SL models, systems trained with RL by receiving feedback during users interactions showed improved model robustness against diverse dialogue scenarios.", "labels": [], "entities": []}, {"text": "A critical step in learning RL based taskoriented dialogue models is dialogue policy learning.", "labels": [], "entities": [{"text": "RL based taskoriented dialogue", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.8369326144456863}, {"text": "dialogue policy learning", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.8537558714548746}]}, {"text": "Training dialogue policy online from scratch typically requires a large number of interactive learning sessions before an agent can reach a satisfactory performance level.", "labels": [], "entities": [{"text": "dialogue policy online", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.7729914585749308}]}, {"text": "Recent works) explored pre-training the dialogue model using human-human or human-machine dialogue corpora before performing interactive learning with RL to address this concern.", "labels": [], "entities": []}, {"text": "A potential drawback with such pre-training approach is that the model may suffer from the mismatch of dialogue state distributions between supervised training and interactive learning stages.", "labels": [], "entities": []}, {"text": "While interacting with users, the agent's response at each turn has a direct influence on the distribution of dialogue state that the agent will operate on in the upcoming dialogue turns.", "labels": [], "entities": []}, {"text": "If the agent makes a small mistake and reaches an unfamiliar state, it may not know how to recover from it and get back to a normal dialogue trajectory.", "labels": [], "entities": []}, {"text": "This is because such recovery situation maybe rare for good human agents and thus are not well covered in the supervised training corpus.", "labels": [], "entities": []}, {"text": "This will result in compounding errors in a dialogue which may lead to failure of a task.", "labels": [], "entities": []}, {"text": "RL exploration might finally help to find corresponding actions to recover from a bad state, but the search process can be very inefficient.", "labels": [], "entities": [{"text": "RL exploration", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.942171573638916}]}, {"text": "To ameliorate the effect of dialogue state distribution mismatch between offline training and RL interactive learning, we propose a hybrid imitation and reinforcement learning method.", "labels": [], "entities": [{"text": "RL interactive learning", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.8818144003550211}]}, {"text": "We first let the agent to interact with users using its own policy learned from supervised pre-training.", "labels": [], "entities": []}, {"text": "When an agent makes a mistake, we ask users to correct the mistake by demonstrating the agent the right actions to take at each turn.", "labels": [], "entities": []}, {"text": "This user corrected dialogue sample, which is guided by the agent's own policy, is then added to the existing training corpus.", "labels": [], "entities": []}, {"text": "We fine-tune the dialogue policy with this dialogue sample aggregation) and continue such user teaching process fora number of cycles.", "labels": [], "entities": []}, {"text": "Since asking for user teaching at each dialogue turn is costly, we want to reduce this user teaching cycles as much as possible and continue the learning process with RL by collecting simple forms of user feedback (e.g. a binary feedback, positive or negative) only at the end of a dialogue.", "labels": [], "entities": []}, {"text": "Our main contributions in this work are: \u2022 We design a neural network based taskoriented dialogue system which can be optimized end-to-end for natural language understanding, dialogue state tracking, and dialogue policy learning.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 143, "end_pos": 173, "type": "TASK", "confidence": 0.6390833556652069}, {"text": "dialogue state tracking", "start_pos": 175, "end_pos": 198, "type": "TASK", "confidence": 0.7522456844647726}, {"text": "dialogue policy learning", "start_pos": 204, "end_pos": 228, "type": "TASK", "confidence": 0.8600659767786661}]}, {"text": "\u2022 We propose a hybrid imitation and reinforcement learning method for end-to-end model training in addressing the challenge with dialogue state distribution mismatch between offline training and interactive learning.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we discuss related work in building end-to-end task-oriented dialogue systems.", "labels": [], "entities": []}, {"text": "In section 3, we describe the proposed model and learning method in detail.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the experiment setup and discuss the results.", "labels": [], "entities": []}, {"text": "Section 5 gives the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed method on DSTC2 () dataset in restaurant search domain and an internally collected dialogue corpus 1 in movie booking domain.", "labels": [], "entities": [{"text": "DSTC2 () dataset", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.8828317324320475}]}, {"text": "The movie booking dialogue corpus has an average number of 8.4 turns per dialogue.", "labels": [], "entities": [{"text": "movie booking dialogue corpus", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.571543961763382}]}, {"text": "Its training set has 100K dialogues, and the development set and test set each has 10K dialogues.", "labels": [], "entities": []}, {"text": "The movie booking dialogue corpus is generated () using a finite state machine based dialogue agent and an agenda based user simulator () with natural language utterances rewritten by real users.", "labels": [], "entities": [{"text": "movie booking dialogue", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7277534604072571}]}, {"text": "The user simulator can be configured with different personalities, showing various levels of randomness and cooperativeness.", "labels": [], "entities": []}, {"text": "This user simulator is also used to interact with our end-to-end training agent during imitation and reinforcement learning stages.", "labels": [], "entities": []}, {"text": "We randomly select a user profile when conducting each dialogue simulation.", "labels": [], "entities": []}, {"text": "During model evaluation, we use an extended set of natural language surface forms over the ones used during training time to evaluate the generalization capability of the proposed end-to-end model in handling diverse natural language inputs.", "labels": [], "entities": []}, {"text": "We further evaluate the proposed method with human judges recruited via Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.9625743627548218}]}, {"text": "Each judge is asked to read a dialogue between our model and user simulator and rate each system turn on a scale of 1 (frustrating) to 5 (optimal way to help the user).", "labels": [], "entities": []}, {"text": "Each turn is rated by 3 different judges.", "labels": [], "entities": []}, {"text": "We collect and rate 100 dialogues for each of the three models: (i) SL model, (ii) SL model followed by 1000 episodes of IL, (iii) SL and IL followed by RL.", "labels": [], "entities": [{"text": "RL", "start_pos": 153, "end_pos": 155, "type": "METRIC", "confidence": 0.8218154311180115}]}, {"text": "lists the mean and standard deviation of human scores overall system turns.", "labels": [], "entities": []}, {"text": "Performing interactive learning with imitation and reinforcement learning clearly improves the quality of the model according to human judges.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dialogue state tracking results on DSTC2", "labels": [], "entities": [{"text": "Dialogue state tracking", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7197884519894918}, {"text": "DSTC2", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.6639542579650879}]}, {"text": " Table 2: DST results on movie booking dataset", "labels": [], "entities": [{"text": "DST", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9240497946739197}, {"text": "movie booking dataset", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.7132176160812378}]}]}