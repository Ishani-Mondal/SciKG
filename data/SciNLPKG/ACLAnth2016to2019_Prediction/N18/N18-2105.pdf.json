{"title": [{"text": "Unsupervised Keyphrase Extraction with Multipartite Graphs", "labels": [], "entities": [{"text": "Unsupervised Keyphrase Extraction", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5432604352633158}]}], "abstractContent": [{"text": "We propose an unsupervised keyphrase extraction model that encodes topical information within a multipartite graph structure.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7275757491588593}]}, {"text": "Our model represents keyphrase candidates and topics in a single graph and exploits their mutually reinforcing relationship to improve candidate ranking.", "labels": [], "entities": []}, {"text": "We further introduce a novel mechanism to incorporate keyphrase selection preferences into the model.", "labels": [], "entities": []}, {"text": "Experiments conducted on three widely used datasets show significant improvements over state-of-the-art graph-based models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have witnessed a resurgence of interest in automatic keyphrase extraction, and a number of diverse approaches were explored in the literature ().", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7535665929317474}]}, {"text": "Among them, graph-based approaches are appealing in that they offer strong performance while remaining completely unsupervised.", "labels": [], "entities": []}, {"text": "These approaches typically involve two steps: 1) building a graph representation of the document where nodes are lexical units (usually words) and edges are semantic relations between them; 2) ranking nodes using a graph-theoretic measure, from which the top-ranked ones are used to form keyphrases.", "labels": [], "entities": []}, {"text": "Since the seminal work of Mihalcea and Tarau, researchers have devoted a substantial amount of effort to develop better ways of modelling documents as graphs.", "labels": [], "entities": []}, {"text": "Most if not all previous work, however, focus on either measuring the semantic relatedness between nodes () or devising node ranking functions.", "labels": [], "entities": []}, {"text": "So far, little attention has been paid to the use of different types of graphs.", "labels": [], "entities": []}, {"text": "Yet, a key challenge in keyphrase extraction is to ensure topical coverage and diversity, which are not naturally handled by graph-ofwords representations.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8075096607208252}]}, {"text": "Most attempts at using topic information in graph-based approaches involve biasing the ranking function towards topic distributions (.", "labels": [], "entities": []}, {"text": "Unfortunately, these models suffer from several limitations: they aggregate multiple topic-biased rankings which makes their time complexity prohibitive for long documents 1 , they require a large dataset to estimate word-topic distributions that is not always available or easy to obtain, and they assume that topics are independent of one another, making it hard to ensure topic diversity.", "labels": [], "entities": []}, {"text": "For the latter case, supervised approaches were proposed to optimize the broad coverage of topics (.", "labels": [], "entities": []}, {"text": "Another strand of work models documents as graphs of topics and selects keyphrases from the top-ranked ones.", "labels": [], "entities": []}, {"text": "This higher level representation (see), in which topic relations are measured as the semantic relatedness between the keyphrase candidates they instantiate, was shown to improve the overall ranking and maximize topic coverage.", "labels": [], "entities": []}, {"text": "The downside is that candidates belonging to a single topic are viewed as equally important, so that post-ranking heuristics are required to select the most representative keyphrase from each topic.", "labels": [], "entities": []}, {"text": "Also, errors in forming topics propagate throughout the model severely impacting its performance.", "labels": [], "entities": []}, {"text": "Here, we build upon this latter line of work and propose a model that implicitly enforces topical diversity while ranking keyphrase candidates in a Inverse problems fora mathematical model of ion exchange in a compressible ion exchanger A mathematical model of ion exchange is considered, allowing for ion exchanger compression in the process of ion exchange . Two inverse problems are investigated for this model , unique solvability is proved, and numerical solution methods are proposed.", "labels": [], "entities": [{"text": "ion exchanger compression", "start_pos": 302, "end_pos": 327, "type": "TASK", "confidence": 0.6274669667085012}]}, {"text": "The efficiency of the proposed methods is demonstrated by a numerical experiment . Topic Topic Topic Topic Topic Topic Topic Topic (a) TopicRank graph.", "labels": [], "entities": [{"text": "Topic Topic Topic Topic Topic Topic Topic Topic", "start_pos": 83, "end_pos": 130, "type": "TASK", "confidence": 0.7860155254602432}]}, {"text": "To do this, we use a particular graph structure, called multipartite graph, to represent documents as tightly connected sets of topic related candidates (see).", "labels": [], "entities": []}, {"text": "This representation allows for the seamless integration of any topic decomposition, and enables the ranking algorithm to make full use of the mutually reinforcing relation between topics and candidates.", "labels": [], "entities": []}, {"text": "Another contribution of this work is a mechanism to incorporate intra-topic keyphrase selection preferences into the model.", "labels": [], "entities": []}, {"text": "It allows the ranking algorithm to go beyond semantic relatedness by leveraging information from additional salience features.", "labels": [], "entities": []}, {"text": "Technically, keyphrase candidates that exhibit certain properties, e.g. that match a thesaurus entry or occur in specific parts of the document, are promoted in ranking through edge weight adjustments.", "labels": [], "entities": []}, {"text": "Here, we show the effectiveness of this mechanism by introducing a bias towards keyphrase candidates occurring first in the document.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carryout our experiments on three datasets: SemEval-2010 (, which is composed of scientific articles collected from the ACM Digital Library.", "labels": [], "entities": [{"text": "ACM Digital Library", "start_pos": 123, "end_pos": 142, "type": "DATASET", "confidence": 0.9602526227633158}]}, {"text": "We use the set of combined author-and reader-assigned keyphrases as reference keyphrases.", "labels": [], "entities": []}, {"text": "Model Hulth-2003, which is made of paper abstracts about computer science and information technology.", "labels": [], "entities": [{"text": "Model Hulth-2003", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8329998850822449}]}, {"text": "Reference keyphrases were assigned by professional indexers.", "labels": [], "entities": []}, {"text": "Marujo-2012 () that contains news articles distributed over 10 categories (e.g. Politics, Sports).", "labels": [], "entities": [{"text": "Marujo-2012", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9720562100410461}]}, {"text": "Reference keyphrases were assigned by readers via crowdsourcing.", "labels": [], "entities": []}, {"text": "We follow the common practice and evaluate the performance of our model in terms of fmeasure (F 1 ) at the top N keyphrases, and apply stemming to reduce the number of mismatches.", "labels": [], "entities": [{"text": "fmeasure (F 1 )", "start_pos": 84, "end_pos": 99, "type": "METRIC", "confidence": 0.8857073426246643}]}, {"text": "We also report the Mean Average Precision (MAP) scores of the ranked lists of keyphrases.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP) scores", "start_pos": 19, "end_pos": 54, "type": "METRIC", "confidence": 0.9698490330151149}]}], "tableCaptions": [{"text": " Table 1: F 1 -scores computed at the top 5, 10 extracted keyphrases and Mean Average Precision (MAP) scores.  \u2020  indicate significance at the 0.05 level using Student's t-test.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9813530296087265}, {"text": "Mean Average Precision (MAP) scores", "start_pos": 73, "end_pos": 108, "type": "METRIC", "confidence": 0.9749441657747541}, {"text": "Student's t-test", "start_pos": 160, "end_pos": 176, "type": "DATASET", "confidence": 0.833243211110433}]}]}