{"title": [{"text": "Quality Estimation for Automatically Generated Titles of eCommerce Browse Pages", "labels": [], "entities": []}], "abstractContent": [{"text": "At eBay, we are automatically generating a large amount of natural language titles for eCommerce browse pages using machine translation (MT) technology.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.8346200227737427}]}, {"text": "While automatic approaches can generate millions of titles very fast, they are prone to errors.", "labels": [], "entities": []}, {"text": "We therefore develop quality estimation (QE) methods which can automatically detect titles with low quality in order to prevent them from going live.", "labels": [], "entities": []}, {"text": "In this paper, we present different approaches: The first one is a Random Forest (RF) model that explores hand-crafted, robust features, which area mix of established features commonly used in Machine Translation Quality Estimation (MTQE) and new features developed specifically for our task.", "labels": [], "entities": [{"text": "Machine Translation Quality Estimation (MTQE)", "start_pos": 193, "end_pos": 238, "type": "TASK", "confidence": 0.8488546354430062}]}, {"text": "The second model is based on Siamese Networks (SNs) which embed the metadata input sequence and the generated title in the same space and do not require hand-crafted features at all.", "labels": [], "entities": []}, {"text": "We thoroughly evaluate and compare those approaches on in-house data.", "labels": [], "entities": []}, {"text": "While the RF models are competitive for scenarios with smaller amounts of training data and somewhat more robust, they are clearly outperformed by the SN models when the amount of training data is larger.", "labels": [], "entities": []}], "introductionContent": [{"text": "On eCommerce sites, multiple items can be grouped on a common page called browse page (BP).", "labels": [], "entities": []}, {"text": "Each browse page contains an overview of various items which share some, but not necessarily all characteristics.", "labels": [], "entities": []}, {"text": "The characteristics can be expressed as slot/value pairs.", "labels": [], "entities": []}, {"text": "shows an example of a browse page with a title, with navigation elements leading to related browse pages as well as the individual items listed on this page.", "labels": [], "entities": []}, {"text": "The browse pages are linked among each other and can be organized in a hierarchy.", "labels": [], "entities": []}, {"text": "This structure  allows users to navigate laterally between different browse pages, or to dive deeper and refine their search.", "labels": [], "entities": []}, {"text": "The example browse page in shows different white ACME smartphones with capacity 32GB.", "labels": [], "entities": []}, {"text": "This page is linked from various browse pages, e.g. those for white ACME Smartphones, for ACME smartphones with 32GB, or for white smartphones with 32GB.", "labels": [], "entities": [{"text": "ACME Smartphones", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.8654329180717468}]}, {"text": "It also links to browse pages with a higher number of slots, i.e. refining the set of listed items by additional features like network provider.", "labels": [], "entities": []}, {"text": "Different combinations of characteristics bijectively correspond to different browse pages, and consequently to different browse page titles.", "labels": [], "entities": []}, {"text": "To show customers which items are grouped on a browse page, we need a human-readable description of the content of that particular page.", "labels": [], "entities": []}, {"text": "Large eCommerce sites can easily have tens of millions of such browse pages in many different languages.", "labels": [], "entities": []}, {"text": "Each browse page has one to six slots to be realized.", "labels": [], "entities": []}, {"text": "The number of unique slot-value pairs are in the order of hundreds of thousands.", "labels": [], "entities": []}, {"text": "All these factors render the task of human creation of browse page titles infeasible.", "labels": [], "entities": []}, {"text": "We have therefore developed several strategies to generate these human-readable titles automatically for any possible browse page.", "labels": [], "entities": []}, {"text": "These strategies are based on MT technology and take the slot/value pairs mentioned in Section 1 as input.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9753109812736511}]}, {"text": "Examples of such slot/value pairs are the category to which the products belong, and characteristics like brand, color, size, storage capacity, which are dependant on the category.", "labels": [], "entities": []}, {"text": "The slot/value pairs for the browse page from are shown in  These metadata are fed into an MT system and translated into natural language.", "labels": [], "entities": []}, {"text": "We have developed three different MT-based systems, which are tailored towards different amounts of training data available across languages.", "labels": [], "entities": [{"text": "MT-based", "start_pos": 34, "end_pos": 42, "type": "TASK", "confidence": 0.9648492932319641}]}, {"text": "These systems are shortly described in Section 4.", "labels": [], "entities": []}, {"text": "In this paper, we compare our QE methods on output from different MT systems on English titles.", "labels": [], "entities": [{"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9577746391296387}]}], "datasetContent": [{"text": "We constantly carryout human evaluation of title quality.", "labels": [], "entities": []}, {"text": "From these evaluations, we have two test sets with approximately 500 browse pages each, called test1 and test2.", "labels": [], "entities": []}, {"text": "For those browse pages, we have automatically generated titles from three different systems along with manual assessment of title quality.", "labels": [], "entities": []}, {"text": "The three different title generation systems are described in detail in.", "labels": [], "entities": [{"text": "title generation", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7232871502637863}]}, {"text": "In short, they are: \u2022 a strictly rule-based approach with a manually created grammar.", "labels": [], "entities": []}, {"text": "This is especially useful when the amount of human-curated training data is limited.", "labels": [], "entities": []}, {"text": "\u2022 a hybrid generation approach which combines rule-based language generation and statistical MT techniques for situations in which monolingual data for the language is available, but human-curated titles are not.", "labels": [], "entities": [{"text": "rule-based language generation", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.6146255731582642}, {"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9119895100593567}]}, {"text": "\u2022 an Automatic Post-Editing (APE) system which first generates titles with the rulebased approach, and then uses statistical MT techniques for automatically correcting the errors made by the rule-based approach.", "labels": [], "entities": [{"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9016056060791016}]}, {"text": "See for the amount of data and title quality across these different test sets and system outputs.", "labels": [], "entities": []}, {"text": "Apart from the APE system, the class distribution is similar for all sets, and also similar to the distribution on the training data.", "labels": [], "entities": []}, {"text": "The APE system was significantly improved between these two evaluation rounds, leading to a much higher percentage of 'OK' labels on test2.", "labels": [], "entities": [{"text": "APE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9755209684371948}, {"text": "percentage of 'OK' labels", "start_pos": 104, "end_pos": 129, "type": "METRIC", "confidence": 0.6990077694257101}]}, {"text": "The hybrid system was manually evaluated only on test2.", "labels": [], "entities": []}, {"text": "We are constantly improving the system for BP title generation and have implemented different approaches.", "labels": [], "entities": [{"text": "BP title generation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7281089226404825}]}, {"text": "It is therefore important that the QE models work equally well for output from different title generation systems, i.e. they should not be heavily tailored to one specific system.", "labels": [], "entities": []}, {"text": "We evaluated the QE models per evaluation set (test1 and test2) and per title generation system.", "labels": [], "entities": []}, {"text": "The QE performance per system output is shown in, with notable difference in F1-score and Matthew's correlation across the five different sets.", "labels": [], "entities": [{"text": "QE", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9266068339347839}, {"text": "F1-score", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9995625615119934}, {"text": "Matthew's correlation", "start_pos": 90, "end_pos": 111, "type": "METRIC", "confidence": 0.8261670271555582}]}, {"text": "The SN models perform best on the titles from the rule-based generation system, i.e. when training and test titles are similar -with F1-scores around 0.8 and Matthew's correlation in the high 50s.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9994450211524963}, {"text": "Matthew's correlation", "start_pos": 158, "end_pos": 179, "type": "METRIC", "confidence": 0.890876849492391}]}, {"text": "The worst classification performance is achieved for the APE titles on test1, which is the set with the lowest title quality (see).", "labels": [], "entities": [{"text": "APE titles on test1", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.7771401554346085}]}, {"text": "This is also the only set on which the RF models outperform the SN models.", "labels": [], "entities": []}, {"text": "The RF models were trained with class weights adjusted inversely proportional to class frequencies in the training data, making them more robust w.r.t. the differences between training and test data.", "labels": [], "entities": []}, {"text": "The neural network model does not have the same class imbalance treatment, which makes the model biased towards most frequent classes in training data sets in which the imbalance is high (e.g. the rule-based system).", "labels": [], "entities": []}, {"text": "In future work, we plan to apply the same balancing to SN training.", "labels": [], "entities": [{"text": "SN training", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9093347787857056}]}, {"text": "This setting could potentially improve the SN performance.", "labels": [], "entities": [{"text": "SN", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9844895601272583}]}], "tableCaptions": [{"text": " Table 3: Training data statistics.", "labels": [], "entities": [{"text": "Training data", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.6847979873418808}]}, {"text": " Table 4: Evaluation data statistics. Numbers are %  Good / % Bad in the three rightmost columns.", "labels": [], "entities": []}, {"text": " Table 5: F1-scores and Matthew's correlation (MC)  for different QE models. Training on train1, evaluation  on test1+2. Best results in bold.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9986107349395752}, {"text": "Matthew's correlation (MC)", "start_pos": 24, "end_pos": 50, "type": "METRIC", "confidence": 0.9616079529126486}]}, {"text": " Table 6: QE performance for different amounts of  training data. Evaluation on test1+2. RF with all  features. SN with word2vec embeddings. Best results  in bold.", "labels": [], "entities": [{"text": "QE", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9150986671447754}, {"text": "RF", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.8299857378005981}]}, {"text": " Table 7: QE performance per title generation system. RF with all features. SN with word2vec embeddings. Best  results marked in bold. RB is rule-based.", "labels": [], "entities": [{"text": "RF", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.8740721940994263}, {"text": "RB", "start_pos": 135, "end_pos": 137, "type": "METRIC", "confidence": 0.9933122992515564}]}]}