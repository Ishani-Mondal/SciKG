{"title": [{"text": "On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.6532138387362162}]}], "abstractContent": [{"text": "We propose a process for investigating the extent to which sentence representations arising from neural machine translation (NMT) systems encode distinct semantic phenomena.", "labels": [], "entities": [{"text": "sentence representations arising from neural machine translation (NMT) systems encode distinct semantic phenomena", "start_pos": 59, "end_pos": 172, "type": "TASK", "confidence": 0.8112603982289632}]}, {"text": "We use these representations as features to train a natural language inference (NLI) classifier based on datasets recast from existing semantic annotations.", "labels": [], "entities": []}, {"text": "In applying this process to a representative NMT system, we find its enco-der appears most suited to supporting inferences at the syntax-semantics interface, as compared to anaphora resolution requiring world-knowledge.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.8026953339576721}]}, {"text": "We conclude with a discussion on the merits and potential deficiencies of the existing process, and how it maybe improved and extended as a broader framework for evaluating semantic coverage.", "labels": [], "entities": []}], "introductionContent": [{"text": "What do neural machine translation (NMT) models learn about semantics?", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.8400331139564514}]}, {"text": "Many researchers suggest that state-of-the-art NMT models learn representations that capture the meaning of sentences (.", "labels": [], "entities": []}, {"text": "However, there is limited understanding of how specific semantic phenomena are captured in NMT representations beyond this broad notion.", "labels": [], "entities": []}, {"text": "For instance, how well do these representations capture's thematic proto-roles?", "labels": [], "entities": []}, {"text": "Are these representations sufficient for understanding paraphrastic inference?", "labels": [], "entities": [{"text": "understanding paraphrastic inference", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.6775475243727366}]}, {"text": "Do the sentence representations encompass complex anaphora resolution?", "labels": [], "entities": [{"text": "complex anaphora resolution", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.7601953744888306}]}, {"text": "We argue that existing semantic annotations recast as Natural Language Inference (NLI) can be leveraged to investigate whether sentence representations encoded by NMT models capture these semantic phenomena.", "labels": [], "entities": []}, {"text": "Sara adopted Jill, she wanted a child DPR Sara adopted Jill, Jill wanted a child We use sentence representations from pretrained NMT encoders as features to train classifiers for NLI, the task of determining if one sentence (a hypothesis) is supported by another (a context).", "labels": [], "entities": []}, {"text": "If the sentence representations learned by NMT models capture distinct semantic phenomena, we hypothesize that those representations should be sufficient to perform well on NLI datasets that test a model's ability to capture these phenomena.", "labels": [], "entities": []}, {"text": "shows example NLI sentence pairs with their respective labels and semantic phenomena.", "labels": [], "entities": []}, {"text": "We evaluate NMT sentence representations of 4 NMT models from 2 domains on 4 different NLI datasets to investigate how well they capture different semantic phenomena.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.831110805273056}]}, {"text": "We use's Unified Semantic Evaluation Framework (USEF) that recasts three semantic phenomena NLI: 1) semantic proto-roles, 2) paraphrastic inference, 3) and complex anaphora resolution.", "labels": [], "entities": [{"text": "complex anaphora resolution", "start_pos": 156, "end_pos": 183, "type": "TASK", "confidence": 0.7099104523658752}]}, {"text": "Additionally, we evaluate the NMT sentence representations on 4) Multi-NLI, a recent extension of the Stanford Natural Language Inference dataset (SNLI)) that includes multiple genres and domains.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference dataset (SNLI))", "start_pos": 102, "end_pos": 153, "type": "DATASET", "confidence": 0.7150352261960506}]}, {"text": "We contextualize our results with a standard neural encoder described in and used in.", "labels": [], "entities": []}, {"text": "Based on the recast NLI datasets, our investigation suggests that NMT encoders might learn more about semantic proto-roles than anaphora resolution or paraphrastic inference.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.6503774970769882}, {"text": "anaphora resolution", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7672621607780457}]}, {"text": "We note that the target-side language affects how an NMT sourceside encoder captures these semantic phenomena.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use 4-layer NMT systems with 500-dimensional word embeddings and LSTM states (i.e., d = 500).", "labels": [], "entities": []}, {"text": "The vocabulary size is 75K words.", "labels": [], "entities": []}, {"text": "We train NMT models until convergence and take the models that performed best on the development set for generating representations to feed into the entailment classifier.", "labels": [], "entities": []}, {"text": "We use the hidden states from the top encoding layer for obtaining sentence representations since it has been hypothesized that higher layers focus on word meaning, as opposed to syntax.", "labels": [], "entities": []}, {"text": "We remove long sentences (> 50 words) when training both the classifier and the NMT model, as is common NMT practice ( We train English\u2192Arabic/Spanish/Chinese NMT models on the first 2 million sentences of the United Nations parallel corpus training set (, and the English\u2192German model on the WMT data-set ().", "labels": [], "entities": [{"text": "United Nations parallel corpus training set", "start_pos": 210, "end_pos": 253, "type": "DATASET", "confidence": 0.7852126061916351}, {"text": "WMT data-set", "start_pos": 293, "end_pos": 305, "type": "DATASET", "confidence": 0.9777760207653046}]}, {"text": "We use the official training/development/test splits.", "labels": [], "entities": []}, {"text": "In our NLI experiments, we do not train on Multi-NLI and test on the recast datasets, or viceversa, since Multi-NLI since Multi-NLI uses a 3-way classification (entailment, neutral, and contradictions) while the recast datasets use just two labels (entailed and not-entailed).", "labels": [], "entities": []}, {"text": "In preliminary experiments, we also used a 3-layered MLP.", "labels": [], "entities": [{"text": "MLP", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.6711989045143127}]}, {"text": "Although the results slightly improved, we noted similar trends to the linear classifier.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy on NLI with representations generated by encoders of English\u2192{ar,es,zh,de} NMT models.  Rows correspond to the training and validation sets and major columns correspond to the test set. The column  labeled \"USEF\" refers to the test accuracies reported in White et al. (2017). The numbers on the top row represents  each dataset's majority baseline. Bold numbers indicate the highest performing model for the given dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9343666434288025}, {"text": "USEF", "start_pos": 226, "end_pos": 230, "type": "DATASET", "confidence": 0.8286252021789551}]}, {"text": " Table 3: Accuracies on FN+'s dev set based on whether  the swapped paraphrases share the same POS tag.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9876596331596375}, {"text": "FN+'s dev set", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.858441424369812}]}, {"text": " Table 4: Accuracies on the SPR test set broken down  by each proto-role. \"avg\" represents the score for the  proto-role averaged across target languages. Bold and", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.985181987285614}, {"text": "SPR", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9393944144248962}]}, {"text": " Table 5: Accuracies for MNLI test sets. MNLI-1 refers  to the matched case and MNLI-2 is the mismatched.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9957691431045532}, {"text": "MNLI test sets", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.7920681834220886}, {"text": "MNLI-1", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.7547963857650757}]}, {"text": " Table 6: NLI results on fine-grained semantic pheno- mena. FN+ = paraphrases; DPR = pronoun resolution;  SPRL = proto-roles. NMT representations are com- bined with either a simple concatenation (results co- pied from Table 2) or the InferSent representation.  State-of-the-art (SOTA) is from White et al. (2017).", "labels": [], "entities": []}, {"text": " Table 7: Results on language inference on MultiN- LI (Williams et al., 2017), matched/mismatched sce- nario (MNLI1/2).", "labels": [], "entities": [{"text": "language inference", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7405659854412079}]}]}