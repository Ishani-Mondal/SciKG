{"title": [{"text": "Discriminating between Lexico-Semantic Relations with the Specialization Tensor Model", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a simple and effective feed-forward neural architecture for discriminating between lexico-semantic relations (synonymy, antonymy, hypernymy, and meronymy).", "labels": [], "entities": []}, {"text": "Our Specialization Tensor Model (STM) simultaneously produces multiple different special-izations of input distributional word vectors, tailored for predicting lexico-semantic relations for word pairs.", "labels": [], "entities": []}, {"text": "STM outperforms more complex state-of-the-art architectures on two benchmark datasets and exhibits stable performance across languages.", "labels": [], "entities": [{"text": "STM", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.795108437538147}]}, {"text": "We also show that, if coupled with a lingual distributional space, the proposed model can transfer the prediction of lexico-semantic relations to a resource-lean target language without any training data.", "labels": [], "entities": [{"text": "prediction of lexico-semantic relations", "start_pos": 103, "end_pos": 142, "type": "TASK", "confidence": 0.824485257267952}]}], "introductionContent": [{"text": "Distributional vector spaces (i.e., word embeddings) ( are ubiquitous in modern natural language processing (NLP).", "labels": [], "entities": []}, {"text": "While such vector spaces capture general semantic relatedness, their well-known limitation is the inability to indicate the exact nature of the semantic relation that holds between words.", "labels": [], "entities": []}, {"text": "Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (, natural language inference (), text simplification (Glava\u0161 and\u0160tajner and\u02c7and\u0160tajner, 2015), and paraphrase generation, to name a few.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.8626058399677277}, {"text": "text simplification", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.7652835547924042}, {"text": "paraphrase generation", "start_pos": 229, "end_pos": 250, "type": "TASK", "confidence": 0.9607691168785095}]}, {"text": "This is why numerous methods have been proposed that either (1) specialize distributional vectors to better reflect a particular relation (most commonly synonymy) or (2) train supervised relation classifiers using lexico-semantic relations (i.e., labeled word pairs) from external resources such as WordNet) as training data (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 299, "end_pos": 306, "type": "DATASET", "confidence": 0.9554572701454163}]}, {"text": "We present the Specialization Tensor Model (STM), a simple and effective feedforward neural model for discriminating between (arguably) most prominent lexico-semantic relations -synonymy, antonymy, hypernymy, and meronymy.", "labels": [], "entities": []}, {"text": "The STM architecture is based on the hypothesis that different specializations of input distributional vectors are needed for predicting different lexico-semantic relations.", "labels": [], "entities": [{"text": "STM", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9635324478149414}]}, {"text": "Our results show that, despite its simplicity, STM outperforms more complex models on the benchmarking CogALex-V dataset (.", "labels": [], "entities": [{"text": "STM", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9341569542884827}, {"text": "CogALex-V dataset", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.8373290598392487}]}, {"text": "Further, it exhibits stable performance across languages.", "labels": [], "entities": []}, {"text": "Finally, we show that, when coupled with a method for inducing a multilingual distributional space (, inter alia), STM can predict lexico-semantic relations also for languages with no training data available from external linguistic resources.", "labels": [], "entities": [{"text": "STM", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9732781052589417}]}, {"text": "While in this work we use STM to discriminate between four prominent lexico-semantic relations, it can, at least conceptually, be trained to predict over an arbitrary set of lexico-semantic relations, provided the availability of respective training data.", "labels": [], "entities": [{"text": "STM", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9494448304176331}]}], "datasetContent": [{"text": "We first describe the evaluation setup (datasets, baselines, and model optimization) and then show STM's performance on a benchmarking relation classification dataset ().", "labels": [], "entities": [{"text": "STM", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9323487281799316}]}, {"text": "Finally, we report how STM performs for different languages and in the language transfer setting.", "labels": [], "entities": [{"text": "STM", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9840419292449951}, {"text": "language transfer", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7135155498981476}]}, {"text": "We use the CogALex-V dataset from the shared task on corpus-based identification of semantic relations (.", "labels": [], "entities": [{"text": "CogALex-V dataset", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.8987816572189331}, {"text": "corpus-based identification of semantic relations", "start_pos": 53, "end_pos": 102, "type": "TASK", "confidence": 0.7327542901039124}]}, {"text": "Its train and test portions contain 3,054 and 4,260 word pairs, respectively, covering four relations (synonymy: 5.4%; antonymy: 8.8%; hypernymy: 8.6%; and meronymy: 6.1%) and randomly paired words (71.1%).", "labels": [], "entities": []}, {"text": "CogALex-V is severely skewed in favor of random word pairs and its training portion is very limited in size.", "labels": [], "entities": []}, {"text": "Nonetheless to the best of our knowledge, it is the only publicly available dataset for multi-class classification of lexico-semantic relations on which other models have been comparatively evaluated (.", "labels": [], "entities": [{"text": "multi-class classification of lexico-semantic relations", "start_pos": 88, "end_pos": 143, "type": "TASK", "confidence": 0.8005933284759521}]}, {"text": "Besides the skewed class distribution and the limited size, CogALex-V also suffers from lexical repetitiveness.", "labels": [], "entities": []}, {"text": "We have thus created an additional larger and more balanced dataset by randomly sampling triples from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 102, "end_pos": 109, "type": "DATASET", "confidence": 0.9826585054397583}]}, {"text": "This dataset, termed WN-LS, contains 10,000 word pairs (approximately 2,000 pairs for each of the four lexico-semantic relations and 2,000 randomly created pairs), split by 8:2 train-to-test ratio.", "labels": [], "entities": []}, {"text": "To support the multilingual analysis, we semiautomatically translated the whole English (EN) WN-LS dataset into German (DE) and Spanish (ES).", "labels": [], "entities": [{"text": "English (EN) WN-LS dataset", "start_pos": 80, "end_pos": 106, "type": "DATASET", "confidence": 0.6010334293047587}]}, {"text": "We additionally translated the test portion of WN-LS to Croatian (HR), as an example of a resource-lean language.", "labels": [], "entities": []}, {"text": "We compare STM against two baseline models.", "labels": [], "entities": [{"text": "STM", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.8976306319236755}]}, {"text": "The first baseline (CONCAT) feeds the concatenation of the distributional embeddings to a feed-forward classifier with a single hidden layer: The second baseline, named BILIN-TENS is an STM reduction in which we directly forward the input vectors into the bilinear product tensor W Optimization.", "labels": [], "entities": [{"text": "CONCAT", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9020816683769226}, {"text": "BILIN-TENS", "start_pos": 169, "end_pos": 179, "type": "METRIC", "confidence": 0.9958826303482056}]}, {"text": "We learn the STM's parameters using the Adam algorithm (, with initial learning rate set to 0.0001.", "labels": [], "entities": [{"text": "STM", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8975732326507568}]}, {"text": "We train in mini-batches of size Nb = 50 and apply dropout with the retaining probability of 0.5 to all model layers.", "labels": [], "entities": [{"text": "retaining probability", "start_pos": 68, "end_pos": 89, "type": "METRIC", "confidence": 0.9564139544963837}]}, {"text": "In all experiments, we find the optimal hyperparameters (the number of specialization tensor slices K, the size of the specialized vectors n, and the regularization factor \u03bb) via grid search within the 5-fold cross-validation on the training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on the CogALex-V dataset.", "labels": [], "entities": [{"text": "CogALex-V dataset", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.9417865574359894}]}, {"text": " Table 2: STM performance for three languages on (re- spective translations of) the WN-LS dataset.", "labels": [], "entities": [{"text": "STM", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.977399468421936}, {"text": "WN-LS dataset", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9062509536743164}]}, {"text": " Table 3: Zero-shot cross-lingual transfer. Best perfor- mance for each test set is shown in bold.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.6942211985588074}]}, {"text": " Table 4: Language transfer results on the HR WN-LS.  Training on combinations of EN, ES, and DE data.", "labels": [], "entities": [{"text": "Language transfer", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7426819503307343}, {"text": "HR WN-LS", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.783332109451294}]}]}