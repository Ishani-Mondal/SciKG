{"title": [{"text": "Demand-Weighted Completeness Prediction fora Knowledge Base", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we introduce the notion of Demand-Weighted Completeness, allowing estimation of the completeness of a knowledge base with respect to how it is used.", "labels": [], "entities": []}, {"text": "Defining an entity by its classes, we employ usage data to predict the distribution over relations for that entity.", "labels": [], "entities": []}, {"text": "For example, instances of person in a knowledge base may require a birth date, name and nationality to be considered complete.", "labels": [], "entities": []}, {"text": "These predicted relation distributions enable detection of important gaps in the knowledge base, and define the required facts for unseen entities.", "labels": [], "entities": []}, {"text": "Such characterisation of the knowledge base can also quantify how usage and completeness changeover time.", "labels": [], "entities": []}, {"text": "We demonstrate a method to measure Demand-Weighted Completeness, and show that a simple neural network model performs well at this prediction task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge Bases (KBs) are widely used for representing information in a structured format.", "labels": [], "entities": []}, {"text": "Such KBs, including), Google Knowledge Vault (), and YAGO (, often store information as facts in the form of triples, consisting of two entities and a relation between them.", "labels": [], "entities": [{"text": "YAGO", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.6249221563339233}]}, {"text": "KBs have many applications in fields such as machine translation, information retrieval and question answering.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8160938918590546}, {"text": "information retrieval", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.8361105620861053}, {"text": "question answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.9345934987068176}]}, {"text": "When considering a KB's suitability fora task, primary considerations are the number of facts it contains (, and the precision of those facts.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9991716146469116}]}, {"text": "One metric which is often overlooked is completeness.", "labels": [], "entities": []}, {"text": "This can be defined as the proportion of facts about an entity that are present in the KB as compared to an ideal KB which has every fact that can be known about that entity.", "labels": [], "entities": []}, {"text": "For example, previous research has shown that between 69% and 99% of entities in popular KBs lack at least one relation that other entities in the same class have.", "labels": [], "entities": []}, {"text": "As of 2016, Wikidata knows the father of only 2% of all people in the KB (.", "labels": [], "entities": [{"text": "KB", "start_pos": 70, "end_pos": 72, "type": "DATASET", "confidence": 0.8326998949050903}]}, {"text": "Google found that 71% of people in Freebase have no known place of birth, and 75% have no known nationality (.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.9545440077781677}]}, {"text": "Previous work has focused on a general concept of completeness, where all KB entities are expected to be fully complete, independent of how the KB is used.", "labels": [], "entities": []}, {"text": "This is a problem because different use cases of a KB may have different completeness requirements.", "labels": [], "entities": []}, {"text": "For this work, we were interested in determining a KB's completeness with respect to its query usage, which we term Demand-Weighted Completeness.", "labels": [], "entities": []}, {"text": "For example, a relation used 100 times per day is more important than one only used twice per day.", "labels": [], "entities": []}], "datasetContent": [{"text": "To create a class signature, we first determine the binary class membership vector for every entity in the usage dataset.", "labels": [], "entities": []}, {"text": "We then group entities by class signature, so entities with identical class membership are grouped together.", "labels": [], "entities": []}, {"text": "For each class signature, we generate the relation distribution from the usage data of the entities with that signature.", "labels": [], "entities": []}, {"text": "In our case, this usage data is a random subset of query traffic against the KB taken from a specific period of time.", "labels": [], "entities": []}, {"text": "The more usage a class signature has, the more fine-grained the  distribution of relations becomes.", "labels": [], "entities": []}, {"text": "The data is divided into 10 cross-validation folds to ensure that no class signature appears in both the validation and training sets.", "labels": [], "entities": []}, {"text": "We generate 3 different sizes of dataset for experimentation (see), to see how dataset size influences the models.", "labels": [], "entities": []}, {"text": "We compare the predicted relation distributions to those observed for the test examples in two ways: Weighted Jaccard Index.", "labels": [], "entities": [{"text": "Weighted Jaccard Index", "start_pos": 101, "end_pos": 123, "type": "METRIC", "confidence": 0.8706753253936768}]}, {"text": "We modified the Jaccard index to include a weighting term, which weights every relation with the mean weight in the predicted and observed distribution (see).", "labels": [], "entities": []}, {"text": "This rewards a correctly predicted relation without focusing on the proportion predicted for that relation, and is sufficient to define a set of important relations fora class sig-  We also evaluated the models using the Weighted Jaccard index and Intersection methods, but weighting by usage counts for each signature.", "labels": [], "entities": []}, {"text": "This metric rewards the models more for correctly predicting relation distributions for common class signatures in the usage data.", "labels": [], "entities": []}, {"text": "While unweighted analysis is useful to examine how the model covers the breadth of the problem space, weighted evaluation more closely reflects the model's utility for real usage data.", "labels": [], "entities": []}, {"text": "We include in the results using the weighted evaluation scheme described in Section 5.1.", "labels": [], "entities": []}, {"text": "This gives more usage-focused evaluation, emphasizing the non-uniform usage of different class signatures.", "labels": [], "entities": []}, {"text": "The D3 large neural model achieves 85% precision with a weighted evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9994105100631714}]}, {"text": "With the low rate of false negatives, this indicates that a similar model could be used to predict the necessary relations for KB usage.", "labels": [], "entities": []}, {"text": "gives measurements of the intersection metric.", "labels": [], "entities": []}, {"text": "These show a similar trend to the Jaccard scores, with lower absolute values from the stricter evaluation metric.", "labels": [], "entities": []}, {"text": "Although the Jaccard measure shows correct relation set prediction with a precision of 0.700, predicting the proportions for those relations accurately remains a difficult problem.", "labels": [], "entities": [{"text": "relation set prediction", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.6276294589042664}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9933282136917114}]}, {"text": "The best value we achieved was 0.398.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Unweighted results for the three models on the  three datasets.", "labels": [], "entities": []}, {"text": " Table 3: Usage-weighted results for the three models  on the three datasets.", "labels": [], "entities": []}, {"text": " Table 4: Results for the three methods for the D3 large  dataset using the intersection metric. The difference  between the methods is similar to the Jaccard measure  above.", "labels": [], "entities": [{"text": "D3 large  dataset", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.762414813041687}]}, {"text": " Table 5: Results of training a neural model on all avail- able data for D1 small -D3 large , then evaluating on T1- T3. The values for D2 medium and D3 large are higher  than cross-validation, as cross-validation never tests a  model on examples used to train it. However, the T  datasets contain all data from the specified period. The  downward trend with increasing T is clear, but slight.", "labels": [], "entities": []}]}