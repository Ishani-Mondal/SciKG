{"title": [{"text": "Pay-Per-Request Deployment of Neural Network Models Using Serverless Architectures", "labels": [], "entities": []}], "abstractContent": [{"text": "We demonstrate the serverless deployment of neural networks for model inferencing in NLP applications using Amazon's Lambda service for feedforward evaluation and DynamoDB for storing word embeddings.", "labels": [], "entities": [{"text": "DynamoDB", "start_pos": 163, "end_pos": 171, "type": "DATASET", "confidence": 0.7870383262634277}]}, {"text": "Our architecture realizes a pay-per-request pricing model, requiring zero ongoing costs for maintaining server instances.", "labels": [], "entities": []}, {"text": "All virtual machine management is handled behind the scenes by the cloud provider without any direct developer intervention.", "labels": [], "entities": [{"text": "virtual machine management", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6476082305113474}]}, {"text": "We describe a number of techniques that allow efficient use of serverless resources, and evaluations confirm that our design is both scalable and inexpensive.", "labels": [], "entities": []}], "introductionContent": [{"text": "Client-server architectures are currently the dominant approach to deploying neural network models for inferencing, both in industry and for academic research.", "labels": [], "entities": []}, {"text": "Once a model has been trained, deployment generally involves \"wrapping\" the model in an RPC mechanism (such as Thrift) or a REST interface (e.g., Flask in Python), or alternatively using a dedicated framework such as TensorFlowServing (.", "labels": [], "entities": []}, {"text": "This approach necessitates provisioning machines (whether physical or virtual) to serve the model.", "labels": [], "entities": []}, {"text": "Load management is an important aspect of running an inference service.", "labels": [], "entities": [{"text": "Load management", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8875181376934052}]}, {"text": "As query load increases, new server instances must be brought online, typically behind a load-balancing frontend.", "labels": [], "entities": []}, {"text": "While these issues are generally well understood and industry has evolved best practices and standard toolsets, the developer still shoulders the burden of managing these tasks.", "labels": [], "entities": []}, {"text": "A server-based architecture, moreover, involves a minimum commitment of resources, since some server must be running all the time, even if it is the smallest and cheapest virtual instance provided by a cloud service.", "labels": [], "entities": []}, {"text": "Particularly relevant for academic researchers, this means that a service costs money to run even if no one is actually using it.", "labels": [], "entities": []}, {"text": "As an alternative, we demonstrate a pay-perrequest serverless architecture for deploying neural network models for NLP applications.", "labels": [], "entities": []}, {"text": "We applied our approach to two real-world CNNs: the model of for sentence classification and the CNN of for answer selection in question answering (SM CNN for short).", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.7374601364135742}, {"text": "answer selection in question answering", "start_pos": 108, "end_pos": 146, "type": "TASK", "confidence": 0.7230462670326233}]}, {"text": "On Amazon Web Services, the models cost less than a thousandth of a cent per invocation.", "labels": [], "entities": []}, {"text": "Model inference does not require the developer to explicitly manage machines, and there are zero ongoing costs for service deployment.", "labels": [], "entities": [{"text": "Model inference", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7537600994110107}]}, {"text": "We show that our design can transparently scale to moderate query loads without requiring any systems engineering expertise.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first provide some implementation details: Kim CNN is a sentence classification model that consists of convolutions over a single sentence input matrix and pooling followed by a fullyconnected layer with dropout and softmax output.", "labels": [], "entities": [{"text": "Kim CNN", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9228753745555878}, {"text": "sentence classification", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.725505605340004}]}, {"text": "We used the variant where the word embeddings are not fine-tuned via backpropagation during training (called the \"static\" variant).", "labels": [], "entities": []}, {"text": "SM CNN is a model for ranking short text pairs that consists of convolutions using shared filters over both inputs, pooling, and a fully-connected layer with one hidden layer in between.", "labels": [], "entities": [{"text": "SM CNN", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.7702427804470062}]}, {"text": "We used the variant described by, which excludes the similarity matrix (found to increase accuracy) as well as the additional features that involve inverse document frequency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9991299510002136}]}, {"text": "In our experiments, we are focused only on execution performance, which is not affected by these minor tweaks, primarily for expediency.", "labels": [], "entities": []}, {"text": "All of our code and experiment utilities are open-sourced on GitHub.", "labels": [], "entities": []}, {"text": "Before detailing our experimental procedure and results, we need to explain Amazon's cost model.", "labels": [], "entities": []}, {"text": "Lambda costs are very straightforward, billed simply by how long each function executes in increments of 100ms, fora particular amount of allocated memory that the developer specifies.", "labels": [], "entities": []}, {"text": "DynamoDB's cost model is more complex: it supports two modes of operation, termed manual provisioning and auto scaling.", "labels": [], "entities": [{"text": "DynamoDB", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9179613590240479}]}, {"text": "In the first mode, the developer must explicitly allocate read and write capacity.", "labels": [], "entities": []}, {"text": "Amazon provides the capacity, but the downside is a fixed cost, even if the capacity is not fully utilized (and over-utilization will result in timeouts).", "labels": [], "entities": [{"text": "Amazon", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.963706374168396}]}, {"text": "Thus, this mode is not truly \"pay as you go\".", "labels": [], "entities": []}, {"text": "The alternative is what Amazon calls auto scaling, where the service continuously monitors and adjusts capacity on the fly.", "labels": [], "entities": []}, {"text": "For our experiments, we opted to manually provision 500 Read Capacity Units (RCUs), which translates into supporting a DynamoDB query load of 1000 queries per second (fetching the word vector for each word constitutes a query).", "labels": [], "entities": [{"text": "Read Capacity Units (RCUs)", "start_pos": 56, "end_pos": 82, "type": "METRIC", "confidence": 0.7138625184694926}]}, {"text": "This choice makes our experimental results easier to interpret, since we have little insight into how Amazon handles auto scaling behind the scenes.", "labels": [], "entities": []}, {"text": "Note however, that we adopted this configuration for experimental clarity, because otherwise we would be conflating unknown \"backend knobs\" in our performance measurements.", "labels": [], "entities": []}, {"text": "In production, auto scaling would be the preferred solution.", "labels": [], "entities": []}, {"text": "To evaluate performance, we built a test harness that dispatches requests in parallel, with a single parameter to control the number of outstanding requests allowed when issuing queries.", "labels": [], "entities": []}, {"text": "We call this  the concurrency parameter, which we vary to simulate different amounts of query load.", "labels": [], "entities": []}, {"text": "With different concurrency settings (ramping down from maximum load), we measured latency (mean, 50th and 99th percentile) and throughput.", "labels": [], "entities": [{"text": "latency", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.976237416267395}]}, {"text": "For Kim CNN, we used input sentences from the validation set of the Stanford Sentiment Treebank (1101 sentences).", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 68, "end_pos": 95, "type": "DATASET", "confidence": 0.8788357973098755}]}, {"text": "For SM CNN, we used input sentence pairs from the validation set of the TrecQA dataset (1148 sentences).", "labels": [], "entities": [{"text": "SM CNN", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.9721189141273499}, {"text": "TrecQA dataset", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.9560965895652771}]}, {"text": "We conducted each experimental trial multiple times before taking measurements to \"warm up\" the backend.", "labels": [], "entities": []}, {"text": "Results are shown in for SM CNN.", "labels": [], "entities": [{"text": "SM CNN", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.9383807182312012}]}, {"text": "Our deployment package is bundled with OpenBLAS to take advantage of optimized linear algebra routines.", "labels": [], "entities": [{"text": "OpenBLAS", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9652423858642578}]}, {"text": "In both cases, we see that latency increases slightly as throughput ramps up.", "labels": [], "entities": [{"text": "latency", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9642142653465271}]}, {"text": "This suggests that we are not achieving perfect scale up.", "labels": [], "entities": []}, {"text": "In theory, AWS should be proportionally increasing backend resources to maintain constant latency.", "labels": [], "entities": [{"text": "AWS", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.8129322528839111}]}, {"text": "It is not clear if this behavior is due to some nuance in Lambda usage that we are not aware of, or if there are actual bottlenecks in our design.", "labels": [], "entities": []}, {"text": "Note that Kim CNN is slower because it is manipulating much larger word vectors (300 vs. 50 dimensions).", "labels": [], "entities": [{"text": "Kim CNN", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.9288217723369598}]}, {"text": "The final column in report Lambda charges in US dollars per million queries based on the mean latency.", "labels": [], "entities": [{"text": "latency", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.8904197812080383}]}, {"text": "As of February 2018, for functions allocated 128 MB of memory, the cost is $0.000000208 for every 100ms of running time (rounded up).", "labels": [], "entities": []}, {"text": "Note that these costs do not include provisioning DynamoDB, which costs 0.013 cents per Read Capacity Unit per hour.", "labels": [], "entities": [{"text": "provisioning", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.9836483001708984}, {"text": "DynamoDB", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.6565934419631958}]}, {"text": "We have not probed the scalability limits of our current architecture, but it is likely that our design can handle even larger query loads without additional modification.", "labels": [], "entities": []}, {"text": "We performed additional analyses to understand the latency breakdown: logs show that approximately 60-70% of time inside each function invocation is spent building the embedding matrix, which requires fetching word vectors from DynamoDB.", "labels": [], "entities": [{"text": "DynamoDB", "start_pos": 228, "end_pos": 236, "type": "DATASET", "confidence": 0.9697707295417786}]}, {"text": "In other words, inference latency is dominated by data fetching.", "labels": [], "entities": [{"text": "data fetching", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.7493757009506226}]}, {"text": "This is no surprise since these queries involve cross-machine requests.", "labels": [], "entities": []}, {"text": "The rest of the time is spent primarily on feedforward evaluation.", "labels": [], "entities": [{"text": "feedforward evaluation", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.5743010193109512}]}, {"text": "The amortized cost of loading the model is negligible since it can be reused in subsequent invocations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Latency, throughput, and cost of serverless  Kim CNN under different loads (C).", "labels": [], "entities": [{"text": "Latency", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9950942993164062}]}, {"text": " Table 2: Latency, throughput, and cost of serverless SM  CNN under different loads (C).", "labels": [], "entities": [{"text": "Latency", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9947439432144165}, {"text": "SM  CNN", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.8891859352588654}]}]}