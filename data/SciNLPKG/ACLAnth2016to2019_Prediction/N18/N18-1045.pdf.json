{"title": [{"text": "Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection", "labels": [], "entities": [{"text": "Distributional Inclusion Vector Embedding", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.95347099006176}, {"text": "Hypernymy Detection", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.6738632768392563}]}], "abstractContent": [{"text": "Modeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, such as entailment, coref-erence, relation extraction, and question answering.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.8805720210075378}, {"text": "question answering", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.8964661359786987}]}, {"text": "Supervised learning from labeled hypernym sources, such as WordNet, limits the coverage of these models, which can be addressed by learning hypernyms from un-labeled text.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9701007604598999}]}, {"text": "Existing unsupervised methods either do not scale to large vocabularies or yield unacceptably poor accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9966719150543213}]}, {"text": "This paper introduces distributional inclusion vector embedding (DIVE), a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embed-dings which preserve the inclusion property of word contexts in a low-dimensional and interpretable space.", "labels": [], "entities": [{"text": "distributional inclusion vector embedding (DIVE)", "start_pos": 22, "end_pos": 70, "type": "TASK", "confidence": 0.5825864970684052}, {"text": "hypernym discovery", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7512740194797516}]}, {"text": "In experimental evaluations more comprehensive than any previous literature of which we are aware-evaluating on 11 datasets using multiple existing as well as newly proposed scoring functions-we find that our method provides up to double the precision of previous unsupervised embeddings, and the highest average performance, using a much more compact word representation, and yielding many new state-of-the-art results.", "labels": [], "entities": [{"text": "precision", "start_pos": 242, "end_pos": 251, "type": "METRIC", "confidence": 0.9988434314727783}]}], "introductionContent": [{"text": "Numerous applications benefit from compactly representing context distributions, which assign meaning to objects under the rubric of distributional semantics.", "labels": [], "entities": []}, {"text": "In natural language processing, distributional semantics has long been used to assign meanings to words (that is, to lexemes in the dictionary, not individual instances of word tokens).", "labels": [], "entities": []}, {"text": "The meaning of a word in the distributional sense is often taken to be the set of textual contexts (nearby tokens) in which that word appears, represented as a large sparse bag of words (SBOW).", "labels": [], "entities": []}, {"text": "Without any supervision,), among other approaches based on matrix factorization (), successfully compress the SBOW into a much lower dimensional embedding space, increasing the scalability and applicability of the embeddings while preserving (or even improving) the correlation of geometric embedding similarities with human word similarity judgments.", "labels": [], "entities": []}, {"text": "While embedding models have achieved impressive results, context distributions capture more semantic information than just word similarity.", "labels": [], "entities": []}, {"text": "The distributional inclusion hypothesis (DIH) () posits that the context set of a word tends to be a subset of the contexts of its hypernyms.", "labels": [], "entities": []}, {"text": "For a concrete example, most adjectives that can be applied to poodle can also be applied to dog, because dog is a hypernym of poodle (e.g. both can be obedient).", "labels": [], "entities": []}, {"text": "However, the converse is not necessarily true -a dog can be straight-haired but a poodle cannot.", "labels": [], "entities": []}, {"text": "Therefore, dog tends to have a broader context set than poodle.", "labels": [], "entities": []}, {"text": "Many asymmetric scoring functions comparing SBOW features based on DIH have been developed for hypernymy detection.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.859599232673645}]}, {"text": "Hypernymy detection plays a key role in many challenging NLP tasks, such as textual entailment), coreference (), relation extraction) and question answering (.", "labels": [], "entities": [{"text": "Hypernymy detection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8393847942352295}, {"text": "relation extraction", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.8549757301807404}, {"text": "question answering", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.9083531200885773}]}, {"text": "Leveraging the variety of contexts and inclusion properties in context distributions can greatly increase the ability to discover taxonomic structure among words (.", "labels": [], "entities": []}, {"text": "The inability to preserve these features limits the semantic representation power and downstream applicability of some popular unsupervised learning approaches such as Word2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 168, "end_pos": 176, "type": "DATASET", "confidence": 0.945528507232666}]}, {"text": "Several recently proposed methods aim to en-code hypernym relations between words in dense embeddings, such as Gaussian embedding, Boolean Distributional Semantic Model (, order embedding (), H-feature detector,, dual tensor (, Poincar\u00e9 embedding (, and LEAR.", "labels": [], "entities": [{"text": "LEAR", "start_pos": 254, "end_pos": 258, "type": "METRIC", "confidence": 0.9941065311431885}]}, {"text": "However, the methods focus on supervised or semisupervised settings where a massive amount of hypernym annotations are available (, do not learn from raw text or lack comprehensive experiments on the hypernym detection task.", "labels": [], "entities": [{"text": "hypernym detection task", "start_pos": 200, "end_pos": 223, "type": "TASK", "confidence": 0.7991076509157816}]}, {"text": "Recent studies ( have underscored the difficulty of generalizing supervised hypernymy annotations to unseen pairs -classifiers often effectively memorize prototypical hypernyms ('general' words) and ignore relations between words.", "labels": [], "entities": []}, {"text": "These findings motivate us to develop more accurate and scalable unsupervised embeddings to detect hypernymy and propose several scoring functions to analyze the embeddings from different perspectives.", "labels": [], "entities": []}], "datasetContent": [{"text": "The embeddings are tested on 11 datasets.", "labels": [], "entities": []}, {"text": "We trained all methods on the first 51.2 million tokens of WaCkypedia corpus () because DIH holds more often in this subset (i.e. SBOW works better) compared with that in the whole WaCkypedia corpus.", "labels": [], "entities": [{"text": "WaCkypedia corpus", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.9058164656162262}, {"text": "DIH", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.8050661087036133}, {"text": "WaCkypedia corpus", "start_pos": 181, "end_pos": 198, "type": "DATASET", "confidence": 0.8794370889663696}]}, {"text": "The window size |W | of DIVE and Gaussian embedding are set as 20 (left 10 words and right 10 words).", "labels": [], "entities": []}, {"text": "The number of embedding dimensions in DIVE L is set to be 100.", "labels": [], "entities": []}, {"text": "The other hyper-parameters of DIVE and Gaussian embedding are determined by the training set of HypeNet.", "labels": [], "entities": [{"text": "HypeNet", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9276054501533508}]}, {"text": "Other experimental details are described in our supplementary materials.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison with other unsupervised embedding methods. The scores are AP@all (%) for the first 10  datasets and Spearman \u03c1 (%) for HyperLex. Avg (10 datasets) shows the micro-average AP of all datasets except  HyperLex. Word2Vec+C scores word pairs using cosine similarity on skip-grams. GE+C and GE+KL compute  cosine similarity and negative KL divergence on Gaussian embedding, respectively.", "labels": [], "entities": [{"text": "AP", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9977900981903076}, {"text": "Spearman \u03c1", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9664221107959747}, {"text": "Avg", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.9834991097450256}]}, {"text": " Table 2: AP@all (%) of 10 datasets. The box at lower right corner compares the micro average AP across all  10 datasets. Numbers in different rows come from different feature or embedding spaces. Numbers in different  columns come from different datasets and unsupervised scoring functions. We also present the micro average AP  across the first 4 datasets (BLESS, EVALution, Lenci/Benotto and Weeds), which are used as a benchmark for  unsupervised hypernym detection (Shwartz et al., 2017). IS refers to inclusion shift on the shifted PMI matrix.", "labels": [], "entities": [{"text": "AP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9896681308746338}, {"text": "micro average AP", "start_pos": 80, "end_pos": 96, "type": "METRIC", "confidence": 0.599592516819636}, {"text": "micro average AP", "start_pos": 312, "end_pos": 328, "type": "METRIC", "confidence": 0.5813980102539062}, {"text": "BLESS", "start_pos": 359, "end_pos": 364, "type": "METRIC", "confidence": 0.9899224638938904}, {"text": "hypernym detection", "start_pos": 451, "end_pos": 469, "type": "TASK", "confidence": 0.7503142356872559}]}, {"text": " Table 3: Spearman \u03c1 (%) in HyperLex.", "labels": [], "entities": [{"text": "Spearman \u03c1", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9493420422077179}, {"text": "HyperLex", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.8983940482139587}]}, {"text": " Table 4: The average number of non-zero dimensions  across all testing words in 10 datasets.", "labels": [], "entities": []}]}