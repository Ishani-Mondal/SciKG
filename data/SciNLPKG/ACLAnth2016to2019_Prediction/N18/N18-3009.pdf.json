{"title": [{"text": "Using Aspect Extraction Approaches to Generate Review Summaries and User Profiles", "labels": [], "entities": [{"text": "Generate Review Summaries", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6205447514851888}]}], "abstractContent": [{"text": "Reviews of products or services on Internet marketplace websites contain a rich amount of information.", "labels": [], "entities": []}, {"text": "Users often wish to survey reviews or review snippets from the perspective of a certain aspect, which has resulted in a large body of work on aspect identification and extraction from such corpora.", "labels": [], "entities": [{"text": "aspect identification", "start_pos": 142, "end_pos": 163, "type": "TASK", "confidence": 0.8025857508182526}]}, {"text": "In this work, we evaluate a newly-proposed neural model for aspect extraction on two practical tasks.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8089466691017151}]}, {"text": "The first is to extract canonical sentences of various aspects from reviews, and is judged by human evaluators against alternatives.", "labels": [], "entities": []}, {"text": "A k-means baseline does remarkably well in this setting.", "labels": [], "entities": []}, {"text": "The second experiment focuses on the suitability of the recovered aspect distributions to represent users by the reviews they have written.", "labels": [], "entities": []}, {"text": "Through a set of review reranking experiments, we find that aspect-based profiles can largely capture notions of user preferences , by showing that divergent users generate markedly different review rankings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Aspect extraction has traditionally been associated with the sentiment analysis community, with the goal being to decompose a small document of text (e.g., a review) into multiple facets, each of which may possess their own sentiment marker.", "labels": [], "entities": [{"text": "Aspect extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9457671642303467}, {"text": "sentiment analysis", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.9155452847480774}]}, {"text": "For example, a restaurant review may comment on the ambiance, service, and food, preventing the assignment of a uniform sentiment over the entire review.", "labels": [], "entities": []}, {"text": "A common approach to aspect extraction is to treat the aspects as latent variables and utilize latent Dirichlet allocation (LDA;) to extract relevant aspects from a collection of documents in an unsupervised or semi-supervised (Mukherjee and Liu, 2012) fash- * Equal contribution. ion.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7907633483409882}, {"text": "latent Dirichlet allocation (LDA;)", "start_pos": 95, "end_pos": 129, "type": "METRIC", "confidence": 0.7795244455337524}]}, {"text": "Subsequent research has taken the latent variable approach further by encoding more complicated dependencies between aspects and sentiment (, or between aspects, ratings, and sentiment (), using probabilistic graphical models to jointly learn the parameters.", "labels": [], "entities": []}, {"text": "However, it has been argued that the coherence of aspects extracted from the family of LDA-based approaches is low; words clustered together within a specific aspect are often unrelated, which can be attributed to the lack of word co-occurrence information in these models), since conventional LDA assumes each word in a document is generated independently.", "labels": [], "entities": []}, {"text": "Recently, proposed a neural attention-based aspect extraction (ABAE) approach, which like LDA, is an unsupervised model.", "labels": [], "entities": [{"text": "neural attention-based aspect extraction (ABAE)", "start_pos": 21, "end_pos": 68, "type": "TASK", "confidence": 0.6782733840601785}]}, {"text": "The starting point is a set of word embeddings, where the vector representation of the word encapsulates cooccurrence . The embeddings are used to represent a sentence as a bag-of-words, weighted with a self-attention mechanism (, and learning amounts to encoding the resulting attention-based sentence embedding as a linear combination of aspect embeddings, optimized using an autoencoder formulation ( \u00a72).", "labels": [], "entities": []}, {"text": "The attention mechanism thus learns to highlight words that will be pertinent for aspect identification.", "labels": [], "entities": [{"text": "aspect identification", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.9365979433059692}]}, {"text": "In this work, we apply the ABAE model to a large corpus of reviews on Airbnb 2 , an online marketplace for travel; users (guests) utilize the site to find accommodation (listings) all around the world, and a large number of these guests write reviews of the listing post-stay.", "labels": [], "entities": [{"text": "ABAE", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9668092131614685}, {"text": "Airbnb 2", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.971265435218811}]}, {"text": "We first provide additional details on the workings of the ABAE model ( \u00a72).", "labels": [], "entities": [{"text": "ABAE", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.6896359920501709}]}, {"text": "ABAE is then applied to two tasks: the first ( \u00a73.1) is to extract a representative sentence from a set of listing-specific reviews fora number of pre-defined aspects e.g., cleanliness and location, with the efficacy of extractive summarization evaluated by humans ( \u00a74.3).", "labels": [], "entities": []}, {"text": "Surprisingly, we find that the k-means baseline performs very well on aspects that occur more frequently, but ABAE maybe better for infrequent aspects.", "labels": [], "entities": [{"text": "ABAE", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9932733774185181}]}, {"text": "In the second task, we analyze the suitability of aspect embeddings to represent guest profiles.", "labels": [], "entities": []}, {"text": "The hypothesis is that the content of guest reviews reveals the guest's preferences and priorities (, and that these preferences correspond to extracted aspects.", "labels": [], "entities": []}, {"text": "We investigate several ways to aggregate sentence-level aspect embeddings at the review and user levels and compute distances between user aspect and listing review embeddings, in order to personalize listing reviews by reranking them for each user.", "labels": [], "entities": []}, {"text": "The correlation between guest profile distances (computed on pairs of guests) and review rank distances (computed on pairs of ordinal rankings over reviews) is then measured to evaluate our hypothesis ( \u00a74.4).", "labels": [], "entities": [{"text": "review rank distances", "start_pos": 82, "end_pos": 103, "type": "METRIC", "confidence": 0.805356502532959}]}, {"text": "We find a robust relationship between distances in the two spaces, with the correlation increasing at finer granularities like sentences compared to reviews or listings.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corpus was extracted from all reviews across all listings on Airbnb written between January 1, 2010 and January 1, 2017.", "labels": [], "entities": [{"text": "Airbnb written between January 1", "start_pos": 65, "end_pos": 97, "type": "DATASET", "confidence": 0.9679878354072571}]}, {"text": "We used spaCy 6 to segment reviews into sentences and remove nonEnglish sentences.", "labels": [], "entities": []}, {"text": "All sentences were subsequently preprocessed in the same manner as, which entailed restricting the vocabulary to the 9,000 most frequent words in the corpus after stopword and punctuation removal.", "labels": [], "entities": []}, {"text": "From the resulting set, we randomly sampled 10 million sentences across 5.8 million guests and 1.8 million listings to form a training set, and used the remaining unsampled sentences to select validation and test sets for the human evaluation ( \u00a74.3) and ranking ( \u00a74.4) experiments.", "labels": [], "entities": []}, {"text": "To select datasets for human evaluation, we identified all listings with at least 50 and at most 100 reviews in all languages and filtered out any listing in the training set, resulting in 900 listings which were split into validation and test sets.", "labels": [], "entities": []}, {"text": "The validation set is used to select an appropriate number of aspects, by computing coherence scores () as the number of aspects is varied in the ABAE model ( \u00a74.2).", "labels": [], "entities": [{"text": "ABAE", "start_pos": 146, "end_pos": 150, "type": "DATASET", "confidence": 0.783587634563446}]}, {"text": "The test set was used to extract review sentences that were presented to our human evaluators; we ensured that every listing in the test set has at least 3 non-empty English review sentences.", "labels": [], "entities": []}, {"text": "For the ranking correlation experiments, we first identified users who had written at least 10 review sentences in our corpus and removed those users that featured in the training set from this list.", "labels": [], "entities": []}, {"text": "We then selected 20 users uniformly at random to form our validation set i.e., to compute guest profiles for . A subset of the human evaluation test set was used to compute the correlation between aspect space and ranking order distances; we selected all listings that had at least 20 review sentences, resulting in 69 listings for evaluation.", "labels": [], "entities": []}, {"text": "mapped to that aspect in parentheses as well as the percentage of validation set sentences assigned to that cluster (the remaining sentences were assigned to \"Other\").", "labels": [], "entities": []}, {"text": "For the aspects with multiple clusters, we select a roughly equal number of words from each cluster.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics for the datasets that we use. All", "labels": [], "entities": []}, {"text": " Table 2: Coherence scores as a function of the number of", "labels": [], "entities": []}, {"text": " Table 3: Representative words for each aspect of interest across experimental setups, along with the number of clusters", "labels": [], "entities": []}, {"text": " Table 4: Precision@1 and precision@3 for the extractive", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9944188594818115}, {"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9993864297866821}]}]}