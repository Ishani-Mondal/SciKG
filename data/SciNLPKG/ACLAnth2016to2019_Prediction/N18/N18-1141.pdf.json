{"title": [{"text": "Learning to Collaborate for Question Answering and Asking", "labels": [], "entities": [{"text": "Question Answering and Asking", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.8088632971048355}]}], "abstractContent": [{"text": "Question answering (QA) and question generation (QG) are closely related tasks that could improve each other; however, the connection of these two tasks is not well explored in literature.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9132714390754699}, {"text": "question generation (QG)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.855022644996643}]}, {"text": "In this paper, we give a systematic study that seeks to leverage the connection to improve both QA and QG.", "labels": [], "entities": []}, {"text": "We present a training algorithm that generalizes both Gen-erative Adversarial Network (GAN) and Gen-erative Domain-Adaptive Nets (GDAN) under the question answering scenario.", "labels": [], "entities": [{"text": "question answering", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.7974337339401245}]}, {"text": "The two key ideas are improving the QG model with QA through incorporating additional QA-specific signal as the loss function, and improving the QA model with QG through adding artificially generated training instances.", "labels": [], "entities": []}, {"text": "We conduct experiments on both document based and knowledge based question answering tasks.", "labels": [], "entities": [{"text": "knowledge based question answering tasks", "start_pos": 50, "end_pos": 90, "type": "TASK", "confidence": 0.6837525546550751}]}, {"text": "We have two main findings.", "labels": [], "entities": []}, {"text": "Firstly, the performance of a QG model (e.g in terms of BLEU score) could be easily improved by a QA model via policy gradient.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.984563022851944}]}, {"text": "Secondly, directly applying GAN that regards all the generated questions as negative instances could not improve the accuracy of the QA model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9992989301681519}]}, {"text": "Learning when to regard generated questions as positive instances could bring performance boost.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this work, we consider the task of joint learning of question answering and question generation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.8183045983314514}, {"text": "question generation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.822114884853363}]}, {"text": "Question answering (QA) and question generation (QG) are closely related natural language processing tasks.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9164235591888428}, {"text": "question generation (QG)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8575934886932373}]}, {"text": "The goal of QA is to obtain an answer given a question.", "labels": [], "entities": [{"text": "QA", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9131432771682739}]}, {"text": "The goal of QG is almost reverse which is to generate a question from the answer.", "labels": [], "entities": []}, {"text": "In this work, we consider answer selection ( as the QA task, which assigns a numeric score to each candidate answer, and selects the top ranked one as the answer.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.9354125261306763}]}, {"text": "We consider QG as a generation problem and exploit sequence-to-sequence learning (Seq2Seq) ( as the backbone of the QG model.", "labels": [], "entities": []}, {"text": "The key idea of this work is that QA and QG are two closely tasks and we seek to leverage the connection between these two tasks to improve both QA and QG.", "labels": [], "entities": []}, {"text": "Our primary motivations are twofolds.", "labels": [], "entities": []}, {"text": "On one hand, the Seq2Seq based QG model is trained by maximizing the literal similarity between the generated sentence and the ground truth sentence with maximum-likelihood estimation objective function (.", "labels": [], "entities": []}, {"text": "However, there is no signal indicating whether or not the generated sentence could be correctly answered by the input.", "labels": [], "entities": []}, {"text": "This problem could be precisely mitigated through incorporating QA-specific signal into the QG loss function.", "labels": [], "entities": []}, {"text": "On the other hand, the capacity of a statistical model depends on the quality and the amount of the training data (.", "labels": [], "entities": []}, {"text": "In our scenario, the capacity of the QA model depends on the difference between the positive and negative patterns embodied in the training examples.", "labels": [], "entities": []}, {"text": "A desirable training dataset should contain the question-answer pairs that are literally similar yet have different category labels, i.e. some question-answer pairs are correct and some are wrong.", "labels": [], "entities": []}, {"text": "However, this kind of dataset is hard to obtain inmost situations because of the lack of manual annotation efforts.", "labels": [], "entities": []}, {"text": "From this perspective, the QA model could exactly benefit from the QG model through incorporating additional questionanswer pairs whose questions are automatically generated by the QG model . To achieve this goal, we present a training algorithm that improves the QA model and the An alternative way is to automatically generate answers for each question.", "labels": [], "entities": []}, {"text": "Solving the problem in this condition requires an answer generation model (, which is out of the focus of this work.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.8250482380390167}]}, {"text": "Our algorithm could also be adapted to this scenario.", "labels": [], "entities": []}, {"text": "QG model in a loop.", "labels": [], "entities": []}, {"text": "The QA model improves QG through introducing an additional QA-specific loss function, the objective of which is to maximize the expectation of the QA scores of the generated question-answer pairs.", "labels": [], "entities": [{"text": "QG", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9728941917419434}]}, {"text": "Policy gradient method) is used to update the QG model.", "labels": [], "entities": []}, {"text": "In turn, the QG model improves QA through incorporating additional training instances.", "labels": [], "entities": []}, {"text": "Here the key problem is how to label the generated question-answer pair.", "labels": [], "entities": []}, {"text": "The application of Generative Adversarial Network (GAN) () in this scenario regards every generated question-answer pair as a negative instance.", "labels": [], "entities": []}, {"text": "On the contrary, Generative Domain-Adaptive Nets (GDAN) ( ) regards every generated question-answer pair appended with special domain tag as a positive instance.", "labels": [], "entities": []}, {"text": "However, it is non-trivial to label the generated question-answer pairs because some of which are good paraphrases of the ground truth yet some might be negative instances with similar utterances.", "labels": [], "entities": []}, {"text": "To address this, we bring in a collaboration detector, which takes two question-answer pairs as the input and determines their relation as collaborative or competitive.", "labels": [], "entities": []}, {"text": "The output of the collaboration detector is regarded as the label of the generated questionanswer pair.", "labels": [], "entities": []}, {"text": "We conduct experiments on both document based and knowledge (e.g. web table) based question answering tasks (.", "labels": [], "entities": [{"text": "question answering tasks", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.7700647910435995}]}, {"text": "Results show that the performance of a QG model (e.g in terms of BLEU score) could be consistently improved by a QA model via policy gradient.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9845601916313171}]}, {"text": "However, regarding all the generated questions as negative instances (competitive) could not improve the accuracy of the QA model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9994370341300964}]}, {"text": "Learning when to regard generated questions as positive instances (collaborative) could improve the accuracy of the QA model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9992460012435913}]}], "datasetContent": [{"text": "We conduct experiments on table-based QA and document-based QA tasks.", "labels": [], "entities": []}, {"text": "We will describe experimental settings and report results on these two tasks in this section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The performances of single systems on table- based question answering (p-value < 0.01 with t-test  between TQNN and GCN).", "labels": [], "entities": [{"text": "table- based question answering", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.6310130655765533}, {"text": "GCN", "start_pos": 126, "end_pos": 129, "type": "DATASET", "confidence": 0.7163335680961609}]}, {"text": " Table 2. We can  see that the baseline system could be dramatical- ly improved by our system, despite the improve- ments of different approaches are on par.", "labels": [], "entities": []}, {"text": " Table 2: The performances of combined systems on  table-based question answering..", "labels": [], "entities": [{"text": "table-based question answering.", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.5987059672673544}]}, {"text": " Table 3. Different from the  trends on QA, \"competitive\" performs better than  \"collaborative\" on QG. This is reasonable because  as the joint training progresses, the QA model in  \"collaborative\" keeps telling the QG model that  the generated instances are good enough. On the  contrary, the \"competitive\" model is more critical,  which tells the QG model how wrong the generat- ed questions are. In this way, the QG model could  be increasingly improved by the QA signal. The  QG model is easier to be improved compared to  the QA model. Our GCN approach obtains a sig- nificant improvement over the baseline model on  this task.", "labels": [], "entities": []}, {"text": " Table 3: The performances on table-based question  generation. Evaluation metric is BLEU-4 score.", "labels": [], "entities": [{"text": "table-based question  generation", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.6261190374692281}, {"text": "Evaluation", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9282665252685547}, {"text": "BLEU-4 score", "start_pos": 85, "end_pos": 97, "type": "METRIC", "confidence": 0.9790972471237183}]}, {"text": " Table 4: The performance on document-based QA task  (p-value < 0.05 with t-test between DQNN and GCN).", "labels": [], "entities": [{"text": "GCN", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.6452271342277527}]}, {"text": " Table 5.  We can see that the results are almost consistent  with the results on table-based QA and QG tasks.  Our GCN algorithms achieves promising perfor- mances compared to strong baseline methods.", "labels": [], "entities": []}, {"text": " Table 5: The performance on document-based QG task.", "labels": [], "entities": []}]}