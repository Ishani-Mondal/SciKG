{"title": [{"text": "Microblog Conversation Recommendation via Joint Modeling of Topics and Discourse", "labels": [], "entities": []}], "abstractContent": [{"text": "Millions of conversations are generated everyday on social media platforms.", "labels": [], "entities": []}, {"text": "With limited attention, it is challenging for users to select which discussions they would like to participate in.", "labels": [], "entities": []}, {"text": "Here we propose anew method for microblog conversation recommendation.", "labels": [], "entities": [{"text": "microblog conversation recommendation", "start_pos": 32, "end_pos": 69, "type": "TASK", "confidence": 0.7895002166430155}]}, {"text": "While much prior work has focused on post-level recommendation, we exploit both the conversational context, and user content and behavior preferences.", "labels": [], "entities": []}, {"text": "We propose a statistical model that jointly captures: (1) topics for representing user interests and conversation content , and (2) discourse modes for describing user replying behavior and conversation dynamics.", "labels": [], "entities": []}, {"text": "Experimental results on two Twitter datasets demonstrate that our system outper-forms methods that only model content without considering discourse.", "labels": [], "entities": []}], "introductionContent": [{"text": "Online platforms have revolutionized the way individuals collect and share information, but the vast bulk of online content is irrelevant or unpalatable to any given individual.", "labels": [], "entities": []}, {"text": "A user interested in political discussion, for instance, might prefer content concerning a specific candidate or issue, and only then if discussed in a positive light without controversy.", "labels": [], "entities": []}, {"text": "How do individuals facing such large quantities of superfluous material select which conversations to engage in, and how might we better algorithmically recommend conversations suited to individual users?", "labels": [], "entities": []}, {"text": "We approach this problem from a microblog conversation recommendation framework.", "labels": [], "entities": []}, {"text": "Where prior work has focused on the content of individual posts for recommendation (Chen Conversation 1 ...", "labels": [], "entities": [{"text": "Chen Conversation 1", "start_pos": 84, "end_pos": 103, "type": "DATASET", "confidence": 0.8475826978683472}]}, {"text": "[U1]: The sheer cognitive dissonance required fora \"liberal\" to say Clinton is as bad as Trump is just staggering.", "labels": [], "entities": []}, {"text": "[U2]: Hillarists, Troll; they insult Liberals trying to distract from Hillary's Conseratism.", "labels": [], "entities": []}, {"text": "[U3]: I still prefer Hillarist b/c it describes their Cultish and ideological aspects.", "labels": [], "entities": []}, {"text": "[U4]: I do not like trump at all, but Comey left her in place knowing Bernie is much stronger.", "labels": [], "entities": [{"text": "Comey", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.7617618441581726}]}, {"text": "[U1]: If you're going to actively start rooting against the Democrats, get off my mentions.", "labels": [], "entities": []}, {"text": "I have enough GOP doing that.", "labels": [], "entities": [{"text": "GOP", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.8265272974967957}]}, {"text": "[U5]: Your tweets are an example of why open primaries are stupid.", "labels": [], "entities": []}, {"text": "You're not a Dem, you're just for one guy.", "labels": [], "entities": []}, {"text": "----------: No offense, but you've been wrong about pretty much everything so far.", "labels": [], "entities": []}, {"text": "Why would I trust your prognostication now?", "labels": [], "entities": []}, {"text": "[U i ]: The message is posted by user U i . \"-\" is the dividing line between training history and test part.", "labels": [], "entities": []}, {"text": "U 1 did not reengage in Conversation 1 but reengaged in et al.,, we examine the entire history and context of a conversation, including both topical content and discourse modes such as agreement, question-asking, argument and other dialogue acts (.", "labels": [], "entities": []}, {"text": "And where leveraged conversation reply structure (such as previous user engagement), their model is unable to predict first entry into new conversations, while ours is able to predict both new and repeated entry into conversations based on a combination of topical and discourse features.", "labels": [], "entities": []}, {"text": "To illustrate the interplay between topics and discourse, displays two snippets of conversations on Twitter collected during the 2016 United States presidential election.", "labels": [], "entities": []}, {"text": "User U 1 participates in both conversations.", "labels": [], "entities": []}, {"text": "The first conversation is centered around Clinton, and U 1 , who is more typically involved with conversations about candidate Sanders, does not return.", "labels": [], "entities": []}, {"text": "In the second conversation, however, U 1 is involved in a heated back-and-forth debate, and thus is drawn back to a conversation that they may otherwise have abandoned but for their enjoyment of adversarial discourse.", "labels": [], "entities": []}, {"text": "Effective conversation prediction and recommendation requires an understanding of both user interests and discourse behaviors, such as agreement, disagreement, inquiry, backchanneling, and emotional reactions.", "labels": [], "entities": [{"text": "conversation prediction", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.734889417886734}]}, {"text": "However, acquiring manual labels for both is a time-consuming process and hard to scale for new datasets.", "labels": [], "entities": []}, {"text": "We instead propose a unified statistical learning framework for conversation recommendation, which jointly learns (1) hidden factors that reflect user interests based on conversation history, and (2) topics and discourse modes in ongoing conversations, as discovered by a novel probabilistic latent variable model.", "labels": [], "entities": [{"text": "conversation recommendation", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.7773662805557251}]}, {"text": "Our model is built on the success of collaborative filtering (CF) in recommendation systems, where latent dimensions of product ratings or movie reviews are extracted to better capture user preferences (.", "labels": [], "entities": [{"text": "collaborative filtering (CF)", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.6875729620456695}]}, {"text": "To the best of our knowledge, we are the first to model both topics and discourse modes as part of a CF framework and apply it to microblog conversation recommendation.", "labels": [], "entities": [{"text": "microblog conversation recommendation", "start_pos": 130, "end_pos": 167, "type": "TASK", "confidence": 0.6138190229733785}]}, {"text": "Experimental results on two Twitter conversation datasets show that our proposed model yields significantly better performance than state-of-theart post-level recommendation systems.", "labels": [], "entities": []}, {"text": "For example, by leveraging both topical content and discourse structure, our model achieves a mean average precision (MAP) of 0.76 on conversations about the U.S. presidential election, compared with 0.70 by, which only considers topics.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 94, "end_pos": 122, "type": "METRIC", "confidence": 0.9087073107560476}]}, {"text": "We further con-ducted detailed analysis on the latent topics and discourse modes and find that our model can discover reasonable topic and discourse representations, which play an important role in characterizing reply behaviors.", "labels": [], "entities": []}, {"text": "Finally, we also provide a pilot study on recommendation for first time replies, which shows that our model outperforms comparable recommendation systems.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "The related work is discussed in Section 2.", "labels": [], "entities": []}, {"text": "We then present our microblog conversation recommendation model in Section 3.", "labels": [], "entities": [{"text": "microblog conversation recommendation", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.5866752564907074}]}, {"text": "The experimental setup and results are described in Sections 4 and 5.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected two microblog conversation datasets from Twitter for experiments : one contains discussions about the U.S. presidential election (henceforth US Election), the other gathers conversations of diverse topics based on the tweets released by TREC 2011 microblog track (henceforth TREC) . US Election was collected from January to June of 2016 using Twitter's Streaming API 5 with a small set of political keywords.", "labels": [], "entities": [{"text": "TREC 2011 microblog track", "start_pos": 250, "end_pos": 275, "type": "DATASET", "confidence": 0.8152457177639008}, {"text": "US Election", "start_pos": 296, "end_pos": 307, "type": "DATASET", "confidence": 0.7681319117546082}]}, {"text": "To recover conversations, Tweet Search API 7 was used to retrieve messages with the \"inreply-to\" relations to collect tweets in a recursive way until full conversations were recovered.", "labels": [], "entities": []}, {"text": "Statistics of the datasets are shown in. displays the number of conversations individual users participated in.", "labels": [], "entities": []}, {"text": "As can be seen, most users are involved in only a few conversations.", "labels": [], "entities": []}, {"text": "Simply leveraging personal chat history will not produce good performance for conversation The datasets are available at http://www.ccs.", "labels": [], "entities": []}, {"text": "neu.edu/home/luwang/ 4 http://trec.nist.gov/data/tweets/ 5 https://developer.twitter.com/ en/docs/tweets/filter-realtime/ api-reference/post-statuses-filter.html 6 Keyword list: \"trump\", \"hillary\", \"clinton\", \"president\", \"politics\", and \"election.\"", "labels": [], "entities": []}, {"text": "7 https://developer.twitter.com/en/ docs/tweets/search/api-reference/ get-saved_searches-show-id recommendation.", "labels": [], "entities": []}, {"text": "In our experiments, we predict whether a user will engage in a conversation given the previous messages in that conversation and past conversations the user is involved.", "labels": [], "entities": []}, {"text": "For model training and testing, we divide conversations into three ordered segments, corresponding to training, development, and test sets at 75%, 12.5%, and 12.5%.", "labels": [], "entities": []}, {"text": "8 Preprocessing and Hyperparameter Tuning.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.9706388115882874}]}, {"text": "For preprocessing, links, mentions (i.e., @user-name), and hashtags in tweets were replaced with generic tags of \"URL\", \"MENTION\", and \"HASHTAG\".", "labels": [], "entities": [{"text": "MENTION", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9725661873817444}, {"text": "HASHTAG", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.9936668276786804}]}, {"text": "We then utilized the Twitter NLP tool) for tokenization and non-alphabetic token removal.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.9829707145690918}, {"text": "non-alphabetic token removal", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.6775574386119843}]}, {"text": "We removed stop words and punctuations for all comparisons to ensure comparable performance.", "labels": [], "entities": []}, {"text": "We maintain a vocabulary with the 5,000 most frequent words.", "labels": [], "entities": []}, {"text": "Our model parameters are tuned on the development set based on grid search, i.e. the parameters that give the lowest value for our objective are selected.", "labels": [], "entities": []}, {"text": "Specifically, the number of discourse modes (D) and topics (K) are tuned to be 10.", "labels": [], "entities": []}, {"text": "The trade-off parameter \u00b5 between user preference and corpus negative log-likelihood takes value of 0.1, and \u03bb, the parameter for balancing topic and discourse, is set to 0.5.", "labels": [], "entities": []}, {"text": "Finally, the confidence parameter s takes a value of 200 to give higher weight for positive instances, i.e., a user replied to a conversation.", "labels": [], "entities": []}, {"text": "Following prior work on social media post recommendation), we treat our task on conversation recommendation as a ranking problem.", "labels": [], "entities": [{"text": "conversation recommendation", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.6980413347482681}]}, {"text": "Therefore, popular information retrieval evaluation metrics, including precision at K (P@K), mean average precision (MAP) (, and normalized Discounted Cumulative Gain at K (nDCG@K)) are reported.", "labels": [], "entities": [{"text": "information retrieval evaluation", "start_pos": 19, "end_pos": 51, "type": "TASK", "confidence": 0.799028476079305}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9990929365158081}, {"text": "mean average precision (MAP)", "start_pos": 93, "end_pos": 121, "type": "METRIC", "confidence": 0.969544380903244}]}, {"text": "The metrics are computed per user in the dataset and then averaged overall users.", "labels": [], "entities": []}, {"text": "The values range from 0.0 to 1.0, with higher values indicating better performance.", "labels": [], "entities": []}, {"text": "conversations randomly (RANDOM); 2) longer conversations (i.e., more words) ranked higher (LENGTH); 3) conversations with more distinct users ranked higher (POPULARITY).", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.8614875078201294}, {"text": "LENGTH", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9968122839927673}, {"text": "POPULARITY", "start_pos": 157, "end_pos": 167, "type": "METRIC", "confidence": 0.8773316144943237}]}, {"text": "We further compare results with three established recommendation models: \u2022 OCCF: one-class Collaborative Filtering (, which only considers users' reply history without modeling content in conversations.", "labels": [], "entities": []}, {"text": "\u2022 RSVM: ranking SVM (Joachims, 2002), which ranks conversations for each user with the content and Twitter features as in.", "labels": [], "entities": []}, {"text": "\u2022 CTR: messages in one conversation are aggregated into one post and a state-of-the art Collaborative Filtering-based post recommendation model is applied).", "labels": [], "entities": []}, {"text": "Finally, we also adapt the \"hidden factors as topics\" (HFT) model proposed in.", "labels": [], "entities": []}, {"text": "Because the original model leverages the ratings for all product reviews and does not handle implicit user feedback well, we replace their user preference objective function with ours (Eq. 2).", "labels": [], "entities": []}, {"text": "In this section, we first discuss our main evaluation in Section 5.1.", "labels": [], "entities": []}, {"text": "A case study and corresponding discussion are provided in Section 5.2 to provide further insights, which is followed by an analysis of the topics and discourse modes discovered by our model (Section 5.3).", "labels": [], "entities": []}, {"text": "We also examine our performance on first time replies (Section 5.4).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of two datasets.", "labels": [], "entities": []}, {"text": " Table 2: Conversation recommendation results on US  Election and TREC. The best result for each column is  highlighted in bold. Our model performs significantly  better than all the comparisons (p < 0.01, paired t- test).", "labels": [], "entities": [{"text": "US  Election and TREC", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.8413587063550949}, {"text": "paired t- test", "start_pos": 206, "end_pos": 220, "type": "METRIC", "confidence": 0.8088552057743073}]}, {"text": " Table 3: Predicted recommendation scores by different  models of U 1 for conversations c 1 and c 2 in", "labels": [], "entities": [{"text": "Predicted recommendation scores", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.8987942735354105}]}, {"text": " Table 5: MAP of different variants of our model. Best  results in each column is in bold.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9408035278320312}]}, {"text": " Table 7: MAP of models considering only first time  replies. Best results in each column is in bold.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.99176424741745}]}]}