{"title": [{"text": "Self-Attentive Residual Decoder for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.7424896558125814}]}], "abstractContent": [{"text": "Neural sequence-to-sequence networks with attention have achieved remarkable performance for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8252309858798981}]}, {"text": "One of the reasons for their effectiveness is their ability to capture relevant source-side contextual information at each time-step prediction through an attention mechanism.", "labels": [], "entities": []}, {"text": "However, the target-side context is solely based on the sequence model which, in practice, is prone to a recency bias and lacks the ability to capture effectively non-sequential dependencies among words.", "labels": [], "entities": []}, {"text": "To address this limitation, we propose a target-side-attentive residual recurrent network for decoding , where attention over previous words contributes directly to the prediction of the next word.", "labels": [], "entities": []}, {"text": "The residual learning facilitates the flow of information from the distant past and is able to emphasize any of the previously translated words, hence it gains access to a wider context.", "labels": [], "entities": []}, {"text": "The proposed model outperforms a neural MT baseline as well as a memory and self-attention network on three language pairs.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9381471872329712}]}, {"text": "The analysis of the attention learned by the decoder confirms that it emphasizes a wider context, and that it captures syntactic-like structures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) has recently become the state-of-the-art approach to machine translation (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8168456057707468}, {"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7759999334812164}]}, {"text": "Several architectures have been proposed for this task), but the attention-based NMT model designed by is still considered the de-facto baseline.", "labels": [], "entities": []}, {"text": "This architecture is composed of two recurrent neural networks (RNNs), an encoder and a decoder, and an attention mechanism between them for modeling a  soft word-alignment.", "labels": [], "entities": []}, {"text": "First, the model encodes the complete source sentence, and then decodes one word at a time.", "labels": [], "entities": []}, {"text": "The decoder has access to all the context on the source side through the attention mechanism.", "labels": [], "entities": []}, {"text": "However, on the target side, the contextual information is represented only through a fixed-length vector, namely the hidden state of the decoder.", "labels": [], "entities": []}, {"text": "As observed by, this creates a bottleneck which hinders the ability of the sequential model to learn longer-term information effectively.", "labels": [], "entities": []}, {"text": "As pointed out by, sequential models present two main problems for natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6724907358487447}]}, {"text": "First, the memory of the encoder is shared across multiple words and is prone to bias towards the recent past.", "labels": [], "entities": []}, {"text": "Second, such models do not fully capture the structural composition of language.", "labels": [], "entities": []}, {"text": "To address these limitations, several recent models have been proposed, namely memory networks () and self-attention networks.", "labels": [], "entities": []}, {"text": "We experimented with these methods, applying them to NMT: memory RNN ( and self-attentive.", "labels": [], "entities": []}, {"text": "How-ever, we observed no significant gains in performance over the baseline architecture.", "labels": [], "entities": []}, {"text": "In this paper, we propose a self-attentive residual recurrent decoder, presented in, which, if unfolded overtime, represents a denselyconnected residual network.", "labels": [], "entities": []}, {"text": "The self-attentive residual connections focus selectively on previously translated words and propagate useful information to the output of the decoder, within an attention-based NMT architecture.", "labels": [], "entities": []}, {"text": "The attention paid to the previously predicted words is analogous to a read-only memory operation, and enables the learning of syntactic-like structures which are useful for the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 178, "end_pos": 194, "type": "TASK", "confidence": 0.9206831455230713}]}, {"text": "Our evaluation on three language pairs shows that the proposed model improves over several baselines, with only a small increase in computational overhead.", "labels": [], "entities": []}, {"text": "In contrast, other similar approaches have lower scores but a higher computational overhead.", "labels": [], "entities": []}, {"text": "The contributions of this paper can be summarized as follows: \u2022 We propose and compare several options for using self-attentive residual learning within a standard decoder, which facilitates the flow of contextual information on the target side.", "labels": [], "entities": []}, {"text": "\u2022 We demonstrate consistent improvements over a standard baseline, and two advanced variants, which make use of memory and self-attention on three language pairs (English-to-Chinese, Spanish-to-English, and English-to-German).", "labels": [], "entities": [{"text": "memory", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9652444124221802}]}, {"text": "\u2022 We perform an ablation study and analyze the learned attention function, providing additional insights on its actual contributions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the proposed MT models in different conditions, we select three language pairs with increasing amounts of training data: EnglishChinese (0.5M sentence pairs), Spanish-English (2.1M), and English-German (4.5M).", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9870201349258423}]}, {"text": "For English-to-Chinese, we use a subset of the UN parallel corpus), corresponding to Europarl v7 and News Commentary v11 with ca.", "labels": [], "entities": [{"text": "UN parallel corpus", "start_pos": 47, "end_pos": 65, "type": "DATASET", "confidence": 0.8222039341926575}, {"text": "Europarl", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.9864502549171448}]}, {"text": "Newstest2012 and Newstest2013 were used for development and testing respectively.", "labels": [], "entities": [{"text": "Newstest2012", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9734089970588684}, {"text": "Newstest2013", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9696658253669739}]}, {"text": "Finally, we use the complete English-to-German set from WMT 2016 () with a total of ca.", "labels": [], "entities": [{"text": "English-to-German set from WMT 2016", "start_pos": 29, "end_pos": 64, "type": "DATASET", "confidence": 0.7152307391166687}]}, {"text": "The development set is Newstest2013, and the testing set is Newstest2014.", "labels": [], "entities": [{"text": "Newstest2013", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.9711201786994934}, {"text": "Newstest2014", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.9890856146812439}]}, {"text": "Additionally, we include as testing sets Newstest2015 and Newstest2016, for comparison with the state of the art.", "labels": [], "entities": [{"text": "Newstest2015", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.9594069123268127}, {"text": "Newstest2016", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9455441832542419}]}, {"text": "We report translation quality using (a) BLEU over tokenized and truecased texts, and (b) NIST BLEU over detokenized and detruecased texts 2 .  Manual evaluation on samples of 50 sentences for each language pair helped to corroborate the conclusions obtained from the BLEU scores, and to provide a qualitative understanding of the improvements brought by our model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9973452687263489}, {"text": "NIST", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.8108533620834351}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.7171316146850586}, {"text": "BLEU", "start_pos": 267, "end_pos": 271, "type": "METRIC", "confidence": 0.9955012202262878}]}, {"text": "For each language, we employed one evaluator who was a native speaker of the target language and had good knowledge of the source language.", "labels": [], "entities": []}, {"text": "The evaluators ranked three translations of the same source sentence -one from each of our models: baseline, mean residual connections, and self-attentive residual connections -according to their translation quality.", "labels": [], "entities": []}, {"text": "The three translations were presented in a random order, so that the system that had generated them could not be identified.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU score (multi-bleu) on tokenized text.  The highest score per dataset is marked in bold. The  self-attentive residual connections make use of the con- tent attention function. |\u0398| indicates the number of pa- rameters per model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.974231094121933}]}, {"text": " Table 2: BLEU score (multi-bleu) on tokenized text for  English-to-German on Newstest (NT) 2014, and 2015.  The highest score per dataset is marked in bold. The  self-attentive residual connections makes use of the  content attention function.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9790585041046143}, {"text": "Newstest (NT) 2014", "start_pos": 78, "end_pos": 96, "type": "DATASET", "confidence": 0.878928256034851}]}, {"text": " Table 3: NIST BLEU scores on detokenized and de- truecased text for English-to-German on Newstest (NT)", "labels": [], "entities": [{"text": "NIST", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7059198021888733}, {"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9160956144332886}, {"text": "Newstest (NT)", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.9001429975032806}]}, {"text": " Table 4: BLEU scores for two scoring variants of the  attention function of the proposed decoder.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989724159240723}]}, {"text": " Table 5: Human evaluation of sentence-level transla- tion quality on three language pairs. We compare the  models in pairs, indicating the percentages of sentences  that were ranked higher (>), equal to (=), or lower (<)  for the first system with respect to the second one. The  values correspond to percentages (%).", "labels": [], "entities": []}, {"text": " Table 6: Evaluation of the proposed methods on lan- guage modeling. The number of parameter for all mod- els is 47M.", "labels": [], "entities": []}]}