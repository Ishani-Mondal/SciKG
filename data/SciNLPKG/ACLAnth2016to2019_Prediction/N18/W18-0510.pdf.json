{"title": [{"text": "Annotating Picture Description Task Responses for Content Analysis", "labels": [], "entities": [{"text": "Annotating Picture Description Task Responses", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8174529433250427}, {"text": "Content Analysis", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.7374404817819595}]}], "abstractContent": [{"text": "Given that all users of a language can be creative in their language usage, the overar-ching goal of this work is to investigate issues of variability and acceptability in written text, for both non-native speakers (NNSs) and native speakers (NSs).", "labels": [], "entities": []}, {"text": "We control for meaning by collecting a dataset of picture description task (PDT) responses from a number of NSs and NNSs, and we define and annotate a handful of features pertaining to form and meaning, to capture the multi-dimensional ways in which responses can vary and can be acceptable.", "labels": [], "entities": [{"text": "picture description task (PDT) responses", "start_pos": 50, "end_pos": 90, "type": "TASK", "confidence": 0.7284639733178275}]}, {"text": "By examining the decisions made in this corpus development, we highlight the questions facing anyone working with learner language properties like variability, acceptability and native-likeness.", "labels": [], "entities": []}, {"text": "We find reliable inter-annotator agreement, though disagreements point to difficult areas for establishing a link between form and meaning.", "labels": [], "entities": []}], "introductionContent": [{"text": "The (written) data of second language learners poses many challenges, whether it is being analyzed for grammatical errors (), for linguistic patterns, for content analysis, or for interactions with intelligent computer-assisted language learning (ICALL) systems.", "labels": [], "entities": [{"text": "content analysis", "start_pos": 155, "end_pos": 171, "type": "TASK", "confidence": 0.7272761762142181}]}, {"text": "One of the core issues in doing anything with learner data is the inherent amount of variability in how linguistic forms are used to convey meaning (cf., e.g.,.", "labels": [], "entities": []}, {"text": "It may indeed seem like learners can use an infinite variety of forms to express a particular meaning; here we attempt to investigate how large the problem of variability in one particular testing context is for computational processing.", "labels": [], "entities": []}, {"text": "To investigate variability and the mappings between linguistic form and meaning, in this paper we control for meaning by collecting a dataset of picture description task (PDT) responses from a number of NSs and NNSs, and we annotate a handful of features, thereby capturing the multifaceted ways in which responses can vary and can be acceptable or unacceptable.", "labels": [], "entities": []}, {"text": "We call this the SAILS Corpus, for Semantic Analysis of Imagebased Learner Sentences-our intended use.", "labels": [], "entities": [{"text": "SAILS Corpus", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.7800260186195374}, {"text": "Semantic Analysis of Imagebased Learner Sentences-our", "start_pos": 35, "end_pos": 88, "type": "TASK", "confidence": 0.8621435662110647}]}, {"text": "By examining the decisions made in this corpus development, we highlight the questions facing anyone working with learner language properties such as variability, acceptability and native-likeness.", "labels": [], "entities": []}, {"text": "Given the form-meaning aspect of variability, we are interested in how variable linguistic behavior is for the same content, both within and between NS and NNS groups, and the potential use of NS responses to evaluate NNS responses.", "labels": [], "entities": []}, {"text": "There is a long-standing notion that systems processing learner data would be wise to constrain the data in someway (e.g.,), but we do not know how much constraint is needed-or whether we sacrifice the possibility of observing particular learner behavior for the sake of a constraintwithout knowing more about the ways in which variation happens (cf..", "labels": [], "entities": []}, {"text": "The corpus presented here bears some similarities to other task-based learner corpora.", "labels": [], "entities": []}, {"text": "examined German learner responses to short-answer reading comprehension questions.", "labels": [], "entities": []}, {"text": "A target answer was produced by an expert, and annotators used this target to label the meaning of responses as corrector incorrect, along with a more detailed set of labels related to form, meaning, and task appropriateness.", "labels": [], "entities": []}, {"text": "In our own previous work, we annotated a small set of PDT responses as corrector incorrect, with incorrect responses further labeled as errors of form or meaning.", "labels": [], "entities": []}, {"text": "presented work on PDT re-sponses in which respondents used provided vocabulary words.", "labels": [], "entities": []}, {"text": "Responses were manually annotated on a holistic four point scale, and a set of five features (relating to meaning, relevance and language use) were calculated based on statistical assumptions.", "labels": [], "entities": []}, {"text": "performed a nearly identical analysis with transcribed texts from a six-picture narration task, but neither of these datasets is publicly available.", "labels": [], "entities": [{"text": "six-picture narration task", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.7606610655784607}]}, {"text": "Our work reverses this mapping by providing manually annotated features, which we hope will be useful for mapping to holistic scores.", "labels": [], "entities": []}, {"text": "For example, a response may present the main content of an item correctly but add imaginary details, while another may address background information not asked about in the prompt (see section 3).", "labels": [], "entities": []}, {"text": "The acceptability of a response is thus taken as a function of several interacting features, most of which relate the text to the known semantic content.", "labels": [], "entities": []}, {"text": "Relating to known content is distinct from typical grammatical error correction (GEC) () and from more linguistically driven work such as parsing (e.g.,), but providing the dimensions of acceptability and elucidating how they are applied provides insight for any enterprise desiring to connect learner text with semantic content, in addition to unpacking the sources of variation and of difficulty in processing a range of learner data.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 51, "end_pos": 85, "type": "TASK", "confidence": 0.7922116617361704}]}, {"text": "In section 2 we outline the picture description task (PDT) we use, designed with items that elicit specific types of linguistic behavior.", "labels": [], "entities": [{"text": "picture description task (PDT)", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.8042679528395335}]}, {"text": "Section 3 outlines the annotation, tackling the five-dimensional scheme; inter-anntotator agreement results are in section 4.", "labels": [], "entities": []}, {"text": "While agreement seems reliable, highlighting areas of disagreement showcases difficult areas for establishing a link between form and meaning (cf., e.g.,).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Test sample items and example responses  with Core Event annotations from Annotators 1 and 2.", "labels": [], "entities": []}, {"text": " Table 2: NS and NNS type-to-token ratios (TTR) for  complete responses (not words), for all the data.", "labels": [], "entities": [{"text": "NNS type-to-token ratios (TTR)", "start_pos": 17, "end_pos": 47, "type": "METRIC", "confidence": 0.6722357322772344}]}, {"text": " Table 3: Targeted and untargeted sample responses  from the development set transitive item, shown with  adjudicated annotations for the five features: core event  (C), verifiability (V), answerhood (A), interpretability  (I) and grammaticality (G).", "labels": [], "entities": []}, {"text": " Table 4: Agreement scores broken down by different properties of the test set: total annotations (Total), yes anno- tations for Annotator 1 and 2 (A1Yes, A2Yes), average yes annotations (AvgYes), total expected chance agreement  for yeses and nos (Chance), actual raw agreement (Agree) and Cohen's kappa (Kappa).", "labels": [], "entities": [{"text": "actual raw agreement (Agree)", "start_pos": 258, "end_pos": 286, "type": "METRIC", "confidence": 0.6944668193658193}]}, {"text": " Table 5.  Comparing the average rate of yes annotations  shows that the NNSs outperform the NSs by be- tween roughly 8% and 12% on all features ex- cept grammaticality. It is not surprising that  NSs outperform NNSs on this feature (90.2% to  79.3%), but to account for their superior perfor- mance on the other features, one must consider  the fact that the NNSs were recruited from ESL  courses and performed the task with peers and re- searchers present. The NNSs were more likely to  make a good faith effort than the NSs, the major- ity of whom performed the task anonymously and  remotely. Furthermore, with twice as many re- sponses to provide for each item for NSs, fatigue  and boredom may have been a contributing factor.", "labels": [], "entities": []}, {"text": " Table 5: NS and NNS test set responses: average yes  annotations (AvgYes) and Cohen's kappa (Kappa).", "labels": [], "entities": [{"text": "AvgYes", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.7918500304222107}, {"text": "Cohen's kappa (Kappa)", "start_pos": 79, "end_pos": 100, "type": "METRIC", "confidence": 0.6806244601806005}]}]}