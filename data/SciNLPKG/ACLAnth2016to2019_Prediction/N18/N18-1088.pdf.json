{"title": [], "abstractContent": [{"text": "We study the problem of analyzing tweets with Universal Dependencies (UD; Nivre et al., 2016).", "labels": [], "entities": [{"text": "UD; Nivre et al., 2016)", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.8448320105671883}]}, {"text": "We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-of-speech tagging, and labeled dependencies.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.6922023743391037}]}, {"text": "Using the extended guidelines, we create anew tweet treebank for English (TWEEBANK V2) that is four times larger than the (unlabeled) TWEEBANK V1 introduced by Kong et al.", "labels": [], "entities": []}, {"text": "We characterize the disagreements between our annotators and show that it is challenging to deliver consistent annotation due to ambiguity in understanding and explaining tweets.", "labels": [], "entities": []}, {"text": "Nonetheless, using the new treebank, we build a pipeline system to parse raw tweets into UD.", "labels": [], "entities": []}, {"text": "To overcome annotation noise without sacrificing computational efficiency, we propose anew method to distill an ensemble of 20 transition-based parsers into a single one.", "labels": [], "entities": []}, {"text": "Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-of-the-art on other treebanks in both accuracy and speed.", "labels": [], "entities": [{"text": "LAS", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.84654301404953}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.999344527721405}, {"text": "speed", "start_pos": 170, "end_pos": 175, "type": "METRIC", "confidence": 0.9795883893966675}]}], "introductionContent": [{"text": "NLP for social media messages is challenging, requiring domain adaptation and annotated datasets (e.g., treebanks) for training and evaluation.", "labels": [], "entities": []}, {"text": "Pioneering work by Foster et al.", "labels": [], "entities": []}, {"text": "(2011) annotated 7,630 tokens' worth of tweets according to the phrase-structure conventions of the Penn Treebank (PTB;, enabling conversion to Stanford Dependencies.", "labels": [], "entities": [{"text": "Penn Treebank (PTB;", "start_pos": 100, "end_pos": 119, "type": "DATASET", "confidence": 0.974924373626709}, {"text": "Stanford Dependencies", "start_pos": 144, "end_pos": 165, "type": "DATASET", "confidence": 0.8676778972148895}]}, {"text": "further studied the challenges in annotating tweets and presented a tweet treebank (TWEEBANK), consisting of 12,149 tokens and largely following conventions suggested by , fairly close to dependencies (without labels).", "labels": [], "entities": [{"text": "TWEEBANK", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.6142342686653137}]}, {"text": "Both annotation efforts were highly influenced by the PTB, whose guidelines have good grammatical coverage on newswire.", "labels": [], "entities": [{"text": "PTB", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9787847399711609}]}, {"text": "However, when it comes to informal, unedited, user-generated text, the guidelines may leave many annotation decisions unspecified.", "labels": [], "entities": []}, {"text": "Universal Dependencies ( were introduced to enable consistent annotation across different languages.", "labels": [], "entities": []}, {"text": "To allow such consistency, UD was designed to be adaptable to different genres () and languages (.", "labels": [], "entities": []}, {"text": "We propose that analyzing the syntax of tweets can benefit from such adaptability.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew English tweet treebank of 55,607 tokens that follows the UD guidelines, but also contends with social media-specific challenges that were not covered by UD guidelines.", "labels": [], "entities": [{"text": "English tweet treebank", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.6165816684563955}, {"text": "UD", "start_pos": 90, "end_pos": 92, "type": "DATASET", "confidence": 0.861238420009613}]}, {"text": "1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies.", "labels": [], "entities": []}, {"text": "We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines.", "labels": [], "entities": []}, {"text": "Based on these annotations, we nonetheless designed a pipeline to parse raw tweets into Universal Dependencies.", "labels": [], "entities": []}, {"text": "Our pipeline includes: a bidirectional LSTM (bi-LSTM) tokenizer, a word cluster-enhanced POS tagger (following, and a stack LSTM parser with character-based word representations (, which we refer to as our \"baseline\" parser.", "labels": [], "entities": [{"text": "word cluster-enhanced POS tagger", "start_pos": 67, "end_pos": 99, "type": "TASK", "confidence": 0.5103848092257977}]}, {"text": "To overcome the noise in our annotated data and achieve better performance without sacrificing computational efficiency, we distill a 20-parser ensemble into a single greedy parser).", "labels": [], "entities": []}, {"text": "We show further that learning directly from the exploration of the ensemble parser is more beneficial than learning from the gold standard \"oracle\" transition sequence.", "labels": [], "entities": []}, {"text": "Experimental results show that an improvement of more than 2.2 points in LAS over the baseline parser can be achieved with our distillation method.", "labels": [], "entities": [{"text": "LAS", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.871958315372467}]}, {"text": "It outperforms other state-of-the-art parsers in both accuracy and speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9995911717414856}, {"text": "speed", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9943599104881287}]}, {"text": "The contributions of this paper include: \u2022 We study the challenges of annotating tweets in UD ( \u00a72) and create anew tweet treebank (TWEEBANK V2), which includes tokenization, part-of-speech tagging, and labeled Universal Dependencies.", "labels": [], "entities": [{"text": "UD", "start_pos": 91, "end_pos": 93, "type": "DATASET", "confidence": 0.8524776697158813}]}, {"text": "We also characterize the difficulties of creating such annotation.", "labels": [], "entities": []}, {"text": "\u2022 We introduce and evaluate a pipeline system to parse the raw tweet text into Universal Dependencies ( \u00a73).", "labels": [], "entities": []}, {"text": "Experimental results show that it performs better than a pipeline of the state-of-the-art alternatives.", "labels": [], "entities": []}, {"text": "\u2022 We propose anew distillation method for training a greedy parser, leading to better performance than existing methods and without efficiency sacrifices.", "labels": [], "entities": []}, {"text": "Our dataset and system are publicly available at https://github.com/Oneplus/Tweebank and https://github.com/Oneplus/twpipe.", "labels": [], "entities": [{"text": "Oneplus/Tweebank", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.8361520965894064}, {"text": "Oneplus", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9311376810073853}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Proportions of non-syntactic tokens in our  annotation. These statistics are obtained on 140  character-limited tweets.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of TWEEBANK V2 and comparison  with TWEEBANK V1.", "labels": [], "entities": [{"text": "TWEEBANK V2", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.8694147169589996}, {"text": "TWEEBANK V1", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9008407890796661}]}, {"text": " Table 3: Tokenizer comparison on the TWEEBANK V2  test set.", "labels": [], "entities": [{"text": "TWEEBANK V2  test set", "start_pos": 38, "end_pos": 59, "type": "DATASET", "confidence": 0.9339551031589508}]}, {"text": " Table 4: POS tagger comparison on gold-standard to- kens in the TWEEBANK V2 test set.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.6642469465732574}, {"text": "TWEEBANK V2 test set", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.9261194318532944}]}, {"text": " Table 6: Dependency parser comparison on TWEE- BANK V2 test set, with automatic POS tags. We use", "labels": [], "entities": [{"text": "Dependency parser", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8264660835266113}, {"text": "TWEE- BANK V2 test set", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.8486681083838145}]}, {"text": " Table 7: Evaluating our pipeline against a state-of-the- art pipeline.", "labels": [], "entities": []}]}