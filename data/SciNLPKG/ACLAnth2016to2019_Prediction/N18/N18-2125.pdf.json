{"title": [{"text": "Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning", "labels": [], "entities": [{"text": "Video Captioning", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7070785909891129}]}], "abstractContent": [{"text": "A major challenge for video captioning is to combine audio and visual cues.", "labels": [], "entities": [{"text": "video captioning", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7592986524105072}]}, {"text": "Existing multi-modal fusion methods have shown encouraging results in video understanding.", "labels": [], "entities": [{"text": "video understanding", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8286277055740356}]}, {"text": "However, the temporal structures of multiple modalities at different granularities are rarely explored, and how to selectively fuse the multi-modal representations at different levels of details remains uncharted.", "labels": [], "entities": []}, {"text": "In this paper , we propose a novel hierarchically aligned cross-modal attention (HACA) framework to learn and selectively fuse both global and local temporal dynamics of different modalities.", "labels": [], "entities": []}, {"text": "Furthermore, for the first time, we validate the superior performance of the deep audio features on the video captioning task.", "labels": [], "entities": [{"text": "video captioning task", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.7736501594384512}]}, {"text": "Finally , our HACA model significantly outper-forms the previous best systems and achieves new state-of-the-art results on the widely used MSR-VTT dataset.", "labels": [], "entities": [{"text": "MSR-VTT dataset", "start_pos": 139, "end_pos": 154, "type": "DATASET", "confidence": 0.9312939345836639}]}], "introductionContent": [{"text": "Video captioning, the task of automatically generating a natural-language description of a video, is a crucial challenge in both NLP and vision communities.", "labels": [], "entities": [{"text": "Video captioning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6570456326007843}]}, {"text": "In addition to visual features, audio features can also play a key role in video captioning.", "labels": [], "entities": [{"text": "video captioning", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7132462561130524}]}, {"text": "shows an example where the caption system made a mistake analyzing only visual features.", "labels": [], "entities": []}, {"text": "In this example, it could be very hard even fora human to correctly determine if the girl is singing or talking by only watching without listening.", "labels": [], "entities": []}, {"text": "Thus to describe the video content accurately, a good understanding of the audio signature is a must.", "labels": [], "entities": []}, {"text": "In the multi-modal fusion domain, many approaches attempted to jointly learn temporal features from multiple modalities (), such as feature-level (early) fusion, decision-level (late) fusion, model-level fusion (, and attention fusion (Chen Ground Truth: A girl is singing.", "labels": [], "entities": [{"text": "attention fusion", "start_pos": 218, "end_pos": 234, "type": "TASK", "confidence": 0.7127807289361954}]}, {"text": "A girl sings to a song.", "labels": [], "entities": []}, {"text": "Video Only: A woman is talking in a room.", "labels": [], "entities": []}, {"text": "Video + Audio: A girl is singing a song. and, etc.", "labels": [], "entities": []}, {"text": "But these techniques do not learn the cross-modal attention and thus fail to selectively attend to a certain modality when producing the descriptions.", "labels": [], "entities": []}, {"text": "Another issue is that little efforts have been exerted on utilizing temporal transitions of the different modalities with varying analysis granularities.", "labels": [], "entities": []}, {"text": "The temporal structures of a video are inherently layered since the video usually contains temporally sequential activities (e.g. a video where a person reads a book, then throws it on the table. Next, he pours a glass of milk and drinks it).", "labels": [], "entities": []}, {"text": "There are strong temporal dependencies among those activities.", "labels": [], "entities": []}, {"text": "Meanwhile, to understand each of them requires understanding many action components (e.g., pouring a glass of milk is a complicated action sequence).", "labels": [], "entities": []}, {"text": "Therefore we hypothesize that it is beneficial to learn and align both the high-level (global) and low-level (local) temporal transitions of multiple modalities.", "labels": [], "entities": []}, {"text": "Moreover, prior work only employed handcrafted audio features (e.g. MFCC) for video captioning (.", "labels": [], "entities": [{"text": "video captioning", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7354127466678619}]}, {"text": "While deep audio features have shown superior performance on some audio processing tasks like audio event classification, their use in video captioning needs to be validated.: Overview of our HACA framework.", "labels": [], "entities": [{"text": "audio event classification", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.6505661209424337}, {"text": "video captioning", "start_pos": 135, "end_pos": 151, "type": "TASK", "confidence": 0.7486293911933899}, {"text": "HACA framework", "start_pos": 192, "end_pos": 206, "type": "DATASET", "confidence": 0.8129364848136902}]}, {"text": "Note that in the encoding stage, for the sake of simplicity, the step size of high-level LSTM in both hierarchical attentive encoders is 2 here, but in practice usually they are set much longer.", "labels": [], "entities": []}, {"text": "In the decoding stage, we only show the computations of the time step t (the decoders have the same behavior at other time steps).", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel hierarchically aligned cross-modal attentive network (HACA) to learn and align both global and local contexts among different modalities of the video.", "labels": [], "entities": []}, {"text": "The goal is to overcome the issues mentioned above and generate better descriptions of the input videos.", "labels": [], "entities": []}, {"text": "Our contributions are fourfold: (1) we invent a hierarchical encoder-decoder network to adaptively learn the attentive representations of multiple modalities, including visual attention, audio attention, and decoder attention; (2) our proposed model is capable of aligning and fusing both the global and local contexts of different modalities for video understanding and sentence generation; (3) we are the first to utilize deep audio features for video captioning and empirically demonstrate its effectiveness over hand-crafted MFCC features; and (4) we achieve the new state of the art on the MSR-VTT dataset.", "labels": [], "entities": [{"text": "video understanding", "start_pos": 347, "end_pos": 366, "type": "TASK", "confidence": 0.7299504578113556}, {"text": "sentence generation", "start_pos": 371, "end_pos": 390, "type": "TASK", "confidence": 0.7185379862785339}, {"text": "video captioning", "start_pos": 448, "end_pos": 464, "type": "TASK", "confidence": 0.7798526287078857}, {"text": "MSR-VTT dataset", "start_pos": 595, "end_pos": 610, "type": "DATASET", "confidence": 0.9532159864902496}]}, {"text": "Among the network architectures for video captioning (, sequence-to-sequence models () have shown promising results.", "labels": [], "entities": [{"text": "video captioning", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7196562886238098}]}, {"text": "Pan et al. introduced a hierarchical recurrent encoder to capture the temporal visual features at different levels.", "labels": [], "entities": []}, {"text": "proposed a hierarchical decoder for paragraph generation, and most recently invented a hierarchical reinforced framework to generate the caption phrase by phrase.", "labels": [], "entities": [{"text": "paragraph generation", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.8937612473964691}]}, {"text": "But none had tried to model and align the global and local contexts of different modalities as we do.", "labels": [], "entities": []}, {"text": "Our HACA model does only learn the representations of different modalities at different granularities, but also align and dynamically fuse them both globally and locally with hierarchically aligned cross-modal attentions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on the MSR-VTT dataset.", "labels": [], "entities": [{"text": "MSR-VTT dataset", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.9604659676551819}]}, {"text": " Table 2: Performance of the cross-modal attention  model with various audio features.", "labels": [], "entities": []}]}