{"title": [{"text": "Inducing a Lexicon of Abusive Words -A Feature-Based Approach", "labels": [], "entities": [{"text": "Inducing a Lexicon of Abusive Words", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8315383891264597}]}], "abstractContent": [{"text": "We address the detection of abusive words.", "labels": [], "entities": [{"text": "detection of abusive words", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.8689073622226715}]}, {"text": "The task is to identify such words among a set of negative polar expressions.", "labels": [], "entities": []}, {"text": "We propose novel features employing information from both corpora and lexical resources.", "labels": [], "entities": []}, {"text": "These features are calibrated on a small manually annotated base lexicon which we use to produce a large lexicon.", "labels": [], "entities": []}, {"text": "We show that the word-level information we learn cannot be equally derived from a large dataset of annotated mi-croposts.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our (domain-independent) lexicon in the cross-domain detection of abusive microposts.", "labels": [], "entities": [{"text": "cross-domain detection of abusive microposts", "start_pos": 76, "end_pos": 120, "type": "TASK", "confidence": 0.8178163647651673}]}], "introductionContent": [{"text": "Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person.", "labels": [], "entities": []}, {"text": "Examples are (1)-(3).", "labels": [], "entities": []}, {"text": "In the literature, closely related terms include hate speech ( or cyber bullying (.", "labels": [], "entities": []}, {"text": "While there maybe nuanced differences in meaning 2 , they are all compatible with the general definition above for abusive language.", "labels": [], "entities": []}, {"text": "(1) stop editing this, you dumbass.", "labels": [], "entities": []}, {"text": "(2) Just want to slap the stupid out of these bimbos!!!", "labels": [], "entities": []}, {"text": "(3) Go lick a pig you arab muslim piece of scum.", "labels": [], "entities": []}, {"text": "Due to the rise of user-generated web content, in particular on social media networks, the amount of abusive language is also steadily growing.", "labels": [], "entities": []}, {"text": "NLP methods are required to focus human review efforts towards the most relevant microposts.", "labels": [], "entities": []}, {"text": "In this paper, we address the task of detecting abusive words (e.g. dumbass, bimbo, scum).", "labels": [], "entities": []}, {"text": "Our main assumption is that abusive words form a subset of negative polar expressions.", "labels": [], "entities": []}, {"text": "The classification task is to filter the abusive words from a given set of negative polar expressions.", "labels": [], "entities": []}, {"text": "On abase lexicon that is a small subset of negative polar expressions where the abusive words among them have been marked via crowdsourcing ( \u00a73), we calibrate a supervised classifier by examining various novel features ( \u00a74).", "labels": [], "entities": []}, {"text": "A classifier trained on that base lexicon, which contains 551 abusive words, is then applied to a very large list of unlabeled negative polar expressions (from Wiktionary) to extract an expanded lexicon of 2989 abusive words ( \u00a75).", "labels": [], "entities": []}, {"text": "We extrinsically evaluate our new lexicon in the novel task of cross-domain classification of abusive documents ( \u00a76) where we use it as a highlevel feature.", "labels": [], "entities": [{"text": "cross-domain classification of abusive documents", "start_pos": 63, "end_pos": 111, "type": "TASK", "confidence": 0.853578519821167}]}, {"text": "In this work, we consider microposts as documents.", "labels": [], "entities": []}, {"text": "While for in-domain classification, supervised classifiers trained on generic features, such as bag of words or word embeddings, usually score very well, on cross-domain classification they perform poorly since they latch onto domain-specific information.", "labels": [], "entities": [{"text": "cross-domain classification", "start_pos": 157, "end_pos": 184, "type": "TASK", "confidence": 0.7400745153427124}]}, {"text": "In subjectivity, polarity and emotion classification, high-level features based on predictive domain-independent word lists have been proposed to bridge the domain mismatch (.", "labels": [], "entities": [{"text": "emotion classification", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7317754030227661}]}, {"text": "New abusive words constantly enter natural language.", "labels": [], "entities": []}, {"text": "For example, according to Wiktionary 4 the word gimboid, which refers to an incompetent person, was coined in the British television series Red Dwarf, possibly from the word gimp and the suffix -oid.", "labels": [], "entities": []}, {"text": "According to Urban Dictionary , the word twunt, which is a portmanteau of the swearwords twat and cunt, has been invented by humourist Chris Morris for the Channel 4 series 'Jam' in 2000.", "labels": [], "entities": [{"text": "Urban Dictionary", "start_pos": 13, "end_pos": 29, "type": "DATASET", "confidence": 0.9269160330295563}, {"text": "Channel 4 series 'Jam' in 2000", "start_pos": 156, "end_pos": 186, "type": "DATASET", "confidence": 0.6556144766509533}]}, {"text": "One of the most recent abusive words is remoaner which describes someone who complains about or rejects the outcome of the 2016 EU referendum on the UK's membership of the European Union.", "labels": [], "entities": []}, {"text": "It is a blend of moan and remainer.", "labels": [], "entities": []}, {"text": "Wiktionary states that this word has a pejorative connotation.", "labels": [], "entities": []}, {"text": "These examples show that the task of creating a lexicon of abusive words cannot be reduced to a one-time manual annotation effort.", "labels": [], "entities": []}, {"text": "Recent web corpora and crowdsourced dictionaries (e.g. Wiktionary) should be ideal resources to find evidence of such words.", "labels": [], "entities": []}, {"text": "Our contribution is that we present the first work that systematically describes the automatic construction of a lexicon of abusive words.", "labels": [], "entities": [{"text": "automatic construction of a lexicon of abusive words", "start_pos": 85, "end_pos": 137, "type": "TASK", "confidence": 0.7419610470533371}]}, {"text": "We examine novel features derived from various textual resources.", "labels": [], "entities": []}, {"text": "We show that the information we learn cannot be equally derived from a large dataset with labeled microposts.", "labels": [], "entities": []}, {"text": "The effectiveness of our expanded lexicon is demonstrated on cross-domain detection of abusive microposts.", "labels": [], "entities": [{"text": "cross-domain detection of abusive microposts", "start_pos": 61, "end_pos": 105, "type": "TASK", "confidence": 0.7953347563743591}]}, {"text": "This is also the first work to address this task in general.", "labels": [], "entities": []}, {"text": "The supplementary material to this paper 6 includes all resources newly created for our research.", "labels": [], "entities": []}, {"text": "We frame our task as a binary classification problem.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.7005316913127899}]}, {"text": "Each given expression is to be classified as either abusive or not.", "labels": [], "entities": []}, {"text": "We study this problem on English.", "labels": [], "entities": []}, {"text": "However, many of our features should also be applicable to other languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on our base lexicon (Table 1) and report macro-average precision, recall and f-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9276151061058044}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9995846152305603}, {"text": "f-score", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9782333374023438}]}, {"text": "SVMs are evaluated on a 10-fold crossvalidation.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.7205246090888977}]}, {"text": "shows the performance of SVMs using different linguistic features ( \u00a74.1).", "labels": [], "entities": []}, {"text": "Among the three intensity types, the most effective one is the person-based intensity (INT person ).", "labels": [], "entities": [{"text": "person-based intensity (INT person )", "start_pos": 63, "end_pos": 99, "type": "METRIC", "confidence": 0.7548759877681732}]}, {"text": "However, it can be effectively combined with the remaining types.", "labels": [], "entities": []}, {"text": "Among the lexical sentiment resources used (i.e. NRC, INT bin and VIEW), VIEW is most effective.", "labels": [], "entities": [{"text": "NRC", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8964360356330872}, {"text": "INT bin", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.6032845377922058}, {"text": "VIEW", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.8478111624717712}, {"text": "VIEW", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.7870185971260071}]}, {"text": "Their combination also results in an improvement.", "labels": [], "entities": []}, {"text": "The surface patterns (PAT) are surprisingly predictive.", "labels": [], "entities": []}, {"text": "Of the general-purpose lexical resources (i.e. WN, WK and FN), WN and WK are both very effective resources.", "labels": [], "entities": []}, {"text": "Glosses from WN are the strongest individual feature.", "labels": [], "entities": [{"text": "Glosses", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.979592502117157}, {"text": "WN", "start_pos": 13, "end_pos": 15, "type": "DATASET", "confidence": 0.7567400336265564}]}, {"text": "Combining WK, WN and FN results in significant improvement.", "labels": [], "entities": [{"text": "FN", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9606274366378784}]}, {"text": "The best feature set combines all features.", "labels": [], "entities": []}, {"text": "Our results also suggest that for languages other than English, there are some very strong features, such as PAT, WK or embeddings, that could be easily adopted since they do not depend on a resource which is only available in English.  are identified by applying to the vocabulary of Wiktionary an SVM trained on the words from the Subjectivity Lexicon with their respective polarities.", "labels": [], "entities": [{"text": "PAT", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9955710768699646}]}, {"text": "As features, we use word embeddings ( \u00a74.2).", "labels": [], "entities": []}, {"text": "In order to produce the feature-based lexicon of abusive words another SVM is trained on our base lexicon) using the best feature set from.", "labels": [], "entities": []}, {"text": "With 2989 abusive words, our expanded lexicon is 5 times as large as the base lexicon.", "labels": [], "entities": []}, {"text": "In order to measure the impact of our proposed features on the quality of the resulting lexicon, we devised an alternative expansion which just employs word embeddings.", "labels": [], "entities": []}, {"text": "For this, we used SentProp, the most effective induction method from the SocialSent package ( 6 Cross-domain Classification", "labels": [], "entities": [{"text": "SocialSent package", "start_pos": 73, "end_pos": 91, "type": "DATASET", "confidence": 0.9442796409130096}]}], "tableCaptions": [{"text": " Table 1: The base lexicon: 1650 entries in total of  which 551 are abusive.", "labels": [], "entities": []}, {"text": " Table 3: Percentage of abusive/not abusive instances  among (binary) intensity and views.", "labels": [], "entities": []}, {"text": " Table 6: Different classifiers on base lexicon (Table 1).", "labels": [], "entities": []}, {"text": " Table 7: Performance of the different linguistic features  on base lexicon (Table 1).", "labels": [], "entities": []}, {"text": " Table 8: Datasets comprising labeled microposts.", "labels": [], "entities": []}, {"text": " Table 9: Lexicons used in cross-domain classification  of microposts (figures denote the amount of unigrams).", "labels": [], "entities": [{"text": "cross-domain classification  of microposts", "start_pos": 27, "end_pos": 69, "type": "TASK", "confidence": 0.7752834558486938}]}, {"text": " Table 10: In-domain classification of microposts (eval.:  F1-score).", "labels": [], "entities": [{"text": "In-domain classification of microposts", "start_pos": 11, "end_pos": 49, "type": "TASK", "confidence": 0.7251046001911163}, {"text": "F1-score", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9987263083457947}]}, {"text": " Table 12: Cross-domain classification of microposts:  all test data vs. explicit subset (eval.: F1-score).", "labels": [], "entities": [{"text": "Cross-domain classification of microposts", "start_pos": 11, "end_pos": 52, "type": "TASK", "confidence": 0.738690197467804}, {"text": "F1-score", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9987988471984863}]}, {"text": " Table 11: Different classifiers on cross-domain classification of microposts; best result in bold; (eval.: F1-score).", "labels": [], "entities": [{"text": "cross-domain classification of microposts", "start_pos": 36, "end_pos": 77, "type": "TASK", "confidence": 0.7538769915699959}, {"text": "F1-score", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9987892508506775}]}]}