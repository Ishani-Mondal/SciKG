{"title": [{"text": "Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning Problem", "labels": [], "entities": [{"text": "Word Emotion Induction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7847271263599396}]}], "abstractContent": [{"text": "Predicting the emotional value of lexical items is a well-known problem in sentiment analysis.", "labels": [], "entities": [{"text": "Predicting the emotional value of lexical items", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.818752578326634}, {"text": "sentiment analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9621130228042603}]}, {"text": "While research has focused on polarity for quite along time, meanwhile this early focus has been shifted to more expressive emotion representation models (such as Basic Emotions or Valence-Arousal-Dominance).", "labels": [], "entities": []}, {"text": "This change resulted in a proliferation of heterogeneous formats and, in parallel, often small-sized, non-interoperable resources (lexicons and corpus annotations).", "labels": [], "entities": []}, {"text": "In particular, the limitations in size hampered the application of deep learning methods in this area because they typically require large amounts of input data.", "labels": [], "entities": []}, {"text": "We here present a solution to get around this language data bottleneck by rephrasing word emotion induction as a multi-task learning problem.", "labels": [], "entities": [{"text": "word emotion induction", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.776756763458252}]}, {"text": "In this approach, the prediction of each independent emotion dimension is considered as an individual task and hidden layers are shared between these dimensions.", "labels": [], "entities": []}, {"text": "We investigate whether multi-task learning is more advantageous than single-task learning for emotion prediction by comparing our model against a wide range of alternative emotion and polarity induction methods featuring 9 typologically diverse languages and a total of 15 conditions.", "labels": [], "entities": [{"text": "emotion prediction", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7320442348718643}]}, {"text": "Our model turns out to out-perform each one of them.", "labels": [], "entities": []}, {"text": "Against all odds, the proposed deep learning approach yields the largest gain on the smallest data sets, merely composed of one thousand samples.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep Learning (DL) has radically changed the rules of the game in NLP by dramatically boosting performance figures in almost all applications areas.", "labels": [], "entities": []}, {"text": "Yet, one of the major premises of highperformance DL engines is their dependence on huge amounts of training data.", "labels": [], "entities": []}, {"text": "As such, DL seems ill-suited for areas where training data are scarce, such as in the field of word emotion induction.", "labels": [], "entities": [{"text": "DL", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.8740975856781006}, {"text": "word emotion induction", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.7707410454750061}]}, {"text": "We will use the terms polarity and emotion hereto distinguish between research focusing on \"semantic orientation\" (Hatzivassiloglou and McKeown, 1997) (the positiveness or negativeness) of affective states, on the one hand, and approaches which provide predictions based on some of the many more elaborated representational systems for affective states, on the other hand.", "labels": [], "entities": []}, {"text": "Originally, research activities focused on polarity alone.", "labels": [], "entities": []}, {"text": "In the meantime, a shift towards more expressive representation models for emotion can be observed that heavily draws inspirations from psychological theory, e.g., Basic Emotions) or the Valence-Arousal-Dominance model.", "labels": [], "entities": []}, {"text": "Though this change turned out to be really beneficial for sentiment analysis in NLP, a large variety of mutually incompatible encodings schemes for emotion and, consequently, annotation formats for emotion metadata in corpora have emerged that hinder the interoperability of these resources and their subsequent reuse, e.g., on the basis of alignments or mergers.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.9409641623497009}]}, {"text": "As an alternative way of dealing with thus unwarranted heterogeneity, we here examine the potential of multi-task learning) for word-level emotion prediction.", "labels": [], "entities": [{"text": "word-level emotion prediction", "start_pos": 128, "end_pos": 157, "type": "TASK", "confidence": 0.697576661904653}]}, {"text": "In MTL for neural networks, a single model is fitted to solve multiple, independent tasks (in our case, to predict different emotional dimensions) which typically results in learning more robust and meaningful intermediate representations.", "labels": [], "entities": [{"text": "MTL", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9466389417648315}]}, {"text": "MTL has been shown to greatly decrease the risk of overfitting, work well for various NLP tasks (, and practically increases sample size, thus making it a natural choice for small-sized data sets typically found in the area of word emotion induction.", "labels": [], "entities": [{"text": "word emotion induction", "start_pos": 227, "end_pos": 249, "type": "TASK", "confidence": 0.7642151514689127}]}, {"text": "After a discussion of related work in Section 2, we will introduce several reference methods and describe our proposed deep MTL model in Section 3.", "labels": [], "entities": [{"text": "MTL", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.9073288440704346}]}, {"text": "In our experiments (Section 4), we will first validate our claim that MTL is superior to single-task learning for word emotion induction.", "labels": [], "entities": [{"text": "MTL", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9309979677200317}, {"text": "word emotion induction", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.7717539668083191}]}, {"text": "After that, we will provide a large-scale evaluation of our model featuring 9 typologically diverse languages and multiple publicly available embedding models fora total of 15 conditions.", "labels": [], "entities": []}, {"text": "Our MTL model surpasses the current state-of-the-art for each of them, and even performs competitive relative to human reliability.", "labels": [], "entities": [{"text": "MTL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.937286913394928}]}, {"text": "Most notably however, our approach yields the largest benefit on the smallest data sets, comprising merely one thousand samples.", "labels": [], "entities": []}, {"text": "This finding, counterintuitive as it maybe, strongly suggests that MTL is particularly beneficial for solving the word emotion induction problem.", "labels": [], "entities": [{"text": "MTL", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9402102828025818}, {"text": "word emotion induction problem", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.7491850480437279}]}, {"text": "Our code base as well as the resulting experimental data is freely available.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Emotion lexicons used in our experiments (with their bibliographic source, identifier, language they refer  to, emotion representation format, and number of lexical entries they contain).", "labels": [], "entities": []}, {"text": " Table 3: Embedding models used for our experiments with identifier, language, embedding algorithm, training  corpus, its size in the number of tokens, size of the vocabulary (types) of the resulting embedding model and its  dimensionality.", "labels": [], "entities": []}, {"text": " Table 4: Results of our main experiment in averaged Pearson correlation; best result per condition (in rows) in  bold, second best result underlined; significant difference (paired two-tailed t-test) over the second best system  marked with \"*\", \"**\", or \"***\" for p < .05, .", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 53, "end_pos": 72, "type": "METRIC", "confidence": 0.7440114319324493}]}, {"text": " Table 5: Comparison of the MTLNN model against  inter-study reliability (ISR) between the EN and the  EN+ data set and split-half reliability (SHR) of the EN+  data set (in Pearson correlation).", "labels": [], "entities": [{"text": "inter-study reliability (ISR)", "start_pos": 49, "end_pos": 78, "type": "METRIC", "confidence": 0.802067244052887}, {"text": "EN+ data set", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.7284245863556862}, {"text": "split-half reliability (SHR)", "start_pos": 120, "end_pos": 148, "type": "METRIC", "confidence": 0.8807921528816223}, {"text": "EN+  data set", "start_pos": 156, "end_pos": 169, "type": "DATASET", "confidence": 0.7147434577345848}, {"text": "Pearson correlation", "start_pos": 174, "end_pos": 193, "type": "METRIC", "confidence": 0.8546421527862549}]}]}