{"title": [{"text": "Integrating Stance Detection and Fact Checking in a Unified Corpus", "labels": [], "entities": [{"text": "Stance Detection", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.8668437898159027}, {"text": "Fact Checking", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.7708859741687775}]}], "abstractContent": [{"text": "A reasonable approach for fact checking a claim involves retrieving potentially relevant documents from different sources (e.g., news websites, social media, etc.), determining the stance of each document with respect to the claim, and finally making a prediction about the claim's factuality by aggregating the strength of the stances, while taking the reliability of the source into account.", "labels": [], "entities": [{"text": "fact checking a claim", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.8696836233139038}]}, {"text": "Moreover, a fact checking system should be able to explain its decision by providing relevant extracts (ra-tionales) from the documents.", "labels": [], "entities": []}, {"text": "Yet, this setup is not directly supported by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.8540458679199219}, {"text": "document retrieval", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7830397486686707}, {"text": "stance detection", "start_pos": 131, "end_pos": 147, "type": "TASK", "confidence": 0.897313266992569}, {"text": "rationale extraction", "start_pos": 152, "end_pos": 172, "type": "TASK", "confidence": 0.7657917737960815}]}, {"text": "In this paper, we support the interdependencies between these tasks as annotations in the same corpus.", "labels": [], "entities": []}, {"text": "We implement this setup on an Arabic fact checking corpus, the first of its kind.", "labels": [], "entities": []}], "introductionContent": [{"text": "Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people's opinions () or to influence the outcome of major events such as political elections.", "labels": [], "entities": [{"text": "Fact checking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.982994019985199}]}, {"text": "While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (), hoaxes, and satire (.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.8665689826011658}]}, {"text": "Hence, there is need for automatic fact checking.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.927478164434433}]}, {"text": "While most previous research has focused on English, here we target Arabic.", "labels": [], "entities": []}, {"text": "Moreover, we propose some guidelines, which we believe should betaken into account when designing fact-checking corpora, irrespective of the target language.", "labels": [], "entities": []}, {"text": "Automatic fact checking typically involves retrieving potentially relevant documents (news articles, tweets, etc.), determining the stance of each document with respect to the claim, and finally predicting the claim's factuality by aggregating the strength of the different stances, taking into consideration the reliability of the documents' sources (news medium, Twitter account, etc.).", "labels": [], "entities": [{"text": "Automatic fact checking", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7291218439737955}]}, {"text": "Despite the interdependency between fact checking and stance detection, research on these two problems has not been previously supported by an integrated corpus.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9250022172927856}, {"text": "stance detection", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.9714677631855011}]}, {"text": "This is a gap we aim to bridge by retrieving documents for each claim and annotating them for stance, thus ensuring a natural distribution of the stance labels.", "labels": [], "entities": []}, {"text": "Moreover, in order to be trusted by users, a factchecking system should be able to explain the reasoning that led to its decisions.", "labels": [], "entities": []}, {"text": "This is best supported by showing extracts (such as sentences or phrases) from the retrieved documents that illustrate the detected stance (.", "labels": [], "entities": []}, {"text": "Unfortunately, existing datasets do not offer manual annotation of sentence-or phrase-level supporting evidence.", "labels": [], "entities": []}, {"text": "While deep neural networks with attention mechanisms can infer and extract such evidence automatically in an unsupervised way, potentially better results can be achieved when having the target sentence provided in advance, which enables supervised or semi-supervised training of the attention.", "labels": [], "entities": []}, {"text": "This would allow not only more reliable evidence extraction, but also better stance prediction, and ultimately better factuality prediction.", "labels": [], "entities": [{"text": "evidence extraction", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7236905694007874}, {"text": "stance prediction", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.9061962366104126}, {"text": "factuality prediction", "start_pos": 118, "end_pos": 139, "type": "TASK", "confidence": 0.8332816064357758}]}, {"text": "Following this idea, our corpus also identifies the most relevant stance-marking sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with our Arabic corpus, after preprocessing it with ATB-style segmentation using MADAMIRA (), using the following systems: \u2022 FNC BASELINE SYSTEM.", "labels": [], "entities": [{"text": "MADAMIRA", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.8028844594955444}, {"text": "FNC", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.8478759527206421}, {"text": "BASELINE", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.6558408141136169}]}, {"text": "This is the FNC organizers' system, which trains a gradient boosting classifier using hand-crafted features reflecting polarity, refute, similarity and overlap between the document and the claim.", "labels": [], "entities": [{"text": "FNC organizers'", "start_pos": 12, "end_pos": 27, "type": "DATASET", "confidence": 0.9118675887584686}]}, {"text": "It was second at FNC (, and was based on a multi-layer perceptron with the baseline system's features, word n-grams, and features generated using latent semantic analysis and other factorization techniques.", "labels": [], "entities": [{"text": "FNC", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.9331384897232056}]}, {"text": "It was third at FNC (, training a softmax layer using similarity features.", "labels": [], "entities": [{"text": "FNC (", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.7817013561725616}]}, {"text": "We also experimented with an end-to-end memory network that showed state-of-the-art results on the FNC data ( ).", "labels": [], "entities": [{"text": "FNC data", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.892646461725235}]}, {"text": "The evaluation results are shown in.", "labels": [], "entities": []}, {"text": "We use 5-fold cross-validation, where all claimdocument pairs for the same claim are assigned to the same fold.", "labels": [], "entities": []}, {"text": "We report accuracy, macro-average F 1 -score, and weighted accuracy, which is the official evaluation metric of FNC.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9975385665893555}, {"text": "F 1 -score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9657293260097504}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.7652013897895813}, {"text": "FNC", "start_pos": 112, "end_pos": 115, "type": "DATASET", "confidence": 0.8431702256202698}]}, {"text": "Overall, our corpus appears to be much harder than FNC.", "labels": [], "entities": []}, {"text": "For instance, the FNC baseline system achieves weighted accuracy of 75.2 on FNC vs. 55.6 (up to 64.8) on our corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9347593784332275}, {"text": "FNC", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.9039513468742371}]}, {"text": "We believe that this is because we used a realistic information retrieval approach (see Section 3), whereas the FNC corpus contains a significant number of totally unrelated document-claim pairs, e.g., about 40% of the unrelated examples have no word overlap with the claim (even after stemming!), which makes it much easier to correctly predict the unrelated class (and this class is also by far the largest).: Performance of some stance detection models from FNC when applied to our Arabic corpus.", "labels": [], "entities": [{"text": "FNC corpus", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.9475614726543427}, {"text": "stance detection", "start_pos": 432, "end_pos": 448, "type": "TASK", "confidence": 0.8266146183013916}, {"text": "FNC", "start_pos": 461, "end_pos": 464, "type": "DATASET", "confidence": 0.9282141327857971}]}, {"text": "allows us to study the utility of having gold rationales for the stance (for the agree and disagree classes only) under different scenarios.", "labels": [], "entities": []}, {"text": "First, we show the results when using the full document along with the claim, which is the default representation.", "labels": [], "entities": []}, {"text": "Then, we use the best sentence from the document, i.e., the one that is most similar to the claim as measured by the cosine of their average word embeddings.", "labels": [], "entities": []}, {"text": "This performs worse, which can be attributed to sometimes selecting the wrong sentence.", "labels": [], "entities": []}, {"text": "Next, we experiment with using the rationale instead of the best sentence when applicable (i.e., for agree and disagree), while still using the best sentence for discuss and unrelated.", "labels": [], "entities": []}, {"text": "This yields sizable improvements on all evaluation metrics, compared to using the best sentence (5-12 point absolute) or the full document (3-9 points absolute).", "labels": [], "entities": []}, {"text": "We further evaluate the impact of using the rationales, when applicable, but using the full document otherwise.", "labels": [], "entities": []}, {"text": "This setting performed best (80.2% accuracy with ATHENE, and 3-8 points of improvement over best+rationale), as it has access to most information: full document + rationale.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9987749457359314}, {"text": "ATHENE", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9964845180511475}]}, {"text": "Overall, the above experiments demonstrate that having a gold rationale can enable better learning.", "labels": [], "entities": []}, {"text": "However, the results should be considered as a kind of upper bound on the expected performance improvement, since here we used gold rationales attest time, which would not be available in a real-world scenario.", "labels": [], "entities": []}, {"text": "Still, we believe that sizable improvements would still be possible when using the gold rationales for training only.", "labels": [], "entities": []}, {"text": "Finally, we built a simple fact-checker, where the factuality of a claim is determined based on aggregating the predicted stances (using FNC's baseline system) of the documents we retrieved for it.", "labels": [], "entities": [{"text": "FNC", "start_pos": 137, "end_pos": 140, "type": "DATASET", "confidence": 0.9064455628395081}]}, {"text": "This yielded an accuracy of 56.2 when using the full documents, and 59.7 when using the best sentence + rationale (majority baseline of 50.5), thus confirming once again the utility of having a rationale, this time fora downstream task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9994726777076721}]}], "tableCaptions": [{"text": " Table 1: Statistics about stance and factuality labels.", "labels": [], "entities": [{"text": "stance and factuality labels", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.7571563273668289}]}, {"text": " Table 2: Performance of some stance detection models from FNC when applied to our Arabic corpus.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.8931050896644592}, {"text": "FNC", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.8277794122695923}]}]}