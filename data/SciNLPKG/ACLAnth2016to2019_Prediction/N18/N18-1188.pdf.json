{"title": [{"text": "LSDSCC: A Large Scale Domain-Specific Conversational Corpus for Response Generation with Diversity Oriented Evaluation Metrics", "labels": [], "entities": [{"text": "Response Generation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.9084039032459259}]}], "abstractContent": [{"text": "It has been proven that automatic conversational agents can be built up using the End-to-End Neural Response Generation (NRG) framework, and such a data-driven methodology requires a large number of dialog pairs for model training and reasonable evaluation met-rics for testing.", "labels": [], "entities": [{"text": "End-to-End Neural Response Generation (NRG)", "start_pos": 82, "end_pos": 125, "type": "TASK", "confidence": 0.7401984248842511}]}, {"text": "This paper proposes a Large Scale Domain-Specific Conversational Corpus (LSDSCC) composed of high-quality query-response pairs extracted from the domain-specific online forum, with thorough pre-processing and cleansing procedures.", "labels": [], "entities": []}, {"text": "Also, a testing set, including multiple diverse responses annotated for each query, is constructed , and on this basis, the metrics for measuring the diversity of generated results are further presented.", "labels": [], "entities": []}, {"text": "We evaluate the performances of neural dialog models with the widely applied diversity boosting strategies on the proposed dataset.", "labels": [], "entities": []}, {"text": "The experimental results have shown that our proposed corpus can betaken as anew benchmark dataset for the NRG task, and the presented metrics are promising to guide the optimization of NRG models by quantifying the diversity of the generated responses reasonably.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conversational agents (a.k.a. Chat-bots) are effective media to establish communications with human beings and have received much attention from academic and industrial experts in recent years ( . One essential fact promoting the research work on conversational agents is the explosive growth of human interaction data accumulated in the social network services, such as Twitter and Reddit 2 . So, it is possible to build Chat-bots based on data-driven approaches).", "labels": [], "entities": []}, {"text": "Nevertheless, there still remains a great challenge for building such conversational agents: at present, the automatic evaluation metrics of NRG models can hardly afford to measure the semantic relevance and diversity of generated results reasonably, and even the latter evaluation aspect has been paid little attention.", "labels": [], "entities": []}, {"text": "The widely accepted evaluating methods employed by the existing NRG models can be categorized as: a) metrics inherited from Machine Translation, e.g., BLEU, Perplexity, etc.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7610365748405457}, {"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9985307455062866}]}, {"text": "(); b) discrete scores measuring the quality of generated results by human labeling; and c) case study comparing the generated results of different NRG models (.", "labels": [], "entities": []}, {"text": "The disappointing situation is that these evaluating methods have not revealed tangible difference among NRG models, the reasons for which can be reflected by the example given in.", "labels": [], "entities": []}, {"text": "Query: Where did you get that from?", "labels": [], "entities": []}, {"text": "Ground-truth responses: I got it from her.", "labels": [], "entities": []}, {"text": "-I do not know.", "labels": [], "entities": []}], "datasetContent": [{"text": "As one of the most important qualities of the conversational corpus, the query-response relevance demonstrates the overall quality of the dataset.", "labels": [], "entities": []}, {"text": "Human evaluations of the query-response relevance are conducted to validate the quality of the dataset used in this paper.", "labels": [], "entities": []}, {"text": "Nine experienced annotators are invited to evaluate the query-response relevance of 500 single-turn dialogs uniformly sampled from the whole dataset obtained in Subsection 3.1.", "labels": [], "entities": []}, {"text": "In the evaluation, we ask each annotator to label whether the response is appropriate to the corresponding query in the given queryresponse pair.", "labels": [], "entities": []}, {"text": "A pair is tagged as \"Unsure\" if the annotator could not confirm the degrees of relevance without related context and background movie knowledge.", "labels": [], "entities": [{"text": "Unsure", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9809831976890564}]}, {"text": "The labeled result is shown in.", "labels": [], "entities": []}, {"text": "It is observed that 85% samples in the query-response relevance task are confirmed to keep high relevance between the query and the corresponding responses.", "labels": [], "entities": []}, {"text": "Moreover, there exist only about 6.6% irrelevant noises.", "labels": [], "entities": []}, {"text": "So, the resource can be considered as a high-quality one and can be used in the practical task.", "labels": [], "entities": []}, {"text": "Existing evaluation metrics of dialog agents measure the quality of the generated sentences only by referring to the existing responses, which obeys the same principle with NMT models' metrics.", "labels": [], "entities": []}, {"text": "However, one essential difference between NRG and NMT lies in the fact that, a large group of responses can be considered as relevant to a given query in conversations, while the number of references to a translation result is quite limited for NMT models.", "labels": [], "entities": []}, {"text": "So the diversity degree of candidates which have not covered by NMT oriented evaluation metrics, is supposed to be quantified and measured in NRG models.", "labels": [], "entities": []}, {"text": "Currently, few studies focus on the evaluation based on the group of references, which is more meaningful and reasonable for NRG models.", "labels": [], "entities": []}, {"text": "Therefore, we proposed three metrics: MaxBLEU, Mean Diversity Score, and Probabilistic Diversity Score, to quantify both the relevance and diversity of the generated responses.", "labels": [], "entities": [{"text": "MaxBLEU", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.7726847529411316}, {"text": "Mean Diversity Score", "start_pos": 47, "end_pos": 67, "type": "METRIC", "confidence": 0.8976052602132162}]}, {"text": "Since these metrics are based on the multi-reference, we first describe the procedure of building testing set, with multi-references for each query.", "labels": [], "entities": []}, {"text": "Then, the metrics for NRG models are detailed based on the multireference testing set.", "labels": [], "entities": []}, {"text": "illustrates the response quantity distribution of queries in the preprocessed data.", "labels": [], "entities": []}, {"text": "While the testing set is randomly sampled from the preprocessed data, the response quantity distribution of the testing set is the same as that in.", "labels": [], "entities": []}, {"text": "In this case, the multi-reference testing set for NRG evaluation is difficult to construct by directly extracting samples from the dialog corpus, since there are too few queries that contain more than three responses.", "labels": [], "entities": [{"text": "NRG evaluation", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.9149862229824066}]}, {"text": "Roughly choosing samples from such data is possible to bring topic bias into the testing set, and manually filtering suitable candidate pairs from them is also time-consuming and expensive.", "labels": [], "entities": []}, {"text": "Nevertheless, there exist large amounts of queries that are highly semantically similar or correlated with each other.", "labels": [], "entities": []}, {"text": "This indicates that the multiple references can be obtained by selecting responses of queries that are semantically identical to the original query.", "labels": [], "entities": []}, {"text": "What's more, the human-annotation is involved to proofread the filtered pairs' quality and complete the final labeling.", "labels": [], "entities": []}, {"text": "When constructing the testing set, the very first step is getting semantically similar (or even identical) queries with the given ones.", "labels": [], "entities": []}, {"text": "For this purpose, this paper adopts the TF-IDF similarity and semantic embedding based distance to measure the similarity between queries.", "labels": [], "entities": [{"text": "TF-IDF similarity", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.8029045164585114}]}, {"text": "The procedure of gaining similar queries is divided into two stage: In the first stage, we employ Apache Lucene 5 to exploit the word-level TF-IDF patterns within queries, and then extract the top 100 similar queries with highest scores given by Lucene for each query.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 246, "end_pos": 252, "type": "DATASET", "confidence": 0.9558830857276917}]}, {"text": "Yet, these candidates only capture n-gram level similarity with the probably diverged semantics.", "labels": [], "entities": []}, {"text": "Thus, in the second stage, we utilize paragraph vector algorithm (a.k.a Doc2vec 6 ) () to resort the selected similar queries in the semantic space and only queries of similarity score higher than a certain threshold (i.e., 0.9) are reserved.", "labels": [], "entities": []}, {"text": "lists several identical queries filtered by Lucene and Doc2vec methods with the given query.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.923550546169281}]}, {"text": "It should be noted that the Lucene index and Doc2vec need to be initialized by feeding all the sentences in the dialogue corpus.", "labels": [], "entities": [{"text": "Lucene index", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9443570673465729}, {"text": "Doc2vec", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9316805005073547}]}, {"text": "In this section, we present the detailed experiments on the single-turn dialog dataset and analysis on generated results, in accordance to the proposed metrics.", "labels": [], "entities": []}, {"text": "Experiments are conducted using the popular Seq2Seq based models with the currently available diversity prompting strategies as follows: 1) Basic Seq2Seq.", "labels": [], "entities": []}, {"text": "We employ the basic Seq2Seq to build the encoder-decoder architecture running on the proposed dataset, by taking the bidirectional LSTM cell as the encoder to address the input sentences ordering problem and classic LSTM cell as the decoder (Vinyals and Le, 2015).", "labels": [], "entities": []}, {"text": "In our research, we implement these models on the TensorFlow platform 9 , and Adam optimizer () is employed for gradient optimization during training.", "labels": [], "entities": [{"text": "gradient optimization", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.7194687128067017}]}, {"text": "Besides, we choose to prune the words whose frequencies are below 2, so the source and target vocabulary are set to 42, 257 and 46, 865 respectively.", "labels": [], "entities": []}, {"text": "In addition, we set the batch size to 50, hidden size of encoder to 256, hidden size of decoder to 512 and learning rate to 2e \u2212 4.", "labels": [], "entities": []}, {"text": "The gradients are clipped within [\u22123.0, 3.0] to avoid the gradient explosion problem.", "labels": [], "entities": []}, {"text": "Every model runs on a single GPU separately for at least one week before convergence.", "labels": [], "entities": []}, {"text": "Afterwards, for all these methods, we generate a set of hypothesis sentences with beam size set to k = 50, and the evaluation scores are obtained using the proposed metrics.", "labels": [], "entities": []}, {"text": "After running through 25 epochs on the dataset, the training log-loss of the basic Seq2Seq mod-  els converge to about 4.2 and the Seq2Seq models augmented with attention converge to 3.1.", "labels": [], "entities": []}, {"text": "Also, we set the dropout rate to 0.5, which enables us to tune the models though much more epochs and avoid the over-fitting problems.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 17, "end_pos": 29, "type": "METRIC", "confidence": 0.9697527885437012}]}], "tableCaptions": [{"text": " Table 2: Composition of noise words in the query and  response vocabulary of the raw data.", "labels": [], "entities": []}, {"text": " Table 3. It is observed that 85% samples in  the query-response relevance task are confirmed  to keep high relevance between the query and the  corresponding responses. Moreover, there exist only about 6.6% irrelevant noises. So, the resource  can be considered as a high-quality one and can be  used in the practical task.", "labels": [], "entities": []}, {"text": " Table 3: Query-Response Relevance on the single-turn  training set.", "labels": [], "entities": []}, {"text": " Table 5: Performances of different models trained on the LSDSCC dataset with three metrics: MaxBLEU, MDS, PDS.", "labels": [], "entities": [{"text": "LSDSCC dataset", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.9450298845767975}, {"text": "MaxBLEU", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9711289405822754}, {"text": "MDS", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9532492756843567}]}, {"text": " Table 5.  From this benchmarking table, it can be observed  that the attention mechanism is helpful for de- coders to improve the relevance of the generated  responses, since the Attention-Seq2Seq performs  better than the basic Seq2Seq on the dataset, in  terms of all the three metrics. However, the rela- tive gain of the attention layer is limited, indicat- ing that modeling relation of query and response  by attention module is not able to directly solve  the learning paradigm of conversations.  In accordance to the results of Greedy-Seq2Seq  (\u03b3 = 0.1) and Greedy-Seq2Seq (\u03b3 = 0.8), the  hyper-parameter \u03b3 actually plays an important role  in the generation steps of the decoder. Since \u03b3  is introduced to constrain the selection probabil- ity of the next-step word by performing the re- ranking process, and the larger value of this pa- rameter will lead to the greater impact upon gen- erating steps and produce more diverse sentences,  we evaluate this greedy strategy with \u03b3 set with  two empirical value. It can be seen that the model  with the smaller \u03b3 performs better than the one  with the larger parameter, which can be attributed  to the fact that responses with more diversity are  less similar to references. Similar observation can  be get from the results of models Greedy-Attn- Seq2Seq (\u03b3 = 0.1) and Greedy-Attn-Seq2Seq  (\u03b3 = 0.8). Besides, the reason for setting \u03b3 =  0.1, 0.8 in this part is that they are well represented  for the poor diversity and good diversity, which the  exact score of \u03b3 will vary under different configu- rations and structures of model.", "labels": [], "entities": []}, {"text": " Table 6: Correlation between the proposed metrics and  human judgments for the Reddit dataset.", "labels": [], "entities": [{"text": "Reddit dataset", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.9171688854694366}]}]}