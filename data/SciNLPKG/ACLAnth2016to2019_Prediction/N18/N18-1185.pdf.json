{"title": [{"text": "Challenging Reading Comprehension on Daily Conversation: Passage Completion on Multiparty Dialog", "labels": [], "entities": [{"text": "Challenging Reading Comprehension on Daily Conversation", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.5915862818559011}, {"text": "Passage Completion", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7875620126724243}]}], "abstractContent": [{"text": "This paper presents anew corpus and a robust deep learning architecture fora task in reading comprehension, passage completion, on multi-party dialog.", "labels": [], "entities": [{"text": "passage completion", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.889302134513855}]}, {"text": "Given a dialog in text and a passage containing factual descriptions about the dialog where mentions of the characters are replaced by blanks, the task is to fill the blanks with the most appropriate character names that reflect the contexts in the dialog.", "labels": [], "entities": []}, {"text": "Since there is no dataset that challenges the task of passage completion in this genre, we create a corpus by selecting transcripts from a TV show that comprise 1,681 dialogs, generating passages for each dialog through crowdsourcing, and annotating mentions of characters in both the dialog and the passages.", "labels": [], "entities": [{"text": "passage completion", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8475237190723419}]}, {"text": "Given this dataset, we build a deep neural model that integrates rich feature extraction from convolutional neural networks into sequence modeling in recurrent neural networks , optimized by utterance and dialog level attentions.", "labels": [], "entities": []}, {"text": "Our model outperforms the previous state-of-the-art model on this task in a different genre using bidirectional LSTM, showing a 13.0+% improvement for longer dialogs.", "labels": [], "entities": []}, {"text": "Our analysis shows the effectiveness of the attention mechanisms and suggests a direction to machine comprehension on multiparty dialog.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reading comprehension that challenges machine's ability to understand a document through question answering has gained lots of interests.", "labels": [], "entities": [{"text": "question answering", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7127720713615417}]}, {"text": "Most of the previous works for reading comprehension have focused on either children's stories () or newswire (.", "labels": [], "entities": []}, {"text": "Few approaches have attempted comprehension on small talks, although they are evaluated on toy examples not suitable to project real-life performance.", "labels": [], "entities": []}, {"text": "It is apparent that the mainstream of reading comprehension has not been on the genre of multiparty dialog although it is the most common and natural means of human communication.", "labels": [], "entities": [{"text": "reading comprehension", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7786515355110168}]}, {"text": "The volume of data accumulating from group chat or messaging continues to outpace data accumulation from other writing sources.", "labels": [], "entities": []}, {"text": "The combination of available and rapidly developing analytic options, a marked need for dialogue processing, and the disproportionate generation of data from conversations through text platforms inspires us to create a corpus consisting of multiparty dialogs and develop learning models that make robust inference on their contexts.", "labels": [], "entities": []}, {"text": "Passage completion is a popular method of evaluating reading comprehension that is adapted by several standardized tests (e.g., SAT, TOEFL, GRE).", "labels": [], "entities": [{"text": "Passage completion", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8726320564746857}, {"text": "SAT", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.944000244140625}, {"text": "TOEFL", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.6433204412460327}, {"text": "GRE", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9235193729400635}]}, {"text": "Given a document and a passage containing factual descriptions about the contexts in the document, the task replaces keywords in the passage with blanks and asks the reader to fill in the blanks.", "labels": [], "entities": []}, {"text": "This task is particularly challenging when the document is in a form of dialog because it needs to match contexts between colloquial (dialog) and formal (passage) writings.", "labels": [], "entities": []}, {"text": "Moreover, a context that can be described in a short passage, say a sentence, tends to be expressed across multiple utterances in dialog, which requires discourse-level processing to make the full interpretation of the context.", "labels": [], "entities": []}, {"text": "This paper introduces anew corpus for passage completion on multiparty dialog (Section 3), and a deep learning architecture that produces robust results for understanding dialog contexts (Section 4).", "labels": [], "entities": [{"text": "passage completion", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9473019242286682}]}, {"text": "Our experiments show that models trained by this architecture significantly outperform the previous state-of-the-art model using bidirectional LSTM, especially on longer dialogs (Section 5).", "labels": [], "entities": []}, {"text": "Our analysis highlights the comprehension of our models for matching utterances in dialogs to words in passages (Section 6).", "labels": [], "entities": [{"text": "matching utterances in dialogs to words in passages", "start_pos": 60, "end_pos": 111, "type": "TASK", "confidence": 0.7690603137016296}]}, {"text": "To the best of our knowledge, this is the first time that the sentence completion task is thoroughly examined with a challenging dataset on multiparty dialog using deep learning models.", "labels": [], "entities": [{"text": "sentence completion task", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.8160397211710612}]}, {"text": "The overview of passage generation.", "labels": [], "entities": [{"text": "passage generation", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9660070538520813}]}, {"text": "Each episode is split into scenes, and each summary is segmented to sentences.", "labels": [], "entities": []}, {"text": "Elasticsearch passes the scene-sentence pairs to crowd workers who are asked to check the relevancy, replace all pronouns with the corresponding names, and generate new passages for the scenes (Section 3.1).", "labels": [], "entities": []}, {"text": "introduced the CNN/Daily Mail dataset where documents and passages were news articles and their summaries respectively, and evaluated neural models with three types of readers.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 15, "end_pos": 37, "type": "DATASET", "confidence": 0.9110539674758911}]}, {"text": "proposed the entity centric model and the bidirectional LSTM model using attention, and conducted a thorough analysis on this dataset.", "labels": [], "entities": []}, {"text": "presented the EpiReader that combined a reasoner with an extractor for encoding documents and passages using both CNN and RNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.9232822060585022}]}, {"text": "proposed the gated-attention reader that incorporated attention on multiplicative interactions between documents and passages.", "labels": [], "entities": []}, {"text": "At last, introduced the attention-overattention reader that placed document-to-passage attention over passage-to-document attention.", "labels": [], "entities": []}, {"text": "released the Children Book Test dataset where documents were children's book stories and passages were excerpts from those stories.", "labels": [], "entities": [{"text": "Children Book Test dataset", "start_pos": 13, "end_pos": 39, "type": "DATASET", "confidence": 0.9682152718305588}]}, {"text": "introduced the LAMBADA dataset comprising novels from the Book corpus.", "labels": [], "entities": [{"text": "LAMBADA dataset comprising novels from the Book corpus", "start_pos": 15, "end_pos": 69, "type": "DATASET", "confidence": 0.7325624562799931}]}, {"text": "introduced the Who-did-What dataset consisting of articles from the LDC English Gigaword newswire corpus.", "labels": [], "entities": [{"text": "Who-did-What dataset", "start_pos": 15, "end_pos": 35, "type": "DATASET", "confidence": 0.6979764103889465}, {"text": "LDC English Gigaword newswire corpus", "start_pos": 68, "end_pos": 104, "type": "DATASET", "confidence": 0.8985695958137512}]}, {"text": "All corpora described above provide queries, that are passages where certain words are masked by blanks, for the evaluation of passage completion.", "labels": [], "entities": [{"text": "passage completion", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7188768982887268}]}, {"text": "More datasets are available for another type of a reading comprehension task, that is multiple choice question answering, such as MCTest (),), RACE (, and SQuAD (.", "labels": [], "entities": [{"text": "multiple choice question answering", "start_pos": 86, "end_pos": 120, "type": "TASK", "confidence": 0.6405386105179787}, {"text": "RACE", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.48791518807411194}]}], "datasetContent": [{"text": "The Glove 100-dimensional pre-trained word embeddings () are used for all experiments (d = 100).", "labels": [], "entities": []}, {"text": "The maximum lengths of utterances and queries are m = 92 and n = 126, and the maximum number of utterances is k = 25.", "labels": [], "entities": []}, {"text": "For the 2/1D convolutions in Sections 4.1 and 4.3, f = e = 50 filters are used, and the ReLu activation is applied to all convolutional layers.", "labels": [], "entities": []}, {"text": "The dimension of the LSTM outputs h \u2193\u2191 * is 32, and the tanh activation is applied to all hidden states of LSTMs.", "labels": [], "entities": []}, {"text": "Finally, the Adam optimizer with the learning rate of 0.001 is used to learn the weights of all models.", "labels": [], "entities": []}, {"text": "The average number of utterances per dialog is 15.8 in our corpus, which is relatively short.", "labels": [], "entities": []}, {"text": "To demonstrate the model robustness for longer dialogs, three more datasets are created in which all dialogs have the fixed lengths of 25, 50, and 100 by borrowing utterances from their consecutive scenes.", "labels": [], "entities": []}, {"text": "The same sets of queries are used although models need to search through much longer dialogs in order to answer the queries for these new datasets.", "labels": [], "entities": []}, {"text": "The three pseudo-generated datasets as well as the original dataset are used for all our experiments.", "labels": [], "entities": []}, {"text": "Human performance is examined on the evaluation set of the original length using crowdsourcing.", "labels": [], "entities": []}, {"text": "The workers are presented with passages and the corresponding dialogs and asked to choose the answer from the list of entities that appear in the dialog.", "labels": [], "entities": []}, {"text": "For fair comparisons, the encoded input where the character names are replaced with the entity IDs are used for this evaluation as well, to minimize the bias from external knowledge.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results on the development and the evaluation sets from all models.", "labels": [], "entities": []}, {"text": " Table 4: Dataset split for our experiments, where each  query is considered a separate instance.", "labels": [], "entities": []}, {"text": " Table 5: The confusion matrix between Bi-LSTM and  CNN+LSTM+UA+DA.", "labels": [], "entities": [{"text": "CNN+LSTM+UA+DA", "start_pos": 52, "end_pos": 66, "type": "METRIC", "confidence": 0.6487258076667786}]}]}