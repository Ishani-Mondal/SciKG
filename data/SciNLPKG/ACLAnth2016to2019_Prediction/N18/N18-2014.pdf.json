{"title": [{"text": "A Corpus of Non-Native Written English Annotated for Metaphor", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a corpus of 240 argumentative essays written by non-native speakers of English annotated for metaphor.", "labels": [], "entities": []}, {"text": "The corpus is made publicly available.", "labels": [], "entities": []}, {"text": "We provide benchmark performance of state-of-the-art systems on this new corpus, and explore the relationship between writing proficiency and metaphor use.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the ubiquity of metaphor across genres of written and spoken communication, the ability of NLP systems to deal with metaphor effectively is an actively researched topic (.", "labels": [], "entities": []}, {"text": "Most current work in the supervised machine learning paradigm uses data from the British National Corpus (BNC).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 81, "end_pos": 110, "type": "DATASET", "confidence": 0.9803408781687418}]}, {"text": "Beigman reported an evaluation of a metaphor detection system on students' writing; however, their corpus was not released for public use.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.9124443829059601}]}, {"text": "Our contributions are as follows: (1) We release metaphor annotations of 240 argumentative essays written by non-native speakers of English.", "labels": [], "entities": []}, {"text": "This is the first publicly available metaphor annotated data in this genre we are aware of.", "labels": [], "entities": []}, {"text": "We evaluate state-ofart (SoA) feature sets on the new data.", "labels": [], "entities": []}, {"text": "(3) We show that use of argumentation-relevant metaphor is a significant predictor of a holistic score of essay quality, above and beyond essay length.", "labels": [], "entities": []}], "datasetContent": [{"text": "Second, Arg outperforms VUA.", "labels": [], "entities": [{"text": "Arg", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9976353645324707}, {"text": "VUA", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.7010416388511658}]}, {"text": "In some cases, the difference could be attributed to data being indomain; the sets marked with a plus in are taken from the same testing programs as the training data for Arg, although the specific prompts are different.", "labels": [], "entities": [{"text": "Arg", "start_pos": 171, "end_pos": 174, "type": "DATASET", "confidence": 0.7216463685035706}]}, {"text": "However, Arg does better across the board, including data completely unrelated to the annotation campaign.", "labels": [], "entities": [{"text": "Arg", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.6511582136154175}]}, {"text": "It is likely that the protocol that emphasized specifically the need to pay attention to the role played by the metaphor in the author's argument is at least partially responsible for the higher performance indicators.", "labels": [], "entities": []}, {"text": "Next, we observe better performance on F sets, those with a very general, single-sentence prompt (see example in section 3.2) than on S datasets with extensive prompts that directed test-takers to criticize, summarize, or draw upon arguments presented in specific textual sources.", "labels": [], "entities": []}, {"text": "Again, this could be due to the short-prompt-based arguments being more inline with the annotated data; however, since there is a similar tendency for the VUAtrained system, it could also be a more general issue of the extent to which the author controls the vocabulary in her essay.", "labels": [], "entities": [{"text": "VUAtrained", "start_pos": 155, "end_pos": 165, "type": "DATASET", "confidence": 0.923127293586731}]}, {"text": "With extensive prompts that contain information that needs to be reflected in the essay, a substantial part of the vocabulary is forced by the prompt and not drawn from the author's more creative faculties and knowledge.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of the data. #T = # of texts; #I = #  of instances; %M = proportion of metaphors.", "labels": [], "entities": []}, {"text": " Table 3: Performance of Logistic Regression with SoA  feature sets on essay data.", "labels": [], "entities": []}, {"text": " Table 4: Partial correlation controlling for length be- tween essay score and metaphor use, for a system  trained on essays (Arg) vs VUA data. The underlined  figures are not statistically significant (p > 0.01). Plus  signs are explained in the text.", "labels": [], "entities": [{"text": "VUA data", "start_pos": 134, "end_pos": 142, "type": "DATASET", "confidence": 0.9339143931865692}]}]}