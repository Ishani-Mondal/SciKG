{"title": [{"text": "Sensing and Learning Human Annotators Engaged in Narrative Sensemaking", "labels": [], "entities": [{"text": "Sensing and Learning Human Annotators Engaged in Narrative Sensemaking", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.754950589603848}]}], "abstractContent": [{"text": "While labor issues and quality assurance in crowdwork are increasingly studied, how an-notators make sense of texts and how they are personally impacted by doing so are not.", "labels": [], "entities": []}, {"text": "We study these questions via a narrative-sorting annotation task, where carefully selected (by sequentiality, topic, emotional content, and length) collections of tweets serve as examples of everyday storytelling.", "labels": [], "entities": []}, {"text": "As readers process these narratives, we measure their facial expressions, galvanic skin response, and self-reported reactions.", "labels": [], "entities": []}, {"text": "From the perspective of an-notator well-being, a reassuring outcome was that the sorting task did not cause a measurable stress response, however readers reacted to humor.", "labels": [], "entities": [{"text": "sorting task", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.9019078016281128}]}, {"text": "In terms of sensemaking, readers were more confident when sorting sequential , target-topical, and highly emotional tweets.", "labels": [], "entities": []}, {"text": "As crowdsourcing becomes more common , this research sheds light onto the perceptive capabilities and emotional impact of human readers.", "labels": [], "entities": []}], "introductionContent": [{"text": "A substantial sector of the gig economy is the use of crowdworkers to annotate data for machine learning and analysis.", "labels": [], "entities": []}, {"text": "For instance, storytelling is an essential human activity, especially for information sharing, making it the subject of many data annotation tasks.", "labels": [], "entities": [{"text": "information sharing", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7113509625196457}]}, {"text": "Microblogging sites such as Twitter have reshaped the narrative format, especially through character restrictions and nonstandard language, elements which contribute to a relatively unexplored mode of narrative construction; yet, little is known about reader responses to such narrative content.", "labels": [], "entities": []}, {"text": "We explore reader reactions to narrative sensemaking using anew sorting task that varies the presumed cognitive complexity of the task and elicits readers' interpretations of a target topic and its emotional tone.", "labels": [], "entities": [{"text": "reader reactions to narrative sensemaking", "start_pos": 11, "end_pos": 52, "type": "TASK", "confidence": 0.586513102054596}]}, {"text": "We carefully and systematically extracted 60 sets of tweets from a 1M-tweet dataset and presented them to participants via an interface that mimicks the appearance of Twitter ().", "labels": [], "entities": []}, {"text": "We asked subjects to sort chronologically the tweets in each set, half with true narrative sequence and half without.", "labels": [], "entities": []}, {"text": "Each set consisted of 3-4 tweets from a previously collected corpus, where each tweet was labeled using a framework by as work-related or not.", "labels": [], "entities": []}, {"text": "In addition to sequentiality and job-relatedness, sets were evenly distributed across two other variables with two levels each, of interest for understanding narrative sensemaking.", "labels": [], "entities": []}, {"text": "We recorded readers' spoken responses to four questions) about each set, which involved the sorting task, reader confidence, topical content (jobrelatedness), and emotional tone.", "labels": [], "entities": [{"text": "sorting task", "start_pos": 92, "end_pos": 104, "type": "TASK", "confidence": 0.8816660046577454}]}, {"text": "We used galvanic skin response (GSR) and facial expression analysis to explore potentially quantifiable metrics for stress-based reactions and other aspects of reader-annotator response.", "labels": [], "entities": []}, {"text": "Our results add understanding of how annotators react to and process everyday microblog narratives.", "labels": [], "entities": []}, {"text": "This study makes these contributions: 1) Opens a dialogue on annotator well-being; 2) Presents a method to study annotator reactions; 3) Indicates that narrative sorting (task with degrees of complexity) does not cause an increased stress response as task complexity increases; 4) Studies the role of topic saliency and emotional tone in narrative sense-making; and 5) Probes how features model annotator reactions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Classification accuracies rounded to nearest  percent for Trial 1 (T1), Trial 2 (T2) and both trials  combined (C). Bold values indicate the most accurate  feature set's prediction percentage per trial or com- bined. Because trials 1 and 2 differed in having a true  narrative sequence, no seq+/-prediction is reported by  trial.", "labels": [], "entities": [{"text": "Classification accuracies", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.6896461546421051}]}]}