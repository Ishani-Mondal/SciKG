{"title": [{"text": "Neural Metaphor Detecting with CNN-LSTM Model", "labels": [], "entities": [{"text": "Neural Metaphor Detecting", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8217944701512655}]}], "abstractContent": [{"text": "Metaphors are figurative languages widely used in daily life and literatures.", "labels": [], "entities": []}, {"text": "It's an important task to detect the metaphors evoked by texts.", "labels": [], "entities": [{"text": "detect the metaphors evoked by texts", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.7950186183055242}]}, {"text": "Thus, the metaphor shared task is aimed to extract metaphors from plain texts at word level.", "labels": [], "entities": []}, {"text": "We propose to use a CNN-LSTM model for this task.", "labels": [], "entities": []}, {"text": "Our model combines CNN and LSTM layers to utilize both local and long-range contextual information for identifying metaphorical information.", "labels": [], "entities": [{"text": "identifying metaphorical information", "start_pos": 103, "end_pos": 139, "type": "TASK", "confidence": 0.8660868207613627}]}, {"text": "In addition, we compare the performance of the softmax classifier and conditional random field (CRF) for sequential labeling in this task.", "labels": [], "entities": [{"text": "conditional random field (CRF)", "start_pos": 70, "end_pos": 100, "type": "METRIC", "confidence": 0.7161059180895487}]}, {"text": "We also incorporated some additional features such as part of speech (POS) tags and word cluster to improve the performance of model.", "labels": [], "entities": []}, {"text": "Our best model achieved 65.06% F-score in the all POS testing subtask and 67.15% in the verbs testing subtask.", "labels": [], "entities": [{"text": "F-score", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.999538779258728}]}], "introductionContent": [{"text": "A metaphor is a type of conceptual mapping to represent one thing as another.", "labels": [], "entities": []}, {"text": "They are widely used in verbal and written languages to convey rich linguistic and sentiment information.", "labels": [], "entities": []}, {"text": "Detecting the metaphors in texts are important to mine the semantic and sentiment information better, which is beneficial to many applications such as machine translation, dialog systems and sentiment analysis).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.7947903275489807}, {"text": "sentiment analysis", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.9123603701591492}]}, {"text": "However, detecting metaphors is a challenging task.", "labels": [], "entities": [{"text": "detecting metaphors", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.9453135430812836}]}, {"text": "The semantic differences between metaphorical and non-metaphorical texts are often subtle.", "labels": [], "entities": []}, {"text": "For example, the sentence Her hair is a white snowflake is metaphorical, while the sentence Her hair is white doesn't contain metaphors.", "labels": [], "entities": []}, {"text": "In addition, detecting metaphors can be influenced by subjective factors, and may need specific domain knowledge ().", "labels": [], "entities": [{"text": "detecting metaphors", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.9234558641910553}]}, {"text": "Existing computational approaches to detect metaphors are mainly based on lexicons) and supervised methods.", "labels": [], "entities": [{"text": "detect metaphors", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.832253098487854}]}, {"text": "Lexiconbased methods are free from data annotation, but they are unable to detect novel metaphorical usages and capture the contextual information.", "labels": [], "entities": []}, {"text": "Supervised methods such as logistic regression classifier () can capture richer metaphor information.", "labels": [], "entities": [{"text": "logistic regression classifier", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.6364280680815378}]}, {"text": "However, they need sophisticated hand-crafted features.", "labels": [], "entities": []}, {"text": "To improve the collective techniques on detecting metaphors, the metaphor shared task 1 aims to detect both metaphorical verbs and metaphors with other POS.", "labels": [], "entities": []}, {"text": "Given a sentence and their words with specific POS tags, systems are required to determine whether each word is a metaphor.", "labels": [], "entities": []}, {"text": "We propose a CNN-LSTM model with CRF or weighted softmax classifier to address this task.", "labels": [], "entities": []}, {"text": "Our model can take advantage of both long-range and local information by utilizing both LSTM and CNN layers.", "labels": [], "entities": []}, {"text": "We propose to use a weighted softmax classifier to predict the label sequence of sentence, which outperforms the CRF method.", "labels": [], "entities": []}, {"text": "We apply a model ensemble strategy to help our model predict more accurately.", "labels": [], "entities": []}, {"text": "In addition, we incorporated additional features such as POS tags and word cluster features to further improve our model.", "labels": [], "entities": []}, {"text": "Our best model achieved 65.06% F-score on the test data in the all POS testing subtask, and 67.15% in the verbs testing subtask.", "labels": [], "entities": [{"text": "F-score", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9997110962867737}]}], "datasetContent": [{"text": "The dataset for this task is the VU Amsterdam Metaphor Corpus (VUA) . There are 12,122 sentences for training, and 4,080 sentences for test.", "labels": [], "entities": [{"text": "VU Amsterdam Metaphor Corpus (VUA)", "start_pos": 33, "end_pos": 67, "type": "DATASET", "confidence": 0.910715469292232}]}, {"text": "We tune the hyper-parameters of our model via cross validation.", "labels": [], "entities": []}, {"text": "The pre-trained word embeddings are the 300-dim Google embedding 4 released by.", "labels": [], "entities": []}, {"text": "They were trained by the skip-gram model on about 100-billion words on Google News.", "labels": [], "entities": [{"text": "Google News", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.8085042834281921}]}, {"text": "These word embedding were fine-tuned during model training.", "labels": [], "entities": []}, {"text": "The hyper-parameters in our model were tuned via cross-validation.", "labels": [], "entities": []}, {"text": "The dimension of Bi-LSTM hidden states is 200, the window sizes of CNN filters are 3, 5, 7 and 9 respectively.", "labels": [], "entities": []}, {"text": "The number of CNN filters is 100.", "labels": [], "entities": []}, {"text": "We set the dropout rate to 0.2 for each layer.", "labels": [], "entities": []}, {"text": "The loss weights w p and w n of metaphors and non-metaphorical words are set to http://ota.ahds.ac.uk/headers/2541.xml 4 https://code.google.com/archive/p/word2vec/ 2.0 and 1.0 respectively.", "labels": [], "entities": []}, {"text": "The class number of word cluster is set 50.", "labels": [], "entities": []}, {"text": "The batch size is 50, and the max training epoch is set to 15.", "labels": [], "entities": [{"text": "max training epoch", "start_pos": 30, "end_pos": 48, "type": "METRIC", "confidence": 0.8690696557362875}]}, {"text": "The optimizer we use is RMSProp in our experiment.", "labels": [], "entities": []}, {"text": "The performance of both all POS testing and verbs testing subtasks is evaluated by precision, recall and F-score as a standard binary classification task.", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.999716579914093}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9991949200630188}, {"text": "F-score", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9988659620285034}]}, {"text": "We compare the performance of the variants of our model and several baseline methods.", "labels": [], "entities": []}, {"text": "The methods to be compared include: 1) CNN+CRF, using CNN to extract local information and CRF for word-level metaphor detection; 2) LSTM+CRF, using Bi-LSTM to obtain the text representation and CRF inference layer; 3) CNN+LSTM+CRF, using the combination of LSTM, CNN and CRF inference layer; 4) CNN+LSTM+CRF+ensemble, adding ensemble strategy to the CNN+LSTM+CRF model; 5) CNN+Softmax, using CNN and weighted softmax classifier for sequential labeling; 6) LSTM+Softmax, using Bi-LSTM and softmax inference layer; 7) CNN+LSTM+Softmax w/o lemma, using the combination of LSTM, CNN and softmax inference layer, but without the lemmatizing process; 8) CNN+LSTM+Softmax, using the combination of LSTM, CNN and softmax inference layer; 9) CNN+LSTM+Softmax+ensemble, adding ensemble strategy to the CNN+LSTM+Softmax model.", "labels": [], "entities": [{"text": "word-level metaphor detection", "start_pos": 99, "end_pos": 128, "type": "TASK", "confidence": 0.7993545730908712}]}, {"text": "Our official submissions are obtained by model 3), 4), 8), 9) and the different combinations of additional features, which will be discussed in the next subsection.", "labels": [], "entities": []}, {"text": "According to, we have several observations: (1) The combination of LSTM and CNN outperforms the single CNN and LSTM in both subtasks.", "labels": [], "entities": []}, {"text": "It proves that the combination of CNN and LSTM can help to mine both local and longdistance information from texts, which is beneficial for detecting the metaphors in texts.", "labels": [], "entities": []}, {"text": "(2) Comparing the modeling using CRF and softmax layer, best precision score can be achieved by using CRF.", "labels": [], "entities": [{"text": "precision score", "start_pos": 61, "end_pos": 76, "type": "METRIC", "confidence": 0.9724462330341339}]}, {"text": "But the recall and F-score are significantly better when using weighted softmax classifier.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.999846339225769}, {"text": "F-score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9995253086090088}]}, {"text": "This is probably because the numbers of metaphors are usually less than normal non-metaphorical words.", "labels": [], "entities": []}, {"text": "The metaphors can be identified better when they are assigned larger loss weights.", "labels": [], "entities": []}, {"text": "(3) Improvement can be brought by the lemmatizing process: The performance of different methods.", "labels": [], "entities": []}, {"text": "*The results of these baseline methods were not submitted due to the limited submission time.", "labels": [], "entities": []}, {"text": "We evaluate their performance using the labels of testing data after the competition.", "labels": [], "entities": []}, {"text": "It maybe because the lemmatized verbal metaphors are more simple, and there will be fewer out-of-vocabulary words in the embedding look-up table.", "labels": [], "entities": []}, {"text": "(4) the ensenmble strategy can also help our model identify metaphors more accurately.", "labels": [], "entities": []}, {"text": "It validates that using a series of models to predict can reduce the data noise and improve the generalization ability of our model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The performance of different methods. *The results of these baseline methods were not submitted due to  the limited submission time. We evaluate their performance using the labels of testing data after the competition.", "labels": [], "entities": []}, {"text": " Table 2: The influence of additional features on our  best-performance model.", "labels": [], "entities": []}, {"text": " Table 2. Here we use the  CNN+LSTM+Softmax model to investigate the in- fluence of features. The results show that both  POS tags and word cluster features can help im- prove the performance of detecting metaphors. It  proves that POS tags contain useful information  to identify the metaphors, since metaphors usually  have specific POS tags and they can be easier to  be identified by incorporating POS information.  Thus, combing the POS tag features is beneficial.  Incorporating the word cluster features is also use- ful to improve the performance. It may be because  words with similar semantic information have  some inherent relatedness and they share similar  metaphor information. Our model can identify  such information better if word cluster features are  incorporated. In addition, it can also enrich the in-", "labels": [], "entities": []}]}