{"title": [{"text": "Target Foresight based Attention for Neural Machine Translation *", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.7454962929089864}]}], "abstractContent": [{"text": "in neural machine translation, an attention model is used to identify the aligned source words fora target word target foresight wordin order to select translation context , but it does not make use of any information of this target foresight word at all.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.7077553669611613}]}, {"text": "previous work proposed an approach to improve the attention model by explicitly accessing this target foresight word and demonstrated the substantial gains in alignment task.", "labels": [], "entities": []}, {"text": "however, this approach is useless in machine translation task on which the target foresight word is unavailable.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.788385659456253}]}, {"text": "in this paper, we propose anew attention model enhanced by the implicit information of target foresight word oriented to both alignment and translation tasks.", "labels": [], "entities": []}, {"text": "empirical experiments on chinese-to-english and japanese-to-english datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and bleu.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 167, "end_pos": 187, "type": "METRIC", "confidence": 0.7844958901405334}, {"text": "bleu", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9939137101173401}]}], "introductionContent": [{"text": "Since neural machine translation (NMT) was proposed (, it has been attracted increasing interests in machine translation community (.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 6, "end_pos": 38, "type": "TASK", "confidence": 0.8151710828145345}, {"text": "machine translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7847747206687927}]}, {"text": "NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9586904048919678}]}, {"text": "Compared with traditional statistical machine translation (), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a * Work done when X.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.6403269072373708}]}, {"text": "Li interning at Tencent AI Lab.", "labels": [], "entities": []}, {"text": "L. Liu is the corresponding author.", "labels": [], "entities": []}, {"text": "pipeline manner, and it thereby has the potential to alleviate the issue of error propagation.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.726550430059433}]}, {"text": "In NMT, the attention mechanism plays an important role.", "labels": [], "entities": []}, {"text": "It calculates the alignments of a target word with respect to the source words for translation context selection.", "labels": [], "entities": [{"text": "translation context selection", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.9061594804128011}]}, {"text": "Although the source words are always available in inference, the target word, called target foresight word, i.e. the first light color word in(a), is not known but to be translated at the next time step.", "labels": [], "entities": []}, {"text": "Therefore, this may lead to inadequate modeling for attention mechanism (.", "labels": [], "entities": []}, {"text": "Regarding to this, explicitly feed this target word into the attention model, and demonstrate the significant improvements in alignment accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.8621794581413269}]}, {"text": "Unfortunately, this approach relies on the premise that the target foresight word is available in advance in its alignment scenario, and thus it cannot be used in the translation scenario.", "labels": [], "entities": []}, {"text": "To address this issue, in this paper, we propose a target foresight based attention (TFA) model oriented to both alignment and translation tasks.", "labels": [], "entities": [{"text": "alignment and translation tasks", "start_pos": 113, "end_pos": 144, "type": "TASK", "confidence": 0.6764770448207855}]}, {"text": "Its basic idea includes two steps: it firstly designs an auxiliary mechanism to predict some information for the target foresight word which is helpful for alignment; and then it feeds the predicted result into the attention model for translation.", "labels": [], "entities": []}, {"text": "For the sake of efficiency, instead of predicting the target foresight word with large vocabulary size, we only predict its partial information, i.e. partof-speech tag, which is proved to be helpful for word alignment ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 203, "end_pos": 217, "type": "TASK", "confidence": 0.774100661277771}]}, {"text": "shows the main idea of TFA based on NMT.", "labels": [], "entities": [{"text": "TFA", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.6343726515769958}, {"text": "NMT", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.8898388743400574}]}, {"text": "In order to remit the negative effects due to the prediction errors, we feed the distribution of the prediction result instead of the maximum a posteriori result into the attention model.", "labels": [], "entities": []}, {"text": "In addition, since the target foresight words are available during the training, we jointly learn the prediction model for the target foresight words and the translation model in a supervised manner.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: \u2022 It proposes a novel TFA-NMT for neural machine translation by using an auxiliary mechanism to predict the target foresight word which is subsequently used to enhance the attention model.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 80, "end_pos": 106, "type": "TASK", "confidence": 0.6936564048131307}]}, {"text": "\u2022 It empirically shows that the proposed TFA-NMT can lead to better alignment accuracy, and achieves significant improvements on both Chinese-toEnglish and Japanese-to-English translation tasks.", "labels": [], "entities": [{"text": "alignment", "start_pos": 68, "end_pos": 77, "type": "TASK", "confidence": 0.9481979012489319}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9461624026298523}, {"text": "Japanese-to-English translation tasks", "start_pos": 156, "end_pos": 193, "type": "TASK", "confidence": 0.7207208275794983}]}], "datasetContent": [{"text": "We conduct experiments on Chinese-toEnglish and Japanese-to-English translation tasks.", "labels": [], "entities": [{"text": "Japanese-to-English translation tasks", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.7667918304602305}]}, {"text": "The specific analyses are based on Chinese-to-English task, and the generalization ability is shown by Japanese-to-English task.", "labels": [], "entities": []}, {"text": "Case-insensitive 4-gram BLEU is used to evaluate translation quality, and the multibleu.perl is adopted as its implementation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9949047565460205}, {"text": "translation", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9582450985908508}]}], "tableCaptions": [{"text": " Table 1: Speeds and performances of the proposed models. \"Speed\" is measured in words/second for  both training and decoding, and performances are measured in terms of BLEU scores (\"BLEU\") and  foresight prediction accuracy (\"FPA\") on the development set. Higher BLEU and FPA scores denote  better performance.", "labels": [], "entities": [{"text": "Speed", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9533577561378479}, {"text": "BLEU scores (\"BLEU", "start_pos": 169, "end_pos": 187, "type": "METRIC", "confidence": 0.8923886716365814}, {"text": "foresight prediction accuracy (\"FPA\")", "start_pos": 195, "end_pos": 232, "type": "METRIC", "confidence": 0.8580705026785532}, {"text": "BLEU", "start_pos": 264, "end_pos": 268, "type": "METRIC", "confidence": 0.9978253841400146}, {"text": "FPA", "start_pos": 273, "end_pos": 276, "type": "METRIC", "confidence": 0.9892565608024597}]}, {"text": " Table 2: Performances on syntactic categories.  \"Base\" denotes \"Nematus\", and Ours denotes  the proposed model.", "labels": [], "entities": []}, {"text": " Table 3: Effect of foresight supervision signal in  training (i.e., \u03bb) and foresight representations in  decoding: Exp for expectation and Map for max- imum a posteriori.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of translation performance on Chinese-to-English task.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9707247018814087}]}, {"text": " Table 5: Evaluation of translation performance on  Japanese-to-English task.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9697796106338501}]}]}