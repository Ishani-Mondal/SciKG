{"title": [{"text": "Improving Implicit Discourse Relation Classification by Modeling Inter-dependencies of Discourse Units in a Paragraph", "labels": [], "entities": [{"text": "Improving Implicit Discourse Relation Classification", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.9094531178474426}]}], "abstractContent": [{"text": "We argue that semantic meanings of a sentence or clause cannot be interpreted independently from the rest of a paragraph, or independently from all discourse relations and the overall paragraph-level discourse structure.", "labels": [], "entities": []}, {"text": "With the goal of improving implicit discourse relation classification, we introduce a paragraph-level neural networks that model inter-dependencies between discourse units as well as discourse relation continuity and patterns, and predict a sequence of discourse relations in a paragraph.", "labels": [], "entities": [{"text": "implicit discourse relation classification", "start_pos": 27, "end_pos": 69, "type": "TASK", "confidence": 0.600111335515976}]}, {"text": "Experimental results show that our model out-performs the previous state-of-the-art systems on the benchmark corpus of PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.6933111548423767}]}], "introductionContent": [{"text": "PDTB-style discourse relations, mostly defined between two adjacent text spans (i.e., discourse units, either clauses or sentences), specify how two discourse units are logically connected (e.g., causal, contrast).", "labels": [], "entities": []}, {"text": "Recognizing discourse relations is one crucial step in discourse analysis and can be beneficial for many downstream NLP applications such as information extraction, machine translation and natural language generation.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7564243376255035}, {"text": "information extraction", "start_pos": 141, "end_pos": 163, "type": "TASK", "confidence": 0.829013854265213}, {"text": "machine translation", "start_pos": 165, "end_pos": 184, "type": "TASK", "confidence": 0.8084307909011841}, {"text": "natural language generation", "start_pos": 189, "end_pos": 216, "type": "TASK", "confidence": 0.64458300669988}]}, {"text": "Commonly, explicit discourse relations were distinguished from implicit ones, depending on whether a discourse connective (e.g., \"because\" and \"after\") appears between two discourse units ().", "labels": [], "entities": []}, {"text": "While explicit discourse relation detection can be framed as a discourse connective disambiguation problem) and has achieved reasonable performance (F1 score > 90%), implicit discourse relations have no discourse connective and are especially difficult to identify (.", "labels": [], "entities": [{"text": "explicit discourse relation detection", "start_pos": 6, "end_pos": 43, "type": "TASK", "confidence": 0.64655701816082}, {"text": "F1 score", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9831750988960266}]}, {"text": "To fill the gap, implicit discourse relation prediction has drawn significant research interest recently and progress has been made (;  by modeling compositional meanings of two discourse units and exploiting word interactions between discourse units using neural tensor networks or attention mechanisms in neural nets.", "labels": [], "entities": [{"text": "implicit discourse relation prediction", "start_pos": 17, "end_pos": 55, "type": "TASK", "confidence": 0.6216230243444443}]}, {"text": "However, most of existing approaches ignore wider paragraph-level contexts beyond the two discourse units that are examined for predicting a discourse relation in between.", "labels": [], "entities": []}, {"text": "To further improve implicit discourse relation prediction, we aim to improve discourse unit representations by positioning a discourse unit (DU) in its wider context of a paragraph.", "labels": [], "entities": [{"text": "discourse relation prediction", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.6621880729993185}]}, {"text": "The key observation is that semantic meaning of a DU cannot be interpreted independently from the rest of the paragraph that contains it, or independently from the overall paragraph-level discourse structure that involve the DU.", "labels": [], "entities": []}, {"text": "Considering the following paragraph with four discourse relations, one relation between each two adjacent DUs: (1): [The Butler, Wis., manufacturer went public at $15.75 a share in DU 1 and (Explicit-Expansion) Sim's goal then was a $29 per-share price by 1992.]", "labels": [], "entities": [{"text": "Butler, Wis., manufacturer", "start_pos": 121, "end_pos": 147, "type": "DATASET", "confidence": 0.8988932013511658}]}, {"text": "DU 2 (Implicit-Expansion) [Strong earnings growth helped achieve that price far ahead of schedule, in August 1988.]", "labels": [], "entities": [{"text": "DU 2", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7627032995223999}]}, {"text": "DU 3 (Implicit-Comparison)The stock has since softened, trading around $25 a share last week and closing yesterday at $23 in national over-the-counter trading.]", "labels": [], "entities": [{"text": "DU 3", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6163447499275208}]}, {"text": "DU 4 But (Explicit-Comparison) [Mr. Sim has set afresh target of $50 a share by the end of reaching that goal.]", "labels": [], "entities": [{"text": "DU", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.5433866381645203}]}, {"text": "Clearly, each DU is an integral part of the paragraph and not independent from other units.", "labels": [], "entities": []}, {"text": "First, predicting a discourse relation may require understanding wider paragraph-level contexts beyond two relevant DUs and the overall discourse structure of a paragraph.", "labels": [], "entities": [{"text": "predicting a discourse relation", "start_pos": 7, "end_pos": 38, "type": "TASK", "confidence": 0.8809617608785629}]}, {"text": "For example, the implicit \"Comparison\" discourse relation between DU3 and DU4 is difficult to identify without the back-ground information (the history of per-share price) introduced in DU1 and DU2.", "labels": [], "entities": [{"text": "DU3", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9123469591140747}, {"text": "DU4", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.6246381998062134}, {"text": "DU2", "start_pos": 194, "end_pos": 197, "type": "DATASET", "confidence": 0.6966332197189331}]}, {"text": "Second, a DU maybe involved in multiple discourse relations (e.g., DU4 is connected with both DU3 and DU5 with a \"Comparison\" relation), therefore the pragmatic meaning representation of a DU should reflect all the discourse relations the unit was involved in.", "labels": [], "entities": []}, {"text": "Third, implicit discourse relation prediction should benefit from modeling discourse relation continuity and patterns in a paragraph that involve easy-to-identify explicit discourse relations (e.g., \"Implicit-Comparison\" relation is followed by \"Explicit-Comparison\" in the above example).", "labels": [], "entities": [{"text": "implicit discourse relation prediction", "start_pos": 7, "end_pos": 45, "type": "TASK", "confidence": 0.7402145639061928}]}, {"text": "Following these observations, we construct a neural net model to process a paragraph each time and jointly build meaning representations for all DUs in the paragraph.", "labels": [], "entities": []}, {"text": "The learned DU representations are used to predict a sequence of discourse relations in the paragraph, including both implicit and explicit relations.", "labels": [], "entities": []}, {"text": "Although explicit relations are not our focus, predicting an explicit relation will help to reveal the pragmatic roles of its two DUs and reconstruct their representations, which will facilitate predicting neighboring implicit discourse relations that involve one of the DUs.", "labels": [], "entities": []}, {"text": "In addition, we introduce two novel designs to further improve discourse relation classification performance of our paragraph-level neural net model.", "labels": [], "entities": [{"text": "discourse relation classification", "start_pos": 63, "end_pos": 96, "type": "TASK", "confidence": 0.7369254430135092}]}, {"text": "First, previous work has indicated that recognizing explicit and implicit discourse relations requires different strategies, we therefore untie parameters in the discourse relation prediction layer of the neural networks and train two separate classifiers for predicting explicit and implicit discourse relations respectively.", "labels": [], "entities": [{"text": "discourse relation prediction", "start_pos": 162, "end_pos": 191, "type": "TASK", "confidence": 0.6881838043530782}]}, {"text": "This unique design has improved both implicit and explicit discourse relation identification performance.", "labels": [], "entities": [{"text": "discourse relation identification", "start_pos": 59, "end_pos": 92, "type": "TASK", "confidence": 0.6617335379123688}]}, {"text": "Second, we add a CRF layer on top of the discourse relation prediction layer to fine-tune a sequence of predicted discourse relations by modeling discourse relation continuity and patterns in a paragraph.", "labels": [], "entities": [{"text": "discourse relation prediction layer", "start_pos": 41, "end_pos": 76, "type": "TASK", "confidence": 0.6886721551418304}]}, {"text": "Experimental results show that the intuitive paragraph-level discourse relation prediction model achieves improved performance on PDTB for both implicit discourse relation classification and explicit discourse relation classification.", "labels": [], "entities": [{"text": "paragraph-level discourse relation prediction", "start_pos": 45, "end_pos": 90, "type": "TASK", "confidence": 0.6211804300546646}, {"text": "implicit discourse relation classification", "start_pos": 144, "end_pos": 186, "type": "TASK", "confidence": 0.603861853480339}, {"text": "explicit discourse relation classification", "start_pos": 191, "end_pos": 233, "type": "TASK", "confidence": 0.6419048756361008}]}], "datasetContent": [{"text": "The Penn Discourse Treebank (PDTB): We experimented with PDTB v2.0 () which is the largest annotated corpus containing 36k discourse relations in 2,159 Wall Street Journal (WSJ) articles.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.937831699848175}, {"text": "Wall Street Journal (WSJ) articles", "start_pos": 152, "end_pos": 186, "type": "DATASET", "confidence": 0.8656922493662152}]}, {"text": "In this work, we focus on the top-level 4 discourse relation senses which are consist of four major semantic classes: Comparison (Comp), Contingency (Cont), Expansion (Exp) and Temporal (Temp).", "labels": [], "entities": []}, {"text": "We followed the same PDTB section partition  as previous work and used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set.", "labels": [], "entities": [{"text": "PDTB section partition", "start_pos": 21, "end_pos": 43, "type": "DATASET", "confidence": 0.8329600691795349}]}, {"text": "presents the data distributions we collected from PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9634678363800049}]}, {"text": "Preprocessing: The PDTB dataset documents its annotations as a list of discourse relations, with each relation associated with its two discourse units.", "labels": [], "entities": [{"text": "PDTB dataset", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.8249229192733765}]}, {"text": "To recover the paragraph context fora discourse relation, we match contents of its two annotated discourse units with all paragraphs in corresponding raw WSJ article.", "labels": [], "entities": [{"text": "WSJ article", "start_pos": 154, "end_pos": 165, "type": "DATASET", "confidence": 0.9395388066768646}]}, {"text": "When all the matching was completed, each paragraph was split into a sequence of discourse units, with one discourse relation (implicit or explicit) between each two ad- shows the distribution of paragraphs based on the number of discourse units in a paragraph.", "labels": [], "entities": []}, {"text": "On the PDTB corpus, both binary classification and multi-way classification settings are commonly used to evaluate the implicit discourse relation recognition performance.", "labels": [], "entities": [{"text": "PDTB corpus", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9257600009441376}, {"text": "discourse relation recognition", "start_pos": 128, "end_pos": 158, "type": "TASK", "confidence": 0.579539825518926}]}, {"text": "We noticed that all the recent works report class-wise implicit relation prediction performance in the binary classification setting, while none of them report detailed performance in the multi-way classification setting.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.6382064074277878}]}, {"text": "In the binary classification setting, separate \"oneversus-all\" binary classifiers were trained, and each classifier is to identify one class of discourse relations.", "labels": [], "entities": []}, {"text": "Although separate classifiers are generally more flexible in combating with imbalanced distributions of discourse relation classes and obtain higher class-wise prediction performance, one pair of discourse units maybe tagged with all four discourse relations without proper conflict resolution.", "labels": [], "entities": [{"text": "conflict resolution", "start_pos": 274, "end_pos": 293, "type": "TASK", "confidence": 0.7876168489456177}]}, {"text": "Therefore, the multi-way classification setting is more appropriate and natural in evaluating a practical end-to-end discourse parser, and we mainly evaluate our proposed models using the four-way multi-class classification setting.", "labels": [], "entities": []}, {"text": "Since none of the recent previous work reported class-wise implicit relation classification performance in the multi-way classification setting, for better comparisons, we re-implemented the neural tensor network architecture (so-called SWIM in () which is essentially a Bi-LSTM model with tensors and report its detailed evaluation result in the multi-way classification setting.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.6771979182958603}]}, {"text": "As another baseline, we report the performance of a Bi-LSTM model without tensors as well.", "labels": [], "entities": []}, {"text": "Both baseline models take two relevant discourse units as the only input.", "labels": [], "entities": []}, {"text": "For additional comparisons, We also report the performance of our proposed models in the binary classification setting.", "labels": [], "entities": []}, {"text": "Multi-way Classification: The first section of table 3 shows macro average F1-scores and accuracies of previous works.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9229954481124878}, {"text": "accuracies", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9807676672935486}]}, {"text": "The second section of table 3 shows the multi-class classification results of our implemented baseline systems.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.6922629028558731}]}, {"text": "Consistent with results of previous works, neural tensors, when applied to Bi-LSTMs, improved implicit discourse relation prediction performance.", "labels": [], "entities": [{"text": "implicit discourse relation prediction", "start_pos": 94, "end_pos": 132, "type": "TASK", "confidence": 0.5830383077263832}]}, {"text": "However, the performance on the three small classes (Comp, Cont and Temp) remains low.", "labels": [], "entities": []}, {"text": "The third section of table 3 shows the multi-class classification results of our proposed paragraph-level neural network models that capture inter-dependencies among discourse units.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6754578053951263}]}, {"text": "The first row shows the performance of a variant of our basic model, where we only identify implicit relations and ignore identifying explicit relations by setting the \u03b1 in equation to be 0.", "labels": [], "entities": []}, {"text": "Compared with the baseline Bi-LSTM model, the only difference is that this model considers paragraph-wide contexts and model inter-dependencies among discourse units when building representation for individual DU.", "labels": [], "entities": []}, {"text": "We can see that this model has greatly improved implicit relation classification perfor-   mance across all the four relations and improved the macro-average F1-score by over 7 percents.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9194800853729248}]}, {"text": "In addition, compared with the baseline Bi-LSTM model with tensor, this model improved implicit relation classification performance across the three small classes, with clear performance gains of around 2 and 8 percents on contingency and temporal relations respectively, and overall improved the macro-average F1-score by 2.2 percents.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.6783357560634613}, {"text": "F1-score", "start_pos": 311, "end_pos": 319, "type": "METRIC", "confidence": 0.9681136608123779}]}, {"text": "The second row shows the performance of our basic paragraph-level model which predicts both implicit and explicit discourse relations in a paragraph.", "labels": [], "entities": []}, {"text": "Compared to the variant system (the first row), the basic model further improved the classification performance on the first three implicit relations.", "labels": [], "entities": []}, {"text": "Especially on the contingency relation, the classification performance was improved by another 1.42 percents.", "labels": [], "entities": [{"text": "classification", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.860409140586853}]}, {"text": "Moreover, the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.8860762715339661}, {"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9991583824157715}]}, {"text": "After untying parameters in the softmax prediction layer, implicit discourse relation classification performance was improved across all four relations, meanwhile, the explicit discourse relation classification performance was also improved.", "labels": [], "entities": [{"text": "implicit discourse relation classification", "start_pos": 58, "end_pos": 100, "type": "TASK", "confidence": 0.567497156560421}, {"text": "explicit discourse relation classification", "start_pos": 168, "end_pos": 210, "type": "TASK", "confidence": 0.6449171602725983}]}, {"text": "The CRF layer further improved implicit discourse relation recognition performance on the three small classes.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.5788709446787834}]}, {"text": "In summary, our full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., () by more than 2 percents and outperforms the best previous system () by 1 percent.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9718367457389832}, {"text": "predicting implicit discourse relations", "start_pos": 112, "end_pos": 151, "type": "TASK", "confidence": 0.8147132843732834}]}, {"text": "Binary Classification: From table 4, we can see that compared against the best previous systems, our paragraph-level model with untied parameters in the prediction layer achieves F1-score improvements of 6 points on Comparison and 7 points on Temporal, which demonstrates that paragraphwide contexts are important in detecting minority discourse relations.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9979719519615173}, {"text": "detecting minority discourse relations", "start_pos": 317, "end_pos": 355, "type": "TASK", "confidence": 0.7841685563325882}]}, {"text": "Note that the CRF layer of the model is not suitable for binary classification.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.6461372077465057}]}], "tableCaptions": [{"text": " Table 1: Distributions of Four Top-level Discourse Re- lations in PDTB.", "labels": [], "entities": [{"text": "Top-level Discourse Re- lations", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.5291245460510254}, {"text": "PDTB", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.8856475353240967}]}, {"text": " Table 2: Distributions of Paragraphs.", "labels": [], "entities": []}, {"text": " Table 3: Multi-class Classification Results on PDTB. We report accuracy (Acc) and macro-average F1-scores for  both explicit and implicit discourse relation predictions. We also report class-wise F1 scores.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.8750272393226624}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9994995594024658}, {"text": "Acc)", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.8968135118484497}, {"text": "F1-scores", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9382410645484924}, {"text": "F1", "start_pos": 197, "end_pos": 199, "type": "METRIC", "confidence": 0.9628292918205261}]}, {"text": " Table 4: Binary Classification Results on PDTB. We report F1-scores for implicit discourse relations.", "labels": [], "entities": [{"text": "Binary Classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8647302687168121}, {"text": "PDTB", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8199148774147034}, {"text": "F1-scores", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9964516162872314}]}, {"text": " Table 5: Multi-class Classification Results of Ensemble Models on PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9500773549079895}]}]}