{"title": [{"text": "Adversarial Example Generation with Syntactically Controlled Paraphrase Networks", "labels": [], "entities": [{"text": "Adversarial Example Generation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.75570015112559}]}], "abstractContent": [{"text": "We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples.", "labels": [], "entities": []}, {"text": "Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax.", "labels": [], "entities": []}, {"text": "We show it is possible to create training data for this task by first doing back-translation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process.", "labels": [], "entities": []}, {"text": "Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax.", "labels": [], "entities": []}, {"text": "A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems.", "labels": [], "entities": []}, {"text": "Furthermore, they are more capable of generating syntactically ad-versarial examples that both (1) \"fool\" pre-trained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing datasets often suffer from a dearth of linguistic variation, which can hurt the generalization of models trained on them.", "labels": [], "entities": []}, {"text": "Recent work has shown it is possible to easily \"break\" many learned models by evaluating them on adversarial examples (), which are generated by manually introducing lexical, pragmatic, and syntactic variation not seen in the training set (.", "labels": [], "entities": []}, {"text": "Robustness to such adversarial examples can potentially be improved by augmenting the training data, as shown by prior work that introduces rulebased lexical substitutions ( Authors contributed equally.", "labels": [], "entities": []}, {"text": ": Adversarial examples for sentiment analysis (left) and textual entailment (right) generated by our syntactically controlled paraphrase network (SCPN) according to provided parse templates.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9551872909069061}]}, {"text": "In both cases, a pretrained classifier correctly predicts the label of the original sentence but not the corresponding paraphrase.", "labels": [], "entities": []}, {"text": "However, more complex transformations, such as generating syntactically adversarial examples, remain an open challenge, as input semantics must be preserved in the face of potentially substantial structural modifications.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew approach for learning to do syntactically controlled paraphrase generation: given a sentence and a target syntactic form (e.g., a constituency parse), a system must produce a paraphrase of the sentence whose syntax conforms to the target.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.7117928266525269}]}, {"text": "General purpose syntactically controlled paraphrase generation is a challenging task.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7582307159900665}]}, {"text": "Approaches that rely on handcrafted rules and grammars, such as the question generation system of, support only a limited number of syntactic targets.", "labels": [], "entities": [{"text": "question generation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7696213126182556}]}, {"text": "We introduce the first learning approach for this problem, building on the generality of neural encoder-decoder models to support a wide range of transformations.", "labels": [], "entities": []}, {"text": "In doing so, we face two new challenges: (1) obtaining a large amount of paraphrase pairs for training, and (2) defining syntactic transformations with which to label these pairs.", "labels": [], "entities": []}, {"text": "Since no large-scale dataset of sentential paraphrases exists publicly, we follow  and automatically generate millions of paraphrase pairs using neural backtranslation.", "labels": [], "entities": []}, {"text": "Backtranslation naturally injects linguistic variation between the original sentence and its backtranslated counterpart.", "labels": [], "entities": []}, {"text": "By running the process at a very large scale and testing for the specific variations we want to produce, we can gather ample input-output pairs fora wide range of phenomena.", "labels": [], "entities": []}, {"text": "Our focus is on syntactic transformations, which we define using templates derived from linearized constituency parses ( \u00a72).", "labels": [], "entities": []}, {"text": "Given such parallel data, we can easily train an encoder-decoder model that takes a sentence and target syntactic template as input, and produces the desired paraphrase.", "labels": [], "entities": []}, {"text": "A combination of automated and human evaluations show that the generated paraphrases almost always follow their target specifications, while paraphrase quality does not significantly deteriorate compared to vanilla neural backtranslation ( \u00a74).", "labels": [], "entities": []}, {"text": "Our model, the syntactically controlled paraphrase network (SCPN), is capable of generating adversarial examples for sentiment analysis and textual entailment datasets that significantly impact the performance of pretrained models).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.9390178918838501}]}, {"text": "We also show that augmenting training sets with such examples improves robustness without harming accuracy on the original test sets ( \u00a75).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9988038539886475}]}, {"text": "Together these results not only establish the first general purpose syntactically controlled paraphrase approach, but also suggest that this general paradigm could be used for controlling many other aspects of the target text.", "labels": [], "entities": []}], "datasetContent": [{"text": "Before using SCPN to generate adversarial examples on downstream datasets, we need to make sure that its output paraphrases are valid and grammatical and that its outputs follow the specified target syntax.", "labels": [], "entities": []}, {"text": "In this section, we compare SCPN to a neural backtranslation baseline (NMT-BT) on the development set of our PARANMT-50M split using both human and automated experiments.", "labels": [], "entities": [{"text": "PARANMT-50M split", "start_pos": 109, "end_pos": 126, "type": "DATASET", "confidence": 0.757125049829483}]}, {"text": "NMT-BT is the same pretrained Czech-English model used to create PARANMT-50M; however, here we use it to generate in both directions (i.e., EnglishCzech and Czech-English).", "labels": [], "entities": [{"text": "NMT-BT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9466463327407837}]}, {"text": "However, we do provide some qualitative examples of rare and medium-frequency templates in.", "labels": [], "entities": []}, {"text": "After qualitatively analyzing the impact of different filtering choices, we set minimum n-gram overlap to 0.5 and  We evaluate our syntactically adversarial paraphrases on the Stanford Sentiment Treebank ( and SICK entailment detection ().", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 176, "end_pos": 203, "type": "DATASET", "confidence": 0.9064747293790182}, {"text": "SICK entailment detection", "start_pos": 210, "end_pos": 235, "type": "TASK", "confidence": 0.7911080916722616}]}, {"text": "While both are relatively small datasets, we select them because they offer different experimental conditions: SST contains complicated sentences with high syntactic variance, while SICK almost exclusively consists of short, simple sentences.", "labels": [], "entities": []}, {"text": "As a baseline, we compare the ten most probable beams from NMT-BT to controlled paraphrases generated by SCPN using ten templates randomly sampled from the template set described in Section 3.3.", "labels": [], "entities": []}, {"text": "We also need pretrained models We also experimented with the diverse beam search modification proposed by for NMT-BT but found that it dramatically warped the semantics of many beams; crowdsourced workers rated 49% of its outputs as 0 template paraphrase original with the help of captain picard , the borg will be prepared for everything .", "labels": [], "entities": [{"text": "NMT-BT", "start_pos": 110, "end_pos": 116, "type": "DATASET", "confidence": 0.9072157740592957}]}], "tableCaptions": [{"text": " Table 1: A crowdsourced paraphrase evaluation on a  three-point scale (0 = no paraphrase, 1 = ungrammat- ical paraphrase, 2 = grammatical paraphrase) shows  both that NMT-BT and SCPN produce mostly grammat- ical paraphrases. Feeding parse templates to SCPN in- stead of full parses does not impact its quality.", "labels": [], "entities": []}, {"text": " Table 4: SCPN generates more legitimate adversarial examples than NMT-BT, shown by the results of a crowd- sourced validity experiment and the percentage of held-out examples that are broken through paraphrasing. Fur- thermore, we show that by augmenting the training dataset with syntactically-diverse paraphrases, we can improve  the robustness of downstream models to syntactic adversaries (see \"Dev Broken\" before and after augmentation)  without harming accuracy on the original test set.", "labels": [], "entities": [{"text": "Fur- thermore", "start_pos": 214, "end_pos": 227, "type": "METRIC", "confidence": 0.9101172486941019}, {"text": "accuracy", "start_pos": 460, "end_pos": 468, "type": "METRIC", "confidence": 0.9985150694847107}]}]}