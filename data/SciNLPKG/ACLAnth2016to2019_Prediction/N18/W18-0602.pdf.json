{"title": [{"text": "A Linguistically-Informed Fusion Approach for Multimodal Depression Detection", "labels": [], "entities": [{"text": "Multimodal Depression Detection", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.6936755577723185}]}], "abstractContent": [{"text": "Automated depression detection is inherently a multimodal problem.", "labels": [], "entities": [{"text": "Automated depression detection", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6169240971406301}]}, {"text": "Therefore, it is critical that researchers investigate fusion techniques for multimodal design.", "labels": [], "entities": []}, {"text": "This paper presents the first ever comprehensive study of fusion techniques for depression detection.", "labels": [], "entities": [{"text": "depression detection", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.9419613480567932}]}, {"text": "In addition, we present novel linguistically-motivated fusion techniques, which we find outperform existing approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Depression is an extremely heterogeneous disorder that is difficult to diagnose.", "labels": [], "entities": []}, {"text": "Given this difficulty, psychologists and linguists have investigated possible objective markers and have shown that depression influences how a person behaves and communicates, affecting facial expression, prosody, syntax, and semantics.", "labels": [], "entities": []}, {"text": "Given that depression affects both nonverbal and verbal behavior, an automated detection system should be multimodal.", "labels": [], "entities": []}, {"text": "Initial studies on depression detection from multimodal features have shown performance gains can be achieved by combining information from various modalities.", "labels": [], "entities": [{"text": "depression detection", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.7358189970254898}]}, {"text": "However, few studies have investigated fusion approaches for depression detection.", "labels": [], "entities": [{"text": "depression detection", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.9623379409313202}]}, {"text": "In this paper, we present a novel linguistically motivated approach to fusion: syntaxinformed fusion.", "labels": [], "entities": []}, {"text": "We compare this novel approach to early fusion and find it is able to outperform it.", "labels": [], "entities": [{"text": "early fusion", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.73039710521698}]}, {"text": "We also demonstrate that this approach overcomes some of the limitations of early fusion.", "labels": [], "entities": []}, {"text": "Moreover, we test our approach's robustness by applying the same framework to generate a visual-informed fusion model.", "labels": [], "entities": []}, {"text": "We find video-informed fusion also outperforms early fusion.", "labels": [], "entities": []}, {"text": "In addition to presenting novel fusion techniques, we also evaluate existing approaches to fusion including early, late, and hybrid fusion.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this work presents the first in-depth investigation of fusion techniques for depression detection.", "labels": [], "entities": [{"text": "depression detection", "start_pos": 107, "end_pos": 127, "type": "TASK", "confidence": 0.9287767708301544}]}, {"text": "Lastly, we present interesting results to further support the relationship between depression and syntax.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we use the Distress Analysis Interview Corpus-Wizard of Oz (DAIC-WOZ;).", "labels": [], "entities": [{"text": "Distress Analysis Interview Corpus-Wizard of Oz (DAIC-WOZ", "start_pos": 25, "end_pos": 82, "type": "DATASET", "confidence": 0.7794655039906502}]}, {"text": "The corpus is multimodal (video, audio, and transcripts) and is comprised of video interviews between participants and an animated virtual interviewer called Ellie, which is controlled by a human interviewer in another room.", "labels": [], "entities": []}, {"text": "Interview participants were drawn from the Greater Los Angeles metropolitan area and included two distinct populations: (1) the general public and (2) veterans of the U.S. armed forces.", "labels": [], "entities": []}, {"text": "Participants were coded for depression, Posttraumatic Stress Disorder (PTSD), and anxiety based on accepted psychiatric questionnaires.", "labels": [], "entities": [{"text": "Posttraumatic Stress Disorder (PTSD)", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.7440682798624039}]}, {"text": "All participants were fluent English speakers and all interviews were conducted in English.", "labels": [], "entities": []}, {"text": "The DAIC-WOZ interviews ranged from 5 to 20 minutes.", "labels": [], "entities": [{"text": "DAIC-WOZ interviews", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8798343539237976}]}, {"text": "The interview started with neutral questions, which were designed to build rapport and make the participant comfortable.", "labels": [], "entities": []}, {"text": "The interview then progressed into more targeted questions about symptoms and events related to depression and PTSD.", "labels": [], "entities": []}, {"text": "Lastly, the interview ended with a 'cool-down' phase, which ensured that participants would not leave the interview in a distressed state.", "labels": [], "entities": []}, {"text": "The depression label provided includes a PHQ-8 1 score (scale from 0 to 24) as well as a binary depression class label, i.e., score >= 10.", "labels": [], "entities": [{"text": "PHQ-8 1 score", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.8922932545344034}]}, {"text": "which uses) and Parsey McParseface () to extract voice and syntax features.", "labels": [], "entities": [{"text": "Parsey McParseface", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.8012389540672302}]}, {"text": "In order to evaluate our approach, we conduct a series of participant-level binary classification experiments.", "labels": [], "entities": []}, {"text": "We train both unimodal and multimodal models.", "labels": [], "entities": []}, {"text": "Our early + syntax-informed fusion model combines both the early fusion and syntax-informed fusion feature sets, by early fusion, i.e. simple concatenation.", "labels": [], "entities": []}, {"text": "Using scikitlearn we train a Support Vector Machine (SVM) for classification, (linear kernel, C = 0.1).", "labels": [], "entities": []}, {"text": "We conduct 5-fold cross-validation on 136 participant interviews (depressed = 26, non-depressed = 110).", "labels": [], "entities": []}, {"text": "During cross-validation, each fold is speaker independent and drawn at random.", "labels": [], "entities": []}, {"text": "Given the skewness of the dataset, we set the SVM model's class weight parameter to 'balanced', which automatically adjusts the weights of the model inversely proportional to the class frequencies in the data, helping adjust for the class imbalance.", "labels": [], "entities": []}, {"text": "Given the possibility of sparse feature values and the differences in dimensionality across feature sets, we also perform feature selection.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7748779356479645}]}, {"text": "We use scikitlearn's Select K-Best feature selection approach, which computes the ANOVA F-value across features and identifies the K most significant features.", "labels": [], "entities": [{"text": "ANOVA F-value", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.831879198551178}]}, {"text": "We set K to 20 and evaluate each feature set's best set.", "labels": [], "entities": []}, {"text": "We report our findings in?.", "labels": [], "entities": [{"text": "?", "start_pos": 25, "end_pos": 26, "type": "DATASET", "confidence": 0.8999630808830261}]}, {"text": "We report precision, recall, and F1-score for the depressed class.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9997814297676086}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9997497200965881}, {"text": "F1-score", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.999813973903656}]}, {"text": "We choose to report these values instead of the average values across both classes because the depressed class label is the harder class to detect.", "labels": [], "entities": []}, {"text": "As a result, the non-depressed class usually reports very high scores which tend to inflate the average score.", "labels": [], "entities": []}, {"text": "If we can increase the performance of the depressed class, it can be assumed that the overall performance will go up as a result.", "labels": [], "entities": []}, {"text": "We find that the novel syntax-informed fusion approach performs best, with an F1-score of 0.49.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9996496438980103}]}, {"text": "We believe this approach is able to leverage syntactic information to target more informative aspects of the speech signal resulting in higher performing models.", "labels": [], "entities": []}, {"text": "By conditioning acoustic models on syntactic information this approach com- bines information from both modalities in away a human clinician might.", "labels": [], "entities": []}, {"text": "Syntax-informed fusion substantially outperforms early fusion in precision and F1-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9997683167457581}, {"text": "F1-score", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9960899353027344}]}, {"text": "In recall, performance is similar for both approaches.", "labels": [], "entities": [{"text": "recall", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.9833813905715942}]}, {"text": "In addition, the syntax-informed method surfaces novel multimodal features.", "labels": [], "entities": []}, {"text": "For example, creak is not a useful feature in the early fusion or the acoustic model.", "labels": [], "entities": []}, {"text": "However, when we consider verb creak we find it extremely useful.", "labels": [], "entities": []}, {"text": "To better understand each model, we inspect the coefficient weights of the SVM models.", "labels": [], "entities": []}, {"text": "Using the weight coefficients from the models, we plot the top 5 most important features by class in.", "labels": [], "entities": []}, {"text": "The absolute size of the coefficients in relation to each other can be used to determine feature importance for the depression detection task.", "labels": [], "entities": [{"text": "depression detection task", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.8955732186635336}]}, {"text": "If we consider the audio and early fusion models in, we find that both models weight the same features highly.", "labels": [], "entities": []}, {"text": "Although the early fusion model also includes the set of syntax features, it still prefers the same five features as the audioonly model.", "labels": [], "entities": []}, {"text": "Since early fusion is simply concatenating the audio and syntax feature vectors it is understandable to find similar features performing well.", "labels": [], "entities": []}, {"text": "These results show the promise of these specific audio features, which include spectral and prosodic (F 0 ) features.", "labels": [], "entities": []}, {"text": "These results support previous work that showed spectral and prosodic features were useful for detecting depression.", "labels": [], "entities": [{"text": "detecting depression", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.9416549801826477}]}, {"text": "However, these findings also highlight the limitation of early fusion.", "labels": [], "entities": [{"text": "early fusion", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.7123130857944489}]}, {"text": "The intention behind early fusion is to have access to multiple modalities that observe the same phenomenon to allow for more robust predictions, allowing for complementary information from each modality.", "labels": [], "entities": [{"text": "early fusion", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.7474675476551056}]}, {"text": "Something not visible in individual modalities may appear when using multiple modalities.", "labels": [], "entities": []}, {"text": "However, in early fusion, we cannot guarantee that information from both modalities is considered.", "labels": [], "entities": []}, {"text": "For example, if we inspect the feature set for early fusion we find that no syntax features appear; this could be attributed to the strength of the audio features as well as the difference in dimensionality size between the audio and syntax sets; the audio feature set is almost 5 times larger than the syntax set.", "labels": [], "entities": []}, {"text": "The syntax-informed fusion model is promising because it does not possess the same limitation as early fusion; with syntax-informed fusion we can guarantee that information from both modalities is considered.", "labels": [], "entities": []}, {"text": "This could also be considered: Results for fusion experiments using SVM.Results for fusion approaches including features from audio (A), syntax (S), and video (V).", "labels": [], "entities": []}, {"text": "a drawback of syntax-informed fusion, in circumstances where one would like to be agnostic regarding the value of each modality.", "labels": [], "entities": [{"text": "syntax-informed fusion", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.6699155867099762}]}, {"text": "However, in a task for which multiple modalities are known to be important and interconnected, such as depression detection, it is valuable to represent them jointly.", "labels": [], "entities": [{"text": "depression detection", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.8236269056797028}]}, {"text": "The syntax-informed fusion model in demonstrates that syntax-informed fusion is able to capture important information from both modalities.", "labels": [], "entities": []}, {"text": "We find the best features used to distinguish between classes are spectral features that span the production of pronouns, verbs, and adverbials.", "labels": [], "entities": []}, {"text": "In other words, the best syntax-informed features represent a fused multimodal representation of the best features from each unimodal domain.", "labels": [], "entities": []}, {"text": "We also find further support of the relationship between depression and syntax.", "labels": [], "entities": []}, {"text": "From the syntaxonly model, we find pronouns (PRON) to be useful in identifying the depressed class, which supports previous findings that pronoun use can help identify depression).", "labels": [], "entities": []}, {"text": "In addition, we find the POS tag category X (other) to be useful in distinguishing between classes.", "labels": [], "entities": []}, {"text": "After manually inspecting the transcripts, we find the X POS tag is often assigned to filler words such as uh, um, mm.", "labels": [], "entities": [{"text": "X POS tag", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.7353583375612894}]}, {"text": "These results suggest filler words can be helpful in identifying depression.", "labels": [], "entities": [{"text": "identifying depression", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.9122351109981537}]}, {"text": "Lastly, we find adverbials (ADV) to be useful in distinguishing between classes.", "labels": [], "entities": []}, {"text": "These results are especially interesting because argued that adverbial clauses could help predict the improvement of depression symptoms.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, these results are the first to show support that adverbial clauses could also help predict depression.", "labels": [], "entities": []}, {"text": "We find similar results for video-informed fusion.", "labels": [], "entities": []}, {"text": "Video-informed fusion outperforms early fusion in recall and F1-score.", "labels": [], "entities": [{"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9983299374580383}, {"text": "F1-score", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.995976984500885}]}, {"text": "Similar to syntaxinformed fusion we find that video-informed fused features are able to jointly capture the most informative features from each individual modality.", "labels": [], "entities": []}, {"text": "For example, we find the best performing acoustic features and AUs from the unimodal systems to appear together in the video-informed system 5 .  In addition to evaluating how well our novel approach compares to early fusion, we also evaluate other types of fusion such as late and hybrid fusion.", "labels": [], "entities": []}, {"text": "These series of experiments follow the same configuration as our first series of experiments: 5 fold cross-validation using SVM (linear kernel, C = 0.1, class weights balanced).", "labels": [], "entities": []}, {"text": "We evaluate each method of fusion -early, informed, late (vote/ensemble), and hybrid (early/informed) -and report our results in?.", "labels": [], "entities": []}, {"text": "As mentioned previously, in regards to early fusion methods, the informed fusion approaches outperform simple early fusion.", "labels": [], "entities": []}, {"text": "When we compare the syntax and video-informed fusion techniques with other approaches, such as late and hybrid fusion, we do not find differences between the systems.", "labels": [], "entities": []}, {"text": "When we evaluate systems that use all three modalities (A + S + V), we find a late ensemble approach performs best.", "labels": [], "entities": []}, {"text": "We also find that late fusion techniques which rely on voting perform the worst.", "labels": [], "entities": []}, {"text": "We believe these results can be attributed to the low performing unimodal video system, as demonstrated in?.", "labels": [], "entities": []}, {"text": "This finding highlights a weakness of the late fusion (voting) approach.", "labels": [], "entities": [{"text": "late fusion (voting)", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.6389817237854004}]}, {"text": "Since it weighs the prediction from each system equally, this can lead to poor performance when one of the unimodal systems is weak.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for 5-fold cross-validation using SVM. Results reported for the audio  (A), syntax (S), video (V), and fusion (A + S) approaches. Fusion types include early  (E), syntax-informed (I), and both (E + I).", "labels": [], "entities": []}, {"text": " Table 2: Results for fusion experiments using SVM.Results for fusion approaches including fea- tures from audio (A), syntax (S), and video (V).", "labels": [], "entities": []}]}