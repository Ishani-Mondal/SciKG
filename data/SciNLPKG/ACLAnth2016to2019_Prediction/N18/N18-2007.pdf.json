{"title": [{"text": "Neural Models for Reasoning over Multiple Mentions using Coreference", "labels": [], "entities": [{"text": "Coreference", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.8117092847824097}]}], "abstractContent": [{"text": "Many problems in NLP require aggregating information from multiple mentions of the same entity which maybe far apart in the text.", "labels": [], "entities": []}, {"text": "Existing Recurrent Neural Network (RNN) layers are biased towards short-term dependencies and hence not suited to such tasks.", "labels": [], "entities": []}, {"text": "We present a recurrent layer which is instead biased towards coreferent dependencies.", "labels": [], "entities": []}, {"text": "The layer uses coreference annotations extracted from an external system to connect entity mentions belonging to the same cluster.", "labels": [], "entities": []}, {"text": "Incorporating this layer into a state-of-the-art reading comprehension model improves performance on three datasets-Wikihop, LAMBADA and the bAbi AI tasks-with large gains when training data is scarce.", "labels": [], "entities": []}], "introductionContent": [{"text": "A long-standing goal of NLP is to build systems capable of reasoning about the information present in text.", "labels": [], "entities": [{"text": "reasoning about the information present in text", "start_pos": 59, "end_pos": 106, "type": "TASK", "confidence": 0.7003713165010724}]}, {"text": "One important form of reasoning for Question Answering (QA) models is the ability to aggregate information from multiple mentions of entities.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.8510111927986145}]}, {"text": "We call this coreference-based reasoning since multiple pieces of information, which may lie across sentence, paragraph or document boundaries, are tied together with the help of referring expressions which denote the same real-world entity.", "labels": [], "entities": []}, {"text": "QA models which directly read text to answer questions (commonly known as Reading Comprehension systems) (), typically consist of RNN layers.", "labels": [], "entities": []}, {"text": "RNN layers have a bias towards sequential recency, i.e. a tendency to favor short-term dependencies.", "labels": [], "entities": []}, {"text": "Attention mechanisms alleviate part of the issue, but empirical studies suggest RNNs with attention also have difficulty modeling long-term dependencies.", "labels": [], "entities": []}, {"text": "We conjecture that when training data is scarce, and inductive biases play an important role, RNN-based models would have trouble with coreference-based reasoning.", "labels": [], "entities": []}, {"text": "Example questions which require coreference-based reasoning from the bAbi dataset (top) and Wikihop dataset (bottom).", "labels": [], "entities": [{"text": "bAbi dataset", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9466579258441925}, {"text": "Wikihop dataset", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.963474452495575}]}, {"text": "Coreferences are in bold, and the correct answers are underlined.", "labels": [], "entities": []}, {"text": "At the same time, systems for coreference resolution have seen a gradual increase inaccuracy over the years (.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.9754338562488556}]}, {"text": "Hence, in this work we use the annotations produced by such systems to adapt a standard RNN layer by introducing a bias towards coreferent recency.", "labels": [], "entities": []}, {"text": "Specifically, given an input sequence and coreference clusters extracted from an external system, we introduce a term in the update equations for Gated Recurrent Units (GRU) () which depends on the hidden state of the coreferent antecedent of the current token (if it exists).", "labels": [], "entities": []}, {"text": "This way hidden states are propagated along coreference chains and the original sequence in parallel.", "labels": [], "entities": []}, {"text": "We compare our Coref-GRU layer with the regular GRU layer by incorporating it in a recent model for reading comprehension.", "labels": [], "entities": []}, {"text": "On synthetic data specifically constructed to test coreferencebased reasoning), C-GRUs lead to a large improvement over regular GRUs.", "labels": [], "entities": []}, {"text": "We show that the structural bias introduced and coreference signals are both important to reach high performance in this case.", "labels": [], "entities": []}, {"text": "On a more re-alistic dataset, with noisy coreference annotations, we see small but significant improvements over a state-of-the-art baseline.", "labels": [], "entities": []}, {"text": "As we reduce the training data, the gains become larger.", "labels": [], "entities": []}, {"text": "Lastly, we apply the same model to a broad-context language modeling task (, where coreference resolution is an important factor, and show improved performance over state-of-the-art.", "labels": [], "entities": [{"text": "broad-context language modeling task", "start_pos": 37, "end_pos": 73, "type": "TASK", "confidence": 0.7277146875858307}, {"text": "coreference resolution", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.8790718019008636}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Accuracy on bAbi-1K, averaged across all 20  tasks. Following previous work we run each task for 10  random seeds, and report the Avg and Max (based on  dev set) performance. A task is considered failed if its  Max performance is < 0.95.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9970616698265076}, {"text": "Avg", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9919841885566711}]}, {"text": " Table 2: Accuracy on Wikihop. Follow: annotated as  answer follows from the given passages. Follow +mul- tiple: annotated as requiring multiple passages for an- swering. Follow +single annotated as requiring one  passage for answering.  \u2020 p = 0.057 using Mcnemar's  test compared to GA w/ GRU.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9916290044784546}, {"text": "Mcnemar's  test", "start_pos": 256, "end_pos": 271, "type": "DATASET", "confidence": 0.8315746784210205}, {"text": "GA w/ GRU", "start_pos": 284, "end_pos": 293, "type": "DATASET", "confidence": 0.6934141218662262}]}, {"text": " Table 3: Accuracy on LAMBADA test set, averaged  across two runs with random initializations. context:  passages for which the answer is in context. overall:  full test set for comparison to prior work.  \u2020 p < 0.0001  using Mcnemar's test compared to GA w/ GRU.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9921852946281433}, {"text": "LAMBADA test set", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.8490192095438639}, {"text": "Mcnemar's test", "start_pos": 225, "end_pos": 239, "type": "DATASET", "confidence": 0.8724971016248068}, {"text": "GA w/ GRU", "start_pos": 252, "end_pos": 261, "type": "DATASET", "confidence": 0.7208205163478851}]}, {"text": " Table 4: Breakdown of task-wise performance on bAbi  dataset. Tasks where C-GRU is significant better /  worse than either GRU or QRNs are highlighted.", "labels": [], "entities": [{"text": "bAbi  dataset", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9379386007785797}]}]}