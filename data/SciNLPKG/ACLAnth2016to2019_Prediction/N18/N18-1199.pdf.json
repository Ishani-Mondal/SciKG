{"title": [{"text": "Quantifying the visual concreteness of words and topics in multimodal datasets", "labels": [], "entities": [{"text": "Quantifying the visual concreteness of words and topics", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.6461136564612389}]}], "abstractContent": [{"text": "Multimodal machine learning algorithms aim to learn visual-textual correspondences.", "labels": [], "entities": []}, {"text": "Previous work suggests that concepts with concrete visual manifestations maybe easier to learn than concepts with abstract ones.", "labels": [], "entities": []}, {"text": "We give an algorithm for automatically computing the visual concreteness of words and topics within multimodal datasets.", "labels": [], "entities": []}, {"text": "We apply the approach in four settings, ranging from image captions to images/text scraped from historical books.", "labels": [], "entities": []}, {"text": "In addition to enabling explorations of concepts in multi-modal datasets, our concreteness scores predict the capacity of machine learning algorithms to learn textual/visual relationships.", "labels": [], "entities": []}, {"text": "We find that 1) concrete concepts are indeed easier to learn; 2) the large number of algorithms we consider have similar failure cases; 3) the precise positive relationship between concreteness and performance varies between datasets.", "labels": [], "entities": []}, {"text": "We conclude with recommendations for using con-creteness scores to facilitate future multi-modal research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text and images are often used together to serve as a richer form of content.", "labels": [], "entities": []}, {"text": "For example, news articles maybe accompanied by photographs or infographics; images shared on social media are often coupled with descriptions or tags; and textbooks include illustrations, photos, and other visual elements.", "labels": [], "entities": []}, {"text": "The ubiquity and diversity of such \"text+image\" material (henceforth referred to as multimodal content) suggest that, from the standpoint of sharing information, images and text are often natural complements.", "labels": [], "entities": []}, {"text": "Ideally, machine learning algorithms that incorporate information from both text and images should have a fuller perspective than those that consider either text or images in isolation.", "labels": [], "entities": []}, {"text": "But observe that for their particular multimodal architecture, the level of concreteness of a concept being represented -intuitively, the idea of a dog is more concrete than that of beauty -affects whether multimodal or single-channel representations are more effective.", "labels": [], "entities": []}, {"text": "In their case, concreteness was derived for 766 nouns and verbs from a fixed psycholinguistic database of human ratings.", "labels": [], "entities": []}, {"text": "In contrast, we introduce an adaptive algorithm for characterizing the visual concreteness of all the concepts indexed textually (e.g., \"dog\") in a given multimodal dataset.", "labels": [], "entities": []}, {"text": "Our approach is to leverage the geometry of image/text space.", "labels": [], "entities": []}, {"text": "Intuitively, a visually concrete concept is one associated with more locally similar sets of images; for example, images associated with \"dog\" will likely contain dogs, whereas images associated with \"beautiful\" may contain flowers, sunsets, weddings, or an abundance of other possibilities -see.", "labels": [], "entities": []}, {"text": "Allowing concreteness to be dataset-specific is an important innovation because concreteness is contextual.", "labels": [], "entities": []}, {"text": "For example, in one dataset we work with, our method scores \"London\" as highly concrete because of a preponderance of iconic London images in it, such as Big Ben and double-decker buses; whereas fora separate dataset, \"London\" is used as a geotag for diverse images, so the same word scores as highly non-concrete.", "labels": [], "entities": []}, {"text": "In addition to being dataset-specific, our method readily scales, does not depend on an external search engine, and is compatible with both discrete and continuous textual concepts (e.g., topic distributions).", "labels": [], "entities": []}, {"text": "Dataset-specific visual concreteness scores enable a variety of purposes.", "labels": [], "entities": []}, {"text": "In this paper, we: Demonstration of visual concreteness estimation on an example from the COCO dataset.", "labels": [], "entities": [{"text": "visual concreteness estimation", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.6234558324019114}, {"text": "COCO dataset", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.9705789387226105}]}, {"text": "The degree of visual clustering of textual concepts is measured using a nearest neighbor technique.", "labels": [], "entities": []}, {"text": "The concreteness of \"dogs\" is greater than the concreteness of \"beautiful\" because images associated with \"dogs\" are packed tightly into two clusters, while images associated with \"beautiful\" are spread evenly.", "labels": [], "entities": []}, {"text": "focus on using them to: 1) explore multimodal datasets; and 2) predict how easily concepts will be learned in a machine learning setting.", "labels": [], "entities": []}, {"text": "We apply our method to four large multimodal datasets, ranging from image captions to image/text data scraped from Wikipedia, 2 to examine the relationship between concreteness scores and the performance of machine learning algorithms.", "labels": [], "entities": []}, {"text": "Specifically, we consider the cross-modal retrieval problem, and examine a number of NLP, vision, and retrieval algorithms.", "labels": [], "entities": []}, {"text": "Across all 320 significantly different experimental settings (= 4 datasets \u00d7 2 image-representation algorithms \u00d7 5 textualrepresentation algorithms \u00d7 4 text/image alignment algorithms \u00d7 2 feature pre-processing schemes), we find that more concrete instances are easier to retrieve, and that different algorithms have similar failure cases.", "labels": [], "entities": [{"text": "text/image alignment", "start_pos": 152, "end_pos": 172, "type": "TASK", "confidence": 0.6980129182338715}]}, {"text": "Interestingly, the relationship between concreteness and retrievability varies significantly based on dataset: some datasets appear to have a linear relationship between the two, whereas others exhibit a concreteness threshold beyond which retrieval becomes much easier.", "labels": [], "entities": []}, {"text": "We believe that our work can have a positive impact on future multimodal research.", "labels": [], "entities": []}, {"text": "\u00a78 gives more detail, but in brief, we see implications in (1) evaluation -more credit should perhaps be assigned to performance on non-concrete concepts; (2) creating or augmenting multimodal datasets, where one might a priori consider the desired relative proportion of concrete vs. non-concrete concepts; and (3) curriculum learning (), Image copyright information is provided in the supplementary material.", "labels": [], "entities": []}, {"text": "We release our Wikipedia and British Library data at http://www.cs.cornell.edu/ \u02dc jhessel/ concreteness/concreteness.html where ordering of training examples could take concreteness levels into account.", "labels": [], "entities": [{"text": "British Library data", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.9788758754730225}]}], "datasetContent": [{"text": "We consider four datasets that span a variety of multimodal settings.", "labels": [], "entities": []}, {"text": "Two are publicly available and widely used (COCO/Flickr); we collected and preprocessed the other two (Wiki/BL).", "labels": [], "entities": [{"text": "COCO/Flickr", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.7642578283945719}, {"text": "Wiki/BL)", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.7360310256481171}]}, {"text": "The Wikipedia and British Library sets are available for download at http: //www.cs.cornell.edu/ \u02dc jhessel/ concreteness/concreteness.html.", "labels": [], "entities": [{"text": "British Library sets", "start_pos": 18, "end_pos": 38, "type": "DATASET", "confidence": 0.980689803759257}]}, {"text": "Dataset statistics are given in, and summarized as follows: Wikipedia (Wiki).", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.9839852452278137}]}, {"text": "We collected a dataset consisting of 192K articles from the English Wikipedia, along with the 549K images contained in those we selected this subset of Wikipedia by identifying articles that received at least 50 views on March 5th, 2016.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.8811215460300446}]}, {"text": "5 To our knowledge, the previous largest publicly available multimodal Wikipedia dataset comes from ImageCLEF's 2011 retrieval task (, which consists of 137K images associated with English articles.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.8773396015167236}]}, {"text": "Images often appear on multiple pages: an image of the Eiffel tower might appear on pages for Paris, for Gustave Eiffel, and for the tower itself.", "labels": [], "entities": []}, {"text": "Historical Books from British Library (BL).", "labels": [], "entities": [{"text": "Historical Books from British Library (BL)", "start_pos": 0, "end_pos": 42, "type": "DATASET", "confidence": 0.9495812430977821}]}, {"text": "The British Library has released a set of digitized books (British Library Labs, 2016) consisting of 25M pages of OCRed text, alongside 500K+ images scraped from those pages of text.", "labels": [], "entities": [{"text": "British Library", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9877396821975708}, {"text": "British Library Labs, 2016)", "start_pos": 59, "end_pos": 86, "type": "DATASET", "confidence": 0.9745674530665079}]}, {"text": "The release splits images into four categories; we ignore \"bound covers\" and \"embellishments\" and use images identified as \"plates\" and \"medium sized.\"", "labels": [], "entities": []}, {"text": "We associated images with all text within a 3-page window.", "labels": [], "entities": []}, {"text": "This raw data collection is noisy.", "labels": [], "entities": []}, {"text": "Many books are not in English, some books contain far more images than others, and the images themselves are of varying size and rotation.", "labels": [], "entities": []}, {"text": "To combat these issues we only keep books that have identifiably English text; for each cross-validation split in our machinelearning experiments ( \u00a76) we sample at most 10 images from each book; and we use book-level holdout so that no images/text in the test set are from books in the training set.", "labels": [], "entities": []}, {"text": "We also examine two popular existing datasets: Microsoft COCO (captions) () (COCO) and MIRFLICKR-1M (tags) () (Flickr).", "labels": [], "entities": [{"text": "Microsoft COCO", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.8565934002399445}, {"text": "MIRFLICKR-1M", "start_pos": 87, "end_pos": 99, "type": "METRIC", "confidence": 0.7710195183753967}, {"text": "Flickr", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.8723691701889038}]}, {"text": "For COCO, we construct our own training/validation splits from the 123K images, each of which has 5 captions.", "labels": [], "entities": [{"text": "COCO", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7947275042533875}]}, {"text": "For Flickr, as an initial preprocessing step we only consider the 7.3K tags that appear at least 200 times, and the 754K images that are associated with at least 3 of the 7.3K valid tags.", "labels": [], "entities": [{"text": "Flickr", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.9029405117034912}]}, {"text": "To what extent are the concreteness scores datasetspecific?", "labels": [], "entities": []}, {"text": "To investigate this question, we compute the correlation between Flickr and COCO unigram concreteness scores for 1129 overlapping terms.", "labels": [], "entities": [{"text": "Flickr", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.7152012586593628}]}, {"text": "While the two are positively correlated (\u03c1 = .48, p < .01) there are many exceptions that highlight the utility of computing datasetindependent scores.", "labels": [], "entities": []}, {"text": "For instance, \"London\" is extremely concrete in COCO (rank 9) as compared to in Flickr (rank 1110).", "labels": [], "entities": [{"text": "London", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.9487321376800537}, {"text": "COCO", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.86689692735672}, {"text": "Flickr", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.9599842429161072}]}, {"text": "In COCO, images of London tend to be iconic (i.e., Big Ben, double decker buses); in contrast, \"London\" often serves as a geotag fora wider variety of images in Flickr.", "labels": [], "entities": [{"text": "COCO", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.8837775588035583}, {"text": "Flickr", "start_pos": 161, "end_pos": 167, "type": "DATASET", "confidence": 0.9727218151092529}]}, {"text": "Conversely, \"watch\" in Flickr is concrete as it tends to refer to the timepiece, whereas \"watch\" is not concrete in COCO (rank 958) as it tends to refer to the verb; while these relationships are not obvious a priori, our concreteness method has helped to highlight these usage differences between the image tagging and captioning datasets.", "labels": [], "entities": [{"text": "Flickr", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.9581784605979919}, {"text": "COCO", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.8806247711181641}]}], "tableCaptions": [{"text": " Table 1: Dataset statistics: total number of im- ages, average text length in words, and size of the  train/test splits we use in  \u00a76.", "labels": [], "entities": []}]}