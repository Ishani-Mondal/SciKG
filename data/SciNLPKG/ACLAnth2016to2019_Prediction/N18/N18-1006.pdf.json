{"title": [{"text": "Improving Character-based Decoding Using Target-Side Morphological Information for Neural Machine Translation", "labels": [], "entities": [{"text": "Improving Character-based Decoding", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8987816373507181}, {"text": "Neural Machine Translation", "start_pos": 83, "end_pos": 109, "type": "TASK", "confidence": 0.7295920054117838}]}], "abstractContent": [{"text": "Recently, neural machine translation (NMT) has emerged as a powerful alternative to conventional statistical approaches.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8121861318747202}]}, {"text": "However, its performance drops considerably in the presence of morphologically rich languages (MRLs).", "labels": [], "entities": []}, {"text": "Neural engines usually fail to tackle the large vocabulary and high out-of-vocabulary (OOV) word rate of MRLs.", "labels": [], "entities": [{"text": "out-of-vocabulary (OOV) word rate", "start_pos": 68, "end_pos": 101, "type": "METRIC", "confidence": 0.8360790312290192}]}, {"text": "Therefore, it is not suitable to exploit existing word-based models to translate this set of languages.", "labels": [], "entities": []}, {"text": "In this paper, we propose an extension to the state-of-the-art model of Chung et al.", "labels": [], "entities": []}, {"text": "(2016), which works at the character level and boosts the decoder with target-side morphological information.", "labels": [], "entities": []}, {"text": "In our architecture , an additional morphology table is plugged into the model.", "labels": [], "entities": []}, {"text": "Each time the decoder samples from a target vocabulary, the table sends auxiliary signals from the most relevant affixes in order to enrich the decoder's current state and constrain it to provide better predictions.", "labels": [], "entities": []}, {"text": "We evaluated our model to translate English into Ger-man, Russian, and Turkish as three MRLs and observed significant improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphologically complex words (MCWs) are multi-layer structures which consist of different subunits, each of which carries semantic information and has a specific syntactic role.", "labels": [], "entities": [{"text": "Morphologically complex words (MCWs)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6039625555276871}]}, {"text": "gives a Turkish example to show this type of complexity.", "labels": [], "entities": []}, {"text": "This example is a clear indication that word-based models are not suitable to process such complex languages.", "labels": [], "entities": []}, {"text": "Accordingly, when translating MRLs, it might not be a good idea to treat words as atomic units as it demands a large vocabulary that imposes extra overhead.", "labels": [], "entities": [{"text": "translating MRLs", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.8689671456813812}]}, {"text": "Since MCWs can appear in various forms we require a very large vocabulary to i) cover as many morphological forms and words as we can, and ii) reduce the number of OOVs.", "labels": [], "entities": []}, {"text": "Neural models by their nature are complex, and we do not want to make them more complicated by working with large vocabularies.", "labels": [], "entities": []}, {"text": "Furthermore, even if we have quite a large vocabulary set, clearly some words would remain uncovered by that.", "labels": [], "entities": []}, {"text": "This means that a large vocabulary not only complicates the entire process, but also does not necessarily mitigate the OOV problem.", "labels": [], "entities": [{"text": "OOV", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.7457396984100342}]}, {"text": "For these reasons we propose an NMT engine which works at the character level.", "labels": [], "entities": []}, {"text": "In this paper, we focus on translating into MRLs and issues associated with word formation on the target side.", "labels": [], "entities": [{"text": "translating into MRLs", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.8166429400444031}, {"text": "word formation", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7967400550842285}]}, {"text": "To provide a better translation we do not necessarily need a large target lexicon, as an MCW can be gradually formed during decoding by means of its subunits, similar to the solution proposed in character-based decoding models (.", "labels": [], "entities": []}, {"text": "Generating a complex word character-by-character is a better approach compared to word-level sampling, but it has other disadvantages.", "labels": [], "entities": []}, {"text": "One character can co-occur with another with almost no constraint, but a particular word or morpheme can only collocate with a very limited number of other constituents.", "labels": [], "entities": []}, {"text": "Unlike words, characters are not meaning-bearing units and do not preserve syntactic information, so (in the extreme case) the chance of sampling each character by the decoder is almost equal to the others, but this situation is less likely for words.", "labels": [], "entities": []}, {"text": "The only constraint that prioritize which character should be sampled is information stored in the decoder, which we believe is insufficient to cope with all ambiguities.", "labels": [], "entities": []}, {"text": "Furthermore, when everything is segmented into characters the target sentence with a limited number of words is changed to a very long sequence of characters, which clearly makes it harder for the decoder to remember such along history.", "labels": [], "entities": []}, {"text": "Accordingly, character-based information flows in the decoder may not be as informative as wordor morpheme-based information.", "labels": [], "entities": []}, {"text": "In the character-based NMT model everything is almost the same as its word-based counterpart except the target vocabulary whose size is considerably reduced from thousands of words to just hundreds of characters.", "labels": [], "entities": []}, {"text": "If we consider the decoder as a classifier, it should in principle be able to perform much better over hundreds of classes (characters) rather than thousands (words), but the performance of character-based models is almost the same as or slightly better than their wordbased versions.", "labels": [], "entities": []}, {"text": "This underlines the fact that the character-based decoder is perhaps not fed with sufficient information to provide improved performance compared to word-based models.", "labels": [], "entities": []}, {"text": "Character-level decoding limits the search space by dramatically reducing the size of the target vocabulary, but at the same time widens the search space by working with characters whose sampling seems to be harder than words.", "labels": [], "entities": []}, {"text": "The freedom in selection and sampling of characters can mislead the decoder, which prevents us from taking the maximum advantages of character-level decoding.", "labels": [], "entities": []}, {"text": "If we can control the selection process with other constraints, we may obtain further benefit from restricting the vocabulary set, which is the main goal followed in this paper.", "labels": [], "entities": []}, {"text": "In order to address the aforementioned problems we redesign the neural decoder in three different scenarios.", "labels": [], "entities": []}, {"text": "In the first scenario we equip the decoder with an additional morphology table including target-side affixes.", "labels": [], "entities": []}, {"text": "We place an attention module on top of the table which is controlled by the decoder.", "labels": [], "entities": []}, {"text": "At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state.", "labels": [], "entities": []}, {"text": "Signals sent from the table can be interpreted as additional constraints.", "labels": [], "entities": []}, {"text": "In the second scenario we share the decoder between two output channels.", "labels": [], "entities": []}, {"text": "The first one samples the target character and the other one predicts the morphological annotation of the character.", "labels": [], "entities": []}, {"text": "This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions.", "labels": [], "entities": []}, {"text": "In the third scenario we combine these two models.", "labels": [], "entities": []}, {"text": "Section 3 provides more details on our models.", "labels": [], "entities": []}, {"text": "Together with different findings that will be discussed in the next sections, there are two main contributions in this paper.", "labels": [], "entities": []}, {"text": "We redesigned and tuned the NMT framework for translating into MRLs.", "labels": [], "entities": [{"text": "translating into MRLs", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.6771971384684244}]}, {"text": "It is quite challenging to show the impact of external knowledge such as morphological information in neural models especially in the presence of large parallel corpora.", "labels": [], "entities": []}, {"text": "However, our models are able to incorporate morphological information into decoding and boost its quality.", "labels": [], "entities": []}, {"text": "We inject the decoder with morphological properties of the target language.", "labels": [], "entities": []}, {"text": "Furthermore, the novel architecture proposed here is not limited to morphological information alone and is flexible enough to provide other types of information for the decoder.", "labels": [], "entities": []}], "datasetContent": [{"text": "As previously reviewed, different models try to capture complexities on the encoder side, but to the best of our knowledge the only model which proposes a technique to deal with complex constituents on the decoder side is that of, which should bean appropriate baseline for our comparisons.", "labels": [], "entities": []}, {"text": "Moreover, it outperforms other existing NMT models, so we prefer to compare our network to the best existing model.", "labels": [], "entities": []}, {"text": "This model is referred to as CDNMT in our experiments.", "labels": [], "entities": []}, {"text": "In the next sections first we explain our experimental setting, corpora, and how we build the morphology table (Section 4.1), and then report experimental results (Section 4.2).", "labels": [], "entities": []}, {"text": "In order to make our work comparable we try to follow the same experimental setting used in CDNMT, where the GRU size is 1024, the affix and word embedding size is 512, and the beam width is 20.", "labels": [], "entities": [{"text": "GRU size", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.8642849028110504}]}, {"text": "Our models are trained using stochastic gradient descent with Adam (. and  demonstrated that bpe boosts NMT, so similar to CDNMT we also preprocess the source side of our corpora using bpe.", "labels": [], "entities": []}, {"text": "We use WMT-15 corpora 1 to train the models, newstest-2013 for tuning and newstest-2015 as the test sets.", "labels": [], "entities": [{"text": "WMT-15 corpora 1", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.8523962696393331}]}, {"text": "For English-Turkish (En-Tr) we use the OpenSubtitle2016 collection (.", "labels": [], "entities": [{"text": "OpenSubtitle2016 collection", "start_pos": 39, "end_pos": 66, "type": "DATASET", "confidence": 0.9418604671955109}]}, {"text": "The training side of the English-German (En-De), English-Russian (EnRu), and En-Tr corpora include 4.5, 2.1, and 4 million parallel sentences, respectively.", "labels": [], "entities": []}, {"text": "We randomly select 3K sentences for each of the development and test sets for En-Tr.", "labels": [], "entities": []}, {"text": "For all language pairs we keep the 400 most frequent characters as the target-side character set and replace the remainder (infrequent characters) with a specific character.", "labels": [], "entities": []}, {"text": "One of the key modules in our architecture is the morphology table.", "labels": [], "entities": []}, {"text": "In order to implement it we use a look-up table whose columns include embeddings for the target language's affixes (each column represents one affix) which are updated during training.", "labels": [], "entities": []}, {"text": "As previously mentioned, the table is intended to provide useful, morphological information so it should be initialized properly, for which we use a morphology-aware embeddinglearning model.", "labels": [], "entities": []}, {"text": "To this end, we use the neural language model of in which each word is represented via a linear combination of the embeddings of its surface form and subunits, e.g. Given a sequence of words, the neural language model tries to predict the next word, so it learns sentence-level dependencies as well as intra-word relations.", "labels": [], "entities": []}, {"text": "The model trains surface form and subword-level embeddings which provides us with high-quality affix embeddings.", "labels": [], "entities": []}, {"text": "Our neural language model is a recurrent network with a single 1000-dimensional GRU layer, which is trained on the target sides of our parallel corpora.", "labels": [], "entities": []}, {"text": "The embedding size is 512 and we use a batch size of 100 to train the model.", "labels": [], "entities": []}, {"text": "Before training the neural language model, we need 1 http://www.statmt.org/wmt15/ to manipulate the training corpus to decompose words into morphemes for which we use Morfessor (), an unsupervised morphological analyzer.", "labels": [], "entities": []}, {"text": "Using Morfessor each word is segmented into different subunits where we consider the longest part as the stem of each word; what appears before the stem is taken as a member of the set of prefixes (there might be one or more prefixes) and what follows the stem is considered as a member of the set of suffixes.", "labels": [], "entities": []}, {"text": "Since Morfessor is an unsupervised analyzer, in order to minimize segmentation errors and avoid noisy results we filter its output and exclude subunits which occur fewer than 500 times.", "labels": [], "entities": []}, {"text": "After decomposing, filtering, and separating stems from affixes, we extracted several affixes which are reported in  Using the neural language model we train word, stem, and affix embeddings, and initialize the look-up table (but not other parts) of the decoder using those affixes.", "labels": [], "entities": []}, {"text": "The look-up table includes high-quality affixes trained on the target side of the parallel corpus by which we train the translation model.", "labels": [], "entities": []}, {"text": "Clearly, such an affix table is an additional knowledge source for the decoder.", "labels": [], "entities": []}, {"text": "It preserves information which is very close to what the decoder actually needs.", "labels": [], "entities": []}, {"text": "However, there might be some missing pieces of information or some incompatibility between the decoder and the table, so we do not freeze the morphology table during training, but let the decoder update it with respect to its needs in the forward and backward passes.", "labels": [], "entities": []}, {"text": "We report results for the bpe\u2192char setting, which means the source token is a bpe unit and the decoder samples a character at each time step.", "labels": [], "entities": []}, {"text": "CD-NMT is the baseline model.", "labels": [], "entities": [{"text": "CD-NMT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8711788058280945}]}, {"text": "includes scores reported from the original CDNMT model () as well as the scores from our reimplementation.", "labels": [], "entities": []}, {"text": "To make our work comparable and show the impact of the new architecture, we tried to replicate CDNMT's results in our experimental setting, we kept everything (parameters, iterations, epochs etc.) unchanged and evaluated the extended model in the same setting.", "labels": [], "entities": []}, {"text": "reports BLEU scores () of our NMT models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9991361498832703}]}], "tableCaptions": [{"text": " Table 2: The number of affixes extracted for each lan- guage.", "labels": [], "entities": []}, {"text": " Table 3: CDNMT  *  is our implementation of CDNMT.  m and o indicates that the base model is extended with  the morphology table and the additional output chan- nel, respectively. mo is the combination of both the ex- tensions. The improvement provided by the boldfaced  number compared to CDNMT  *  is statistically signifi- cant according to paired bootstrap re-sampling", "labels": [], "entities": []}]}