{"title": [{"text": "Guiding Generation for Abstractive Text Summarization based on Key Information Guide Network", "labels": [], "entities": [{"text": "Abstractive Text Summarization", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.5667770703633627}, {"text": "Key Information Guide", "start_pos": 63, "end_pos": 84, "type": "DATASET", "confidence": 0.8217582503954569}]}], "abstractContent": [{"text": "Neural network models, based on the at-tentional encoder-decoder model, have good capability in abstractive text summarization.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.6048151353995005}]}, {"text": "However, these models are hard to be controlled in the process of generation, which leads to alack of key information.", "labels": [], "entities": []}, {"text": "We propose a guiding generation model that combines the extractive method and the abstractive method.", "labels": [], "entities": []}, {"text": "Firstly, we obtain keywords from the text by a extractive model.", "labels": [], "entities": []}, {"text": "Then, we introduce a Key Information Guide Network (KIGN), which encodes the keywords to the key information representation, to guide the process of generation.", "labels": [], "entities": []}, {"text": "In addition, we use a prediction-guide mechanism, which can obtain the long-term value for future decoding, to further guide the summary generation.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.6919540166854858}]}, {"text": "We evaluate our model on the CNN/Daily Mail dataset.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.9421521306037903}]}, {"text": "The experimental results show that our model leads to significant improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text summarization aims to generate a brief summary from an input document while retaining the key information.", "labels": [], "entities": [{"text": "Text summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7581303417682648}]}, {"text": "There are two broad approaches to summarization: extractive and abstractive.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.989018976688385}]}, {"text": "Extractive models ( usually extract a few sentences or keywords from the source text, while abstractive models generate new words and phrases that not in the source text to construct the summary.", "labels": [], "entities": []}, {"text": "Recently, inspired by the success of encoderdecoder model), abstractive summarization models () are able to generate the summaries with high ROUGE scores.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.9658933281898499}]}, {"text": "While these models proved to be capable of capturing the regularities of the text summarization, they are hard to be controlled in the process of generation.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.6113088428974152}]}, {"text": "Without external guidance, these models just get the source * Corresponding Author: Weiran Xu text as input and then output the summary, which certainly leads to alack of key information.", "labels": [], "entities": []}, {"text": "propose a selective gate network to retain more key information in the summary.", "labels": [], "entities": []}, {"text": "However, the selective gate network, which is controlled by the representation of the input text, controls the information flow from encoder to decoder for just once.", "labels": [], "entities": []}, {"text": "If some key information does not pass the network, it is hard for them to appear in the summary.", "labels": [], "entities": []}, {"text": "propose a pointer-generator model, which uses the pointer mechanism () to copy words from the input text, to deal with the out-ofvocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "Without external guidance, it is hard for the pointer to identify keywords.", "labels": [], "entities": []}, {"text": "To address these problems, we combine the extractive model and the abstractive model and use the former one to obtain keywords as guidance for the latter one.", "labels": [], "entities": []}, {"text": "In this paper we propose a guiding generation model for abstractive text summarization.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.6331130464871725}]}, {"text": "Firstly, we use a extractive method to obtain the keywords from the text.", "labels": [], "entities": []}, {"text": "Then, we introduce a Key Information Guide Network (KIGN), which encodes the keywords to the key information representation and integrates it into the abstractive model, to guide the process of generation.", "labels": [], "entities": []}, {"text": "The guidance is mainly in two aspects: the attention mechanism () and the pointer mechanism.", "labels": [], "entities": []}, {"text": "In addition, we propose a novel predictionguide mechanism based on, which predicts the extent of key information covered in the final summary, to further guide the summary generation.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.7491973042488098}]}, {"text": "Experiments show that our model achieves significant improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the CNN/Daily Mail dataset() and use scripts supplied by to obtain the same version of the data, which has 28,7226 training pairs, 13,368 validation pairs and 11,490 test pairs.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 11, "end_pos": 33, "type": "DATASET", "confidence": 0.9400917649269104}]}, {"text": "We use two 256-dimensional LSTMs for the bidirectional encoder and one 256-dimensional LSTM for the decoder.", "labels": [], "entities": []}, {"text": "In our key information guide network, the approach of encoding keywords is same to the encoder.", "labels": [], "entities": []}, {"text": "In addition, we use a vocabulary of 50k words for both source and target and do not pre-train the word embeddings -they are learned from scratch during training.", "labels": [], "entities": []}, {"text": "During training and testing, we truncate the text to 400 tokens and limit the length of the summary to 100 tokens.", "labels": [], "entities": []}, {"text": "We train using Adagrad (Duchi et al., 2011) with learning rate 0.15 and an initial accumulator value of 0.1.", "labels": [], "entities": [{"text": "Adagrad (Duchi et al., 2011)", "start_pos": 15, "end_pos": 43, "type": "DATASET", "confidence": 0.8394699394702911}, {"text": "learning rate 0.15", "start_pos": 49, "end_pos": 67, "type": "METRIC", "confidence": 0.9624922076861063}]}, {"text": "The batch size is set as 16.", "labels": [], "entities": []}, {"text": "Following the previous work, our evaluation metric is F-score of ROUGE (.", "labels": [], "entities": [{"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.999322772026062}, {"text": "ROUGE", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9970894455909729}]}, {"text": "In addition, for the prediction-guide mechanism, we set the single-layer feed forward network with 800 nodes.", "labels": [], "entities": []}, {"text": "For the hyperparameter \u03b1, we test the performances of KIGN+Prediction-guide model using different \u03b1 during decoding.", "labels": [], "entities": []}, {"text": "As can be seen from the figure 2, the performance is stable for the \u03b1 ranging from 0.8 to 0.95.", "labels": [], "entities": []}, {"text": "When \u03b1 is set as 0.9, we can obtain the highest F-score of ROUGE.", "labels": [], "entities": [{"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9992269277572632}, {"text": "ROUGE", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9945477843284607}]}, {"text": "Besides, we set the M as 8 and adapt mini-batch training with batch size to be 16.", "labels": [], "entities": [{"text": "M", "start_pos": 20, "end_pos": 21, "type": "METRIC", "confidence": 0.9912064671516418}]}, {"text": "The network is trained with AdaDelta.", "labels": [], "entities": [{"text": "AdaDelta", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.9531621932983398}]}, {"text": "During training and attest time we truncate the input tokens to 400 and limit the length of the output summary to 100 tokens for training and 120 tokens attest time, which is similar to.", "labels": [], "entities": []}, {"text": "We trained our keywords network model less than 200, 000 training iterations.", "labels": [], "entities": []}, {"text": "Then we trained the single-layer feed forward network based on the KIGN model.", "labels": [], "entities": []}, {"text": "Finally, attest time, we combine the KIGN model and the predictionguide mechanism to generate the summary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most \u00b10.25 as reported by the official ROUGE script.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.763706624507904}, {"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.5662680268287659}, {"text": "CNN/Daily Mail test set", "start_pos": 44, "end_pos": 67, "type": "DATASET", "confidence": 0.9341573913892111}]}]}