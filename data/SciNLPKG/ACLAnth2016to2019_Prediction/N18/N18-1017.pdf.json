{"title": [{"text": "Natural Answer Generation with Heterogeneous Memory", "labels": [], "entities": [{"text": "Natural Answer Generation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.748688538869222}]}], "abstractContent": [{"text": "Memory augmented encoder-decoder framework has achieved promising progress for natural language generation tasks.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.6400889158248901}]}, {"text": "Such frameworks enable a decoder to retrieve from a memory during generation.", "labels": [], "entities": []}, {"text": "However, less research has been done to take care of the memory contents from different sources, which are often of heterogeneous formats.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel attention mechanism to encourage the decoder to actively interact with the memory by taking its heterogeneity into account.", "labels": [], "entities": []}, {"text": "Our solution attends across the generated history and memory to explicitly avoid repetition, and introduce related knowledge to enrich our generated sentences.", "labels": [], "entities": []}, {"text": "Experiments on the answer sentence generation task show that our method can effectively explore heterogeneous memory to produce readable and meaningful answer sentences while maintaining high coverage forgiven answer information .", "labels": [], "entities": [{"text": "answer sentence generation task", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.8098380267620087}]}], "introductionContent": [{"text": "Most previous question answering systems focus on finding candidate words, phrases or sentence snippets from many resources, and ranking them for their users).", "labels": [], "entities": [{"text": "question answering", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7907421588897705}]}, {"text": "Typically, candidate answers are collected from different resources, such as knowledge base (KB) or textual documents, which are often with heterogeneous formats, e.g., KB triples or semi-structured results from Information Extraction (IE).", "labels": [], "entities": []}, {"text": "For factoid questions, a single answer word or phrase is chosen as the response for users, as shown in However, in many real-world scenarios, users may prefer more natural responses rather than a single word.", "labels": [], "entities": []}, {"text": "For example, as A2 in, James Cameron directed the Titanic. is more favorable than the single name James Cameron.", "labels": [], "entities": [{"text": "A2", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9690313935279846}]}, {"text": "A straightforward solution to compose an answer sentence is to build a template based model, where the answer Q Who is the director of the Titanic?", "labels": [], "entities": []}, {"text": "A1 James Cameron A2 James Cameron directed the Titanic.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are designed to answer the following questions: (1) whether our model can properly utilize heterogeneous memories to generate readable answer sentences, (2) whether our model can coverall target answers during generation, (3) whether our model can introduce related knowledge in the output while avoiding repetition.", "labels": [], "entities": []}, {"text": "Our task requires a question, and a memory storing all the answer words and related knowledge as input, and produces a natural, readable sentence as the output.", "labels": [], "entities": []}, {"text": "Unfortunately, there is no existing dataset that naturally fits to our task.", "labels": [], "entities": []}, {"text": "We thus tailor the WikiMovies 1 dataset according to our requirements.", "labels": [], "entities": [{"text": "WikiMovies 1 dataset", "start_pos": 19, "end_pos": 39, "type": "DATASET", "confidence": 0.9048500061035156}]}, {"text": "This WikiMovies dataset was originally constructed for answering simple factoid questions, using memory networks with different knowledge representations, i.e., structured KB (KB entries in), raw textual documents (Doc), or processed documents obtained through information extraction (IE), respectively.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 261, "end_pos": 288, "type": "TASK", "confidence": 0.8267805337905884}]}, {"text": "The first is in the classic subject-predicate-object format.", "labels": [], "entities": []}, {"text": "The second contains sentences from Wikipedia and also sentences automatically generated from predefined templates.", "labels": [], "entities": []}, {"text": "The third is in the subjectverb-object format, collected by applying off-theshell information extractor to all sentences.", "labels": [], "entities": []}, {"text": "As shown in, we treat each question in WikiMovies with its original answer (usually one or more words) as a QA pair, and one of the question's supportive sentences (either from Wikipedia or templates) as its goldstandard answer sentence.", "labels": [], "entities": []}, {"text": "For each question, the memory will contain all knowledge triples about the question's topic movie from the KB entries, and also include entities and keywords extracted from its IE portion.", "labels": [], "entities": []}, {"text": "For each entry in KB entries, we use the predicate as the key and the object as value to construct anew entry in our memory.", "labels": [], "entities": []}, {"text": "For those from IE, we keep the extracted tags as the key and entities or other expressions as the value.", "labels": [], "entities": []}, {"text": "Given a question, if an entity/expression in the memory is not the answer, it will be treated as information enrichment.", "labels": [], "entities": []}, {"text": "According to whether the supportive sentences are generated by predefined templates or not, we split the dataset into WikiMovies-Synthetic and WikiMovies-Wikipedia.", "labels": [], "entities": []}, {"text": "The resulting WikiMovies-Synthetic includes 115 question patterns and 194 answer patterns, covering 10 topics, e.g., director, genre, actor, release year, etc.", "labels": [], "entities": []}, {"text": "We follow its original data split, i.e., 47,226 QA-pairs for training, 8,895 for validation and 8,910 for testing.", "labels": [], "entities": []}, {"text": "In WikiMovies-Wikipedia, answer sentences are extracted from Wikipedia, admittedly noisy in nature.", "labels": [], "entities": []}, {"text": "Note that there are more than 10K Wikipedia sentences that cannot be paired with any questions.", "labels": [], "entities": []}, {"text": "We thus left their questions as blank and treat it as a pure generation task from a given memory, which can be viewed as a form of data augmentation to improve sentence variety.", "labels": [], "entities": []}, {"text": "We split WikiMovies-Wikipedia the dataset randomly into 47,309 cases for training, 4,093 for testing and 3,954 for validation.", "labels": [], "entities": [{"text": "WikiMovies-Wikipedia the dataset", "start_pos": 9, "end_pos": 41, "type": "DATASET", "confidence": 0.8501401543617249}]}, {"text": "We treat normal words occurring less than 10 times as UNK, and, eventually, have 24,850 normal words and 37,898 entity words.", "labels": [], "entities": [{"text": "UNK", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.7955864667892456}]}, {"text": "We cut the maximum length of answer sentences to 20, and the maximum memory size to 10, which covers most cases in both synthetic and Wikipedia datasets.", "labels": [], "entities": [{"text": "Wikipedia datasets", "start_pos": 134, "end_pos": 152, "type": "DATASET", "confidence": 0.920950323343277}]}], "tableCaptions": [{"text": " Table 3: Results on the WikiMovies-Synthetic dataset", "labels": [], "entities": [{"text": "WikiMovies-Synthetic dataset", "start_pos": 25, "end_pos": 53, "type": "DATASET", "confidence": 0.8791625201702118}]}, {"text": " Table 4: Results on the WikiMovies-Wikipedia dataset", "labels": [], "entities": [{"text": "WikiMovies-Wikipedia dataset", "start_pos": 25, "end_pos": 53, "type": "DATASET", "confidence": 0.8640742301940918}]}, {"text": " Table 6: Example answers generated by our model. In an answer sentence, an underlined phrase is the value of  a memory slot selected from the memory by our model, and the subscript number is the index of this slot in the  memory.", "labels": [], "entities": []}]}