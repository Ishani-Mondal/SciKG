{"title": [{"text": "Reference-less Measure of Faithfulness for Grammatical Error Correction", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose USIM, a semantic measure for Grammatical Error Correction (GEC) that measures the semantic faithfulness of the output to the source, thereby complementing existing reference-less measures (RLMs) for measuring the output's grammaticality.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC)", "start_pos": 40, "end_pos": 74, "type": "METRIC", "confidence": 0.5458302199840546}]}, {"text": "USIM operates by comparing the semantic symbolic structure of the source and the correction , without relying on manually-curated references.", "labels": [], "entities": [{"text": "USIM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8471938967704773}]}, {"text": "Our experiments establish the validity of USIM, by showing that (1) semantic annotation can be consistently applied to ungram-matical text; (2) valid corrections obtain a high USIM similarity score to the source; and (3) invalid corrections obtain a lower score.", "labels": [], "entities": [{"text": "validity", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9843907952308655}, {"text": "USIM", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.6084540486335754}, {"text": "USIM similarity score", "start_pos": 176, "end_pos": 197, "type": "METRIC", "confidence": 0.6550810237725576}]}], "introductionContent": [{"text": "Evaluation in Monolingual Translation, and particularly in Grammatical Error Correction (GEC) is a challenging research field, much due to the difficulty in integrating different types of rewriting operations into a single measure, and the vast number of valid outputs.", "labels": [], "entities": [{"text": "Evaluation in Monolingual Translation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7016671374440193}, {"text": "Grammatical Error Correction (GEC)", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.7671691874663035}]}, {"text": "These difficulties have recently motivated a number of proposals for new, improved reference-based measures (RBMs).", "labels": [], "entities": []}, {"text": "Nevertheless, the size and heterogeneity of the space of valid outputs per sentence often prohibits obtaining a reference set that covers this space well, thereby limiting the applicability of RBMs (.", "labels": [], "entities": []}, {"text": "To address this we propose a semantic RLM, USIM, that operates by measuring the graph distance between the semantic representations of the source and the output.", "labels": [], "entities": [{"text": "USIM", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.6226814985275269}]}, {"text": "Reliable RLMs are appealing both in not relying on references, which are costly to collect, and in avoiding the biases incurred by selecting references that necessarily cannot exhaust the vast space of valid corrections.", "labels": [], "entities": []}, {"text": "Our proposal complements the RLM proposed by, which uses grammatical error detection techniques to assess the grammaticality of the output, and the work of, who advocate the use of RLMs for fluency, grammaticality and meaning preservation, but state that a meaning preservation measure for GEC is currently lacking.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 218, "end_pos": 238, "type": "TASK", "confidence": 0.7460234761238098}, {"text": "meaning preservation", "start_pos": 257, "end_pos": 277, "type": "TASK", "confidence": 0.742606520652771}]}, {"text": "A similar decomposition of output quality to its adequacy (similar to faithfulness) and fluency (related to grammaticality), has been used in machine translation (MT) evaluation (e.g.,.", "labels": [], "entities": [{"text": "machine translation (MT) evaluation", "start_pos": 142, "end_pos": 177, "type": "TASK", "confidence": 0.8525769859552383}]}, {"text": "As a test case, we use the UCCA semantic scheme (, motivated by its recent use in semantic evaluation of MT ( and text simplification) systems.", "labels": [], "entities": [{"text": "UCCA semantic scheme", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.8610838055610657}, {"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9287461638450623}]}, {"text": "Nevertheless, USIM can be easily adapted to other semantic schemes, such as AMR (.", "labels": [], "entities": [{"text": "USIM", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.7752707600593567}, {"text": "AMR", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.7884708642959595}]}, {"text": "USIM is conceptually related to RLMs developed for MT.", "labels": [], "entities": [{"text": "USIM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.906727135181427}, {"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9442176222801208}]}, {"text": "Notably, XMEANT () compares the source to the output in terms of their semantic role labeling structures.", "labels": [], "entities": []}, {"text": "Our use of UCCA is motivated by its wider coverage of predicate types, as opposed to MEANT's focus on verbal predicates, and UCCA's preservation of structure across translations (.", "labels": [], "entities": []}, {"text": "See () for further discussion.", "labels": [], "entities": []}, {"text": "We conduct experiments to confirm USIM's validity.", "labels": [], "entities": [{"text": "USIM's", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.8165211081504822}, {"text": "validity", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.7469147443771362}]}, {"text": "Specifically, we show that (1) UCCA can be consistently and automatically applied to learner language (LL) ( \u00a74.2), (2) USIM is not prone to unduly penalize valid corrections ( \u00a74.2), and (3) USIM assigns a lower score to corrections of poor quality ( \u00a74.5).", "labels": [], "entities": [{"text": "USIM", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.8628268241882324}]}, {"text": "Our experiments also indicate that UCCA parsing technology is already sufficiently mature for an automatic variant of USIM to provide reliable results ( \u00a74.3).", "labels": [], "entities": [{"text": "UCCA parsing", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.7359334230422974}, {"text": "USIM", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.8721977472305298}]}], "datasetContent": [{"text": "We conduct four types of experiments to validate USIM, showing that: (1) semantic annotation can be consistently applied to LL through inter-annotator agreement (IAA) experiments; (2) a valid corrector scores high on USIM; (3) an automatic UCCA parser can reliably replace human annotation for USIM; (4) USIM is sensitive to changes in meaning.", "labels": [], "entities": []}, {"text": "We train two UCCA annotators, the first author and a paid in-house annotator by annotating both LL and standard English passages, until a high enough agreement is reached (6 training hours).", "labels": [], "entities": [{"text": "UCCA", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.9084964394569397}]}, {"text": "Training passages are excluded from the evaluation.", "labels": [], "entities": []}, {"text": "We use UCCA's annotation guidelines 2 without any adaptations.", "labels": [], "entities": [{"text": "UCCA's annotation guidelines", "start_pos": 7, "end_pos": 35, "type": "DATASET", "confidence": 0.937878280878067}]}, {"text": "We experiment on 7 essays and their corrections, each comprising about 500 tokens (see supplementary material 1).", "labels": [], "entities": []}, {"text": "In order to measure IAA, we assigned 4 of these essays to both annotators.", "labels": [], "entities": [{"text": "IAA", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.994675874710083}]}, {"text": "In order to measure the faithfulness score fora valid correction, we annotate both the source and the manually corrected versions of 6 essays, 3 of which were annotated by both annotators.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 (left-hand side) presents the USIM scores  obtained by comparing the NUCLE references and the USIM  DISTSIM  s\u2192r r\u2192s Avg  A+D Scene  Different 0.85 0.83 0.84  0.96  0.93  Same  0.92 0.91 0.92  0.97  0.96  IAA  0.85 0.81 0.83  - - SAR15  - - - 0.95  0.96", "labels": [], "entities": [{"text": "USIM", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.5788759589195251}, {"text": "NUCLE references", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.9338513910770416}, {"text": "USIM", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.771989107131958}, {"text": "DISTSIM  s\u2192r r\u2192s Avg  A+D Scene  Different 0.85 0.83 0.84  0.96  0.93  Same  0.92 0.91 0.92  0.97  0.96  IAA  0.85 0.81 0.83  - - SAR15  - - - 0.95  0.96", "start_pos": 109, "end_pos": 262, "type": "METRIC", "confidence": 0.710889033973217}]}]}