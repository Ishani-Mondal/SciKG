{"title": [{"text": "attr2vec: Jointly Learning Word and Contextual Attribute Embeddings with Factorization Machines", "labels": [], "entities": [{"text": "Jointly Learning Word and Contextual Attribute Embeddings", "start_pos": 10, "end_pos": 67, "type": "TASK", "confidence": 0.6068221884114402}]}], "abstractContent": [{"text": "The widespread use of word embeddings is associated with the recent successes of many natural language processing (NLP) systems.", "labels": [], "entities": []}, {"text": "The key approach of popular models such as word2vec and GloVe is to learn dense vector representations from the context of words.", "labels": [], "entities": []}, {"text": "More recently, other approaches have been proposed that incorporate different types of contextual information, including topics, dependency relations, n-grams, and sentiment.", "labels": [], "entities": []}, {"text": "However, these models typically integrate only limited additional contextual information, and often in ad hoc ways.", "labels": [], "entities": []}, {"text": "In this work, we introduce attr2vec, a novel framework for jointly learning embeddings for words and contextual attributes based on fac-torization machines.", "labels": [], "entities": []}, {"text": "We perform experiments with different types of contextual information.", "labels": [], "entities": []}, {"text": "Our experimental results on a text classification task demonstrate that using attr2vec to jointly learn embeddings for words and Part-of-Speech (POS) tags improves results compared to learning the embeddings independently.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.8169206380844116}]}, {"text": "Moreover, we use attr2vec to train dependency-based embeddings and we show that they exhibit higher similarity between functionally related words compared to traditional approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural network-based methods have been successful in advancing the state-of-the-art in a wide range of NLP tasks, such as dependency parsing), sentence classification, machine translation (, and information retrieval.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7977167069911957}, {"text": "sentence classification", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.8033355176448822}, {"text": "machine translation", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.8316545784473419}, {"text": "information retrieval", "start_pos": 195, "end_pos": 216, "type": "TASK", "confidence": 0.8128193318843842}]}, {"text": "In all these approaches, vectorial distributed word representations, known as word embeddings, have become a fundamental building block.", "labels": [], "entities": []}, {"text": "The use of word embeddings is considered a \"secret sauce\" for contributing to the success of many of these algorithms in recent years (.", "labels": [], "entities": []}, {"text": "Popular models for learning such word embeddings include word2vec (,b,c), GloVe) and fastText ().", "labels": [], "entities": []}, {"text": "The main idea behind these techniques is to represent a word by means of its context.", "labels": [], "entities": []}, {"text": "The most popular forms of context are neighboring words in a window of text (), though examples of additional contextual information might also include document topics (, dependency relations (), morphemes (, n-grams (, and sentiment ().", "labels": [], "entities": []}, {"text": "The embedding idea was originally devised to help overcome problems associated with the high dimensionality of sparse vector representations of words, particularly in the case of neural network modeling, though embeddings have since been used in a variety of machine learning approaches.", "labels": [], "entities": []}, {"text": "However, existing models generally exploit just a small portion of the available contextual information, and they tend to do so in ad hoc ways.", "labels": [], "entities": []}, {"text": "The main purpose of context in these models is to shape the word vector space (that is, to associate a representation to the word), but contextual information is not usually represented in this space.", "labels": [], "entities": []}, {"text": "For instance, used document topics to derive multiple vectors for the same word, each capturing a different sense, but their method does not represent topics in the vector space, that is, it does not generate topic vectors.", "labels": [], "entities": []}, {"text": "Such contextual representations, jointly learned with the word representations, could potentially be useful for multiple tasks.", "labels": [], "entities": []}, {"text": "For instance, pre-trained contextual vectors could be used as additional features, together with pre-trained word vectors, to improve the performance of existing models.", "labels": [], "entities": []}, {"text": "In this paper, we propose attr2vec, a novel framework for learning word embedding models that jointly associate distributed representations with words and with generic contextual attributes.", "labels": [], "entities": []}, {"text": "attr2vec is inspired by the GloVe approach of and can mimic it when no additional contextual attribute is considered.", "labels": [], "entities": []}, {"text": "In contrast with GloVe, attr2vec uses Factorization Machines (FMs).", "labels": [], "entities": []}, {"text": "FMs area generalization of matrix factorization approaches, such as GloVe, and can combine different generic feature types, even when the input data is sparse.", "labels": [], "entities": []}, {"text": "Moreover, FMs do not consider input features as independent but model their interaction by factorizing their latent representation in pairwise fashion.", "labels": [], "entities": []}, {"text": "Here, we conduct an experimental study to assess whether the proposed embedding model can lead to better performance fora text classification task on the Reuters-21578 dataset, using trained vectors as input to a convolutional neural network.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.811141570409139}, {"text": "Reuters-21578 dataset", "start_pos": 154, "end_pos": 175, "type": "DATASET", "confidence": 0.9847873747348785}]}, {"text": "The results show that jointly learned word and Part-of-Speech (POS) embeddings with attr2vec can achieve higher F1 and precision scores compared to embeddings learned independently.", "labels": [], "entities": [{"text": "F1", "start_pos": 112, "end_pos": 114, "type": "METRIC", "confidence": 0.9995715022087097}, {"text": "precision scores", "start_pos": 119, "end_pos": 135, "type": "METRIC", "confidence": 0.9212954640388489}]}, {"text": "Moreover, we use attr2vec to train dependency-based word embeddings and show, using the publicly available WordSim353 dataset, that such embeddings yield more functional similarities than embeddings trained using a linear bag-of-word approach (such as GloVe).", "labels": [], "entities": [{"text": "WordSim353 dataset", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.9852651655673981}]}, {"text": "We also performed a qualitative analysis that provides insights on how contextual attributes affects the distribution of words in the vector space.", "labels": [], "entities": []}, {"text": "Summing up, the main contributions of our work are the following: \u2022 we extend the GloVe model to consider additional contextual information.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work to present a general model able to jointly train dense vector representations for word and multiple arbitrary contextual attributes; \u2022 we define a novel loss function based on factorization machines to jointly learn word and contextual attribute embeddings; \u2022 we show how to model the input data and compute co-occurrence statistics using either a linear bag-of-word approach or syntactic dependency relations.", "labels": [], "entities": []}, {"text": "We provide the source code for the attr2vec model at https://github.com/ thomsonreuters/attr2vec.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides an overview of related work and Section 3 introduces the attr2vec model.", "labels": [], "entities": []}, {"text": "In Section 4, we present the experimental results, and close this paper with some concluding remarks in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted an experimental study on realworld data to compare our attr2vec model with other state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "For a training corpus to learn embeddings, we used the Reuters News Archive, in particular, the collection of all news stories published by the Reuters News Agency from: Average F1 score (and precision in parentheses) for topic prediction on the Reuters-21578 dataset.", "labels": [], "entities": [{"text": "Reuters News Archive", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.9760749737421671}, {"text": "Reuters News Agency", "start_pos": 144, "end_pos": 163, "type": "DATASET", "confidence": 0.8675447106361389}, {"text": "F1 score", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.914496123790741}, {"text": "precision", "start_pos": 192, "end_pos": 201, "type": "METRIC", "confidence": 0.9665239453315735}, {"text": "topic prediction", "start_pos": 222, "end_pos": 238, "type": "TASK", "confidence": 0.7862929105758667}, {"text": "Reuters-21578 dataset", "start_pos": 246, "end_pos": 267, "type": "DATASET", "confidence": 0.9889822006225586}]}, {"text": "w p indicates the concatenation of word and POS tag vectors.", "labels": [], "entities": []}, {"text": "w r refer to randomly initialized word vectors and pr to randomly initialized POS tag vector.", "labels": [], "entities": []}, {"text": "w i and pi respectively refer to vectors independently trained with the GloVe model for words and POS tags; w j and p j respectively refer to vectors jointly trained with attr2vec for words and POS tags.", "labels": [], "entities": [{"text": "GloVe model", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.8998308479785919}]}, {"text": "first applied a heuristic filtering approach to exclude non-textual documents, resulting in a collection of \u223c8M news articles (\u223c3B tokens).", "labels": [], "entities": []}, {"text": "We then performed tokenization, part-of-speech tagging, and syntactic dependency parsing on the corpus using NLP4J 2 (.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.965542733669281}, {"text": "part-of-speech tagging", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.7054329663515091}, {"text": "syntactic dependency parsing", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.6985849340756735}]}, {"text": "The POS tagger achieves an accuracy score of 97.64%, the dependency parser achieve a label accuracy score of 94.94% (.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.6416851729154587}, {"text": "accuracy score", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.9782436192035675}, {"text": "accuracy score", "start_pos": 91, "end_pos": 105, "type": "METRIC", "confidence": 0.8618186116218567}]}, {"text": "As a baseline we considered 200-dimensional GloVe vectors trained on the corpus using the code and hyperparameters of.", "labels": [], "entities": []}, {"text": "In particular, we used y max = 100 and \u03b1 = 3/4 for all our experiments.", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 39, "end_pos": 40, "type": "METRIC", "confidence": 0.953305184841156}]}, {"text": "Experimental Setup For this experiment we trained 200-dimensional attr2vec vectors using part-of-speech tags as additional contextual information and a linear bag-of-words approach -i.e., each row in the feature matrix consists of a pair of words and the corresponding pair of POS tags (see the first two groups of columns for the example in.", "labels": [], "entities": []}, {"text": "We used the same hyperparameters as in GloVe.", "labels": [], "entities": []}, {"text": "To make a fair comparison we trained two independent GloVe models, one to obtain word vectors ( w i ) and one to obtain POS tag vectors ( pi ).", "labels": [], "entities": []}, {"text": "The latter model is trained by substituting each word in the corpus with the corresponding POS tag.", "labels": [], "entities": []}, {"text": "Note that our attr2vec model can jointly learn a representation for both words ( w j ) and POS tags ( p j ).", "labels": [], "entities": []}, {"text": "As a baseline we also considered randomly initialized vectors for words ( w r ) and POS tags ( pr ).", "labels": [], "entities": []}, {"text": "To train attr2vec we 2 https://emorynlp.github.io/nlp4j used a modified version 3 of tffm (, an open-source TensorFlow implementation of Factorization Machines.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the attr2vec model, we used the trained vectors as input fora convolutional neural network (CNN).", "labels": [], "entities": []}, {"text": "We used the CNN architecture described by, in particular a modified version of the TensorFlow implementation in, where we add support for pre-trained embeddings.", "labels": [], "entities": []}, {"text": "As hyperparameters, we used a batch size of 128 training samples, no dropout, one layer, filter windows of 3, 4, 5 with 100 feature maps each.", "labels": [], "entities": []}, {"text": "We trained using the Adam optimizer and a learning rate of 0.001 and let the models train for 100 epochs (an epoch is an iteration overall the training points).", "labels": [], "entities": []}, {"text": "We executed three independent runs for each experiment and we report averaged results.", "labels": [], "entities": []}, {"text": "As a benchmark we used the following text classification task: predict all the topic codes associated with an article using the first \u03c4 tokens in the article.", "labels": [], "entities": [{"text": "text classification", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7051529437303543}]}, {"text": "We used the Reuters-21578 4 dataset.", "labels": [], "entities": [{"text": "Reuters-21578 4 dataset", "start_pos": 12, "end_pos": 35, "type": "DATASET", "confidence": 0.9432442585627238}]}, {"text": "This corpus contains 10788 news documents classified into 90 topics.", "labels": [], "entities": []}, {"text": "We used the provided training/test split.", "labels": [], "entities": []}, {"text": "For each document, we considered the first \u03c4 = 250 tokens as input text for the CNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.9037994146347046}]}, {"text": "Note that, in contrast to other previous work (, we consider all topics and formulate a multi-label classification problem.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.6759531795978546}]}, {"text": "For each test article we computed precision, recall and F1 score comparing the actual topic codes and those predicted by the CNN.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9996641874313354}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9994843006134033}, {"text": "F1 score", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9818050265312195}, {"text": "CNN", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.9670358896255493}]}, {"text": "As evaluation metrics we used the average F1 score and the average preci-sion across all test articles.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9885374903678894}]}, {"text": "We trained multiple CNN models, using either word vectors ( w) or the concatenation of word and POS tag vectors ( w p) as inputs, and keeping these vectors static throughout training or allowing the CNN to update them via backpropagation (non-static).", "labels": [], "entities": []}, {"text": "We also considered logistic regression as baseline method, using averaged vectors calculated over the input text as features, as in.", "labels": [], "entities": []}, {"text": "As this is a multilabel task, we used the one-vs-all formulation of logistic regression, which attempts to fit one classifier per class with each class being fitted against all other classes.", "labels": [], "entities": []}, {"text": "L 1 regularization was applied with a weight of 0.005.", "labels": [], "entities": []}, {"text": "Results reports the result of our experiments.", "labels": [], "entities": []}, {"text": "Each entry shows the average F1 score and the average precision in parentheses.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9861667454242706}, {"text": "average", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.967252790927887}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.693175733089447}]}, {"text": "First note that the CNN model consistently outperforms logistic regression for all considered settings.", "labels": [], "entities": []}, {"text": "The CNN performance improves if it receives as input pre-trained vectors as opposed to random ones, consistently with other works).", "labels": [], "entities": []}, {"text": "The performance is comparable when GloVe or attr2vec word vectors are used as input.", "labels": [], "entities": []}, {"text": "The key advantage of our attr2vec model over GloVe is demonstrated when additional contextual information is considered in the CNN model.", "labels": [], "entities": []}, {"text": "The performance of the CNN model improves if POS vectors are considered together with GloVe word vectors in input, both when such POS vectors are randomly initialized ( w i pr ) and independently trained with the GloVe model ( w i pi ).", "labels": [], "entities": []}, {"text": "However, the best performance is achieved when word and POS tags vectors are jointly trained with our attr2vec model ( w j p j ).", "labels": [], "entities": []}, {"text": "Note that the aim of the paper was not to show that POS tags help for text classification tasks (to that end, an exhaustive exploration of the parameter space would have been needed); instead, the goal of this work is to introduce anew embedding model that jointly learns a representation for words and POS tags, capturing the interaction between them, and to show that such representation is beneficial fora CNN with respect to embeddings learned in an independent fashion, given the same network settings.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.8160434166590372}]}, {"text": "Standard embedding models (like GloVe), in fact, can capture either the interactions between words or between POS tags in an independent fashion.", "labels": [], "entities": []}, {"text": "Our attr2vec model, in addition, captures the cross-interaction between words and contextual attributes, jointly learning their representation, and our results suggest that this additional information is beneficial for the performance of the CNN model.", "labels": [], "entities": []}, {"text": "Moreover, note that our attr2vec algorithm, unlike GloVe, can handle generic contextual information.", "labels": [], "entities": []}, {"text": "In our second experiment we wanted to address if our attr2vec model was able to produce dependency-based embeddings that exhibit more functional similarity than GloVe embeddings (that usually yield broad topical similarities).", "labels": [], "entities": []}, {"text": "To this end, we trained 200-dimensional attr2vec vectors using a dependency-based approach -i.e, each row in the feature matrix consist of a word and a dependency label (see the example in).", "labels": [], "entities": []}, {"text": "Our evaluation closely follows the one in.", "labels": [], "entities": []}, {"text": "In particular, we used the WordSim353 dataset () containing pairs of similar words that reflect either relatedness (topical similarity) or similarity (functional similarity) relations.", "labels": [], "entities": [{"text": "WordSim353 dataset", "start_pos": 27, "end_pos": 45, "type": "DATASET", "confidence": 0.9842907190322876}]}, {"text": "The pairs are ranked according to the cosine similarity between the corresponding word vectors.", "labels": [], "entities": []}, {"text": "The idea is that a model that focuses on functional similarity should rank similar pairs in the dataset above the related ones.", "labels": [], "entities": []}, {"text": "For instance, such a model should rank the pair money-currency (i.e., functionally similar words) above the pair money-laundering (i.e., topically similar words).", "labels": [], "entities": []}, {"text": "We drew a recall-precision curve by considering related pair as amiss and similar pair as a hit.", "labels": [], "entities": [{"text": "recall-precision", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9978213310241699}]}, {"text": "In this way we aimed to capture the embeddings affinity towards the similarity subset over the re-latedness one.", "labels": [], "entities": []}, {"text": "reports the result of the experiment.", "labels": [], "entities": []}, {"text": "The attr2vec curve (orange solid line) is higher than the GloVe one (blue dashed line) and the area under the cuve is larger (0.74 with respect to 0.57), suggesting that attr2vec yields more functional similarities with respect to GloVe.", "labels": [], "entities": []}, {"text": "Note that a similar behaviour has been observed in for context-predictive models (i.e., the skip-gram model with negative sampling).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, attr2vec is the first model that incorporates syntactic dependency relations in a co-occurrence counts based model (such as GloVe).", "labels": [], "entities": []}, {"text": "Moreover, attr2vec is a general model that can handle additional arbitrary contextual information.", "labels": [], "entities": []}, {"text": "Our final evaluation is qualitative.", "labels": [], "entities": []}, {"text": "We trained 200-dimensional attr2vec embeddings using news topics as additional contextual information and a linear bag-of-words approach -i.e., each row in the feature matrix consists of a pair of words and the topic of the news article where such pair has been observed (see the first and the last group of columns for the example in).", "labels": [], "entities": []}, {"text": "In particular, we used the same collection of \u223c8M news articles presented in Section 4.1 and we considered the following two article topics: general news stories (G) and sport news (SP O).", "labels": [], "entities": []}, {"text": "shows a two-dimensional projection of the 200-dimensional vector space where words and topics representations lie, obtained using the t-SNE 5 visualisation technique.", "labels": [], "entities": []}, {"text": "Here the two topic points (G on the left and SP O on the right of the figure) seem to metaphorically act as \"magnets\", modifying the space and forming two clusters of words.", "labels": [], "entities": []}, {"text": "The left cluster around the representation of topic G includes general words not related to sports such as \"mars\", \"sound\", \"warranty\", \"finance\", \"train\", while the right cluster around the representation of topic SP O contains words related to sports such as \"football\", \"coach\", \"game\", \"stadium\", \"cricket\".", "labels": [], "entities": [{"text": "mars", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.974406361579895}]}, {"text": "Words that are related with both general news stories and sport news lie somewhere in the middle between these two clusters.", "labels": [], "entities": []}, {"text": "Examples of such words include \"penalties\", \"transfer\", \"medical\", \"goal\", \"supporters\".", "labels": [], "entities": []}, {"text": "Note that there are other attractive and repulsive forces in the vec- We used the TensorBoard implementation of t-SNE.", "labels": [], "entities": []}, {"text": "tor space driven byword similarity, and that a twodimensional representation is only able to capture a small portion of all relations that take place in the higher dimensional space.", "labels": [], "entities": []}], "tableCaptions": []}