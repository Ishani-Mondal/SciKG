{"title": [{"text": "Exploiting Semantics in Neural Machine Translation with Graph Convolutional Networks", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6484641035397848}]}], "abstractContent": [{"text": "Semantic representations have long been argued as potentially useful for enforcing meaning preservation and improving generalization performance of machine translation methods.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7015953660011292}, {"text": "machine translation", "start_pos": 148, "end_pos": 167, "type": "TASK", "confidence": 0.7150487899780273}]}, {"text": "In this work, we are the first to incorporate information about predicate-argument structure of source sentences (namely, semantic-role representations) into neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 158, "end_pos": 184, "type": "TASK", "confidence": 0.7325930794080099}]}, {"text": "We use Graph Convolutional Networks (GCNs) to inject a semantic bias into sentence encoders and achieve improvements in BLEU scores over the linguistic-agnostic and syntax-aware versions on the English-German language pair.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9992792010307312}]}], "introductionContent": [{"text": "It has long been argued that semantic representations may provide a useful linguistic bias to machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7332755327224731}]}, {"text": "Semantic representations provide an abstraction which can generalize over different surface realizations of the same underlying 'meaning'.", "labels": [], "entities": []}, {"text": "Providing this information to a machine translation system, can, in principle, improve meaning preservation and boost generalization performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7107947915792465}, {"text": "meaning preservation", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.8107208907604218}]}, {"text": "Though incorporation of semantic information into traditional statistical machine translation has been an active research topic (e.g.,;), we are not aware of any previous work considering semantic structures in neural machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.64121213555336}, {"text": "neural machine translation", "start_pos": 211, "end_pos": 237, "type": "TASK", "confidence": 0.759017805258433}]}, {"text": "In this work, we aim to fill this gap by showing how information about predicate-argument structure of source sentences can be integrated into standard attentionbased NMT models (.", "labels": [], "entities": []}, {"text": "We consider PropBank-style () semantic role structures, or more specifi- cally their dependency versions (.", "labels": [], "entities": [{"text": "PropBank-style", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.8815535306930542}]}, {"text": "The semantic-role representations mark semantic arguments of predicates in a sentence and categorize them according to their semantic roles.", "labels": [], "entities": []}, {"text": "Consider, the predicate gave has three arguments: 1 John (semantic role A0, 'the giver'), wife (A2, 'an entity given to') and present (A1, 'the thing given').", "labels": [], "entities": []}, {"text": "Semantic roles capture commonalities between different realizations of the same underlying predicate-argument structures.", "labels": [], "entities": []}, {"text": "For example, present will still be A1 in sentence \"John gave a nice present to his wonderful wife\", despite different surface forms of the two sentences.", "labels": [], "entities": [{"text": "A1", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.988506555557251}]}, {"text": "We hypothesize that semantic roles can be especially beneficial in NMT, as 'argument switching' (flipping arguments corresponding to different roles) is one of frequent and severe mistakes made by NMT systems.", "labels": [], "entities": [{"text": "argument switching", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7314288169145584}]}, {"text": "There is a limited amount of work on incorporating graph structures into neural sequence models.", "labels": [], "entities": []}, {"text": "Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax (, our graphs are different from syntactic structures.", "labels": [], "entities": []}, {"text": "Unlike syntactic dependency graphs, they are not trees and thus cannot be processed in a bottom-up fashion as in or easily linearized as in.", "labels": [], "entities": []}, {"text": "Luckily, the modeling approach of does not make any assumptions about the graph structure, and thus we build on their method.", "labels": [], "entities": []}, {"text": "used Graph Convolutional Networks (GCNs) to encode syntactic structure.", "labels": [], "entities": []}, {"text": "GCNs were originally proposed by and modified to handle labeled and automatically predicted (hence noisy) syntactic dependency graphs by . Representations of nodes (i.e. words in a sentence) in GCNs are directly influenced by representations of their neighbors in the graph.", "labels": [], "entities": []}, {"text": "The form of influence (e.g., transition matrices and parameters of gates) are learned in such away as to benefit the end task (i.e. translation).", "labels": [], "entities": []}, {"text": "These linguistically-aware word representations are used within a neural encoder.", "labels": [], "entities": []}, {"text": "Although recent research has shown that neural architectures are able to learn some linguistic phenomena without explicit linguistic supervision (, informing word representations with linguistic structures can provide a useful inductive bias.", "labels": [], "entities": []}, {"text": "We apply GCNs to the semantic dependency graphs and experiment on the English-German language pair (WMT16).", "labels": [], "entities": [{"text": "WMT16", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.8949700593948364}]}, {"text": "We observe an improvement over the semantics-agnostic baseline (a BiRNN encoder; 23.3 vs 24.5 BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9950459003448486}]}, {"text": "As we use exactly the same modeling approach as in the syntactic method of, we can easily compare the influence of the types of linguistic structures (i.e., syntax vs. semantics).", "labels": [], "entities": []}, {"text": "We observe that when using full WMT data we obtain better results with semantics than with syntax (23.9 BLEU for syntactic GCN).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9984470009803772}]}, {"text": "Using syntactic and semantic GCNs together, we obtain a further gain (24.9 BLEU) which suggests the complementarity of information encoded by the syntactic and semantic representations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.997931957244873}]}], "datasetContent": [{"text": "We experimented with the English-to-German WMT16 dataset (\u21e04.5 million sentence pairs for training).", "labels": [], "entities": [{"text": "WMT16 dataset", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9045379459857941}]}, {"text": "We use its subset, News Commentary v11, for development and additional experiments (\u21e0226.000 sentence pairs).", "labels": [], "entities": []}, {"text": "For all these experiments, we use newstest2015 and newstest2016 as a validation and test set, respectively.", "labels": [], "entities": [{"text": "newstest2016", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9051207304000854}]}, {"text": "We parsed the English partitions of these datasets with a syntactic dependency parser) and dependency-based semantic role labeler ( ).", "labels": [], "entities": []}, {"text": "We constructed the English vocabulary by taking all words with frequency higher than three, while for German we used byte-pair encodings (BPE)).", "labels": [], "entities": [{"text": "byte-pair encodings (BPE", "start_pos": 117, "end_pos": 141, "type": "METRIC", "confidence": 0.625681683421135}]}, {"text": "All hyperparameter selection was performed on the validation set (see Appendix A).", "labels": [], "entities": []}, {"text": "We measured the performance of the models with (cased) BLEU scores ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9924935102462769}]}, {"text": "The settings and the framework) used for experiments are the ones used in, which we use as baselines.", "labels": [], "entities": []}, {"text": "As RNNs, we use GRUs ().", "labels": [], "entities": [{"text": "GRUs", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.8802615404129028}]}, {"text": "We now discuss the impact that different architectures and linguistic information have on the translation quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test BLEU, En-De, News Commentary.", "labels": [], "entities": [{"text": "Test", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.6314695477485657}, {"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9749143719673157}]}, {"text": " Table 3: Validation BLEU, News commentary only", "labels": [], "entities": [{"text": "Validation", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.8355071544647217}, {"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9663389921188354}]}]}