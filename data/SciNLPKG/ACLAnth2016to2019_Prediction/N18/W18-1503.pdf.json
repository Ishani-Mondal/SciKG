{"title": [{"text": "A Pipeline for Creative Visual Storytelling", "labels": [], "entities": []}], "abstractContent": [{"text": "Computational visual storytelling produces a textual description of events and interpretations depicted in a sequence of images.", "labels": [], "entities": [{"text": "Computational visual storytelling", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6864932974179586}]}, {"text": "These texts are made possible by advances and cross-disciplinary approaches in natural language processing, generation, and computer vision.", "labels": [], "entities": [{"text": "computer vision", "start_pos": 124, "end_pos": 139, "type": "TASK", "confidence": 0.7316802591085434}]}, {"text": "We define a computational creative visual sto-rytelling as one with the ability to alter the telling of a story along three aspects: to speak about different environments, to produce variations based on narrative goals, and to adapt the narrative to the audience.", "labels": [], "entities": []}, {"text": "These aspects of creative storytelling and their effect on the narrative have yet to be explored in visual story-telling.", "labels": [], "entities": []}, {"text": "This paper presents a pipeline of task-modules, Object Identification, Single-Image Inferencing, and Multi-Image Narration, that serve as a preliminary design for building a creative visual storyteller.", "labels": [], "entities": [{"text": "Object Identification", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7130539268255234}]}, {"text": "We have piloted this design fora sequence of images in an annotation task.", "labels": [], "entities": []}, {"text": "We present and analyze the collected corpus and describe plans towards automation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Telling stories from multiple images is a creative challenge that involves visually analyzing the images, drawing connections between them, and producing language to convey the message of the narrative.", "labels": [], "entities": [{"text": "Telling stories from multiple images", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.9004170894622803}]}, {"text": "To computationally model this creative phenomena, a visual storyteller must take into consideration several aspects that will influence the narrative: the environment and presentation of imagery), the narrative goals which affect the desired response of the reader or listener, and the audience, who may prefer to read or hear different narrative styles.", "labels": [], "entities": []}, {"text": "The environment is the content of the imagery, but also its interpretability (e.g., image quality).", "labels": [], "entities": []}, {"text": "Canonical images are available from a number of high-quality datasets), however, there is little coverage of low-resourced domains with low-quality images or atypical camera perspectives that might appear in a sequence of pictures taken from blind persons, a child learning to use a camera, or a robot surveying a site.", "labels": [], "entities": []}, {"text": "For this work, we studied an environment with odd surroundings taken from a camera mounted on aground robot.", "labels": [], "entities": []}, {"text": "Narrative goals guide the selection of what objects or inferences in the image are relevant or uncharacteristic.", "labels": [], "entities": []}, {"text": "The result is a narrative tailored to different goals such as a general \"describe the scene\", or a more focused \"look for suspicious activity\".", "labels": [], "entities": []}, {"text": "The most salient narrative may shift as new information, in the form of images, is presented, offering different possible interpretations of the scene.", "labels": [], "entities": []}, {"text": "This work posed a forensic task with the narrative goal to describe what may have occurred within a scene, assuming some temporal consistency across images.", "labels": [], "entities": []}, {"text": "This open-endedness evoked creativity in the resulting narratives.", "labels": [], "entities": []}, {"text": "The telling of the narrative will also differ based upon the target audience.", "labels": [], "entities": []}, {"text": "A concise narrative is more appropriate if the audience is expecting to hear news or information, while a verbose and humorous narrative is suited for entertainment.", "labels": [], "entities": []}, {"text": "Audiences may differ in how they would best experience the narrative: immersed in the first person or through an omniscient narrator.", "labels": [], "entities": []}, {"text": "The audience in this work was unspecified, thus the audience was the same as the storyteller defining the narrative.", "labels": [], "entities": []}, {"text": "To build a computational creative visual storyteller that customizes a narrative along these three aspects, we propose a creative visual storytelling pipeline requiring separate task-modules for Object Identification, Single-Image Inferencing, and Multi-Image Narration.", "labels": [], "entities": [{"text": "Object Identification", "start_pos": 195, "end_pos": 216, "type": "TASK", "confidence": 0.8096694052219391}, {"text": "Multi-Image Narration", "start_pos": 248, "end_pos": 269, "type": "TASK", "confidence": 0.6779465973377228}]}, {"text": "We have conducted an exploratory pilot experiment following this pipeline to collect data from each task-module to train the computational storyteller.", "labels": [], "entities": []}, {"text": "The collected data provides instances of creative storytelling from which we have analyzed what people see and pay attention to, what they interpret, and how they weave together a story across a series of images.", "labels": [], "entities": []}, {"text": "Creative visual storytelling requires an understanding of the creative processes.", "labels": [], "entities": []}, {"text": "We argue that existing systems cannot achieve these creative aspects of visual storytelling.", "labels": [], "entities": []}, {"text": "Current object identification algorithms may perform poorly on low-resourced environments with minimal training data.", "labels": [], "entities": [{"text": "object identification", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.8801898658275604}]}, {"text": "Computer vision algorithms may overidentify objects, that is, describe more objects than are ultimately needed for the goal of a coherent narrative.", "labels": [], "entities": []}, {"text": "Algorithms that generate captions of an image often produce generic language, rather than language tailored to a specific audience.", "labels": [], "entities": []}, {"text": "Our pilot experiment is an attempt to reveal the creative processes involved when humans perform this task, and then to computationally model the phenomena from the observed data.", "labels": [], "entities": []}, {"text": "Our pipeline is introduced in Section 2, where we also discuss computational considerations and the application of this pipeline to our pilot experiment.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the exploratory pilot experiment, in which we presented images of a low-quality and atypical environment and have annotators answer \"what may have happened here?\"", "labels": [], "entities": []}, {"text": "This open-ended narrative goal has the potential to elicit diverse and creative narratives.", "labels": [], "entities": []}, {"text": "We did not specify the audience, leaving the annotator free to write in a style that appeals to them.", "labels": [], "entities": []}, {"text": "The data and analysis of the pilot are presented in Section 4, as well as observations for extending to crowdsourcing a larger corpus and how to use these creative insights to build computational models that follow this pipeline.", "labels": [], "entities": []}, {"text": "In Section 5 we compare our approach to recent works in other storytelling methodologies, then conclude and describe future directions of this work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Annotators first addressed the questions posed in the Object Identification (T1) and Single-Image Inference (T2) task-modules for image 1 . They repeated the process for image 2 and image 3 , and authored a Multi-Image Narrative (T3).", "labels": [], "entities": []}, {"text": "The annotator workflow mimicked the pipeline presented in.", "labels": [], "entities": []}, {"text": "For each subsequent image, the time allotted increased from five, to eight, to eleven minutes to allow more time for the narrative to be constructed after annotators processed the additional images.", "labels": [], "entities": []}, {"text": "An example image sequence with answers was provided prior to the experiment.", "labels": [], "entities": []}, {"text": "A 5 gave a brief, oral, open-ended explanation of the experiment as not to bias annotators to what they should focus on in the scene or what kind of language they should use.", "labels": [], "entities": []}, {"text": "The goal of this data collection is to gather data that models the creative storytelling processes, not to track these processes in real-time.", "labels": [], "entities": []}, {"text": "A future web-based interface will allow us to track the timing of annotation, what information is added when, and how each taskmodule influences the other task-modules for each image.", "labels": [], "entities": []}, {"text": "Object Identification did not require annotators to define a bounding box for labeled objects, nor were annotators required to provide objective descriptors 2 . Annotators authored natural language labels, phrases, or sentences to describe objects, attributes, and spatial relations while indicating confidence levels if appropriate.", "labels": [], "entities": [{"text": "Object Identification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8479739427566528}]}, {"text": "During Single Image Inferencing, annotators were shown their response from T1 as they authored a natural language description of activity or functions of the image, as well as a natural language explanation of inferences for that determination, citing supporting evidence from T1 output.", "labels": [], "entities": [{"text": "Single Image Inferencing", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.7567945122718811}]}, {"text": "For a single image, annotators may answer the questions posed by T1 and T2 in any order to build the most informed narrative.", "labels": [], "entities": []}, {"text": "Annotators authored a Multi-Image Narrative to explain what has happened in the sequence of images presented so far.", "labels": [], "entities": []}, {"text": "For each image seen in the sequence, annotators were shown their own natural language responses from T1 and T2 for those images.", "labels": [], "entities": []}, {"text": "Annotators were encouraged to look back to their responses in previous images (as the bottom row of indicates), but not to make changes to their responses about the previous images.", "labels": [], "entities": []}, {"text": "They were, however, encouraged to incorporate previous feedback into the context of the current image.", "labels": [], "entities": []}, {"text": "From this task-module, annotators wrote a natural language narrative connecting activity or functions in the images which will be used to learn how to weave together a story across the images.", "labels": [], "entities": []}, {"text": "The open-ended \"what has happened here?\" narrative goal has no single answer.", "labels": [], "entities": []}, {"text": "These annotations maybe treated as ground truth, but we run the risk of potentially missing out on creative alternatives.", "labels": [], "entities": []}, {"text": "Bootstraping all possible objects and inferences would achieve greater coverage, yet this quickly becomes infeasible.", "labels": [], "entities": [{"text": "coverage", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9607709050178528}]}, {"text": "We lean toward the middle, where the answers collected will help determine what annotators deem as important.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Objects identified by annotators in image 1", "labels": [], "entities": []}, {"text": " Table 2: Objects identified by annotators in image 2", "labels": [], "entities": []}, {"text": " Table 3: Objects identified by annotators in image 3 (to- tal of 7 annotators)", "labels": [], "entities": []}, {"text": " Table 4: Object descriptor summary with counts per  annotator (A 2 -A 4 excluded from average, min, and  max; see footnote 4)", "labels": [], "entities": [{"text": "A 2 -A 4", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.8381805181503296}]}, {"text": " Table 7: Object Identification for image 1", "labels": [], "entities": [{"text": "Object Identification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8328225016593933}]}, {"text": " Table 8: Object Identification for image 2", "labels": [], "entities": [{"text": "Object Identification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.827516108751297}]}]}