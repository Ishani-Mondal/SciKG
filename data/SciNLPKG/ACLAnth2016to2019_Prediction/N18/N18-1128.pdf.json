{"title": [{"text": "Reusing Weights in Subword-aware Neural Language Models", "labels": [], "entities": [{"text": "Reusing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9629515409469604}]}], "abstractContent": [{"text": "We propose several ways of reusing subword embeddings and other weights in subword-aware neural language models.", "labels": [], "entities": []}, {"text": "The proposed techniques do not benefit a competitive character-aware model, but some of them improve the performance of syllable-and morpheme-aware models while showing significant reductions in model sizes.", "labels": [], "entities": []}, {"text": "We discover a simple hands-on principle: in a multi-layer input embedding model, layers should be tied consecutively bottom-up if reused at output.", "labels": [], "entities": []}, {"text": "Our best morpheme-aware model with properly reused weights beats the competitive word-level model by a large margin across multiple languages and has 20%-87% fewer parameters.", "labels": [], "entities": []}], "introductionContent": [{"text": "A statistical language model (LM) is a model which assigns a probability to a sequence of words.", "labels": [], "entities": [{"text": "statistical language model (LM)", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.6312165309985479}]}, {"text": "It is used in speech recognition, machine translation, part-of-speech tagging, information retrieval and other applications.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.82858806848526}, {"text": "machine translation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.8077932894229889}, {"text": "part-of-speech tagging", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7541134059429169}, {"text": "information retrieval", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.8307037651538849}]}, {"text": "Data sparsity is a major problem in building traditional n-gram language models, which assume that the probability of a word only depends on the previous n words.", "labels": [], "entities": []}, {"text": "To deal with potentially severe problems when confronted with any n-grams that have not explicitly been seen before, some form of smoothing is necessary.", "labels": [], "entities": []}, {"text": "Recent progress in statistical language modeling is connected with neural language models (NLM), which tackle the data sparsity problem by representing words as vectors.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.8098109165827433}]}, {"text": "Typically this is done twice: at input (to embed the current word of a sequence into a vector space) and at output (to embed candidates for the next word of a sequence).", "labels": [], "entities": []}, {"text": "Especially successful are the models in which the architecture of the neural network between input and output is recurrent (, which we refer to as recurrent neural network language models (RNNLM).", "labels": [], "entities": []}, {"text": "Tying input and output word embeddings in word-level RNNLM is a regularization technique, which was introduced earlier () but has been widely used relatively recently, and there is empirical evidence as well as theoretical justification) that such a simple trick improves language modeling quality while decreasing the total number of trainable parameters almost two-fold, since most of the parameters are due to embedding matrices.", "labels": [], "entities": []}, {"text": "Unfortunately, this regularization technique is not directly applicable to subword-aware neural language models as they receive subwords at input and return words at output.", "labels": [], "entities": []}, {"text": "This raises the following questions: Is it possible to reuse embeddings and other parameters in subword-aware neural language models?", "labels": [], "entities": []}, {"text": "Would it benefit language modeling quality?", "labels": [], "entities": [{"text": "language modeling", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8346899747848511}]}, {"text": "We experimented with different subword units, embedding models, and ways of reusing parameters, and our answer to both questions is as follows: There are several ways to reuse weights in subword-aware neural language models, and none of them improve a competitive character-aware model, but some of them do benefit syllable-and morphemeaware models, while giving significant reductions in model sizes.", "labels": [], "entities": []}, {"text": "A simple morpheme-aware model that sums morpheme embeddings of a word benefits most from appropriate weight tying, showing a significant gain over the competitive word-level baseline across different languages and data set sizes.", "labels": [], "entities": []}, {"text": "Another contribution of this paper is the discovery of a hands-on principle that in a multi-layer input embedding model, layers should be tied consecutively bottom-up if reused at output.", "labels": [], "entities": []}, {"text": "The source code for the morpheme-aware model is available at https://github.com/ zh3nis/morph-sum.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data sets: All models are trained and evaluated on the PTB () and the WikiTextin vocabulary.", "labels": [], "entities": [{"text": "PTB", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.6531900763511658}, {"text": "WikiTextin vocabulary", "start_pos": 70, "end_pos": 91, "type": "DATASET", "confidence": 0.9337015748023987}]}], "tableCaptions": [{"text": " Table 2: Results. The pure word-level models and original versions of subword-aware models (with regular soft- max) serve as baselines. Reusing the input embedding architecture at output in CharCNN leads to prohibitively  slow models when trained on WikiText-2 (\u2248800 tokens/sec on NVIDIA Titan X Pascal); we therefore abandoned  evaluation of these configurations.", "labels": [], "entities": []}, {"text": " Table 4: Reusing different combinations of layers in small CharCNN (left), small SylConcat (top right) and small  MorphSum on PTB data. \"\" means that the layer is reused at output.", "labels": [], "entities": [{"text": "PTB data", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.942405641078949}]}, {"text": " Table 5: Training on PTB and testing on Wikitext-2.", "labels": [], "entities": [{"text": "PTB", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.4193733334541321}, {"text": "Wikitext-2", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.9483724236488342}]}, {"text": " Table 7: Replacing LSTM with AWD-LSTM.", "labels": [], "entities": [{"text": "Replacing LSTM", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7928596436977386}]}, {"text": " Table 8: Non-English corpora statistics. T = number  of tokens in training set; |W| = word vocabulary size;  |M| = morph vocabulary size.", "labels": [], "entities": []}, {"text": " Table 9: Model sizes in millions of trainable parame- ters.", "labels": [], "entities": []}]}