{"title": [{"text": "A Deep Generative Model of Vowel Formant Typology", "labels": [], "entities": []}], "abstractContent": [{"text": "What makes some types of languages more probable than others?", "labels": [], "entities": []}, {"text": "For instance, we know that almost all spoken languages contain the vowel phoneme /i/; why should that be?", "labels": [], "entities": []}, {"text": "The field of linguistic typology seeks to answer these questions and, thereby, divine the mechanisms that underlie human language.", "labels": [], "entities": []}, {"text": "In our work, we tackle the problem of vowel system typology, i.e., we propose a generative probability model of which vowels a language contains.", "labels": [], "entities": []}, {"text": "In contrast to previous work, we work directly with the acoustic information-the first two formant values-rather than modeling discrete sets of phonemic symbols (IPA).", "labels": [], "entities": []}, {"text": "We develop a novel generative probability model and report results based on a corpus of 233 languages .", "labels": [], "entities": [{"text": "generative probability", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.8860238492488861}]}], "introductionContent": [{"text": "Human languages are far from arbitrary; crosslinguistically, they exhibit surprising similarity in many respects and many properties appear to be universally true.", "labels": [], "entities": []}, {"text": "The field of linguistic typology seeks to investigate, describe and quantify the axes along which languages vary.", "labels": [], "entities": []}, {"text": "One facet of language that has been the subject of heavy investigation is the nature of vowel inventories, i.e., which vowels a language contains.", "labels": [], "entities": []}, {"text": "It is a cross-linguistic universal that all spoken languages have vowels, and the underlying principles guiding vowel selection are understood: vowels must be both easily recognizable and well-dispersed (.", "labels": [], "entities": []}, {"text": "In this work, we offer a more formal treatment of the subject, deriving a generative probability model of vowel inventory typology.", "labels": [], "entities": []}, {"text": "Our work builds on ( by investigating not just discrete IPA inventories but the cross-linguistic variation in acoustic formants.", "labels": [], "entities": []}, {"text": "The philosophy behind our approach is that linguistic typology should be treated probabilistically and its goal should be the construction of a universal prior over potential languages.", "labels": [], "entities": []}, {"text": "A probabilistic approach does not rule out linguistic systems completely (as long as one's theoretical formalism can describe them at all), but it can position phenomena on a scale from very common to very improbable.", "labels": [], "entities": []}, {"text": "Probabilistic modeling also provides a discipline for drawing conclusions from sparse data.", "labels": [], "entities": [{"text": "Probabilistic modeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8008101284503937}]}, {"text": "While we know of over 7000 human languages, we have some sort of linguistic analysis for only 2300 of them (, and the dataset used in this paper) provides simple vowel data for fewer than 250 languages.", "labels": [], "entities": []}, {"text": "Formants are the resonant frequencies of the human vocal tract during the production of speech sounds.", "labels": [], "entities": []}, {"text": "We propose a Bayesian generative model of vowel inventories, where each language's inventory is a finite subset of acoustic vowels represented as points (F 1 , F 2 ) \u2208 R 2 . We deploy tools from the neural-network and point-process literatures and experiment on a dataset with 233 distinct languages.", "labels": [], "entities": []}, {"text": "We show that our most complicated model outperforms simpler models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation in our setting is tricky.", "labels": [], "entities": []}, {"text": "The scientific goal of our work is to place a bit of linguistic theory on a firm probabilistic footing, rather than a downstream engineering-task, whose performance we could measure.", "labels": [], "entities": []}, {"text": "Our first evaluation metric is cross-entropy: the average negative log-probability of the vowel systems in held-out test data, given the universal inventory of N phones that we trained through EM.", "labels": [], "entities": []}, {"text": "We find this to be the cleanest method for scientific evaluation-it is the metric of optimization and has a clear interpretation: how surprised was the model to seethe vowel systems of held-out, but attested, languages?", "labels": [], "entities": []}, {"text": "The cross-entropy is the negative log of the \u00b7 \u00b7 \u00b7 expression in eq., with now rang- and expected Euclidean-distance error of the cloze prediction (lower is better).", "labels": [], "entities": [{"text": "rang", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9576557874679565}]}, {"text": "The overall best value for each task is boldfaced.", "labels": [], "entities": []}, {"text": "The case N = 50 is compared against our supervised baseline.", "labels": [], "entities": []}, {"text": "The N = 57 row is the case where we allowed N to fluctuate during inference using reversible-jump MCMC; this was the N value selected at the final EM iteration.", "labels": [], "entities": []}, {"text": "3 give several methods for estimating the intractable sum in language . We use the simple harmonic mean estimator, based on 50 samples of a drawn with our Gibbs sampler (warm-started from the final E-step of training).", "labels": [], "entities": []}, {"text": "In addition, following Cotterell and Eisner (2017), we evaluate our trained model's ability to perform a cloze task.", "labels": [], "entities": []}, {"text": "Given n \u2212 1 or n \u2212 2 of the vowels in heldout language , can we predict the pronunciations v k of the remaining 1 or 2?", "labels": [], "entities": []}, {"text": "We predict v k to be \u03bd \u03b8 (\u00b5 i ) where i = a k is the phone inferred by the sampler.", "labels": [], "entities": []}, {"text": "Note that the sampler's inference here is based only on the observed vowels (the likelihood) and the focalization-dispersion preferences of the DPP (the prior).", "labels": [], "entities": []}, {"text": "We report the expected error of such a prediction-where error is quantified by Euclidean distance in (F 1 , F 2 ) formant space-over the same 50 samples of a . For instance, consider a previously unseen vowel system with formant values {(499, 2199), (861, 1420), (571, 1079)}.", "labels": [], "entities": []}, {"text": "A \"cloze1\" evaluation would aim to predict {(499, 2199)} as the missing vowel, given {(861, 1420), (571, 1079)}, and the fact that n = 3.", "labels": [], "entities": []}, {"text": "A \"cloze12\" evaluation would aim to predict two missing vowels.", "labels": [], "entities": []}, {"text": "Here, we report experimental details and the hyperparameters that we use to achieve the results reported.", "labels": [], "entities": []}, {"text": "We consider a neural network \u03bd \u03b8 with k \u2208 layers and find k = 1 the best performer on development data.", "labels": [], "entities": []}, {"text": "Recall that our diffeomorphism constraint requires that each layer have exactly two hidden units, the same as the number of observed formants.", "labels": [], "entities": []}, {"text": "We consider N \u2208 {15, 25, 50, 100} phones as well as letting N fluctuate with reversible-jump MCMC (see footnote 1).", "labels": [], "entities": []}, {"text": "We train for 100 iterations of EM, taking S = 5 samples at each E-step.", "labels": [], "entities": []}, {"text": "At each M-step, we run 50 iterations of SGD for the focalization NN and also for the diffeomorphism NN.", "labels": [], "entities": []}, {"text": "For each N , we selected (\u03c3 2 , \u03c1) by minimizing cross-entropy on a held-out development set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cross-entropy in nats per language (lower is better)", "labels": [], "entities": []}]}