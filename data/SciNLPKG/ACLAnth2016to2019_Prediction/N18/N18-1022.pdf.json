{"title": [], "abstractContent": [{"text": "We present a content-based method for recommending citations in an academic paper draft.", "labels": [], "entities": []}, {"text": "We embed a given query document into a vector space, then use its nearest neighbors as candidates, and rerank the candidates using a discriminative model trained to distinguish between observed and unobserved citations.", "labels": [], "entities": []}, {"text": "Unlike previous work, our method does not require metadata such as author names which can be missing, e.g., during the peer review process.", "labels": [], "entities": []}, {"text": "Without using metadata, our method outperforms the best reported results on PubMed and DBLP datasets with relative improvements of over 18% in F1@20 and over 22% in MRR.", "labels": [], "entities": [{"text": "PubMed and DBLP datasets", "start_pos": 76, "end_pos": 100, "type": "DATASET", "confidence": 0.8038726896047592}, {"text": "F1", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.9982151985168457}]}, {"text": "We show empirically that, although adding metadata improves the performance on standard metrics, it favors self-citations which are less useful in a citation recommendation setup.", "labels": [], "entities": []}, {"text": "We release an online portal for citation recommendation based on our method, 1 and anew dataset OpenCorpus of 7 million research articles to facilitate future research on this task.", "labels": [], "entities": [{"text": "citation recommendation", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.915477067232132}, {"text": "OpenCorpus", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.9440528154373169}]}], "introductionContent": [{"text": "Due to the rapid growth of the scientific literature, conducting a comprehensive literature review has become challenging, despite major advances in digital libraries and information retrieval systems.", "labels": [], "entities": []}, {"text": "Citation recommendation can help improve the quality and efficiency of this process by suggesting published scientific documents as likely citations fora query document, e.g., a paper draft to be submitted for ACL 2018.", "labels": [], "entities": [{"text": "Citation recommendation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7925160527229309}]}, {"text": "Existing citation recommendation systems rely on various information of the query documents such as author names and publication venue; Yu et al., * Work done while on contract with AI2 \u2020 Work done while at AI2 1 http://labs.semanticscholar.org/ citeomatic/ 2012), or a partial list of citations provided by the author (;) which may not be available, e.g., during the peer review processor in the early stage of a research project.", "labels": [], "entities": []}, {"text": "Our method uses a neural model to embed all available documents into a vector space by encoding the textual content of each document.", "labels": [], "entities": []}, {"text": "We then select the nearest neighbors of a query document as candidates and rerank the candidates using a second model trained to discriminate between observed and unobserved citations.", "labels": [], "entities": []}, {"text": "Unlike previous work, we can embed new documents in the same vector space used to identify candidate citations based on their text content, obviating the need to re-train the models to include new published papers.", "labels": [], "entities": []}, {"text": "Further, unlike prior work (), our model is computationally efficient and scalable during both training and test time.", "labels": [], "entities": []}, {"text": "We assess the feasibility of recommending citations when some metadata for the query document is missing, and find that we are able to outperform the best reported results on two datasets while only using papers' textual content (i.e. its title and abstract).", "labels": [], "entities": []}, {"text": "While adding metadata helps further improve the performance of our method on standard metrics, we found that it introduces a bias for selfcitation which might not be desirable in a citation recommendation system.", "labels": [], "entities": []}, {"text": "See \u00a75 for details of our experimental results.", "labels": [], "entities": []}, {"text": "Our main contributions are: \u2022 a content-based method for citation recommendation which remains robust when metadata are missing for query documents, \u2022 large improvements overstate of the art results on two citation recommendation datasets despite omitting the metadata, \u2022 anew dataset of seven million research papers, addressing some of the limitations in Figure 1: An overview of our Citation Recommendation system.", "labels": [], "entities": [{"text": "citation recommendation", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.8537542223930359}]}, {"text": "In Phase 1 (NNSelect), we project all documents in the corpus (7 in this toy example) in addition to the query document d q into a vector space, and use its (K=4) nearest neighbors: d 2 , d 6 , d 3 , and d 4 as candidates.", "labels": [], "entities": []}, {"text": "We also add d 7 as a candidate because it was cited ind 3 . In Phase 2 (NNRank), we score each pair previous datasets used for citation recommendation, and \u2022 a scalable web-based literature review tool based on this work.", "labels": [], "entities": [{"text": "citation recommendation", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.8843498229980469}]}], "datasetContent": [{"text": "In this section, we describe experimental results of our citation recommendation method and compare it to previous work.", "labels": [], "entities": [{"text": "citation recommendation", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.8903245329856873}]}, {"text": "We use the DBLP and PubMed datasets) to compare with previous work on citation recommendation.", "labels": [], "entities": [{"text": "DBLP", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.9381371140480042}, {"text": "PubMed datasets", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.9425424635410309}, {"text": "citation recommendation", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.911113828420639}]}, {"text": "The DBLP dataset contains over 50K scientific articles in the computer science domain, with an average of 5 citations per article.", "labels": [], "entities": [{"text": "DBLP dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.968242347240448}]}, {"text": "The PubMed dataset contains over 45K scientific articles in the medical domains, with an average of 17 citations per article.", "labels": [], "entities": [{"text": "PubMed dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9787817895412445}]}, {"text": "In both datasets, a document is accompanied by its title, abstract, venue (i.e. journal or conference where the document was published), authors, citations (i.e. other documents in the corpus that are referenced in the given document) and keyphrases (i.e. phrases considered important by automated extraction methods).", "labels": [], "entities": []}, {"text": "We replicate the experimental setup of by excluding papers with fewer than 10 citations and using the standard train, dev and test splits.", "labels": [], "entities": []}, {"text": "We also introduce OpenCorpus, 8 anew dataset of 7 million scientific articles primarily drawn from the computer science and neuroscience domain.", "labels": [], "entities": []}, {"text": "Due to licensing constraints, documents in the corpus do not include the full text of the scientific articles, but include the title, abstract, year, author, venue, keyphrases and citation information.", "labels": [], "entities": []}, {"text": "The mutually exclusive training, development, and test splits were selected such that no document in the development or test set has a publication year less than that of any document in the training set.", "labels": [], "entities": []}, {"text": "Papers with zero citations were removed from the development and test sets.", "labels": [], "entities": [{"text": "citations", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9377076625823975}]}, {"text": "We describe the key characteristics of OpenCorpus in.", "labels": [], "entities": []}, {"text": "We compare our method to two baseline methods for recommending citations: ClusCite and BM25.", "labels": [], "entities": [{"text": "ClusCite", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.8501087427139282}, {"text": "BM25", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.7496764659881592}]}, {"text": "ClusCite () clusters nodes in a heterogeneous graph of terms, authors and venues in order to find related documents which should be cited.", "labels": [], "entities": []}, {"text": "We use the ClusCite results as reported in, which compared it to several other citation recommendation methods and found that it obtains state of the art results on the PubMed and DBLP datasets.", "labels": [], "entities": [{"text": "PubMed and DBLP datasets", "start_pos": 169, "end_pos": 193, "type": "DATASET", "confidence": 0.7785225361585617}]}, {"text": "The BM25 results are based on our implementation of the popular ranking function Okapi BM25 used in many information retrieval systems.", "labels": [], "entities": [{"text": "Okapi BM25", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.7169432044029236}]}, {"text": "See Appendix \u00a7D for details of our BM25 implementation.", "labels": [], "entities": [{"text": "BM25", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9025191068649292}]}], "tableCaptions": [{"text": " Table 2: F1@20 and MRR results for two baselines and three variants of our method. BM25 results are based  on our implementation of this baseline, while ClusCite results are based on the results reported in Ren et al.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9971914887428284}, {"text": "MRR", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9647197723388672}, {"text": "BM25", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.7567557692527771}]}, {"text": " Table 3: Comparison of PubMed results of the full  model with model without (i) intersection features, (ii)  negative nearest neighbors in training samples, and (iii)  numerical features.", "labels": [], "entities": []}, {"text": " Table 4: OpenCorpus results for NNSelect step  with varying number of nearest neighbors on 1,000 val- idation documents.", "labels": [], "entities": []}, {"text": " Table 5: Results of our BM25 implementation on  DBLP and Pubmed datasets.", "labels": [], "entities": [{"text": "BM25", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.7196224331855774}, {"text": "DBLP", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8792691230773926}, {"text": "Pubmed datasets", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.7932928204536438}]}, {"text": " Table 6: DBLP hyperparameter tuning results. Note that the dense dimension when using pretrained vectors is  fixed to be 300. A '-' indicates that the variable was not tuned.", "labels": [], "entities": [{"text": "DBLP hyperparameter tuning", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.5689572989940643}]}, {"text": " Table 7: PubMed hyperparameter tuning results. Note that the dense dimension when using pretrained GloVe  vectors is fixed to be 300. A '-' indicates that the variable was not tuned.", "labels": [], "entities": []}, {"text": " Table 8: Per-dataset parameters. These were hand-specified. *LazyAdamOptimizer is part of TensorFlow.  **Nadam is part of Keras.", "labels": [], "entities": []}, {"text": " Table 9: Hyperparameters used for OpenCorpus", "labels": [], "entities": [{"text": "OpenCorpus", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.7006649971008301}]}, {"text": " Table 10: Comparing NNRank with ClusCite. (", "labels": [], "entities": []}]}