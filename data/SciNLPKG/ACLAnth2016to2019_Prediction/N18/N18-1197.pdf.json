{"title": [], "abstractContent": [{"text": "The named concepts and compositional operators present in natural language provide a rich source of information about the abstractions humans use to navigate the world.", "labels": [], "entities": []}, {"text": "Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies?", "labels": [], "entities": []}, {"text": "This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure.", "labels": [], "entities": []}, {"text": "Ina pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions.", "labels": [], "entities": [{"text": "language interpretation", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7648270428180695}]}, {"text": "To learn anew concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter's loss on training examples.", "labels": [], "entities": []}, {"text": "Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning.", "labels": [], "entities": []}, {"text": "Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without.", "labels": [], "entities": [{"text": "image classification", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.8011254668235779}, {"text": "text editing", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.8147333860397339}]}], "introductionContent": [{"text": "The structure of natural language reflects the structure of the world.", "labels": [], "entities": []}, {"text": "For example, the fact that it is easy for humans to communicate the concept left of the circle but comparatively difficult to communicate mean saturation of the first five pixels in the third column reveals something about the abstractions we find useful for interpreting and navigating our environment.", "labels": [], "entities": []}, {"text": "In machine learning, efficient automatic discovery of reusable abstract structure remains a major challenge.", "labels": [], "entities": []}, {"text": "This paper investigates whether Code and data are available at https://github.com/ jacobandreas/l3.", "labels": [], "entities": []}, {"text": "background knowledge from language can provide a useful scaffold for acquiring it.", "labels": [], "entities": []}, {"text": "We specifically propose to use language as a latent parameter space for few-shot learning problems of all kinds, including classification, transduction and policy search.", "labels": [], "entities": [{"text": "policy search", "start_pos": 156, "end_pos": 169, "type": "TASK", "confidence": 0.687669575214386}]}, {"text": "We aim to show that this linguistic parameterization produces models that are both more accurate and more interpretable than direct approaches to few-shot learning.", "labels": [], "entities": []}, {"text": "Like many recent frameworks for multitaskand meta-learning, our approach consists of three phases: a pretraining phase, a concept-learning phase, and an evaluation phase.", "labels": [], "entities": []}, {"text": "Here, the product of pretraining is a language interpretation model that maps from descriptions to predictors (e.g. image classifiers or reinforcement learners).", "labels": [], "entities": [{"text": "language interpretation", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7369741797447205}]}, {"text": "Our thesis is that language learning is a powerful, generalpurpose kind of pretraining, even for tasks that do not directly involve language.", "labels": [], "entities": []}, {"text": "New concepts are learned by searching directly in the space of natural language strings to mini- x y x y Figure 2: Formulation of the learning problem.", "labels": [], "entities": []}, {"text": "Ultimately, we care about our model's ability to learn a concept from a small number of training examples (b) and successfully generalize it to held-out data (c).", "labels": [], "entities": []}, {"text": "In this paper, concept learning is supported by a language learning phase (a) that makes use of natural language annotations on other learning problems.", "labels": [], "entities": [{"text": "concept learning", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.8096716701984406}]}, {"text": "These annotations are not provided for the real target task in (b-c).", "labels": [], "entities": []}, {"text": "mize the loss incurred by the language interpretation model ().", "labels": [], "entities": [{"text": "language interpretation", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.7082068026065826}]}, {"text": "Especially on tasks that require the learner to model high-level compositional structure shared by training examples, natural language hypotheses serve a threefold purpose: they make it easier to discover these compositional concepts, harder to overfit to few examples, and easier for humans to understand inferred patterns.", "labels": [], "entities": []}, {"text": "Our approach can be implemented using a standard kit of neural components, and is simple and general.", "labels": [], "entities": []}, {"text": "Ina variety of settings, we find that the structure imposed by a natural-language parameterization is helpful for efficient learning and exploration.", "labels": [], "entities": []}, {"text": "The approach outperforms both multitask-and meta-learning approaches that map directly from training examples to outputs byway of a real-valued parameterization, as well as approaches that make use of natural language annotations as an additional supervisory signal rather than an explicit latent parameter.", "labels": [], "entities": []}, {"text": "The natural language concept descriptions inferred by our approach often agree with human annotations when they are correct, and provide an interpretable debugging signal when incorrect.", "labels": [], "entities": []}, {"text": "In short, by equipping models with the ability to \"think out loud\" when learning, they become both more comprehensible and more accurate.", "labels": [], "entities": []}], "datasetContent": [{"text": "ShapeWorld This is the only fully-synthetic dataset used in our experiments.", "labels": [], "entities": []}, {"text": "Each scene features 4 or 5 non-overlapping entities.", "labels": [], "entities": []}, {"text": "Descriptions refer to spatial relationships between pairs of entities identified by shape, color, or both.", "labels": [], "entities": []}, {"text": "There are 8 colors and 8 shapes.", "labels": [], "entities": []}, {"text": "The total vocabulary size is only 30 words, but the dataset contains 2643 distinct captions.", "labels": [], "entities": []}, {"text": "Descriptions are on average 12.0 words long.", "labels": [], "entities": []}, {"text": "Regular expressions Annotations were collected from Mechanical Turk users.", "labels": [], "entities": []}, {"text": "Each user was presented with the same task as the learner in this paper: they observed five strings being transformed, and had to predict how to transform a sixth.", "labels": [], "entities": []}, {"text": "Only after they correctly generated the heldout word were they asked fora description of the rule.", "labels": [], "entities": []}, {"text": "Workers were additionally presented with hints like \"look at the beginning of the word\" or \"look at the vowels\".", "labels": [], "entities": []}, {"text": "Descriptions are automatically preprocessed to strip punctuation and ensure that every character literal appears as a single token.", "labels": [], "entities": []}, {"text": "The regular expression data has a vocabulary of 1015 rules and a total of 1986 distinct descriptions.", "labels": [], "entities": []}, {"text": "Descriptions are on average 12.3 words in length but as long as 46 words in some cases.", "labels": [], "entities": []}, {"text": "Navigation The data used was obtained from.", "labels": [], "entities": []}, {"text": "We created our own variant of the dataset containing collections of related tasks.", "labels": [], "entities": []}, {"text": "Beginning with the \"local\" tasks in the dataset, we generated alternative goal positions at fixed offsets from landmarks as described in the main section of this paper.", "labels": [], "entities": []}, {"text": "Natural-language descriptions were selected for each task collection from the human annotations provided with the dataset.", "labels": [], "entities": []}, {"text": "The vocabulary size is 74 and the number of distinct hints 446.", "labels": [], "entities": []}, {"text": "The original action space for the environment is also modified slightly: rather than simply reaching the goal cell (achieved with reasonably high frequency by a policy that takes random moves), we require the agent to commit to an individual goal cell and end the episode with a special DIG action.", "labels": [], "entities": []}, {"text": "Data augmentation Due to their comparatively small size, a data augmentation scheme) is employed for the regular expression and navigation datasets.", "labels": [], "entities": [{"text": "Data augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7318741083145142}]}, {"text": "In particular, wherever a description contains a recognizable entity name (i.e. a character literal or a landmark name), a description template is extracted.", "labels": [], "entities": []}, {"text": "These templates are then randomly swapped in at training time on other examples with the same high-level semantics.", "labels": [], "entities": []}, {"text": "For example, the description replace first b withe is abstracted to replace first CHAR1 with CHAR2, and can subsequently be specialized to, e.g., replace first c with d.", "labels": [], "entities": []}, {"text": "This templating is easy to implement because we have access to ground-truth structured concept representations at training time.", "labels": [], "entities": []}, {"text": "If these were not available it would be straightforward to employ an automatic template induction system () instead.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1:  Evaluation on image classification. Val  (old) and Val (new) denote subsets of the validation set  that contain respectively previously-used and novel vi- sual concepts. L 3 consistently outperforms alternative  learning methods based on multitask learning, meta- learning, and meta-learning jointly trained to predict  descriptions (Meta+Joint). The last row shows results  when the model is given a ground-truth concept de- scription rather than having to infer it from examples.", "labels": [], "entities": [{"text": "image classification", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7689987123012543}]}, {"text": " Table 2: Results for string editing. The reported num- ber is the percentage of cases in which the predicted  string exactly matches the reference. L 3 is the best per- forming model; using language data for joint training  rather than as a hypothesis space provides little benefit.", "labels": [], "entities": [{"text": "string editing", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7514336407184601}, {"text": "num- ber", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.7853537797927856}]}, {"text": " Table 3.  A few interesting facts stand out. Under the  ordinary evaluation condition (with no ground- truth annotations provided), language-learning  with natural language data is actually better than  language-learning with regular expressions. This  might be because the extra diversity helps the  model determine the relevant axes of variation and  avoid overfitting to individual strings. Allowing  the model to do its own inference is also better  than providing ground-truth natural language de- scriptions, suggesting that it is actually better at  generalizing from the relevant concepts than our  human annotators (who occasionally write things  like I have no idea for the inferred rule). Unsur- prisingly, with ground truth REs (which unlike the  human data are always correct) we can do bet- ter than any of the models that require inference.  Coupling our inference procedure with an oracle  RE evaluator, we essentially recover the synthesis- based approach of", "labels": [], "entities": []}, {"text": " Table 3: Inference and representation experiments for  string editing. Italicized numbers correspond to entries  in Table 2. Allowing the model to use multiple samples  rather than the 1-best decoder output substantially im- proves performance. The full model does better with  inferred natural language descriptions than either regu- lar expressions or ground-truth natural language.", "labels": [], "entities": [{"text": "string editing", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.7415139675140381}]}]}