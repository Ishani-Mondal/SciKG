{"title": [{"text": "Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction", "labels": [], "entities": []}], "abstractContent": [{"text": "Translation-based methods for grammar correction that directly map noisy, ungrammat-ical text to their clean counterparts are able to correct abroad range of errors; however , such techniques are bottlenecked by the need fora large parallel corpus of noisy and clean sentence pairs.", "labels": [], "entities": [{"text": "grammar correction", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7669950127601624}]}, {"text": "In this paper, we consider synthesizing parallel data by noising a clean monolingual corpus.", "labels": [], "entities": []}, {"text": "While most previous approaches introduce perturbations using features computed from local context windows , we instead develop error generation processes using a neural sequence transduc-tion model trained to translate clean examples to their noisy counterparts.", "labels": [], "entities": []}, {"text": "Given a corpus of clean examples, we propose beam search noising procedures to synthesize additional noisy examples that human evalua-tors were nearly unable to discriminate from nonsynthesized examples.", "labels": [], "entities": []}, {"text": "Surprisingly, when trained on additional data synthesized using our best-performing noising scheme, our model approaches the same performance as when trained on additional nonsynthesized data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Correcting noisy, ungrammatical text remains a challenging task in natural language processing.", "labels": [], "entities": []}, {"text": "Ideally, given some piece of writing, an error correction system would be able to fix minor typographical errors, as well as grammatical errors that involve longer dependencies such as nonidiomatic phrasing or errors in subject-verb agreement.", "labels": [], "entities": []}, {"text": "Existing methods, however, are often only able to correct highly local errors, such as spelling errors or errors involving articles or prepositions.", "labels": [], "entities": []}, {"text": "Classifier-based approaches to error correction are limited in their ability to capture abroad range of error types ().", "labels": [], "entities": [{"text": "error correction", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7494403719902039}]}, {"text": "Machine translation-based approaches-that instead trans- Denoise \"New Orleans\" \"NLP\" \"new Orleens\" \"nlp\"", "labels": [], "entities": [{"text": "Machine translation-based", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7547482848167419}, {"text": "New Orleans\" \"NLP\" \"new Orleens\" \"nlp", "start_pos": 66, "end_pos": 103, "type": "DATASET", "confidence": 0.7384812782208124}]}], "datasetContent": [{"text": "To determine the effectiveness of the described noising schemes, we synthesize additional data using each and evaluate the performance of models trainined using the additional data on two benchmarks.", "labels": [], "entities": []}, {"text": "Datasets For training our sequence transduction models, we combine the publicly available English Lang-8 dataset, a parallel corpus collected from a language learner forum, with training data from the CoNLL 2014 challenge).", "labels": [], "entities": [{"text": "English Lang-8 dataset", "start_pos": 90, "end_pos": 112, "type": "DATASET", "confidence": 0.8826364477475485}, {"text": "CoNLL 2014 challenge", "start_pos": 201, "end_pos": 221, "type": "DATASET", "confidence": 0.8598495324452718}]}, {"text": "We refer to this as the \"base\" dataset.", "labels": [], "entities": []}, {"text": "Junczys-Dowmunt and Grundkiewicz (2016) additionally scraped 3.3M pairs of sentences from Lang-8.", "labels": [], "entities": []}, {"text": "Although this expanded dataset, which we call the \"expanded\" dataset, is not typically used when comparing performance on grammar correction benchmarks, we use it instead to compare performance when training on additional synthesized data versus nonsynthesized data.", "labels": [], "entities": [{"text": "grammar correction", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7433287799358368}]}, {"text": "For clean text to be noised, we use the LDC New York Times corpus for 2007, which yields roughly 1 million sentences.", "labels": [], "entities": [{"text": "LDC New York Times corpus for 2007", "start_pos": 40, "end_pos": 74, "type": "DATASET", "confidence": 0.9557809403964451}]}, {"text": "A summary of the data used for training is given in.", "labels": [], "entities": []}, {"text": "We use the CoNLL 2013 evaluation set as our development set in all cases (.", "labels": [], "entities": [{"text": "CoNLL 2013 evaluation set", "start_pos": 11, "end_pos": 36, "type": "DATASET", "confidence": 0.9630544483661652}]}, {"text": "Our test sets are the CoNLL 2014 evaluation set and the JFLEG test set (.", "labels": [], "entities": [{"text": "CoNLL 2014 evaluation set", "start_pos": 22, "end_pos": 47, "type": "DATASET", "confidence": 0.9473881721496582}, {"text": "JFLEG test set", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.966008186340332}]}, {"text": "Because CoNLL 2013 only has a single set of gold annotations while CoNLL 2014 has two, performance metrics tend to be significantly higher on CoNLL 2014.", "labels": [], "entities": [{"text": "CoNLL 2013", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.8801254034042358}, {"text": "CoNLL 2014", "start_pos": 142, "end_pos": 152, "type": "DATASET", "confidence": 0.9168396294116974}]}, {"text": "We report precision, recall, and F 0.5 score, which is standard for the task, as precision is valued over recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9981802701950073}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9996500015258789}, {"text": "F 0.5 score", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9844845334688822}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9991437196731567}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9961715340614319}]}, {"text": "On JFLEG, we report results with the GLEU metric (similar to BLEU) developed for the dataset.", "labels": [], "entities": [{"text": "JFLEG", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8274155855178833}, {"text": "GLEU metric", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9768148362636566}, {"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9987258315086365}]}, {"text": "Training and decoding details All models are trained using stochastic gradient descent with annealing based on validation perplexity on a small held-out subset of the Lang-8 corpus.", "labels": [], "entities": [{"text": "Lang-8 corpus", "start_pos": 167, "end_pos": 180, "type": "DATASET", "confidence": 0.8093796074390411}]}, {"text": "We apply both dropout and weight decay regularization.", "labels": [], "entities": []}, {"text": "We observed that performance tended to saturate after 30 epochs.", "labels": [], "entities": []}, {"text": "Decoding is done with abeam size of 8; in early experiments, we did not observe significant gains with larger beam sizes ().", "labels": [], "entities": []}, {"text": "First, we manually compare each of the different noising methods to evaluate how \"realistic\" the errors introduced are.", "labels": [], "entities": []}, {"text": "This is reminiscent of the generative adversarial network setting, where the generator seeks to produce samples that fool the discriminator.", "labels": [], "entities": [{"text": "generative adversarial network", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.8956624468167623}]}, {"text": "Here the discriminator is a human evaluator who, given the clean sentence Y , tries to determine which of two sentences X and\u02dcYand\u02dc and\u02dcY is the true noisy sentence, and which is the synthesized sentence.", "labels": [], "entities": []}, {"text": "To be clear, 52.1 Junczys-Dowmunt et al.", "labels": [], "entities": []}, {"text": "51.5 Chollampatt and Ng 57.5: Results on the JFLEG test set (we use best hyperparameter settings from CoNLL dev set).", "labels": [], "entities": [{"text": "JFLEG test set", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.9413141210873922}, {"text": "CoNLL dev set", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.8767419457435608}]}, {"text": "GLEU is a variant of BLEU developed for this task; higher is better (.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8834653496742249}, {"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9963997602462769}]}, {"text": "\u2020 Tuned to JFLEG dev set.", "labels": [], "entities": [{"text": "JFLEG dev set", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.9424853324890137}]}, {"text": "we do not train with a discriminator-the beam search noising procedures we proposed alone are intended to yield convincing errors.", "labels": [], "entities": [{"text": "beam search noising", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8155727585156759}]}, {"text": "For each noising scheme, we took 100 (X, Y ) pairs from the development set (500 randomly chosen pairs combined), then generated\u02dcYgenerated\u02dc generated\u02dcY from Y . We then shuffled the examples and the order of X and\u02dcYand\u02dc and\u02dcY such that the identity of X and\u02dcYand\u02dc and\u02dcY as well as the noising scheme used to generate\u02dcYgenerate\u02dc generate\u02dcY were unknown 2 . Given Y , the task for human evaluators is to predict whether X or\u02dcYor\u02dc or\u02dcY was the synthesized example.", "labels": [], "entities": []}, {"text": "For every example, we had two separate evaluators label the sentence they thought was synthesized.", "labels": [], "entities": []}, {"text": "We chose to do this labeling task ourselves (blind to system) since we were familiar with the noising schemes used to generate examples, which should reduce the number of misclassifications.", "labels": [], "entities": []}, {"text": "Results are shown in, and examples of the evaluation task are provided in Table 4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of training corpora.", "labels": [], "entities": []}, {"text": " Table 2: Results on CoNLL 2013 (Dev) and CoNLL 2014 (Test) sets. All results use the \"base\" parallel corpus of  1.3M sentence pairs along with additional synthesized data (totaling 2.3M sentence pairs) except for \"expanded\",  which uses 3.3M nonsynthesized sentence pairs (and no synthesized data).", "labels": [], "entities": [{"text": "CoNLL 2013", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.8539538681507111}, {"text": "CoNLL 2014 (Test) sets", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.8777345617612203}]}, {"text": " Table 3: Results on the JFLEG test set (we use best  hyperparameter settings from CoNLL dev set). GLEU  is a variant of BLEU developed for this task; higher is  better (", "labels": [], "entities": [{"text": "JFLEG test set", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9097669720649719}, {"text": "CoNLL dev set", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.8809000849723816}, {"text": "GLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9790343642234802}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9947252869606018}]}, {"text": " Table 4: Examples of nonsynthesized and synthesized sentences from validation set. Which example (1 or 2) was  synthesized?", "labels": [], "entities": []}]}