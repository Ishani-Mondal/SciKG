{"title": [{"text": "Training Structured Prediction Energy Networks with Indirect Supervision", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces rank-based training of structured prediction energy networks (SPENs).", "labels": [], "entities": []}, {"text": "Our method samples from output structures using gradient descent and minimizes the ranking violation of the sampled structures with respect to a scalar scoring function defined using domain knowledge.", "labels": [], "entities": []}, {"text": "We have successfully trained SPEN for citation field extraction without any labeled data instances , where the only source of supervision is a simple human-written scoring function.", "labels": [], "entities": [{"text": "SPEN", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.7304093241691589}, {"text": "citation field extraction", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.8425188859303793}]}, {"text": "Such scoring functions are often easy to provide; the SPEN then furnishes an efficient structured prediction inference procedure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Structured prediction, or the task of predicting multiple inter-dependent variables, is important in many domains, including computer vision, computational biology and natural language processing.", "labels": [], "entities": [{"text": "Structured prediction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9041449129581451}, {"text": "natural language processing", "start_pos": 168, "end_pos": 195, "type": "TASK", "confidence": 0.6602383355299631}]}, {"text": "For example, in sequence labelling, image segmentation, and parsing we are given input variables x, and must predict output variables y, where the number of possible y values are typically exponential in the number of variables that comprise it.", "labels": [], "entities": [{"text": "sequence labelling", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.6609258651733398}, {"text": "image segmentation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8028738796710968}, {"text": "parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9686012864112854}]}, {"text": "Not only does this sometimes give rise to computational difficulties, it also leads to statistical parameter estimation issues, where learning precise models requires large amounts of labeled training data.", "labels": [], "entities": [{"text": "statistical parameter estimation", "start_pos": 87, "end_pos": 119, "type": "TASK", "confidence": 0.7495477398236593}]}, {"text": "In some cases, unsupervised learning from plentiful unlabeled data may provide helpful outputs).", "labels": [], "entities": []}, {"text": "But usually some form of more direct supervision is required to create a model truly useful to the task at hand.", "labels": [], "entities": []}, {"text": "In the absence of abundant labeled data we may consider alternative forms of supervision.", "labels": [], "entities": []}, {"text": "For example, rather than providing labeled data instances, humans may more easily inject their domain knowledge by providing \"labels on features,\" or \"expectations\" about correct outputs, as in generalized expectation criteria, or by providing constraints, as in posterior regularization ( or constraint driven learning (.", "labels": [], "entities": []}, {"text": "A major weakness of these methods, however, is that at training time inference must be done in the factor graph encompassing the union of the model's factor graph and the expectation dependenciesoften leading to prohibitively expensive inference.", "labels": [], "entities": []}, {"text": "Moreover, these methods cannot learn from nondecomposable domain knowledge, where the domain knowledge is not in a form of a set of labeled features or constraints.", "labels": [], "entities": []}, {"text": "An easy way for humans to express domain knowledge is by writing a simple scalar scoring function that indicates preferences among choices for y given x.", "labels": [], "entities": []}, {"text": "These human-coded functions may, for example, be based on arbitrary rule systems (or even Turing-complete programs) of the sort written by humans to solve problems before machine learning became so wide-spread.", "labels": [], "entities": []}, {"text": "In general, the human written domain knowledge functions are not expected to be perfectmost likely only examining a subset of features and not covering all cases.", "labels": [], "entities": []}, {"text": "Thus we are now faced with two challenges: (1) the domain knowledge functions have limited generalization; (2) the domain knowledge functions provide a ranking, but do not provide an inference (search) procedure.", "labels": [], "entities": []}, {"text": "This paper presents anew training method for structured prediction energy networks (SPENs)) that aims to address both these challenges, yielding efficient inference for structured prediction, trained from human-coded domain knowledge plus unlabeled data, but not requiring any labeled data instances.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 169, "end_pos": 190, "type": "TASK", "confidence": 0.7019584476947784}]}, {"text": "In SPENs, the factor graph that typically represents output variable dependencies is replaced with a deep neural network that takes y and x as input and outputs a scalar energy score, but is able to learn much richer correlations than are typically captured in factor graphs.", "labels": [], "entities": [{"text": "SPENs", "start_pos": 3, "end_pos": 8, "type": "TASK", "confidence": 0.9329725503921509}]}, {"text": "Inference in SPENs is performed by gradient descent in the energy, back-propagated to cause steps in a relaxed y space.", "labels": [], "entities": [{"text": "SPENs", "start_pos": 13, "end_pos": 18, "type": "TASK", "confidence": 0.9524335265159607}]}, {"text": "Whereas previous training procedures for SPENs used labeled data, here we train SPENs from only unlabeled data plus human-coded domain knowledge in the form of a scoring function.", "labels": [], "entities": [{"text": "SPENs", "start_pos": 41, "end_pos": 46, "type": "TASK", "confidence": 0.9625383019447327}]}, {"text": "We do so by building on, which enforces that the rank of two sampled ys according to the trained factor graph is consistent with their rank according to distance to the labeled, true y.", "labels": [], "entities": []}, {"text": "In our training method, pairs of y's are obtained from successive steps of training-time gradient-descent inference on y; when their rank is not consistent with that of the domain knowledge function, we accordingly update the energy network parameters.", "labels": [], "entities": []}, {"text": "We demonstrate our method on a citation field extraction task, for which we learn a neural network (1) that generalizes beyond the original domain knowledge function, and (2) that provides efficient test-time inference by gradient descent.", "labels": [], "entities": [{"text": "citation field extraction task", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.8270715028047562}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of R-SPEN with GE and differ- ent search algorithms in terms of token-level accuracy,  test set average score, and time taken for inference dur- ing test time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9552493095397949}]}]}