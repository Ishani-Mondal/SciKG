{"title": [{"text": "Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds using Paraphrases in a Neural Model", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic interpretation of the relation between the constituents of a noun compound , e.g. olive oil (source) and baby oil (purpose) is an important task for many NLP applications.", "labels": [], "entities": [{"text": "interpretation of the relation between the constituents of a noun compound", "start_pos": 10, "end_pos": 84, "type": "TASK", "confidence": 0.6905270110477101}]}, {"text": "Recent approaches are typically based on either noun-compound representations or paraphrases.", "labels": [], "entities": []}, {"text": "While the former has initially shown promising results , recent work suggests that the success stems from memorizing single pro-totypical words for each relation.", "labels": [], "entities": []}, {"text": "We explore a neural paraphrasing approach that demonstrates superior performance when such memorization is not possible.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic classification of a noun-compound (NC) to the implicit semantic relation that holds between its constituent words is beneficial for applications that require text understanding.", "labels": [], "entities": [{"text": "classification of a noun-compound (NC)", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.7869318212781634}, {"text": "text understanding", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.7102460116147995}]}, {"text": "For instance, a personal assistant asked \"do I have a morning meeting tomorrow?\" should search the calendar for meetings occurring in the morning, while for group meeting it should look for meetings with specific participants.", "labels": [], "entities": []}, {"text": "The NC classification task is a challenging one, as the meaning of an NC is often not easily derivable from the meaning of its constituent words.", "labels": [], "entities": [{"text": "NC classification task", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8762162327766418}]}, {"text": "Previous work on the task falls into two main approaches.", "labels": [], "entities": []}, {"text": "The first maps NCs to paraphrases that express the relation between the constituent words (e.g., such as mapping coffee cup and garbage dump to the pattern [w 1 ] CONTAINS [w 2 ].", "labels": [], "entities": []}, {"text": "The second approach computes a representation for NCs from the distributional representation of their individual constituents.", "labels": [], "entities": []}, {"text": "While this approach * Work done during an internship at Google.", "labels": [], "entities": []}, {"text": "yielded promising results, recently, showed that similar performance is achieved by representing the NC as a concatenation of its constituent embeddings, and attributed it to the lexical memorization phenomenon (.", "labels": [], "entities": []}, {"text": "In this paper we apply lessons learned from the parallel task of semantic relation classification.", "labels": [], "entities": [{"text": "semantic relation classification", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.8120733102162679}]}, {"text": "We adapt HypeNET () to the NC classification task, using their path embeddings to represent paraphrases and combining with distributional information.", "labels": [], "entities": [{"text": "NC classification task", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.8207625945409139}]}, {"text": "We experiment with various evaluation settings, including settings that make lexical memorization impossible.", "labels": [], "entities": []}, {"text": "In these settings, the integrated method performs better than the baselines.", "labels": [], "entities": []}, {"text": "Even so, the performance is mediocre for all methods, suggesting that the task is difficult and warrants further investigation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We follow and evaluate on the Tratz (2011) dataset, with 19,158 instances and two levels of labels: fine-grained (Tratz-fine, 37 relations) and coarse-grained (Tratz-coarse, 12 relations).", "labels": [], "entities": [{"text": "Tratz (2011) dataset", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.6089952945709228}]}, {"text": "We report results on both versions.", "labels": [], "entities": []}, {"text": "See Tratz (2011) for the list of relations.", "labels": [], "entities": []}, {"text": "showed that a classifier based only on v w 1 and v w 2 performs on par with compound representations, and that the success comes from lexical memorization (: memorizing the majority label of single words in particular slots of the compound (e.g. TOPIC for travel guide, fishing guide, etc.).", "labels": [], "entities": []}, {"text": "This memorization paints a skewed picture of the stateof-the-art performance on this difficult task.", "labels": [], "entities": []}, {"text": "To better test this hypothesis, we evaluate on 4 different splits of the datasets to train, test, and validation sets: (1) random, in a 75:20:5 ratio, (2) lexical-full, in which the train, test, and validation  sets each consists of a distinct vocabulary.", "labels": [], "entities": []}, {"text": "The split was suggested by, and it randomly assigns words to distinct sets, such that for example, including travel guide in the train set promises that fishing guide would not be included in the test set, and the models do not benefit from memorizing that the head guide is always annotated as TOPIC.", "labels": [], "entities": [{"text": "TOPIC", "start_pos": 295, "end_pos": 300, "type": "METRIC", "confidence": 0.6287803053855896}]}, {"text": "Given that the split discards many NCs, we experimented with two additional splits: (3) lexical-mod split, in which thew 1 words are unique in each set, and (4) lexical-head split, in which thew 2 words are unique in each set.", "labels": [], "entities": []}, {"text": "Table 2 displays the sizes of each split.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: All methods' performance (F 1 ) on the various splits: best freq: best performing frequency baseline (head  / modifier), 3 best comp: best model from Dima (2016).", "labels": [], "entities": [{"text": "F 1 )", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9774140318234762}]}, {"text": " Table 2: Number of instances in each dataset split.", "labels": [], "entities": []}]}