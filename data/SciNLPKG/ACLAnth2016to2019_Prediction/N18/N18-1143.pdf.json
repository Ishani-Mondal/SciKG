{"title": [{"text": "Supervised and Unsupervised Transfer Learning for Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8052140474319458}]}], "abstractContent": [{"text": "Although transfer learning has been shown to be successful for tasks like object and speech recognition, its applicability to question answering (QA) has yet to be well-studied.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.9668833017349243}, {"text": "object and speech recognition", "start_pos": 74, "end_pos": 103, "type": "TASK", "confidence": 0.629206195473671}, {"text": "question answering (QA)", "start_pos": 126, "end_pos": 149, "type": "TASK", "confidence": 0.8443059682846069}]}, {"text": "In this paper, we conduct extensive experiments to investigate the transferability of knowledge learned from a source QA dataset to a target dataset using two QA models.", "labels": [], "entities": []}, {"text": "The performance of both models on a TOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) is significantly improved via a simple transfer learning technique from MovieQA (Tapaswi et al., 2016).", "labels": [], "entities": [{"text": "MovieQA", "start_pos": 201, "end_pos": 208, "type": "DATASET", "confidence": 0.952975332736969}]}, {"text": "In particular , one of the models achieves the state-of-the-art on all target datasets; for the TOEFL listening comprehension test, it outperforms the previous best model by 7%.", "labels": [], "entities": [{"text": "TOEFL listening comprehension", "start_pos": 96, "end_pos": 125, "type": "TASK", "confidence": 0.5722396671772003}]}, {"text": "Finally, we show that transfer learning is helpful even in unsupervised scenarios when correct answers for target QA dataset examples are not available.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.9588172733783722}]}], "introductionContent": [], "datasetContent": [{"text": "We used MovieQA ( as the source MCQA dataset, and TOEFL listening comprehension test) and MCTest () as two separate target datasets.", "labels": [], "entities": [{"text": "MovieQA", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.9262508153915405}, {"text": "MCQA dataset", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.8495720028877258}, {"text": "TOEFL listening comprehension test", "start_pos": 50, "end_pos": 84, "type": "METRIC", "confidence": 0.7629212290048599}]}, {"text": "Examples of the three datasets are shown in.", "labels": [], "entities": []}, {"text": "MovieQA is a dataset that aims to evaluate automatic story comprehension from both video and text.", "labels": [], "entities": [{"text": "MovieQA", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8741106986999512}]}, {"text": "The dataset provides multiple sources of information such as plot synopses, scripts, subtitles, and video clips that can be used to infer answers.", "labels": [], "entities": []}, {"text": "We only used the plot synopses of the dataset, so our setting is the same as pure textual MCQA.", "labels": [], "entities": []}, {"text": "The dataset contains 9,848/1,958 train/dev examples; each question comes with a set of five possible answer choices with only one correct answer.", "labels": [], "entities": []}, {"text": "TOEFL listening comprehension testis a recently published, very challenging MCQA dataset that contains 717/124/122 train/dev/test examples.", "labels": [], "entities": [{"text": "MCQA dataset", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.9646338820457458}]}, {"text": "It aims to test knowledge and skills of academic English for global English learners whose native languages are not English.", "labels": [], "entities": []}, {"text": "There are only four answer choices for each question.", "labels": [], "entities": []}, {"text": "The stories in this dataset are in audio form.", "labels": [], "entities": []}, {"text": "Each story comes with two transcripts: manual and ASR transcriptions, where the latter is obtained by running the CMU Sphinx recognizer () on the original audio files.", "labels": [], "entities": [{"text": "ASR transcriptions", "start_pos": 50, "end_pos": 68, "type": "METRIC", "confidence": 0.8092764317989349}]}, {"text": "We use TOEFL-manual and TOEFL-ASR to denote the two versions, respectively.", "labels": [], "entities": [{"text": "TOEFL-manual", "start_pos": 7, "end_pos": 19, "type": "METRIC", "confidence": 0.7344244718551636}, {"text": "TOEFL-ASR", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.5533360838890076}]}, {"text": "We highlight that the questions in this dataset are not easy because most of the answers cannot be found by simply matching the question and the choices without understanding the story.", "labels": [], "entities": []}, {"text": "For example, there are questions regarding the gist of the story or the conclusion for the conversation.", "labels": [], "entities": []}, {"text": "MCTest is a collection of 660 elementary-level children's stories.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9538818001747131}]}, {"text": "Each question comes with a set of four answer choices.", "labels": [], "entities": []}, {"text": "There are two variants in this dataset: MC160 and MC500.", "labels": [], "entities": [{"text": "MC160", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.9368618726730347}, {"text": "MC500", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.8846434950828552}]}, {"text": "The former contains 280/120/240 train/dev/test examples, while the latter contains 1,200/200/600 train/dev/test examples and is considered more difficult.", "labels": [], "entities": []}, {"text": "The two chosen target datasets are challenging because the stories and questions are complicated, and only small training sets are available.", "labels": [], "entities": []}, {"text": "Therefore, it is difficult to train statistical models on only their training sets because the small size limits the number of parameters in the models, and prevents learning any complex language concepts simultaneously with the capacity to answer questions.", "labels": [], "entities": []}, {"text": "We demonstrate that we can effectively overcome these difficulties via transfer learning in Section 5.", "labels": [], "entities": []}, {"text": "From(a) and(b) we can observe that without ground truth in the target dataset for supervised fine-tuning, transfer learning from a source dataset can still improve the performance through a simple iterative self-labeling mechanism.", "labels": [], "entities": []}, {"text": "For TOEFL-manual and TOEFL-ASR, QACNN achieves the highest testing accuracy at Epoch 7 and 8, outperforming its counterpart without fine-tuning by approximately 4% and 5%, respectively.", "labels": [], "entities": [{"text": "TOEFL-manual", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7350794672966003}, {"text": "TOEFL-ASR", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.8563913106918335}, {"text": "QACNN", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.7573694586753845}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9802737236022949}]}, {"text": "For MC160 and MC500, the QACNN achieves the peak at Epoch 3 and 6, outperforming its counterpart without fine-tuning by about 2% and 6%, respectively.", "labels": [], "entities": [{"text": "QACNN", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9307559132575989}, {"text": "Epoch", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9665723443031311}]}, {"text": "The results also show that the performance of unsupervised transfer learning is still worse than supervised transfer learning, which is not surprising, but the effectiveness of unsupervised transfer learning when no ground truth labels are provided is validated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of transfer learning on the target  datasets. The number in the parenthesis indicates  the accuracy increased via transfer learning (com- pared to rows (a) and (g)). The best performance  for each target dataset is marked in bold. We also  include the results of the previous best performing  models on the target datasets in the last three rows.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8868966400623322}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9992351531982422}]}, {"text": " Table 3: Results of varying sizes of the target  datasets used for fine-tuning QACNN. The num- ber in the parenthesis indicates the accuracy in- creases from using the previous percentage for  fine-tuning to the current percentage.", "labels": [], "entities": [{"text": "fine-tuning QACNN", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.525501161813736}, {"text": "num- ber", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.8192885120709738}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9990639090538025}]}, {"text": " Table 4: Results of varying sizes of the MovieQA  used for pre-training QACNN. The number in the  parenthesis indicates the accuracy increases from  using the previous percentage for pre-training to  the current percentage.", "labels": [], "entities": [{"text": "MovieQA", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9526960849761963}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9993645548820496}]}]}