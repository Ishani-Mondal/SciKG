{"title": [{"text": "Multi-Task Learning for Argumentation Mining in Low-Resource Settings", "labels": [], "entities": [{"text": "Argumentation Mining", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8277409374713898}]}], "abstractContent": [{"text": "We investigate whether and where multi-task learning (MTL) can improve performance on NLP problems related to argu-mentation mining (AM), in particular argument component identification.", "labels": [], "entities": [{"text": "argu-mentation mining (AM)", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.8210942327976227}, {"text": "argument component identification", "start_pos": 152, "end_pos": 185, "type": "TASK", "confidence": 0.6420665979385376}]}, {"text": "Our results show that MTL performs particularly well (and better than single-task learning) when little training data is available for the main task, a common scenario in AM.", "labels": [], "entities": [{"text": "MTL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9606517553329468}]}, {"text": "Our findings challenge previous assumptions that conceptualizations across AM datasets are divergent and that MTL is difficult for semantic or higher-level tasks.", "labels": [], "entities": [{"text": "MTL", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9483331441879272}]}], "introductionContent": [{"text": "Computational argumentation mining (AM) deals with the automatic identification of argumentative structures within natural language.", "labels": [], "entities": [{"text": "Computational argumentation mining (AM)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8651590446631113}, {"text": "automatic identification of argumentative structures within natural language", "start_pos": 55, "end_pos": 131, "type": "TASK", "confidence": 0.7503786906599998}]}, {"text": "This can be beneficial in many applications such as summarizing arguments in texts to improve comprehensibility for end-users, or information retrieval and extraction (.", "labels": [], "entities": [{"text": "summarizing arguments in texts", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.8977651596069336}, {"text": "information retrieval and extraction", "start_pos": 130, "end_pos": 166, "type": "TASK", "confidence": 0.7602118998765945}]}, {"text": "A common task is to segment a text into argumentative and nonargumentative components and identify the type of argumentative components.", "labels": [], "entities": []}, {"text": "As an illustration, consider the (simplified) example from : \"Since [it killed many marine lives] [tourism has threatened nature] .\" Here, the non-argumentative token \"Since\" is followed by two argumentative components: a premise that supports a claim.", "labels": [], "entities": []}, {"text": "Argumentation is highly subjective and conceptualized in different ways ().", "labels": [], "entities": []}, {"text": "On the one hand, this implies that creating reliable ground-truth datasets for AM is costly, as it requires trained annotators.", "labels": [], "entities": [{"text": "AM", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9709356427192688}]}, {"text": "However, even trained annotators have problems identifying and classifying arguments reliably in texts . To tackle AM in anew domain or develop new AM tasks, it may thus not be possible to create large datasets as required by most state-of-the-art machine learning approaches.", "labels": [], "entities": []}, {"text": "On the other hand, the different conceptualizations of argumentation resulted in AM corpora with different argument component types, with very little conceptual overlap between some of these corpora . This distinguishes AM from more established NLP tasks like discourse parsing and makes it particularly challenging.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 260, "end_pos": 277, "type": "TASK", "confidence": 0.7131965160369873}]}, {"text": "Therefore, a natural question is how to handle new AM datasets in anew domain and with sparse data.", "labels": [], "entities": []}, {"text": "Here, we investigate how existing AM datasets from different domains and with different conceptualizations of arguments can be leveraged to tackle these challenges.", "labels": [], "entities": []}, {"text": "More precisely, we study whether conceptually diverse AM datasets from different domains can help deal with new AM datasets when data is limited.", "labels": [], "entities": []}, {"text": "A promising direction to incorporate existing datasets as \"auxiliary knowledge\" is by means of multi-task learning (MTL), a paradigm that dates back to the 1990s, but has only recently gained large attention).", "labels": [], "entities": [{"text": "multi-task learning (MTL)", "start_pos": 95, "end_pos": 120, "type": "TASK", "confidence": 0.6843183994293213}]}, {"text": "The idea behind MTL is to learn several tasks jointly, similarly to human learning, so that tasks serve as mutual sources of \"inductive bias\" for one another.", "labels": [], "entities": [{"text": "MTL", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9758092761039734}]}, {"text": "MTL has been reported particularly beneficial when tasks exhibit \"natural hierarchies\"  or when the amount of training data for the main task is sparse (, where the auxiliary tasks may act as regularizers to prevent overfitting (.", "labels": [], "entities": [{"text": "MTL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9628419280052185}]}, {"text": "The latter is precisely the scenario most relevant to us.", "labels": [], "entities": []}, {"text": "In this paper, we (1) investigate to which de-gree training a system to solve several conceptually different AM tasks jointly improves performance over learning in isolation,  MTL has been applied in many different settings. and use data from different domains as different tasks and thereby improve historical spelling normalization and Chinese word segmentation and NER, respectively.", "labels": [], "entities": [{"text": "historical spelling normalization", "start_pos": 300, "end_pos": 333, "type": "TASK", "confidence": 0.6127217908700308}, {"text": "Chinese word segmentation", "start_pos": 338, "end_pos": 363, "type": "TASK", "confidence": 0.5883212586243948}]}, {"text": "apply an MTL setup to POS tagging across 22 different languages, where the auxiliary task is to predict token frequency.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.8693024814128876}]}, {"text": "explore sub-tasks (such as component identification) of a complex AM tagging problem (including relations between components) as auxiliaries and find that this improves performances.", "labels": [], "entities": [{"text": "component identification", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.7840356230735779}, {"text": "AM tagging problem", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9068590005238851}]}, {"text": "However, they stay within one single domain and dataset, and thus their approach does not address the question how new AM datasets with sparse data can profit from existing AM resources.", "labels": [], "entities": []}, {"text": "Conceptually closest to our work, leverage data from different languages as well as different domains in order to improve discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.6917541325092316}]}, {"text": "While MTL was shown effective for syntactic tasks under certain conditions , find that MTL does not improve performances in four The code and data used for our experiments are available from https://github.com/UKPLab/ naacl18-multitask_argument_mining.", "labels": [], "entities": [{"text": "UKPLab", "start_pos": 210, "end_pos": 216, "type": "DATASET", "confidence": 0.9851459264755249}]}, {"text": "out of five semantic (i.e., higher level) tasks that they study.", "labels": [], "entities": []}, {"text": "We are among the first to perform a structured investigation of MTL for higher-level pragmatic tasks, which are thought to be much more challenging than syntactic tasks, and in particular, explore it for AM in cross-domain settings.", "labels": [], "entities": [{"text": "MTL", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9696499109268188}, {"text": "AM", "start_pos": 204, "end_pos": 206, "type": "TASK", "confidence": 0.9551233649253845}]}], "datasetContent": [{"text": "Data We experiment with six datasets for argument component identification, i.e. the token-level segmentation and typing of components.", "labels": [], "entities": [{"text": "argument component identification", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.6646897693475088}]}, {"text": "These datasets are all of different sizes, have different average text lengths, and different argument component types and label distributions, as summarized in.", "labels": [], "entities": []}, {"text": "We only choose datasets containing both argumentative components and nonargumentative text.", "labels": [], "entities": []}, {"text": "Claims are available in five of six datasets, and all datasets have premises (resp.", "labels": [], "entities": []}, {"text": "\"justification\"), although it is unclear how large the conceptual overlap is across datasets.", "labels": [], "entities": []}, {"text": "Further component types are idiosyncractic.", "labels": [], "entities": []}, {"text": "hotel has the largest number of types, namely, six.", "labels": [], "entities": []}, {"text": "Most datasets also come with further information, e.g. relations between argument components, which are not considered here.", "labels": [], "entities": []}, {"text": "Approach Due to the difference in annotations used in the different datasets, we consider each dataset as a separate AM task.", "labels": [], "entities": [{"text": "Approach", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9856888055801392}]}, {"text": "We treat all of them as sequence tagging problems, where predicting BIO tags (argument segmentation) and argument component types (component classification) is framed as a joint task.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7118510454893112}, {"text": "predicting BIO tags (argument segmentation) and argument component types (component classification)", "start_pos": 57, "end_pos": 156, "type": "TASK", "confidence": 0.7363995790481568}]}, {"text": "This is achieved through token-level BIO tagging with the label set {O} \u222a {B, I} \u00d7 T , where T is a dataset specific set of argument component types, e.g. T = {claim, premise, . .", "labels": [], "entities": [{"text": "BIO tagging", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.6761524379253387}]}, {"text": "Thus, the overall number of tags in each dataset is twice the number of non-\"O\" component types plus one (2 \u00b7 |T | + 1).", "labels": [], "entities": []}, {"text": "We use the state-of-the-art framework by Reimers and Gurevych (2017) for both single-task learning (STL) and MTL.", "labels": [], "entities": [{"text": "MTL", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9551778435707092}]}, {"text": "It employs a bidirectional LSTM (BILSTM) model with a CRF layer over individual LSTM outputs to account for label dependencies.", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9033569693565369}]}, {"text": "We use nadam as optimizer.", "labels": [], "entities": []}, {"text": "For MTL, the recurrent layers of the deep BILSTM are shared by all tasks, with a separate CRF layer for each task.", "labels": [], "entities": [{"text": "MTL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.961908221244812}, {"text": "BILSTM", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.6903582215309143}]}, {"text": "All tasks terminate at the same level.", "labels": [], "entities": []}, {"text": "The main task determines the number of mini-batches used for training, i.e. in every iteration the main task is trained on all its mini-batches and all other   hotel reviews 194 185 C (39), P (22), major C (7), implicit P (8), background (7), recommendation (5), O (12)  web discourse 340 250 C (4), P (25), backing (13), rebuttal (3), refutation (1), O (54)  news comments 1927 108 P (53), O (47)  persuasive essays 402 366 C (15), P (45), major C (8), O (32): AM datasets: C -claim, P -premise, O -non-argumentative; numbers in parentheses are label distributions in %; 'tokens' is the average in each document.", "labels": [], "entities": [{"text": "AM datasets", "start_pos": 462, "end_pos": 473, "type": "DATASET", "confidence": 0.6982246488332748}]}, {"text": "(auxiliary) tasks are trained on the same number of (randomly drawn) mini-batches.", "labels": [], "entities": []}, {"text": "To simulate data sparsity, we experiment with different sizes of training data for the main task.", "labels": [], "entities": []}, {"text": "We first draw a \"sparse\" training set of 21K tokens 2 for each of the six AM datasets and a dev set of 9K to simulate a sparse scenario with 30K given tokens.", "labels": [], "entities": [{"text": "AM datasets", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.6941318362951279}]}, {"text": "The remaining data of each specific dataset is used as its test set (at least 5K tokens).", "labels": [], "entities": []}, {"text": "We then randomly draw a subset of the training data to create three more 'sparsity scenarios' with 12K, 6K, and 1K tokens, respectively.", "labels": [], "entities": []}, {"text": "Both dev and test set remain the same as in the 21K scenario.", "labels": [], "entities": []}, {"text": "It is worth emphasizing how little data is used in the 1K scenario-only 1-10 documents (or roughly 50 sentences).", "labels": [], "entities": []}, {"text": "We train a separate STL system for each of the six datasets and each of the four sparsity scenarios.", "labels": [], "entities": []}, {"text": "In the MTL setup, the respective sparsity data is used as the main task, all other (auxiliary) AM datasets, each considered a separate task, are trained on all their available data.", "labels": [], "entities": [{"text": "MTL", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9160735011100769}]}, {"text": "To measure the effect of MTL as opposed to a mere increase of training data, we furthermore train for each main task (i.e. each dataset and sparsity scenario) an STL system on the union of (training data of) main and auxiliary task, and evaluate it on the main task's test data.", "labels": [], "entities": [{"text": "MTL", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.835996687412262}]}, {"text": "Hyperparameter optimization For each sparsity scenario and dataset we train 50 STL/MTL systems using GloVe embeddings () and 50 using the embeddings by.", "labels": [], "entities": []}, {"text": "For each run we randomly choose a layout with either one hidden layer of h \u2208 {50, 100, 150} units or two layers of 100 units as well as variational dropout rates between 0.2 and 0.5 for the input layer and for the hidden units.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: AM datasets: C -claim, P -premise, O -non-argumentative; numbers in parentheses are label  distributions in %; 'tokens' is the average in each document.", "labels": [], "entities": [{"text": "AM datasets", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.7700276672840118}]}, {"text": " Table 2: Macro-F1 for AM component identifi- cation, comparing MTL, STL (significant differ- ences in bold with p < 0.01, p < 0.05 if * using  Mann-Whitney U Test) and union baseline (BL).", "labels": [], "entities": [{"text": "union baseline (BL)", "start_pos": 169, "end_pos": 188, "type": "METRIC", "confidence": 0.9210829615592957}]}]}