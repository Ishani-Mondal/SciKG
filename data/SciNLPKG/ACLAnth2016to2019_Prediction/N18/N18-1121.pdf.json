{"title": [{"text": "Handling Homographs in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.676062802473704}]}], "abstractContent": [{"text": "Homographs, words with different meanings but the same surface form, have long caused difficulty for machine translation systems, as it is difficult to select the correct translation based on the context.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7089148759841919}]}, {"text": "However, with the advent of neural machine translation (NMT) systems , which can theoretically take into account global sentential context, one may hypothesize that this problem has been alleviated.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8480654160181681}]}, {"text": "In this paper, we first provide empirical evidence that existing NMT systems in fact still have significant problems in properly translating ambiguous words.", "labels": [], "entities": [{"text": "translating ambiguous words", "start_pos": 129, "end_pos": 156, "type": "TASK", "confidence": 0.8500033418337504}]}, {"text": "We then proceed to describe methods, inspired by the word sense disambiguation literature, that model the context of the input word with context-aware word embeddings that help to differentiate the word sense before feeding it into the en-coder.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.6219324072202047}]}, {"text": "Experiments on three language pairs demonstrate that such models improve the performance of NMT systems both in terms of BLEU score and in the accuracy of translating homographs.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9850758910179138}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9986373782157898}]}], "introductionContent": [{"text": "Neural machine translation (NMT;;, \u00a72), a method for MT that performs translation in an end-toend fashion using neural networks, is quickly becoming the de-facto standard in MT applications due to its impressive empirical results.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6488209962844849}, {"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.995202898979187}, {"text": "MT", "start_pos": 174, "end_pos": 176, "type": "TASK", "confidence": 0.984982430934906}]}, {"text": "One of the drivers behind these results is the ability of NMT to capture long-distance context using recurrent neural networks in both the encoder, which takes the input and turns it into a continuous-space representation, and the decoder, which tracks the * * Equal contribution.", "labels": [], "entities": []}, {"text": "\u2020 \u2020 Now at Snap Inc.", "labels": [], "entities": [{"text": "Snap Inc", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.7688101530075073}]}, {"text": "\u2021 \u2021 Now at Google 1 Code for our translation models is available at https://goo.gl/oaiqoT Figure 1: Homographs where the baseline system makes mistakes (red words) but our proposed system incorporating a more direct representation of context achieves the correct translation (blue words).", "labels": [], "entities": []}, {"text": "Definitions of corresponding blue and red words are in parenthesis.", "labels": [], "entities": []}, {"text": "target-sentence state, deciding which word to output next.", "labels": [], "entities": []}, {"text": "As a result of this ability to capture long-distance dependencies, NMT has achieved great improvements in a number of areas that have bedeviled traditional methods such as phrasebased MT (PBMT;), including agreement and long-distance syntactic dependencies (.", "labels": [], "entities": [{"text": "phrasebased MT", "start_pos": 172, "end_pos": 186, "type": "TASK", "confidence": 0.5454672425985336}]}, {"text": "One other phenomenon that was poorly handled by PBMT was homographs -words that have the same surface form but multiple senses.", "labels": [], "entities": []}, {"text": "As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense or phrase-sense) disambiguation to improve their handling of these phenomena.", "labels": [], "entities": []}, {"text": "Thus, we may wonder: do NMT systems suffer from the same problems when translating homographs?", "labels": [], "entities": []}, {"text": "Or are the recurrent nets applied in the encoding step, and the strong language model in the decoding step enough to alleviate all problems of word sense ambiguity?", "labels": [], "entities": []}, {"text": "In \u00a73 we first attempt to answer this question quantitatively by examining the word translation accuracy of a baseline NMT system as a function of the number of senses that each word has.", "labels": [], "entities": [{"text": "word translation", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.6936534643173218}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.7601736187934875}]}, {"text": "Results demonstrate that standard NMT systems make a significant number of errors on homographs, a few of which are shown in.", "labels": [], "entities": []}, {"text": "With this result in hand, we propose a method for more directly capturing contextual information that may help disambiguate difficult-to-translate homographs.", "labels": [], "entities": []}, {"text": "Specifically, we learn from neural models for word sense disambiguation, examining three methods inspired by this literature ( \u00a74).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.7144370675086975}]}, {"text": "In order to incorporate this information into NMT, we examine two methods: gating the word-embeddings in the model (similarly to), and concatenating the context-aware representation to the word embedding ( \u00a75).", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of our method, we compare our context-aware models with a strong baseline () on the EnglishGerman, English-French, and English-Chinese WMT dataset.", "labels": [], "entities": [{"text": "EnglishGerman", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.9552326798439026}, {"text": "English-Chinese WMT dataset", "start_pos": 149, "end_pos": 176, "type": "DATASET", "confidence": 0.6325746675332388}]}, {"text": "We show that our proposed model outperforms the baseline in the overall BLEU score across three different language pairs.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9713179171085358}]}, {"text": "Quantitative analysis demonstrates that our model performs better on translating homographs.", "labels": [], "entities": []}, {"text": "Lastly, we show sample translations of the baseline system and our proposed model.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare our proposed contextaware NMT models with baseline models on English-German dataset.", "labels": [], "entities": [{"text": "English-German dataset", "start_pos": 89, "end_pos": 111, "type": "DATASET", "confidence": 0.7417067885398865}]}, {"text": "Our baseline models are encoder-decoder models using global-general attention and input feeding on the decoder side as described in \u00a72, varying the settings on the encoder side.", "labels": [], "entities": []}, {"text": "Our proposed model builds upon baseline models by concatenating or gating different types of context vectors.", "labels": [], "entities": []}, {"text": "We use LSTM for encoder, decoder, and context network.", "labels": [], "entities": []}, {"text": "The decoder is the same across baseline models and proposed models, having 500 hidden units.", "labels": [], "entities": []}, {"text": "During testing, we use beam search with abeam size of 5.", "labels": [], "entities": []}, {"text": "The dimension for input word embedding dis set to 500 across encoder, decoder, and context network.", "labels": [], "entities": []}, {"text": "Settings for three different baselines are listed below.", "labels": [], "entities": []}, {"text": "Baseline 1: An uni-directional LSTM with 500 hidden units and 2 layers of stacking LSTM.", "labels": [], "entities": []}, {"text": "Baseline 2: A bi-directional LSTM with 250 hidden units and 2 layers of stacking LSTM.", "labels": [], "entities": []}, {"text": "Each state is summarized by concatenating the hidden states of forward and backward encoder into 500 hidden units.", "labels": [], "entities": []}, {"text": "The context network uses the below settings.", "labels": [], "entities": []}, {"text": "We define overfitting to be when perplexity on the dev set of the current epoch is worse than the previous epoch.", "labels": [], "entities": []}, {"text": "NBOW: Average word embedding of the input sequence.", "labels": [], "entities": [{"text": "NBOW", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5298604369163513}]}, {"text": "BiLSTM: A single-layer bi-directional LSTM with 250 hidden units.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7428075671195984}]}, {"text": "The context vector is represented by concatenating the hidden states of forward and backward LSTM into a 500 dimensional vector.", "labels": [], "entities": []}, {"text": "HoLSTM: A single-layer uni-directional LSTM with 500 hidden units.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The first thing we observe is that the best context-aware model (results in bold in the table) achieved improvements of around 0.7 BLEU on both WMT14 and WMT15 over the respective baseline methods with 2 layers.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9992282390594482}, {"text": "WMT14", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.9462403655052185}, {"text": "WMT15", "start_pos": 154, "end_pos": 159, "type": "DATASET", "confidence": 0.8884696364402771}]}, {"text": "This is in contrast to simply using a 3-layer network, which actually degrades performance, perhaps due to the vanishing gradients problem it increases the difficulty in learning.", "labels": [], "entities": []}, {"text": "Next, comparing different methods for incorporating context, we can see that BiLSTM performs best across all settings.", "labels": [], "entities": []}, {"text": "HoLSTM performs slightly better than NBOW, and NBOW obviously suffers from having the same context vector for every word in the input sequence failing to outperform the corresponding baselines.", "labels": [], "entities": []}, {"text": "Comparing the two integration methods that incorporate context into word embeddings.", "labels": [], "entities": []}, {"text": "Both methods improve over the baseline with BiLSTM as the context network.", "labels": [], "entities": []}, {"text": "Concatenating the context vector and the word embedding performed better than gating.", "labels": [], "entities": []}, {"text": "Finally, in contrast to the baseline, it is not obvious whether using uni-directional or bi-directional as the encoder is better for our proposed models, particularly when BiLSTM is used for calculating the context network.", "labels": [], "entities": []}, {"text": "This is likely due to the fact that bi-directional information is already captured by the context network, and may not be necessary in the encoder itself.", "labels": [], "entities": []}, {"text": "We further compared the two systems on two different languages, French and Chinese.", "labels": [], "entities": []}, {"text": "We achieved 0.5-0.8 BLEU improvement, showing our proposed models are stable and consistent across different language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9986222982406616}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "To show that our 3-layer models are properly trained, we ran a 3-layer bidirectional encoder with residual networks on En-Fr and got 27.45 for WMT13 and 30.60 for WMT14, which is similarly lower than the two layer result.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 143, "end_pos": 148, "type": "DATASET", "confidence": 0.9070209860801697}, {"text": "WMT14", "start_pos": 163, "end_pos": 168, "type": "DATASET", "confidence": 0.9305696487426758}]}, {"text": "It should be noted that previous work such as: Translation results for homographs and all words in our NMT vocabulary.", "labels": [], "entities": [{"text": "Translation", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.9587956070899963}]}, {"text": "We compare scores for baseline and our best proposed model on three different language pairs.", "labels": [], "entities": []}, {"text": "We performed bootstrap resampling for 1000 times: our best model improved more on homographs than all words in terms of either f1, precision, or recall with p < 0.05, indicating statistical significance across all measures.", "labels": [], "entities": [{"text": "bootstrap resampling", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7699066996574402}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9986816048622131}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9991222023963928}]}, {"text": "also noted that the gains for encoders beyond two layers is minimal.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WMT'14, WMT'15 English-German results -We show perplexities (Ppl) on development set and  tokenized BLEU on WMT'14 and WMT'15 test set of various NMT systems. We also show different settings  for different systems. \u2192 represents uni-directional, and \u2194 represents bi-directional. We also highlight the best  baseline model and the best proposed model in bold. The best baseline model will be referred as base or baseline  and the best proposed model will referred to as best for further experiments.", "labels": [], "entities": [{"text": "WMT'14", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.9403209686279297}, {"text": "WMT'15", "start_pos": 18, "end_pos": 24, "type": "DATASET", "confidence": 0.8721929788589478}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9760631918907166}, {"text": "WMT'14", "start_pos": 118, "end_pos": 124, "type": "DATASET", "confidence": 0.9586140513420105}, {"text": "WMT'15 test set", "start_pos": 129, "end_pos": 144, "type": "DATASET", "confidence": 0.960886021455129}]}, {"text": " Table 3: Translation results for homographs and all words in our NMT vocabulary. We compare scores for baseline  and our best proposed model on three different language pairs. Improvements are in italic. We performed bootstrap  resampling for 1000 times: our best model improved more on homographs than all words in terms of either f1,  precision, or recall with p < 0.05, indicating statistical significance across all measures.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9569949507713318}, {"text": "NMT vocabulary", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.8172181844711304}, {"text": "precision", "start_pos": 338, "end_pos": 347, "type": "METRIC", "confidence": 0.9991361498832703}, {"text": "recall", "start_pos": 352, "end_pos": 358, "type": "METRIC", "confidence": 0.9990604519844055}]}]}