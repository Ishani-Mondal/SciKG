{"title": [{"text": "Simple and Effective Semi-Supervised Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.6789792478084564}]}], "abstractContent": [{"text": "Recent success of deep learning models for the task of extractive Question Answering (QA) is hinged on the availability of large annotated corpora.", "labels": [], "entities": [{"text": "extractive Question Answering (QA)", "start_pos": 55, "end_pos": 89, "type": "TASK", "confidence": 0.7824024657408396}]}, {"text": "However, large domain specific annotated corpora are limited and expensive to construct.", "labels": [], "entities": []}, {"text": "In this work, we envision a system where the end user specifies a set of base documents and only a few labelled examples.", "labels": [], "entities": []}, {"text": "Our system exploits the document structure to create cloze-style questions from these base documents ; pre-trains a powerful neural network on the cloze style questions; and further fine-tunes the model on the labeled examples.", "labels": [], "entities": []}, {"text": "We evaluate our proposed system across three diverse datasets from different domains, and find it to be highly effective with very little labeled data.", "labels": [], "entities": []}, {"text": "We attain more than 50% F1 score on SQuAD and TriviaQA with less than a thousand labelled examples.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9788122177124023}, {"text": "SQuAD", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.8180710077285767}, {"text": "TriviaQA", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.9038798809051514}]}, {"text": "We are also releasing a set of 3.2M cloze-style questions for practitioners to use while building QA systems 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep learning systems have shown a lot of promise for extractive Question Answering (QA), with performance comparable to humans when large scale data is available.", "labels": [], "entities": [{"text": "extractive Question Answering (QA)", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.7675321946541468}]}, {"text": "However, practitioners looking to build QA systems for specific applications may not have the resources to collect tens of thousands of questions on corpora of their choice.", "labels": [], "entities": []}, {"text": "At the same time, state-of-the-art machine reading systems do not lend well to low-resource QA settings where the number of labeled questionanswer pairs are limited (c.f.).", "labels": [], "entities": []}, {"text": "Semisupervised QA methods like (  aim to improve this performance by leveraging unlabeled data which is easier to collect.", "labels": [], "entities": []}, {"text": "In this work, we present a semi-supervised QA system which requires the end user to specify a set of base documents and only a small set of question-answer pairs over a subset of these documents.", "labels": [], "entities": []}, {"text": "Our proposed system consists of three stages.", "labels": [], "entities": []}, {"text": "First, we construct cloze-style questions (predicting missing spans of text) from the unlabeled corpus; next, we use the generated clozes to pre-train a powerful neural network model for extractive QA; and finally, we fine-tune the model on the small set of provided QA pairs.", "labels": [], "entities": [{"text": "predicting missing spans of text)", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.8092418710390726}]}, {"text": "Our cloze construction process builds on atypical writing phenomenon and document structure: an introduction precedes and summarizes the main body of the article.", "labels": [], "entities": []}, {"text": "Many large corpora follow such a structure, including Wikipedia, academic papers, and news articles.", "labels": [], "entities": []}, {"text": "We hypothesize that we can benefit from the un-annotated corpora to better answer various questions -at least ones that are lexically similar to the content in base documents and directly require factual information.", "labels": [], "entities": []}, {"text": "We apply the proposed system on three datasets from different domains -SQuAD (, TriviaQA-Web ( and the BioASQ challenge (.", "labels": [], "entities": []}, {"text": "We observe significant improvements in a low-resource setting across all three datasets.", "labels": [], "entities": []}, {"text": "For SQuAD and TriviaQA, we attain an F1 score of more than 50% by merely using 1% of the training data.", "labels": [], "entities": [{"text": "TriviaQA", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.7995008230209351}, {"text": "F1 score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9902030527591705}]}, {"text": "Our system outperforms the approaches for semi-supervised QA presented in , and a baseline which uses the same unlabeled data but with a language modeling objective for pretraining.", "labels": [], "entities": []}, {"text": "In the BioASQ challenge, we outperform the best performing system from previous year's challenge, improving over a baseline which does transfer learning from the SQuAD dataset.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.5729584693908691}, {"text": "SQuAD dataset", "start_pos": 162, "end_pos": 175, "type": "DATASET", "confidence": 0.8653523325920105}]}, {"text": "Our analysis reveals that questions which ask for factual information and match to specific parts of the context documents benefit the most from pretraining on automatically constructed clozes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply our system to three datasets from different domains.", "labels": [], "entities": []}, {"text": "consists of questions whose answers are free form spans of text from passages in Wikipedia articles.", "labels": [], "entities": []}, {"text": "We follow the same setting as in ( , and split 10% of training questions as the test set, and report performance when training on subsets of the remaining data ranging from 1% to 90% of the full set.", "labels": [], "entities": []}, {"text": "We also report the performance on the dev set when trained on the full training set (1 * in).", "labels": [], "entities": []}, {"text": "We use the same hyperparameter settings as in prior work.", "labels": [], "entities": []}, {"text": "We compare and study four different settings: 1) the Supervised Learning (SL) setting, which is only trained on the supervised data, 2) the best performing GDAN model from , 3) pretraining on a Language Modeling (LM) objective and finetuning on the supervised data, and 4) pretraining on the Cloze dataset and fine-tuning on the supervised data.", "labels": [], "entities": [{"text": "Cloze dataset", "start_pos": 292, "end_pos": 305, "type": "DATASET", "confidence": 0.9576564431190491}]}, {"text": "The LM and Cloze methods use exactly the same data for pretraining, but differ in the loss functions used.", "labels": [], "entities": []}, {"text": "We report F1 and EM scores on our test set using the official evaluation scripts provided by the authors of the dataset.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9990639090538025}, {"text": "EM scores", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.969033420085907}]}, {"text": "TriviaQA (Joshi et al., 2017) comprises of over 95K web question-answer-evidence triples.", "labels": [], "entities": []}, {"text": "Like SQuAD, the answers are spans of text.", "labels": [], "entities": []}, {"text": "Similar to the setting in SQuAD, we create multiple smaller subsets of the entire set.", "labels": [], "entities": []}, {"text": "For our semi-supervised QA system, we use the BiDAF+SA model -the highest performing publicly available system for TrivaQA.", "labels": [], "entities": [{"text": "BiDAF+SA", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8358218669891357}]}, {"text": "Here again, we compare the supervised learning (SL) settings against the pretraining on Cloze set and fine tuning on the supervised set.", "labels": [], "entities": []}, {"text": "We report F1 and EM scores on the dev set 4 . We also test on the BioASQ 5b dataset, which consists of question-answer pairs from PubMed abstracts.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9989472031593323}, {"text": "EM", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9211229681968689}, {"text": "BioASQ 5b dataset", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.8719945748647054}]}, {"text": "We use the publicly available system 5 from, and follow the exact same setup as theirs, focusing only on factoid and list questions.", "labels": [], "entities": []}, {"text": "For this setting, there are only 899 questions for training.", "labels": [], "entities": []}, {"text": "Since this is already a lowresource problem we only report results using 5-fold cross-validation on all the available data.", "labels": [], "entities": []}, {"text": "We report Mean Reciprocal Rank (MRR) on the factoid questions, and F1 score for the list questions.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9668981532255808}, {"text": "F1 score", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.991267055273056}]}, {"text": "Column groups represent different fractions of the training set used for training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: A holistic view of the performance of our system compared against baseline systems on SQuAD and TriviaQA.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.8525004386901855}, {"text": "TriviaQA", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.892919659614563}]}, {"text": " Table 3: 5-fold cross-validation results on BioASQ Task 5b.", "labels": [], "entities": [{"text": "BioASQ Task 5b", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.8275510470072428}]}]}