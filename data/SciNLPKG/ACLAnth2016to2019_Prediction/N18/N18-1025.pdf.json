{"title": [{"text": "QuickEdit: Editing Text & Translations by Crossing Words Out", "labels": [], "entities": [{"text": "QuickEdit", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8477718830108643}, {"text": "Editing Text & Translations", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.6202496960759163}]}], "abstractContent": [{"text": "We propose a framework for computer-assisted text editing.", "labels": [], "entities": [{"text": "text editing", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.6890316903591156}]}, {"text": "It applies to translation post-editing and to paraphrasing.", "labels": [], "entities": [{"text": "translation post-editing", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.9073430597782135}]}, {"text": "Our proposal relies on very simple interactions: a human editor modifies a sentence by marking tokens they would like the system to change.", "labels": [], "entities": []}, {"text": "Our model then generates anew sentence which reformulates the initial sentence by avoiding marked words.", "labels": [], "entities": []}, {"text": "The approach builds upon neu-ral sequence-to-sequence modeling and introduces a neural network which takes as input a sentence along with change markers.", "labels": [], "entities": []}, {"text": "Our model is trained on translation bitext by simulating post-edits.", "labels": [], "entities": [{"text": "translation bitext", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8270665109157562}]}, {"text": "We demonstrate the advantage of our approach for translation post-editing through simulated post-edits.", "labels": [], "entities": [{"text": "translation post-editing", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.9003319144248962}]}, {"text": "We also evaluate our model for paraphrasing through a user study.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computers can help humans edit text more efficiently.", "labels": [], "entities": []}, {"text": "In particular, statistical models are used for that purpose, for instance to help correct spelling mistakes) or suggest likely completions of a sentence (.", "labels": [], "entities": []}, {"text": "In this work, we rely on statistical learning to enable a computer to rephrase a sentence by only pointing at words that should be avoided.", "labels": [], "entities": []}, {"text": "Specifically, we consider the task of reformulating either a sentence, i.e. paraphrasing), or a translation, i.e. translation postediting.", "labels": [], "entities": [{"text": "translation postediting", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.8495503664016724}]}, {"text": "Paraphrasing reformulates a sentence with different words preserving its meaning, while translation post-editing takes a candidate translation along with the corresponding source sentence and improves it.", "labels": [], "entities": []}, {"text": "Our proposal relies on very simple interactions: a human editor modifies a sentence by selecting tokens they would like the system to replace and no other feedback.", "labels": [], "entities": []}, {"text": "Our system then generates anew sentence which reformulates the initial sentence by avoiding the word types from the selected tokens.", "labels": [], "entities": []}, {"text": "Our approach builds upon neural sequence-to-sequence and introduces a neural network which takes as input a sentence along with token markers.", "labels": [], "entities": []}, {"text": "We introduce a novel attentionbased architecture suited to this goal and propose a training procedure based on simulated post-edits on translation bitext ( \u00a73).", "labels": [], "entities": []}, {"text": "This approach allows to get substantial modifications of the initial sentence -including deletion, reordering and insertion of multiple words -with limited user effort.", "labels": [], "entities": []}, {"text": "Our experiments ( \u00a74) relies on large scale simulated post-edits.", "labels": [], "entities": []}, {"text": "They show that our model outperforms our post-editing baseline by up to 5 BLEU points on WMT'14 English-German and WMT'14 German-English translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9993314743041992}, {"text": "WMT'14 English-German", "start_pos": 89, "end_pos": 110, "type": "DATASET", "confidence": 0.8898227214813232}, {"text": "WMT'14 German-English translation", "start_pos": 115, "end_pos": 148, "type": "DATASET", "confidence": 0.8899732629458109}]}, {"text": "The advantage of our method is also highlighted in monolingual settings, where we analyze the quality of the paraphrases generated by our model in a user study.", "labels": [], "entities": []}, {"text": "Before introducing our method ( \u00a73) and its empirical evaluation ( \u00a74), we describe related work in the next section.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate on three translation datasets of increasing size and we report results in both language directions: IWSLT'14 GermanEnglish (), WMT'14 GermanEnglish (, and WMT'14 English-French ().", "labels": [], "entities": [{"text": "IWSLT'14 GermanEnglish", "start_pos": 112, "end_pos": 134, "type": "DATASET", "confidence": 0.7037617862224579}, {"text": "WMT'14 GermanEnglish", "start_pos": 139, "end_pos": 159, "type": "DATASET", "confidence": 0.7552569806575775}, {"text": "WMT'14", "start_pos": 167, "end_pos": 173, "type": "DATASET", "confidence": 0.9359161257743835}]}, {"text": "Our postediting baseline is our initial neural translation system, complemented with decoding constraints to disallow marked guess words to be considered in the beam.", "labels": [], "entities": [{"text": "initial neural translation", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.6758839090665182}]}, {"text": "For paraphrasing, we compare our model trained on WMT'14 fr-en to the model of) on the MTC dataset () following their setup.", "labels": [], "entities": [{"text": "paraphrasing", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9607419967651367}, {"text": "WMT'14 fr-en", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9325710535049438}, {"text": "MTC dataset", "start_pos": 87, "end_pos": 98, "type": "DATASET", "confidence": 0.9723660349845886}]}, {"text": "We relied on WMT'14 fr-en training data motivated by its size . For IWSLT'14 we train on 160K sentence pairs and we validate on a random subset of 7,250 sentence-pairs held-out from the original training corpus.", "labels": [], "entities": [{"text": "WMT'14 fr-en training data", "start_pos": 13, "end_pos": 39, "type": "DATASET", "confidence": 0.8697759658098221}, {"text": "IWSLT'14", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.8972245454788208}]}, {"text": "We test on the concatenation of tst2010, tst2011, tst2012, tst2013, dev2010 and dev2012 comprising 6,750 sentence pairs.", "labels": [], "entities": []}, {"text": "The vocabulary for this dataset is 24k for English and 36k for German.", "labels": [], "entities": []}, {"text": "For WMT'14 English to German and German to English, we use the same setup as Luong et al. which comprises 4.5M sentence pairs for training and we test on newstest2014.", "labels": [], "entities": [{"text": "WMT'14 English to German", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.5872892960906029}, {"text": "newstest2014", "start_pos": 154, "end_pos": 166, "type": "DATASET", "confidence": 0.9649679064750671}]}, {"text": "We took 45k sentences out of the training set for validation purpose.", "labels": [], "entities": [{"text": "validation", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.9730224013328552}]}, {"text": "As vocabulary, we learn a joint source and target byte-pair encoding (BPE) with 44k types from the training set (Sennrich et al., 2016b,a).", "labels": [], "entities": []}, {"text": "Note that even when using BPE, we solely rely on full word markers, i.e. all the BPE tokens of a given word carry the same binary indication (to be changed/no preference).", "labels": [], "entities": []}, {"text": "For WMT'14 English to French and French to English (), we also rely on BPE with 44k types.", "labels": [], "entities": [{"text": "WMT'14 English to French", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6270086318254471}, {"text": "BPE", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.7901650667190552}]}, {"text": "This dataset is larger with 35.4M sentences for training and 26k sentences for validation.", "labels": [], "entities": []}, {"text": "We rely on newstest2014 for testing 3 . The model architecture settings are borrowed from).", "labels": [], "entities": []}, {"text": "For IWSLT'14 deen and IWSLT'14 en-de, we rely on 4-layer encoders and 3-layer decoders, both with 256 hidden units and kernel width 3.", "labels": [], "entities": [{"text": "IWSLT'14", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7652313113212585}, {"text": "IWSLT'14 en-de", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.8226106762886047}]}, {"text": "The word embedding for source and target as well as the output matrix have 256 dimensions.", "labels": [], "entities": []}, {"text": "For WMT'14 en-de and WMT'14 de-en, both encoders and decoders have 15 layers (9 layers with 512 hidden units, 4 layers with 1,024 units followed by 2 layers with 2,048 units).", "labels": [], "entities": [{"text": "WMT'14", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8302003145217896}, {"text": "WMT'14", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.9025342464447021}]}, {"text": "Input embeddings have 768 dimensions, output embedding have 512.", "labels": [], "entities": []}, {"text": "For WMT'14 en-fr and WMT'14 fr-en, both encoders and decoders have 15 layers (6 layers with 512 hidden units, 4 layers with 768 units, 3 layers with 1024 units, followed by two larger layers with 2048 and 4096 units).", "labels": [], "entities": [{"text": "WMT'14", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8949935436248779}, {"text": "WMT'14", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.9156415462493896}]}, {"text": "Similar to the German model, input embeddings have 768 dimensions, output embedding have 512 dimensions.", "labels": [], "entities": []}, {"text": "For all datasets, we decode using beam search with abeam of size 5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Editing results (BLEU4) when all incorrect tokens are requested to be changed.", "labels": [], "entities": [{"text": "BLEU4)", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9476371109485626}]}, {"text": " Table 3: Monolingual editing examples from the WMT'14 fr-en test set. Examples originate from news sentences  of the newstest2014 dataset. Strike-through text indicates the tokens marked to be changed. Bold text indicates  tokens introduced by the model, i.e. tokens not present in the original guess.", "labels": [], "entities": [{"text": "WMT'14 fr-en test set", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.9285611510276794}, {"text": "newstest2014 dataset", "start_pos": 118, "end_pos": 138, "type": "DATASET", "confidence": 0.9098980128765106}]}, {"text": " Table 4: Paraphrasing experiments on the MTC dataset.", "labels": [], "entities": [{"text": "MTC dataset", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.9128526151180267}]}]}