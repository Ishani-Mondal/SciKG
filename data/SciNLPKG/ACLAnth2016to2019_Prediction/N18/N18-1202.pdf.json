{"title": [], "abstractContent": [{"text": "We introduce anew type of deep contextual-ized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).", "labels": [], "entities": []}, {"text": "Our word vectors are learned functions of the internal states of a deep bidirec-tional language model (biLM), which is pre-trained on a large text corpus.", "labels": [], "entities": []}, {"text": "We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, tex-tual entailment and sentiment analysis.", "labels": [], "entities": [{"text": "question answering", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.8879126608371735}, {"text": "sentiment analysis", "start_pos": 212, "end_pos": 230, "type": "TASK", "confidence": 0.9551134407520294}]}, {"text": "We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.", "labels": [], "entities": []}], "introductionContent": [{"text": "Pre-trained word representations () area key component in many neural language understanding models.", "labels": [], "entities": [{"text": "neural language understanding", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.7970162828763326}]}, {"text": "However, learning high quality representations can be challenging.", "labels": [], "entities": []}, {"text": "They should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 272, "end_pos": 294, "type": "TASK", "confidence": 0.7719529867172241}]}, {"text": "Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.", "labels": [], "entities": []}, {"text": "We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus.", "labels": [], "entities": []}, {"text": "For this reason, we call them ELMo (Embeddings from Language Models) representations.", "labels": [], "entities": []}, {"text": "Unlike previous approaches for learning contextualized word vectors (, ELMo representations are deep, in the sense that they area function of all of the internal layers of the biLM.", "labels": [], "entities": []}, {"text": "More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer.", "labels": [], "entities": []}, {"text": "Combining the internal states in this manner allows for very rich word representations.", "labels": [], "entities": []}, {"text": "Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lowerlevel states model aspects of syntax (e.g., they can be used to do part-of-speech tagging).", "labels": [], "entities": [{"text": "word sense disambiguation tasks", "start_pos": 196, "end_pos": 227, "type": "TASK", "confidence": 0.7468583732843399}, {"text": "part-of-speech tagging", "start_pos": 307, "end_pos": 329, "type": "TASK", "confidence": 0.7041972875595093}]}, {"text": "Simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of semi-supervision that are most useful for each end task.", "labels": [], "entities": []}, {"text": "Extensive experiments demonstrate that ELMo representations work extremely well in practice.", "labels": [], "entities": []}, {"text": "We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.7478277087211609}, {"text": "textual entailment", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.6970541477203369}, {"text": "question answering", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.8620001971721649}, {"text": "sentiment analysis", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.9068733751773834}]}, {"text": "The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions.", "labels": [], "entities": []}, {"text": "For tasks where direct comparisons are possible, ELMo outperforms CoVe (, which computes contextualized representations using a neural machine translation encoder.", "labels": [], "entities": []}, {"text": "Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform those derived from just the top layer of an LSTM.", "labels": [], "entities": []}, {"text": "Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.", "labels": [], "entities": []}], "datasetContent": [{"text": "Question answering The Stanford Question Answering Dataset (SQuAD) ( contains 100K+ crowd sourced questionanswer pairs where the answer is a span in a given Wikipedia paragraph.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8738477230072021}, {"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 23, "end_pos": 66, "type": "DATASET", "confidence": 0.855545026915414}]}, {"text": "Our baseline model) is an improved version of the Bidirectional Attention Flow model in.", "labels": [], "entities": []}, {"text": "It adds a self-attention layer after the bidirectional attention component, simplifies some of the pooling operations and substitutes the LSTMs for gated recurrent units (GRUs;).", "labels": [], "entities": []}, {"text": "After adding ELMo to the baseline model, test set F 1 improved by 4.7% from 81.1% to 85.8%, a 24.9% relative error reduction over the baseline, and improving the overall single model state-of-the-art by 1.4%.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.8340635299682617}, {"text": "F 1", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9308136999607086}, {"text": "relative error reduction", "start_pos": 100, "end_pos": 124, "type": "METRIC", "confidence": 0.8104398250579834}]}, {"text": "A 11 member ensemble pushes F 1 to 87.4, the overall state-of-the-art at time of submission to the leaderboard.", "labels": [], "entities": [{"text": "F 1", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9849685430526733}]}, {"text": "The increase of 4.7% with ELMo is also significantly larger then the 1.8% improvement from adding CoVe to a baseline model ( Due to the small test sizes for NER and SST-5, we report the mean and standard deviation across five runs with different random seeds.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.6503785848617554}, {"text": "NER", "start_pos": 157, "end_pos": 160, "type": "DATASET", "confidence": 0.7649825215339661}]}, {"text": "The \"increase\" column lists both the absolute and relative improvements over our baseline.", "labels": [], "entities": []}, {"text": "Textual entailment Textual entailment is the task of determining whether a \"hypothesis\" is true, given a \"premise\".", "labels": [], "entities": [{"text": "Textual entailment Textual entailment", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7833256348967552}]}, {"text": "The Stanford Natural Language Inference (SNLI) corpus) provides approximately 550K hypothesis/premise pairs.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 4, "end_pos": 53, "type": "DATASET", "confidence": 0.5737734138965607}]}, {"text": "Our baseline, the ESIM sequence model from, uses a biL-STM to encode the premise and hypothesis, followed by a matrix attention layer, a local inference layer, another biLSTM inference composition layer, and finally a pooling operation before the output layer.", "labels": [], "entities": []}, {"text": "Overall, adding ELMo to the ESIM model improves accuracy by an average of 0.7% across five random seeds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9993427395820618}]}, {"text": "A five member ensemble pushes the overall accuracy to 89.3%, exceeding the previous ensemble best of 88.9% ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9967154264450073}]}, {"text": "Semantic role labeling A semantic role labeling (SRL) system models the predicate-argument structure of a sentence, and is often described as answering \"Who did what to whom\".", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7245846192042033}, {"text": "semantic role labeling (SRL)", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.8085433840751648}]}, {"text": "modeled SRL as a BIO tagging problem and used an 8-layer deep biLSTM with forward and backward directions interleaved, following, our ELMo enhanced biLSTM-CRF achieves 92.22% F 1 averaged over five runs.", "labels": [], "entities": [{"text": "SRL", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.8099700808525085}, {"text": "BIO tagging", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.7032337188720703}, {"text": "F 1", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.9908090531826019}]}, {"text": "The key difference between our system and the previous state of the art from is that we allowed the task model to learn a weighted average of all biLM layers, whereas Peters et al.", "labels": [], "entities": []}, {"text": "(2017) only use the top biLM layer.", "labels": [], "entities": []}, {"text": "5.1, using all layers instead of just the last layer improves performance across multiple tasks.", "labels": [], "entities": []}, {"text": "Sentiment analysis The fine-grained sentiment classification task in the Stanford Sentiment Treebank (SST-5;) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.923150897026062}, {"text": "sentiment classification", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7074887901544571}, {"text": "Stanford Sentiment Treebank (SST-5", "start_pos": 73, "end_pos": 107, "type": "DATASET", "confidence": 0.8436910152435303}]}, {"text": "The sentences contain diverse linguistic phenomena such as idioms and complex syntac-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set comparison of ELMo enhanced neural models with state-of-the-art single model baselines across  six benchmark NLP tasks. The performance metric varies across tasks -accuracy for SNLI and SST-5; F 1 for  SQuAD, SRL and NER; average F 1 for Coref.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9991955161094666}, {"text": "F", "start_pos": 212, "end_pos": 213, "type": "METRIC", "confidence": 0.9828594326972961}, {"text": "F", "start_pos": 249, "end_pos": 250, "type": "METRIC", "confidence": 0.9644246697425842}]}, {"text": " Table 2: Development set performance for SQuAD,  SNLI and SRL comparing using all layers of the biLM  (with different choices of regularization strength \ud97b\udf59) to  just the top layer.", "labels": [], "entities": []}, {"text": " Table 3: Development set performance for SQuAD,  SNLI and SRL when including ELMo at different lo- cations in the supervised model.", "labels": [], "entities": []}, {"text": " Table 5: All-words fine grained WSD F 1 . For CoVe  and the biLM, we report scores for both the first and  second layer biLSTMs.", "labels": [], "entities": []}, {"text": " Table 6: Test set POS tagging accuracies for PTB. For  CoVe and the biLM, we report scores for both the first  and second layer biLSTMs.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.6677886694669724}, {"text": "PTB", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.8170353174209595}]}, {"text": " Table 7: Development set ablation analysis for  SQuAD, SNLI and SRL comparing different choices  for the context-independent type representation and  contextual representation. From left to right, the table  compares systems with only GloVe vectors; only the  ELMo context-independent type representation with- out the ELMo biLSTM layers; full ELMo representa- tions without GloVe; both GloVe and ELMo.", "labels": [], "entities": []}]}