{"title": [{"text": "Commonsense mining as knowledge base completion? A study on the impact of novelty", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6270804901917776}]}], "abstractContent": [{"text": "Commonsense knowledge bases such as Con-ceptNet represent knowledge in the form of relational triples.", "labels": [], "entities": []}, {"text": "Inspired by recent work by (Li et al., 2016), we analyse if knowledge base completion models can be used to mine com-monsense knowledge from raw text.", "labels": [], "entities": []}, {"text": "We propose novelty of predicted triples with respect to the training set as an important factor in interpreting results.", "labels": [], "entities": [{"text": "interpreting", "start_pos": 99, "end_pos": 111, "type": "TASK", "confidence": 0.9638108611106873}]}, {"text": "We critically analyse the difficulty of mining novel commonsense knowledge , and show that a simple baseline method outperforms the previous state of the art on predicting more novel triples.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many natural language understanding tasks require commonsense knowledge in order to resolve ambiguities involving implicit assumptions.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 5, "end_pos": 35, "type": "TASK", "confidence": 0.6489084462324778}]}, {"text": "Collecting such knowledge and representing it in a reusable way is thus an important challenge.", "labels": [], "entities": []}, {"text": "There exist several commonsense knowledge bases maintained by experts (CyC) or acquired by crowdsourcing (ConceptNet) which represent commonsense knowledge as relational triples (e.g., (\"pen\", \"UsedFor\", \"writing\")) ().", "labels": [], "entities": []}, {"text": "Automatic mining of commonsense knowledge, the focus of this work, aims to improve the coverage of such resources.", "labels": [], "entities": [{"text": "Automatic mining of commonsense knowledge", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8031297147274017}]}, {"text": "One common way of improving the coverage of knowledge bases is through knowledge base completion (KBC), which can be formalized as predicting the existence of edges between (usually) pre-existing nodes in the graph.", "labels": [], "entities": [{"text": "knowledge base completion (KBC)", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.739355593919754}]}, {"text": "Recent work by approached commonsense mining as a KBC task.", "labels": [], "entities": [{"text": "commonsense mining", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.9222889840602875}]}, {"text": "Their method mines candidate triples * Work partially done as intern in MILA \u2020 CIFAR Senior Fellow from Wikipedia and reranks the triples with a KBC model in order to extend ConceptNet.", "labels": [], "entities": [{"text": "MILA", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.7511892318725586}]}, {"text": "The goal of this paper is to investigate why recent systems such as the above achieve good performance, and understand their potential for mining commonsense.", "labels": [], "entities": []}, {"text": "We approach it by breaking down the previously reported aggregate results into the cases in which models perform well or poorly.", "labels": [], "entities": []}, {"text": "We focus in particular on the issue of the novelty of model predictions with respect to the triples in the training set.", "labels": [], "entities": []}, {"text": "For example, a triple predicted by a system could be correct because it generates output with a slightly different wording or morphological inflection (e.g., (\"fish\", \"AtLocation\", \"water\") from (\"fish\", \"AtLocation\", \"in water\")), or it could be correct because it exhibits some degree of semantic generalization (e.g., (\"fish\", \"IsCapableOf\", \"swimming\") from (\"fish\", \"AtLocation\", \"in water\")).", "labels": [], "entities": []}, {"text": "Arguably, the former could be handled by better standardization of data set formats or more comprehensive model pre-processing, whereas the latter presents an example of genuine commonsense inference and novelty.", "labels": [], "entities": []}, {"text": "This analysis is especially important for commonsense mining because of the diversity of the entities, relations, and linguistic expressions thereof in current datasets.", "labels": [], "entities": [{"text": "commonsense mining", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.912465900182724}]}, {"text": "The contribution of this paper is two-fold.", "labels": [], "entities": []}, {"text": "First, we test if the KBC task as it is setup in recent work can gauge a model's ability to mine novel commonsense (i.e. find novel commonsense facts based on some resource).", "labels": [], "entities": []}, {"text": "We present a model that performs poorly on KBC but matches the best model on the task of mining novel commonsense (evaluated by re-ranking extracted candidate triples from Wikipedia).", "labels": [], "entities": [{"text": "KBC", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.7884876132011414}]}, {"text": "We then examine the cause of this discrepancy, and find that around 60% of triples in the KBC test set used by are minor rewordings of existing triples in the training set.", "labels": [], "entities": [{"text": "KBC test set", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.9176869789759318}]}, {"text": "This suggests that controlling for the novelty of triples in both KBC and Wikipedia evaluation is needed.", "labels": [], "entities": [{"text": "novelty", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.948614776134491}]}, {"text": "Second, we present a reassessment of previous methods in which we control the dataset for novelty, extending the results of.", "labels": [], "entities": []}, {"text": "We introduce a simple automated novelty metric and show that it correlates with human judgment.", "labels": [], "entities": []}, {"text": "We then show that the performance of most models on both KBC and Wikipedia triple reranking drops drastically when we evaluate them examples that are genuinely new according to our metric.", "labels": [], "entities": [{"text": "KBC and Wikipedia triple reranking", "start_pos": 57, "end_pos": 91, "type": "DATASET", "confidence": 0.7332513928413391}]}, {"text": "Finally, we demonstrate that a simple baseline model that does not model all interactions between elements in a triple performs surprisingly well on both KBC and reranking when we focus on novel triples.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, we directly test if the performance of a model on the KBC task is predictive of its performance on the mining task.", "labels": [], "entities": []}, {"text": "We follow the mining evaluation protocol from (Li et al., 2016): we X X X X X X X X   rank triples by assigned scores and manually evaluate the top 100 resulting triples on a scale from 0 (nonsensical) to 4 (true statement).", "labels": [], "entities": []}, {"text": "We re-evaluate their model against our baselines and find that the knowledge base completion task is a poor indicator of performance on Wikipedia.", "labels": [], "entities": []}, {"text": "Even though the Factorized and Prototypical models achieve the same or much worse score than DNN on the KBC task (see the first row of), their mining performance on the top 100 triples is better (than both DNN and Bilinear), see.", "labels": [], "entities": []}, {"text": "Triples were scored by two students and scores were averaged, with 0.81 Pearson correlation and 0.48 kappa inter-annotator agreement.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 72, "end_pos": 91, "type": "METRIC", "confidence": 0.9522103071212769}]}, {"text": "Motivated by the described similarity of train and test sets in the KBC task, we shift our attention to re-evaluating models on datasets controlled for novelty, extending results of.", "labels": [], "entities": []}, {"text": "We consider the same tasks as in Sec.", "labels": [], "entities": []}, {"text": "3: ConceptNet5 completion task and commonsense mining task based on Wikipedia triples.", "labels": [], "entities": [{"text": "commonsense mining task", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.8617557287216187}]}, {"text": "We now re-evaluate the KBC models using our proposed novelty metric.", "labels": [], "entities": []}, {"text": "First, we examine the performance on different subsets of the  confidence-based split of ConceptNet5.", "labels": [], "entities": []}, {"text": "Specifically, we split the confidence-based test set into 3 buckets, according to 33% (1.93 distance) and 66% (2.80 distance) quantile of distance to the training set.", "labels": [], "entities": []}, {"text": "Second, we run a similar experiment but on a random split of the training set (bucket thresholds at 2.1 and 2.95).", "labels": [], "entities": []}, {"text": "As expected, the performance of models degrades quickly across buckets.", "labels": [], "entities": []}, {"text": "The performance on the farthest bucket drops from 10 to 20% F1 score with respect to the performance on the closest bucket.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.985501766204834}]}, {"text": "We observe that the Factorized model achieves the strongest performance on the farthest bucket.", "labels": [], "entities": []}, {"text": "Similar to Section 4.2, we analyse splitting candidate triples for the mining task using our novelty metric.", "labels": [], "entities": []}, {"text": "We split the Wikipedia dataset into 3 buckets based on 33% (3.21 distance) and 66% (4.22 distance) quantiles of distance to the training set, and we manually score the top 100 triples in each bucket on the same scale from 1 to 5.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 13, "end_pos": 30, "type": "DATASET", "confidence": 0.9598037302494049}]}, {"text": "As in Section 4.2, we note a degradation of performance across buckets for all models (from 1.06 to 0.32 mean human assigned score) and again the Factorized model achieves the best performance on the farthest bucket (mean score 2.26 compared to 1.63 and 1.41).", "labels": [], "entities": []}, {"text": "The Factorized model outperforms DNN on all buckets despite being a simpler X X X X X X X X  model, which we hypothesize is due to DNN being more prone to overfitting.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1 scores on Li et al. (2016) confidence- based test set. F1 score is reported on each bucket  (based on the percentile of triple novelty) and the  entire test set.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9992715716362}, {"text": "F1 score", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9866100251674652}]}, {"text": " Table 2: Average human assigned score (from 1  to 5) of the top 100 Wikipedia triples ranked by  baselines compared to DNN and Bilinear from Li  et al. (2016).", "labels": [], "entities": [{"text": "Average human assigned score", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.6866859644651413}, {"text": "Bilinear", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9510118961334229}]}, {"text": " Table 3: Human assigned novelty categories to  triples from 3 different test datasets. High qual- ity triples are usually trivial. Each column reports  percentage of triples in each category ordered by  novelty. Category 1 corresponds to \"same relation  and minor rewording\". Category 5 corresponds to  \"no directly related triple\".", "labels": [], "entities": []}, {"text": " Table 4: F1 scores on random split. F1 score is  reported on each bucket (based on the percentile  of triple novelty) and the entire set.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9985427856445312}, {"text": "F1 score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9898547828197479}]}, {"text": " Table 5: Novelty based evaluation of quality of  mined triples from Wikipedia dataset. Triples are  scored by humans on scale from 1 to 5.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.8865239322185516}]}]}