{"title": [{"text": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7129355669021606}]}], "abstractContent": [{"text": "The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 19, "end_pos": 51, "type": "TASK", "confidence": 0.8077922463417053}]}, {"text": "Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data.", "labels": [], "entities": [{"text": "natural language analysis tasks", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.7159852907061577}]}, {"text": "However, their utility for NMT has not been extensively explored.", "labels": [], "entities": [{"text": "NMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9429890513420105}]}, {"text": "In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks.", "labels": [], "entities": [{"text": "NMT tasks", "start_pos": 121, "end_pos": 130, "type": "TASK", "confidence": 0.9160566031932831}]}, {"text": "We show that such embeddings can be surprisingly effective in some cases-providing gains of up to 20 BLEU points in the most favorable setting.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9990196228027344}]}], "introductionContent": [{"text": "Pre-trained word embeddings have proven to be highly useful in neural network models for NLP tasks such as sequence tagging and text classification.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.6730974763631821}, {"text": "text classification", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7883450388908386}]}, {"text": "However, it is much less common to use such pre-training in NMT (, largely because the large-scale training corpora used for tasks such as WMT 2 tend to be several orders of magnitude larger than the annotated data available for other tasks, such as the Penn Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 254, "end_pos": 267, "type": "DATASET", "confidence": 0.9944294095039368}]}, {"text": "However, for lowresource languages or domains, it is not necessarily the case that bilingual data is available in abundance, and therefore the effective use of monolingual data becomes a more desirable option.", "labels": [], "entities": []}, {"text": "Researchers have worked on a number of methods for using monolingual data in NMT systems ().", "labels": [], "entities": []}, {"text": "Among these, pre-trained word embeddings have been used either in standard translation systems or as a method for learning translation lexicons in an entirely unsupervised manner.", "labels": [], "entities": []}, {"text": "Both methods show potential improvements in BLEU score when pre-training is properly integrated into the NMT system.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9705817699432373}]}, {"text": "However, from these works, it is still not clear as to when we can expect pre-trained embeddings to be useful in NMT, or why they provide performance improvements.", "labels": [], "entities": [{"text": "NMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.8571985960006714}]}, {"text": "In this paper, we examine these questions more closely, conducting five sets of experiments to answer the following questions: Q1 Is the behavior of pre-training affected by language families and other linguistic features of source and target languages?", "labels": [], "entities": []}, {"text": "( \u00a73) Q2 Do pre-trained embeddings help more when the size of the training data is small?", "labels": [], "entities": []}, {"text": "( \u00a74) Q3 How much does the similarity of the source and target languages affect the efficacy of using pre-trained embeddings?", "labels": [], "entities": []}, {"text": "( \u00a75) Q4 Is it helpful to align the embedding spaces between the source and target languages?", "labels": [], "entities": []}, {"text": "( \u00a76) Q5 Do pre-trained embeddings help more in multilingual systems as compared to bilingual systems?", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to perform experiments in a controlled, multilingual setting, we created a parallel corpus from TED talks transcripts.", "labels": [], "entities": [{"text": "TED talks transcripts", "start_pos": 105, "end_pos": 126, "type": "DATASET", "confidence": 0.807695229848226}]}, {"text": "Specifically, we prepare data between English (EN) and three pairs of languages, where the two languages in the pair are similar, with one being relatively lowresourced compared to the other: Galician (GL) and Portuguese (PT), Azerbaijani (AZ) and Turkish (TR), and Belarusian (BE) and Russian (RU).", "labels": [], "entities": []}, {"text": "The languages in each pair are similar in vocabulary, grammar and sentence structure, which controls for language characteristics and also improves the possibility of transfer learning in multi-lingual models (in \u00a77).", "labels": [], "entities": []}, {"text": "They also represent different language families -GL/PT are Romance; AZ/TR are Turkic; BE/RU are Slavic -allowing for comparison across languages with different caracteristics.", "labels": [], "entities": [{"text": "BE", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9945367574691772}]}, {"text": "Tokenization was done using Moses tokenizer 4 and hard punctuation symbols were used to identify sentence boundaries.", "labels": [], "entities": []}, {"text": "For our experiments, we use a standard 1-layer encoder-decoder model with attention () with abeam size of 5 implemented in xnmt 5 (.", "labels": [], "entities": []}, {"text": "Training uses a batch size of 32 and the Adam optimizer) with an initial learning rate of 0.0002, decaying the learning rate by 0.5 when development loss decreases.", "labels": [], "entities": []}, {"text": "We evaluate the model's performance using BLEU metric ().", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9830544888973236}]}, {"text": "We use available pre-trained word embeddings () trained using fastText 6 on Wikipedia 7 for each language.", "labels": [], "entities": [{"text": "fastText 6 on Wikipedia 7", "start_pos": 62, "end_pos": 87, "type": "DATASET", "confidence": 0.8079533576965332}]}, {"text": "These word embeddings ( incorporate character-level, phrase-level and positional information of words and are trained using CBOW algorithm ().", "labels": [], "entities": []}, {"text": "The dimension of word embeddings is set to 300.", "labels": [], "entities": []}, {"text": "The embedding layer weights of our model are initialized using these pre-trained word vectors.", "labels": [], "entities": []}, {"text": "In baseline models without pre-training, we use Glorot and Bengio (2010)'s uniform initialization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of sentences for each language pair.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9559714198112488}]}, {"text": " Table 2: Effect of pre-training on BLEU score over six  languages. The systems use either random initializa- tion (std) or pre-training (pre) on both the source and  target sides.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9647307395935059}]}, {"text": " Table 3: Effect of linguistic similarity and pre-training  on BLEU. The language family in the second column is  the most recent common ancestor of source and target  language.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9901959300041199}]}, {"text": " Table 4: Correlation between word embedding align- ment and BLEU score in bilingual translation task.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9503446817398071}, {"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9770303070545197}, {"text": "bilingual translation task", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.7147604922453562}]}, {"text": " Table 5: Effect of pre-training on multilingual trans- lation into English. bi is a bilingual system trained  on only the eval source language and all others are  multi-lingual systems trained on two similar source  languages.", "labels": [], "entities": [{"text": "trans- lation", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.6139938433965048}]}, {"text": " Table 5. When applying  pre-trained embeddings, the gains in each transla- tion pair are roughly in order of their similarity,  with GL/PT showing the largest gains, and BE/RU  showing a small decrease. In addition, it is also  interesting to note that as opposed to previous sec- tion, aligning the word embeddings helps to in- crease the BLEU scores for all three tasks. These  increases are intuitive, as a single encoder is used  for both of the source languages, and the encoder  would have to learn a significantly more compli- cated transform of the input if the word embed- dings for the languages were in a semantically sep- arate space. Pre-training and alignment ensures  that the word embeddings of the two source lan- guages are put into similar vector spaces, allowing", "labels": [], "entities": [{"text": "GL/PT", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.7219379544258118}, {"text": "BE/RU", "start_pos": 171, "end_pos": 176, "type": "METRIC", "confidence": 0.8339441418647766}, {"text": "BLEU", "start_pos": 341, "end_pos": 345, "type": "METRIC", "confidence": 0.998823344707489}]}]}