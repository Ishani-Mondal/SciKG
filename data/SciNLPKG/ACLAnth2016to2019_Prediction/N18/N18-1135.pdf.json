{"title": [{"text": "Learning Joint Semantic Parsers from Disjoint Data", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew approach to learning semantic parsers from multiple datasets, even when the target semantic formalisms are drastically different, and the underlying corpora do not overlap.", "labels": [], "entities": []}, {"text": "We handle such \"disjoint\" data by treating annotations for unobserved formalisms as latent structured variables.", "labels": [], "entities": []}, {"text": "Building on state-of-the-art baselines, we show improvements both in frame-semantic parsing and semantic dependency parsing by model-ing them jointly.", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.6885394006967545}, {"text": "semantic dependency parsing", "start_pos": 96, "end_pos": 123, "type": "TASK", "confidence": 0.6532549758752187}]}, {"text": "Our code is open-source and available at https://github.com/ Noahs-ARK/NeurboParser.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing aims to automatically predict formal representations of meaning underlying natural language, and has been useful in question answering), text-to-scene generation (), dialog systems () and social-network extraction (), among others.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.828445166349411}, {"text": "question answering", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.8404523730278015}, {"text": "text-to-scene generation", "start_pos": 154, "end_pos": 178, "type": "TASK", "confidence": 0.7288178354501724}, {"text": "social-network extraction", "start_pos": 205, "end_pos": 230, "type": "TASK", "confidence": 0.7471269369125366}]}, {"text": "Various formal meaning representations have been developed corresponding to different semantic theories;).", "labels": [], "entities": []}, {"text": "The distributed nature of these efforts results in a set of annotated resources that are similar in spirit, but not strictly compatible.", "labels": [], "entities": []}, {"text": "A major axis of structural divergence in semantic formalisms is whether based on spans () or dependencies (, inter alia).", "labels": [], "entities": []}, {"text": "Depending on application requirements, either might be most useful in a given situation.", "labels": [], "entities": []}, {"text": "Learning from a union of these resources seems promising, since more data almost always translates into better performance.", "labels": [], "entities": []}, {"text": "This is indeed the case for two prior techniques-parameter sharing  (, and joint decoding across multiple formalisms using cross-task factors that score combinations of substructures from each (.", "labels": [], "entities": []}, {"text": "Parameter sharing can be used in a wide range of multitask scenarios, when there is no data overlap or even any similarity between the tasks.", "labels": [], "entities": [{"text": "Parameter sharing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9136706292629242}]}, {"text": "But techniques involving joint decoding have so far only been shown to work for parallel annotations of dependency-based formalisms, which are structurally very similar to each other ().", "labels": [], "entities": []}, {"text": "Of particular interest is the approach of Peng et al., where three kinds of semantic graphs are jointly learned on the same input, using parallel annotations.", "labels": [], "entities": []}, {"text": "However, as new annotation efforts cannot be expected to use the same original texts as earlier efforts, the utility of this approach is limited.", "labels": [], "entities": []}, {"text": "We propose an extension to Peng et al.'s formulation which addresses this limitation by considering disjoint resources, each containing only a single kind of annotation.", "labels": [], "entities": []}, {"text": "Moreover, we consider structurally divergent formalisms, one dealing with semantic spans and the other with semantic dependencies.", "labels": [], "entities": []}, {"text": "We experiment on frame-semantic parsing (, a span-based semantic role labeling (SRL) task ( \u00a72.1), and on a dependency-based minimum recursion semantic parsing (DELPH-IN MRS, or DM; Flickinger et al., 2012) task ( \u00a72.2).", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7295116931200027}, {"text": "span-based semantic role labeling (SRL) task", "start_pos": 45, "end_pos": 89, "type": "TASK", "confidence": 0.777983196079731}, {"text": "dependency-based minimum recursion semantic parsing", "start_pos": 108, "end_pos": 159, "type": "TASK", "confidence": 0.6045278131961822}]}, {"text": "See for an example sentence with gold FrameNet annotations, and author-annotated DM representations.", "labels": [], "entities": []}, {"text": "Our joint inference formulation handles missing annotations by treating the structures that are not present in a given training example as latent variables ( \u00a73).", "labels": [], "entities": []}, {"text": "1 Specifically, semantic dependencies are treated as a collection of latent variables when training on FrameNet examples.", "labels": [], "entities": []}, {"text": "Using this latent variable formulation, we present an approach for relating spans and dependencies, by explicitly scoring affinities between pairs of potential spans and dependencies.", "labels": [], "entities": []}, {"text": "Because there area huge number of such pairs, we limit our consideration to only certain pairs-our design is inspired by the head rules of.", "labels": [], "entities": []}, {"text": "Further possible span-dependency pairs are pruned using an 1 -penalty technique adapted from sparse structure learning ( \u00a75).", "labels": [], "entities": []}, {"text": "Neural network architectures are used to score framesemantic structures, semantic dependencies, as well as cross-task structures ( \u00a74).", "labels": [], "entities": []}, {"text": "To summarize, our contributions include: \u2022 using a latent variable formulation to extend cross-task scoring techniques to scenarios where datasets do not overlap; \u2022 learning cross-task parts across structurally divergent formalisms; and \u2022 using an 1 -penalty technique to prune the space of cross task parts.", "labels": [], "entities": [{"text": "cross-task scoring", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7845292687416077}]}, {"text": "Our approach results in anew state-of-the-art in frame-semantic parsing, improving prior work by 0.8% absolute F 1 points ( \u00a76), and achieves competitive performance on semantic dependency parsing.", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7057152390480042}, {"text": "F 1", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9522397816181183}, {"text": "semantic dependency parsing", "start_pos": 169, "end_pos": 196, "type": "TASK", "confidence": 0.6278070906798044}]}, {"text": "Our code is available at https://github.com/Noahs-ARK/ NeurboParser.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model is evaluated on two different releases of FrameNet: FN 1.5 and FN 1.7, 9 using splits from.", "labels": [], "entities": [{"text": "FN 1.5", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.9431297481060028}, {"text": "FN 1.7", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.9311154186725616}]}, {"text": "Following and, each target annotation is treated as a separate training instance.", "labels": [], "entities": []}, {"text": "We also include as training data the exemplar sentences, each annotated fora single target, as they have been reported to improve performance (: FN 1.5 full structure extraction test performance.", "labels": [], "entities": [{"text": "FN 1.5 full structure extraction", "start_pos": 145, "end_pos": 177, "type": "TASK", "confidence": 0.6997075557708741}]}, {"text": "\u2020 denotes the models jointly predicting frames and arguments, and other systems implement two-stage pipelines and use the algorithm by to predict frames.", "labels": [], "entities": []}, {"text": "K\u00d7 denotes a product-of-experts ensemble of K models.", "labels": [], "entities": []}, {"text": "* Ensembles a sequential tagging CRF and a relational model.", "labels": [], "entities": [{"text": "sequential tagging CRF", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.6561056971549988}]}, {"text": "Bold font indicates best performance among all systems.", "labels": [], "entities": []}, {"text": "et al., denoted as NeurboParser (BASIC).", "labels": [], "entities": [{"text": "BASIC", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.6571928262710571}]}, {"text": "To ensure fair comparison with our FULL model, we made several modifications to their implementation ( \u00a76.3).", "labels": [], "entities": []}, {"text": "We observed performance improvements from our reimplementation, which can be seen in.", "labels": [], "entities": []}, {"text": "For frame SRL, we discard argument spans longer than 20 tokens (.", "labels": [], "entities": [{"text": "frame SRL", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.39003677666187286}]}, {"text": "We further pretrain an unlabeled model and prune spans with posteriors lower than 1/n 2 , with n being the input sentence length.", "labels": [], "entities": []}, {"text": "For semantic dependencies, we generally follow Martins and Almeida (2014), replacing their feature-rich pruner with neural networks.", "labels": [], "entities": []}, {"text": "We observe that O(n) spans/arcs remain after pruning, with around 96% FN development recall, and more than 99% for DM.", "labels": [], "entities": [{"text": "O(n) spans/arcs", "start_pos": 16, "end_pos": 31, "type": "METRIC", "confidence": 0.8458955713680812}, {"text": "FN development", "start_pos": 70, "end_pos": 84, "type": "METRIC", "confidence": 0.7647688686847687}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.518669843673706}]}], "tableCaptions": [{"text": " Table 1: Number of instances in datasets.", "labels": [], "entities": []}, {"text": " Table 2: FN 1.5 full structure extraction test per- formance.  \u2020 denotes the models jointly predicting  frames and arguments, and other systems imple- ment two-stage pipelines and use the algorithm by", "labels": [], "entities": [{"text": "FN", "start_pos": 10, "end_pos": 12, "type": "DATASET", "confidence": 0.6398133039474487}]}, {"text": " Table 5.  Pruning strategies. For frame SRL, we  discard argument spans longer than 20 to- kens (", "labels": [], "entities": [{"text": "frame SRL", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.4766427129507065}]}, {"text": " Table 4: FN 1.7 full structure extraction and frame  identification test results. Bold font indicates best  performance. FN 1.7 test set is an extension of FN  1.5 test, hence the results here are not comparable  to those reported in Table 2.", "labels": [], "entities": [{"text": "FN 1.7 full structure extraction", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8534073233604431}, {"text": "frame  identification", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.6685496717691422}, {"text": "FN 1.7 test set", "start_pos": 122, "end_pos": 137, "type": "DATASET", "confidence": 0.9577382057905197}, {"text": "FN  1.5 test", "start_pos": 157, "end_pos": 169, "type": "DATASET", "confidence": 0.9410950541496277}]}, {"text": " Table 5: Labeled parsing performance in F 1 score  for DM semantic dependencies. id denotes in- domain WSJ test data, and ood denotes out-of- domain brown corpus test data. Bold font indi- cates best performance.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9410711526870728}, {"text": "DM semantic dependencies", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.8082359830538431}, {"text": "WSJ test data", "start_pos": 104, "end_pos": 117, "type": "DATASET", "confidence": 0.8649888237317404}]}, {"text": " Table 2. We are the first to report  frame-semantic parsing results on FN 1.7, and we  encourage future efforts to do so as well.", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.6436080783605576}, {"text": "FN 1.7", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.9478347301483154}]}, {"text": " Table 6: Percentage of errors made by BASIC  and FULL models on the FN 1.5 development  set. Parenthesized numbers show the percentage  of role errors when frame predictions are correct.", "labels": [], "entities": [{"text": "FN 1.5 development  set", "start_pos": 69, "end_pos": 92, "type": "DATASET", "confidence": 0.9459648430347443}, {"text": "Parenthesized", "start_pos": 94, "end_pos": 107, "type": "METRIC", "confidence": 0.9593665599822998}]}, {"text": " Table 6. Entirely  missing an argument accounts for most of the er- rors for both models, but we observe fewer er- rors by FULL compared to BASIC in this category.  FULL tends to predict more arguments in general,  including more incorrect arguments.", "labels": [], "entities": [{"text": "FULL", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9390031099319458}, {"text": "FULL", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.7020218968391418}]}]}