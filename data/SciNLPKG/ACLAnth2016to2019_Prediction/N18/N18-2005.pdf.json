{"title": [{"text": "Is Something Better than Nothing? Automatically Predicting Stance-based Arguments using Deep Learning and Small Labelled Dataset", "labels": [], "entities": [{"text": "Predicting Stance-based Arguments", "start_pos": 48, "end_pos": 81, "type": "TASK", "confidence": 0.6816226045290629}]}], "abstractContent": [{"text": "Online reviews have become a popular portal among customers making decisions about purchasing products.", "labels": [], "entities": []}, {"text": "A number of corpora of reviews have been widely investigated in NLP in general, and, in particular, in argument mining.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.8878852725028992}]}, {"text": "This is a subset of NLP that deals with extracting arguments and the relations among them from user-based content.", "labels": [], "entities": []}, {"text": "A major problem faced by argument mining research is the lack of human-annotated data.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7910163402557373}]}, {"text": "In this paper , we investigate the use of weakly supervised and semi-supervised methods for automatically annotating data, and thus providing large annotated datasets.", "labels": [], "entities": []}, {"text": "We do this by building on previous work that explores the classification of opinions present in reviews based on whether the stance is expressed explicitly or implicitly.", "labels": [], "entities": []}, {"text": "In the work described here, we automatically annotate stance as implicit or explicit and our results show that the datasets we generate, although noisy, can be used to learn better models for implicit/explicit opinion classification.", "labels": [], "entities": [{"text": "implicit/explicit opinion classification", "start_pos": 192, "end_pos": 232, "type": "TASK", "confidence": 0.6664356529712677}]}], "introductionContent": [{"text": "Sentiment analysis and opinion mining are widely researched NLP sub-fields that have extensively investigated opinion-based data such as online reviews ().", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9327782690525055}, {"text": "opinion mining", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.8418718874454498}]}, {"text": "Reviews contain a wide range of opinions posted by users, and are useful for customers in deciding whether to buy a product or not.", "labels": [], "entities": []}, {"text": "With abundant data available online, analysing online reviews becomes difficult, and tasks such as sentiment analysis are inadequate to identify the reasoning behind a user's review.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.9527603685855865}]}, {"text": "Argument mining is an emerging research field that attempts to solve this problem by identifying arguments and the relation between them using ideas from argumentation theory ().", "labels": [], "entities": [{"text": "Argument mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8768244385719299}]}, {"text": "An argument can be defined in two different ways -(1) abstract arguments which do not refer to any internal structure and structured arguments where an argument is a collection of premises leading to a conclusion.", "labels": [], "entities": []}, {"text": "One major problem that is faced by argument mining researchers is the variation in the definition of an argument, which is highly dependent on the data at hand.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7773589789867401}]}, {"text": "Previous work in argument mining has mostly focussed on a particular domain (.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.8824240267276764}]}, {"text": "Furthermore, an argument can be defined in a variety of ways depending on the problem being solved.", "labels": [], "entities": []}, {"text": "As a result, we focus on the specific domain of opinionated texts such as those found in online reviews.", "labels": [], "entities": []}, {"text": "Prior work) in identifying arguments in online reviews has considered sentence-level statements to be arguments based on abstract argumentation models.", "labels": [], "entities": []}, {"text": "However, to extract arguments at a finer level based on the idea of structured arguments is a harder task, requiring us to manually annotate argument components such that they can be used by supervised learning techniques.", "labels": [], "entities": []}, {"text": "Because of the heterogenous nature of user-based content, this labelling task is time-consuming and expensive ( and often domain-dependent.", "labels": [], "entities": []}, {"text": "Here, we are interested in analysing the problem of using supervised learning where the quantity of human-annotated or labelled data is small, and investigating how this issue can be handled by using weakly-supervised and semi-supervised techniques.", "labels": [], "entities": []}, {"text": "We build on our prior work (), which created a small manually annotated dataset for the supervised binary classification of opinions present in online reviews, based", "labels": [], "entities": [{"text": "supervised binary classification of opinions present in online reviews", "start_pos": 88, "end_pos": 158, "type": "TASK", "confidence": 0.7859047121471829}]}], "datasetContent": [{"text": "We used Keras 1 to implement an LSTM model with an embedding layer using pre-trained 300 dimensional GloVe embeddings, followed by an LSTM layer of size 100 with a dropout rate of 0.5 and a sigmoid output layer.", "labels": [], "entities": []}, {"text": "The input length is padded to 50.", "labels": [], "entities": []}, {"text": "Parameter optimisation is done using Adam ().", "labels": [], "entities": [{"text": "Parameter optimisation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7929389774799347}]}, {"text": "For the semisupervised approaches, we consider the number of iterations, m = 1 \u2212 25. 2 reports under Size the number of unannotated data that is automatically labelled using the weakly-supervised approaches.", "labels": [], "entities": []}, {"text": "The corresponding columns Exp and Imp contain the number of manually annotated opinions that are used to train the SVM classifier used in the first-step of the proposed method.", "labels": [], "entities": [{"text": "Exp", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9546880125999451}, {"text": "Imp", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9783154726028442}]}, {"text": "The Acc column denotes the accuracy for predicting the labels of the annotated dataset using the LSTM model trained on the automatically labelled, unannotated data.", "labels": [], "entities": [{"text": "Acc", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9990795850753784}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9996223449707031}]}, {"text": "1 https://keras.io/ Looking at the performance of the weaklysupervised approach in 2, we observe the effect of varying the size of the explicit and the implicit opinion sets that are used to train the SVMbased classifier (see columns Emp and Imp in Table. 2).", "labels": [], "entities": [{"text": "Imp", "start_pos": 242, "end_pos": 245, "type": "METRIC", "confidence": 0.8473514318466187}]}, {"text": "Comparing these with the accuracy scores, we find that using the largest set of explicit opinions in training the initial SVMs gives new annotated data that can train classifiers that perform best on the original annotated data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9994503855705261}]}, {"text": "Overall, using the entire undersampled data for training the SVMs and using the Partially-Strict voting based method gives the best performance with an accuracy of 0.84.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.7805581092834473}, {"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.999333918094635}]}, {"text": "3 reports the results obtained using the self-training method and the reserved method.", "labels": [], "entities": []}, {"text": "These show how the size of the labelled unannotated dataset increases at each iteration and these newly annotated opinions are added to the training data.", "labels": [], "entities": []}, {"text": "The accuracy of the LSTM model in predicting the labels of annotated opinions improves with the size of the automatically labelled dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9982636570930481}, {"text": "predicting the labels of annotated opinions", "start_pos": 34, "end_pos": 77, "type": "TASK", "confidence": 0.8376591900984446}]}, {"text": "However, the accuracy of the reserved method decreases in performance after 20 iterations 2 . Of the two methods, the self-training method performs best, showing that using training data with the lowest confidence does not help in this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9994943141937256}]}, {"text": "Overall, the results are positive, showing a range of methods that can create automatically labelled data which is accurate enough to be useful for deep-learning methods.", "labels": [], "entities": []}, {"text": "The dataset is publicly available at https://goo.gl/Bym2Vz.", "labels": [], "entities": [{"text": "Bym2Vz", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.8324271440505981}]}], "tableCaptions": [{"text": " Table 2: Datasets vary in the number of explicit and implicit opinions that are randomly sampled from the labelled  data to be trained by the SVM classifier. For each of the weakly supervised approach, we give size, the number of  the predicted labels that are used to train an LSTM-based model. This model was then tested on the entire labelled  data, and the accuracy of this LSTM model is reported.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 362, "end_pos": 370, "type": "METRIC", "confidence": 0.99932861328125}]}, {"text": " Table 3: Accuracy of the LSTM model on annotated  data using a set of automatically labelled unannotated  opinions of Size.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9821683168411255}, {"text": "Size", "start_pos": 119, "end_pos": 123, "type": "TASK", "confidence": 0.8919380903244019}]}]}