{"title": [{"text": "RankME: Reliable Human Ratings for Natural Language Generation", "labels": [], "entities": [{"text": "RankME", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5820909142494202}]}], "abstractContent": [{"text": "Human evaluation for natural language generation (NLG) often suffers from inconsistent user ratings.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.8266856869061788}]}, {"text": "While previous research tends to attribute this problem to individual user preferences, we show that the quality of human judgements can also be improved by experimental design.", "labels": [], "entities": []}, {"text": "We present a novel rank-based magnitude estimation method (RankME), which combines the use of continuous scales and relative assessments.", "labels": [], "entities": [{"text": "rank-based magnitude estimation", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.5026943733294805}]}, {"text": "We show that RankME significantly improves the reliability and consistency of human ratings compared to traditional evaluation methods.", "labels": [], "entities": [{"text": "reliability", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9810474514961243}, {"text": "consistency", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9272013902664185}]}, {"text": "In addition, we show that it is possible to evaluate NLG systems according to multiple , distinct criteria, which is important for error analysis.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.6498602032661438}]}, {"text": "Finally, we demonstrate that RankME, in combination with Bayesian estimation of system quality, is a cost-effective alternative for ranking multiple NLG systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human judgement is the primary evaluation criterion for language generation tasks (.", "labels": [], "entities": [{"text": "language generation tasks", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.8246754805246989}]}, {"text": "However, limited effort has been made to improve the reliability of these subjective ratings.", "labels": [], "entities": [{"text": "reliability", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.990456759929657}]}, {"text": "In this research, we systematically compare and analyse a wide range of alternative experimental designs for eliciting intrinsic user judgements for the task of comparing multiple systems.", "labels": [], "entities": []}, {"text": "We draw upon previous studies in language generation, e.g. (, as well as in the related field of machine translation (MT), e.g. ().", "labels": [], "entities": [{"text": "language generation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7597484588623047}, {"text": "machine translation (MT)", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.8514879822731019}]}, {"text": "In particular, we investigate the following challenges: Distinct criteria: Traditionally, NLG outputs are evaluated according to different criteria, such as naturalness and informativeness.", "labels": [], "entities": []}, {"text": "Naturalness, also known as fluency or readability, targets the linguistic competence of the text.", "labels": [], "entities": []}, {"text": "Informativeness, otherwise known as accuracy or adequacy, targets the relevance and correctness of the output relative to the input specification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.997786283493042}]}, {"text": "Ideally, we want to measure outputs of NLG systems with respect to these distinct criteria, especially for error analysis.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 107, "end_pos": 121, "type": "TASK", "confidence": 0.6410022526979446}]}, {"text": "For instance, one system may produce syntactically fluent output but misses important information, while another system, although being less fluent, may generate output that covers the meaning perfectly.", "labels": [], "entities": []}, {"text": "Nevertheless, human judges often fail to distinguish between these different aspects, which results in highly correlated scores, e.g. ().", "labels": [], "entities": []}, {"text": "This is one of the reasons why some more recent research adds a general, overall quality criterion (, or even uses only that ().", "labels": [], "entities": []}, {"text": "In the following, we show that discriminative ratings for different aspects can still be obtained, using distinctive task design.", "labels": [], "entities": []}, {"text": "Consistency: Previous research has identified a high degree of inconsistency inhuman judgements of NLG outputs, where ratings often differ significantly (p < 0.001) for the same utterance (.", "labels": [], "entities": []}, {"text": "While this might be attributed to individual preferences, e.g. (), we also show that consistency (as measured by inter-annotator agreement) can be improved by different experimental setups, e.g. the use of continuous scales instead of discrete ones.", "labels": [], "entities": [{"text": "consistency", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9920415282249451}]}, {"text": "Inconsistent user ratings are problematic in many ways, e.g. when developing metrics for automatic evaluation).", "labels": [], "entities": []}, {"text": "Intrinsic human evaluation methods are typically designed to assess the quality of a system.", "labels": [], "entities": []}, {"text": "However, they are frequently used to compare the quality of different NLG systems, which is not necessarily appropriate.", "labels": [], "entities": []}, {"text": "In the following, we show that relative assessment methods produce more consistent and more discriminative human ratings than direct assessment methods.", "labels": [], "entities": []}, {"text": "In order to investigate these challenges, we compare several state-of-the-art NLG systems, which are evaluated by human crowd workers using a range of evaluation setups.", "labels": [], "entities": []}, {"text": "We show that our newly introduced method, called rank-based magnitude estimation (RankME), outperforms traditional evaluation methods.", "labels": [], "entities": [{"text": "rank-based magnitude estimation (RankME", "start_pos": 49, "end_pos": 88, "type": "TASK", "confidence": 0.45166327357292174}]}, {"text": "It combines advances suggested by previous research, such as continuous scales (), magnitude estimation ( ) and relative assessment.", "labels": [], "entities": [{"text": "magnitude estimation", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.8813349902629852}, {"text": "relative assessment", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.9170072078704834}]}, {"text": "All code and data, as well as a more detailed description of the study setup are publicly available at: https://github.com/jeknov/RankME", "labels": [], "entities": [{"text": "RankME", "start_pos": 130, "end_pos": 136, "type": "DATASET", "confidence": 0.7806997895240784}]}], "datasetContent": [{"text": "We were able to obtain outputs of 3 systems from the recent E2E NLG challenge (): 1 the Sheffield NLP system () and the Slug2Slug system (, as well as the outputs of the baseline TGen system.", "labels": [], "entities": [{"text": "E2E NLG challenge", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.9571668108304342}, {"text": "Sheffield NLP system", "start_pos": 88, "end_pos": 108, "type": "DATASET", "confidence": 0.974398672580719}, {"text": "TGen system", "start_pos": 179, "end_pos": 190, "type": "DATASET", "confidence": 0.8642311692237854}]}, {"text": "We chose these systems in order to assess whether our methods can discriminate between outputs of different quality: Automatic metric scores, including BLEU, ME-TEOR, etc., indicate that the Slug2Slug and TGen systems show similar performance while Sheffield's is further apart.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9992321729660034}, {"text": "ME-TEOR", "start_pos": 158, "end_pos": 165, "type": "METRIC", "confidence": 0.9804911613464355}]}, {"text": "All three systems are based on the sequenceto-sequence (seq2seq) architecture with attention (.", "labels": [], "entities": []}, {"text": "Sheffield NLP and TGen both use this basic architecture with LSTM recurrent cells) and abeam search, TGen further adds a reranker to penalize semantically invalid outputs.", "labels": [], "entities": [{"text": "Sheffield NLP", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9083201885223389}]}, {"text": "Slug2Slug is an ensemble of three seq2seq models with LSTM recurrent decoders.", "labels": [], "entities": []}, {"text": "Two of them use LSTM recurrent encoders and one uses a convolutional encoder.", "labels": [], "entities": []}, {"text": "A reranker checking for semantic validity selects among the outputs of all three models.", "labels": [], "entities": []}, {"text": "We use the first one hundred outputs for each system, and we collect human ratings from three independent crowd workers for each output using the CrowdFlower platform.", "labels": [], "entities": []}, {"text": "We use three different methods to collect human evaluation data: 6-point Likert scales, plain magnitude estimation: Three methods used to collect human evaluation data.", "labels": [], "entities": [{"text": "plain magnitude estimation", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.6726062893867493}]}, {"text": "Here, DA = direct assessment, RR = relative ranking, DS = discrete scale, CS = continuous scale.", "labels": [], "entities": [{"text": "DA", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.9788000583648682}, {"text": "RR", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9627494215965271}, {"text": "DS", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9618496298789978}]}, {"text": "(plain ME), and rank-based magnitude estimation (RankME).", "labels": [], "entities": [{"text": "rank-based magnitude estimation (RankME)", "start_pos": 16, "end_pos": 56, "type": "METRIC", "confidence": 0.8889517188072205}]}, {"text": "Ina magnitude estimation (ME) task (, subjects provide a relative rating of an experimental sentence to a reference sentence, which is associated with a pre-set/fixed number.", "labels": [], "entities": [{"text": "magnitude estimation (ME) task", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.6587441911300024}]}, {"text": "If the target sentence appears twice as good as the reference sentence, for instance, subjects are to multiply the reference score by two; if it appears half as good, they should divide it in half, etc.", "labels": [], "entities": []}, {"text": "Note that ME implies the use of continuous scales, i.e. rating scales without numerical labels, similar to the visual analogue scales used by or direct assessment scales of (, however, without given end-points.", "labels": [], "entities": [{"text": "ME", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.6683487296104431}]}, {"text": "have previously used ME for evaluating readability of automatically generated texts.", "labels": [], "entities": [{"text": "ME", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.7545676231384277}]}, {"text": "RankME extends this idea by asking subjects to provide a relative ranking of all target sentences.", "labels": [], "entities": []}, {"text": "provides a summary of methods and scales, and indicates whether relative ranking or direct assessment was used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Spearman correlation between ratings of naturalness and quality, collected using two different  setups and three data collection methods -Likert, plain ME and RankME. Here, \"*\" denotes p < 0.05.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.804341733455658}]}, {"text": " Table 3: ICC scores for human ratings of natural- ness, informativeness and quality. \"*\" denotes  p < 0.05.", "labels": [], "entities": []}]}