{"title": [{"text": "Ranking Sentences for Extractive Summarization with Reinforcement Learning", "labels": [], "entities": [{"text": "Extractive Summarization", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.7750159204006195}]}], "abstractContent": [{"text": "Single document summarization is the task of producing a shorter version of a document while preserving its principal information content.", "labels": [], "entities": [{"text": "Single document summarization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6661734382311503}]}, {"text": "In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.6057496666908264}, {"text": "sentence ranking task", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.756618489821752}]}, {"text": "We use our algorithm to train a neu-ral summarization model on the CNN and Dai-lyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extrac-tive and abstractive systems when evaluated automatically and by humans.", "labels": [], "entities": [{"text": "CNN and Dai-lyMail datasets", "start_pos": 67, "end_pos": 94, "type": "DATASET", "confidence": 0.7613922655582428}]}], "introductionContent": [{"text": "Automatic summarization has enjoyed wide popularity in natural language processing due to its potential for various information access applications.", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6703854501247406}]}, {"text": "Examples include tools which aid users navigate and digest web content (e.g., news, social media, product reviews), question answering, and personalized recommendation engines.", "labels": [], "entities": [{"text": "question answering", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.813617616891861}]}, {"text": "Single document summarization -the task of producing a shorter version of a document while preserving its information content -is perhaps the most basic of summarization tasks that have been identified over the years (see fora comprehensive overview).", "labels": [], "entities": [{"text": "Single document summarization -the task of producing a shorter version of a document while preserving its information content", "start_pos": 0, "end_pos": 125, "type": "Description", "confidence": 0.7823653174074072}, {"text": "summarization", "start_pos": 156, "end_pos": 169, "type": "TASK", "confidence": 0.9675096869468689}]}, {"text": "Modern approaches to single document summarization are data-driven, taking advantage of the success of neural network architectures and their ability to learn continuous features without recourse to preprocessing tools or linguistic annotations.", "labels": [], "entities": [{"text": "single document summarization", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.6068788270155588}]}, {"text": "Abstractive summarization involves various text rewriting operations (e.g., substitution, deletion, reordering) and has been recently framed as a sequence-to-sequence problem).", "labels": [], "entities": [{"text": "Abstractive summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7280340194702148}]}, {"text": "Central inmost approaches () is an encoder-decoder architecture modeled by recurrent neural networks.", "labels": [], "entities": []}, {"text": "The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence.", "labels": [], "entities": []}, {"text": "An attention mechanism ( is often used to locate the region of focus during decoding.", "labels": [], "entities": []}, {"text": "Extractive systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document.", "labels": [], "entities": []}, {"text": "A few recent approaches conceptualize extractive summarization as a sequence labeling task in which each label specifies whether each document sentence should be included in the summary.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6248233318328857}]}, {"text": "Existing models rely on recurrent neural networks to derive a meaning representation of the document which is then used to label each sentence, taking the previously labeled sentences into account.", "labels": [], "entities": []}, {"text": "These models are typically trained using cross-entropy loss in order to maximize the likelihood of the ground-truth labels and do not necessarily learn to rank sentences based on their importance due to the absence of a ranking-based objective.", "labels": [], "entities": []}, {"text": "Another discrepancy comes from the mismatch between the learning objective and the evaluation criterion, namely ROUGE (, which takes the entire summary into account.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.9965643286705017}]}, {"text": "In this paper we argue that cross-entropy training is not optimal for extractive summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.6409238278865814}]}, {"text": "Models trained this way are prone to generating verbose summaries with unnecessarily long sentences and redundant information.", "labels": [], "entities": []}, {"text": "We propose to overcome these difficulties by globally optimizing the ROUGE evaluation metric and learning to rank sentences for summary generation through a reinforcement learning objective.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.816796600818634}, {"text": "summary generation", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.7217765599489212}]}, {"text": "Similar to previous work, our neural summarization model consists of a hierarchical docu-ment encoder and a hierarchical sentence extractor.", "labels": [], "entities": []}, {"text": "During training, it combines the maximumlikelihood cross-entropy loss with rewards from policy gradient reinforcement learning to directly optimize the evaluation metric relevant for the summarization task.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 187, "end_pos": 205, "type": "TASK", "confidence": 0.9150305688381195}]}, {"text": "We show that this global optimization framework renders extractive models better at discriminating among sentences for the final summary; a sentence is ranked high for selection if it often occurs in high scoring summaries.", "labels": [], "entities": []}, {"text": "We report results on the CNN and DailyMail news highlights datasets ( which have been recently used as testbeds for the evaluation of neural summarization systems.", "labels": [], "entities": [{"text": "CNN and DailyMail news highlights datasets", "start_pos": 25, "end_pos": 67, "type": "DATASET", "confidence": 0.8901210029919943}, {"text": "neural summarization", "start_pos": 134, "end_pos": 154, "type": "TASK", "confidence": 0.5395182073116302}]}, {"text": "Experimental results show that when evaluated automatically (in terms of ROUGE), our model outperforms state-of-the-art extractive and abstractive systems.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9911655783653259}]}, {"text": "We also conduct two human evaluations in order to assess (a) which type of summary participants prefer (we compare extractive and abstractive systems) and (b) how much key information from the document is preserved in the summary (we ask participants to answer questions pertaining to the content in the document by reading system summaries).", "labels": [], "entities": []}, {"text": "Both evaluations overwhelmingly show that human subjects find our summaries more informative and complete.", "labels": [], "entities": []}, {"text": "Our contributions in this work are three-fold: a novel application of reinforcement learning to sentence ranking for extractive summarization; corroborated by analysis and empirical results showing that cross-entropy training is not well-suited to the summarization task; and large scale user studies following two evaluation paradigms which demonstrate that state-of-the-art abstractive systems lag behind extractive ones when the latter are globally trained.", "labels": [], "entities": [{"text": "sentence ranking", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.6996584236621857}, {"text": "summarization task", "start_pos": 252, "end_pos": 270, "type": "TASK", "confidence": 0.913915604352951}]}], "datasetContent": [{"text": "In this section we present our experimental setup for assessing the performance of our model which we call REFRESH as a shorthand for REinFoRcement Learning-based Extractive Summarization.", "labels": [], "entities": [{"text": "REFRESH", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.9621556401252747}, {"text": "REinFoRcement Learning-based Extractive Summarization", "start_pos": 134, "end_pos": 187, "type": "TASK", "confidence": 0.6104336306452751}]}, {"text": "We describe our datasets, discuss implementation details, our evaluation protocol, and the systems used for comparison.", "labels": [], "entities": []}, {"text": "We evaluated our models on the CNN and DailyMail news highlights datasets (.", "labels": [], "entities": [{"text": "CNN and DailyMail news highlights datasets", "start_pos": 31, "end_pos": 73, "type": "DATASET", "confidence": 0.8745405276616415}]}, {"text": "We used the standard splits of for training, validation, and testing (90,266/1,220/1,093 documents for CNN and 196,961/12,148/10,397 for DailyMail).", "labels": [], "entities": [{"text": "CNN", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.9732306599617004}, {"text": "DailyMail", "start_pos": 137, "end_pos": 146, "type": "DATASET", "confidence": 0.9737008213996887}]}, {"text": "We did not anonymize entities or lowercase tokens.", "labels": [], "entities": []}, {"text": "We followed previous studies) in assuming that the \"story highlights\" associated with each article are gold-standard abstractive summaries.", "labels": [], "entities": []}, {"text": "During training we use these to generate high scoring extracts and to estimate rewards for them, but during testing, they are used as reference summaries to evaluate our models.", "labels": [], "entities": []}, {"text": "The results of our QA evaluation are shown in the last column of follow the same pattern as ROUGE in, differences among systems are now greatly amplified.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9834818840026855}]}, {"text": "QAbased evaluation is more focused and a closer reflection of users' information need (i.e., to find out what the article is about), whereas ROUGE simply captures surface similarity (i.e., n-gram overlap) between output summaries and their references.", "labels": [], "entities": []}, {"text": "Interestingly, LEAD is considered better than in the QA evaluation, whereas we find the opposite when participants are asked to rank systems.", "labels": [], "entities": [{"text": "LEAD", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9972259402275085}]}, {"text": "We hypothesize that LEAD is indeed more informative than but humans prefer shorter summaries.", "labels": [], "entities": [{"text": "LEAD", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.6308311223983765}]}, {"text": "The average length of LEAD summaries is 105.7 words compared to 61.6 for.", "labels": [], "entities": [{"text": "LEAD summaries", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.487363338470459}]}], "tableCaptions": [{"text": " Table 1: An abridged CNN article (only first 15 out of 31 sentences are shown) and its \"story highlights\". The  latter are typically written by journalists to allow readers to quickly gather information on stories. Highlights are  often used as gold standard abstractive summaries in the summarization literature.", "labels": [], "entities": [{"text": "summarization", "start_pos": 289, "end_pos": 302, "type": "TASK", "confidence": 0.9727422595024109}]}, {"text": " Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.", "labels": [], "entities": [{"text": "CNN and DailyMail test sets", "start_pos": 25, "end_pos": 52, "type": "DATASET", "confidence": 0.829436469078064}, {"text": "ROUGE-1 (R1)", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.9061984866857529}, {"text": "ROUGE-2 (R2)", "start_pos": 78, "end_pos": 90, "type": "METRIC", "confidence": 0.8957392275333405}, {"text": "ROUGE-L  (RL) F 1 scores", "start_pos": 96, "end_pos": 120, "type": "METRIC", "confidence": 0.9182627882276263}]}, {"text": " Table 3: System ranking and QA-based evaluations.  Rankings (1st, 2nd, 3rd and 4th) are shown as propor- tions. Rank 1 is the best and Rank 4, the worst. The  column QA shows the percentage of questions that par- ticipants answered correctly by reading system sum- maries.", "labels": [], "entities": [{"text": "QA-based", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9030572175979614}, {"text": "QA", "start_pos": 167, "end_pos": 169, "type": "METRIC", "confidence": 0.9611977338790894}]}]}