{"title": [{"text": "A Deeper Look into Dependency-Based Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate the effect of various dependency-based word embeddings on distinguishing between functional and domain similarity, word similarity rankings, and two downstream tasks in English.", "labels": [], "entities": []}, {"text": "Variations include word embeddings trained using context windows from Stanford and Universal dependencies at several levels of enhancement (ranging from unlabeled, to Enhanced++ dependencies).", "labels": [], "entities": []}, {"text": "Results are compared to basic linear contexts and evaluated on several datasets.", "labels": [], "entities": []}, {"text": "We found that embeddings trained with Universal and Stanford dependency contexts excel at different tasks, and that enhanced dependencies often improve performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "For many natural language processing applications, it is important to understand word-level semantics.", "labels": [], "entities": []}, {"text": "Recently, word embeddings trained with neural networks have gained popularity, and have been successfully used for various tasks, such as machine translation () and information retrieval.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.8219775557518005}, {"text": "information retrieval", "start_pos": 165, "end_pos": 186, "type": "TASK", "confidence": 0.8373762369155884}]}, {"text": "Word embeddings are usually trained using linear bag-of-words contexts, i.e. tokens positioned around a word are used to learn a dense representation of that word.", "labels": [], "entities": []}, {"text": "challenged the use of linear contexts, proposing instead to use contexts based on dependency parses.", "labels": [], "entities": []}, {"text": "(This is akin to prior work that found that dependency contexts are useful for vector models).)", "labels": [], "entities": []}, {"text": "They found that embeddings trained this way are better at capturing semantic similarity, rather than relatedness.", "labels": [], "entities": []}, {"text": "For instance, embeddings trained using linear contexts place Hogwarts (the fictional setting of the Harry Potter series) near Dumbledore (a character from the series), whereas embeddings trained with dependency contexts place Hogwarts near Sunnydale (fictional setting of the series Buffy the Vampire Slayer).", "labels": [], "entities": []}, {"text": "The former is relatedness, whereas the latter is similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9701583385467529}]}, {"text": "Work since examined the use of dependency contexts and sentence feature representations for sentence classification (.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.7310069054365158}]}, {"text": "filled in research gaps relating to model type (e.g., CBOW, Skip-Gram, GloVe) and dependency labeling.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.8535518050193787}, {"text": "dependency labeling", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.8413825333118439}]}, {"text": "Interestingly, recently found that dependency-based word embeddings excel at predicting brain activation patterns.", "labels": [], "entities": [{"text": "predicting brain activation patterns", "start_pos": 77, "end_pos": 113, "type": "TASK", "confidence": 0.8302056342363358}]}, {"text": "The best model to date for distinguishing between similarity and relatedness combines word embeddings, WordNet, and dictionaries (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.93083655834198}]}, {"text": "One limitation of existing work is that it has only explored one dependency scheme: the English-tailored Stanford Dependencies.", "labels": [], "entities": [{"text": "English-tailored Stanford Dependencies", "start_pos": 88, "end_pos": 126, "type": "DATASET", "confidence": 0.6984036564826965}]}, {"text": "We provide further analysis using the cross-lingual Universal Dependencies (.", "labels": [], "entities": []}, {"text": "Although we do not compare cross-lingual embeddings in our study, we will address one important question for English: are Universal Dependencies, which are less tailored to English, actually better or worse than the English-specific labels and graphs?", "labels": [], "entities": []}, {"text": "Furthermore, we investigate approaches to simplifying and extending dependencies, including Enhanced dependencies and Enhanced++ dependencies (, as well as two levels of relation simplification.", "labels": [], "entities": []}, {"text": "We hypothesize that the cross-lingual generalizations from universal dependencies and the additional context from enhanced dependencies should improve the performance of word embeddings at distinguishing between functional and domain similarity.", "labels": [], "entities": []}, {"text": "We also investigate how these differences impact word embedding performance at word similarity rankings and two downstream tasks: question-type classification and named entity recognition.", "labels": [], "entities": [{"text": "question-type classification", "start_pos": 130, "end_pos": 158, "type": "TASK", "confidence": 0.7714438438415527}, {"text": "named entity recognition", "start_pos": 163, "end_pos": 187, "type": "TASK", "confidence": 0.6094472308953603}]}], "datasetContent": [{"text": "We use the Stanford CoreNLP parser 1 to parse basic, Enhanced, and Enhanced++ dependencies.", "labels": [], "entities": [{"text": "Stanford CoreNLP parser 1", "start_pos": 11, "end_pos": 36, "type": "DATASET", "confidence": 0.9100146889686584}]}, {"text": "We use the Stanford english SD model to parse Stanford dependencies (trained on the Penn Treebank) and english UD model to parse Universal dependencies (trained on the Universal Dependencies Corpus for English).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9947261810302734}]}, {"text": "We acknowledge that differences in both the size of the training data (Penn Treebank is larger than the Universal Dependency Corpus for English), and the accuracy of the parse can have an effect on our overall performance.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9914616048336029}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9989572763442993}]}, {"text": "We used our own converter to generate simple dependencies based on the rules shown in.", "labels": [], "entities": []}, {"text": "We use the modified word2vecf software 2 that works with arbitrary embedding contexts to train dependencybased word embeddings.", "labels": [], "entities": []}, {"text": "As baselines, we train the following linear-context embeddings using the original word2vec software: 3 CBOW with k = 2, CBOW with k = 5, and Skip-Gram.", "labels": [], "entities": []}, {"text": "We also train enriched Skip-Gram embeddings including subword information () using fastText.", "labels": [], "entities": []}, {"text": "For all embeddings, we use a cleaned recent dump of English Wikipedia (November 2017, 4.3B tokens) as training data.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.8782012760639191}]}, {"text": "We evaluate each on the following tasks: Similarity over Relatedness Akin to the quantitative analysis done by, we test to see how well each approach ranks similar items above related items.", "labels": [], "entities": []}, {"text": "Given pairs of similar and related words, we rank each word pair by the cosine similarity of the corresponding word embeddings, and report the area-under-curve (AUC) of the resulting precision-recall curve.", "labels": [], "entities": [{"text": "area-under-curve (AUC)", "start_pos": 143, "end_pos": 165, "type": "METRIC", "confidence": 0.9270952939987183}, {"text": "precision-recall", "start_pos": 183, "end_pos": 199, "type": "METRIC", "confidence": 0.9895703792572021}]}, {"text": "We use the labeled WordSim-353 () and the Chiarello dataset () as a source of similar and related word pairs.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.9758255481719971}, {"text": "Chiarello dataset", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.7920123934745789}]}, {"text": "For WordSim-353, we only consider pairs with similarity/relatedness scores of at least 5/10, yielding 90 similar pairs and 147 related pairs.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9509403109550476}]}, {"text": "For Chiarello, we disregard pairs that are marked as both similar and related, yielding 48 similar pairs and 48 related pairs.", "labels": [], "entities": []}, {"text": "Ranked Similarity This evaluation uses a list of word pairs that are ranked by degree of functional similarity.", "labels": [], "entities": []}, {"text": "For each word pair, we calculate the cosine similarity, and compare the ranking to that of the human-annotated list using the Spearman correlation.", "labels": [], "entities": []}, {"text": "We use SimLex-999 ( as a ranking of functional similarity.", "labels": [], "entities": [{"text": "SimLex-999", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.889350175857544}]}, {"text": "Since this dataset distinguishes between nouns, adjectives, and verbs, we report individual correlations in addition to the overall correlation.", "labels": [], "entities": []}, {"text": "Question-type Classification (QC) We use an existing QC implementation 5 that uses a bidirectional LSTM.", "labels": [], "entities": [{"text": "Question-type Classification (QC)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8276425957679748}]}, {"text": "We train the model with 20 epochs, and report the average accuracy over 10 runs for each set of embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.999180257320404}]}, {"text": "We train and evaluate using the TREC QC dataset ().", "labels": [], "entities": [{"text": "TREC QC dataset", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.811585545539856}]}, {"text": "We modified the approach to use fixed (non-trainable) embeddings, allowing us to compare the impact of each embedding type.", "labels": [], "entities": []}, {"text": "Named Entity Recognition (NER) We use the Dernoncourt et al.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6971964240074158}, {"text": "Dernoncourt et al.", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.8034006208181381}]}, {"text": "(2017) NER implementation 6 that uses a bidirectional LSTM.", "labels": [], "entities": []}, {"text": "Training consists of a maximum of 100 epochs, with early stopping after 10 consecutive epochs with no improvement to validation performance.", "labels": [], "entities": []}, {"text": "We evaluate NER using the F1 score on the CoNLL NER dataset).", "labels": [], "entities": [{"text": "NER", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.920850396156311}, {"text": "F1 score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9739737808704376}, {"text": "CoNLL NER dataset", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.9623874425888062}]}, {"text": "Like the QC task, we use a non-trainable embedding layer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of various dependency-based word embeddings, and baseline linear contexts at (a) similarity over  relatedness, (b) ranked similarity, and (c) downstream tasks of question classification and named entity recognition.", "labels": [], "entities": [{"text": "question classification", "start_pos": 180, "end_pos": 203, "type": "TASK", "confidence": 0.7546266913414001}, {"text": "named entity recognition", "start_pos": 208, "end_pos": 232, "type": "TASK", "confidence": 0.6253510415554047}]}, {"text": " Table 3: Performance results when embeddings are  further trained for the particular task. The number in  parentheses gives the performance improvement com- pared to when embeddings are not trainable", "labels": [], "entities": []}, {"text": " Table 3. As expected, this im- proves the results because the training captures  task-specific information in the embeddings. Gen- erally, the worst-performing embeddings gained  the most (e.g., CBOW k = 5 for QC, and basic  Stanford for NER). However, the simplified Stan- ford embeddings and the embeddings with sub- word information still outperform the other ap-", "labels": [], "entities": []}]}