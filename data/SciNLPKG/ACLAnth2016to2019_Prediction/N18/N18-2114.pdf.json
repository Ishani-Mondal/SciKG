{"title": [{"text": "Gated Multi-Task Network for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7700226604938507}]}], "abstractContent": [{"text": "Multi-task learning with Convolutional Neu-ral Network (CNN) has shown great success in many Natural Language Processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "This success can be largely attributed to the feature sharing by fusing some layers among tasks.", "labels": [], "entities": []}, {"text": "However, most existing approaches just fully or proportionally share the features without distinguishing the help-fulness of them.", "labels": [], "entities": []}, {"text": "By that the network would be confused by the helpless even harmful features , generating undesired interference between tasks.", "labels": [], "entities": []}, {"text": "In this paper, we introduce gate mechanism into multi-task CNN and propose anew Gated Sharing Unit, which can filter the feature flows between tasks and greatly reduce the interference.", "labels": [], "entities": []}, {"text": "Experiments on 9 text classification datasets shows that our approach can learn selection rules automatically and gain a great improvement over strong baselines.", "labels": [], "entities": [{"text": "text classification", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.6907613426446915}]}], "introductionContent": [{"text": "The combination of multi-task learning and neural networks has shown its advantages in many tasks, ranging from computer vision () to natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.6431976954142252}]}, {"text": "Multi-task learning (MTL) has the ability to share the knowledge among the joint tasks, which implicitly increases the training materials.", "labels": [], "entities": [{"text": "Multi-task learning (MTL)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8467228293418885}]}, {"text": "The shared knowledge help the network learn a more universal representation for the inputs.", "labels": [], "entities": []}, {"text": "Inspired by this, more DNN-based approaches ( utilize multi-task learning to improve their performance.", "labels": [], "entities": []}, {"text": "The scheme for information sharing is the linchpin for designing an elaborate multi-task network.", "labels": [], "entities": [{"text": "information sharing", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7679672539234161}]}, {"text": "Most existing work attempts to find a appropriate proportion to sharing the layers between tasks, despite they entirely reuse the shallow layers ( or add the layers up at a ratio (.", "labels": [], "entities": []}, {"text": "And recently, the latter one shows its advantages for controlling relational intensity among tasks and become prevailing.", "labels": [], "entities": []}, {"text": "More models adopt this thought to enhance the performance (.", "labels": [], "entities": []}, {"text": "However, under the scheme of proportional addition (, all the features are shared with the same weight between every pair of tasks.", "labels": [], "entities": []}, {"text": "Helpless or harmful features maybe transported between tasks with the same importance as helpful ones, namely, the interference is generated.", "labels": [], "entities": []}, {"text": "This would burden the network for distinguishing the helpful features and even mislead the predictions.", "labels": [], "entities": []}, {"text": "To solve above problem, we propose anew CNN-based architecture for multi-task learning, which can share features in a selective way.", "labels": [], "entities": []}, {"text": "Our model allocates a private subnet to each task and transport the features between the subnets with a well-designed module-Gated Sharing Unit.", "labels": [], "entities": []}, {"text": "It has the ability to filter features with gate mechanism ( and select the helpful ones to benefit the tasks in hand, which expands the feature spaces and provides more evidence for right predictions.", "labels": [], "entities": []}, {"text": "Our model is an end-to-end method and the proposed Gated Sharing Unit is easy to train.", "labels": [], "entities": [{"text": "Gated Sharing", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.8593847453594208}]}, {"text": "We conduct extensive experiments on 9 benchmark datasets for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8167653381824493}]}, {"text": "The results show that our model greatly improves the performance and surpasses the single-task models and other competitors.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we demonstrate the empirical performance of our model on 9 related benchmark tasks for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.8532648682594299}]}, {"text": "And the results are compared with the state-of-the-art models.", "labels": [], "entities": []}, {"text": "As shows, we select 9 related benchmark datasets for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.861435204744339}]}, {"text": "The first 6 datasets are all about product reviews, which are comprised of Amazon product reviews in 6 domains, including books, DVDs, cameras, etc.", "labels": [], "entities": []}, {"text": "These corpora are classified according to the sentiment of positiveness or negativeness.", "labels": [], "entities": []}, {"text": "They are collected from the raw data published by).", "labels": [], "entities": []}, {"text": "The rest 3 datasets are RN, SUBJ and TREC.", "labels": [], "entities": [{"text": "RN", "start_pos": 24, "end_pos": 26, "type": "DATASET", "confidence": 0.45917361974716187}, {"text": "TREC", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.951261043548584}]}, {"text": "RN is a dataset about news topic classification, which is collected from Reuters Newswire and published by; SUBJ is a subjectivity dataset, whose task is to classify a sentence level text as being subjective or objective (); TREC dataset has the task of classifying a question into 6 types (the questions are about location, person, numeric information, etc.)().", "labels": [], "entities": [{"text": "news topic classification", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6646869977315267}, {"text": "Reuters Newswire", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.961217850446701}, {"text": "TREC dataset", "start_pos": 225, "end_pos": 237, "type": "DATASET", "confidence": 0.795150876045227}]}], "tableCaptions": [{"text": " Table 1: Statistics of the text classification datasets.  Train, Dev. and Test denote the size of train, devel- opment and test set respectively; C: Vocabulary size;  L: Average sentence length.", "labels": [], "entities": [{"text": "text classification", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.6909704208374023}]}]}