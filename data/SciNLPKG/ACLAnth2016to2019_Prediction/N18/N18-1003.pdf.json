{"title": [{"text": "Joint Bootstrapping Machines for High Confidence Relation Extraction", "labels": [], "entities": [{"text": "High Confidence Relation Extraction", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.7285266444087029}]}], "abstractContent": [{"text": "Semi-supervised bootstrapping techniques for relationship extraction from text iteratively expand a set of initial seed instances.", "labels": [], "entities": [{"text": "relationship extraction from text", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.8433249592781067}]}, {"text": "Due to the lack of labeled data, a key challenge in boot-strapping is semantic drift: if a false positive instance is added during an iteration, then all following iterations are contaminated.", "labels": [], "entities": []}, {"text": "We introduce BREX, anew bootstrapping method that protects against such contamination by highly effective confidence assessment.", "labels": [], "entities": [{"text": "BREX", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9618741273880005}]}, {"text": "This is achieved by using entity and template seeds jointly (as opposed to just one as in previous work), by expanding entities and templates in parallel and in a mutually constraining fashion in each iteration and by introducing higher-quality similarity measures for templates.", "labels": [], "entities": []}, {"text": "Experimental results show that BREX achieves an F 1 that is 0.13 (0.87 vs. 0.74) better than the state of the art for four relationships.", "labels": [], "entities": [{"text": "BREX", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.8888657093048096}, {"text": "F 1", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9962131977081299}]}], "introductionContent": [{"text": "Traditional semi-supervised bootstrapping relation extractors (REs) such as BREDS (, SnowBall) and DIPRE require an initial set of seed entity pairs for the target binary relation.", "labels": [], "entities": [{"text": "bootstrapping relation extractors (REs", "start_pos": 28, "end_pos": 66, "type": "TASK", "confidence": 0.7492608547210693}, {"text": "BREDS", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9163941740989685}, {"text": "SnowBall", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.9164023995399475}]}, {"text": "They find occurrences of positive seed entity pairs in the corpus, which are converted into extraction patterns, i.e., extractors, where we define an extractor as a cluster of instances generated from the corpus.", "labels": [], "entities": []}, {"text": "The initial seed entity pair set is expanded with the relationship entity pairs newly extracted by the extractors from the text iteratively.", "labels": [], "entities": []}, {"text": "The augmented set is then used to extract new relationships until a stopping criterion is met.", "labels": [], "entities": []}, {"text": "Due to lack of sufficient labeled data, rulebased systems dominate commercial use.", "labels": [], "entities": []}, {"text": "Rules are typically defined by creating patterns around the entities (entity extraction) or entity pairs (relation extraction).", "labels": [], "entities": [{"text": "entity extraction) or entity pairs (relation extraction", "start_pos": 70, "end_pos": 125, "type": "TASK", "confidence": 0.694192475742764}]}, {"text": "Recently, supervised machine learning, especially deep learning techniques (, have shown promising results in entity and relation extraction; however, they need sufficient hand-labeled data to train models, which can be costly and time consuming for webscale extractions.", "labels": [], "entities": [{"text": "entity and relation extraction", "start_pos": 110, "end_pos": 140, "type": "TASK", "confidence": 0.615050695836544}, {"text": "webscale extractions", "start_pos": 250, "end_pos": 270, "type": "TASK", "confidence": 0.7059125751256943}]}, {"text": "Bootstrapping machine-learned rules can make extractions easier on large corpora.", "labels": [], "entities": []}, {"text": "Thus, open information extraction systems have recently been popular for domain specific or independent pattern learning.", "labels": [], "entities": [{"text": "open information extraction", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.642235686381658}, {"text": "domain specific or independent pattern learning", "start_pos": 73, "end_pos": 120, "type": "TASK", "confidence": 0.6118735373020172}]}, {"text": "used handwritten rules to generate more rules to extract hypernym-hyponym pairs, without distributional similarity.", "labels": [], "entities": []}, {"text": "For entity extraction, used seed entities to generate extractors with heuristic rules and scored them by counting positive extractions.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7325773984193802}]}, {"text": "Prior work () investigated different extractor scoring measures.", "labels": [], "entities": []}, {"text": "improved scores by introducing expected number of negative entities.", "labels": [], "entities": []}, {"text": "Brin (1998) developed the bootstrapping relation extraction system DIPRE that generates extractors by clustering contexts based on string matching.", "labels": [], "entities": [{"text": "bootstrapping relation extraction", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.603210985660553}]}, {"text": "SnowBall) is inspired by DIPRE but computes a TF-IDF representation of each context.", "labels": [], "entities": [{"text": "SnowBall", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9588575959205627}]}, {"text": "BREDS () uses word embeddings () to bootstrap relationships.", "labels": [], "entities": [{"text": "BREDS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5708227753639221}]}, {"text": "Related work investigated adapting extractor scoring measures in bootstrapping entity extraction with either entities or templates as seeds (.", "labels": [], "entities": [{"text": "bootstrapping entity extraction", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.6510450839996338}]}, {"text": "The state-of-the-art relation extractors bootstrap with only seed entity pairs and suffer due to a surplus of unknown extractions and the lack of labeled data, leading to low confidence extractors.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7069223821163177}]}, {"text": "This in turn leads to to low confidence in the system output.", "labels": [], "entities": []}, {"text": "Prior RE sys-: Notation and definition of key terms tems do not focus on improving the extractors' scores.", "labels": [], "entities": []}, {"text": "In addition, SnowBall and BREDS used a weighting scheme to incorporate the importance of contexts around entities and compute a similarity score that introduces additional parameters and does not generalize well.", "labels": [], "entities": [{"text": "SnowBall", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.9754647016525269}, {"text": "BREDS", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.645469069480896}]}, {"text": "(1) We propose a Joint Bootstrapping Machine 1 (JBM), an alternative to the entity-pair-centered bootstrapping for relation extraction that can take advantage of both entity-pair and template-centered methods to jointly learn extractors consisting of instances due to the occurrences of both entity pair and template seeds.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.806498646736145}]}, {"text": "It scales up the number of positive extractions for non-noisy extractors and boosts their confidence scores.", "labels": [], "entities": []}, {"text": "We focus on improving the scores for non-noisy-low-confidence extractors, resulting in higher recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9990634322166443}]}, {"text": "The relation extractors bootstrapped with entity pair, template and joint seeds are named as BREE, BRET and BREJ, respectively.", "labels": [], "entities": [{"text": "BREE", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.997674286365509}, {"text": "BRET", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9914247989654541}, {"text": "BREJ", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9922892451286316}]}, {"text": "(2) Prior work on embedding-based context comparison has assumed that relations have consistent syntactic expression and has mainly addressed synonymy by using embeddings (e.g.,\"acquired\" -\"bought\").", "labels": [], "entities": [{"text": "embedding-based context comparison", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.660333921511968}]}, {"text": "In reality, there is large variation in the syntax of how relations are expressed, e.g., \"MSFT to acquire NOK for $8B\" 1 github.com/pgcool/Joint-Bootstrapping-Machines vs. \"MSFT earnings hurt by NOK acquisition\".", "labels": [], "entities": [{"text": "NOK", "start_pos": 106, "end_pos": 109, "type": "DATASET", "confidence": 0.837472677230835}, {"text": "NOK acquisition", "start_pos": 195, "end_pos": 210, "type": "TASK", "confidence": 0.6312177777290344}]}, {"text": "We introduce cross-context similarities that compare all parts of the context (e.g., \"to acquire\" and \"acquisition\") and show that these perform better (in terms of recall) than measures assuming consistent syntactic expression of relations.", "labels": [], "entities": [{"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9967825412750244}]}, {"text": "(3) Experimental results demonstrate a 13% gain in F 1 score on average for four relationships and suggest eliminating four parameters, compared to the state-of-the-art method.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9919465780258179}]}, {"text": "The motivation and benefits of the proposed JBM for relation extraction is discussed in depth in section 2.3.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.9217377603054047}]}, {"text": "The method is applicable for both entity and relation extraction tasks.", "labels": [], "entities": [{"text": "relation extraction tasks", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.7845341165860494}]}, {"text": "However, in context of relation extraction, we call it BREJ.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.9158357381820679}, {"text": "BREJ", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9761645793914795}]}], "datasetContent": [{"text": "We re-run BREE () for baseline with a set of 5.5 million news articles from AFP and APW).", "labels": [], "entities": [{"text": "BREE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9915034770965576}, {"text": "AFP", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.9724090099334717}, {"text": "APW", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8887216448783875}]}, {"text": "We use processed dataset of 1.2 million sentences (released by BREE) containing at least two entities linked to FreebaseEasy ().", "labels": [], "entities": [{"text": "BREE", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.7034576535224915}]}, {"text": "We extract four relationships: acquired (ORG-ORG), founderof (ORG-PER), headquartered (ORG-LOC) and affiliation (ORG-PER) for Organization (ORG), Person (PER) and Location (LOC) entity types.", "labels": [], "entities": []}, {"text": "We bootstrap relations in BREE, BRET and BREJ, each with 4 similarity measures using seed entity: Precision (P ), Recall (R) and F 1 compared to the state-of-the-art (baseline).", "labels": [], "entities": [{"text": "BREE", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.8770856857299805}, {"text": "BRET", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.911532461643219}, {"text": "BREJ", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.8655310869216919}, {"text": "Precision (P )", "start_pos": 98, "end_pos": 112, "type": "METRIC", "confidence": 0.944469541311264}, {"text": "Recall (R)", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9388763457536697}, {"text": "F 1", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.971308171749115}]}, {"text": "#out: count of output instances with cnfpi, \u039b, Gq \u00a5 0.5.", "labels": [], "entities": [{"text": "out", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.9631319046020508}]}, {"text": "Bold and underline: Maximum due to BREJ and sim cc , respectively.", "labels": [], "entities": [{"text": "BREJ", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9955418705940247}]}, {"text": "See and 5 for the count of candidates, hyperparameters and different configurations, respectively.", "labels": [], "entities": []}, {"text": "Our evaluation is based on's framework to estimate precision and recall of large-scale RE systems using FreebaseEasy ().", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9972630739212036}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9979427456855774}]}, {"text": "Also following, we use Pointwise Mutual Information (PMI)) to evaluate our system automatically, in addition to relying on an external knowledge base.", "labels": [], "entities": []}, {"text": "We consider only extracted relationship instances with confidence scores cnfpi, \u039b, Gq equal or above 0.5.", "labels": [], "entities": []}, {"text": "We follow the same approach as BREE () to detect the correct order of entities in a relational triple, where we try to identify the presence of passive voice using partof-speech (POS) tags and considering any form of the verb to be, followed by a verb in the past tense or past participle, and ending in the word 'by'.", "labels": [], "entities": [{"text": "BREE", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9900380373001099}]}, {"text": "We use GloVe () embeddings.", "labels": [], "entities": []}, {"text": "shows the experimental results in the three systems for the different relationships with ordered entity pairs and similarity measures (sim match , sim cc ).", "labels": [], "entities": []}, {"text": "Observe that BRET (config 5 ) is precision-oriented while BREJ (config 9 ) recalloriented when compared to BREE (baseline).", "labels": [], "entities": [{"text": "BRET", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9460251331329346}, {"text": "precision-oriented", "start_pos": 33, "end_pos": 51, "type": "METRIC", "confidence": 0.9957820177078247}, {"text": "BREJ", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9942612648010254}, {"text": "BREE", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9530884623527527}]}, {"text": "We seethe number of output instances #out are also higher in BREJ, therefore the higher recall.", "labels": [], "entities": [{"text": "BREJ", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9171923398971558}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9993433356285095}]}, {"text": "The BREJ system in the different similarity configura-: Comparative analysis using different thresholds \u03c4 to evaluate the extracted instances for acquired tions outperforms the baseline BREE and BRET in terms of F 1 score.", "labels": [], "entities": [{"text": "BREJ", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9614625573158264}, {"text": "BREE", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.9857203364372253}, {"text": "BRET", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.9946847558021545}, {"text": "F 1 score", "start_pos": 212, "end_pos": 221, "type": "METRIC", "confidence": 0.9802804390589396}]}, {"text": "On an average for the four relations, BREJ in configurations config and config 10 results in F 1 that is 0.11 (0.85 vs 0.74) and 0.13 (0.87 vs 0.74) better than the baseline BREE.", "labels": [], "entities": [{"text": "BREJ", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9972777962684631}, {"text": "F 1", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9910611808300018}, {"text": "BREE", "start_pos": 174, "end_pos": 178, "type": "METRIC", "confidence": 0.9747822880744934}]}], "tableCaptions": [{"text": " Table 5: Precision (P ), Recall (R) and F 1 compared to the state-of-the-art (baseline). #out: count of output in- stances with cnfpi, \u039b, Gq \u00a5 0.5. avg: average. Bold and underline: Maximum due to BREJ and sim cc , respectively.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9982622265815735}, {"text": "Recall (R)", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9327428042888641}, {"text": "F 1", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9845504760742188}, {"text": "BREJ", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.9958309531211853}]}, {"text": " Table 6: Iterations (k it ) Vs Scores with thresholds (\u03c4 )  for relation acquired in BREJ. \u03c4 refers to \u03c4 sim and \u03c4 cnf", "labels": [], "entities": [{"text": "BREJ", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.550639271736145}]}, {"text": " Table 7: Comparative analysis using different thresh- olds \u03c4 to evaluate the extracted instances for acquired", "labels": [], "entities": []}, {"text": " Table 8: Disjunctive matching of Instances. #hit: the  count of instances matched to positive seeds in k it 1", "labels": [], "entities": [{"text": "Disjunctive matching of Instances", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8153364658355713}]}, {"text": " Table 9: Analyzing the attributes of extractors \u039b  learned for each relationship. Attributes are: number of  extractors (|\u039b|), avg number of instances in \u039b (AIE),  avg \u039b score (AES), avg number of noisy \u039b (ANE),  avg number of non-noisy \u039b (ANNE), avg number of  \u039b N N LC below confidence 0.5 (ANNLC), avg number  of positives (AP) and negatives (AN), ratio of AN to  AP (ANP). The bold indicates comparison of BREE  and BREJ with sim match . avg: average", "labels": [], "entities": [{"text": "avg \u039b score (AES)", "start_pos": 165, "end_pos": 182, "type": "METRIC", "confidence": 0.7230358769496282}, {"text": "avg number  of positives (AP) and negatives (AN)", "start_pos": 302, "end_pos": 350, "type": "METRIC", "confidence": 0.6134898314873377}, {"text": "AN to  AP (ANP)", "start_pos": 361, "end_pos": 376, "type": "METRIC", "confidence": 0.8056694666544596}, {"text": "BREE", "start_pos": 411, "end_pos": 415, "type": "METRIC", "confidence": 0.7656197547912598}, {"text": "BREJ", "start_pos": 421, "end_pos": 425, "type": "METRIC", "confidence": 0.6764318346977234}]}, {"text": " Table 10: BREX+sim match :Scores when w n ignored", "labels": [], "entities": [{"text": "BREX", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9290034770965576}, {"text": "Scores", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9609484076499939}]}, {"text": " Table 11: Subset of the non-noisy extractors (simplified) with their confidence scores cnfp\u03bb, Gq learned in different  configurations for each relation. \u00a6 denotes that the extractor was never learned in config 1 and config 5 . X indicates  that the extractor was never learned in config 1 , config 5 and config 9 .", "labels": [], "entities": []}, {"text": " Table 12: BREX+sim match :Scores with entity bisets", "labels": [], "entities": [{"text": "BREX", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.8737211227416992}]}]}