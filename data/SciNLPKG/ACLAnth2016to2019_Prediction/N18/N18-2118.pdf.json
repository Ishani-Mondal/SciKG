{"title": [{"text": "Slot-Gated Modeling for Joint Slot Filling and Intent Prediction", "labels": [], "entities": [{"text": "Slot-Gated Modeling", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8368246555328369}, {"text": "Joint Slot Filling and Intent Prediction", "start_pos": 24, "end_pos": 64, "type": "TASK", "confidence": 0.6708852201700211}]}], "abstractContent": [{"text": "Attention-based recurrent neural network models for joint intent detection and slot filling have achieved the state-of-the-art performance, while they have independent attention weights.", "labels": [], "entities": [{"text": "joint intent detection", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7590193152427673}, {"text": "slot filling", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.8520498275756836}]}, {"text": "Considering that slot and intent have the strong relationship, this paper proposes a slot gate that focuses on learning the relationship between intent and slot attention vectors in order to obtain better semantic frame results by the global optimization.", "labels": [], "entities": []}, {"text": "The experiments show that our proposed model significantly improves sentence-level semantic frame accuracy with 4.2% and 1.9% relative improvement compared to the attentional model on benchmark ATIS and Snips datasets respectively 1 .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9532658457756042}, {"text": "Snips datasets", "start_pos": 203, "end_pos": 217, "type": "DATASET", "confidence": 0.8257777094841003}]}], "introductionContent": [{"text": "Spoken language understanding (SLU) is a critical component in spoken dialogue systems.", "labels": [], "entities": [{"text": "Spoken language understanding (SLU)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8882977863152822}]}, {"text": "SLU is aiming to form a semantic frame that captures the semantics of user utterances or queries.", "labels": [], "entities": []}, {"text": "It typically involves two tasks: intent detection and slot filling).", "labels": [], "entities": [{"text": "intent detection", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.8957485854625702}, {"text": "slot filling", "start_pos": 54, "end_pos": 66, "type": "TASK", "confidence": 0.8676582276821136}]}, {"text": "These two tasks focus on predicting speakers intent and extracting semantic concepts as constraints for the natural language.", "labels": [], "entities": [{"text": "predicting speakers intent", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.876680831114451}]}, {"text": "Take a movie-related utterance as an example, \"find comedies by James Cameron\", as shown in.", "labels": [], "entities": []}, {"text": "There are different slot labels for each word in the utterance, and a specific intent for the whole utterance.", "labels": [], "entities": []}, {"text": "Slot filling can be treated as a sequence labeling task that maps an input word sequence x = (x 1 , \u00b7 \u00b7 \u00b7 , x T ) to the corresponding slot label sequence y S = (y S 1 , \u00b7 \u00b7 \u00b7 , y ST ), and intent detection can be seen as a classification problem to decide the intent label y I . Popular approaches for slot filling include conditional random fields (CRF) (Ray- mond and and recurrent neural network (RNN) (, and different classification methods, such as support vector machine (SVM) and RNN, have been applied to intent prediction.", "labels": [], "entities": [{"text": "Slot filling", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8307586014270782}, {"text": "intent detection", "start_pos": 190, "end_pos": 206, "type": "TASK", "confidence": 0.7549352645874023}, {"text": "slot filling", "start_pos": 303, "end_pos": 315, "type": "TASK", "confidence": 0.7446707487106323}, {"text": "intent prediction", "start_pos": 514, "end_pos": 531, "type": "TASK", "confidence": 0.7204950451850891}]}, {"text": "Considering that pipelined approaches usually suffer from error propagation due to their independent models, the joint model for slot filling and intent detection has been proposed to improve sentence-level semantics via mutual enhancement between two tasks (.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 129, "end_pos": 141, "type": "TASK", "confidence": 0.7556321322917938}, {"text": "intent detection", "start_pos": 146, "end_pos": 162, "type": "TASK", "confidence": 0.7017110288143158}]}, {"text": "In addition, the attention mechanism () was introduced and leveraged into the model in order to provide the precise focus, which allows the network to learn whereto pay attention in the input sequence for each output label ().", "labels": [], "entities": []}, {"text": "The attentional model proposed by achieved the state-of-the-art performance for joint slot filling and intent prediction, where the parameters for slot filling and intent prediction are learned in a single model with a shared objective.", "labels": [], "entities": [{"text": "slot filling and intent prediction", "start_pos": 86, "end_pos": 120, "type": "TASK", "confidence": 0.6500969707965851}, {"text": "slot filling and intent prediction", "start_pos": 147, "end_pos": 181, "type": "TASK", "confidence": 0.699507611989975}]}, {"text": "However, the prior work did not \"explicitly\" model the relationships between the intent and slots; instead, it applied a joint loss function to \"implicitly\" consider both cues.", "labels": [], "entities": []}, {"text": "Because the slots often highly depend on the intent, this work focuses on how to model the explicit relationships between slots and intent vectors by introducing a slot-gated mechanism.", "labels": [], "entities": []}, {"text": "The contributions are three-fold: 1) the proposed slot-", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the proposed model, we conduct experiments on the benchmark datasets, ATIS (Airline Travel Information System) and Snips.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.8354018330574036}]}, {"text": "The statistics are shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of ATIS and Snips datasets.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.9030358791351318}, {"text": "Snips datasets", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.8116038739681244}]}, {"text": " Table 2: Intents and examples in Snips dataset.", "labels": [], "entities": [{"text": "Snips dataset", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.9143248796463013}]}, {"text": " Table 3: SLU performance on ATIS and Snips datasets (%).  \u2020 indicates the significant improvement over all  baselines (p < 0.05).", "labels": [], "entities": [{"text": "ATIS and Snips datasets", "start_pos": 29, "end_pos": 52, "type": "DATASET", "confidence": 0.7278358489274979}]}]}