{"title": [{"text": "Efficient Sequence Learning with Group Recurrent Networks", "labels": [], "entities": []}], "abstractContent": [{"text": "Recurrent neural networks have achieved state-of-the-art results in many artificial intelligence tasks, such as language modeling, neural machine translation, speech recognition and soon.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.741363912820816}, {"text": "neural machine translation", "start_pos": 131, "end_pos": 157, "type": "TASK", "confidence": 0.7158090472221375}, {"text": "speech recognition", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.7662990391254425}]}, {"text": "One of the key factors to these successes is big models.", "labels": [], "entities": []}, {"text": "However, training such big models usually takes days or even weeks of time even if using tens of GPU cards.", "labels": [], "entities": []}, {"text": "In this paper, we propose an efficient architecture to improve the efficiency of such RNN model training, which adopts the group strategy for recurrent layers, while exploiting the representation rearrangement strategy between layers as well as time steps.", "labels": [], "entities": [{"text": "RNN model training", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7502643465995789}]}, {"text": "To demonstrate the advantages of our models, we conduct experiments on several datasets and tasks.", "labels": [], "entities": []}, {"text": "The results show that our architecture achieves comparable or better accuracy comparing with baselines, with a much smaller number of parameters and at a much lower computational cost.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9977649450302124}]}], "introductionContent": [{"text": "Recurrent Neural Networks (RNNs) have been widely used for sequence learning, and achieved state-of-the-art results in many artificial intelligence tasks in recent years, including language modeling (, neural machine translation (, and speech recognition (.", "labels": [], "entities": [{"text": "sequence learning", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.8120114207267761}, {"text": "language modeling", "start_pos": 181, "end_pos": 198, "type": "TASK", "confidence": 0.7466877102851868}, {"text": "neural machine translation", "start_pos": 202, "end_pos": 228, "type": "TASK", "confidence": 0.7094628016153971}, {"text": "speech recognition", "start_pos": 236, "end_pos": 254, "type": "TASK", "confidence": 0.7593684494495392}]}, {"text": "To get better accuracy, recent state-of-the-art RNN models are designed toward big scale, include going deep (stacking multiple recurrent layers) () and/or going wide (increasing dimensions of hidden states).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9973003268241882}]}, {"text": "For example, an RNN based commercial Neural Machine Translation (NMT) system would employ tens of layers in total, resulting in a large model with hundreds of millions of parameters (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.8382603923479716}]}, {"text": "However, when the model size increases, the computational cost, as well as the memory needed for the training, increases dramatically.", "labels": [], "entities": []}, {"text": "The training cost of aforementioned NMT model reaches as high as 10 19 FLOPs, and the training procedure spends several days with even 96 GPU cards () -such complexity is prohibitively expensive.", "labels": [], "entities": [{"text": "FLOPs", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.998895525932312}]}, {"text": "While above models benefit from big neural networks, it is observed that such networks often have redundancy of parameters (, motivating us to improve parameter efficiency and design more compact architectures that are more efficient in training while keeping good performance.", "labels": [], "entities": []}, {"text": "Recently, many efficient architectures for convolution neural networks have been proposed to reduce training cost in computer vision domain.", "labels": [], "entities": []}, {"text": "Among them, the group convolution is one of the most widely used and successful attempts (, which splits the channels into groups and conducts convolution separately for each group.", "labels": [], "entities": []}, {"text": "It's essentially a diagonal sparse operation to the convolutional layer, which reduces the number of parameters as well as the computation complexity linearly w.r.t. the group size.", "labels": [], "entities": []}, {"text": "Empirical results for such group convolution optimization show great speedup with small degradation on accuracy.", "labels": [], "entities": [{"text": "group convolution optimization", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.644249677658081}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9986318945884705}]}, {"text": "In contrast, there are very limited attempts for designing better architectures for RNNs.", "labels": [], "entities": []}, {"text": "Inspired by those works on CNNs, in this paper, we generalize the group idea to RNNs to conduct recurrent learning in the group level.", "labels": [], "entities": []}, {"text": "Different from CNNs, there are two kinds of parameter redundancy in RNNs: (1) the weight matrices transforming a low-level feature representation to a high-level one may contain redundancy, and (2) the recurrent weight matrices transferring the hidden state of the current step to the hidden state of the next step may also contain redundancy.", "labels": [], "entities": []}, {"text": "Therefore, when designing efficient RNNs, we need to consider both the kinds of redundancy.", "labels": [], "entities": []}, {"text": "We present a simple architecture for efficient sequence learning which consists of group recurrent layers and representation rearrangement layers.", "labels": [], "entities": [{"text": "sequence learning", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.728528618812561}]}, {"text": "First, in a recurrent layer, we split both the input of the sequence and the hidden states into disjoint groups, and do recurrent learning separately for each group.", "labels": [], "entities": []}, {"text": "This operation clearly reduces the model complexity, and can learn intragroup features efficiently.", "labels": [], "entities": []}, {"text": "However, it fails to capture dependency cross different groups.", "labels": [], "entities": []}, {"text": "To recover the inter-groups correlation, we further introduce a representation rearrangement layer between any two consecutive recurrent layers, as well as any two time steps.", "labels": [], "entities": []}, {"text": "With these two operations, we explicitly factorize a recurrent temporal learning into intra-group temporal learning and inter-group temporal learning with a much smaller number of parameters.", "labels": [], "entities": []}, {"text": "The group recurrent layer we proposed is equivalent to the standard recurrent layer with a blockdiagonal sparse weight matrix.", "labels": [], "entities": []}, {"text": "That is, our model employs a uniform sparse structure which can be computed very efficiently.", "labels": [], "entities": []}, {"text": "To show the advantages of our model, we analyze the computation cost and memory usage comparing with standard recurrent networks.", "labels": [], "entities": []}, {"text": "The efficiency improvement is linear to the number of groups.", "labels": [], "entities": []}, {"text": "We conduct experiments on language modeling, neural machine translation and abstractive summarization by using a state-ofthe-art RNN architecture as baseline.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7669630348682404}, {"text": "neural machine translation", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.7121698260307312}, {"text": "abstractive summarization", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6303046643733978}]}, {"text": "The results show that our model can achieve comparable or better accuracy, with a much smaller number of parameters and in a shorter training time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9988093376159668}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first present our newly proposed architecture and conduct in depth analysis on its efficiency improvement.", "labels": [], "entities": []}, {"text": "Then we show a series of empirical study to verify the effectiveness of our methods.", "labels": [], "entities": []}, {"text": "Finally, to better position our work, we introduce some related work and then conclude our work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present results on three sequence learning tasks to show the effectiveness of our method: 1).", "labels": [], "entities": []}, {"text": "language modeling; 2).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7446978092193604}]}, {"text": "neural machine translation; 3).", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6651275257269541}]}], "tableCaptions": [{"text": " Table 1: Single model complexity on validation and test sets for the Penn Treebank language modeling task. BD  is Bayesian dropout. WT is word tying.", "labels": [], "entities": [{"text": "Penn Treebank language modeling task", "start_pos": 70, "end_pos": 106, "type": "DATASET", "confidence": 0.8960695505142212}, {"text": "BD", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.9973600506782532}, {"text": "WT", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.43781471252441406}, {"text": "word tying", "start_pos": 139, "end_pos": 149, "type": "TASK", "confidence": 0.7894431352615356}]}, {"text": " Table 2: BLEU scores on IWSLT 2014 De-En test set.  We report BLEU score results together with number of  parameters of recurrent layers.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990906715393066}, {"text": "IWSLT 2014 De-En test set", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.9601125359535218}, {"text": "BLEU score", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9754326939582825}]}, {"text": " Table 3: BLEU scores on WMT'14 En-De test set. We  report BLEU score results together with number of pa- rameters of recurrent layers. Numbers with  \u2021 are ap- proximately calculated by ourselves according to the  settings described in the paper.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989697933197021}, {"text": "WMT'14 En-De test set", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.9241598397493362}, {"text": "BLEU score", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9771001636981964}]}, {"text": " Table 4: ROUGE F1 scores on abstractive summariza- tion test set. RG-N stands for N-gram based ROUGE  F1 score, RG-L stands for longest common subse- quence based ROUGE F1 score. Params stands for the  parameters of the recurrent layers.", "labels": [], "entities": [{"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.5455983281135559}, {"text": "summariza- tion test set", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.5958532094955444}, {"text": "N-gram based ROUGE  F1 score", "start_pos": 83, "end_pos": 111, "type": "METRIC", "confidence": 0.6239600896835327}]}, {"text": " Table 5: The effect of representation rearrangement to  model performance.", "labels": [], "entities": []}]}