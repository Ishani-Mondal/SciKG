{"title": [{"text": "Structure Regularized Neural Network for Entity Relation Classification for Chinese Literature Text", "labels": [], "entities": [{"text": "Entity Relation Classification", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.7960202594598135}]}], "abstractContent": [{"text": "Relation classification is an important semantic processing task in the field of natural language processing.", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9705998301506042}, {"text": "semantic processing", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7627611458301544}, {"text": "natural language processing", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.6512998541196188}]}, {"text": "In this paper, we propose the task of relation classification for Chinese literature text.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.9456111788749695}]}, {"text": "A new dataset of Chinese literature text is constructed to facilitate the study in this task.", "labels": [], "entities": []}, {"text": "We present a novel model, named Structure Regularized Bidirectional Recurrent Convolutional Neural Network (SR-BRCNN), to identify the relation between entities.", "labels": [], "entities": []}, {"text": "The proposed model learns relation representations along the shortest dependency path (SDP) extracted from the structure regularized dependency tree, which has the benefits of reducing the complexity of the whole model.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed method significantly improves the F 1 score by 10.3, and outperforms the state-of-the-art approaches on Chinese literature text 1 .", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.981402595837911}]}], "introductionContent": [{"text": "Relation classification is the task of identifying the semantic relation holding between two nominal entities in text.", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9693171381950378}]}, {"text": "Recently, neural networks are widely used in relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.9776556193828583}]}, {"text": "proposes a convolutional neural network with two levels of attention.", "labels": [], "entities": []}, {"text": "uses bidirectional long short-term memory networks to model the sentence with sequential information.", "labels": [], "entities": []}, {"text": "first uses SDP between two entities to capture the predicate-argument sequences.", "labels": [], "entities": []}, {"text": "explores the idea of incorporating syntactic parse tree into neural networks.", "labels": [], "entities": []}, {"text": "proposes a noise-tolerant method to deal with wrong labels in distant-supervised relation extraction with soft labels.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7184447199106216}]}, {"text": "In recent years, we The Chinese literature text corpus for relation classification, developed and used by this paper, is available at https://github.com/lancopku/Chinese-Li terature-NER-RE-Dataset have seen a move towards deep learning architectures.", "labels": [], "entities": [{"text": "Chinese literature text corpus", "start_pos": 24, "end_pos": 54, "type": "DATASET", "confidence": 0.7269117087125778}, {"text": "relation classification", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.9113560914993286}]}, {"text": "applies long short term memory (LSTM)) based recurrent neural networks (RNNs) along with the SDP.", "labels": [], "entities": []}, {"text": "In this paper, we focus on relation classification of Chinese literature text, which to our knowledge has not been studied before, due to the challenge.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.9080030024051666}]}, {"text": "Chinese literature text tends to express intuitions and feelings.", "labels": [], "entities": []}, {"text": "It has a wide range of topics.", "labels": [], "entities": []}, {"text": "Many literature articles express feelings in a subtle and special way, making it more difficult to recognize entities.", "labels": [], "entities": []}, {"text": "Chinese literature text is not organized very logically, whether among paragraphs or sentences.", "labels": [], "entities": []}, {"text": "They tend to use various and flexible forms of sentences to create free feelings.", "labels": [], "entities": []}, {"text": "The sentences are not associated with each other by evident conjunctions.", "labels": [], "entities": []}, {"text": "Besides, Chinese is a topic-prominent language, the subject is usually covert and the usage of words is relatively flexible.", "labels": [], "entities": []}, {"text": "In short, sentences of Chinese literature text contain many non-essential words, and embody very complex and flexible structures.", "labels": [], "entities": []}, {"text": "Existing methods make intensive use of the syntactical information, such as part-of-speech tags, and dependency relations.", "labels": [], "entities": []}, {"text": "However, the automatically generated information is not reliable and of poor quality for Chinese literature text.", "labels": [], "entities": []}, {"text": "It is of great challenge for the existing methods to achieve satisfying performance.", "labels": [], "entities": []}, {"text": "To mitigate the noisy syntactical information, we propose to apply structure regularization to the structures used in relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.8859755992889404}]}, {"text": "Recently, many existing systems on structured prediction focus on increasing the level of structural dependencies within the model.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7670437693595886}]}, {"text": "However, the theoretical and experimental study of Sun (2014a) suggests that complex structures are tend to increase the overfitting risk, and can potentially be harm-", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on the Chinese literature text corpus.", "labels": [], "entities": [{"text": "Chinese literature text corpus", "start_pos": 29, "end_pos": 59, "type": "DATASET", "confidence": 0.7328486517071724}]}, {"text": "It contains 9 distinguished types of relations among 837 articles.", "labels": [], "entities": []}, {"text": "The dataset contains 695 articles for training, 58 for validation, and 84 for testing.", "labels": [], "entities": []}, {"text": "We use pre-trained word embeddings, which are trained on Gigaword with word2vec (.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9294807314872742}]}, {"text": "The embeddings of relation are initialized randomly and are 50-dimensional.", "labels": [], "entities": []}, {"text": "The hidden layers of LSTMs to extract information from entities and relations are the same as the embedding dimension of entities and relations.", "labels": [], "entities": []}, {"text": "We applied L2 regularization to weights in neural networks and dropout to embeddings with a keep probability 0.5.", "labels": [], "entities": [{"text": "keep", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9795007705688477}]}, {"text": "AdaDelta) is used for optimization.", "labels": [], "entities": [{"text": "AdaDelta", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8226389288902283}]}, {"text": "compares our SR-BRCNN model with other state-of-the-art methods on the corpus of Chinese literature text, including the basic BR-CNN method.", "labels": [], "entities": []}, {"text": "Structure regularization helps improve the result substantially.", "labels": [], "entities": [{"text": "Structure regularization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7478827238082886}]}, {"text": "The method of structure regularization could prevent the overfitting of poor quality SDPs. and show an example of structure regularized SDP.", "labels": [], "entities": [{"text": "structure regularization", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.6669720858335495}]}, {"text": "The relation is between the two circled elements.", "labels": [], "entities": []}, {"text": "The main idea of the method is to avoid the incorrect structure from the dependency trees generated by the parser.", "labels": [], "entities": []}, {"text": "The SDP in is longer than the SR-SDP in.", "labels": [], "entities": []}, {"text": "However, the dependency tree of the example is not completely correct.", "labels": [], "entities": []}, {"text": "The longer the SDP is, the more incorrect information the model learns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Different structure regularization results  on Chinese literature text.", "labels": [], "entities": []}]}