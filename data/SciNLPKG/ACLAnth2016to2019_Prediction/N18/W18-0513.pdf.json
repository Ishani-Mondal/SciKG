{"title": [{"text": "Generating Feedback for English Foreign Language Exercises", "labels": [], "entities": []}], "abstractContent": [{"text": "While immediate feedback on learner language is often discussed in the Second Language Acquisition literature (e.g., Mackey 2006), few systems used in real-life educational settings provide helpful, metalinguistic feedback to learners.", "labels": [], "entities": [{"text": "Second Language Acquisition", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.6691451470057169}]}, {"text": "In this paper, we present a novel approach leveraging task information to generate the expected range of well-formed and ill-formed variability in learner answers along with the required diagnosis and feedback.", "labels": [], "entities": []}, {"text": "We combine this offline generation approach with an online component that matches the actual student answers against the pre-computed hypotheses.", "labels": [], "entities": []}, {"text": "The results obtained fora set of 33 thousand answers of 7th grade German high school students learning English show that the approach successfully covers frequent answer patterns.", "labels": [], "entities": []}, {"text": "At the same time, paraphrases and meaning errors require a more flexible alignment approach , for which we are planning to complement the method with the CoMiC approach successfully used for the analysis of reading comprehension answers (Meurers et al., 2011).", "labels": [], "entities": []}], "introductionContent": [{"text": "In Second Language Acquisition research and Foreign Language Teaching and Learning practice, the importance of individualized, immediate feedback on learner production for learner proficiency development has long been emphasized (e.g.,).", "labels": [], "entities": [{"text": "Second Language Acquisition", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7312737107276917}, {"text": "Foreign Language Teaching and Learning", "start_pos": 44, "end_pos": 82, "type": "TASK", "confidence": 0.7738996982574463}]}, {"text": "In the classroom, the teacher is generally the only source of reliable, accurate feedback available to students, which poses a wellknown practical problem: in a class of 30 students, with substantial individual differences warranting individual feedback to students, it is highly challenging fora teacher to provide feedback in class or, in a timely fashion, on homework.", "labels": [], "entities": []}, {"text": "* http://icall-research.de Intelligent Language Tutoring Systems (ILTS) are one possible means of addressing this problem.", "labels": [], "entities": []}, {"text": "For form-focused feedback, ILTS have traditionally relied on online processing of learner language.", "labels": [], "entities": [{"text": "ILTS", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.9205434918403625}]}, {"text": "They model ill-formed variation either explicitly via so-called mal-rules (e.g., or by allowing for violations in the language system using a constraint relaxation mechanism (e.g., L'.", "labels": [], "entities": []}, {"text": "One problem with such approaches is that they do not take into account what the learner was trying to do with the language they wrote, e.g., which task or exercise they were trying to complete.", "labels": [], "entities": []}, {"text": "Yet the potential well-formed and ill-formed variability exhibited by learner language can lead to vast search spaces so that integrating top-down, task information is particularly relevant for obtaining valid interpretations of learner language.", "labels": [], "entities": []}, {"text": "Given that incorrect feedback is highly problematic for language learners, ensuring valid interpretations is particularly important.", "labels": [], "entities": []}, {"text": "Combining the bottom-up analysis of learner data with top-down expectations, such as those that can be derived from an exercise being completed, can also be relevant for obtaining efficient processing.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach that pursues this idea of integrating task-based information into the analysis of learner language by combining offline hypothesis generation based on the exercise with online answer analysis in order to provide immediate and reliable form-focused feedback.", "labels": [], "entities": []}, {"text": "Basing our approach on curricular demands and the exercise properties resulting from these demands, we generate the space of wellformed and ill-formed variability expected of the learner answers, using the well-formed target answers provided for the exercises as a starting point.", "labels": [], "entities": []}, {"text": "We thus avoid the problems introduced by directly analyzing potentially ill-formed learner language.", "labels": [], "entities": []}, {"text": "Since generation is done ahead of time, before learners actually interact with the system, we also avoid the performance bottleneck associated with creating and exploring the full search space at run time.", "labels": [], "entities": []}, {"text": "The resulting system can be precise and fast in providing feedback on the grammar concepts in a curriculum underlying a given set of exercises.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 discusses relevant related work before section 3 introduces our system and section 4 provides an overview on the data we elicit.", "labels": [], "entities": []}, {"text": "In section 5, we dive into the feedback architecture and explain both the offline and online component of the mechanism in detail.", "labels": [], "entities": []}, {"text": "Section 6 then provides both a quantitative and a qualitative evaluation before section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe an evaluation of the feedback currently given by our system.", "labels": [], "entities": []}, {"text": "Ina real end-to-end evaluation of a tutoring system, the most interesting evaluation would be to assess the learning gains for the students.", "labels": [], "entities": []}, {"text": "We are currently designing a randomized controlled field study for just such an evaluation involving several classes in the coming school year.", "labels": [], "entities": []}, {"text": "At this point, however, we can at least report offline evaluation metrics calculated on the student answer data that we collected so far.", "labels": [], "entities": []}, {"text": "We plan to make a more comprehensive data set available for research after having conducted the full-year intervention study.", "labels": [], "entities": []}, {"text": "Based on the elicited data introduced in section 4, we selected all individual student answers from the interaction logs of tasks with active, im-132   mediate feedback.", "labels": [], "entities": []}, {"text": "However, since some of these tasks have meaning-oriented goals (e.g., comprehension, translation), which we do not yet provide feedback on, we excluded data from tasks where the title clearly indicated such a goal (e.g., \"Reading: . .", "labels": [], "entities": [{"text": "translation)", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.9047658741474152}]}, {"text": "On the other end of the spectrum, we excluded tasks where students only need to enter single characters as part of words.", "labels": [], "entities": []}, {"text": "The remaining set of 33,589 individual student answers (6,755 distinct types) was provided as input to the feedback algorithm of.", "labels": [], "entities": []}, {"text": "Note that this data set consists of the authentic learner answers entered into the system at any stage of development.", "labels": [], "entities": []}, {"text": "So we run the current version of the feedback algorithm on all the authentic learner data to obtain a complete, current picture of current system performance.", "labels": [], "entities": []}, {"text": "summarizes the results (TA = target answer).", "labels": [], "entities": [{"text": "TA", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9967318773269653}]}, {"text": "We report both answer type counts and answer token counts.", "labels": [], "entities": []}, {"text": "For the answers differing from the target answer (i.e., the ones the system provided feedback on), we also report the percentage relative to the total number of answers differing from the target forms.", "labels": [], "entities": []}, {"text": "For the majority of differing answers (74.72%) the system provides default feedback, where a diff with the target answer is shown to the student, as exemplified by.", "labels": [], "entities": []}, {"text": "As the example illustrates and we will argue in section 6.2, default feedback does not necessarily mean the system missed a potentially relevant error, but can also mean that the default feedback is appropriate or the type of task does not lend itself well to formfocused feedback.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Quantitative evaluation results", "labels": [], "entities": [{"text": "Quantitative evaluation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8812907338142395}]}]}