{"title": [{"text": "Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss", "labels": [], "entities": [{"text": "Neural Fine-Grained Entity Type Classification", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7647491574287415}]}], "abstractContent": [{"text": "The task of Fine-grained Entity Type Classification (FETC) consists of assigning types from a hierarchy to entity mentions in text.", "labels": [], "entities": [{"text": "Fine-grained Entity Type Classification (FETC)", "start_pos": 12, "end_pos": 58, "type": "TASK", "confidence": 0.7164756613118308}]}, {"text": "Existing methods rely on distant supervision and are thus susceptible to noisy labels that can be out-of-context or overly-specific for the training sentence.", "labels": [], "entities": []}, {"text": "Previous methods that attempt to address these issues do so with heuristics or with the help of hand-crafted features.", "labels": [], "entities": []}, {"text": "Instead, we propose an end-to-end solution with a neu-ral network model that uses a variant of cross-entropy loss function to handle out-of-context labels, and hierarchical loss normalization to cope with overly-specific ones.", "labels": [], "entities": []}, {"text": "Also, previous work solve FETC a multi-label classification followed by ad-hoc post-processing.", "labels": [], "entities": [{"text": "FETC", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.5382159948348999}]}, {"text": "In contrast , our solution is more elegant: we use public word embeddings to train a single-label that jointly learns representations for entity mentions and their context.", "labels": [], "entities": []}, {"text": "We show experimentally that our approach is robust against noise and consistently outperforms the state-of-the-art on established benchmarks for the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Fine-grained Entity Type Classification (FETC) aims at labeling entity mentions in context with one or more specific types organized in a hierarchy (e.g., actor as a subtype of artist, which in turn is a subtype of person).", "labels": [], "entities": [{"text": "Fine-grained Entity Type Classification (FETC)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7493223462785993}]}, {"text": "Fine-grained types help in many applications, including relation extraction (, question answering (), entity linking (), knowledge base completion () and entity recommendation ().", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.851632684469223}, {"text": "question answering", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8040046393871307}, {"text": "entity linking", "start_pos": 102, "end_pos": 116, "type": "TASK", "confidence": 0.7432294189929962}, {"text": "knowledge base completion", "start_pos": 121, "end_pos": 146, "type": "TASK", "confidence": 0.622217079003652}]}, {"text": "Because of the high cost in labeling large training corpora with fine-grained types, current FETC systems resort to distant supervision () and annotate mentions in the training corpus with all types associated with the entity in a knowledge graph.", "labels": [], "entities": []}, {"text": "This is illustrated in, with three training sentences about entity Steve Kerr.", "labels": [], "entities": []}, {"text": "Note that while the entity belongs to three fine-grained types (person, athlete, and coach), some sentences provide evidence of only some of the types: person and coach from S1, person and athlete from S2, and just person for S3.", "labels": [], "entities": []}, {"text": "Clearly, direct distant supervision leads to noisy training data which can hurt the accuracy of the FETC model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9994351267814636}, {"text": "FETC", "start_pos": 100, "end_pos": 104, "type": "DATASET", "confidence": 0.4353342056274414}]}, {"text": "One kind of noise introduced by distant supervision is assigning labels that are out-of-context (athlete in S1 and coach in S2) for the sentence.", "labels": [], "entities": []}, {"text": "Current FETC systems sidestep the issue by either ignoring out-of-context labels or using simple pruning heuristics like discarding training examples with entities assigned to multiple types in the knowledge graph.", "labels": [], "entities": []}, {"text": "However, both strategies are inelegant and hurt accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9827954173088074}]}, {"text": "Another source of noise introduced by distant supervision is when the type is overly-specific for the context.", "labels": [], "entities": []}, {"text": "For instance, example S3 does not support the inference that Mr. Kerr is either an athlete or a coach.", "labels": [], "entities": [{"text": "Mr. Kerr", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.8683788478374481}]}, {"text": "Since existing knowledge graphs give more attention to notable entities with more specific types, overly-specific labels bias the model towards popular subtypes instead of generic ones, i.e., preferring athlete over person.", "labels": [], "entities": []}, {"text": "Instead of correcting for this bias, most existing FETC systems ignore the problem and treat each type equally and independently, ignoring that many types are semantically related.", "labels": [], "entities": []}, {"text": "Besides failing to handle noisy training data there are two other limitations of previous FETC approaches we seek to address.", "labels": [], "entities": []}, {"text": "First, they rely on hand-crafted features derived from various NLP tools; therefore, the inevitable errors introduced by these tools propagate to the FETC systems via the training data.", "labels": [], "entities": [{"text": "FETC", "start_pos": 150, "end_pos": 154, "type": "DATASET", "confidence": 0.8555072546005249}]}, {"text": "Second, previous systems treat FETC as a multi-label classification problem: during type inference they predict a plausibility score for each type, and, then, either classify types with scores above a threshold ( or perform a top-down search in the given type hierarchy (.", "labels": [], "entities": [{"text": "FETC", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.658922016620636}, {"text": "multi-label classification", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7399152219295502}]}, {"text": "Contributions: We propose a neural network based model to overcome the drawbacks of existing FETC systems mentioned above.", "labels": [], "entities": []}, {"text": "With publicly available word embeddings as input, we learn two different entity representations and use bidirectional long-short term memory (LSTM) with attention to learn the context representation.", "labels": [], "entities": []}, {"text": "We propose a variant of cross entropy loss function to handle out-of-context labels automatically during the training phase.", "labels": [], "entities": []}, {"text": "Also, we introduce hierarchical loss normalization to adjust the penalties for correlated types, allowing our model to understand the type hierarchy and alleviate the negative effect of overly-specific labels.", "labels": [], "entities": []}, {"text": "Moreover, in order to simplify the problem and take advantage of previous research on hierarchical classification, we transform the multi-label classification problem to a single-label classification problem.", "labels": [], "entities": [{"text": "hierarchical classification", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.7092812061309814}, {"text": "multi-label classification", "start_pos": 132, "end_pos": 158, "type": "TASK", "confidence": 0.7088750302791595}]}, {"text": "Based on the assumption that each mention can only have one type-path depending on the context, we leverage the fact that type hierarchies are forests, and represent each type-path uniquely by the terminal type (which might not be a leaf node).", "labels": [], "entities": []}, {"text": "For Example, type-path rootperson-coach can be represented as just coach, while root-person can be unambiguously represented as the non-leaf person.", "labels": [], "entities": []}, {"text": "Finally, we report on an experimental validation against the state-of-the-art on established benchmarks that shows that our model can adapt to noise in training data and consistently outperform previous methods.", "labels": [], "entities": []}, {"text": "In summary, we describe a single, much simpler and more elegant neural network model that attempts FETC \"end-to-end\" without post-processing or ad-hoc features and improves on the state-of-the-art for the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports an experimental evaluation of our NFETC approach using the previous state-ofthe-art as baselines.", "labels": [], "entities": []}, {"text": "We evaluate the proposed model on two standard and publicly available datasets, provided in a preprocessed tokenized format by. shows statistics about the benchmarks.", "labels": [], "entities": []}, {"text": "The details are as follows: \u2022 FIGER(GOLD): The training data consists of Wikipedia sentences and was automatically generated with distant supervision, by mapping Wikipedia identifiers to Freebase ones.", "labels": [], "entities": [{"text": "FIGER(GOLD)", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.8238014280796051}]}, {"text": "The test data, mainly consisting of sentences from news reports, was manually annotated as described by.", "labels": [], "entities": []}, {"text": "\u2022 OntoNotes: The OntoNotes dataset consists of sentences from newswire documents present in the OntoNotes text corpus (.", "labels": [], "entities": [{"text": "OntoNotes dataset", "start_pos": 17, "end_pos": 34, "type": "DATASET", "confidence": 0.8369787931442261}, {"text": "OntoNotes text corpus", "start_pos": 96, "end_pos": 117, "type": "DATASET", "confidence": 0.8337501287460327}]}, {"text": "DBpedia spotlight () was used to automatically link entity mention in sentences to Freebase.", "labels": [], "entities": [{"text": "DBpedia spotlight", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.8251076936721802}]}, {"text": "Manually annotated test data was shared by.", "labels": [], "entities": []}, {"text": "Because the type hierarchy can be somewhat understood by our proposed model, the quality of the type hierarchy can also be a key factor to the performance of our model.", "labels": [], "entities": []}, {"text": "We find that the type hierarchy for FIGER(GOLD) dataset following Freebase has some flaws.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9184250831604004}, {"text": "GOLD) dataset", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.6336784462134043}, {"text": "Freebase", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.49712902307510376}]}, {"text": "For example, software is not a subtype of product and government is not a subtype of organization.", "labels": [], "entities": []}, {"text": "Following the proposed type hierarchy of, we refine the Freebase-based type hierarchy.", "labels": [], "entities": [{"text": "Freebase-based type hierarchy", "start_pos": 56, "end_pos": 85, "type": "DATASET", "confidence": 0.9037929972012838}]}, {"text": "The process is a one-to-one mapping for types in the original dataset and we didn't add or drop any type or sentence in the original dataset.", "labels": [], "entities": []}, {"text": "As a result, we can directly compare the results of our proposed model with or without this refinement.", "labels": [], "entities": []}, {"text": "Aside from the advantages brought by adopting the single label classification setting, we can see one disadvantage of this setting based on Table 2.", "labels": [], "entities": []}, {"text": "That is, the performance upper bounds of our proposed model are no longer 100%: for example, the best strict accuracy we can get in this setting is 88.28% for FIGER(GOLD).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.8034905791282654}, {"text": "FIGER", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.9363200068473816}]}, {"text": "However, as the strict accuracy of state-of-the-art methods are still nowhere near 80%, the evaluation we perform is still informative.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9990506768226624}]}, {"text": "For evaluation metrics, we adopt the same criteria as, that is, we evaluate the model performance by strict accuracy, loose macro, and loose micro F-scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9979731440544128}]}, {"text": "These measures are widely used in existing FETC systems.", "labels": [], "entities": []}, {"text": "We use pre-trained word embeddings that were not updated during training to help the model generalize to words not appearing in the training set.", "labels": [], "entities": []}, {"text": "For this purpose, we used the freely available 300-dimensional cased word embedding trained on 840 billion tokens from the Common Crawl supplied by.", "labels": [], "entities": []}, {"text": "For both datasets, we randomly sampled 10% of the test set as a development set, on which we do the hyperparameters tuning.", "labels": [], "entities": []}, {"text": "The remaining 90% is used for final evaluation.", "labels": [], "entities": []}, {"text": "We run each model with the welltuned hyperparameter setting five times and report their average strict accuracy, macro F1 and micro F1 on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9492589235305786}, {"text": "F1", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.5647704601287842}, {"text": "F1", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.5299822688102722}]}, {"text": "The proposed model was implemented using the TensorFlow framework.", "labels": [], "entities": []}, {"text": "Parameter FIGER(GOLD) OntoNotes lr 0.0002 0.0002", "labels": [], "entities": [{"text": "Parameter", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.906330406665802}, {"text": "FIGER", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8116088509559631}, {"text": "GOLD) OntoNotes lr 0.0002 0.0002", "start_pos": 16, "end_pos": 48, "type": "METRIC", "confidence": 0.7060951789220175}]}], "tableCaptions": [{"text": " Table 2: Statistics of the datasets", "labels": [], "entities": []}]}