{"title": [{"text": "The Web as a Knowledge-base for Answering Complex Questions", "labels": [], "entities": [{"text": "Answering Complex Questions", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.9484613140424093}]}], "abstractContent": [{"text": "Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information.", "labels": [], "entities": [{"text": "Answering complex questions", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9106557567914327}]}, {"text": "Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge.", "labels": [], "entities": []}, {"text": "Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model.", "labels": [], "entities": []}, {"text": "We propose to decompose complex questions into a sequence of simple questions , and compute the final answer from the sequence of answers.", "labels": [], "entities": []}, {"text": "To illustrate the viability of our approach, we create anew dataset of complex questions, COMPLEXWEBQUES-TIONS, and present a model that decomposes questions and interacts with the web to compute an answer.", "labels": [], "entities": []}, {"text": "We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 preci-sion@1 on this new dataset.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.8519728183746338}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9893794655799866}]}], "introductionContent": [{"text": "Humans often want to answer complex questions that require reasoning over multiple pieces of evidence, e.g., \"From what country is the winner of the Australian Open women's singles 2008?\".", "labels": [], "entities": [{"text": "Australian Open women's singles 2008", "start_pos": 149, "end_pos": 185, "type": "DATASET", "confidence": 0.9369679689407349}]}, {"text": "Answering such questions in broad domains can be quite onerous for humans, because it requires searching and integrating information from multiple sources.", "labels": [], "entities": []}, {"text": "Recently, interest in question answering (QA) has surged in the context of reading comprehension, where an answer is sought fora question given one or more documents (: Given a complex questions q, we decompose the question to a sequence of simple questions q 1 , q 2 , . .", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.9038605928421021}]}, {"text": ", use a search engine and a QA model to answer the simple questions, from which we compute the final answer a.", "labels": [], "entities": []}, {"text": "Neural models trained overlarge datasets led to great progress in RC, nearing human-level performance ( . However, analysis of models revealed) that they mostly excel at matching questions to local contexts, but struggle with questions that require reasoning.", "labels": [], "entities": [{"text": "RC", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9803712964057922}]}, {"text": "Moreover, RC assumes documents with the information relevant for the answer are available -but when questions are complex, even retrieving the documents can be difficult.", "labels": [], "entities": [{"text": "RC", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9562364220619202}]}, {"text": "Conversely, work on QA through semantic parsing has focused primarily on compositionality: questions are translated to compositional programs that encode a sequence of actions for finding the answer in a knowledge-base (KB) ().", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7293149530887604}]}, {"text": "However, this reliance on a manually-curated KB has limited the coverage and applicability of semantic parsers.", "labels": [], "entities": []}, {"text": "In this paper we present a framework for QA that is broad, i.e., it does not assume information is in a KB or in retrieved documents, and compositional, i.e., to compute an answer we must perform some computation or reasoning.", "labels": [], "entities": []}, {"text": "Our thesis is that answering simple questions can be achieved by combining a search engine with a RC model.", "labels": [], "entities": []}, {"text": "Thus, answering complex questions can be addressed by decomposing the question into a sequence of simple questions, and computing the answer from the corresponding answers.", "labels": [], "entities": []}, {"text": "Our model decomposes the question in the figure into a sequence of simple questions, each is submitted to a search engine, and then an answer is extracted from the search result.", "labels": [], "entities": []}, {"text": "Once all answers are gathered, a final answer can be computed using symbolic operations such as union and intersection.", "labels": [], "entities": []}, {"text": "To evaluate our framework we need a dataset of complex questions that calls for reasoning over multiple pieces of information.", "labels": [], "entities": []}, {"text": "Because an adequate dataset is missing, we created COM-PLEXWEBQUESTIONS, anew dataset for complex questions that builds on WEBQUESTION-SSP, a dataset that includes pairs of simple questions and their corresponding SPARQL query.", "labels": [], "entities": []}, {"text": "We take SPARQL queries from WEBQUESTIONSSP and automatically create more complex queries that include phenomena such as function composition, conjunctions, superlatives and comparatives.", "labels": [], "entities": [{"text": "WEBQUESTIONSSP", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9253519773483276}]}, {"text": "Then, we use Amazon Mechanical Turk (AMT) to generate natural language questions, and obtain a dataset of 34,689 question-answer pairs (and also SPARQL queries that our model ignores).", "labels": [], "entities": []}, {"text": "Data analysis shows that examples are diverse and that AMT workers perform substantial paraphrasing of the original machine-generated question.", "labels": [], "entities": [{"text": "AMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9597419500350952}]}, {"text": "We propose a model for answering complex questions through question decomposition.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7003061920404434}]}, {"text": "Our model uses a sequence-to-sequence architecture) to map utterances to short programs that indicate how to decompose the question and compose the retrieved answers.", "labels": [], "entities": []}, {"text": "To obtain supervision for our model, we perform a noisy alignment from machine-generated questions to natural language questions and automatically generate noisy supervision for training.", "labels": [], "entities": []}, {"text": "We evaluate our model on COMPLEXWEBQUESTIONSand find that question decomposition substantially improves precision@1 from 20.8 to 27.5.", "labels": [], "entities": [{"text": "COMPLEXWEBQUESTIONSand", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.8509201407432556}, {"text": "question decomposition", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.8641672730445862}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.999151349067688}]}, {"text": "We find that humans are able to reach 63.0 precision@1 under a limited time budget, leaving ample room for improvement in future work.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9839662909507751}]}, {"text": "To summarize, our main contributions are: Figure 2: A computation tree for \"What city is the birthplace of the author of 'Without end', and hosted Euro 2012?\".", "labels": [], "entities": []}, {"text": "The leaves are strings, and inner nodes are functions (red) applied to their children to produce answers (blue).", "labels": [], "entities": []}, {"text": "1. A framework for answering complex questions through question decomposition.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.6989677995443344}]}, {"text": "2. A sequence-to-sequence model for question decomposition that substantially improves performance.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.8519827425479889}]}], "datasetContent": [{"text": "broad questions, along with answers, web snippets, and SPARQL queries.", "labels": [], "entities": []}, {"text": "Our dataset, COMPLEXWEBQUESTIONS, can be downloaded from http://nlp.cs.tau.", "labels": [], "entities": [{"text": "COMPLEXWEBQUESTIONS", "start_pos": 13, "end_pos": 32, "type": "DATASET", "confidence": 0.593468189239502}]}, {"text": "ac.il/compwebq and our codebase can be downloaded from https://github.com/ alontalmor/WebAsKB.", "labels": [], "entities": [{"text": "WebAsKB", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9547460675239563}]}, {"text": "Evaluating our framework requires a dataset of broad and complex questions that examine the importance of question decomposition.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7253424972295761}]}, {"text": "While many QA datasets have been developed recently (, they lack a focus on the importance of question decomposition.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.8041190207004547}, {"text": "question decomposition", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.7906274497509003}]}, {"text": "Most RC datasets contain simple questions that can be answered from a short input document.", "labels": [], "entities": []}, {"text": "Recently, TRIVIAQA () presented a larger portion of complex questions, but still most do not require reasoning.", "labels": [], "entities": [{"text": "TRIVIAQA", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.4234362244606018}]}, {"text": "Moreover, the focus of TRIVIAQA is on answer extraction from documents that are given.", "labels": [], "entities": [{"text": "TRIVIAQA", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.6303837895393372}, {"text": "answer extraction from documents", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.8961179405450821}]}, {"text": "We, conversely, highlight question decomposition for finding the relevant documents.", "labels": [], "entities": []}, {"text": "Put differently, RC is complementary to question decomposition and can be used as part of the implementation of SIMPQA.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.8295107483863831}]}, {"text": "In Sec-  tion 6 we demonstrate that question decomposition is useful for two different RC approaches.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.8685892820358276}]}, {"text": "To generate complex questions we use the dataset WEBQUESTIONSSP ( , which contains 4,737 questions paired with SPARQL queries for Freebase (.", "labels": [], "entities": [{"text": "WEBQUESTIONSSP", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.5426308512687683}]}, {"text": "Questions are broad but simple.", "labels": [], "entities": []}, {"text": "Thus, we sample question-query pairs, automatically create more complex SPARQL queries, generate automatically questions that are understandable to AMT workers, and then have them paraphrase those into natural language (similar to).", "labels": [], "entities": []}, {"text": "We compute answers by executing complex SPARQL queries against Freebase, and obtain broad and complex questions.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9536058306694031}]}, {"text": "provides an example for this procedure, and we elaborate next.", "labels": [], "entities": []}, {"text": "Generating SPARQL queries Given a SPARQL query r, we create four types of more complex queries: conjunctions, superlatives, comparatives, and compositions.", "labels": [], "entities": []}, {"text": "gives the exact rules for generation.", "labels": [], "entities": []}, {"text": "For conjunctions, superlatives, and comparatives, we identify queries in WEBQUESTIONSSP whose denotation is a set A, |A| \u2265 2, and generate anew query r whose denotation is a strict subset A , A \u2282 A, A = \u03c6.", "labels": [], "entities": [{"text": "WEBQUESTIONSSP", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.6822706460952759}]}, {"text": "For conjunctions this is done by traversing the KB and looking for SPARQL triplets that can be added and will yield a valid set A . For comparatives and superlatives we find a numerical property common to all a \u2208 A, and add a triplet and restrictor tor accordingly.", "labels": [], "entities": []}, {"text": "For compositions, we find an entity e in r, and replace e with a variable y and add tor a triplet such that the denotation of that triplet is {e}.", "labels": [], "entities": []}, {"text": "Machine-generated (MG) questions To have AMT workers paraphrase SPARQL queries into natural language, we need to present them in an understandable form.", "labels": [], "entities": [{"text": "AMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9709905982017517}]}, {"text": "Therefore, we automatically generate a question they can paraphrase.", "labels": [], "entities": []}, {"text": "When we generate new SPARQL queries, new predicates are added to the query.", "labels": [], "entities": []}, {"text": "We manually annotated 687 templates mapping KB predicates to text for different compositionality types (with 462 unique KB predicates), and use those templates to modify the original WebQuestionsSP question according to the meaning of the generated SPARQL query.", "labels": [], "entities": []}, {"text": "E.g., the template for ?x ns:book.author.works written obj is \"the author who wrote OBJ\".", "labels": [], "entities": [{"text": "OBJ", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8620408177375793}]}, {"text": "For brevity, we provide the details in the supplementary material.", "labels": [], "entities": []}, {"text": "Question Rephrasing We used AMT workers to paraphrase MG questions into natural language (NL).", "labels": [], "entities": [{"text": "Question Rephrasing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6924939304590225}]}, {"text": "Each question was paraphrased by one AMT worker and validated by 1-2 other workers.", "labels": [], "entities": [{"text": "AMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.8539870381355286}]}, {"text": "To generate diversity, workers got a bonus if the edit distance of a paraphrase was high compared to the MG question.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 50, "end_pos": 63, "type": "METRIC", "confidence": 0.8778105080127716}, {"text": "MG question", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.7514204382896423}]}, {"text": "A total of 200 workers were involved, and 34,689 examples were produced with an average cost of 0.11$ per question.", "labels": [], "entities": []}, {"text": "gives an example for each compositionality type.", "labels": [], "entities": []}, {"text": "A drawback of our method for generating data is that because queries are generated automatically the question distribution is artificial from a semantic perspective.", "labels": [], "entities": []}, {"text": "Still, developing models that are capable of reasoning is an important direction for natural language understanding and COM-PLEXWEBQUESTIONS provides an opportunity to develop and evaluate such models.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.6455679436524709}]}, {"text": "To summarize, each of our examples contains a question, an answer, a SPARQL query (that our models ignore), and all web snippets harvested by our model when attempting to answer the question.", "labels": [], "entities": []}, {"text": "This renders COMPLEXWEBQUESTIONS useful for both the RC and semantic parsing communities.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.6540585905313492}]}, {"text": "COMPLEXWEBQUESTIONS builds on the WE-BQUESTIONS (.", "labels": [], "entities": [{"text": "WE-BQUESTIONS", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.5263491272926331}]}, {"text": "Questions in WEBQUESTIONS are usually about properties of entities (\"What is the capital of France?\"), often with some filter for the semantic type of the answer (\"Which director\", \"What city\").", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.5698473453521729}]}, {"text": "WE-BQUESTIONS also contains questions that refer to events with multiple entities (\"Who did Brad Pitt play in Troy?\").", "labels": [], "entities": [{"text": "WE-BQUESTIONS", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.7284227609634399}]}, {"text": "COMPLEXWEBQUESTIONS contains all these semantic phenomena, but we add four compositionality types by generating composition questions (45% of the times), conjunctions (45%), superlatives (5%) and comparatives (5%).", "labels": [], "entities": [{"text": "COMPLEXWEBQUESTIONS", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.9318564534187317}]}, {"text": "In this section, we aim to examine whether question decomposition can empirically improve performance of QA models over complex questions.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.740363359451294}]}, {"text": "Experimental setup We used 80% of the examples in COMPLEXWEBQUESTIONS for training, 10% for development, and 10% for test, training the pointer network on 24,708 composition and conjunction examples.", "labels": [], "entities": [{"text": "COMPLEXWEBQUESTIONS", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.9212886095046997}]}, {"text": "The hidden state dimension of the pointer network is 512, and we used Adagrad () combined with L 2 regularization and a dropout rate of 0.25.", "labels": [], "entities": []}, {"text": "We initialize 50-dimensional word embeddings using GloVe and learn embeddings for missing words.", "labels": [], "entities": []}, {"text": "Simple QA model As our SIMPQA function, we download the web-based QA model of.", "labels": [], "entities": []}, {"text": "This model sends the question to Google's search engine and extracts a distribution over answers from the top-100 web snippets using manually-engineered features.", "labels": [], "entities": []}, {"text": "We re-train the model on our data with one new feature: for every question q and candidate answer mention in a snippet, we run RASOR, a RC model by, and add the output logit score as a feature.", "labels": [], "entities": [{"text": "RASOR", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9282420873641968}]}, {"text": "We found that combining the web-facing model of and RASOR, resulted in improved performance.", "labels": [], "entities": [{"text": "RASOR", "start_pos": 52, "end_pos": 57, "type": "TASK", "confidence": 0.5589672923088074}]}, {"text": "Evaluation For evaluation, we measure precision@1 (p@1), i.e., whether the highest scoring answer returned string-matches one of the correct answers (while answers are sets, 70% of the questions have a single answer, and the average size of the answer set is 2.3).", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9982910752296448}]}, {"text": "We evaluate the following models and oracles: with the the RC model DOCQA, whose performance is comparable to state-of-the-art on TRIVIAQA.", "labels": [], "entities": [{"text": "TRIVIAQA", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.7486816644668579}]}, {"text": "5. SPLITRCQA: This is identical to SPLITQA, except that we replace the RC model from Talmor et al. with DOCQA.", "labels": [], "entities": []}, {"text": "6. GOOGLEBOX: We sample 100 random development set questions and check whether Google returns a box that contains one of the correct answers.", "labels": [], "entities": [{"text": "GOOGLEBOX", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9444822669029236}]}, {"text": "7. HUMAN: We sample 100 random development set questions and manually answer the questions with Google's search engine, including all available information.", "labels": [], "entities": [{"text": "HUMAN", "start_pos": 3, "end_pos": 8, "type": "METRIC", "confidence": 0.9612542390823364}]}, {"text": "We limit the amount of time allowed for answering to 4 minutes.", "labels": [], "entities": [{"text": "answering", "start_pos": 40, "end_pos": 49, "type": "TASK", "confidence": 0.969054639339447}]}, {"text": "presents the results on the development and test sets.", "labels": [], "entities": []}, {"text": "SIMPQA, which does not decompose questions obtained 20.8 p@1, while by performing question decomposition we substantially improve performance to 27.5 p@1.", "labels": [], "entities": [{"text": "SIMPQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6194391846656799}, {"text": "question decomposition", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7072297036647797}]}, {"text": "An upper bound with perfect knowledge on when to decompose is given by SPLITQAORACLE at 33.7 p@1.", "labels": [], "entities": [{"text": "SPLITQAORACLE", "start_pos": 71, "end_pos": 84, "type": "METRIC", "confidence": 0.94298255443573}]}, {"text": "RCQA obtained lower performance SIMPQA, as it was trained on data from a different distribution.", "labels": [], "entities": [{"text": "RCQA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9312462210655212}]}, {"text": "More importantly SPLITRCQA outperforms RCQA by 3.4 points, illustrating that this RC model also benefits from question decomposition, despite the fact that it was not created with question decomposition in mind.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.8446062207221985}]}, {"text": "This shows the importance of question decomposition for retrieving documents from which an RC model can extract answers.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7833126485347748}]}, {"text": "GOOGLEBOX finds a correct answer in 2.5% of the cases, showing that complex questions are challenging for search engines.", "labels": [], "entities": [{"text": "GOOGLEBOX", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.6566193699836731}]}, {"text": "To conclude, we demonstrated that question decomposition substantially improves performance on answering complex questions using two independent RC models.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8482385873794556}]}, {"text": "Analysis We estimate human performance (HUMAN) at 63.0 p@1.", "labels": [], "entities": [{"text": "HUMAN)", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.8764683902263641}]}, {"text": "We find that answering complex questions takes roughly 1.3 minutes on average.", "labels": [], "entities": [{"text": "answering complex questions", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.8614117304484049}]}, {"text": "For questions we were unable to answer, we found that in 27% the answer was correct but exact string match with the gold answers failed; in 23.1% the time required to compute the answer was beyond our capabilities; for 15.4% we could not find an answer on the web; 11.5% were of ambiguous nature; 11.5% involved paraphrasing errors of AMT workers; and an additional 11.5% did not contain a correct gold answer.", "labels": [], "entities": []}, {"text": "SPLITQA decides if to decompose questions or not based on the confidence of SIMPQA.", "labels": [], "entities": [{"text": "SIMPQA", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.7711016535758972}]}, {"text": "In 61% of the questions the model chooses to decompose the question, and in the rest it sends the question as-is to the search engine.", "labels": [], "entities": []}, {"text": "If one of the strategies (decomposition vs. no decomposition) works, our model chooses that right one in 86% of the cases.", "labels": [], "entities": []}, {"text": "Moreover, in 71% of these answerable questions, only one strategy yields a correct answer.", "labels": [], "entities": []}, {"text": "We evaluate the ability of the pointer network to mimic our labeling heuristic on the development set.", "labels": [], "entities": []}, {"text": "We find that the model outputs the exact correct output sequence 60.9% of the time, and allowing errors of one word to the left and right (this often does not change the final output) accuracy is at 77.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9995717406272888}]}, {"text": "Token-level accuracy is 83.0% and allowing one-word errors 89.7%.", "labels": [], "entities": [{"text": "Token-level", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9168127179145813}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9021868109703064}]}, {"text": "This shows that SPLITQA learned to identify decomposition points in the questions.", "labels": [], "entities": [{"text": "SPLITQA", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.9366841316223145}]}, {"text": "We also observed that often SPLITQA produced decomposition points that are better than the heuristic, e.g., for \"What is the place of birth for the lyricist of Roman Holiday\", SPLITQA produced \"the lyricist of Roman Holiday\", but the heuristic produced \"the place of birth for the lyricist of Roman Holiday\".", "labels": [], "entities": []}, {"text": "Additional examples of SPLITQA question decompositions are provided in.", "labels": [], "entities": [{"text": "SPLITQA question decompositions", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.8678809205691019}]}, {"text": "ComplexQuestions To further examine the ability of web-based QA models, we run an experiment against COMPLEXQUESTIONS (, a small dataset of question-answer pairs designed for semantic parsing against Freebase.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 175, "end_pos": 191, "type": "TASK", "confidence": 0.7407826781272888}]}, {"text": "We ran SIMPQA on this dataset) and obtained 38.6 F 1 (the official metric), slightly lower than COMPQ, the best system, which op-Question Split-1 Split-2 \"Find the actress who played Hailey Rogers, \"the actress who played Hailey Rogers\" \"Find VAR , what label is she signed to\" what label is she signed to\" \"What are the colors of the sports team whose \"the sports team whose arena stadium \"What are the colors of VAR\" arena stadium is the AT&T Stadium\" is the AT&T Stadium\" \"What amusement park is located in Madrid \"What amusement park is located in \"park includes the stunt fall ride\" Spain and includes the stunt fall ride\" Madrid Spain and\" \"Which university whose mascot is \"Which university whose mascot is \"university Derek Fisher attend\" The Trojan did Derek Fisher attend\" The Trojan did\"  erates directly against Freebase.", "labels": [], "entities": [{"text": "F 1", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.8678294122219086}, {"text": "COMPQ", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9362699389457703}, {"text": "VAR", "start_pos": 243, "end_pos": 246, "type": "METRIC", "confidence": 0.9534213542938232}]}, {"text": "By analyzing the training data, we found that we can decompose COMP questions with a rule that splits the question when the words \"when\" or \"during\" appear, e.g., \"Who was vice president when JFK was president?\".", "labels": [], "entities": []}, {"text": "We decomposed questions with this rule and obtained 39.7 F 1 (SPLITQARULE).", "labels": [], "entities": [{"text": "39.7 F 1", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.8560200730959574}, {"text": "SPLITQARULE", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.941893994808197}]}, {"text": "Analyzing the development set errors, we found that occasionally SPLITQARULE returns a correct answer that fails to string-match with the gold answer.", "labels": [], "entities": []}, {"text": "By manually fixing these cases, our development set F 1 reaches 46.9 (SPLITQARULE++).", "labels": [], "entities": [{"text": "F 1", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.8982525169849396}, {"text": "SPLITQARULE", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.7642797231674194}]}, {"text": "Note that COMPQ does not suffer from any string matching issue, as it operates directly against the Freebase KB and thus is guaranteed to output the answer in the correct form.", "labels": [], "entities": [{"text": "string matching", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.7244338840246201}, {"text": "Freebase KB", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.9580110907554626}]}, {"text": "This short experiment shows that a web-based QA model can rival a semantic parser that works against a KB, and that simple question decomposition is beneficial and leads to results comparable to state-of-the-art.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.7264822125434875}]}], "tableCaptions": [{"text": " Table 4: precision@1 results on the development set  and test set for COMPLEXWEBQUESTIONS.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9989670515060425}, {"text": "COMPLEXWEBQUESTIONS", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.8107361793518066}]}, {"text": " Table 6: F 1 results for COMPLEXQUESTIONS.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9703037142753601}, {"text": "COMPLEXQUESTIONS", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.691861093044281}]}]}