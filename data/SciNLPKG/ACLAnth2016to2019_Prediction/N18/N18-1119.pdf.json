{"title": [{"text": "Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.7130328019460043}]}], "abstractContent": [{"text": "The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.853177676598231}]}, {"text": "Recent work, however, has introduced anew capability: lexically constrained or guided decoding , a modification to beam search that forces the inclusion of pre-specified words and phrases in the output.", "labels": [], "entities": []}, {"text": "However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (An-derson et al., 2017) in the number of constraints.", "labels": [], "entities": []}, {"text": "We present an algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints.", "labels": [], "entities": [{"text": "O", "start_pos": 80, "end_pos": 81, "type": "METRIC", "confidence": 0.9928984642028809}]}, {"text": "We demonstrate the algorithm's remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9973548650741577}]}, {"text": "Our implementation is available as part of SOCKEYE.", "labels": [], "entities": []}], "introductionContent": [{"text": "One appeal of the phrase-based statistical approach to machine translation ( was that it provided control over system output.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8014505505561829}]}, {"text": "For example, it was relatively easy to incorporate domain-specific dictionaries, or to force a translation choice for certain words.", "labels": [], "entities": []}, {"text": "These kinds of interventions were useful in a range of settings, including interactive machine translation or domain adaptation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7148012965917587}, {"text": "domain adaptation", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7127566337585449}]}, {"text": "In the new paradigm of neural machine translation (NMT), these kinds of manual interventions are much more difficult, and a lot of time has been spent investigating how to restore them (cf.).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.8462297717730204}]}, {"text": "At the same time, NMT has also provided new capabilities.", "labels": [], "entities": [{"text": "NMT", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.9213266372680664}]}, {"text": "One interesting recent innovation is lexically constrained decoding, a modification to beam search that allows the user to specify words No one has the intention of building a wall.", "labels": [], "entities": []}, {"text": "Keiner hat die Absicht, eine Mauer zu errichten.", "labels": [], "entities": [{"text": "Absicht", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.7013294696807861}]}, {"text": "\"No one has the intention, a wall to construct.\": An example translating from English to German.", "labels": [], "entities": []}, {"text": "The first translation is unconstrained, whereas the remaining ones have one or two constraints imposed.", "labels": [], "entities": []}, {"text": "A word-for-word translation of the German output has been provided for the convenience of nonGerman speaking readers. and phrases that must appear in the system output ().", "labels": [], "entities": []}, {"text": "Two algorithms have been proposed for this: grid beam search (Hokamp and Liu, 2017, GBS) and constrained beam search.", "labels": [], "entities": [{"text": "grid beam search", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.732117235660553}, {"text": "GBS", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.6625428199768066}, {"text": "constrained beam search", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.5958020091056824}]}, {"text": "These papers showed that these algorithms do a good job automatically placing constraints and improving results in tasks such as simulated post-editing, domain adaptation, and caption generation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 153, "end_pos": 170, "type": "TASK", "confidence": 0.7940983474254608}, {"text": "caption generation", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.9397963583469391}]}, {"text": "A downside to these algorithms is their runtime complexity: linear (GBS) or exponential (CBS) in the number of constraints.", "labels": [], "entities": [{"text": "exponential (CBS)", "start_pos": 76, "end_pos": 93, "type": "METRIC", "confidence": 0.801298052072525}]}, {"text": "Neither paper reported decoding speeds, but the complexities alone suggest a large penalty in runtime.", "labels": [], "entities": []}, {"text": "Beyond this, other factors of these approaches (a variable sized beam, finite-state machinery) change the decoding procedure such that it is difficult to integrate with other operations known to increase throughput, like batch decoding.", "labels": [], "entities": []}, {"text": "We propose and evaluate anew algorithm, dynamic beam allocation (DBA), that is constant in the number of provided constraints).", "labels": [], "entities": [{"text": "dynamic beam allocation (DBA)", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.744745800892512}]}, {"text": "Our algorithm works by grouping together hypotheses that have met the same number of constraints into work complexity O(N k2 C ) Hokamp and Liu O(N kC) This work O(N k): Complexity of decoding (sentence length N , beam size k, and constraint count C) with target-side constraints under various approaches.", "labels": [], "entities": []}, {"text": "banks (similar in spirit to the grouping of hypotheses into stacks for phrase-based decoding () and dynamically dividing a fixed-size beam across these banks at each time step.", "labels": [], "entities": []}, {"text": "As a result, the algorithm scales easily to large constraint sets that can be created when words and phrases are expanded, for example, by sub-word processing such as BPE (.", "labels": [], "entities": [{"text": "BPE", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.7590044140815735}]}, {"text": "We compare it to GBS and demonstrate empirically that it is significantly faster, making constrained decoding with an arbitrary number of constraints feasible with GPU-based inference.", "labels": [], "entities": [{"text": "GBS", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.7798306345939636}]}, {"text": "We also use the algorithm to study beam search interactions between model and metric scores, beam size, and pruning.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were done using SOCKEYE (. We used an English-German model trained on the complete WMT'17 training corpora), which we pre-Algorithm 2 k-best extraction with DBA.", "labels": [], "entities": [{"text": "SOCKEYE", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.7574846148490906}, {"text": "WMT'17 training corpora", "start_pos": 99, "end_pos": 122, "type": "DATASET", "confidence": 0.843039333820343}, {"text": "DBA", "start_pos": 173, "end_pos": 176, "type": "METRIC", "confidence": 0.8975977301597595}]}, {"text": "Inputs: A k \u00d7 |V T | matrix of model states.", "labels": [], "entities": []}, {"text": "For decoding, we normalize completed hypotheses (those that have generated /s), dividing the cumulative sentence score by the number of words.", "labels": [], "entities": []}, {"text": "Unless otherwise noted, we apply threshold pruning to the beam, removing hypotheses whose log probability is not within 20 compared to the best completed hypothesis.", "labels": [], "entities": []}, {"text": "This pruning is applied to all hypotheses, whether they are complete or not.", "labels": [], "entities": []}, {"text": "(We explore the importance of this pruning in \u00a76.3).", "labels": [], "entities": []}, {"text": "Decoding stops when either all hypotheses still on the beam are completed or the maximum length, N , is reached.", "labels": [], "entities": [{"text": "Decoding", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.7367825508117676}]}, {"text": "All experiments were run on a single a Volta P100 GPU.", "labels": [], "entities": [{"text": "Volta P100 GPU", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.8512941797574362}]}, {"text": "No ensembling or batching were used.", "labels": [], "entities": [{"text": "batching", "start_pos": 17, "end_pos": 25, "type": "TASK", "confidence": 0.8993628025054932}]}, {"text": "For experiments, we used the newstest2014 English-German test set (the developer version, with 2,737 sentences).", "labels": [], "entities": [{"text": "newstest2014 English-German test set", "start_pos": 29, "end_pos": 65, "type": "DATASET", "confidence": 0.8744082003831863}]}, {"text": "All BLEU scores are computed on detokenized output using SACREBLEU (Post, 2018), 1 and are thus directly comparable to scores reported in the WMT evaluations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9986439347267151}, {"text": "SACREBLEU (Post, 2018)", "start_pos": 57, "end_pos": 79, "type": "DATASET", "confidence": 0.795898417631785}, {"text": "WMT", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.7244632840156555}]}, {"text": "We center our exploration of DBA by experimenting with constraints randomly selected from the references.", "labels": [], "entities": [{"text": "DBA", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.858777642250061}]}, {"text": "We extract five sets of constraints: from one to four randomly selected words from the reference (rand1 to rand4), and a randomly selected four-word phrase (phr4).", "labels": [], "entities": []}, {"text": "We then apply BPE to these sets, which often yields a much larger number of token constraints.", "labels": [], "entities": [{"text": "BPE", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9339219331741333}]}, {"text": "Statistics about these extracted phrases can be found in.", "labels": [], "entities": []}, {"text": "We simulate the GBS baseline within our framework.", "labels": [], "entities": [{"text": "GBS baseline", "start_pos": 16, "end_pos": 28, "type": "DATASET", "confidence": 0.8719042241573334}]}, {"text": "After applying BPE, We group together translations with the same number of constraints, C, and then translate them as a group, with the beam set for that group set to b(C + 1), where b is the \"base beam\" parameter.", "labels": [], "entities": [{"text": "BPE", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.6378341913223267}]}, {"text": "We use b = 10 as reported in Hokamp et al., but also try smaller values of b = 5 and 1.", "labels": [], "entities": []}, {"text": "Finally, we disable beam adjustment ( \u00a73.2), so that the space allocated to each constraint bank does not change.", "labels": [], "entities": [{"text": "beam adjustment", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8403584361076355}]}, {"text": "BPE constraints for the rand3 dataset.", "labels": [], "entities": [{"text": "BPE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8636015057563782}, {"text": "rand3 dataset", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9213002622127533}]}, {"text": "We plot all points for which there were at least 10 sentences.", "labels": [], "entities": []}, {"text": "The times are decoding only, and exclude model loading and other setup.", "labels": [], "entities": []}, {"text": "The linear trend in C is clear for GBS, as is the constant trend for DBA.", "labels": [], "entities": [{"text": "GBS", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.7596761584281921}, {"text": "DBA", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.8713250756263733}]}, {"text": "In terms of absolute runtimes, DBA improves considerably over GBS, whose beam sizes quickly become quite large with a non-unit base beam size.", "labels": [], "entities": [{"text": "GBS", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8515913486480713}]}, {"text": "On the Tesla V100 GPU, DBA (k = 10) takes about 0.6 seconds/sentence, regardless of the number of constraints.", "labels": [], "entities": [{"text": "Tesla V100 GPU", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.7476710677146912}, {"text": "DBA", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9932510256767273}]}, {"text": "2 This is about 3x slower than unconstrained decoding.", "labels": [], "entities": []}, {"text": "It is difficult to compare these algorithms exactly because of GBS's variable beam size.", "labels": [], "entities": [{"text": "GBS", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.8574804663658142}]}, {"text": "An important comparison is that between DBA (k = 10) and GBS/1 (base beam of 1).", "labels": [], "entities": [{"text": "DBA", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.7546136379241943}, {"text": "GBS/1", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.7988335688908895}]}, {"text": "A beam of k = 10 is a common setting for decoding in general, and GBS/1 has abeam size of k \u2265 10 for C \u2265 9.", "labels": [], "entities": [{"text": "GBS/1", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.5516132116317749}]}, {"text": "At this setting, DBA finds better translations (BLEU 26.7 vs. 25.6) with the same runtime and with a fixed, instead of variable-sized, beam.", "labels": [], "entities": [{"text": "DBA", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.7140483260154724}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9981470108032227}]}, {"text": "We note that the bank adjustment correction of the DBA algorithm allows it to work when C >= k.", "labels": [], "entities": [{"text": "bank adjustment correction", "start_pos": 17, "end_pos": 43, "type": "METRIC", "confidence": 0.8539148966471354}]}, {"text": "The DBA (k = 5) plot demonstrates this, while still finding away to increase the BLEU score over GBS (23.5 vs. 22.3).", "labels": [], "entities": [{"text": "DBA", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9873608350753784}, {"text": "BLEU score", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9836696982383728}, {"text": "GBS", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9850550889968872}]}, {"text": "However, while possible, low k relative to C reduces the observed improvement considerably.", "labels": [], "entities": []}, {"text": "Looking at across different constraint sets, we can get a better feel for this relationship.", "labels": [], "entities": []}, {"text": "DBA is still always able to meet the constraints even with abeam size of 5, On a K80, it is about 1.4 seconds / sentence  but the quality suffers.", "labels": [], "entities": [{"text": "DBA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8257994055747986}]}, {"text": "This should not be too surprising; correctly placing independent constraints is at least as hard as finding their correct permutation, which is exponential in the number of independent constraints.", "labels": [], "entities": []}, {"text": "But it is remarkable that the only failure to beat the baseline in terms of BLEU is when the algorithm is tasked with placing four random constraints (before BPE) with abeam size of 5.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9990295171737671}, {"text": "BPE", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.9692110419273376}]}, {"text": "In contrast, DBA never has any trouble placing phrasal constraints (dashed lines).", "labels": [], "entities": [{"text": "DBA", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.7298455834388733}]}, {"text": "The main contribution of this paper is a fast, practical algorithmic improvement to lexicallyconstrained decoding.", "labels": [], "entities": []}, {"text": "While we did not attempt to corroborate the experiments in interactive translation and domain adaptation experiments reported in, the gains discovered there only become more salient with this faster algorithm.", "labels": [], "entities": [{"text": "interactive translation and domain adaptation", "start_pos": 59, "end_pos": 104, "type": "TASK", "confidence": 0.5647361636161804}]}, {"text": "We did try to apply lexical constraints in a few other settings, but without success.", "labels": [], "entities": []}, {"text": "In the spirit of open scientific inquiry and reporting, we provide here a brief report on these experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Histogram of the number of token constraints  for some constraint sets after applying BPE (model  trained with 32k merge operations). mean denotes the  mean number of constraints per sentence in the 2,737- sentence test set.", "labels": [], "entities": [{"text": "BPE", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.6958557963371277}]}, {"text": " Table 3: BLEU scores decoding with a beam size of 10.  Runtimes for unpruned systems (column 0) are nearly  twice those of the other columns. But it is only at large  thresholds that BLEU scores are higher than the un- pruned setting.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9974244832992554}, {"text": "Runtimes", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9716416597366333}, {"text": "BLEU", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.9986463189125061}]}]}