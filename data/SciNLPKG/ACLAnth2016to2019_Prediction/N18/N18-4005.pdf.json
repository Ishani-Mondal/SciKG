{"title": [{"text": "Towards Qualitative Word Embeddings Evaluation: Measuring Neighbors Variation", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a method to study the variation lying between different word embeddings models trained with different parameters.", "labels": [], "entities": []}, {"text": "We explore the variation between models trained with only one varying parameter by observing the distributional neighbors variation and show how changing only one parameter can have a massive impact on a given semantic space.", "labels": [], "entities": []}, {"text": "We show that the variation is not affecting all words of the semantic space equally.", "labels": [], "entities": []}, {"text": "Variation is influenced by parameters such as setting a parameter to its minimum or maximum value but it also depends on the corpus intrinsic features such as the frequency of a word.", "labels": [], "entities": [{"text": "Variation", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.8996928930282593}]}, {"text": "We identify semantic classes of words remaining stable across the models trained and specific words having high variation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings are widely used nowadays in Distributional Semantics and fora variety of tasks in NLP.", "labels": [], "entities": [{"text": "Distributional Semantics", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.8882188498973846}]}, {"text": "Embeddings can be evaluated using extrinsic evaluation methods, i.e. the trained embeddings are evaluated on a specific task such as part-of-speech tagging or named-entity recognition (.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.7035977244377136}, {"text": "named-entity recognition", "start_pos": 159, "end_pos": 183, "type": "TASK", "confidence": 0.6976366341114044}]}, {"text": "Because this type of evaluation is expensive, time consuming and difficult to interpret, embeddings are often evaluated using intrinsic evaluation methods such as word similarity or analogy.", "labels": [], "entities": []}, {"text": "Such methods of evaluation area good way to get a quick insight of the quality of a model.", "labels": [], "entities": []}, {"text": "Many different techniques and parameters can be used to train embeddings and benchmarks are used to select and tune embeddings parameters.", "labels": [], "entities": []}, {"text": "Benchmarks used to evaluate embeddings only focus on a subset of the trained model by only evaluating selected pairs of words.", "labels": [], "entities": []}, {"text": "Thus, they lack information about the overall structure of the semantic space and do not provide enough information to understand the impact of changing one parameter when training a model.", "labels": [], "entities": []}, {"text": "We want to know if some parameters have more influence than others on the global structure of embeddings models and get a better idea of what varies from one model to another.", "labels": [], "entities": []}, {"text": "We specifically investigate the impact of the architecture, the corpus, the window size, the vectors dimensions and the context type when training embeddings.", "labels": [], "entities": []}, {"text": "We analyze to what extent training models by changing only one of these parameters has an impact on the models created and if the different areas of the lexicon are impacted the same by this change.", "labels": [], "entities": []}, {"text": "To do so, we provide a qualitative methodology focusing on the global comparison of semantic spaces based on the overlap of the N nearest neighbors fora given word.", "labels": [], "entities": []}, {"text": "The proposed method is not bound to the subjectivity of benchmarks and gives a global yet precise vision of the variation between different models by evaluating each word from the model.", "labels": [], "entities": []}, {"text": "It provides away to easily investigate selected areas, by observing the variation of a word or of selected subsets of words.", "labels": [], "entities": []}, {"text": "We compare 19 word embedding models to a default model.", "labels": [], "entities": []}, {"text": "All models are trained using the well-known word2vec.", "labels": [], "entities": []}, {"text": "Using the parameters of the default model, we train the other models by changing the value of only one parameter at a time.", "labels": [], "entities": []}, {"text": "We first get some insights by performing a quantitative evaluation using benchmark test sets.", "labels": [], "entities": []}, {"text": "We then proceed to a qualitative evaluation by observing the differences between the default model and every other model.", "labels": [], "entities": []}, {"text": "This allows us to measure the impact of each parameter on the global variation as well as to detect phenomena that were not visible when evaluating only with benchmark test sets.", "labels": [], "entities": []}, {"text": "We also identify some preliminary features for words remaining stable independently of the parameters used for training.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we use a DEFAULT model as a basis of comparison.", "labels": [], "entities": [{"text": "DEFAULT", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.7997292280197144}]}, {"text": "Starting from this model, we trained new models by changing only one parameter at a time among the following parameters: architecture, corpus, window size, vectors dimensions, context type.", "labels": [], "entities": []}, {"text": "We thus trained 19 models which will all be compared to the DEFAULT model.", "labels": [], "entities": [{"text": "DEFAULT", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.47529706358909607}]}, {"text": "Although we compare less models and less parameters than other studies conducted on the evaluation of hyperparameters, we provide both a global and precise evaluation by computing the variation for each word of the model rather than evaluating selected pairs of words.", "labels": [], "entities": []}, {"text": "To get an overview of the different models performance we first ran a partial quantitative evalu- ation.", "labels": [], "entities": []}, {"text": "We used the toolkit 3 provided by.", "labels": [], "entities": []}, {"text": "This toolkit provides several benchmarks to test against the trained vectors.", "labels": [], "entities": []}, {"text": "The evaluation is computed by ranking the different cosine scores obtained for each pair of the chosen dataset.", "labels": [], "entities": []}, {"text": "The evaluation was run on), two benchmarks commonly used for DSMs evaluation (e.g. see;).", "labels": [], "entities": [{"text": "DSMs evaluation", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.8917923271656036}]}, {"text": "shows the performance of the different models on both test sets as well as the confidence interval for the DEFAULT model.", "labels": [], "entities": [{"text": "confidence interval", "start_pos": 79, "end_pos": 98, "type": "METRIC", "confidence": 0.9541142284870148}, {"text": "DEFAULT", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.7523677945137024}]}, {"text": "We see that changing parameters creates differences in models performance and that this difference is generally not significant.", "labels": [], "entities": []}, {"text": "Changing the architecture from Skip-gram to CBOW yields worse results for WordSim-353 than changing the corpus used for training.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.949476957321167}]}, {"text": "However, when testing on SimLex-999, performance is similar for the DEFAULT and CBOW models, while the ACL model performed worse.", "labels": [], "entities": [{"text": "DEFAULT", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.7196034789085388}]}, {"text": "Ina similar way, changing the training corpus gives better result on WordSim-353 than using a different type of contexts, as shown per the results of DEPS and DEPS+.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.9658181667327881}, {"text": "DEPS", "start_pos": 150, "end_pos": 154, "type": "DATASET", "confidence": 0.9351693987846375}]}, {"text": "Performance is not consistent between the two benchmarks.", "labels": [], "entities": []}, {"text": "DEPS and DEPS+ both yields the worst performance on WordSim-353 but at the same time their performance on SimLex-999 is better than most other models.", "labels": [], "entities": [{"text": "DEPS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7175079584121704}, {"text": "WordSim-353", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.9702966809272766}]}, {"text": "The same is true for the WIN1 and WIN10 models.", "labels": [], "entities": [{"text": "WIN1", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.944539487361908}, {"text": "WIN10", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.8173009753227234}]}, {"text": "Increasing the vector dimensions gets slightly better performance, independently of the benchmark used.", "labels": [], "entities": []}, {"text": "Increasing the window size gives better performance results for WordSim-353 but worse for SimLex-999.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.9546265602111816}]}, {"text": "Dependency-based models performs the worst on This kind of evaluation is only performed on selected pairs of words and despite small differences in performance scores, larger differences may exist.", "labels": [], "entities": []}, {"text": "In the next section we introduce a method that quantifies the variation between the different models trained by evaluating the distributional neighbors variation for every word in the corpus.", "labels": [], "entities": []}, {"text": "shows the mean variation score with the standard deviation span between the DEFAULT model and the 19 other models . Since it is known there is inherent variation when training embeddings, we measured the variation across 5 models using word2vec default settings.", "labels": [], "entities": [{"text": "mean variation score", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9138088822364807}, {"text": "DEFAULT model", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.8296512067317963}]}, {"text": "This variation is much lower than for the other models (0.17).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Parameters values used to train embed- dings that are compared.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.964852511882782}]}]}