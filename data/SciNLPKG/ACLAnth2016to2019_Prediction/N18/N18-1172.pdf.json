{"title": [{"text": "Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces", "labels": [], "entities": [{"text": "Multi-task Learning of Pairwise Sequence Classification Tasks", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.6301323005131313}]}], "abstractContent": [{"text": "We combine multi-task learning and semi-supervised learning by inducing a joint embedding space between disparate label spaces and learning transfer functions between label em-beddings, enabling us to jointly leverage un-labelled data and auxiliary, annotated datasets.", "labels": [], "entities": []}, {"text": "We evaluate our approach on a variety of sequence classification tasks with disparate label spaces.", "labels": [], "entities": [{"text": "sequence classification tasks", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.7597542107105255}]}, {"text": "We outperform strong single and multi-task baselines and achieve anew state-of-the-art for topic-based sentiment analysis.", "labels": [], "entities": [{"text": "topic-based sentiment analysis", "start_pos": 91, "end_pos": 121, "type": "TASK", "confidence": 0.7386043270428976}]}], "introductionContent": [{"text": "Multi-task learning (MTL) and semi-supervised learning are both successful paradigms for learning in scenarios with limited labelled data and have in recent years been applied to almost all areas of NLP.", "labels": [], "entities": [{"text": "Multi-task learning (MTL)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8128749132156372}]}, {"text": "Applications of MTL in NLP, for example, include partial parsing), text normalisation (, neural machine translation (, and keyphrase boundary classification.", "labels": [], "entities": [{"text": "partial parsing", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.6525033712387085}, {"text": "text normalisation", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8449290990829468}, {"text": "neural machine translation", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.6610701282819113}, {"text": "keyphrase boundary classification", "start_pos": 123, "end_pos": 156, "type": "TASK", "confidence": 0.7106921672821045}]}, {"text": "Contemporary work in MTL for NLP typically focuses on learning representations that are useful across tasks, often through hard parameter sharing of hidden layers of neural networks.", "labels": [], "entities": [{"text": "MTL", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.955852210521698}]}, {"text": "If tasks share optimal hypothesis classes at the level of these representations, MTL leads to improvements).", "labels": [], "entities": [{"text": "MTL", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.5795606970787048}]}, {"text": "However, while sharing hidden layers of neural networks is an effective regulariser , we potentially loose synergies between the classification functions trained to associate these representations with class labels.", "labels": [], "entities": []}, {"text": "This paper sets out to build an architecture in which such synergies are exploited, The first two authors contributed equally. with an application to pairwise sequence classification tasks.", "labels": [], "entities": [{"text": "pairwise sequence classification tasks", "start_pos": 150, "end_pos": 188, "type": "TASK", "confidence": 0.6772621124982834}]}, {"text": "Doing so, we achieve anew state of the art on topic-based sentiment analysis.", "labels": [], "entities": [{"text": "topic-based sentiment analysis", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.6833062867323557}]}, {"text": "For many NLP tasks, disparate label sets are weakly correlated, e.g. part-of-speech tags correlate with dependencies (, sentiment correlates with emotion, etc.", "labels": [], "entities": []}, {"text": "We thus propose to induce a joint label embedding space (visualised in) using a Label Embedding Layer that allows us to model these relationships, which we show helps with learning.", "labels": [], "entities": []}, {"text": "In addition, for tasks where labels are closely related, we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions.", "labels": [], "entities": []}, {"text": "To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-labels across tasks.", "labels": [], "entities": []}, {"text": "The LTN can be used to label unlabelled and auxiliary task data by utilising the 'dark knowledge') contained in auxiliary model predictions.", "labels": [], "entities": []}, {"text": "This pseudo-labelled data is then incorporated into the model via semisupervised learning, leading to a natural combination of multi-task learning and semi-supervised learning.", "labels": [], "entities": []}, {"text": "We additionally augment the LTN with data-specific diversity features) that aid in learning.", "labels": [], "entities": []}, {"text": "Contributions Our contributions are: a) We model the relationships between labels by inducing a joint label space for multi-task learning.", "labels": [], "entities": []}, {"text": "b) We propose a Label Transfer Network that learns to transfer labels between tasks and propose to use semi-supervised learning to leverage them for training.", "labels": [], "entities": [{"text": "Label Transfer", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7030954211950302}]}, {"text": "c) We evaluate MTL approaches on a variety of classification tasks and shed new light on settings where multi-task learning works.", "labels": [], "entities": [{"text": "MTL", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9714859127998352}]}, {"text": "d) We perform an extensive ablation study of our model.", "labels": [], "entities": []}, {"text": "e) We report state-of-the-art performance on topicbased sentiment analysis.", "labels": [], "entities": [{"text": "topicbased sentiment analysis", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.669626255830129}]}], "datasetContent": [{"text": "For our experiments, we evaluate on a wide range of text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.8280929327011108}]}, {"text": "In particular, we choose pairwise classification tasks-i.e. those that condition the reading of one sequence on another sequence-as we are interested in understanding if knowledge can be transferred even for these more complex interactions.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work on transfer learning between such pairwise sequence classification tasks.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.9223696887493134}, {"text": "pairwise sequence classification tasks", "start_pos": 87, "end_pos": 125, "type": "TASK", "confidence": 0.6997881084680557}]}, {"text": "We implement all our models in Tensorflow ( and release the code at https://github.com/ coastalcph/mtl-disparate.", "labels": [], "entities": []}, {"text": "We use the following tasks and datasets for our experiments, show task statistics in, and summarise examples in: Topic-based sentiment analysis Topic-based sentiment analysis aims to estimate the sentiment of a tweet known to be about a given topic.", "labels": [], "entities": [{"text": "Topic-based sentiment analysis Topic-based sentiment analysis", "start_pos": 113, "end_pos": 174, "type": "TASK", "confidence": 0.8143006215492884}]}, {"text": "We use the data from SemEval-2016 Task 4 Subtask B and C (Nakov et al., 2016) for predicting on a twopoint scale of positive and negative (Topic-2) and five-point scale ranging from highly negative to highly positive (Topic-5) respectively.", "labels": [], "entities": []}, {"text": "An example from this dataset would be to classify the: Example instances from the datasets described in Section 4.1.", "labels": [], "entities": []}, {"text": "tweet \"No power at home, satin the dark listening to AC/DC in the hope it'll make the electricity comeback again\" known to be about the topic \"AC/DC\", which is labelled as a positive sentiment.", "labels": [], "entities": []}, {"text": "The evaluation metrics for Topic-2 and Topic-5 are macro-averaged recall (\u03c1 P N ) and macro-averaged mean absolute error (M AE M ) respectively, which are both averaged across topics.", "labels": [], "entities": [{"text": "recall (\u03c1 P N )", "start_pos": 66, "end_pos": 81, "type": "METRIC", "confidence": 0.9555752774079641}, {"text": "macro-averaged mean absolute error (M AE M )", "start_pos": 86, "end_pos": 130, "type": "METRIC", "confidence": 0.8190576566590203}]}, {"text": "Target-dependent sentiment analysis Targetdependent sentiment analysis (Target) seeks to classify the sentiment of a text's author towards an entity that occurs in the text as positive, negative, or neutral.", "labels": [], "entities": [{"text": "Target-dependent sentiment analysis Targetdependent sentiment analysis (Target)", "start_pos": 0, "end_pos": 79, "type": "TASK", "confidence": 0.8204315172301399}]}, {"text": "We use the data from.", "labels": [], "entities": []}, {"text": "An example instance is the expression \"how do you like settlers of catan for the wii?\" which is labelled as neutral towards the target \"wii'.'", "labels": [], "entities": []}, {"text": "The evaluation metric is macroaveraged F 1 (F M 1 ).", "labels": [], "entities": [{"text": "F 1 (F M 1 )", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.8410560403551374}]}, {"text": "Aspect-based sentiment analysis Aspect-based sentiment analysis is the task of identifying whether an aspect, i.e. a particular property of an item is associated with a positive, negative, or neutral sentiment.", "labels": [], "entities": [{"text": "Aspect-based sentiment analysis Aspect-based sentiment analysis", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.8861078520615896}]}, {"text": "We use the data of SemEval-2016 Task 5 Subtask 1 Slot 3 (Pontiki et al., 2016) for the laptops (ABSA-L) and restaurants (ABSA-R) domains.", "labels": [], "entities": [{"text": "SemEval-2016 Task 5 Subtask 1 Slot 3", "start_pos": 19, "end_pos": 55, "type": "DATASET", "confidence": 0.6752197018691471}]}, {"text": "An example is the sentence \"For the price, you cannot eat this well in Manhattan\", labelled as positive towards both the aspects \"restaurant prices\" and \"food quality\".", "labels": [], "entities": []}, {"text": "The evaluation metric for both domains is accuracy (Acc).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9997058510780334}, {"text": "Acc)", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9266414046287537}]}, {"text": "Stance detection Stance detection (Stance) requires a model, given a text and a target entity, which might not appear in the text, to predict whether the author of the text is in favour or against the target or whether neither inference is likely ( . We use the data of).", "labels": [], "entities": [{"text": "Stance detection Stance detection (Stance)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8351362773350307}]}, {"text": "An example from this dataset would be to predict the stance of the tweet \"Be prepared -if we continue the policies of the liberal left, we will be #Greece\" towards the topic \"Donald Trump\", labelled as \"favor\".", "labels": [], "entities": []}, {"text": "The evaluation metric is the macro-averaged F 1 score of the \"favour\" and \"against\" classes (F F A 1 ).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9599887530008951}, {"text": "F F A 1 )", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.8940066576004029}]}, {"text": "Fake news detection The goal of fake news detection in the context of the Fake News Challenge 2 is to estimate whether the body of a news article agrees, disagrees, discusses, or is unrelated towards a headline.", "labels": [], "entities": [{"text": "Fake news detection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7697337865829468}, {"text": "fake news detection", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.6950357556343079}, {"text": "Fake News Challenge 2", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.7513116300106049}]}, {"text": "We use the data from the first stage of the Fake News Challenge (FNC-1).", "labels": [], "entities": [{"text": "Fake News Challenge (FNC-1)", "start_pos": 44, "end_pos": 71, "type": "DATASET", "confidence": 0.7830324172973633}]}, {"text": "An example for this dataset is the document \"Dino Ferrari hooked the whopper wels catfish, (...), which could be the biggest in the world.\" with the headline \"Fisherman lands 19 STONE catfish which could be the biggest in the world to be hooked\" labelled as \"agree\".", "labels": [], "entities": [{"text": "STONE", "start_pos": 178, "end_pos": 183, "type": "METRIC", "confidence": 0.502357542514801}, {"text": "agree", "start_pos": 259, "end_pos": 264, "type": "METRIC", "confidence": 0.9815882444381714}]}, {"text": "The evaluation metric is accuracy (Acc) 3 . Natural language inference Natural language inference is the task of predicting whether one sentences entails, contradicts, or is neutral towards another one.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.99945467710495}, {"text": "Acc", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9599471688270569}, {"text": "Natural language inference Natural language inference", "start_pos": 44, "end_pos": 97, "type": "TASK", "confidence": 0.7201823790868124}]}, {"text": "We use the Multi-Genre NLI corpus (MultiNLI) from the RepEval 2017 shared task ().", "labels": [], "entities": [{"text": "Multi-Genre NLI corpus (MultiNLI)", "start_pos": 11, "end_pos": 44, "type": "DATASET", "confidence": 0.7502727210521698}, {"text": "RepEval 2017 shared task", "start_pos": 54, "end_pos": 78, "type": "DATASET", "confidence": 0.8139036297798157}]}, {"text": "An example for an instance would be the sentence pair \"Fun for only children\", \"Fun for adults and children\", which are in a \"contradiction\" relationship.", "labels": [], "entities": []}, {"text": "The evaluation metric is accuracy (Acc).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9997469782829285}, {"text": "Acc)", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9704795777797699}]}, {"text": "Stance FNC MultiNLI Topic-2 Topic-5* ABSA-L ABSA-R Target  49.01: Comparison of our best performing models on the test set against a single task baseline and the state of the art, with task specific metrics.", "labels": [], "entities": [{"text": "ABSA-L ABSA-R Target  49.01", "start_pos": 37, "end_pos": 64, "type": "METRIC", "confidence": 0.7745238393545151}]}, {"text": "*: lower is better.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training set statistics and evaluation metrics  of every task. N : # of examples. L: # of labels.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of our best performing models on the test set against a single task baseline and the state of  the art, with task specific metrics. *: lower is better. Bold: best. Underlined: second-best.", "labels": [], "entities": []}, {"text": " Table 5: Ablation results with task-specific evaluation metrics on test set with early stopping on dev set. LTN  means the output of the relabelling function is shown, which does not use the task predictions, only predictions  from other tasks. LTN + main preds feats means main model predictions are used as features for the relabelling  function. LTN, main model means that the main model predictions of the model that trains a relabelling function  are used. Note that for MultiNLI, we down-sample the training data. *: lower is better. Bold: best. Underlined:  second-best.", "labels": [], "entities": []}, {"text": " Table 6: Error analysis of LTN with and without semi- supervised learning for all tasks. Metric shown: per- centage of correct predictions only made by either the  relabelling function or the main model, respectively,  relative to the the number of all correct predictions.", "labels": [], "entities": [{"text": "Metric", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9853251576423645}]}]}