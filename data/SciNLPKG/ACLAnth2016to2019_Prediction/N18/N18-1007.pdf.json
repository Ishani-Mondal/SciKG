{"title": [{"text": "Parsing Speech: A Neural Approach to Integrating Lexical and Acoustic-Prosodic Information", "labels": [], "entities": [{"text": "Integrating Lexical and Acoustic-Prosodic Information", "start_pos": 37, "end_pos": 90, "type": "TASK", "confidence": 0.5900785505771637}]}], "abstractContent": [{"text": "In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses.", "labels": [], "entities": [{"text": "disambiguate difficult parses", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.6198092500368754}]}, {"text": "For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features.", "labels": [], "entities": [{"text": "automatically parsing spoken utterances", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.7137423381209373}]}, {"text": "We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline.", "labels": [], "entities": [{"text": "disfluency detection F1", "start_pos": 155, "end_pos": 178, "type": "METRIC", "confidence": 0.5483343601226807}]}, {"text": "For this study with known sentence boundaries, error analyses show that the main benefit of acoustic-prosodic features is in sentences with disfluen-cies, attachment decisions are most improved, and transcription errors obscure gains from prosody.", "labels": [], "entities": []}], "introductionContent": [{"text": "While parsing has become a relatively mature technology for written text, parser performance on conversational speech lags behind.", "labels": [], "entities": [{"text": "parsing", "start_pos": 6, "end_pos": 13, "type": "TASK", "confidence": 0.9798964262008667}]}, {"text": "Speech poses challenges for parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (\"um\", \"uh\"), interjections (\"like\"), parentheticals (\"you know\", \"I mean\"), and sentence fragments.", "labels": [], "entities": [{"text": "parsing", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.9664751291275024}]}, {"text": "Some of these phenomena can be handled in standard grammars, but disfluencies typically require extensions of the model.", "labels": [], "entities": []}, {"text": "Different approaches have been explored in both constituency parsing) and dependency parsing).", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.8948473334312439}, {"text": "dependency parsing", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.9022234976291656}]}, {"text": "Despite these challenges, speech carries helpful extra information -beyond the words -associated with the prosodic structure of an utterance and encoded via variation in timing and intonation.", "labels": [], "entities": []}, {"text": "Speakers pause in locations that are correlated with syntactic structure (, and listeners use prosodic structure in resolving syntactic ambiguities.", "labels": [], "entities": []}, {"text": "Prosodic cues also signal disfluencies by marking the interruption point.", "labels": [], "entities": []}, {"text": "However, most speech parsing systems in practice take little advantage of these cues.", "labels": [], "entities": [{"text": "speech parsing", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7074098289012909}]}, {"text": "Our study focuses on this last challenge, aiming to incorporate prosodic cues in a neural parser, handling disfluencies as constituents via a neural attention mechanism.", "labels": [], "entities": []}, {"text": "A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape.", "labels": [], "entities": [{"text": "parsing", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.9653294682502747}]}, {"text": "These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction typically involves multiple time windows and normalization techniques.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7174291908740997}]}, {"text": "The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (.", "labels": [], "entities": []}, {"text": "The approach proposed here takes advantage of advances in neural networks to automatically learn a good feature representation without the need to explicitly represent prosodic constituents.", "labels": [], "entities": []}, {"text": "To narrow the scope of this work and facilitate error analysis, our experiments use known transcripts and sentence segmentation.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.6596628427505493}, {"text": "sentence segmentation", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.7264961898326874}]}, {"text": "Our work offers the following contributions.", "labels": [], "entities": []}, {"text": "We introduce a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require handannotated prosodic structure.", "labels": [], "entities": []}, {"text": "We demonstrate improvements in constituent parsing of conversational speech over a high-quality text-only parser and provide analyses showing where prosodic features help and that assessment of their utility is affected by human transcription errors.", "labels": [], "entities": [{"text": "constituent parsing of conversational speech", "start_pos": 31, "end_pos": 75, "type": "TASK", "confidence": 0.8232921838760376}]}], "datasetContent": [{"text": "Our core corpus is Switchboard-NXT (, a subset of the Switchboard corpus (Godfrey and Holliman, 1993): 2,400 telephone conversations between strangers; 642 of these were hand-annotated with syntactic parses and further augmented with richer layers of annotation facilitated by the NITE XML toolkit.", "labels": [], "entities": [{"text": "Switchboard corpus (Godfrey and Holliman, 1993)", "start_pos": 54, "end_pos": 101, "type": "DATASET", "confidence": 0.901282012462616}]}, {"text": "Our sentence segmentations and syntactic trees are based on the annotations from the Treebank set, with a few manual corrections from the NXT release.", "labels": [], "entities": [{"text": "sentence segmentations", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7332152873277664}, {"text": "Treebank set", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.9283692538738251}, {"text": "NXT release", "start_pos": 138, "end_pos": 149, "type": "DATASET", "confidence": 0.9526265263557434}]}, {"text": "This core dataset consists of 100K sentences, totaling 830K tokens forming a vocabulary of 13.5K words.", "labels": [], "entities": []}, {"text": "We use the time alignments available from NXT, which is based on a corrected word transcript that occasionally differs from the Treebank, leading to some missing time alignments.", "labels": [], "entities": [{"text": "NXT", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9412989616394043}]}, {"text": "We follow the sentence boundaries defined by the parsed data available, and the data split (90% train; 5% dev; 5% test) defined by related work done on Switchboard).", "labels": [], "entities": []}, {"text": "The standard evaluation metric for constituent parsing is the parseval metric which uses bracketing precision, recall, and F1, as in the canonical implementation of EVALB.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.6893420070409775}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.5370767712593079}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9983603358268738}, {"text": "F1", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9997486472129822}, {"text": "EVALB", "start_pos": 165, "end_pos": 170, "type": "DATASET", "confidence": 0.8976130485534668}]}, {"text": "For written text, punc-tuation is sometimes represented as part of the sequence and impacts the final score, but for speech the punctuation is not explicitly available so it does not contribute to the score.", "labels": [], "entities": []}, {"text": "Another challenge of transcribed speech is the presence of disfluencies.", "labels": [], "entities": []}, {"text": "Speech repairs are indicated under \"EDITED\" nodes in Switchboard parse trees, which include structure under these nodes that is not of interest for simple text clean-up.", "labels": [], "entities": [{"text": "Speech repairs", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6687855273485184}]}, {"text": "Therefore, some studies report flattened-edit parseval F1 scores (\"flat-F1\"), which is parseval computed on trees where the structure under edit nodes has been eliminated so that all leaves are immediate children.", "labels": [], "entities": [{"text": "flattened-edit parseval F1 scores", "start_pos": 31, "end_pos": 64, "type": "METRIC", "confidence": 0.751989871263504}]}, {"text": "We report both scores for the baseline text-only model showing that the differences are small, then use the standard parseval F1 score for most results.", "labels": [], "entities": [{"text": "parseval F1 score", "start_pos": 117, "end_pos": 134, "type": "METRIC", "confidence": 0.8154879808425903}]}, {"text": "Disfluencies are particularly problematic for statistical parsers, as explained by, and some systems incorporate a separate disfluency detection stage.", "labels": [], "entities": []}, {"text": "For this reason, and because it is useful for understanding system performance, most studies also report disfluency detection performance, which is measured in terms of the F1 score for detecting whether a word is in an edit region.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.6814051270484924}, {"text": "F1 score", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.985454261302948}]}, {"text": "Our approach does not involve a separate disfluency detection stage, but identifies disfluencies implicitly via the parse structure.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7091432064771652}]}, {"text": "Consequently, the disfluency detection results are not competitive with work that directly optimize for disfluency detection.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.8872757256031036}, {"text": "disfluency detection", "start_pos": 104, "end_pos": 124, "type": "TASK", "confidence": 0.7549352943897247}]}, {"text": "We report disfluency detection scores primarily as a diagnostic.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7780165374279022}]}, {"text": "Most previous work on integrating prosody and parsing has used the Switchboard corpus, but it is still difficult to compare results because of differences in constraints, objectives and the use of constituent vs. dependency structure, as discussed further in Section 6.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9015783667564392}, {"text": "Switchboard corpus", "start_pos": 67, "end_pos": 85, "type": "DATASET", "confidence": 0.8756092190742493}]}, {"text": "The most relevant prior studies (on constituent parsing) that we compare to area bit old.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7371808588504791}]}, {"text": "The text-only result from our neural parser represents a stronger baseline and is important for decoupling the impact of prosody vs. the parsing framework.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Scores of text-only models on the dev set:  2044 fluent and 3725 disfluent sentences. C-attn  denotes content-only attention; CL-attn denotes con- tent+location attention. Model  Parse Disf  Berkeley (text only)  85.41 62.45  CL-attn (text only)  87.85 79.50  CL-attn text and  + p  88.37 80.24  + \u03b4  88.04 77.41  + p + \u03b4  88.21 80.84  + f0/E-CNN  88.52 80.81  + p + f0/E-CNN  88.45 81.19  + \u03b4 + f0/E-CNN  88.44 80.09  + p + \u03b4 + f0/E-CNN 88.59 80.84", "labels": [], "entities": []}, {"text": " Table 3: Parse and disfluency detection F1 scores on  the dev set: mean (and standard deviation) over 10 runs  for the baseline text-only model (CL-attn) and the best  model with prosody.", "labels": [], "entities": [{"text": "Parse and disfluency detection", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.5825559720396996}, {"text": "F1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9712202548980713}]}, {"text": " Table 4: Parse and disfluency detection F1 scores on  the test set. The best model has statistically significant  gains over the text-only baseline with p-value < 0.02.", "labels": [], "entities": [{"text": "Parse", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.823317289352417}, {"text": "F1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.956336259841919}]}, {"text": " Table 5: Parse and disfluency detection F1 scores on  the test set comparing to other reported results.", "labels": [], "entities": [{"text": "Parse", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9844873547554016}, {"text": "F1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.7105135321617126}]}, {"text": " Table 6: Dev set F1-score of text-only and best model  on fluent (2029) vs. disfluent (3689) sentences. 10", "labels": [], "entities": [{"text": "F1-score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9874911308288574}]}, {"text": " Table 9: Parse error counts comparison on the fluent (2029 sentences) and disfluent (3689 sentences) subsets of  the development set across three parsers.", "labels": [], "entities": [{"text": "Parse error", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8620779514312744}]}]}