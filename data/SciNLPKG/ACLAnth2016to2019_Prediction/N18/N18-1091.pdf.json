{"title": [{"text": "What's Going On in Neural Constituency Parsers? An Analysis", "labels": [], "entities": []}], "abstractContent": [{"text": "A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.8699082434177399}]}, {"text": "The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods.", "labels": [], "entities": []}, {"text": "To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments.", "labels": [], "entities": [{"text": "F1", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.8803870677947998}, {"text": "PTB", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9387983083724976}]}, {"text": "We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the past several years, many aspects of constituency parsing and natural language processing in general have changed.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.9172799587249756}, {"text": "natural language processing", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6639537115891775}]}, {"text": "Grammars, which were once the central component of many parsers, have played a continually decreasing role.", "labels": [], "entities": []}, {"text": "Rich lexicons and handcrafted lexical features have become less common as well.", "labels": [], "entities": []}, {"text": "On the other hand, recurrent neural networks have gained traction as a powerful and general purpose tool for representation.", "labels": [], "entities": []}, {"text": "So far, not much has been shown about how neural networks are able to compensate for the removal of the structures used in past models.", "labels": [], "entities": []}, {"text": "To gain insight, we introduce a parser that is representative of recent trends and analyze its learned representations to determine what information it captures and what is important for its strong performance.", "labels": [], "entities": []}, {"text": "Our parser is a natural extension of recent work in constituency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.8407368957996368}]}, {"text": "We combine a common span representation based on recurrent neural networks with a novel, simplified scoring model.", "labels": [], "entities": []}, {"text": "In addition, we replace the externally predicted partof-speech tags used in some recent systems with character-level word representations.", "labels": [], "entities": []}, {"text": "Our parser achieves a test F1 score of 92.08 on section 23 of the Penn Treebank, exceeding the performance of many other state-of-the-art models evaluated under comparable conditions.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9841466546058655}, {"text": "section 23 of the Penn Treebank", "start_pos": 48, "end_pos": 79, "type": "DATASET", "confidence": 0.7215652018785477}]}, {"text": "Section 2 describes our model in detail.", "labels": [], "entities": []}, {"text": "The remainder of the paper is focused on analysis.", "labels": [], "entities": []}, {"text": "In Section 3, we look at the decline of grammars and output correlations.", "labels": [], "entities": []}, {"text": "Past work in constituency parsing used context-free grammars with production rules governing adjacent labels (or more generally production-factored scores) to propagate information and capture correlations between output decisions).", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.8653556704521179}]}, {"text": "Many recent parsers no longer have explicit grammar production rules, but still use information about other predictions, allowing them to capture output correlations.", "labels": [], "entities": []}, {"text": "Beyond this, there are some parsers that use no context for bracket scoring and only include mild output correlations in the form of tree constraints.", "labels": [], "entities": [{"text": "bracket scoring", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.7939727604389191}]}, {"text": "In our experiments, we find that we can accurately predict parents from the representation given to a child.", "labels": [], "entities": []}, {"text": "Since a simple classifier can predict the information provided by parent-child relations, this explains why the information no longer needs to be specified explicitly.", "labels": [], "entities": []}, {"text": "We also show that we can completely remove output correlations from our model with a variant of our parser that makes independent span label decisions without any tree constraints while maintaining high F1 scores and mostly producing trees.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 203, "end_pos": 212, "type": "METRIC", "confidence": 0.9759498834609985}]}, {"text": "In Section 4, we look at lexical representations.", "labels": [], "entities": []}, {"text": "In the past, parsers used a variety of cus-tom lexical representations, such as word shape features, prefixes, suffixes, and special tokens for categories like numerals (.", "labels": [], "entities": []}, {"text": "Character-level models have shown promise in parsing and other NLP tasks as away to remove the complexity of these lexical features (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.9674286842346191}]}, {"text": "We compare the performance of characterlevel representations and externally predicted partof-speech tags and show that these two sources of information seem to fill a similar role.", "labels": [], "entities": []}, {"text": "We also perform experiments showing that the representations learned with character-level models contain information that was hand-specified in some other models.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 we look at the surface context captured by recurrent neural networks.", "labels": [], "entities": []}, {"text": "Many recent parsers use LSTMs, a popular type of recurrent neural network, to combine and summarize context for making decisions.", "labels": [], "entities": []}, {"text": "Before LSTMs became common in parsing, systems that included surface features used a fixed-size window around the fenceposts at each end of a span, and the inference procedure handled most of the propagation of information from the rest of the sentence.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9697785973548889}, {"text": "propagation of information from the rest of the sentence", "start_pos": 196, "end_pos": 252, "type": "TASK", "confidence": 0.7159944441583421}]}, {"text": "We perform experiments showing that LSTMs capture far-away surface context and that this information is important for our parser's performance.", "labels": [], "entities": []}, {"text": "We also provide evidence that word order of the far-away context is important and that the amount of context alone does not account for all of the gains seen with LSTMs.", "labels": [], "entities": [{"text": "word order", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.7312668263912201}]}, {"text": "Overall, we find that the same sources of information that were effective for grammar-driven parsers are also captured by parsers based on recurrent neural networks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Development F1 scores on section 22 of the  Penn Treebank for different lexical representations.", "labels": [], "entities": [{"text": "F1", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.5865835547447205}, {"text": "Penn Treebank", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.9637915194034576}]}, {"text": " Table 2: The sizes of the components used in our model.", "labels": [], "entities": []}, {"text": " Table 3: Classification accuracy for various binary word features using the character LSTM representations for  words induced by a pre-trained parser. Performance substantially exceeds that of a majority class classifier in all  cases, reaching 99.7% or higher for all features. The majority class is True for the first four features in the left  column and False for the rest.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9802905321121216}, {"text": "False", "start_pos": 359, "end_pos": 364, "type": "METRIC", "confidence": 0.9823075532913208}]}]}