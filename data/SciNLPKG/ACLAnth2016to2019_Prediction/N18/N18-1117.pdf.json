{"title": [{"text": "Dense Information Flow for Neural Machine Translation", "labels": [], "entities": [{"text": "Dense Information Flow", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8313117822011312}, {"text": "Neural Machine Translation", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.7655386726061503}]}], "abstractContent": [{"text": "Recently, neural machine translation has achieved remarkable progress by introducing well-designed deep neural networks into its encoder-decoder framework.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6424087981383005}]}, {"text": "From the optimization perspective, residual connections are adopted to improve learning performance for both en-coder and decoder inmost of these deep architectures, and advanced attention connections are applied as well.", "labels": [], "entities": []}, {"text": "Inspired by the success of the DenseNet model in computer vision problems, in this paper , we propose a densely connected NMT architecture (DenseNMT) that is able to train more efficiently for NMT.", "labels": [], "entities": []}, {"text": "The proposed DenseNMT not only allows dense connection in creating new features for both encoder and decoder, but also uses the dense attention structure to improve attention quality.", "labels": [], "entities": []}, {"text": "Our experiments on multiple datasets show that DenseNMT structure is more competitive and efficient.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) is a challenging task that attracts lots of attention in recent years.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.812683512767156}]}, {"text": "Starting from the encoder-decoder framework (), NMT starts to show promising results in many language pairs.", "labels": [], "entities": []}, {"text": "The evolving structures of NMT models in recent years have made them achieve higher scores and become more favorable.", "labels": [], "entities": []}, {"text": "The attention mechanism () added on top of encoder-decoder framework is shown to be very useful to automatically find alignment structure, and single-layer RNN-based structure has evolved into deeper models with more efficient transformation functions.", "labels": [], "entities": []}, {"text": "One major challenge of NMT is that its models are hard to train in general due to the complexity of both the deep models and languages.", "labels": [], "entities": []}, {"text": "From the optimization perspective, deeper models are hard to efficiently back-propagate the gradients, and this phenomenon as well as its solution is better explored in the computer vision society.", "labels": [], "entities": []}, {"text": "Residual networks (ResNet) ( achieve great performance in a wide range of tasks, including image classification and image segmentation.", "labels": [], "entities": [{"text": "image classification", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.7769920229911804}, {"text": "image segmentation", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.753972589969635}]}, {"text": "Residual connections allow features from previous layers to be accumulated to the next layer easily, and make the optimization of the model efficiently focus on refining upper layer features.", "labels": [], "entities": []}, {"text": "NMT is considered as a challenging problem due to its sequence-to-sequence generation framework, and the goal of comprehension and reorganizing from one language to the other.", "labels": [], "entities": []}, {"text": "Apart from the encoder block that works as a feature generator, the decoder network combining with the attention mechanism bring new challenges to the optimization of the models.", "labels": [], "entities": []}, {"text": "While nowadays best-performing NMT systems use residual connections, we question whether this is the most efficient way to propagate information through deep models.", "labels": [], "entities": []}, {"text": "In this paper, inspired by the idea of using dense connections for training computer vision tasks, we propose a densely connected NMT framework (DenseNMT) that efficiently propagates information from the encoder to the decoder through the attention component.", "labels": [], "entities": []}, {"text": "Taking the CNN-based deep architecture as an example, we verify the efficiency of DenseNMT.", "labels": [], "entities": []}, {"text": "Our contributions in this work include: (i) by comparing the loss curve, we show that DenseNMT allows the model to pass information more efficiently, and speeds up training; (ii) we show through ablation study that dense con-1294 nections in all three blocks altogether help improve the performance, while not increasing the number of parameters; (iii) DenseNMT allows the models to achieve similar performance with much smaller embedding size; (iv) DenseNMT on IWSLT14 German-English and Turkish-English translation tasks achieves new benchmark BLEU scores, and the result on WMT14 English-German task is more competitive than the residual connections based baseline model.", "labels": [], "entities": [{"text": "IWSLT14 German-English and Turkish-English translation tasks", "start_pos": 462, "end_pos": 522, "type": "TASK", "confidence": 0.701164980729421}, {"text": "BLEU", "start_pos": 546, "end_pos": 550, "type": "METRIC", "confidence": 0.9672999382019043}, {"text": "WMT14 English-German task", "start_pos": 577, "end_pos": 602, "type": "DATASET", "confidence": 0.8927513957023621}]}], "datasetContent": [{"text": "We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German.", "labels": [], "entities": [{"text": "IWSLT14 German-English", "start_pos": 43, "end_pos": 65, "type": "DATASET", "confidence": 0.8394822180271149}, {"text": "WMT14", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.8483749032020569}]}, {"text": "We preprocess the IWSLT14 German-English dataset following byte-pair-encoding (BPE) method (Sennrich et al., 2015b) . We learn 25k BPE codes using the joint corpus of source and target languages.", "labels": [], "entities": [{"text": "IWSLT14 German-English dataset", "start_pos": 18, "end_pos": 48, "type": "DATASET", "confidence": 0.8882113496462504}]}, {"text": "We randomly select 7k from IWSLT14 German-English as the development set , and the test set is a concatenation of dev2010, tst2010, tst2011 and tst2012, which is widely used in prior works (.", "labels": [], "entities": [{"text": "IWSLT14 German-English", "start_pos": 27, "end_pos": 49, "type": "DATASET", "confidence": 0.9012961983680725}]}, {"text": "For the Turkish-English translation task, we use the data provided by IWSLT14 () and the SETimes corpus () following).", "labels": [], "entities": [{"text": "Turkish-English translation task", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.7654155691464742}, {"text": "IWSLT14", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.9357303977012634}, {"text": "SETimes corpus", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.8575619161128998}]}, {"text": "After removing sentence pairs with length ratio over 9, we obtain 360k sentence pairs.", "labels": [], "entities": []}, {"text": "Since there is little commonality between the two languages, we learn 30k size BPE codes separately for Turkish and English.", "labels": [], "entities": [{"text": "BPE", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.6894248127937317}]}, {"text": "In addition to this, we give another preprocessing for Turkish sentences and use word-level English corpus.", "labels": [], "entities": []}, {"text": "For Turkish sentences, following (), we use the morphology tool Zemberek with disambiguation by the morphological analysis () and removal of non-surface tokens 2 . Following (), we concatenate tst2011, tst2012, tst2013, tst2014 as our test set.", "labels": [], "entities": []}, {"text": "We concatenate dev2010 and tst2010 as the development set.", "labels": [], "entities": []}, {"text": "We preprocess the WMT14 English-German 3 dataset using a BPE code size of 40k.", "labels": [], "entities": [{"text": "WMT14 English-German 3 dataset", "start_pos": 18, "end_pos": 48, "type": "DATASET", "confidence": 0.9303655475378036}, {"text": "BPE code size", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.6151568094889323}]}, {"text": "We use the concatenation of newstest2013 and newstest2012 as the development set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU score on IWSLT German-English and Turkish-English translation tasks. We compare models using different  embedding sizes, and keep the model size consistent within each column.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994664788246155}, {"text": "IWSLT German-English and Turkish-English translation tasks", "start_pos": 24, "end_pos": 82, "type": "TASK", "confidence": 0.712743878364563}]}, {"text": " Table 2: Ablation study for encoder block, decoder block,  and attention block in DenseNMT.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9882951974868774}]}, {"text": " Table 3: Accuracy on Turkish-English translation task in terms of BLEU score.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986465573310852}, {"text": "Turkish-English translation", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.660050168633461}, {"text": "BLEU score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9774976968765259}]}, {"text": " Table 4: Accuracy on IWSLT14 German-English translation  task in terms of BLEU score.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9977045655250549}, {"text": "IWSLT14 German-English translation", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.7140729228655497}, {"text": "BLEU score", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.978277862071991}]}]}