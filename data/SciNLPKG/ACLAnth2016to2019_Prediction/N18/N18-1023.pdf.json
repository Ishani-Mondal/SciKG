{"title": [{"text": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences", "labels": [], "entities": [{"text": "Reading Comprehension over Multiple Sentences", "start_pos": 48, "end_pos": 93, "type": "TASK", "confidence": 0.7324599027633667}]}], "abstractContent": [{"text": "We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences.", "labels": [], "entities": []}, {"text": "We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment.", "labels": [], "entities": []}, {"text": "Our challenge dataset contains \u223c6k questions for +800 paragraphs across 7 different domains (ele-mentary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings.", "labels": [], "entities": []}, {"text": "On a subset of our dataset, we found human solvers to achieve an F1-score of 86.4%.", "labels": [], "entities": [{"text": "solvers", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.7325969934463501}, {"text": "F1-score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9997789263725281}]}, {"text": "We analyze a range of baselines, including a recent state-of-art reading comprehension system , and demonstrate the difficulty of this challenge, despite a high human performance.", "labels": [], "entities": []}, {"text": "The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Comprehension of natural language text is a fundamental challenge in AI and it has received significant attention throughout the history of AI.", "labels": [], "entities": [{"text": "Machine Comprehension of natural language text", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8715506990750631}]}, {"text": "In particular, in natural language processing (NLP) it has been studied under various settings, such as multiplechoice Question-Answering (QA), Reading Comprehension (RC)), Recognizing Textual Entailment (RTE) () etc.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 18, "end_pos": 51, "type": "TASK", "confidence": 0.7750363349914551}, {"text": "Recognizing Textual Entailment (RTE)", "start_pos": 173, "end_pos": 209, "type": "TASK", "confidence": 0.7349352190891901}]}, {"text": "The area has seen rapidly increasing interest, thanks to the existence of sizable datasets and standard benchmarks.", "labels": [], "entities": []}, {"text": "CNN/Daily Mail (,) and NewsQA ( to name a few, are some of the datasets that were released recently with the goal of facilitating research in machine comprehension.", "labels": [], "entities": [{"text": "CNN/Daily Mail", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9506514072418213}, {"text": "NewsQA", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.9744206666946411}]}, {"text": "Despite all the excitement fueled by that large data sets and the ability to directly train statistical learning models, current QA systems do not have capabilities comparable to elementary school or younger children . For many of these datasets, researchers point out that models neither need to 'comprehend' in order to correctly predict an answer, nor do they learn to 'reason' in away that generalizes across datasets.", "labels": [], "entities": []}, {"text": "For example,  showed that adversarial perturbation in candidate answers results in a significant drop in performance of a few state-of-art science QA systems.", "labels": [], "entities": []}, {"text": "Similarly, show that adding an adversarially selected sentence to the instances in the SQuAD datasets drastically reduces the performance of many of the existing baselines.", "labels": [], "entities": [{"text": "SQuAD datasets", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.85370072722435}]}, {"text": "show that in the CNN/Daily Mail datasets, \"the required reasoning and inference level . .", "labels": [], "entities": [{"text": "CNN/Daily Mail datasets", "start_pos": 17, "end_pos": 40, "type": "DATASET", "confidence": 0.9289185762405395}]}, {"text": ". is quite simple\" and that a relatively simple algorithm can get almost close to the upper-bound.", "labels": [], "entities": []}, {"text": "We believe that one key reason that simple algorithms can deal with the existing large datasets but, nevertheless, fail at generalization, is that the datasets do not actually require a deep understanding.", "labels": [], "entities": []}, {"text": "We propose to address this shortcoming by developing a reading comprehension challenge in which answering each of the questions requires reasoning over multiple sentences.", "labels": [], "entities": []}, {"text": "There is evidence that answering 'singlesentence questions', i.e. questions that can be answered from a single sentence of the given paragraph, is easier than answering multi-sentence questions', which require multiple sentences to answer a given question.", "labels": [], "entities": []}, {"text": "For example, released a reading comprehension dataset that contained both single-sentence and multi-sentence questions; models proposed for this task yielded considerably better performance on the single-sentence questions than on the multi-sentence questions (according to accuracy of about 83% and 60% on these two types of questions, respectively).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 274, "end_pos": 282, "type": "METRIC", "confidence": 0.9989215135574341}]}, {"text": "There could be multiple reasons for this.", "labels": [], "entities": []}, {"text": "First, multi-sentence reasoning seems to be inherently a difficult task.", "labels": [], "entities": [{"text": "multi-sentence reasoning", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.8155772089958191}]}, {"text": "Research has shown that while complete-sentence construction emerges as early as first grade for many children, their ability to integrate sentences emerges only in fourth grade).", "labels": [], "entities": [{"text": "complete-sentence construction", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.7477002739906311}]}, {"text": "Answering multisentence questions might be more challenging for an automated system because it involves more than just processing individual sentences but rather combining linguistic, semantic and background knowledge across sentences-a computational challenges in itself.", "labels": [], "entities": [{"text": "Answering multisentence questions", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8335896134376526}]}, {"text": "Despite these challenges, multi-sentence questions can be answered by humans and hence present an interesting yet reasonable goal for AI systems.", "labels": [], "entities": []}, {"text": "In this work, we propose a multi-sentence QA challenge in which questions can be answered only using information from multiple sentences.", "labels": [], "entities": [{"text": "QA challenge", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.7613643109798431}]}, {"text": "Specifically, we present MultiRC (Multi-Sentence Reading Comprehension) 1 -a dataset of short paragraphs and multi-sentence questions that can be answered from the content of the paragraph.", "labels": [], "entities": []}, {"text": "Each question is associated with several choices for answer-options, out of which one or more correctly answer the question.", "labels": [], "entities": []}, {"text": "shows two examples from our dataset.", "labels": [], "entities": []}, {"text": "Each instance consists of a multi-sentence paragraph, a question, and answer-options.", "labels": [], "entities": []}, {"text": "All instances were constructed such that it is not possible to answer a question correctly without gathering information from multiple sentences.", "labels": [], "entities": []}, {"text": "Due to space constraints, the figure shows only the relevant sentences from the original paragraph.", "labels": [], "entities": []}, {"text": "The entire corpus consists of 871 paragraphs and about \u223c 6k multi-sentence questions.", "labels": [], "entities": []}, {"text": "The goal of this dataset is to encourage the research community to explore approaches that can do more than sophisticated lexical-level matching.", "labels": [], "entities": []}, {"text": "To accomplish this, we designed the dataset with three key challenges in mind.", "labels": [], "entities": []}, {"text": "(i) The number of correct answer-options for each question is not pre-specified.", "labels": [], "entities": []}, {"text": "This removes the over-reliance of current approaches on answer-options and forces them to decide on the correctness of each candidate answer independently of others.", "labels": [], "entities": []}, {"text": "In other words, unlike previous work, the task here is not 1 http://cogcomp.org/multirc/ S3: Hearing noises in the garage, Mary Murdock finds a bleeding man, mangled and impaled on her jeep's bumper.", "labels": [], "entities": []}, {"text": "S5: Panicked, she hits him with a golf club.", "labels": [], "entities": []}, {"text": "S10: Later the news reveals the missing man is kindergarten teacher, Timothy Emser.", "labels": [], "entities": []}, {"text": "S12: It transpires that Rick, her boyfriend, gets involved in the cover up and goes to retrieve incriminatory evidence off the corpse, but is killed, replaced in Emser's grave.", "labels": [], "entities": []}, {"text": "S13: It becomes clear Emser survived.", "labels": [], "entities": []}, {"text": "S15: He stalks Mary many ways.", "labels": [], "entities": []}, {"text": "A)* Timothy D) Rick B) Timothy's girlfriend E) Murdock C)* The man she hit F) Her Boyfriend S1: Most young mammals, including humans, play.", "labels": [], "entities": []}, {"text": "S2: Play is how they learn the skills that they will need as adults.", "labels": [], "entities": []}, {"text": "S6: Big cats also play.", "labels": [], "entities": []}, {"text": "S8: At the same time, they also practice their hunting skills.", "labels": [], "entities": []}, {"text": "S11: Human children learn by playing as well.", "labels": [], "entities": []}, {"text": "S12: For example, playing games and sports can help them learn to follow rules.", "labels": [], "entities": []}, {"text": "S13: They also learn to work together.", "labels": [], "entities": []}, {"text": "What do human children learn by playing games and sports?", "labels": [], "entities": []}, {"text": "A)* They learn to follow rules and work together B) hunting skills C)* skills that they will need as adult to simply identify the best answer-option, but to evaluate the correctness of each answer-option individually.", "labels": [], "entities": []}, {"text": "For example, the first question in can be answered by combining information from sentences 3, 5, 10, 13 and 15.", "labels": [], "entities": []}, {"text": "It requires not only understanding that the stalker's name is Timothy but also that he is the man who Mary had hit.", "labels": [], "entities": []}, {"text": "(ii) The correct answer(s) is not required to be a span in the text.", "labels": [], "entities": []}, {"text": "For example, the correct answer, A, of the second question in is not present in the paragraph verbatim.", "labels": [], "entities": [{"text": "A", "start_pos": 33, "end_pos": 34, "type": "METRIC", "confidence": 0.7582126259803772}]}, {"text": "It is instead a combination of two spans from 2 sentences: 12 and 13.", "labels": [], "entities": []}, {"text": "Such answer-options force models to process and understand not only the paragraph and the question but also the answer-options.", "labels": [], "entities": []}, {"text": "(iii) The paragraphs in our dataset have diverse provenance by being extracted from 7 different domains such as news, fiction, historical text etc., and hence are expected to be more diverse in their contents as compared to single-domain datasets.", "labels": [], "entities": []}, {"text": "We also expect this to lead to diversity in the types of questions that can be constructed from the passage.", "labels": [], "entities": []}, {"text": "Overall, we introduce a reading comprehension dataset that significantly differs from most other datasets available today in the following ways: \u2022 \u223c6k high-quality multiple-choice RC questions that are generated (and manually verified via crowdsourcing) to require integrating information from multiple sentences.", "labels": [], "entities": []}, {"text": "\u2022 The questions are not constrained to have a single correct answer, generalizing existing paradigms for representing answer-options.", "labels": [], "entities": []}, {"text": "\u2022 Our dataset is constructed using 7 different sources, allowing more diversity in content, style, and possible question types.", "labels": [], "entities": []}, {"text": "\u2022 We show a significant performance gap between current solvers and human performance, indicating an opportunity for developing sophistical reasoning systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "The 4-step process described above was a result of detailed analysis and substantial refinement after two small pilot studies.", "labels": [], "entities": []}, {"text": "In the first pilot study, we ran a set of 10 paragraphs extracted from the CMU Movie Summary Corpus through our pipeline.", "labels": [], "entities": [{"text": "CMU Movie Summary Corpus", "start_pos": 75, "end_pos": 99, "type": "DATASET", "confidence": 0.9554847180843353}]}, {"text": "Our then pipeline looked considerably different from the one described above.", "labels": [], "entities": []}, {"text": "We found the steps that required turkers to write questions and answer-options to often have grammatical errors, possibly because a large majority of turkers were non-native speakers of English.", "labels": [], "entities": []}, {"text": "This probslem was more prominent in questions than in answer-options.", "labels": [], "entities": []}, {"text": "Because of this, we decided to limit the task to native speakers.", "labels": [], "entities": []}, {"text": "Also, based on the results of this pilot, we overhauled the instructions of these steps by including examples of grammatically correct-but undesirable (not multi-sentence)-questions and answeroptions, in addition to several minor changes.", "labels": [], "entities": []}, {"text": "Thereafter, we decided to perform a manual validation of the verification steps (current Steps 2 and 4).", "labels": [], "entities": []}, {"text": "For this, we (the authors of this paper) performed additional annotations ourselves on the data shown to turkers, and compared our results with those provided by the turkers.", "labels": [], "entities": []}, {"text": "We found that in the verification of answer-options, our annotations were in high agreement (98%) with those obtained from mechanical turk.", "labels": [], "entities": []}, {"text": "However, that was not the case for the verification of multi-sentence questions.", "labels": [], "entities": [{"text": "verification", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.968656063079834}]}, {"text": "We made several further changes to the first two steps.", "labels": [], "entities": []}, {"text": "Among other things, we clarified in the instructions that turkers should not use their background knowledge when writing and verifying questions, and also included negative examples of such questions.", "labels": [], "entities": []}, {"text": "Additionally, when turkers judged a question to be answerable using a single sentence, we decided to encourage (but not require) them to guess the answer to the question.", "labels": [], "entities": []}, {"text": "This improved our results considerably, possibly because it forced annotators to think more carefully about what the answer might be, and whether they actually knew the answer or they just thought that they knew it (possibly because of background knowledge or because the sentence contained a lot of information relevant to the question).", "labels": [], "entities": []}, {"text": "Guessed answers in this step were only used to verify the validity of multi-sentence questions.", "labels": [], "entities": []}, {"text": "They were not used in the dataset or subsequent steps.", "labels": [], "entities": []}, {"text": "After revision, we ran a second pilot study in which we processed a set of 50 paragraphs through our updated pipeline.", "labels": [], "entities": []}, {"text": "This second pilot confirmed that our revisions were helpful, but thanks to its larger size, also allowed us to identify a couple of borderline cases for which additional clarifications were required.", "labels": [], "entities": []}, {"text": "Based on the results of the second pilot, we made some additional minor changes and then decided to apply the pipeline for creating the final dataset.", "labels": [], "entities": []}, {"text": "We now provide a brief summary of MultiRC.", "labels": [], "entities": [{"text": "MultiRC", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.8734453916549683}]}, {"text": "Overall, it contains roughly \u223c 6k multi-sentence questions collected for about +800 paragraphs.", "labels": [], "entities": []}, {"text": "The median number of correct and total answer options for each question is 2 and 5, respectively.", "labels": [], "entities": []}, {"text": "Additional statistics are given in.", "labels": [], "entities": []}, {"text": "In Step 1, we also asked annotators to identify sentences required to answer a given question.", "labels": [], "entities": []}, {"text": "We found that answering each question required 2.4 sentences on average.", "labels": [], "entities": []}, {"text": "Also, required sentences are often not contiguous, and the average distance between sentences is 2..", "labels": [], "entities": []}, {"text": "Next, we analyze the types of questions in our dataset.", "labels": [], "entities": []}, {"text": "shows the count of first word(s) for our questions.", "labels": [], "entities": []}, {"text": "We can see that while the popular question words (What, Who, etc.) are very common, there is a wide variety in the first word(s) indicating a diversity in question types.", "labels": [], "entities": []}, {"text": "About 28% of our questions require binary decisions (true/false or yes/no).", "labels": [], "entities": []}, {"text": "We randomly selected 60 multi-sentence questions from our corpus and asked two independent annotators to label them with the type of reasoning phenomenon required to answer them.", "labels": [], "entities": []}, {"text": "During this process, the annotators were shown a list of common reasoning phenomena (shown below), and they had to identify one or more of the phenomena relevant to a given question.", "labels": [], "entities": []}, {"text": "The list of phenomena shown to the annotators included the following categories: mathematical and logical reasoning, spatio-temporal reasoning, list/enumeration, coreference resolution (including implicit references, abstract pronouns, event coreference, etc.), causal relations, paraphrases and contrasts (including lexical relations such as synonyms, antonyms), commonsense knowledge, and 'other'.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 162, "end_pos": 184, "type": "TASK", "confidence": 0.8811144530773163}]}, {"text": "The categories were selected after a manual inspection of a subset of questions by two of the authors.", "labels": [], "entities": []}, {"text": "The annotation process revealed that answering questions in our corpus requires abroad variety of reasoning phenomena.", "labels": [], "entities": []}, {"text": "The left plot in provides detailed results.", "labels": [], "entities": []}, {"text": "The figure shows that a large fraction of questions require coreference resolution, and a more careful inspection revealed that there were different types of coreference phenomena at play here.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.9447965025901794}]}, {"text": "To investigate these further, we conducted a follow-up experiment in which manually annotated all questions that required coreference resolution into finer categories.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.9127193987369537}]}, {"text": "Specifically, each question was shown to two annotators who were asked to select one or more of the following categories: entity coreference (between two entities), event coreference (between two events), set inclusion coreference (one item is part of or included in the other) and 'other'.", "labels": [], "entities": []}, {"text": "shows the results of this experiment.", "labels": [], "entities": []}, {"text": "We can see that, as expected, entity coreference is the most common type of coreference resolution needed in our corpus.", "labels": [], "entities": [{"text": "entity coreference", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7274565100669861}, {"text": "coreference resolution", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.8703749179840088}]}, {"text": "However, a significant number of questions also require other types of coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.9337972700595856}]}, {"text": "We provide some examples of questions along with the required reasoning phenomena in Appendix II.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Various statistics of our dataset. Figures  in parentheses represent standard deviation.", "labels": [], "entities": [{"text": "standard", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.957089364528656}]}]}