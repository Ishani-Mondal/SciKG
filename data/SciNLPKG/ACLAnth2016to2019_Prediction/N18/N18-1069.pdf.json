{"title": [{"text": "Acquisition of Phrase Correspondences using Natural Deduction Proofs", "labels": [], "entities": []}], "abstractContent": [{"text": "How to identify, extract, and use phrasal knowledge is a crucial problem for the task of Recognizing Textual Entailment (RTE).", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 89, "end_pos": 125, "type": "TASK", "confidence": 0.8994773328304291}]}, {"text": "To solve this problem, we propose a method for detecting paraphrases via natural deduction proofs of semantic relations between sentence pairs.", "labels": [], "entities": []}, {"text": "Our solution relies on a graph reformulation of partial variable unifications and an algorithm that induces subgraph alignments between meaning representations.", "labels": [], "entities": []}, {"text": "Experiments show that our method can automatically detect various paraphrases that are absent from existing paraphrase databases.", "labels": [], "entities": []}, {"text": "In addition, the detection of paraphrases using proof information improves the accuracy of RTE tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9990713596343994}, {"text": "RTE tasks", "start_pos": 91, "end_pos": 100, "type": "TASK", "confidence": 0.9231263399124146}]}], "introductionContent": [{"text": "Recognizing Textual Entailment (RTE) is a challenging natural language processing task that aims to judge whether one text fragment logically follows from another text fragment (.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8853555023670197}]}, {"text": "Logic-based approaches have been successful in representing the meanings of complex sentences, ultimately having a positive impact on RTE (.", "labels": [], "entities": [{"text": "representing the meanings of complex sentences", "start_pos": 47, "end_pos": 93, "type": "TASK", "confidence": 0.8720963696638743}, {"text": "RTE", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.5409780740737915}]}, {"text": "Although logic-based approaches succeed in capturing the meanings of functional or logical words, it is difficult to capture the meanings of content words or phrases using genuine logical inference alone.", "labels": [], "entities": []}, {"text": "This remains a crucial problem in accounting for lexical relations between content words or phrases via logical inference.", "labels": [], "entities": [{"text": "accounting for lexical relations between content words or phrases", "start_pos": 34, "end_pos": 99, "type": "TASK", "confidence": 0.6954345670011308}]}, {"text": "To solve this problem, previous logic-based approaches use knowledge databases such as WordNet to identify lexical relations within a sentence pair.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9602527022361755}]}, {"text": "While this solution has been successful in handling word-level paraphrases, its extension to phrase-level semantic relations is still an unsolved problem.", "labels": [], "entities": []}, {"text": "There are three main difficulties that prevent an effective identification and use of phrasal linguistic knowledge.", "labels": [], "entities": []}, {"text": "The first difficulty is the presence of out-ofcontext phrase relations in popular databases such as the Paraphrase Database (PPDB) (.", "labels": [], "entities": []}, {"text": "PPDB may suggest paraphrases that do not adhere to the context of our relevant text segments nor to their semantic structure, which might be problematic.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8586012721061707}]}, {"text": "The second difficulty is finding semantic phrase correspondences between the relevant text segments.", "labels": [], "entities": [{"text": "semantic phrase correspondences between the relevant text segments", "start_pos": 33, "end_pos": 99, "type": "TASK", "confidence": 0.7286534681916237}]}, {"text": "Typical approaches only rely on surface () or syntactic correspondences, often producing inaccurate alignments that significantly impact our inference capabilities.", "labels": [], "entities": []}, {"text": "Instead, a mechanism to compute semantic phrase correspondences could potentially produce, if available, more coherent phrase pairs and solve the recurring issue of discontinuity.", "labels": [], "entities": []}, {"text": "The third difficulty is the intrinsic lack of coverage of databases for logical inference despite their large size.", "labels": [], "entities": [{"text": "logical inference", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7182504683732986}]}, {"text": "Whereas there is a relatively small number of possible word-to-word correspondences and thus their semantic relations can be enumerated, the same is not true for all phrase pairs that might be of interest.", "labels": [], "entities": []}, {"text": "One alternative is to use functions of infinite domain (e.g., cosine similarity) between phrase representations (, but these techniques are still underdevelopment, and we have not seen definitive successful applications when combined with logic systems.", "labels": [], "entities": []}, {"text": "In this study, we tackle these three problems.", "labels": [], "entities": []}, {"text": "The contributions of this paper are summarized as follows: First, we propose anew method of detecting phrase correspondences through natu-ral deduction proofs of semantic relations fora given sentence pair.", "labels": [], "entities": [{"text": "detecting phrase correspondences", "start_pos": 92, "end_pos": 124, "type": "TASK", "confidence": 0.812190592288971}]}, {"text": "Second, we show that our method automatically extracts various paraphrases that compensate fora shortage in previous paraphrase databases.", "labels": [], "entities": []}, {"text": "Experiments show that extracted paraphrases using proof information improve the accuracy of RTE tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9986333250999451}, {"text": "RTE tasks", "start_pos": 92, "end_pos": 101, "type": "TASK", "confidence": 0.9311733543872833}]}], "datasetContent": [{"text": "We use the SemEval-2014 version of the SICK dataset () for evaluation.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.7919976115226746}]}, {"text": "The SICK dataset is a dataset for semantic textual similarity (STS) as well as for RTE.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7111919522285461}, {"text": "semantic textual similarity (STS)", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.7053985049327215}, {"text": "RTE", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.8415528535842896}]}, {"text": "It was originally designed for evaluating compositional distributional semantics, so it contains logically challenging problems involving quantifiers, negation, conjunction, and disjunction, as well as inferences with lexical and phrasal knowledge.", "labels": [], "entities": []}, {"text": "The SNLI dataset (Bowman et al., 2015) contains inference problems requiring phrasal knowledge.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8420999944210052}]}, {"text": "However, it is not concerned with logically challenging expressions; the semantic relationships between a premise and a hypothesis are often limited to synonym/hyponym lexical substitution, replacements of short phrases, or exact word matching.", "labels": [], "entities": [{"text": "synonym/hyponym lexical substitution", "start_pos": 152, "end_pos": 188, "type": "TASK", "confidence": 0.6407012462615966}, {"text": "exact word matching", "start_pos": 224, "end_pos": 243, "type": "TASK", "confidence": 0.5760336816310883}]}, {"text": "This is because hypotheses are often parallel to the premise in structures and vocabularies.", "labels": [], "entities": []}, {"text": "The FraCaS dataset also contains logically complex problems.", "labels": [], "entities": [{"text": "FraCaS dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.97917440533638}]}, {"text": "However, it is confined to purely logical inferences and thus does not contain problems requiring inferences with lexical and phrasal knowledge.", "labels": [], "entities": []}, {"text": "For these reasons, we choose the SICK dataset to evaluate our method of using logical inference to extract phrasal knowledge.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.791520357131958}]}, {"text": "The SICK dataset contains 9927 sentence pairs with a 5000/4927 training/test split.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7767841517925262}]}, {"text": "These sentence pairs are manually annotated with three types of labels yes (entailment), no (contradiction), or unknown (neutral) (see for examples).", "labels": [], "entities": []}, {"text": "In RTE tasks, we need to consider a directional relation between words such as hypernym and hyponym to prove entailment and contradiction.", "labels": [], "entities": [{"text": "RTE tasks", "start_pos": 3, "end_pos": 12, "type": "TASK", "confidence": 0.9388554692268372}]}, {"text": "Hence, to extract phrasal knowledge for RTE tasks, we use the training data whose gold label is entailment or contradiction, excluding those with the neutral label.", "labels": [], "entities": [{"text": "RTE tasks", "start_pos": 40, "end_pos": 49, "type": "TASK", "confidence": 0.9329570531845093}]}, {"text": "For the natural deduction proofs, we used ccg2lambda , a higher-order automatic inference system, which converts CCG derivation trees into semantic representations and conducts natural deduction proofs automatically.", "labels": [], "entities": []}, {"text": "We parsed the tokenized sentences of the premises and hypotheses using three widecoverage CCG parsers: C&C, EasyCCG (, and depccg (.", "labels": [], "entities": [{"text": "EasyCCG", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.8267901539802551}]}, {"text": "CCG derivation trees (parses) were converted into logical semantic representations based on Neo-Davidsonian event semantics (Section 3.1).", "labels": [], "entities": []}, {"text": "The validation of semantic templates used for semantic representations was conducted exclusively on the trial split of the SICK dataset.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.8857503533363342}]}, {"text": "We used Coq, an interactive natural deduction theorem prover that we run fully automatically with a number of built-in theorem-proving routines called tactics, which include first-order logic.", "labels": [], "entities": []}, {"text": "We compare phrase abduction with different experimental conditions.", "labels": [], "entities": [{"text": "phrase abduction", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.8273893594741821}]}, {"text": "No axioms is our system without axiom injection.", "labels": [], "entities": []}, {"text": "W2W is the previous strategy of word abduction . P2P is our strategy of phrase abduction; W2W+P2P combines phrase abduction with word abduction.", "labels": [], "entities": []}, {"text": "In addition, we compare our system with three purely logic-based (unsupervised) approaches: The Meaning Factory (), LangPro (Abzianidze, 2015), and UTexas ().", "labels": [], "entities": [{"text": "LangPro (Abzianidze, 2015)", "start_pos": 116, "end_pos": 142, "type": "DATASET", "confidence": 0.8387255569299062}]}, {"text": "We also compare our system with machine learning-based approaches: the current state-of-the-art deep learning model GRU (, a loglinear regression model, and a hybrid approach combining a logistic regression model and probabilistic logic PL+eclassif (Beltagy et al., 2016).", "labels": [], "entities": [{"text": "GRU", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.7574067711830139}]}, {"text": "When the W2W+P2P result is substituted for the W2W result, there is a 1.1% increase inaccuracy (from 83.1% to 84.3%).", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9860471487045288}]}, {"text": "The accuracy of P2P is almost equal to that of W2W.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9998339414596558}]}, {"text": "This is because the recall improves from 63.6% to 72.1% while the precision decreases from 97.1% to 85.6%.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9996296167373657}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9998513460159302}]}, {"text": "The increase in false positive cases caused this result; some details of false positive cases are described in the next subsection.", "labels": [], "entities": []}, {"text": "W2W+P2P outperformed other purely logic-based systems.", "labels": [], "entities": []}, {"text": "The machine learning-based approaches outperform W2W+P2P, but unlike these approaches, parameter estimation is not used in our method.", "labels": [], "entities": []}, {"text": "This suggests that our method has the potential to increase the accuracy by using a classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.999381422996521}]}, {"text": "shows some positive and negative examples on RTE with the SICK dataset.", "labels": [], "entities": [{"text": "RTE", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8985992074012756}, {"text": "SICK dataset", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9231410324573517}]}, {"text": "For ID 9491, the sentence pair requires the paraphrase from afield of brown grass to a grassy area, not included in previous lexical knowledges.", "labels": [], "entities": [{"text": "ID 9491", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.7438822090625763}]}, {"text": "Our phrasal axiom injection can correctly generate this paraphrase from a natural deduction proof, and the system proves the entailment relation.", "labels": [], "entities": []}, {"text": "ID 2367 is also a positive example of phrasal axiom injection.", "labels": [], "entities": [{"text": "ID 2367", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9161543250083923}, {"text": "phrasal axiom injection", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.6206554075082144}]}, {"text": "The phrasal axiom between set fire to cameras and burn cameras with a blowtorch was generated.", "labels": [], "entities": []}, {"text": "This example shows that our semantic alignment succeeds in acquiring a general paraphrase by separating logical expressions such as some from content words and also by accounting for syntactic structures such as the passiveactive alternation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: RTE results on the SICK dataset.", "labels": [], "entities": [{"text": "RTE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8100795745849609}, {"text": "SICK dataset", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.8084035217761993}]}]}