{"title": [], "abstractContent": [{"text": "We address the task of detecting foiled image captions, i.e. identifying whether a caption contains a word that has been deliberately replaced by a semantically similar word, thus rendering it inaccurate with respect to the image being described.", "labels": [], "entities": [{"text": "detecting foiled image captions", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.7327498346567154}]}, {"text": "Solving this problem should in principle require a fine-grained understanding of images to detect linguistically valid perturbations in captions.", "labels": [], "entities": []}, {"text": "In such contexts , encoding sufficiently descriptive image information becomes a key challenge.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that it is possible to solve this task using simple, interpretable yet powerful representations based on explicit object information.", "labels": [], "entities": []}, {"text": "Our models achieve state-of-the-art performance on a standard dataset, with scores exceeding those achieved by humans on the task.", "labels": [], "entities": []}, {"text": "We also measure the upper-bound performance of our models using gold standard annotations.", "labels": [], "entities": []}, {"text": "Our analysis reveals that the simpler model performs well even without image information, suggesting that the dataset contains strong linguistic bias.", "labels": [], "entities": []}], "introductionContent": [{"text": "Models tackling vision-to-language (V2L) tasks, for example Image Captioning (IC) and Visual Question Answering (VQA), have demonstrated impressive results in recent years in terms of automatic metric scores.", "labels": [], "entities": [{"text": "Image Captioning (IC)", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.826089870929718}, {"text": "Visual Question Answering (VQA)", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.7596779018640518}, {"text": "automatic metric scores", "start_pos": 184, "end_pos": 207, "type": "METRIC", "confidence": 0.8265405098597208}]}, {"text": "However, whether or not these models are actually learning to address the tasks they are designed for is questionable.", "labels": [], "entities": []}, {"text": "For example, showed that IC models do not understand images sufficiently, as reflected by the generated captions.", "labels": [], "entities": []}, {"text": "As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information.", "labels": [], "entities": []}, {"text": "FOIL () is one such dataset.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.47912266850471497}]}, {"text": "It was proposed to evaluate the ability of V2L models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework.", "labels": [], "entities": []}, {"text": "This is done by replacing a word in MSCOCO () captions with a 'foiled' word that is semantically similar or related to the original word (substituting dog with cat), thus rendering the image caption unfaithful to the image content, while yet linguistically valid.", "labels": [], "entities": []}, {"text": "report poor performance for V2L models in classifying captions as foiled (or not).", "labels": [], "entities": []}, {"text": "They suggested that their models (using image embeddings as input) are very poor at encoding structured visuallinguistic information to spot the mismatch between a foiled caption and the corresponding content depicted in the image.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the foiled captions classification task (Section 2), and propose the use of explicit object detections as salient image cues for solving the task.", "labels": [], "entities": [{"text": "foiled captions classification task", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.7463295385241508}]}, {"text": "In contrast to methods from previous work that make use of word based information extracted from captions (, we use explicit object category information directly extracted from the images.", "labels": [], "entities": []}, {"text": "More specifically, we use an interpretable bag of objects as image representation for the classifier.", "labels": [], "entities": []}, {"text": "Our hypothesis is that, to truly 'understand' the image, V2L models should exploit information about objects and their relations in the image and not just global, low-level image embeddings as used by most V2L models.", "labels": [], "entities": []}, {"text": "Our main contributions are: 1.", "labels": [], "entities": []}, {"text": "A model (Section 3) for foiled captions classification using a simple and interpretable 433 object-based representation, which leads to the best performance in the task (Section 4); 2.", "labels": [], "entities": [{"text": "foiled captions classification", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.6723347504933676}]}, {"text": "Insights on upper-bound performance for foiled captions classification using gold standard object annotations (Section 4); 3.", "labels": [], "entities": [{"text": "foiled captions classification", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6617713868618011}]}, {"text": "An analysis of the models, providing insights into the reasons for their strong performance (Section 5).", "labels": [], "entities": []}, {"text": "Our results reveal that the FOIL dataset has a very strong linguistic bias, and that the proposed simple object-based models are capable of finding salient patterns to solve the task.", "labels": [], "entities": [{"text": "FOIL dataset", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.8652921319007874}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Dataset statistics for different foiled parts of  speech. The superscript f oil indicates the number of  foiled captions.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy on Verb, Adjective, Adverb and  Preposition datasets, using Gold Frequency as the im- age representation.  \u2020 is the best performing model as  reported in Shekhar et al. (2017a).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9808930158615112}, {"text": "Gold Frequency", "start_pos": 79, "end_pos": 93, "type": "METRIC", "confidence": 0.7646105587482452}]}, {"text": " Table 4: Ablation study on FOIL (Nouns).", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9219868779182434}, {"text": "FOIL", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.7059535980224609}]}]}