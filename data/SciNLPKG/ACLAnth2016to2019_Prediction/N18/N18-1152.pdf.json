{"title": [{"text": "Estimating Summary Quality with Pairwise Preferences", "labels": [], "entities": [{"text": "Estimating Summary", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7613982260227203}]}], "abstractContent": [{"text": "Automatic evaluation systems in the field of automatic summarization have been relying on the availability of gold standard summaries for over ten years.", "labels": [], "entities": [{"text": "summarization", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.7891103625297546}]}, {"text": "Gold standard summaries are expensive to obtain and often require the availability of domain experts to achieve high quality.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative evaluation approach based on pairwise preferences of sentences.", "labels": [], "entities": []}, {"text": "In comparison to gold standard summaries, they are simpler and cheaper to obtain.", "labels": [], "entities": []}, {"text": "In our experiments, we show that humans are able to provide useful feedback in the form of pairwise preferences.", "labels": [], "entities": []}, {"text": "The new framework performs better than the three most popular versions of ROUGE with less expensive human input.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.6105905175209045}]}, {"text": "We also show that our framework can reuse already available evaluation data and achieve even better results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to the huge amount of information contained in texts, the task of automatic text summarization) is a pressing challenge nowadays and will become even more important in the future.", "labels": [], "entities": [{"text": "text summarization)", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7066630919774374}]}, {"text": "Building summarization systems is, however, not the only challenge in this field.", "labels": [], "entities": []}, {"text": "Evaluation of automatically generated summaries is also an active field of research.", "labels": [], "entities": [{"text": "Evaluation of automatically generated summaries", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7946805238723755}]}, {"text": "Ideally, we would like to ask humans for their opinion about the quality of automatically generated summaries in an extrinsic evaluation.", "labels": [], "entities": []}, {"text": "Since summaries are generated for humans, they should also be evaluated directly by humans.", "labels": [], "entities": []}, {"text": "Unfortunately, manual evaluation cannot be performed at a large scale because of the huge effort which is necessary for evaluation.", "labels": [], "entities": []}, {"text": "reported that 3,000 hours of human effort would be required fora simple evaluation of the summaries for the Document Understanding Conference (DUC) 2003, a popular summarization shared task series.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC) 2003", "start_pos": 108, "end_pos": 152, "type": "TASK", "confidence": 0.58712095447949}, {"text": "summarization shared task series", "start_pos": 164, "end_pos": 196, "type": "TASK", "confidence": 0.9010235071182251}]}, {"text": "This motivates research of automatic evaluation methods for automatic summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.8259178996086121}]}, {"text": "ROUGE, the current method of choice for evaluating automated text summarization, relies on the availability of gold standard summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9416723847389221}, {"text": "evaluating automated text summarization", "start_pos": 40, "end_pos": 79, "type": "TASK", "confidence": 0.5294705852866173}]}, {"text": "The gold standard summaries are used to define the optimal output of a summarization system.", "labels": [], "entities": []}, {"text": "Writing high-quality summaries, however, requires the availability of expert writers and takes a lot of effort.", "labels": [], "entities": []}, {"text": "( reported that creating the reference summaries for the DUC 2005 shared task was a difficult endeavor with an effort of five hours to produce each reference summary.", "labels": [], "entities": [{"text": "DUC 2005 shared task", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.9292392879724503}]}, {"text": "Since ROUGE needs at least four reference summaries to become reasonably reliable, the effort sums up to at least 20 hours of annotation effort per topic.", "labels": [], "entities": []}, {"text": "For this reason, gold standard summaries are only available fora few, rather small datasets.", "labels": [], "entities": []}, {"text": "Also the more accurate (but also even more expensive) Pyramid method () requires expensive gold standard summaries.", "labels": [], "entities": []}, {"text": "Lack of larger and diverse evaluation corpora limits research in automatic summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.8216770887374878}]}, {"text": "Furthermore, currently available automatic evaluation methods are viewed with skepticism (.", "labels": [], "entities": []}, {"text": "Proper evaluation is, however, an indispensable ingredient for good research.", "labels": [], "entities": []}, {"text": "Computing the similarity between two summaries as in ROUGE is a very difficult task.", "labels": [], "entities": []}, {"text": "This seems to be obvious since estimating the similarity between sentences and even words is still an active field of research.", "labels": [], "entities": [{"text": "estimating the similarity between sentences and even words", "start_pos": 31, "end_pos": 89, "type": "TASK", "confidence": 0.7611735016107559}]}, {"text": "In this work, we present an alternative evaluation framework which does not use gold standard summaries to estimate the quality of summaries.", "labels": [], "entities": []}, {"text": "Instead of comparing automatically generated summaries with gold standard summaries, our model is trained with simple and inexpensive pairwise preferences of sentences.", "labels": [], "entities": []}, {"text": "To this end, we provide pairs of sentences from the input document of a summarization task to human annotators and ask which of the two sentences contains more important information.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9194881916046143}]}, {"text": "We use here the idea of intrinsic information importance) which describes that information can be intrinsically important.", "labels": [], "entities": []}, {"text": "For example, the information \"Donald Trump won the U.S. presidential election\" is intrinsically important.", "labels": [], "entities": []}, {"text": "It is likely that it should also be contained in the generated summary if this information is contained in an input document.", "labels": [], "entities": []}, {"text": "After collecting few preferences, our model uses the preferences to generate a ranking of all sentences according to information importance.", "labels": [], "entities": []}, {"text": "Summaries which contain sentences similar the upper part of the ranking are then considered to be better than summaries which contain unimportant sentences from the lower part of the ranking.", "labels": [], "entities": []}, {"text": "Pairwise preferences are an appealing form of annotation, since they are much easier to generate than producing complex gold standard summaries.", "labels": [], "entities": []}, {"text": "Not only collecting the annotations is easier, but also using the collected annotations is much simpler.", "labels": [], "entities": []}, {"text": "The presented model does not have to solve the difficult task of estimating the similarity between generated and gold standard summaries.", "labels": [], "entities": []}, {"text": "Instead, the model uses the ranking to estimate the summary quality.", "labels": [], "entities": []}, {"text": "provides an illustration of the traditional evaluation and our new model.", "labels": [], "entities": []}, {"text": "On the left, the input documents are illustrated which should be summarized.", "labels": [], "entities": []}, {"text": "In the upper part gold standard summaries are generated by humans and used to estimate the quality of an automatically generated summary.", "labels": [], "entities": []}, {"text": "In the lower part, we collect pairwise preferences of sentences and use the preferences for evaluation.", "labels": [], "entities": []}, {"text": "An evaluation on topics from two standard datasets, looking at predicting the relative ratings of automatically generated summaries, shows that our new evaluation model is as good as or better than existing methods, at a much lower annotation cost.", "labels": [], "entities": []}], "datasetContent": [{"text": "Model-free evaluation methods Jensen-Shannon divergence (Louis and Nenkova, 2013) do not require human input such as gold standard summaries and can therefore be applied without additional cost.", "labels": [], "entities": [{"text": "Jensen-Shannon divergence", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.5808114409446716}]}, {"text": "The quality of model-free evaluation methods is however limited, which is validated in our experiments (see Section 5).", "labels": [], "entities": []}, {"text": "In this section, we present a novel framework which does not infer a ranking of automatically generated summaries based on gold standard summaries but based on pairwise preferences.", "labels": [], "entities": []}, {"text": "The fundamental idea is not to rely on expensive gold standard summaries as previous work does, but to ask annotators for their preferences about sentences.", "labels": [], "entities": []}, {"text": "Annotates label pairs of sentences with a preference label which indicates which sentence contains more important information.", "labels": [], "entities": []}, {"text": "illustrates such a pairwise preference annotation.", "labels": [], "entities": []}, {"text": "A human would likely prefer the first sentence to be included in a summary instead of the second sentence because the first sentence contains, compared to the second sentence, relatively important information.", "labels": [], "entities": []}, {"text": "Based on the preferences, our model generates a ranking which reflects the importance of information which is contained in the sentences.", "labels": [], "entities": []}, {"text": "Sentences with important information will be ranked high whereas sentences containing only less important information will be ranked low.", "labels": [], "entities": []}, {"text": "We provide in this section a detailed analysis of our proposed evaluation method.", "labels": [], "entities": []}, {"text": "For the experiment, we use eight topics from two popular multidocument summarization datasets, the DUC 2004 (DUC04) and TAC 2009 (TAC09) corpora, which are freely available upon request.", "labels": [], "entities": [{"text": "DUC 2004 (DUC04) and TAC 2009 (TAC09) corpora", "start_pos": 99, "end_pos": 144, "type": "DATASET", "confidence": 0.8431011339028677}]}, {"text": "Each topic in the datasets contains ten source documents.: Agreement of preference based evaluation as defined in Equation 1 of different versions of JensenShannon, ROUGE and our novel model based on manually labeled pairwise preferences.", "labels": [], "entities": [{"text": "JensenShannon", "start_pos": 150, "end_pos": 163, "type": "DATASET", "confidence": 0.9862365126609802}, {"text": "ROUGE", "start_pos": 165, "end_pos": 170, "type": "METRIC", "confidence": 0.824710488319397}]}, {"text": "generated summaries were evaluated by humans.", "labels": [], "entities": []}, {"text": "Each summary was labeled with a score from 1 to 5 (DUC04) or 1 to 10 (TAC09) indicating the information content of the summary.", "labels": [], "entities": [{"text": "TAC09)", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9431241452693939}]}, {"text": "Evaluation of grammatically, writing style, etc. is not included in the scores.", "labels": [], "entities": []}, {"text": "An evaluation model predicts the preference for two selected summaries correctly if the model predicts the same preference according to the annotated reference scores and incorrectly otherwise.", "labels": [], "entities": []}, {"text": "We do not consider ties in the experiments.", "labels": [], "entities": []}, {"text": "In the following, we report the agreement as described in Equation 1 for various experiments.", "labels": [], "entities": []}, {"text": "We use the abbreviations JS (JensenShannon), R1 -R4 (ROUGE-1 -ROUGE-4), SU4 (ROUGE-SU4), and PY (Pyramid () to denote the reference systems.", "labels": [], "entities": [{"text": "JensenShannon", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.6524879932403564}, {"text": "R1 -R4 (ROUGE-1 -ROUGE-4)", "start_pos": 45, "end_pos": 70, "type": "METRIC", "confidence": 0.679479893296957}, {"text": "PY", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9514962434768677}]}, {"text": "We now investigate the ranking generated by our model directly.", "labels": [], "entities": []}, {"text": "Since individual sentences are annotated in the TAC 2009 corpus with SCUs, we can generate a ranking of the sentences and directly compare this ranking with the ranking generated by our model.", "labels": [], "entities": [{"text": "TAC 2009 corpus", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9586340586344401}]}, {"text": "Smoothed sampling improves the raking of the model if we use 200 manual or 200 reference summary-based preferences in the TAC 2009 corpus.", "labels": [], "entities": [{"text": "TAC 2009 corpus", "start_pos": 122, "end_pos": 137, "type": "DATASET", "confidence": 0.9700575470924377}]}, {"text": "Given that we can sample pairs based on Pyramid scores, the model is able to reconstruct the ranking almost perfectly if we do not use smoothed sampling.", "labels": [], "entities": []}, {"text": "With smoothed sampling, the performance decreases in this case.", "labels": [], "entities": []}, {"text": "The result confirms the previously observed performance at summary scoring where preferences based on Pyramid annotations performed best followed by manually generated preference annotations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Agreement of preference based evaluation as  defined in Equation 1 of different versions of Jensen- Shannon, ROUGE and our novel model based on man- ually labeled pairwise preferences.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.950851321220398}]}, {"text": " Table 2: Agreement of different versions of ROUGE  and Pyramid (PY) and our novel models based on hu- man and automatically generated pairwise preferences  in addition to manually labeled preferences.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9274033308029175}]}, {"text": " Table 3: Agreement with human judgments for refer- ence systems and our model fed with only automati- cally generated preferences labels.", "labels": [], "entities": []}, {"text": " Table 4: Percentage of correctly ordered sentence pairs  in the TAC 2009 corpus for both a non-smoothed and a  smoothed sampling.", "labels": [], "entities": [{"text": "TAC 2009 corpus", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.9581005374590555}]}]}