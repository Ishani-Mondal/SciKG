{"title": [], "abstractContent": [{"text": "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al.", "labels": [], "entities": []}, {"text": "(2017) achieves state-of-the-art results for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.815177321434021}]}, {"text": "In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure.", "labels": [], "entities": []}, {"text": "Instead, it requires adding representations of absolute positions to its inputs.", "labels": [], "entities": []}, {"text": "In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions , or distances between sequence elements.", "labels": [], "entities": []}, {"text": "On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations , respectively.", "labels": [], "entities": [{"text": "WMT 2014 English-to-German and English-to-French translation tasks", "start_pos": 7, "end_pos": 73, "type": "TASK", "confidence": 0.7307108214923314}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9981447458267212}, {"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9926226139068604}]}, {"text": "Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality.", "labels": [], "entities": []}, {"text": "We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent approaches to sequence to sequence learning typically leverage recurrence), convolution), attention (, or a combination of recurrence and attention ( as basic building blocks.", "labels": [], "entities": []}, {"text": "These approaches incorporate information about the sequential position of elements differently.", "labels": [], "entities": []}, {"text": "Recurrent neural networks (RNNs) typically compute a hidden state ht , as a function of their input at time t and a previous hidden state h t\u22121 , capturing relative and absolute positions along the time dimension directly through their sequential structure.", "labels": [], "entities": []}, {"text": "Non-recurrent models do not necessarily consider input elements sequentially and may hence require explicitly encoding position information to be able to use sequence order.", "labels": [], "entities": []}, {"text": "One common approach is to use position encodings which are combined with input elements to expose position information to the model.", "labels": [], "entities": []}, {"text": "These position encodings can be a deterministic function of position ( or learned representations.", "labels": [], "entities": []}, {"text": "Convolutional neural networks inherently capture relative positions within the kernel size of each convolution.", "labels": [], "entities": []}, {"text": "They have been shown to still benefit from position encodings, however.", "labels": [], "entities": []}, {"text": "For the Transformer, which employs neither convolution nor recurrence, incorporating explicit representations of position information is an especially important consideration since the model is otherwise entirely invariant to sequence ordering.", "labels": [], "entities": []}, {"text": "Attention-based models have therefore used position encodings or biased attention weights based on distance.", "labels": [], "entities": []}, {"text": "In this work we present an efficient way of incorporating relative position representations in the self-attention mechanism of the Transformer.", "labels": [], "entities": []}, {"text": "Even when entirely replacing its absolute position encodings, we demonstrate significant improvements in translation quality on two machine translation tasks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7346152365207672}]}, {"text": "Our approach can be cast as a special case of extending the self-attention mechanism of the Transformer to considering arbitrary relations between any two elements of the input, a direction we plan to explore in future work on modeling labeled, directed graphs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the tensor2tensor 1 library for training and evaluating our model.", "labels": [], "entities": []}, {"text": "We evaluated our model on the WMT 2014 machine translation task, using the WMT 2014 English-German dataset consisting of approximately 4.5M sentence pairs and the 2014 WMT English-French dataset consisting of approximately 36M sentence pairs.: Experimental results for WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) translation tasks, using newstest2014 test set.", "labels": [], "entities": [{"text": "WMT 2014 machine translation task", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.6976756572723388}, {"text": "WMT 2014 English-German dataset", "start_pos": 75, "end_pos": 106, "type": "DATASET", "confidence": 0.8738808929920197}, {"text": "WMT English-French dataset", "start_pos": 168, "end_pos": 194, "type": "DATASET", "confidence": 0.8984541098276774}, {"text": "WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) translation tasks", "start_pos": 269, "end_pos": 351, "type": "TASK", "confidence": 0.533708026775947}, {"text": "newstest2014 test set", "start_pos": 359, "end_pos": 380, "type": "DATASET", "confidence": 0.9733195900917053}]}], "tableCaptions": [{"text": " Table 1: Experimental results for WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) trans- lation tasks, using newstest2014 test set.", "labels": [], "entities": [{"text": "WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) trans- lation tasks", "start_pos": 35, "end_pos": 119, "type": "TASK", "confidence": 0.6273507495721181}, {"text": "newstest2014 test set", "start_pos": 127, "end_pos": 148, "type": "DATASET", "confidence": 0.9491848349571228}]}, {"text": " Table 2: Experimental results for varying the clipping  distance, k.", "labels": [], "entities": []}]}