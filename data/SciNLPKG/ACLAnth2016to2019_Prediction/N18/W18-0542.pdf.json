{"title": [{"text": "Predicting Second Language Learner Successes and Mistakes by Means of Conjunctive Features", "labels": [], "entities": [{"text": "Predicting Second Language Learner Successes and Mistakes", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.868014999798366}]}], "abstractContent": [{"text": "This paper describes the system developed by the Centre for English Corpus Linguistics for the 2018 Duolingo SLAM challenge.", "labels": [], "entities": [{"text": "Duolingo SLAM challenge", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.507836103439331}]}, {"text": "It aimed at predicting the successes and mistakes of second language learners on each of the words that compose the exercises they answered.", "labels": [], "entities": []}, {"text": "Its main characteristic is to include conjunctive features, built by combining word ngrams with metadata about the user and the exercise.", "labels": [], "entities": []}, {"text": "It achieved a relatively good performance, ranking fifth out of 15 systems.", "labels": [], "entities": []}, {"text": "Complementary analyses carried out to gauge the contribution of the different sets of features to the performance confirmed the usefulness of the con-junctive features for the SLAM task.", "labels": [], "entities": [{"text": "SLAM task", "start_pos": 176, "end_pos": 185, "type": "TASK", "confidence": 0.9393029510974884}]}], "introductionContent": [{"text": "This paper presents the participation of the Centre for English Corpus Linguistics (CECL) in the 2018 Duolingo shared task on Second Language Acquisition Modeling (SLAM) which was held in conjunction with the 13th Workshop on Innovative Use of NLP for Building Educational Applications.", "labels": [], "entities": [{"text": "Duolingo shared task on Second Language Acquisition Modeling (SLAM)", "start_pos": 102, "end_pos": 169, "type": "TASK", "confidence": 0.6564952189272101}]}, {"text": "The objective of the task is to build a model to predict whether second language learners will make a mistake on each of the words (tokens) that compose the exercises they answered.", "labels": [], "entities": []}, {"text": "There were three tracks: English speakers learning Spanish (es en), Spanish speakers learning English (es en) and English speakers learning French (en en).", "labels": [], "entities": []}, {"text": "To develop the model, the organizers of the challenge made available a very large number of exercises carried out by a large number of learners of Duolingo, a free online language-learning platform, which attracted more than 200 million learners since its launching in 2012 (see for details).", "labels": [], "entities": []}, {"text": "In this training set, the tokens on which each learner made a mistake were marked, but the error itself was not provided.", "labels": [], "entities": []}, {"text": "This task is thus very different from the one at the root of many applications of natural language processing in the field of education that aim to automatically evaluate texts produced by second language learners.", "labels": [], "entities": []}, {"text": "The traditional approach for the latter, which relies on linguistic indices more or less strongly correlated with text quality such as lexical richness, syntactic complexity and especially the presence of errors of different types (e.g.,, is obviously not applicable to the SLAM challenge.", "labels": [], "entities": [{"text": "SLAM challenge", "start_pos": 274, "end_pos": 288, "type": "TASK", "confidence": 0.9330755770206451}]}, {"text": "Compared to the automatic evaluation of learner texts, the SLAM task has several advantages (+), but also several disadvantages (-): + Each learner produced a relatively large number of responses allowing to estimate his or her level of competence; + The learners' responses are spaced out in time making possible to try to model the evolution of their competence throughout their learning; + The same exercises were presented to a large number of different learners making it possible to get a relatively good estimate of the difficulty of each of them; -The exercises are very short, as 99% of the utterances consist of no more than six tokens, which strongly limits the linguistic context available for any NLP procedure; -And above all, as indicated above, the prompt to be processed by the learner is provided, but not the actual answer.", "labels": [], "entities": [{"text": "SLAM task", "start_pos": 59, "end_pos": 68, "type": "TASK", "confidence": 0.909574568271637}]}, {"text": "As previous research of the CECL in this field deals with the question of automatic evaluation and only partially took into account the temporal dimension of learning, I chose to breakdown the problem in two steps: \u2022 Try to get the best prediction without using the sequential information available in the dataset.", "labels": [], "entities": []}, {"text": "\u2022 Add the sequential information and see whether it can improve the prediction.", "labels": [], "entities": []}, {"text": "Having not been successful in the second step, I focused this report on the first.", "labels": [], "entities": []}, {"text": "It is therefore not really an attempt to model second language acquisition, but to predict the successes and mistakes of second language learners.", "labels": [], "entities": [{"text": "second language acquisition", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6999003489812216}]}, {"text": "The proposed system can be seen as a baseline system since it does not take into account the richest information made available.", "labels": [], "entities": []}, {"text": "The developed system achieved a relatively good performance since it ranks fifth out of 15 systems, but nevertheless at a respectable distance from the best systems.", "labels": [], "entities": []}, {"text": "Its main characteristic is to include conjunctive features, built by combining several primitive features.", "labels": [], "entities": []}, {"text": "In machine learning, these conjunctive features are classically obtained by means of a polynomial kernel, but this has the effect of greatly lengthening the time needed to learn the model.", "labels": [], "entities": []}, {"text": "It was more efficient to obtain them manually and to use a (much faster) linear approach to learn the model.", "labels": [], "entities": []}, {"text": "The remainder of this report describes the datasets made available for this challenge, the system developed and the results obtained as well as the analyzes performed to get a better idea of the usefulness of the various components of the system.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Final performances (AUROC) for several sys- tems.", "labels": [], "entities": [{"text": "AUROC)", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9331735074520111}]}, {"text": " Table 2: AUROC for several sets of features in the base  model.", "labels": [], "entities": [{"text": "AUROC", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.601253867149353}]}, {"text": " Table 3: AUROC for the three ngram lengths (base  model).", "labels": [], "entities": [{"text": "AUROC", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.6970475316047668}]}, {"text": " Table 4: Number of features of each type selected by  the L1-LR. Note: The conjunctive features are represented", "labels": [], "entities": []}, {"text": " Table 5: AUROC for several subsets of features in the base model.", "labels": [], "entities": [{"text": "AUROC", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.6969648599624634}]}]}