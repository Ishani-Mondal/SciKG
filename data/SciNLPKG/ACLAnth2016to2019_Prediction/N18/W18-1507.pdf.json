{"title": [{"text": "Neural Events Extraction from Movie Descriptions", "labels": [], "entities": [{"text": "Neural Events Extraction from Movie Descriptions", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7700875600179037}]}], "abstractContent": [{"text": "We present a novel approach for event extraction and abstraction from movie descriptions.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7571845948696136}]}, {"text": "Our event frame consists of 'who\", \"did what\" \"to whom\", \"where\", and \"when\".", "labels": [], "entities": []}, {"text": "We formulate our problem using a recurrent neural network, enhanced with structural features extracted from syntactic parser, and trained using curriculum learning by progressively increasing the difficulty of the sentences.", "labels": [], "entities": []}, {"text": "Our model serves as an intermediate step towards question answering systems, visual storytelling, and story completion tasks.", "labels": [], "entities": [{"text": "question answering", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.905736118555069}, {"text": "story completion", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.7751765549182892}]}, {"text": "We evaluate our approach on MovieQA dataset.", "labels": [], "entities": [{"text": "MovieQA dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9921371936798096}]}], "introductionContent": [{"text": "Understanding events is important to understanding a narrative.", "labels": [], "entities": []}, {"text": "Event complexity varies from one story to another and the ability to extract and abstract them is essential for multiple applications.", "labels": [], "entities": []}, {"text": "For question answering systems, a question narrows the scope of events to examine for an answer.", "labels": [], "entities": [{"text": "question answering", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.843402236700058}]}, {"text": "For storytelling, events build an image in the reader's mind and constructs a storyline.", "labels": [], "entities": [{"text": "storytelling", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9645909667015076}]}, {"text": "For event extraction, we apply Natural Language Processing (NLP) techniques to construct an event frame consisting of: \"who\", \"did what\" \"to whom\", \"where\", and \"when\".", "labels": [], "entities": [{"text": "event extraction", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8225484192371368}]}, {"text": "The more complex questions of \"how\" and \"why\" requires significantly more reasoning and beyond this paper's scope.", "labels": [], "entities": []}, {"text": "Most syntactic NLP parsers, such as CoreNLP () and NLTK (, focused on examining characteristics of the words, grammatical structure, word order, and meaning.", "labels": [], "entities": []}, {"text": "On the other hand neural NLP approaches, such as SLING () relies on large corpora to train such models in addition multiple knowledge databases.", "labels": [], "entities": []}, {"text": "These approaches perform event extraction without context (often visual) of a movie or a movie script.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7597119808197021}]}, {"text": "When performing event extraction in relation to events in a story, the context can be gleaned from descriptions of the set or characters or prior events in a sequence.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7474052906036377}]}, {"text": "Additionally, we intend to develop an event extraction framework fora mixed-initiative, human-computer, system and intend to generate a human-readable event structure for user interaction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7173479199409485}]}, {"text": "In departure from syntactic approaches, we propose a hybrid, neural and symbolic, approach to address this problem.", "labels": [], "entities": []}, {"text": "We benefit from both neural and symbolic formulations to extract events from movie text.", "labels": [], "entities": []}, {"text": "Neural networks have been successfully applied to NLP problems, specifically, sequence-to-sequence or (sequenceto-vector) models) applied to machine translation and word-to-vector ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.8017551898956299}]}, {"text": "Here, we combine those approaches with supplemental structural information, specifically sentence length.", "labels": [], "entities": []}, {"text": "Our approach models local information and global sentence structure.", "labels": [], "entities": []}, {"text": "For our training paradigm, we explored curriculum learning.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to apply it to event extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.8687496185302734}]}, {"text": "Curriculum learning proposes a model can learn to perform better on a difficult task if it is first presented with easier training examples.", "labels": [], "entities": []}, {"text": "Generally, in prior curriculum learning work, the final model attains a higher performance than if it were trained on the most difficult task from the start.", "labels": [], "entities": []}, {"text": "In this work, we base the curriculum on sentence length, reasoning that shorter sentences have a simpler structure.", "labels": [], "entities": []}, {"text": "Other difficulty metrics such as average word length, longest word length, and FleschKincaid readability score were not considered in this experiment, but maybe considered for future work.", "labels": [], "entities": [{"text": "average word length", "start_pos": 33, "end_pos": 52, "type": "METRIC", "confidence": 0.7259168922901154}, {"text": "FleschKincaid readability score", "start_pos": 79, "end_pos": 110, "type": "METRIC", "confidence": 0.9192062417666117}]}, {"text": "Instead of treating the sentence-to-event problem as a complete black-box putting the burden on the deep learning model, we simplify the problem by adding structure to the output sentence following the event frame structure, where some of the components could be present or absent.", "labels": [], "entities": []}, {"text": "Furthermore, some sentences could contain multiple events.", "labels": [], "entities": []}, {"text": "Weak labels were extracted from each sentence using the predicate as an anchor.", "labels": [], "entities": []}, {"text": "We use structure rather than a bag-of-words because it encodes information about the relationships between words.", "labels": [], "entities": []}, {"text": "Our contributions are three-fold: \u2022 New formulation for event extraction in movie descriptions.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7297825068235397}]}, {"text": "\u2022 A curriculum learning framework for difficulty based learning.", "labels": [], "entities": []}, {"text": "\u2022 Benchmarking symbolic and neural approaches on MovieQA dataset.", "labels": [], "entities": [{"text": "MovieQA dataset", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.9757868945598602}]}, {"text": "The paper is organized as follows: section 2 reviews prior work; Section 3 formulates our approach; Section 4 specifies the learning framework; Section 5 presents our experiments; Section 6 describes our future work and conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "MovieQA Dataset (: We use the descriptive video service (DVS) text from MovieQA.", "labels": [], "entities": [{"text": "MovieQA Dataset", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.969137966632843}, {"text": "MovieQA", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.8923850655555725}]}, {"text": "The DVS sentences tend to be simpler and describe the scene explicitly compared to the plot synopsis sentences.", "labels": [], "entities": []}, {"text": "We generate an initial training corpus of extracted events using dependency parsers and information extraction annotators from CoreNLP.", "labels": [], "entities": [{"text": "CoreNLP", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.9175171852111816}]}, {"text": "A total of 36,898 events are generated.", "labels": [], "entities": []}, {"text": "Analyzing the corpus of extracted events, we found the longest sentence length contained 62 words.", "labels": [], "entities": []}, {"text": "However, by limiting our dataset to sentences with 25 words or less, we retained 97% of the data (35791 sentences).", "labels": [], "entities": []}, {"text": "shows the sentence length distribution of the DVS data and the plot synopsis data.", "labels": [], "entities": [{"text": "DVS data", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.889369547367096}]}, {"text": "We did not experiment with the plot synopsis data, rather we wish to highlight the difference in sentence lengths between the 2 sets of data.", "labels": [], "entities": []}, {"text": "The DVS data is heavily skewed towards shorter sentences, most likely due to requiring concise descriptions about what is happening onscreen at that time.", "labels": [], "entities": [{"text": "DVS data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7242107391357422}]}, {"text": "Plot synopsis sentences tend to be longer as they tend to summarize multiple actions and plot points.", "labels": [], "entities": []}, {"text": "Sentences with multiple predicates generated multiple events and this manifested itself as duplicate sentences in our dataset with multiple label sequences.", "labels": [], "entities": []}, {"text": "Sentences with multiple events accounted for about 24% of the data.", "labels": [], "entities": []}, {"text": "This does lead to complications for train-  ing as we did not distinguish between events.", "labels": [], "entities": [{"text": "train-  ing", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.7926300366719564}]}, {"text": "The difficulty classes computed by sentence length and are as follows: easy sentences are 8 words or less with an average sentence length of 6.5 words, medium sentences are 9-12 words with an average length of 10.4 words, and hard sentences are 13 words or longer with an average length of 16.8 words.", "labels": [], "entities": []}, {"text": "Each difficulty class is contains one third of the original data set.", "labels": [], "entities": []}, {"text": "For training with and without a curriculum, we used an 80-20 train-test split.", "labels": [], "entities": []}, {"text": "For curriculum learning, each difficulty was trained on 80% of each of the respective difficulty sets and tested on the remaining 20%.", "labels": [], "entities": []}, {"text": "The extracted events provide weak labels generated by the CoreNLP algorithm approach.", "labels": [], "entities": []}, {"text": "Pre-processing: We tokenized the text assigning an integer to each word after removing capitalization and apostrophes.", "labels": [], "entities": []}, {"text": "Sentences are vectorized using this index.", "labels": [], "entities": []}, {"text": "The output format assigns integers between 1-5 to parts of the sentence based on which elements of the sentence are part of the subject (1), predicate (2), object (3), location (4), or time (5) phrases.", "labels": [], "entities": []}, {"text": "Articles, prepositions, conjunctions, adjectives, and adverbs were often assigned the null class (0) although some maybe included parts of event phrases.", "labels": [], "entities": []}, {"text": "Sentences are leftpadded with zeros make all sentence vectors the same length.", "labels": [], "entities": []}, {"text": "Implementation Details: The trained model is a basic LSTM model.", "labels": [], "entities": [{"text": "Implementation Details", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7350369393825531}]}, {"text": "We employ two different training approaches.", "labels": [], "entities": []}, {"text": "In the first approach, we ignore the sentence length and use random batches of training data.", "labels": [], "entities": []}, {"text": "We train for 300 epochs.", "labels": [], "entities": []}, {"text": "Second, we use a training curriculum based on sentence length, starting training with shorter sentences and progressing to longer sentences.", "labels": [], "entities": []}, {"text": "The sentences are divided into easy, medium, and hard difficulty sets with each set containing roughly one-third of the total data set.", "labels": [], "entities": []}, {"text": "We holdout a set of data from each difficulty level for validation.", "labels": [], "entities": [{"text": "validation", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.9744061231613159}]}, {"text": "We train the model for 100 epochs on each level of difficulty to match the total of 300 epochs of the random training curriculum.", "labels": [], "entities": []}, {"text": "The final approach is to start with shorter sentence lengths and train with longer sentences towards the end.", "labels": [], "entities": []}, {"text": "We examine four parameters: the learning rate, the embedding dimension, the hidden dimension, and number of training epochs.", "labels": [], "entities": []}, {"text": "A grid search was employed to examine the effects of these parameters on the validation accuracy.", "labels": [], "entities": [{"text": "validation", "start_pos": 77, "end_pos": 87, "type": "TASK", "confidence": 0.9568814039230347}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9127733707427979}]}, {"text": "We varied learning rate in powers of 10 from 1e \u2212 5 to 1e \u2212 2.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.8956418931484222}]}, {"text": "The embedding dimension was varied from 50-500 in increments of 50.", "labels": [], "entities": []}, {"text": "The hidden dimension was varied from 48-512 in increments of 16.", "labels": [], "entities": []}, {"text": "Lastly, we varied the number of training epochs from 300-2000 in increments of 200.", "labels": [], "entities": []}, {"text": "Below we include a sample of figures from this search where we fixed the number of epochs (300) and the hidden layer dimension (512) while adjusting the learning rate and the embedding dimension.", "labels": [], "entities": []}, {"text": "For these parameters, we found an embedding dimension of 350 performs best on the easy and medium difficulty levels.", "labels": [], "entities": []}, {"text": "Additional training is in progress.", "labels": [], "entities": []}, {"text": "We used ADAM () for gradient optimization.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.7086594700813293}, {"text": "gradient optimization", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7244890332221985}]}, {"text": "Our loss function was a weighted categorical cross entropy function using the class distribution from the training data as class weights.", "labels": [], "entities": []}, {"text": "Accuracy is calculated by the frequency with which the predicted class matches the labels.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9893256425857544}]}, {"text": "The accuracy is then the total count of actual matches by the total potential matches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997004270553589}]}], "tableCaptions": []}