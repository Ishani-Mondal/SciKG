{"title": [{"text": "Syntactic Patterns Improve Information Extraction for Medical Search", "labels": [], "entities": [{"text": "Syntactic Patterns Improve Information Extraction", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.917936372756958}, {"text": "Medical Search", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.6710384786128998}]}], "abstractContent": [{"text": "Medical professionals search the published literature by specifying the type of patients, the medical intervention(s) and the outcome mea-sure(s) of interest.", "labels": [], "entities": []}, {"text": "In this paper we demonstrate how features encoding syntactic patterns improve the performance of state-of-the-art sequence tagging models (both linear and neu-ral) for information extraction of these medically relevant categories.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 168, "end_pos": 190, "type": "TASK", "confidence": 0.8133522570133209}]}, {"text": "We present an analysis of the type of patterns exploited, and the semantic space induced for these, i.e., the distributed representations learned for identified multi-token patterns.", "labels": [], "entities": []}, {"text": "We show that these learned representations differ substantially from those of the constituent unigrams, suggesting that the patterns capture contextual information that is otherwise lost.", "labels": [], "entities": []}], "introductionContent": [{"text": "The efficacy of medical treatments depends on patient characteristics, treatment administration details (e.g., dosage) and the measures or outcomes used to quantify treatment success.", "labels": [], "entities": []}, {"text": "These criteria should be precisely defined when searching the medical literature ().", "labels": [], "entities": []}, {"text": "Unfortunately, these aspects are not usually described in a structured way.", "labels": [], "entities": []}, {"text": "Abstracts with explicit category headings () partially address this, but these are not standardized nor uniform.", "labels": [], "entities": []}, {"text": "Automated solutions are thus emerging to better support medical search, including methods for: identifying sentences containing key pieces of clinical information (); summarization (; identifying contradictory claims in medical articles (; and information retrieval system prototypes that harness this type of information (.", "labels": [], "entities": [{"text": "medical search", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.7704934775829315}, {"text": "summarization", "start_pos": 167, "end_pos": 180, "type": "TASK", "confidence": 0.9850012063980103}]}, {"text": "* * now at Google Inc.", "labels": [], "entities": []}, {"text": "Several studies have assessed the use of the PICO framework (.", "labels": [], "entities": [{"text": "PICO framework", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.780554324388504}]}, {"text": "Our task is also to identify spans of text describing PICO elements i.e., the participants (P), interventions (I)/comparators (C), and outcomes (O) in the abstracts of articles reporting findings from randomized controlled trails (RCTs).", "labels": [], "entities": []}, {"text": "We exploit the availability of structured abstracts in the medical domain: from these coarse (multi-)sentence labels we derive patterns typically used in bootstrap methods for entity recognition and relation extraction.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.7642450928688049}, {"text": "relation extraction", "start_pos": 199, "end_pos": 218, "type": "TASK", "confidence": 0.8333665728569031}]}, {"text": "We incorporate these patterns into supervised sequence labeling models to improve the identification of P, I and O spans in new texts.", "labels": [], "entities": []}, {"text": "Below we show examples of each extraction type: patterns are bolded and target PICO description spans italicized.", "labels": [], "entities": []}, {"text": "The extracted patterns disambiguate fairly well the type of information expressed in the segment when individual words (e.g., \"children\"), do not.", "labels": [], "entities": []}, {"text": "(P) The trial included 230 children with Stage-IV lymphoblastic leukemia ...", "labels": [], "entities": []}, {"text": "(I) In Group I, the children were treated with prednisone ...", "labels": [], "entities": []}, {"text": "reported that Group 2 children underwent fewer isolated bone marrow relapses ..", "labels": [], "entities": []}, {"text": "We explore three strategies for exploiting extracted patterns in a state-of-the-art LSTM-CRF sequence tagging model (: as additional features at the CRF layer; as one-hot indicators concatenated to distributed representations of words; and as individual units embedded in a semantic space shared with words.", "labels": [], "entities": [{"text": "LSTM-CRF sequence tagging", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.6347059309482574}]}, {"text": "The second representation improves recall for two extraction tasks, and the third improves precision for all three tasks.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9994261264801025}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.999337375164032}]}, {"text": "We analyze the induced semantic space to show that patterns capture contextual information that is otherwise lost.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Models for extracting Participants, Intervention and Outcomes with and without pattern features, evalu- ated via token-level precision, recall and F1 scores. The first and second groups of rows report results for CRF and  LSTM-CRF models without and with pattern features. The bottom group reports results achieved using different  means of incorporating pattern features in neural models.", "labels": [], "entities": [{"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.890648365020752}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9993094205856323}, {"text": "F1", "start_pos": 157, "end_pos": 159, "type": "METRIC", "confidence": 0.9990980625152588}]}, {"text": " Table 3: Results to illustrate syntactic nature of Autoslog bigrams. Row 1 shows results of baseline model with no  added features. Row 2 shows results of the model that uses all bigrams as features and Row 3 shows results of the  model that uses only Autoslog extracted bigrams as features. Features are added before the LSTM, as incorporated  in the best working model from Table 1.", "labels": [], "entities": []}, {"text": " Table 4: LSTM-CRF predictions on word embeddings  trained on the same 6 million documents. Column 1  shows the type of embedding, column 2 shows the size  of the vocabulary and columns 3-5 show F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.984875351190567}]}]}