{"title": [{"text": "Noise-Robust Morphological Disambiguation for Dialectal Arabic", "labels": [], "entities": [{"text": "Noise-Robust Morphological Disambiguation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.6083458264668783}]}], "abstractContent": [{"text": "User-generated text tends to be noisy with many lexical and orthographic inconsistencies, making natural language processing (NLP) tasks more challenging.", "labels": [], "entities": []}, {"text": "The challenging nature of noisy text processing is exacerbated for dialectal content, wherein addition to spelling and lexical differences, dialectal text is characterized with morpho-syntactic and phonetic variations.", "labels": [], "entities": []}, {"text": "These issues increase sparsity in NLP models and reduce accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9976429343223572}]}, {"text": "We present a neural morphological tagging and disam-biguation model for Egyptian Arabic, with various extensions to handle noisy and inconsistent content.", "labels": [], "entities": [{"text": "neural morphological tagging", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.6832514603932699}]}, {"text": "Our models achieve about 5% relative error reduction (1.1% absolute improvement) for full morphological analysis, and around 22% relative error reduction (1.8% absolute improvement) for part-of-speech tagging , over a state-of-the-art baseline.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 28, "end_pos": 52, "type": "METRIC", "confidence": 0.7644853989283243}, {"text": "relative error reduction", "start_pos": 129, "end_pos": 153, "type": "METRIC", "confidence": 0.711820125579834}, {"text": "part-of-speech tagging", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.8236304521560669}]}], "introductionContent": [{"text": "There has been a growing interest in noise-robust NLP tools recently, motivated by the sheer magnitude of user-generated content in social media platforms.", "labels": [], "entities": []}, {"text": "The noisy nature of user-generated content makes its processing very challenging for NLP tools.", "labels": [], "entities": []}, {"text": "Noisy content is non-canonical in nature, with lexical, orthographic, and phonetic variations that increase the perplexity and sparsity of NLP models.", "labels": [], "entities": []}, {"text": "Several contributions show considerable drop in performance fora number of tasks, where simply retraining existing models with social media data does not provide substantial improvement ().", "labels": [], "entities": []}, {"text": "Morphological disambiguation for noisy content is further complicated for dialectal content, with additional morpho-syntactic variations.", "labels": [], "entities": [{"text": "Morphological disambiguation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8279723525047302}]}, {"text": "Morphological disambiguation is also more challenging for morphologically rich and ambiguous languages, like Arabic and Dialectal Arabic (DA).", "labels": [], "entities": [{"text": "Morphological disambiguation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9254007339477539}]}, {"text": "Arabic is morphologically rich, having more fully inflected words (types) than morphologically poorer languages.", "labels": [], "entities": []}, {"text": "It is also ambiguous, with short vowels (diacritic marks) often dropped and disambiguated in context.", "labels": [], "entities": []}, {"text": "These issues result in more morpho-syntactic variations for DA in written text compared to other dialectal content, and increase the number of potential analyses.", "labels": [], "entities": []}, {"text": "We present several morphological disambiguation models for Egyptian Arabic (EGY), based on previous models for EGY and Modern Standard Arabic (MSA).", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 119, "end_pos": 147, "type": "DATASET", "confidence": 0.8904197216033936}]}, {"text": "We use a bidirectional long short term memory (Bi-LSTM) architecture and various noise reduction techniques, including character embedding and embedding space mapping.", "labels": [], "entities": [{"text": "embedding space mapping", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.6946467161178589}]}, {"text": "We also experiment with the width of the embedding window in the pre-trained embeddings.", "labels": [], "entities": [{"text": "width", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9621596336364746}]}, {"text": "Character embeddings allow access to subword units, while the embedding space mapping normalizes non-canonical forms to canonical neighbors.", "labels": [], "entities": []}, {"text": "The narrow/wide embedding window in the pre-trained embeddings allows for more of syntactic/semantic modeling, respectively.", "labels": [], "entities": []}, {"text": "The goal of the various models is to achieve noise-robust analysis, rather than explicit noise normalization.", "labels": [], "entities": []}, {"text": "We therefore use the normalization techniques on the vector-level only, instead of replacing the raw forms, which allows for less aggressive lexical normalization.", "labels": [], "entities": []}, {"text": "The separation of raw forms and vector normalization also allows for independent word and character level normalization, eliminating any propagation of error.", "labels": [], "entities": [{"text": "word and character level normalization", "start_pos": 81, "end_pos": 119, "type": "TASK", "confidence": 0.656138437986374}]}, {"text": "Our system achieves a 5% relative error reduction (1.1% absolute accuracy boost) over a stateof-the-art baseline, using a strict metric.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 25, "end_pos": 49, "type": "METRIC", "confidence": 0.8781088391939799}, {"text": "accuracy boost", "start_pos": 65, "end_pos": 79, "type": "METRIC", "confidence": 0.9456265270709991}]}, {"text": "Our noise-robust system also matches the performance of aversion of the system trained and tested on a manually orthography-normalized copy of the data.", "labels": [], "entities": []}, {"text": "This indicates that the system performs as well as could be expected without orthographic in-consistency.", "labels": [], "entities": []}, {"text": "We also present an error analysis of the system and identify areas of improvement.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "We present common challenges to DA processing in Section 2.", "labels": [], "entities": [{"text": "DA processing", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9890570640563965}]}, {"text": "This is followed by background and related work in Section 3.", "labels": [], "entities": []}, {"text": "We introduce the approach and various models in Section 4, and discuss the experimental setup and results in Section 5.", "labels": [], "entities": []}, {"text": "We conclude and provide some directions for future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the Bi-LSTM tagging architecture we use two hidden layers of size 800.", "labels": [], "entities": [{"text": "Bi-LSTM tagging", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.6952911913394928}]}, {"text": "Each layer is composed of two LSTM layers for each direction, and a dropout wrapper with keep probability of 0.8, and peephole connections.", "labels": [], "entities": [{"text": "keep probability", "start_pos": 89, "end_pos": 105, "type": "METRIC", "confidence": 0.9839032292366028}]}, {"text": "We use Adam optimizer () with a learning rate of 0.002, and cross-entropy cost function.", "labels": [], "entities": []}, {"text": "We use Tensorflow as the development environment.", "labels": [], "entities": []}, {"text": "The LSTM character embedding architecture uses two LSTM layers of size 100, and embedding size 50.", "labels": [], "entities": []}, {"text": "The CNN architecture also uses embedding size 50, with filter widths ranging from one to six and max pooling strides of 50.", "labels": [], "entities": []}, {"text": "As for the neural language models for lemmatization and diacritization, we use two hidden layers of size 400 for lemmatization, and 600 for diacritization.", "labels": [], "entities": []}, {"text": "We also use an input layer of size 300.", "labels": [], "entities": []}, {"text": "We use Adam optimizer as the optimization algorithm, with learning rate of 0.002.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.9568396806716919}]}, {"text": "We use TheanoLM ( to develop the models.", "labels": [], "entities": [{"text": "TheanoLM", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.8875799775123596}]}, {"text": "The pre-trained word embeddings are of size 250, for both narrow and wide window embeddings.", "labels": [], "entities": []}, {"text": "The wide window is set to five, whereas the narrow window is set to two (we experimented with a window of one but it performed slightly lower than a window of two).", "labels": [], "entities": []}, {"text": "The number of nearest neighbors in the embedding space mapping experiment is 10 neighbors.", "labels": [], "entities": [{"text": "embedding space mapping", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7494799296061198}]}, {"text": "Metrics We use the following evaluation metrics for all systems: \u2022 POS Accuracy (POS): The accuracy over the POS tag set comprised of 36 tags).", "labels": [], "entities": [{"text": "POS Accuracy (POS)", "start_pos": 67, "end_pos": 85, "type": "METRIC", "confidence": 0.767434823513031}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.998401939868927}]}, {"text": "\u2022 Morph Tags Accuracy (Morph Tags): The analysis and disambiguation accuracy over the 14 morphological features we work with, excluding lemmas and diacritized forms.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.8114858865737915}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.990003764629364}]}, {"text": "\u2022 Lemmatization Accuracy (Lemma): The accuracy of the lemma form of the words.", "labels": [], "entities": [{"text": "Accuracy (Lemma)", "start_pos": 16, "end_pos": 32, "type": "METRIC", "confidence": 0.852610632777214}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9993676543235779}]}, {"text": "\u2022 Diacritization Accuracy (Diac): The accuracy of the diacritized form of the words.", "labels": [], "entities": [{"text": "Accuracy (Diac)", "start_pos": 17, "end_pos": 32, "type": "METRIC", "confidence": 0.835596889257431}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9991315007209778}]}, {"text": "\u2022 Full Analysis Accuracy (Full): The evaluation accuracy over the entire analysis, including the morphological features, lemma, and diacritized form.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.8858270049095154}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9910338521003723}]}, {"text": "shows the results of all systems for Dev, and shows the results for the Blind Test set.", "labels": [], "entities": [{"text": "Blind Test set", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.784886767466863}]}, {"text": "We use the MADAMIRA results as the baseline.", "labels": [], "entities": [{"text": "MADAMIRA", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9419076442718506}]}, {"text": "Narrow embeddings seem to consistently outperform wide embeddings across all experiments.", "labels": [], "entities": []}, {"text": "Regarding character embeddings, using both CNN and LSTM-based character embeddings improve the overall performance for both wide and narrow word embeddings, but LSTMs show consistent improvement over CNNs, which is inline with the conclusions of.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: An example highlighting the effect of non-standard and ambiguous orthography, along with rich mor-", "labels": [], "entities": []}, {"text": " Table 2: Dataset statistics showing tokens, types, and  lemmas count in EGY and an MSA subset. Both from  the Train set. The average inflections per lemma is cal- culated by counting the average unique types that map  to the same lemma.", "labels": [], "entities": [{"text": "EGY", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8300830721855164}, {"text": "Train set", "start_pos": 111, "end_pos": 120, "type": "DATASET", "confidence": 0.901731014251709}]}, {"text": " Table 3: Results of the various systems over the Dev dataset, with MADAMIRA EGY (Pasha et al., 2014) as a  state-of-the-art baseline.", "labels": [], "entities": [{"text": "Dev dataset", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.946344405412674}, {"text": "MADAMIRA", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9934471249580383}, {"text": "EGY", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.49944761395454407}]}, {"text": " Table 4: Results of the various systems over the Blind Test dataset.", "labels": [], "entities": [{"text": "Blind Test dataset", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.7621725698312124}]}, {"text": " Table 5: Results of training and testing the system using the CODA-based Dev data, compared to the results of our  system (taken from", "labels": [], "entities": [{"text": "CODA-based Dev data", "start_pos": 63, "end_pos": 82, "type": "DATASET", "confidence": 0.8405088782310486}]}]}