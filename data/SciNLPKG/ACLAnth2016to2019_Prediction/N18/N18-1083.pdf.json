{"title": [{"text": "From Phonology to Syntax: Unsupervised Linguistic Typology at Different Levels with Language Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "A core part of linguistic typology is the classification of languages according to linguistic properties, such as those detailed in the World Atlas of Language Structure (WALS).", "labels": [], "entities": [{"text": "World Atlas of Language Structure (WALS)", "start_pos": 136, "end_pos": 176, "type": "DATASET", "confidence": 0.7259853556752205}]}, {"text": "Doing this manually is prohibitively time-consuming, which is in part evidenced by the fact that only 100 out of over 7,000 languages spoken in the world are fully covered in WALS.", "labels": [], "entities": []}, {"text": "We learn distributed language representations, which can be used to predict typological properties on a massively multilingual scale.", "labels": [], "entities": []}, {"text": "Additionally , quantitative and qualitative analyses of these language embeddings can tell us how language similarities are encoded in NLP models for tasks at different typological levels.", "labels": [], "entities": []}, {"text": "The representations are learned in an unsuper-vised manner alongside tasks at three typolog-ical levels: phonology (grapheme-to-phoneme prediction, and phoneme reconstruction), morphology (morphological inflection), and syntax (part-of-speech tagging).", "labels": [], "entities": [{"text": "grapheme-to-phoneme prediction", "start_pos": 116, "end_pos": 146, "type": "TASK", "confidence": 0.7640887796878815}, {"text": "phoneme reconstruction", "start_pos": 152, "end_pos": 174, "type": "TASK", "confidence": 0.7718273103237152}, {"text": "part-of-speech tagging", "start_pos": 228, "end_pos": 250, "type": "TASK", "confidence": 0.7207109928131104}]}, {"text": "We consider more than 800 languages and find significant differences in the language representations encoded, depending on the target task.", "labels": [], "entities": []}, {"text": "For instance, although Norwegian Bokm\u00e5l and Danish are typologically close to one another, they are phonologically distant, which is reflected in their language embed-dings growing relatively distant in a phonolog-ical task.", "labels": [], "entities": []}, {"text": "We are also able to predict typolog-ical features in WALS with high accuracies, even for unseen language families.", "labels": [], "entities": [{"text": "WALS", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.9070948362350464}]}], "introductionContent": [{"text": "For more than two and a half centuries, linguistic typologists have studied languages with respect to their structural and functional properties.", "labels": [], "entities": []}, {"text": "Although typology has along history), computational approaches have only recently gained popularity.", "labels": [], "entities": []}, {"text": "One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures.", "labels": [], "entities": [{"text": "World Atlas of Language Structures", "start_pos": 171, "end_pos": 205, "type": "DATASET", "confidence": 0.911582350730896}]}, {"text": "A recent development which can be seen as analogous to this is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings.", "labels": [], "entities": []}, {"text": "We hypothesise that these language embeddings encode typological properties of language, reminiscent of the features in WALS, or even of parameters in Chomsky's Principles and Parameters framework.", "labels": [], "entities": []}, {"text": "In this paper, we model languages in deep neural networks using language embeddings, considering three typological levels: phonology, morphology and syntax.", "labels": [], "entities": []}, {"text": "We consider four NLP tasks to be representative of these levels: grapheme-tophoneme prediction and phoneme reconstruction, morphological inflection, and part-of-speech tagging.", "labels": [], "entities": [{"text": "grapheme-tophoneme prediction", "start_pos": 65, "end_pos": 94, "type": "TASK", "confidence": 0.7154234498739243}, {"text": "phoneme reconstruction", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.7091460675001144}, {"text": "part-of-speech tagging", "start_pos": 153, "end_pos": 175, "type": "TASK", "confidence": 0.7650864124298096}]}, {"text": "We pose three research questions (RQs): RQ 1 Which typological properties are encoded in task-specific distributed language representations, and can we predict phonological, morphological and syntactic properties of languages using such representations?", "labels": [], "entities": []}, {"text": "RQ 2 To what extent do the encoded properties change as the representations are fine tuned for tasks at different linguistic levels?", "labels": [], "entities": []}, {"text": "RQ 3 How are language similarities encoded in fine-tuned language embeddings?", "labels": [], "entities": []}, {"text": "One of our key findings is that language representations differ considerably depending on the target task.", "labels": [], "entities": []}, {"text": "For instance, for grapheme-to-phoneme mapping, the differences between the representations for Norwegian Bokm\u00e5l and Danish increase rapidly during training.", "labels": [], "entities": [{"text": "grapheme-to-phoneme mapping", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.7090284824371338}]}, {"text": "This is due to the fact that, although the languages are typologically close to one another, they are phonologically distant.", "labels": [], "entities": []}], "datasetContent": [{"text": "Class  We train a sequence-to-sequence model with attention, framed as an auto-encoding problem, using the same sequence-to-sequence architecture and setup as for the grapheme-to-phoneme task.", "labels": [], "entities": []}, {"text": "The model takes as input the characters of each source form together with the language embedding for the language at hand and outputs the predicted target form which is identical to the source form.", "labels": [], "entities": []}, {"text": "Quantitative results Since we also consider phonological reconstruction to be a phonological task, we focus the quantitative evaluation on phonological features from WALS.", "labels": [], "entities": [{"text": "Quantitative", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9301679730415344}, {"text": "phonological reconstruction", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.7581234574317932}]}, {"text": "As with the G2P experiments, shows that the finetuned embeddings do not offer predictive power above the most frequent class baseline (p > 0.05).", "labels": [], "entities": []}, {"text": "Observing the changes in the language embeddings reveals that the embeddings are updated to a very small extent, indicating that these are not used by the model to a large extent.", "labels": [], "entities": []}, {"text": "This can be explained by the fact that the task is highly similar for each language, and that the model largely only needs to learn to copy the input string.", "labels": [], "entities": []}, {"text": "We do, however find that evaluating on a set with an unseen language family does yield results significantly above baseline levels with the pretrained embeddings (p < 0.05), which together with the G2P results indicate that the language modelling objective does encode features to some extent related to phonology.", "labels": [], "entities": []}, {"text": "We train a sequence-to-sequence model based on the system developed by\u00a8Ostlingby\u00a8 by\u00a8Ostling and Bjerva.", "labels": [], "entities": []}, {"text": "The neural architecture is modified so as to include an embedded language representation.", "labels": [], "entities": []}, {"text": "During training, the errors are also backpropagated into this embedding, meaning that the encoded representation will be fine-tuned as the task is learned.", "labels": [], "entities": []}, {"text": "The system architecture is depicted in?.", "labels": [], "entities": []}, {"text": "~ l 4 Phonology 4.1 Grapheme-to-phoneme g2p data  We approach the task of PoS tagging using a fairly standard bi-directional LSTM architecture based on, detailed in Section 8.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.926007091999054}]}, {"text": "Quantitative results contains results on WALS feature prediction using language embeddings fine-tuned on PoS tagging.", "labels": [], "entities": [{"text": "WALS feature prediction", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.9109488129615784}, {"text": "PoS tagging", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.6868275552988052}]}, {"text": "We consider both the set of word order features, which are relevant for the dataset, and a set of all WALS features.", "labels": [], "entities": []}, {"text": "Using the fine-tuned embeddings is significantly better than both the baseline and the pre-trained embeddings (p < 0.05), in both the random and the unseen conditions, indicating that the model learns something about word order typology.", "labels": [], "entities": []}, {"text": "This can be expected, as word order features are highly relevant when assigning a PoS tag to a word.", "labels": [], "entities": []}, {"text": "Qualitative results We now turn to the syntactic similarities between languages as encoded in the fine-tuned language embeddings.", "labels": [], "entities": []}, {"text": "We consider a set of the North-Germanic languages Icelandic, Swedish, Norwegian Nynorsk, Danish, Norwegian Bokm\u00e5l, the West-Germanic language English, and the Romance languages Spanish, French, and Italian.", "labels": [], "entities": []}, {"text": "We apply hierarchical clustering using UPGMA to the pre-trained language embeddings of these languages.", "labels": [], "entities": [{"text": "UPGMA", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.7967899441719055}]}, {"text": "Striking here is that English is grouped together with the Romance languages.", "labels": [], "entities": []}, {"text": "This can be explained by the fact that English has a large amount of vocabulary stemming from Romance loan words, which under the task of language modelling results in a higher similarity with such languages.", "labels": [], "entities": []}, {"text": "We then cluster the embeddings finetuned on PoS tagging in the same way.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.5461364388465881}]}, {"text": "In this condition, English has joined the rest of the Germanic languages' cluster.", "labels": [], "entities": []}, {"text": "This can be explained by the fact that, in terms of word ordering and morphosyntax, English is more similar to these languages than it is to the Romance ones.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.7083978801965714}]}, {"text": "We can also observe that, whereas the orthographically highly similar Norwegian Bokm\u00e5l and Danish form the first sub-cluster in the pre-trained condition, Norwegian Nynorsk replaces Danish in this pairing when fine-tuning on PoS tagging.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 225, "end_pos": 236, "type": "TASK", "confidence": 0.6284822523593903}]}, {"text": "This can be explained by the fact that morpho-syntactic similarities between the two written varieties of Norwegian are more similar to one another.", "labels": [], "entities": []}, {"text": "We train a sequence-to-sequence model identical to the morphological system, using grapheme-tophoneme data.", "labels": [], "entities": []}, {"text": "We approach the task of PoS tagging using a fairly standard bi-directional LSTM architecture, based on.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.949285477399826}]}, {"text": "The system is implemented using DyNet ( . We train using the Adam optimisation algorithm) over a maximum of 10 epochs, using early stopping.", "labels": [], "entities": []}, {"text": "We make two modifications to the bi-LSTM architecture of.", "labels": [], "entities": []}, {"text": "First of all, we do not use any atomic embedded word representations, but rather use only character-based word representations.", "labels": [], "entities": []}, {"text": "This choice was made so as to encourage the model not to rely on language-specific vocabulary.", "labels": [], "entities": []}, {"text": "Additionally, we concatenate a pre-trained language embedding to each word representation.", "labels": [], "entities": []}, {"text": "That is to say, in the original bi-LSTM formulation of, each word w is represented as ~ w + LST Mc(w), where ~ w is an embedded word representation, and LST Mc(w) is the final states of a character bi-LSTM running over the characters in a word.", "labels": [], "entities": []}, {"text": "In our formulation, each word win language l is represented as LST Mc(w) +~ l, where LST Mc(w) is defined as before, and ~ l is an embedded language representation.", "labels": [], "entities": []}, {"text": "We use a two-layer deep bi-LSTM, with 100 units in each layer.", "labels": [], "entities": []}, {"text": "The character embeddings used also have 100 dimensions.", "labels": [], "entities": []}, {"text": "We update the language representations, ~ l, during training.", "labels": [], "entities": []}, {"text": "The language representations are 64-dimensional, and are initialised using the language embeddings from\u00a8Ostlingfrom\u00a8 from\u00a8Ostling and Tiedemann.", "labels": [], "entities": []}, {"text": "All PoS tagging results reported are the average of five runs, each with different initialisation seeds, so as to minimise ranConcat.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.6891749352216721}]}, {"text": "First of all, we do not use any atomic embedded word representations, but rather use only character-based word representations.", "labels": [], "entities": []}, {"text": "This choice was made so as to encourage the model not to rely on language-specific vocabulary.", "labels": [], "entities": []}, {"text": "Additionally, we concatenate a pre-trained language embedding to each word representation.", "labels": [], "entities": []}, {"text": "In our formulation, each word w is represented as LST M c (w) + l, where LST M c (w) is the final states of a character bi-LSTM running over the characters in a word and l is an embedded language representation.", "labels": [], "entities": []}, {"text": "We use a two-layer deep bi-LSTM with 100 units in each layer, and 100-dimensional character embeddings.", "labels": [], "entities": []}, {"text": "The rest of the hyper-parameters are the same as in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of tasks and datasets.", "labels": [], "entities": []}, {"text": " Table 2: Accuracies on prediction of WALS features  with language embeddings fine-tuned on Grapheme-to- Phoneme mapping. Asterisks indicate results signifi- cantly higher than both other conditions (p < 0.05).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9838010668754578}, {"text": "WALS", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.6829047203063965}]}, {"text": " Table 3: Accuracies on prediction of WALS features  with language embeddings fine-tuned on Phonological  Reconstruction. Asterisks indicate results significantly  higher than non-bold conditions (p < 0.05).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9765685796737671}, {"text": "WALS", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.6323814988136292}, {"text": "Phonological  Reconstruction", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.7277000248432159}]}, {"text": " Table 4: Accuracies on prediction of WALS features  with language embeddings fine-tuned on morpholog- ical inflection. Asterisks indicate results significantly  higher than both other conditions (p < 0.05).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9724529981613159}, {"text": "WALS", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.6164085268974304}]}, {"text": " Table 5: Accuracies on prediction of WALS features  with language embeddings fine-tuned on PoS tagging.  Asterisks indicate the result in bold significantly out- performing both other conditions (p < 0.05).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.985829770565033}, {"text": "prediction of WALS", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7450720469156901}, {"text": "PoS tagging", "start_pos": 92, "end_pos": 103, "type": "TASK", "confidence": 0.6063441038131714}]}, {"text": " Table 6: Top 5 and bottom 5 accuracies in feature pre- diction using phonological language embeddings.", "labels": [], "entities": []}]}