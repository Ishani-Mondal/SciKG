{"title": [{"text": "Lessons from the Bible on Modern Topics: Low-Resource Multilingual Topic Model Evaluation", "labels": [], "entities": [{"text": "Low-Resource Multilingual Topic Model Evaluation", "start_pos": 41, "end_pos": 89, "type": "TASK", "confidence": 0.5818636417388916}]}], "abstractContent": [{"text": "Multilingual topic models enable document analysis across languages through coherent multilingual summaries of the data.", "labels": [], "entities": [{"text": "document analysis", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.6856837123632431}]}, {"text": "However, there is no standard and effective metric to evaluate the quality of multilingual topics.", "labels": [], "entities": []}, {"text": "We introduce anew intrinsic evaluation of multilingual topic models that correlates well with human judgments of multilingual topic coherence as well as performance in downstream applications.", "labels": [], "entities": []}, {"text": "Importantly, we also study evaluation for low-resource languages.", "labels": [], "entities": []}, {"text": "Because standard metrics fail to accurately measure topic quality when robust external resources are unavailable , we propose an adaptation model that improves the accuracy and reliability of these metrics in low-resource settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.998458981513977}]}], "introductionContent": [{"text": "Topic models provide a high-level view of the main themes of a document collection . Document collections, however, are often not in a single language, driving the development of multilingual topic models.", "labels": [], "entities": []}, {"text": "These models discover topics that are consistent across languages, providing useful tools for multilingual text analysis), such as detecting cultural differences () and bilingual dictionary extraction (.", "labels": [], "entities": [{"text": "multilingual text analysis", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.6605526109536489}, {"text": "detecting cultural differences", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.8277355432510376}, {"text": "bilingual dictionary extraction", "start_pos": 169, "end_pos": 200, "type": "TASK", "confidence": 0.590670237938563}]}, {"text": "Monolingual topic models can be evaluated through likelihood () or coherence (), but topic model evaluation is not well understood in multilingual settings.", "labels": [], "entities": []}, {"text": "We introduce an improved intrinsic evaluation metric for multilingual topic models, called Crosslingual Normalized Pointwise Mutual Information (CNPMI, Section 2).", "labels": [], "entities": []}, {"text": "We explore the behaviors of CNPMI at both the model and topic levels with six language pairs and varying model specifications.", "labels": [], "entities": []}, {"text": "This metric correlates well with human judgments and crosslingual classification results (Sections 5 and 6).", "labels": [], "entities": [{"text": "crosslingual classification", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6739317625761032}]}, {"text": "We also focus on evaluation in low-resource languages, which lack large parallel corpora, dictionaries, and other tools that are often used in learning and evaluating topic models.", "labels": [], "entities": []}, {"text": "To adapt CNPMI to these settings, we create a coherence estimator (Section 3) that extrapolates statistics derived from antiquated, specialized texts like the Bible: often the only resource available for many languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together).", "labels": [], "entities": [{"text": "topic model evaluation", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.6572734415531158}]}, {"text": "The most successful () is normalized pointwise mutual information.", "labels": [], "entities": []}, {"text": "NPMI compares the joint probability of words appearing together Pr(w i , w j ) to their probability assuming independence Pr(w i ) Pr(w j ), normalized by the joint probability: The word probabilities are calculated from a reference corpus, R, typically a large corpus such as Wikipedia that can provide meaningful cooccurrence patterns that are independent of the target dataset.", "labels": [], "entities": [{"text": "NPMI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8555943369865417}]}, {"text": "The quality of topic k is the average NPMI of all word pairs (w i , w j ) in the topic: where W(k, C) are the C most probable words in the topic-word distribution \u03c6 k (the number of words is the topic's cardinality).", "labels": [], "entities": [{"text": "NPMI", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9617462158203125}]}, {"text": "Higher NPMI k means the topic's top words are more coupled.", "labels": [], "entities": [{"text": "NPMI", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.8209525942802429}]}, {"text": "computer Figure 1: Topic 5 is multilingually coherent: both the English and Swedish topics are about technology.", "labels": [], "entities": []}, {"text": "Topic 6 is about biology in English but food in Romanian, so it is low quality although coherent monolingually.", "labels": [], "entities": []}, {"text": "Topic 7 is monolingually incoherent, so it is a low quality topic even if it contains word translations.", "labels": [], "entities": []}, {"text": "While automatic evaluation has been well-studied for monolingual topic models, there are no robust evaluations for multilingual topic models.", "labels": [], "entities": []}, {"text": "We first consider two straightforward metrics that could be used for multilingual evaluation, both with limitations.", "labels": [], "entities": []}, {"text": "We then propose an extension of NPMI that addresses these limitations.", "labels": [], "entities": [{"text": "NPMI", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8348296284675598}]}, {"text": "A simple adaptation of NPMI is to calculate the monolingual NPMI score for each language independently and take the average.", "labels": [], "entities": []}, {"text": "We refer this as internal NPMI (INPMI) as it evaluates coherence within a language.", "labels": [], "entities": []}, {"text": "However, this metric does not consider whether the topic is coherent across languages-that is, whether a language-specific word distribution \u03c6 1 k is related to the corresponding distribution in another language, \u03c6 2 k . Crosslingual Consistency.", "labels": [], "entities": []}, {"text": "Another straightforward measurement is Matching Translation Accuracy (Boyd-Graber and Blei, 2009, MTA), which counts the number of word translations in a topic between two languages using a bilingual dictionary.", "labels": [], "entities": [{"text": "Matching Translation Accuracy", "start_pos": 39, "end_pos": 68, "type": "METRIC", "confidence": 0.6860438386599222}]}, {"text": "This metric can measure whether a topic is well-aligned across languages literally, but cannot capture non-literal more holistic similarities across languages.", "labels": [], "entities": []}, {"text": "We experiment on six languages  Typical preprocessing methods (stemming, stop word removal, etc.) are often unavailable for lowresource languages.", "labels": [], "entities": [{"text": "stop word removal", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.6900040507316589}]}, {"text": "For a meaningful comparison across languages, we do not apply any stemming or lemmatization strategies, including English, except removing digit numbers and symbols.", "labels": [], "entities": []}, {"text": "However, we remove words that appear in more than 30% of documents for each language.", "labels": [], "entities": []}, {"text": "Each language pair is separately trained using the MALLET) implementation of the polylingual topic model.", "labels": [], "entities": [{"text": "MALLET", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.7751979827880859}]}, {"text": "Each experiment runs five Gibbs sampling chains with 1,000 iterations per chain with twenty topics.", "labels": [], "entities": []}, {"text": "The hyperparameters are set to the default values (\u03b1 = 0.1, \u03b2 = 0.01), and are optimized every 50 iterations in MALLET using slice sampling ().", "labels": [], "entities": [{"text": "MALLET", "start_pos": 112, "end_pos": 118, "type": "DATASET", "confidence": 0.6713391542434692}]}, {"text": "We first study CNPMI at the topic level: does a particular topic make sense?", "labels": [], "entities": []}, {"text": "An effective evaluation should be consistent with human judgment of the topics (.", "labels": [], "entities": []}, {"text": "In this section, we measure gold-standard human interpretability of multilingual topics to establish which automatic measures of topic interpretability work best.", "labels": [], "entities": []}, {"text": "While the previous section looked at individual topics, we also care about how well CNPMI characterizes the quality of models through an average of a model's constituent topics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of document pairs in the training and  reference datasets and number of dictionary entries for  each language pair.", "labels": [], "entities": []}, {"text": " Table 2: Pearson correlations between human judg- ments and CNPMI are higher than INPMI, while MTA  correlations are comparable to CNPMI.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9726891815662384}, {"text": "INPMI", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9170249700546265}, {"text": "MTA  correlations", "start_pos": 96, "end_pos": 113, "type": "METRIC", "confidence": 0.8927473425865173}]}, {"text": " Table 3: Correlations between the Wikipedia-based  CNPMI and the Bible-based CNPMI, before and af- ter using the coherence estimator, at the topic level.  Strong correlations indicate that the estimator improves  CNPMI estimates.", "labels": [], "entities": []}, {"text": " Table 4: At the model level, the estimator improves  correlations between CNPMI and downstream classifi- cation for all languages except for Turkish.", "labels": [], "entities": []}]}