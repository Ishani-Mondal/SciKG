{"title": [{"text": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features", "labels": [], "entities": []}], "abstractContent": [{"text": "The recent tremendous success of unsuper-vised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve em-beddings (i.e. semantic representations) of word sequences as well.", "labels": [], "entities": []}, {"text": "We present a simple but efficient unsupervised objective to train distributed representations of sentences.", "labels": [], "entities": []}, {"text": "Our method outperforms the state-of-the-art unsu-pervised models on most benchmark tasks, highlighting the robustness of the produced general-purpose sentence embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Improving unsupervised learning is of key importance for advancing machine learning methods, as to unlock access to almost unlimited amounts of data to be used as training resources.", "labels": [], "entities": []}, {"text": "The majority of recent success stories of deep learning does not fall into this category but instead relied on supervised training (in particular in the vision domain).", "labels": [], "entities": []}, {"text": "Avery notable exception comes from the text and natural language processing domain, in the form of semantic word embeddings trained unsupervised ().", "labels": [], "entities": []}, {"text": "Within only a few years from their invention, such word representations -which are based on a simple matrix factorization model as we formalize below -are now routinely trained on very large amounts of raw text data, and have become ubiquitous building blocks of a majority of current state-of-the-art NLP applications.", "labels": [], "entities": []}, {"text": "While very useful semantic representations are available for words, it remains challenging to produce and learn such semantic embeddings for longer pieces of text, such as sentences, paragraphs or entire documents.", "labels": [], "entities": []}, {"text": "Even more so, it re-* indicates equal contribution mains a key goal to learn such general-purpose representations in an unsupervised way.", "labels": [], "entities": []}, {"text": "Currently, two contrary research trends have emerged in text representation learning: On one hand, a strong trend in deep-learning for NLP leads towards increasingly powerful and complex models, such as recurrent neural networks (RNNs), LSTMs, attention models and even Neural Turing Machine architectures.", "labels": [], "entities": [{"text": "text representation learning", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.8238275448481241}]}, {"text": "While extremely strong in expressiveness, the increased model complexity makes such models much slower to train on larger datasets.", "labels": [], "entities": []}, {"text": "On the other end of the spectrum, simpler \"shallow\" models such as matrix factorizations (or bilinear models) can benefit from training on much larger sets of data, which can be a key advantage, especially in the unsupervised setting.", "labels": [], "entities": []}, {"text": "Surprisingly, for constructing sentence embeddings, naively using averaged word vectors was shown to outperform LSTMs (see for plain averaging, and for weighted averaging).", "labels": [], "entities": []}, {"text": "This example shows potential in exploiting the trade-off between model complexity and ability to process huge amounts of text using scalable algorithms, towards the simpler side.", "labels": [], "entities": []}, {"text": "In view of this tradeoff, our work here further advances unsupervised learning of sentence embeddings.", "labels": [], "entities": []}, {"text": "Our proposed model can be seen as an extension of the C-BOW (,a) training objective to train sentence instead of word embeddings.", "labels": [], "entities": []}, {"text": "We demonstrate that the empirical performance of our resulting general-purpose sentence embeddings very significantly exceeds the state of the art, while keeping the model simplicity as well as training and inference complexity exactly as low as in averaging methods (, thereby also putting the work by) in perspective.", "labels": [], "entities": []}, {"text": "The main contributions in this work can be summarized as follows: \u2022 Model.", "labels": [], "entities": []}, {"text": "We propose Sent2Vec 1 , a simple unsupervised model allowing to compose sentence embeddings using word vectors along with n-gram embeddings, simultaneously training composition and the embedding vectors themselves.", "labels": [], "entities": []}, {"text": "The computational complexity of our embeddings is only O(1) vector operations per word processed, both during training and inference of the sentence embeddings.", "labels": [], "entities": [{"text": "O", "start_pos": 55, "end_pos": 56, "type": "METRIC", "confidence": 0.9681054949760437}]}, {"text": "This strongly contrasts all neural network based approaches, and allows our model to learn from extremely large datasets, in a streaming fashion, which is a crucial advantage in the unsupervised setting.", "labels": [], "entities": []}, {"text": "Fast inference is a key benefit in downstream tasks and industry applications.", "labels": [], "entities": []}, {"text": "Our method shows significant performance improvements compared to the current state-of-the-art unsupervised and even semi-supervised models.", "labels": [], "entities": []}, {"text": "The resulting general-purpose embeddings show strong robustness when transferred to a wide range of prediction benchmarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use a standard set of supervised as well as unsupervised benchmark tasks from the literature to evaluate our trained models, following).", "labels": [], "entities": []}, {"text": "The breadth of tasks allows to fairly measure generalization to a wide area of different domains, testing the general-purpose quality (universality) of all competing sentence embeddings.", "labels": [], "entities": []}, {"text": "For downstream supervised evaluations, sentence embeddings are combined with logistic regression to predict target labels.", "labels": [], "entities": []}, {"text": "In the unsupervised evaluation for sentence similarity, correlation of the cosine similarity between two embeddings is compared to human annotators.", "labels": [], "entities": []}, {"text": "Sentence embeddings are evaluated for various supervised classification tasks as follows.", "labels": [], "entities": []}, {"text": "We evaluate paraphrase identification (MSRP) (), classification of movie review sentiment (MR) (Pang and), product reviews (CR) (), subjectivity classification (SUBJ) (), opinion polarity (MPQA) () and question type classification (TREC).", "labels": [], "entities": [{"text": "paraphrase identification (MSRP)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.8117280960083008}, {"text": "classification of movie review sentiment (MR)", "start_pos": 49, "end_pos": 94, "type": "TASK", "confidence": 0.80943164229393}, {"text": "Pang", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.8395921587944031}, {"text": "question type classification (TREC)", "start_pos": 202, "end_pos": 237, "type": "TASK", "confidence": 0.6752500832080841}]}, {"text": "To classify, we use the code provided by) in the same manner as in ().", "labels": [], "entities": []}, {"text": "For the MSRP dataset, containing pairs of sentences (S 1 , S 2 ) with associated paraphrase label, we generate feature vectors by concatenating their Sent2Vec representations |v S 1 \u2212 v S 2 | with the component-wise product v S 1 v S 2 . The predefined training split is used to tune the L2 penalty parameter using cross-validation and the accuracy and F1 scores are computed on the test set.", "labels": [], "entities": [{"text": "MSRP dataset", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.7972241938114166}, {"text": "accuracy", "start_pos": 340, "end_pos": 348, "type": "METRIC", "confidence": 0.9995562434196472}, {"text": "F1", "start_pos": 353, "end_pos": 355, "type": "METRIC", "confidence": 0.9986128807067871}]}, {"text": "For the remaining 5 datasets, Sent2Vec embeddings are inferred from input sentences and directly fed to a logistic regression classifier.", "labels": [], "entities": []}, {"text": "Accuracy scores are obtained using 10-fold cross-validation for the MR, CR, SUBJ and MPQA datasets.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9933231472969055}, {"text": "MR", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.5580702424049377}, {"text": "CR", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.6571240425109863}, {"text": "MPQA datasets", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9311650395393372}]}, {"text": "For those datasets nested cross-validation is used to tune the L2 penalty.", "labels": [], "entities": []}, {"text": "For the TREC dataset, as for the MRSP dataset, the L2 penalty is tuned on the predefined train split using 10-fold cross-validation, and the accuracy is computed on the test set.", "labels": [], "entities": [{"text": "TREC dataset", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.8246440589427948}, {"text": "MRSP dataset", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.9151668548583984}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9994458556175232}]}, {"text": "We perform unsupervised evaluation of the learnt sentence embeddings using the sentence cosine similarity, on the STS 2014 ( and SICK 2014) datasets.", "labels": [], "entities": [{"text": "STS 2014 ( and SICK 2014) datasets", "start_pos": 114, "end_pos": 148, "type": "DATASET", "confidence": 0.7388955913484097}]}, {"text": "These similarity scores are compared to the goldstandard human judgements using Pearson's r (Pearson, 1895) and Spearman's \u03c1 (Spearman, 1904) correlation scores.", "labels": [], "entities": [{"text": "similarity", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9701007008552551}, {"text": "Pearson's r (Pearson, 1895)", "start_pos": 80, "end_pos": 107, "type": "METRIC", "confidence": 0.8148718401789665}, {"text": "Spearman's \u03c1 (Spearman, 1904) correlation scores", "start_pos": 112, "end_pos": 160, "type": "METRIC", "confidence": 0.7347633868455887}]}, {"text": "The SICK dataset consists of about 10,000 sentence pairs along with relatedness scores of the pairs.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8462238013744354}]}, {"text": "The STS 2014 dataset contains 3,770 pairs, divided into six different categories on the basis of the origin of sentences/phrases, namely Twitter, headlines, news, forum, WordNet and images.", "labels": [], "entities": [{"text": "STS 2014 dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.7775092720985413}, {"text": "WordNet", "start_pos": 170, "end_pos": 177, "type": "DATASET", "confidence": 0.9333109259605408}]}], "tableCaptions": [{"text": " Table 1: Comparison of the performance of different models on different supervised evaluation tasks. An underline indicates", "labels": [], "entities": []}, {"text": " Table 2: Unsupervised Evaluation Tasks: Comparison of the performance of different models on Spearman/Pearson corre-", "labels": [], "entities": [{"text": "Spearman/Pearson corre", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.704060286283493}]}, {"text": " Table 3: Best unsupervised and semi-supervised methods ranked by macro average along with their training times. ** indicates", "labels": [], "entities": []}, {"text": " Table 4: Comparison of the performance of the unsupervised and semi-supervised sentence embeddings by (Arora et al., 2017)", "labels": [], "entities": []}, {"text": " Table 5: Training parameters for the Sent2Vec models", "labels": [], "entities": []}, {"text": " Table 6: Comparison of the performance of different Sent2Vec models with different semi- supervised/supervised models on different downstream supervised evaluation tasks. An underline  indicates the best performance for the dataset and Sent2Vec model performances are bold if they per- form as well or better than all other non-Sent2Vec models, including those presented in Table 1.", "labels": [], "entities": []}, {"text": " Table 7: Unsupervised Evaluation: Comparison of the performance of different Sent2Vec models with  semi-supervised/supervised models on Spearman/Pearson correlation measures. An underline indicates  the best performance for the dataset and Sent2Vec model performances are bold if they perform as well  or better than all other non-Sent2Vec models, including those presented in Table 2.", "labels": [], "entities": []}, {"text": " Table 8: Average sentence lengths for the datasets used in the comparison.", "labels": [], "entities": []}]}