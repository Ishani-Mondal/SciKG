{"title": [{"text": "Keep your bearings: Lightly-supervised Information Extraction with Ladder Networks that avoids Semantic Drift", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.696186363697052}]}], "abstractContent": [{"text": "We propose a novel approach to semi-supervised learning for information extraction that uses ladder networks (Rasmus et al., 2015).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8293762803077698}]}, {"text": "In particular, we focus on the task of named entity classification, defined as identifying the correct label (e.g., person or organization name) of an entity mention in a given context.", "labels": [], "entities": [{"text": "named entity classification", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.7031684517860413}]}, {"text": "Our approach is simple, efficient and has the benefit of being robust to semantic drift, a dominant problem inmost semi-supervised learning systems.", "labels": [], "entities": []}, {"text": "We empirically demonstrate the superior performance of our system compared to the state-of-the-art on two standard datasets for named entity classification.", "labels": [], "entities": [{"text": "named entity classification", "start_pos": 128, "end_pos": 155, "type": "TASK", "confidence": 0.6390767792860667}]}, {"text": "We obtain between 62% and 200% improvement over the state-of-art baseline on these two datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Training machine learning systems with limited supervision is one of the fundamental challenges in natural language processing (NLP), as annotated data is often scarce and generating it requires costly human supervision.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 99, "end_pos": 132, "type": "TASK", "confidence": 0.8167859613895416}]}, {"text": "Semi-supervised learning addresses this challenge by combining limited supervision with a large, unannotated dataset, thereby mitigating the supervision cost.", "labels": [], "entities": []}, {"text": "For NLP, bootstrapping is a popular approach to semi-supervised learning due its relative simplicity coupled with reasonable performance).", "labels": [], "entities": []}, {"text": "However, a crucial limitation of bootstrapping, which is typically iterative, is that, as learning advances, the task often drifts semantically into a related but different space, e.g., from learning women names into learning flower names.", "labels": [], "entities": []}, {"text": "In this paper, we propose an effective technique for semi-supervised learning for information extraction (IE), which obviates the need for an iterative approach, thereby mitigating the problem of semantic drift.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.8777954459190369}, {"text": "semantic drift", "start_pos": 196, "end_pos": 210, "type": "TASK", "confidence": 0.7690125107765198}]}, {"text": "Our technique is based on the recently proposed ladder networks (LNs)).", "labels": [], "entities": []}, {"text": "Ladder networks are deep denoising auto-encoders which have skip connections and reconstruction targets in the intermediate layers.", "labels": [], "entities": []}, {"text": "Ladder networks are closely related to hierarchical latent variable models ().", "labels": [], "entities": []}, {"text": "The lateral skip connections relieve the pressure on lower layers of the encoder to encode all latent information, thereby making the architecture modular in design, similar to a factor graph.", "labels": [], "entities": []}, {"text": "The integration of the encoder-decoder framework as a neural network, allows one to use backpropagation for training, thereby not having to rely on intractable inference as in a standard graphical model.", "labels": [], "entities": []}, {"text": "Furthermore, LNs have been shown to achieve state-of-the-art performance in image recognition tasks.", "labels": [], "entities": [{"text": "image recognition tasks", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.8637519081433614}]}, {"text": "To the best of our knowledge, our work is one of the first applications of LN to any NLP task.", "labels": [], "entities": [{"text": "NLP task", "start_pos": 85, "end_pos": 93, "type": "TASK", "confidence": 0.8726502954959869}]}, {"text": "Specifically, our contributions are as follows: (1) We provide a novel application of LNs to an IE task, in particular semi-supervised named entity classification (NEC).", "labels": [], "entities": [{"text": "IE task", "start_pos": 96, "end_pos": 103, "type": "TASK", "confidence": 0.9277486503124237}, {"text": "semi-supervised named entity classification (NEC)", "start_pos": 119, "end_pos": 168, "type": "TASK", "confidence": 0.7742675713130406}]}, {"text": "Our approach is simple: we concatenate embeddings of entity mentions with that of its context 1 and feed the resulting vectors into the LN's denoising auto-encoder.", "labels": [], "entities": []}, {"text": "(2) We empirically demonstrate, for the task of semi-supervised NEC on two standard datasetsCoNLL and -that we obtain a classification accuracy of 66.11% and 63.12% with minimal supervision on only 0.3% and 0.6% of the data, respectively.", "labels": [], "entities": [{"text": "NEC", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9445843696594238}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.8800297379493713}]}, {"text": "These results compare favorably against the accuracy of stateof-the-art bootstrapping algorithms of 40.74% and 21.06% on the same datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9983738660812378}]}, {"text": "Further, in our experiments we observed an almost 7-fold decrease in training time compared to an iterative bootstrapping system.", "labels": [], "entities": []}, {"text": "(3) Lastly, we also provide empirical evidence that our approach is robust to the phenomenon of semantic drift.", "labels": [], "entities": [{"text": "semantic drift", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.7907857000827789}]}, {"text": "We obtain consistently better accuracy compared to traditional bootstrapping algorithms and label propagation, when initialized with identical supervision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9992191791534424}, {"text": "label propagation", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.8033950328826904}]}, {"text": "We also demonstrate the reduction in semantic drift by measuring the purity of the entity pools with respect to a category as the algorithm advances ( \u00a74).", "labels": [], "entities": [{"text": "semantic drift", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7989270687103271}]}], "datasetContent": [{"text": "Datasets: We used two datasets, the CoNLL-2003 shared task dataset, which contains 4 entity types, and the OntoNotes dataset (), which contains 11 3 , both of which are benchmark datasets for supervised named entity recognition (NER).", "labels": [], "entities": [{"text": "CoNLL-2003 shared task dataset", "start_pos": 36, "end_pos": 66, "type": "DATASET", "confidence": 0.8438493758440018}, {"text": "OntoNotes dataset", "start_pos": 107, "end_pos": 124, "type": "DATASET", "confidence": 0.7807035744190216}, {"text": "supervised named entity recognition (NER)", "start_pos": 192, "end_pos": 233, "type": "TASK", "confidence": 0.768036961555481}]}, {"text": "These datasets contain marked entity boundaries with labels for each marked entity.", "labels": [], "entities": []}, {"text": "Here we only use the entity boundaries but not the labels of these entities during the training of our bootstrapping systems.", "labels": [], "entities": []}, {"text": "To simulate learning from large texts, we tuned hyper parameters on development, but ran the actual experiments on the train partitions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Num. of annotated labels vs. overall accuracy.  # of mention labels -CoNLL: 13200; OntoNotes:  67000", "labels": [], "entities": [{"text": "Num.", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9500610828399658}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9989786148071289}, {"text": "CoNLL: 13200", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.510347863038381}]}]}