{"title": [{"text": "Fusing Document, Collection and Label Graph-based Representations with Word Embeddings for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.758576899766922}]}], "abstractContent": [{"text": "Contrary to the traditional Bag-of-Words approach, we consider the Graph-of-Words (GoW) model in which each document is represented by a graph that encodes relationships between the different terms.", "labels": [], "entities": []}, {"text": "Based on this formulation, the importance of a term is determined by weighting the corresponding node in the document, collection and label graphs, using node centrality criteria.", "labels": [], "entities": []}, {"text": "We also introduce novel graph-based weighting schemes by enriching graphs with word-embedding similarities, in order to reward or penalize semantic relationships.", "labels": [], "entities": []}, {"text": "Our methods produce more discriminative feature weights for text categorization, out-performing existing frequency-based criteria.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7917868494987488}]}, {"text": "Code and data are available online 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "With the rapid growth of the social media and networking platforms, the available textual resources have been increased.", "labels": [], "entities": []}, {"text": "Text categorization or classification (TC) refers to the supervised learning task of assigning a document to a set of two or more predefined categories (or classes)).", "labels": [], "entities": [{"text": "Text categorization or classification (TC) refers to the supervised learning task of assigning a document to a set of two or more predefined categories (or classes))", "start_pos": 0, "end_pos": 165, "type": "Description", "confidence": 0.7777043501536052}]}, {"text": "Well-known applications of TC include sentiment analysis, spam detection and news classification.", "labels": [], "entities": [{"text": "TC", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9851365089416504}, {"text": "sentiment analysis", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9752935469150543}, {"text": "spam detection", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.9497566223144531}, {"text": "news classification", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.737059161067009}]}, {"text": "In the TC pipeline, each document is modeled using the so-called Vector Space Model (BaezaYates and).", "labels": [], "entities": [{"text": "BaezaYates", "start_pos": 85, "end_pos": 95, "type": "DATASET", "confidence": 0.8543362617492676}]}, {"text": "The main issue here is how to find appropriate weights regarding the importance of each term in a document.", "labels": [], "entities": []}, {"text": "Typically, the Bag-of-Words (BoW) model is applied and a document is represented as a multiset of its terms, disregarding co-occurence between Code and data: github.com/y3nk0/Graph-Based-TC the terms; using this model, the importance of a term in a document is mainly determined by the frequency of the term.", "labels": [], "entities": []}, {"text": "Although several variants and extensions of this modeling approach have been proposed (e.g., the n-gram model), the main weakness comes from the underlying term independence assumption, where the order of the terms is also completely disregarded.", "labels": [], "entities": []}, {"text": "After the introduction of deep learning models for TC, recent work by shows how we could effectively use the order of words with CNNs (.", "labels": [], "entities": [{"text": "TC", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9511472582817078}]}, {"text": "In many cases though, space and time limitations may arise due to complex neural network architectures.", "labels": [], "entities": []}, {"text": "As stated in work by, computation can still be expensive and prohibitive.", "labels": [], "entities": []}, {"text": "In this paper, we explore fast term weighting criteria for TC that go beyond the term independence assumption.", "labels": [], "entities": [{"text": "TC", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9638367891311646}]}, {"text": "The notion of dependencies between terms is introduced via a Graph-of-Words (GoW) representation model.", "labels": [], "entities": []}, {"text": "Under this model, each term is represented as anode in the graph and the edges capture co-occurrence relationships of terms with a specified distance in the document.", "labels": [], "entities": []}, {"text": "We implicitly consider information about n-grams in the document as well as the collection of documents -expressed bypaths in the graph -without increasing the dimensionality of the problem.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce word-embedding similarities as weights in the GoW approach, in order to further boost the performance of our methods.", "labels": [], "entities": [{"text": "GoW", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.9041288495063782}]}, {"text": "Finally, we successfully mix document, collection and label GoWs along with word vector similarities into a single powerful graph-based framework.", "labels": [], "entities": []}, {"text": "An overview of our approach is shown in.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have evaluated our method on six freely available standard TC datasets, covering multi-class document categorization, sentiment analysis and subjectivity.", "labels": [], "entities": [{"text": "TC datasets", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.818355530500412}, {"text": "sentiment analysis", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.965080738067627}]}, {"text": "Specifically: (1) 20NG 3 : newsgroup documents belonging to 20 categories, (2) REUTERS 3 : 8 categories of Reuters-21578, (3) WEBKB 3 : 4 most frequent categories of webpages from Computer Science departments, (4) IMDB (): positive and negative movie reviews; (5) AMAZON: product reviews acquired from Amazon over four different sub-collections; (6) SUBJEC-TIVITY (): contains subjective sentences gathered from Rotten Tomatoes and objective sentences gathered from IMDB.", "labels": [], "entities": [{"text": "REUTERS", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9576472640037537}, {"text": "Reuters-21578", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.9399046897888184}, {"text": "WEBKB", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.8040589690208435}]}, {"text": "A summary of the datasets can be found in.", "labels": [], "entities": []}, {"text": "In the experiments, linear SVMs were used with grid search cross-validation for tuning the C parameter.", "labels": [], "entities": []}, {"text": "We also examined logistic regression, and observed similar performance.", "labels": [], "entities": []}, {"text": "In the text preprocessing step, we have removed stopwords.", "labels": [], "entities": []}, {"text": "No stemming or lowercase transformation was applied in order to match the words in word2vec.", "labels": [], "entities": []}, {"text": "For evaluation we use macro-average F1 score and classification accuracy on the test sets; that way, we deal with the skewed class size distribution of some datasets).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9602527320384979}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.8465691804885864}]}, {"text": "For the notation of the proposed schemes, we use TW (centrality measure) (e.g., TW (degree)) to indicate the centrality and TW-ICW (centrality at G, centrality at G) (e.g., TW-ICW (degree, degree)) for the local and collection-level graphs respectively.", "labels": [], "entities": [{"text": "TW (centrality measure)", "start_pos": 49, "end_pos": 72, "type": "METRIC", "confidence": 0.7808307409286499}, {"text": "TW (degree))", "start_pos": 80, "end_pos": 92, "type": "METRIC", "confidence": 0.9235261380672455}, {"text": "TW-ICW", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.8773925304412842}]}, {"text": "In TW-IDF (w2v), we compute the weighted degree centrality on the document level, with word-embedding similarities as weights.", "labels": [], "entities": []}, {"text": "Similarly, in TW-ICW (w2v) we compute both weighted centralities for document and collection graphs.", "labels": [], "entities": [{"text": "TW-ICW", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.8942133188247681}]}, {"text": "Finally, we denote as TW-ICW-LW the blending of TW, ICW and label graphs (LW).", "labels": [], "entities": []}, {"text": "In label graphs we only make use of the degree centrality, since it is fast and performs best.", "labels": [], "entities": []}, {"text": "presents the results concerning the categorization performance of the proposed schemes for the six datasets.", "labels": [], "entities": []}, {"text": "As discussed previously, the size of the window considered to create the graphs is one of the model's parameters.", "labels": [], "entities": []}, {"text": "From the extensive experimental evaluation that we have performed, we have concluded that small window sizes give the most persistent results across various datasets and weighting schemes.", "labels": [], "entities": []}, {"text": "For completeness in the presentation, we report results for two window sizes.", "labels": [], "entities": []}, {"text": "In order to capture more information, we need larger window sizes for small datasets (e.g. SUBJECTIVITY).", "labels": [], "entities": []}, {"text": "Also, since for the baseline methods (TF, TF binary, TF-IDF,, TF-IDF-w2v) there is no notion of window size, the results for w = {2, 3} are the same.", "labels": [], "entities": []}, {"text": "We have also examined several centrality criteria (using both undirected and directed graphs); undirected giving better results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets' statistics: #ICW shows the  number of edges in the collection-level graph;  #w2v: number of words that exist in pre-trained  vectors.", "labels": [], "entities": [{"text": "ICW", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9729955196380615}]}, {"text": " Table 3: Comparison in accuracy(%) to state-of-the-art deep learning and graph-based approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9995311498641968}]}]}