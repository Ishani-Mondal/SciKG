{"title": [], "abstractContent": [{"text": "There are some important problems in the evaluation of word embeddings using standard word analogy tests.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 86, "end_pos": 98, "type": "TASK", "confidence": 0.722066193819046}]}, {"text": "In particular, in virtue of the assumptions made by systems generating the embeddings, these remain tests over randomness.", "labels": [], "entities": []}, {"text": "We show that even supposing there were such word analogy regularities that should be detected in the word embeddings obtained via unsupervised means, standard word analogy test implementation practices provide distorted or contrived results.", "labels": [], "entities": []}, {"text": "We raise concerns regarding the use of Principal Component Analysis to 2 or 3 dimensions as a provision of visual evidence for the existence of word analogy relations in embeddings.", "labels": [], "entities": []}, {"text": "Finally , we propose some solutions to these problems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Continuous dense representations of words, or word embeddings, are d-dimensional vectors obtained from raw unannotated text.", "labels": [], "entities": []}, {"text": "As weight vectors, they provide, given some model, predictions of either (1) some context of a word, or (2) a word given its context.", "labels": [], "entities": []}, {"text": "The word embeddings are meant to reflect distributional structure as a proxy to semantics and syntax\u00e0syntax`syntax\u00e0 la Harris.", "labels": [], "entities": []}, {"text": "A natural and desirable effect of such context driven learning of word embeddings is distributional similarity, whereby words that are similar to each other will tend to group together in the target hyperspace.", "labels": [], "entities": []}, {"text": "Thus Frenchman, Spaniard, and Dane should group together, as should loves, likes, and admires, or French, Spanish and Danish, as respectively \"a set of words for humans from specific countries\", \"a set of present tense transitive verbs denoting fondness\", and \"a set of languages\".", "labels": [], "entities": []}, {"text": "By employing a transfer learning approach with the use of word embeddings in the place of one-hot word feature vectors, word embeddings obtained in this way have been shown to both simplify and improve the performance of systems across a wide range of NLP tasks.", "labels": [], "entities": []}, {"text": "Moreover, word embeddings trained this way and used as initial word representations are now commonly understood to improve the learning process in neural network based systems across the same array of NLP tasks.", "labels": [], "entities": []}, {"text": "There has been some progress in understanding why these representations work so well and a number of simple tasks developed to evaluate them independently such as (1) word similarity tests, (2) synonym selection tests, and (3) word analogy tests, in addition to a variety of possible downstream system tests.", "labels": [], "entities": [{"text": "synonym selection", "start_pos": 194, "end_pos": 211, "type": "TASK", "confidence": 0.7650605738162994}, {"text": "word analogy", "start_pos": 227, "end_pos": 239, "type": "TASK", "confidence": 0.7683362364768982}]}, {"text": "That the distributional representations of words should reflect semantic similarity (i.e., as tested by and) is inherent in the definition of the word embedding learning task.", "labels": [], "entities": []}, {"text": "However that similar relations between words should be described byword embeddings obtained this way is not straightforward.", "labels": [], "entities": []}, {"text": "There are also standard engineering practices in analogy evaluations that would prevent accurate analogy testing even if it were applicable.", "labels": [], "entities": [{"text": "analogy evaluations", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.9149889349937439}]}, {"text": "In this paper, we hope to survey some main problems concerning the word analogy test as it is currently being calculated, in three separate directions: 1.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.7131862342357635}]}, {"text": "Theoretical assumption misalignment: A purely distributional hypothesis misaligns with testing for analogy relations.", "labels": [], "entities": []}, {"text": "2. Poor conventional engineering choices:", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Spread of norms of GloVe word vectors across  dimensions d.", "labels": [], "entities": []}, {"text": " Table 2: Results of the word analogy tests, also with- out distortion through normalisation (D), without re- moving premise vectors from the set of possible gold  vectors (H), and without either (H,D).", "labels": [], "entities": [{"text": "word analogy", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7977324724197388}]}]}