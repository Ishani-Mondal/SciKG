{"title": [{"text": "A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding", "labels": [], "entities": [{"text": "Large-Scale Domain Classification", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.7301256656646729}]}], "abstractContent": [{"text": "Intelligent personal digital assistants (IPDAs), a popular real-life application with spoken language understanding capabilities, can cover potentially thousands of overlapping domains for natural language understanding, and the task of finding the best domain to handle an utterance becomes a challenging problem on a large scale.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 189, "end_pos": 219, "type": "TASK", "confidence": 0.719924529393514}]}, {"text": "In this paper, we propose a set of efficient and scalable neural shortlisting-reranking models for large-scale domain classification in IPDAs.", "labels": [], "entities": [{"text": "domain classification", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.7090677618980408}]}, {"text": "The shortlisting stage focuses on efficiently trimming all domains down to a list of k-best candidate domains, and the reranking stage performs a list-wise reranking of the initial k-best domains with additional contextual information.", "labels": [], "entities": []}, {"text": "We show the effectiveness of our approach with extensive experiments on 1,500 IPDA domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language understanding (NLU) is a core component of intelligent personal digital assistants (IPDAs) such as Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana.", "labels": [], "entities": [{"text": "Natural language understanding (NLU)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7445070147514343}]}, {"text": "A well-established approach in current real-time systems is to classify an utterance into a domain, followed by domain-specific intent classification and slot sequence tagging (Tur and.", "labels": [], "entities": [{"text": "domain-specific intent classification", "start_pos": 112, "end_pos": 149, "type": "TASK", "confidence": 0.6465469201405843}, {"text": "slot sequence tagging", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.553260087966919}]}, {"text": "A domain is typically defined in terms of a specific application or a functionality such as weather, calendar and music, which narrows down the scope of NLU fora given utterance.", "labels": [], "entities": []}, {"text": "A domain can also be defined as a collection of relevant intents; assuming an utterance belongs to the calendar domain, possible intents could be to create a meeting or cancel one, and possible extracted slots could be people names, meeting title and date from the utterance.", "labels": [], "entities": []}, {"text": "Traditional IPDAs cover only tens of domains that share a common schema.", "labels": [], "entities": []}, {"text": "The schema is designed to separate out the domains in an effort to minimize language ambiguity.", "labels": [], "entities": []}, {"text": "A shared schema, while addressing domain ambiguity, becomes a bottleneck as new domains and intents are added to cover new scenarios.", "labels": [], "entities": []}, {"text": "Redefining the domain, intent and slot boundaries requires relabeling of the underlying data, which is very costly and time-consuming.", "labels": [], "entities": []}, {"text": "On the other hand, when thousands of domains evolve independently without a shared schema, finding the most relevant domain to handle an utterance among thousands of overlapping domains emerges as a key challenge.", "labels": [], "entities": []}, {"text": "The difficulty of solving this problem at scale has led to stopgap solutions, such as requiring an utterance to explicitly mention a domain name and restricting the expression to be in a predefined form as in \"Ask ALLRECIPES, how can I bake an apple pie?\"", "labels": [], "entities": []}, {"text": "However, such solutions lead to an unintuitive and unnatural way of conversing and create interaction friction for the end users.", "labels": [], "entities": []}, {"text": "For the example utterance, a more natural way of saying it is simply, \"How can I bake an apple pie?\" but the most relevant domain to handle it now becomes ambiguous.", "labels": [], "entities": []}, {"text": "There could be a number of candidate domains and even multiple overlapping recipe-related domains that could handle it.", "labels": [], "entities": []}, {"text": "In this paper, we propose efficient and scalable shortlisting-reranking neural models in two steps for effective large-scale domain classification in IPDAs.", "labels": [], "entities": [{"text": "domain classification", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.7431121468544006}]}, {"text": "The first step uses light-weight BiLSTM models that leverage only the character and wordlevel information to efficiently find the k-best list of most likely domains.", "labels": [], "entities": []}, {"text": "The second step uses rich contextual information later in the pipeline and applies another BiLSTM model to a list-wise ranking task to further rerank the k-best domains to find the most relevant one.", "labels": [], "entities": []}, {"text": "We show the effectiveness of our approach for large-scale domain classification with an extensive set of experiments on 1,500 IPDA domains.", "labels": [], "entities": [{"text": "large-scale domain classification", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.6412602563699087}]}], "datasetContent": [{"text": "This section gives detail of our experimental setup, followed by results and discussion.", "labels": [], "entities": []}, {"text": "We evaluated our shortlisting-reranking approach in two different settings of traditional small-scale IPDA and large-scale IPDA for comparison: Traditional IPDA For this setting, we simulated the traditional small-scale IPDA with only 20 domains that are commonly present in any IPDAs.", "labels": [], "entities": []}, {"text": "Since these domains are built-in, which are carefully designed to be non-overlapping and of high quality, the signals from user preferences and domain index become irrelevant compared to the large-scale setting.", "labels": [], "entities": []}, {"text": "The dataset comprises of more than 4M labeled utterances in text evenly distributed across 20+ domains.", "labels": [], "entities": []}, {"text": "Large-Scale IPDA This setting is a large-scale IPDA with 1,500 domains as shown in that could be overlapping with a varying level of quality.", "labels": [], "entities": []}, {"text": "For instance, there could be multiple domains to get a recipe, and a high quality domain could have more recipes with more capabilities such as making recommendations compared to a low quality one.", "labels": [], "entities": []}, {"text": "The dataset comprises of more than 6M utterances having strict invocation patterns.", "labels": [], "entities": []}, {"text": "For instance, we extract \"get me a ride\" as a preprocessed sample belonging to TAXI skill for the original utterance, \"Ask {TAXI} to {get me a ride}.\"", "labels": [], "entities": [{"text": "TAXI", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.8670780062675476}]}, {"text": "Shortlister For Shortlister, we show the results of using 2 different softmax functions of sof tmax a (smx a ) and sof tmax b (smx b ) as described in Section 4.", "labels": [], "entities": [{"text": "Shortlister", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8281558752059937}]}, {"text": "The results are shown in k-best classification accuracies, where the 5-best accuracy means the percentage of test samples that have the ground-truth domain included in the top 5 domains returned by Shortlister.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.8891558051109314}, {"text": "Shortlister", "start_pos": 198, "end_pos": 209, "type": "DATASET", "confidence": 0.9741893410682678}]}, {"text": "Hypotheses Reranker We also evaluate different variations of the reranking model for comparison.", "labels": [], "entities": []}, {"text": "\u2022 SL: Shortlister 1-best result, which is our baseline without using a reranking model.", "labels": [], "entities": [{"text": "SL", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.9826364517211914}, {"text": "Shortlister 1-best result", "start_pos": 6, "end_pos": 31, "type": "METRIC", "confidence": 0.9696429769198099}]}, {"text": "\u2022 LR: LR point-wise: A binary logistic regression model with the hypothesis vector as features (see Section 5.1).", "labels": [], "entities": []}, {"text": "We run it for each hypothesis made from Shortlister's k-best list and select the highest-scoring one, hence the  domain in that hypothesis.", "labels": [], "entities": [{"text": "Shortlister's k-best list", "start_pos": 40, "end_pos": 65, "type": "DATASET", "confidence": 0.9133837670087814}]}, {"text": "\u2022 NP O : Neural point-wise: A feed-forward (FF) layer between the hypothesis vector and the nonlinear output layer.", "labels": [], "entities": []}, {"text": "We run it for each hypothesis made from Shortlister's k-best list and select the highest-scoring hypothesis.", "labels": [], "entities": [{"text": "Shortlister's k-best list", "start_pos": 40, "end_pos": 65, "type": "DATASET", "confidence": 0.9308598786592484}]}, {"text": "\u2022 NP A : Neural pair-wise: A FF layer between the concatenation of two hypothesis vectors and the nonlinear output layer.", "labels": [], "entities": []}, {"text": "We run it k -1 times fora pair of hypotheses in a series of tournament-like competitions in the order of the k-best list.", "labels": [], "entities": []}, {"text": "For instance, the 1st and 2nd hypothesis compete first and the winner of the two competes with the 3rd hypothesis next and soon until the kth hypothesis.", "labels": [], "entities": []}, {"text": "\u2022 N CH : Neural quasi list-wise with manual cross-hypothesis features added to NP O , following past approaches ( such as the ratio of Shortlister scores to the maximum score, relative number of slots across all hypotheses, etc.", "labels": [], "entities": [{"text": "Shortlister scores", "start_pos": 135, "end_pos": 153, "type": "METRIC", "confidence": 0.790844202041626}]}, {"text": "\u2022 LST MO : Using only the BiLSTM output vectors as the input to the FF-layer.", "labels": [], "entities": [{"text": "LST MO", "start_pos": 2, "end_pos": 8, "type": "TASK", "confidence": 0.6242976188659668}]}, {"text": "\u2022 LST MS : Summing up the hypothesis vector and the BiLSTM output vectors as the input to the FF-layer, similar to residual networks ().", "labels": [], "entities": []}, {"text": "\u2022 LST MC : Concatenating the hypothesis vector and the BiLSTM output vectors as the input to the FF-layer.", "labels": [], "entities": []}, {"text": "\u2022 LST M CH : Same as LST MC except that manual cross-hypothesis features used for N CH were also added to see if combining manual and automatic cross-hypothesis features help.", "labels": [], "entities": []}, {"text": "\u2022 U PP ER: Upper bound of HypRank accuracy set by the performance of Shortlister.", "labels": [], "entities": [{"text": "U PP ER", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9632205963134766}, {"text": "HypRank", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9213900566101074}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.7870214581489563}, {"text": "Shortlister", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.7651461362838745}]}, {"text": "shows the distribution of the training, development and test sets for each setting of traditional and large-scale IPDAs.", "labels": [], "entities": []}, {"text": "Note that we ensure  no overlap between the Shortlister and HypRank training sets so that HypRank is not overly tuned on Shortlister results.", "labels": [], "entities": [{"text": "Shortlister", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9411880970001221}, {"text": "HypRank training sets", "start_pos": 60, "end_pos": 81, "type": "DATASET", "confidence": 0.8846039772033691}]}, {"text": "For the NLU models, the intent and slot models are trained on roughly 70% of the available training data.", "labels": [], "entities": [{"text": "slot", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9624502658843994}]}], "tableCaptions": [{"text": " Table 3: The k-best classification accuracies (%) of Short-", "labels": [], "entities": [{"text": "k-best classification accuracies", "start_pos": 14, "end_pos": 46, "type": "METRIC", "confidence": 0.9114105502764384}, {"text": "Short", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.6057268381118774}]}, {"text": " Table 4: The final classification accuracies (%) for different", "labels": [], "entities": [{"text": "accuracies", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.8637750148773193}]}]}