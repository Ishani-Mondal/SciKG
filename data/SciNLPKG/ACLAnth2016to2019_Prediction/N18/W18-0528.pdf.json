{"title": [{"text": "Annotation and Classification of Sentence-level Revision Improvement", "labels": [], "entities": [{"text": "Classification of Sentence-level Revision", "start_pos": 15, "end_pos": 56, "type": "TASK", "confidence": 0.7761570066213608}]}], "abstractContent": [{"text": "Studies of writing revisions rarely focus on revision quality.", "labels": [], "entities": [{"text": "writing revisions", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7866088151931763}]}, {"text": "To address this issue, we introduce a corpus of between-draft revisions of student argumentative essays, annotated as to whether each revision improves essay quality.", "labels": [], "entities": []}, {"text": "We demonstrate a potential usage of our annotations by developing a machine learning model to predict revision improvement.", "labels": [], "entities": [{"text": "revision improvement", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.882208913564682}]}, {"text": "With the goal of expanding training data, we also extract revisions from a dataset edited by expert proofreaders.", "labels": [], "entities": []}, {"text": "Our results indicate that blending expert and non-expert revisions increases model performance, with expert data particularly important for predicting low-quality revisions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supporting student revision behavior is an important area of writing-related natural language processing (NLP) research.", "labels": [], "entities": [{"text": "student revision behavior", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.6970364451408386}, {"text": "writing-related natural language processing (NLP)", "start_pos": 61, "end_pos": 110, "type": "TASK", "confidence": 0.7233335971832275}]}, {"text": "While revision is particularly effective in response to detailed feedback by an instructor, human writing evaluation is time-consuming.", "labels": [], "entities": [{"text": "human writing evaluation", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.6644572714964548}]}, {"text": "To help students improve their writing skills, various writing assistant tools have thus been developed.", "labels": [], "entities": []}, {"text": "While these tools offer instant feedback on a particular writing draft, they typically fail to explicitly compare revisions between drafts.", "labels": [], "entities": []}, {"text": "Our long term goal is to build a system for supporting students in revising argumentative essays, where the system automatically compares multiple drafts and provides useful feedback (e.g., informing students whether their revisions are improving the essay).", "labels": [], "entities": []}, {"text": "One step towards this goal is the development of a machine-learning model to automatically analyze revision improvement.", "labels": [], "entities": [{"text": "revision improvement", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.8848945498466492}]}, {"text": "Specifically, given only two sentences -original and revised, our current goal is to predict if a revised sentence is better than the original.", "labels": [], "entities": []}, {"text": "In this paper, we focus on predicting revision improvement using non-expert (i.e., student) writing data.", "labels": [], "entities": [{"text": "predicting revision improvement", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.7880775928497314}]}, {"text": "We first introduce a corpus of paired original and revised sentences that has been newly annotated as to whether each revision made the original sentence better or not.", "labels": [], "entities": []}, {"text": "The revisions area subset of those in the freely available ArgRewrite corpus (, with improvement annotated using standard rubric criteria for evaluating student argumentative writing.", "labels": [], "entities": [{"text": "ArgRewrite corpus", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.951289564371109}]}, {"text": "By adapting NLP features used in previous revision classification tasks, we then develop a prediction model that outperforms baselines, even though the size of our non-expert revision corpus is small.", "labels": [], "entities": [{"text": "revision classification tasks", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.8231574495633444}]}, {"text": "Hence, we explore extracting paired revisions from an expert edited dataset to increase training data.", "labels": [], "entities": []}, {"text": "The expert revisions area subset of those in the freely available Automated Evaluation of Scientific Writing (AESW) corpus.", "labels": [], "entities": []}, {"text": "Our experiments show that with proper sampling, combining expert and non-expert revisions can improve prediction performance, particularly for low-quality revisions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our goal is to examine whether we can predict improvement for non-expert ArgRewrite revisions, using AESW expert and/or ArgRewrite non-expert revisions for training.", "labels": [], "entities": [{"text": "AESW", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.5729486346244812}]}, {"text": "Our experiments are structured to answer the following research questions: Q1: Can we use only non-expert revisions to train a model that outperforms a baseline?", "labels": [], "entities": []}, {"text": "Q2: Can we use only expert revisions to train a model that outperforms a baseline?", "labels": [], "entities": []}, {"text": "Q3: Can we combine expert and non-expert training revisions to improve model performance?", "labels": [], "entities": []}, {"text": "Our machine learning experiments use Random Forest (RF) 8 from Python scikit-learn toolkit) with 10-fold cross validation.", "labels": [], "entities": []}, {"text": "Parameters were tuned using AESW development data.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9397954940795898}, {"text": "AESW development data", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.893368124961853}]}, {"text": "Because of the ArgRewrite class imbalance, All row), we used SMOTE () oversampling for each training fold.", "labels": [], "entities": [{"text": "ArgRewrite class imbalance", "start_pos": 15, "end_pos": 41, "type": "METRIC", "confidence": 0.9547028541564941}, {"text": "SMOTE", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9488388299942017}]}, {"text": "Feature selection was also performed on each training fold.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6657330393791199}]}, {"text": "Average un-weighted precision, recall and F1 are reported and compared to majorityclass baselines.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.960666298866272}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9998037219047546}, {"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9991808533668518}]}, {"text": "To answer Q1, we train a model using only ArgRewrite data.", "labels": [], "entities": [{"text": "ArgRewrite data", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.8303369283676147}]}, {"text": "shows that this model outperforms the majority baseline, significantly so for Precision and F1.", "labels": [], "entities": [{"text": "Precision", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.6558665633201599}, {"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9953861832618713}]}, {"text": "Compared to all other models), this model can identify 'Better' revisions with the highest recall, and can identify 'NotBetter' revisions with the highest precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9992235898971558}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9960651993751526}]}, {"text": "However, for our long-term goal of building an effective revision assistant tool, intuitively we will  also need to identify 'NotBetter' revisions with higher recall, which is very low for this model.", "labels": [], "entities": [{"text": "revision assistant", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.9173924922943115}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9993853569030762}]}, {"text": "To answer Q2, we train only on AESW data but test on the same ArgRewrite folds as above.", "labels": [], "entities": [{"text": "AESW data", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.7951355278491974}, {"text": "ArgRewrite", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9618918895721436}]}, {"text": "For both AESW revision samples (before and after removing the placeholders), only Precision is significantly better than the baseline.", "labels": [], "entities": [{"text": "AESW revision", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.4902311861515045}, {"text": "Precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9978888630867004}]}, {"text": "However, shows that AESW plaintext has significantly higher (p < 0.05) Recall than any other model in predicting 'NotBetter' revisions (which motivates Q3 as away to address the limitation noted in Q1).", "labels": [], "entities": [{"text": "Recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9963821172714233}]}, {"text": "To answer Q3, during each run of crossvalidation training we inject the AESW data in addition to the 90% ArgRewrite data, then test on the remaining 10% as before.", "labels": [], "entities": [{"text": "AESW", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.985415518283844}, {"text": "ArgRewrite", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9755479693412781}]}, {"text": "As can be seen from, AESW plaintext combined with ArgRewrite shows the best classification performance using all three metrics.", "labels": [], "entities": [{"text": "ArgRewrite", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9090325832366943}]}, {"text": "It also has improved Recall for 'NotBetter' revisions compared to training only on ArgRewrite data.", "labels": [], "entities": [{"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9991470575332642}, {"text": "ArgRewrite data", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.9319412112236023}]}, {"text": "This result indicates that selective extraction of revisions from AESW data helps improve model performance, especially when classifying low-quality revisions.", "labels": [], "entities": [{"text": "AESW data", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.9202187359333038}]}, {"text": "Finally, to understand feature utility, we compute average feature importance in the 10-folds for each experiment.", "labels": [], "entities": [{"text": "average feature importance", "start_pos": 51, "end_pos": 77, "type": "METRIC", "confidence": 0.64471036195755}]}, {"text": "Top important features include unigrams, trigrams, length difference, language errors, edit distance, BLEU score, specificity difference, and parse-tree features.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9847581386566162}]}, {"text": "For example, length difference scores in the top 5 for all experiments.", "labels": [], "entities": [{"text": "length difference", "start_pos": 13, "end_pos": 30, "type": "METRIC", "confidence": 0.94366854429245}]}, {"text": "This is intuitive as the annotation guidelines state that adding evidence can make a better revision.", "labels": [], "entities": []}, {"text": "Other features such as differences in language errors, specificity scores, and BLEU scores show more importance when training on combined ArgRewrite and AESW data than when training on only ArgRewrite.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9995678067207336}, {"text": "AESW data", "start_pos": 153, "end_pos": 162, "type": "DATASET", "confidence": 0.7670843005180359}]}, {"text": "Surprisingly, spelling error corrections show low importance.", "labels": [], "entities": [{"text": "spelling error corrections", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.6672671139240265}]}], "tableCaptions": [{"text": " Table 1: Example annotated revisions from ArgRewrite (1,2,3) and AESW (4,5). The label is calculated  using majority voting (out of 7 annotators) for ArgRewrite and using expert proofreading edits for AESW.", "labels": [], "entities": [{"text": "ArgRewrite", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.9238223433494568}, {"text": "AESW", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.8507952690124512}, {"text": "ArgRewrite", "start_pos": 151, "end_pos": 161, "type": "DATASET", "confidence": 0.9186342358589172}, {"text": "AESW", "start_pos": 202, "end_pos": 206, "type": "DATASET", "confidence": 0.9823418259620667}]}, {"text": " Table 2: Number of revisions, number of Better and N otBetter, and Fleiss's kappa (\u03ba) per increasing  majority voting (out of 7 annotators). Percentage of revisions are shown in parenthesis.", "labels": [], "entities": [{"text": "number of Better and N otBetter", "start_pos": 31, "end_pos": 62, "type": "METRIC", "confidence": 0.7359939515590668}, {"text": "Fleiss's kappa (\u03ba)", "start_pos": 68, "end_pos": 86, "type": "METRIC", "confidence": 0.9448811709880829}]}, {"text": " Table 3: 10-fold cross-validation performance. *  indicates significantly better than majority (p <  0.05). Bold indicates highest column value.", "labels": [], "entities": []}]}