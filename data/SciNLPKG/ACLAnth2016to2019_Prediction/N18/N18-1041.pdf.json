{"title": [{"text": "Abstract Meaning Representation for Paraphrase Detection", "labels": [], "entities": [{"text": "Abstract Meaning Representation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7196782529354095}, {"text": "Paraphrase Detection", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.9152260422706604}]}], "abstractContent": [{"text": "Meaning Representation (AMR) parsing aims at abstracting away from the syntactic realization of a sentence, and denoting only its meaning in a canonical form.", "labels": [], "entities": [{"text": "Meaning Representation (AMR) parsing", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8785271545251211}]}, {"text": "As such, it is ideal for paraphrase detection, a problem in which one is required to specify whether two sentences have the same meaning.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.9773503839969635}]}, {"text": "We show that na\u00a8\u0131vena\u00a8\u0131ve use of AMR in paraphrase detection is not necessarily useful, and turn to describe a technique based on latent semantic analysis in combination with AMR parsing that significantly advances state-of-the-art results in paraphrase detection for the Microsoft Research Paraphrase Corpus.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.9238600432872772}, {"text": "AMR parsing", "start_pos": 175, "end_pos": 186, "type": "TASK", "confidence": 0.7528514564037323}, {"text": "paraphrase detection", "start_pos": 243, "end_pos": 263, "type": "TASK", "confidence": 0.9224178194999695}, {"text": "Microsoft Research Paraphrase Corpus", "start_pos": 272, "end_pos": 308, "type": "DATASET", "confidence": 0.9003272950649261}]}, {"text": "Our best results in the transductive setting are 86.6% for accuracy and 90.0% for F 1 measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.999648928642273}, {"text": "F 1 measure", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.9851007461547852}]}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) parsing focuses on the conversion of natural language sentences into AMR graphs, aimed at abstracting away from the surface realizations of the sentences while preserving their meaning.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR) parsing", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8347198452268328}, {"text": "conversion of natural language sentences into AMR graphs", "start_pos": 61, "end_pos": 117, "type": "TASK", "confidence": 0.7163964062929153}]}, {"text": "We make a first step towards showing that AMR can be used in practice fora task that requires identifying the canonicalization of language: paraphrase detection.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.8360655307769775}]}, {"text": "Ina \"perfect world\" using AMR to test for paraphrasing relation of two sentences should be simple.", "labels": [], "entities": []}, {"text": "It would require finding the two AMR parses for each of the sentences, and then checking whether they are identical.", "labels": [], "entities": [{"text": "AMR parses", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.6875971853733063}]}, {"text": "Since AMR is aimed at abstracting away from the surface form which is used to express meaning, two sentences should be paraphrases only if they have identical AMRs.", "labels": [], "entities": []}, {"text": "For instance, the three sentences: should result in the same AMR graph as shown in.", "labels": [], "entities": []}, {"text": "However, in practice, things are different.", "labels": [], "entities": []}, {"text": "First, there are no known AMR parsers that really distil only the meaning in text.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9036802649497986}]}, {"text": "For example, predicates which have interchangeable meaning use different AMR concepts, and there are errors that exist because of the machine learning techniques that are used for learning the parsers from data.", "labels": [], "entities": []}, {"text": "Finally, even human annotations do not yield perfect AMRs, as the interannotator agreement reported in the literature for AMR is around 80% (.", "labels": [], "entities": [{"text": "AMRs", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.9108316898345947}, {"text": "interannotator agreement", "start_pos": 66, "end_pos": 90, "type": "METRIC", "confidence": 0.8922064304351807}, {"text": "AMR", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.8312479257583618}]}, {"text": "Second, meaning is often contextual, and it is not fully possible to determine the corresponding AMR parse just by looking at a given sentence.", "labels": [], "entities": [{"text": "AMR parse", "start_pos": 97, "end_pos": 106, "type": "TASK", "confidence": 0.8140754103660583}]}, {"text": "Entity mentions denote different entities in different contexts, and similarly predicates and nouns are ambiguous and depend on context.", "labels": [], "entities": []}, {"text": "As such, one cannot expect to use AMR in the transparent way mentioned above to identify paraphrase relations.", "labels": [], "entities": []}, {"text": "However, we demonstrate in this paper that AMR can be used in a \"softer\" way to detect such relations.", "labels": [], "entities": [{"text": "AMR", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.612421452999115}]}, {"text": "Evaluation of AMR parsers is traditionally performed using the Smatch score).", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.868259608745575}]}, {"text": "However, argue that more ad-hoc metrics can be useful for advancing AMR research.", "labels": [], "entities": [{"text": "AMR", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9865200519561768}]}, {"text": "Paraphrase detection can be seen as a further benchmark for AMR parsers, highlighting their ability of abstracting away from syntax and representing the core concepts expressed in the sentence.", "labels": [], "entities": [{"text": "Paraphrase detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9185716509819031}, {"text": "AMR parsers", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.9097693562507629}]}, {"text": "In order to advance research in AMR and its applications, it is important to have metrics that reflect on the ability of AMR graphs to have impact on subsequent tasks.", "labels": [], "entities": [{"text": "AMR", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9884117245674133}]}, {"text": "In this work we therefore use two different AMR parsers, comparing them throughout all experiments.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.7289957404136658}]}], "datasetContent": [{"text": "We now describe the experiments that we devised to discover whether AMR is useful for paraphrase detection.", "labels": [], "entities": [{"text": "AMR", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.8755995631217957}, {"text": "paraphrase detection", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.9739273488521576}]}, {"text": "For AMR parsing, we used the JAMR 2 version published for SemEval, reporting 0.67 Smatch score on LDC2015E86 and the first and only version available for AMREager, 3 obtaining 0.64 Smatch score on the same dataset.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.938919872045517}, {"text": "LDC2015E86", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.8790214657783508}, {"text": "AMREager", "start_pos": 154, "end_pos": 162, "type": "DATASET", "confidence": 0.8850789070129395}]}, {"text": "First, we discuss experiments where the AMRs are used as a mean to extract additional sparse features fora SVM classifier.", "labels": [], "entities": []}, {"text": "Then we turn to LSA to construct a representation of the sentence based on the reweighting on the AMR nodes achieved through either PageRank or TF-IDF.", "labels": [], "entities": []}, {"text": "Results show how the latter, which builds on state-of-the-art systems for this task, is a much more promising approach.", "labels": [], "entities": []}, {"text": "Finally, we analyze how performance changes as a function of the number of dimensions used in the truncated matrix.", "labels": [], "entities": []}, {"text": "For evaluation, we use the Microsoft Research Paraphrase Corpus ().", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase Corpus", "start_pos": 27, "end_pos": 63, "type": "DATASET", "confidence": 0.9310396015644073}]}, {"text": "We use 70% of the dataset as training data and 30% as a test set.", "labels": [], "entities": []}, {"text": "The total number of sentence pairs in the corpus is 5,801.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline results for paraphrase detection with  AMR and with bag-of-words (BOW). \"linear,\" \"poly\"  and \"rbf\" denote the kernel which is used with a  support vector machine classifier. \"Smatch\" denotes  the use of the additional graph similarity feature and  \"BOC\" the use of the additional Jaccard score on the  bag of concept. Best result in each column is in bold.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.9229544699192047}, {"text": "BOW", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9246860146522522}, {"text": "BOC", "start_pos": 269, "end_pos": 272, "type": "METRIC", "confidence": 0.994306743144989}]}, {"text": " Table 2: Comparison of our results with previous work  (\"NR\" stands for \"not reported\"). All work mentioned  above was done in the inductive setting, except for Ji  and Eisenstein (2013), which, like us, was done in both  settings.", "labels": [], "entities": []}, {"text": " Table 3: LSA experiments in the inductive and transductive settings, with two different reweighting schema:  \"PageRank\" and \"TF-IDF\". \"linear,\" \"poly\" and \"rbf\" denote the kernel for the SVM. \"dep.\" denotes the use of  syntactic parsing instead of semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 249, "end_pos": 265, "type": "TASK", "confidence": 0.750799298286438}]}]}