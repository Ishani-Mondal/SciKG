{"title": [{"text": "Pivot Based Language Modeling for Improved Neural Domain Adaptation", "labels": [], "entities": [{"text": "Pivot Based Language Modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6027473732829094}, {"text": "Improved Neural Domain Adaptation", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.591660663485527}]}], "abstractContent": [{"text": "Representation learning with pivot-based methods and with Neural Networks (NNs) have lead to significant progress in domain adaptation for Natural Language Processing.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.7110513150691986}]}, {"text": "However, most previous work that follows these approaches does not explicitly exploit the structure of the input text, and its output is most often a single representation vector for the entire text.", "labels": [], "entities": []}, {"text": "In this paper we present the Pivot Based Language Model (PBLM), a representation learning model that marries together pivot-based and NN modeling in a structure aware manner.", "labels": [], "entities": []}, {"text": "Particularly, our model processes the information in the text with a sequential NN (LSTM) and its output consists of a context-dependent representation vector for every input word.", "labels": [], "entities": []}, {"text": "Unlike most previous representation learning models in domain adaptation, PBLM can naturally feed structure aware text classifiers such as LSTM and CNN.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7482021749019623}]}, {"text": "We experiment with the task of cross-domain sentiment classification on 20 domain pairs and show substantial improvements over strong baselines.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.7919866840044657}]}], "introductionContent": [{"text": "Domain adaptation) is a fundamental challenge in NLP, due to the reliance of many algorithms on costly labeled data which is scarce in many domains.", "labels": [], "entities": [{"text": "Domain adaptation)", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.846517542997996}]}, {"text": "To save annotation efforts, DA aims to import algorithms trained with labeled data from one or several domains to new ones.", "labels": [], "entities": []}, {"text": "While DA algorithms have long been developed for many tasks and domains (e.g. (), the unprecedented growth of heterogeneous online content calls for more progress.", "labels": [], "entities": [{"text": "DA", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.969801127910614}]}, {"text": "DA through Representation Learning (DReL), where the DA method induces shared representations for the examples in the source and the target domains, has become prominent in the Neural Network (NN) era.", "labels": [], "entities": [{"text": "DA through Representation Learning (DReL)", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8073211056845528}]}, {"text": "A seminal (non-NN) DReL work is structural correspondence learning (SCL)) which models the connections between pivot features -features that are frequent in the source and the target domains and are highly correlated with the task label in the source domain -and the other, non-pivot, features.", "labels": [], "entities": [{"text": "structural correspondence learning (SCL))", "start_pos": 32, "end_pos": 73, "type": "TASK", "confidence": 0.7166937539974848}]}, {"text": "While this approach explicitly models the correspondence between the source and the target domains, it has been outperformed by NN-based models, particularly those based on autoencoders) which employ compress-based noise reduction to extract features that empirically support domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 276, "end_pos": 293, "type": "TASK", "confidence": 0.7593351006507874}]}, {"text": "Recently, proposed to marry these approaches.", "labels": [], "entities": []}, {"text": "They have presented the autoencoder-SCL models and demonstrated their superiority over a large number of previous approaches, particularly those that employ pivot-based ideas only or NNs only.", "labels": [], "entities": []}, {"text": "Current DReL methods, however, suffer from a fundamental limitation: they ignore the structure of their input text (usually sentence or document).", "labels": [], "entities": []}, {"text": "This is reflected both in the way they represent their input text, typically with a single vector whose coordinates correspond to word counts or indicators across the text, and in their output which typically consists of a single vector representation.", "labels": [], "entities": []}, {"text": "This structure-indifferent approach stands in a sharp contrast to numerous NLP algorithms where text structure plays a key role.", "labels": [], "entities": []}, {"text": "Moreover, learning a single feature vector per input example, these methods can feed only task classifiers such as SVM and feed-forward NNs that take a single vector as input, but cannot feed sequential (e.g. RNNs and LSTMs) or convolution (CNNs () networks that require an input vector per word or sentence in their input.", "labels": [], "entities": []}, {"text": "This maybe a serious limitation given the excellent performance of structure aware models in a large variety of NLP tasks, including sentiment analysis and text classification (e.g.) -prominent DA evaluation tasks.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.9648201763629913}, {"text": "text classification", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7419304549694061}, {"text": "DA evaluation tasks", "start_pos": 194, "end_pos": 213, "type": "TASK", "confidence": 0.926348090171814}]}, {"text": "demonstrates the limitation of structureindifferent modeling in DA for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.9448849856853485}]}, {"text": "While the example review contains more positive pivot features (see definition in Sec.", "labels": [], "entities": []}, {"text": "2), the sentiment expressed in the review is negative.", "labels": [], "entities": []}, {"text": "A representation learning method should encode the review structure (e.g. the role of the terms at first and However) in order to uncover the sentiment.", "labels": [], "entities": []}, {"text": "In this paper we overcome these limitations.", "labels": [], "entities": []}, {"text": "We present (Section 3) the Pivot Based Language Model (PBLM) -a domain adaptation model that (a) is aware of the structure of its input text; and (b) outputs a representation vector for every input word.", "labels": [], "entities": []}, {"text": "Particularly, the model is a sequential NN (LSTM) that operates very similarly to LSTM language models (LSTM-LMs).", "labels": [], "entities": []}, {"text": "The fundamental difference is that while for every input word LSTM-LMs output a hidden vector and a prediction of the next word, the output of PBLM is a hidden vector and a prediction of the next word if that word is a pivot feature or else, a generic NONE tag.", "labels": [], "entities": []}, {"text": "Hence, PBLM not only exploits the sequential nature of its input text, but its output states can naturally feed LSTM and CNN task classifiers.", "labels": [], "entities": []}, {"text": "Notice that PBLM is very flexible: instead of pivot based unigram prediction it can be defined to predict pivots of arbitrary length (e.g. the next bigram or trigram), or, alternatively, it can be defined over sentences or other textual units instead of words.", "labels": [], "entities": []}, {"text": "Following a large body of DA work, we experiment (Section 5) with the task of binary sentiment classification.", "labels": [], "entities": [{"text": "binary sentiment classification", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.8011354009310404}]}, {"text": "We consider adaptation between each domain pair in the four product review domains of  (12 domain pairs) as well as between these domains and an airline review domain and vice versa (8 domain pairs).", "labels": [], "entities": []}, {"text": "The latter 8 setups are particularly I was at first :::: very ::::::: excited with my new Zyliss salad spinner -it is easy to spin and looks ::::: great ...", "labels": [], "entities": []}, {"text": "it doesn't get your greens very dry.", "labels": [], "entities": []}, {"text": "I've been surprised and disappointed by the amount of water left on lettuce after spinning, and spinning, and spinning.", "labels": [], "entities": []}, {"text": "challenging as the airline reviews tend to be more negative than the product reviews (see Section 4).", "labels": [], "entities": []}, {"text": "We implement PBLM with two task classifiers, LSTM and CNN, and compare them to strong previous models, among which are: SCL (pivot based, no NN), the marginalized stacked denoising autoencoder model) -AE based, no pivots), the MSDA-DAN model) -AE with a Domain Adversarial Network (DAN) enhancement) and AE-SCL-SR (the best performing model of ZR17, combining AEs, pivot information and pretrained word vectors).", "labels": [], "entities": [{"text": "AE-SCL-SR", "start_pos": 304, "end_pos": 313, "type": "METRIC", "confidence": 0.8818129301071167}]}, {"text": "PBLM-LSTM and PBLM-CNN perform very similarly to each other and strongly outperform previous models.", "labels": [], "entities": [{"text": "PBLM-LSTM", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.935093104839325}, {"text": "PBLM-CNN", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.8641340136528015}]}, {"text": "For example, PBLM-CNN achieves averaged accuracies of 80.4%, 84% and 76.2% in the 12 product domain setups, 4 product to airline setups and 4 airline to product setups, respectively, while AE-SCL-SR, the best baseline, achieves averaged accuracies of 78.1%, 78.7% and 68.1%, respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.8768916130065918}, {"text": "AE-SCL-SR", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.5582874417304993}, {"text": "accuracies", "start_pos": 237, "end_pos": 247, "type": "METRIC", "confidence": 0.8318086266517639}]}], "datasetContent": [{"text": "Interestingly, in the product domains unlabeled reviews tend to be much more positive than in the airline domain.", "labels": [], "entities": []}, {"text": "Particularly, in the B domain there are 6.43 positive reviews on every negative review; in D the ratio is 7.39 to 1; in E it is 3.65 to 1; and in Kit is 4.61 to 1.", "labels": [], "entities": [{"text": "Kit", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.775694727897644}]}, {"text": "In the airline domain there are only 1.15 positive reviews for every negative review.", "labels": [], "entities": [{"text": "airline domain", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.8902445435523987}]}, {"text": "We hence expect DA from product to airline reviews and vice versa to be more challenging than DA from one product review domain to another.", "labels": [], "entities": [{"text": "DA", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9110495448112488}]}, {"text": "Baselines We consider the following baselines: (a) AE-SCL-SR (ZR17).", "labels": [], "entities": [{"text": "AE-SCL-SR", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9970550537109375}]}, {"text": "We also experimented with the more basic AE-SCL but, like in ZR17, we got lower results inmost cases; (b) SCL with pivot features selected using the mutual information criterion).", "labels": [], "entities": [{"text": "AE-SCL", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.962977945804596}]}, {"text": "For this method we used the implementation of ZR17; (c) MSDA, with code taken from the authors' web page; (d) The MSDA-DAN model () which employs a domain adversarial network (DAN) with the MSDA vectors as input.", "labels": [], "entities": []}, {"text": "The DAN code is taken from the authors' repository; (e) The no domain adaptation case where the sentiment classifier is trained in the source domain and applied to the target domain without adaptation.", "labels": [], "entities": [{"text": "no domain adaptation", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.6940189599990845}]}, {"text": "For this case we consider three classifiers: logistic regression (denoted NoSt as it is not aware of its input's structure), as well as LSTM and CNN which provide a control for the importance of the structure aware task classifiers in PBLM models.", "labels": [], "entities": [{"text": "CNN", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.7211338877677917}]}, {"text": "To further control for this effect we compare to the PBLM-NoSt model where the PBLM output vectors (h t vectors generated after each input word) are averaged and the averaged vector feeds the logistic regression classifier.", "labels": [], "entities": []}, {"text": "In all the participating methods, the input features consist of word unigrams and bigrams.", "labels": [], "entities": []}, {"text": "The division of the feature set into pivots and nonpivots is based on the the method of ZR17 that followed the work of  (details are in Appendix C).", "labels": [], "entities": []}, {"text": "The sentiment classifier employed with the SCL-MI, MSDA and AE-SCL-SR representations is the same logistic regression classifier as in the NoSt condition mentioned above.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8090728521347046}, {"text": "AE-SCL-SR", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9376932978630066}]}, {"text": "For these methods we concatenate the representation learned by the model with the original representation and this representation is fed to the classifier.", "labels": [], "entities": []}, {"text": "MSDA-DAN jointly learns the feature representation and performs the sentiment classification task.", "labels": [], "entities": [{"text": "MSDA-DAN", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7931004762649536}, {"text": "sentiment classification", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.9415291249752045}]}, {"text": "It is hence fed by a concatenation of the original and the MSDA-induced representations.", "labels": [], "entities": []}, {"text": "Five Fold CV We employ a 5-fold crossvalidation protocol as in.", "labels": [], "entities": []}, {"text": "In all five folds 1600 source domain examples are randomly selected for training data and 400 for development, such that both the training and the development sets are balanced and have the same number of positive and negative reviews.", "labels": [], "entities": []}, {"text": "The results we report are the averaged performance of each model across these 5 folds.", "labels": [], "entities": []}, {"text": "Hyperparameter Tuning For all previous models, we follow the tuning process described in ZR17 (paper and appendices).", "labels": [], "entities": [{"text": "ZR17", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9144895672798157}]}, {"text": "Hyperparameter tuning for the PBLM models and the non-adapted CNN and LSTM is described in Appendix B.  Hyperparameter Tuning As discussed in section 4 of the paper, for all previous work models, we follow the experimental setup of ZR17 (paper and appendices) including their hyperparameter estimation protocol.", "labels": [], "entities": []}, {"text": "The hyperparameters of the PBLM models and the non-adapted CNN and LSTM are provided here.", "labels": [], "entities": []}, {"text": "For PBLM we considered the following hyperparameteres: \u2022 Input word embedding size:).", "labels": [], "entities": [{"text": "Input word embedding size", "start_pos": 57, "end_pos": 82, "type": "METRIC", "confidence": 0.8298303931951523}]}, {"text": "\u2022 Number of pivot features: (100, 200, 300, 400, 500).", "labels": [], "entities": []}, {"text": "\u2022 |h t | :).", "labels": [], "entities": []}, {"text": "\u2022 PBLM model order: second order.", "labels": [], "entities": []}, {"text": "For the LSTM in PBLM-LSTM as well as the baseline non-adapted LSTM we considered the same |h t | and input word embedding size values as for PBLM.", "labels": [], "entities": []}, {"text": "For PBLM-CNN and for the baseline, non-adapted, CNN we only experimented with K = 250 filters and with a kernel of size d = 3.", "labels": [], "entities": []}, {"text": "All the algorithms in the paper that involve a CNN or a LSTM (including the PBLM itself) are trained with the ADAM algorithm (.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.8418335914611816}]}, {"text": "For this algorithm we used the parameters described in the original ADAM article: \u2022 Learning rate: lr = 0.001.", "labels": [], "entities": [{"text": "ADAM article", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9405579268932343}, {"text": "Learning rate", "start_pos": 84, "end_pos": 97, "type": "METRIC", "confidence": 0.9534043967723846}]}, {"text": "\u2022 Exponential decay rate for the 1st moment estimates: \u03b2 1 = 0.9.", "labels": [], "entities": [{"text": "Exponential decay rate", "start_pos": 2, "end_pos": 24, "type": "METRIC", "confidence": 0.962062656879425}]}, {"text": "\u2022 Exponential decay rate for the 2nd moment estimates: \u03b2 2 = 0.999.", "labels": [], "entities": [{"text": "Exponential decay rate", "start_pos": 2, "end_pos": 24, "type": "METRIC", "confidence": 0.9592458407084147}]}, {"text": "\u2022 Fuzz factor: = 1e \u2212 08.", "labels": [], "entities": [{"text": "Fuzz factor", "start_pos": 2, "end_pos": 13, "type": "METRIC", "confidence": 0.9758606255054474}]}, {"text": "\u2022 Learning rate decay over each update: decay = 0.0.", "labels": [], "entities": [{"text": "Learning rate decay", "start_pos": 2, "end_pos": 21, "type": "METRIC", "confidence": 0.8882699608802795}, {"text": "decay", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9574084281921387}]}, {"text": "Experimental Details All sequential models considered in our experiments are fed with one review example at a time.", "labels": [], "entities": []}, {"text": "For all models in the paper, punctuation is first removed from the text before it is processed by the model (sentence boundaries are still encoded).", "labels": [], "entities": []}, {"text": "This is the only preprecessing step we employ in the paper.", "labels": [], "entities": []}, {"text": "We considered several alternative implementations of the PBLM-NoSt baseline.", "labels": [], "entities": [{"text": "PBLM-NoSt baseline", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.9111635088920593}]}, {"text": "In the variant we selected the PBLM output vectors (h t vectors generated after each word of the input review) are averaged and the averaged vector feeds a nonstructured logistic regression classifier.", "labels": [], "entities": []}, {"text": "We also tried to take only the final ht vector of PBLM as an input to the classifier or to sum the ht vectors instead of taking their average.", "labels": [], "entities": []}, {"text": "These alternatives gave worse results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of adaption between product review domains (top", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9943349361419678}, {"text": "adaption between product review domains", "start_pos": 22, "end_pos": 61, "type": "TASK", "confidence": 0.7118447780609131}]}]}