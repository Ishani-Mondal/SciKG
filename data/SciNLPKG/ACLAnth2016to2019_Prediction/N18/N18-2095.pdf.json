{"title": [{"text": "Cross-Domain Review Helpfulness Prediction based on Convolutional Neural Networks with Auxiliary Domain Discriminators", "labels": [], "entities": [{"text": "Helpfulness Prediction", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.6593749821186066}]}], "abstractContent": [{"text": "With the growing amount of reviews in e-commerce websites, it is critical to assess the helpfulness of reviews and recommend them accordingly to consumers.", "labels": [], "entities": []}, {"text": "Recent studies on review helpfulness require plenty of labeled samples for each domain/category of interests.", "labels": [], "entities": []}, {"text": "However, such an approach based on close-world assumption is not always practical , especially for domains with limited reviews or the \"out-of-vocabulary\" problem.", "labels": [], "entities": []}, {"text": "Therefore, we propose a convolutional neural network (CNN) based model which leverages both word-level and character-based representations.", "labels": [], "entities": []}, {"text": "To transfer knowledge between domains , we further extend our model to jointly model different domains with auxiliary domain discriminators.", "labels": [], "entities": []}, {"text": "On the Amazon product review dataset, our approach significantly out-performs the state of the art in terms of both accuracy and cross-domain robustness.", "labels": [], "entities": [{"text": "Amazon product review dataset", "start_pos": 7, "end_pos": 36, "type": "DATASET", "confidence": 0.9227904081344604}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9991428852081299}]}], "introductionContent": [{"text": "Product reviews significantly help consumers finalize their purchasing decisions.", "labels": [], "entities": []}, {"text": "With online reviews being ubiquitous, it is critical to examine the quality of reviews and present consumers more useful information.", "labels": [], "entities": []}, {"text": "Both academia and industry have drawn close attention to the task of review helpfulness prediction (.", "labels": [], "entities": [{"text": "review helpfulness prediction", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.7322764794031779}]}, {"text": "Recent studies on review helpfulness prediction have been shown effective by using handcrafted features.", "labels": [], "entities": [{"text": "review helpfulness prediction", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.8326956431070963}]}, {"text": "For example, semantic features like LIWC, INQUIRER, and GALC (), aspect-() and argument-based () features.", "labels": [], "entities": []}, {"text": "However, those methods require a large amount of labeled samples which is not always practical and yields models limited to product domains/categories of interests.", "labels": [], "entities": []}, {"text": "For example, the * * Yinfei Yang is now with Google.", "labels": [], "entities": [{"text": "Google", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.9577010869979858}]}, {"text": "\"Electronics\" category used in our experiment from Amazon.com Review Dataset) has more than 354k labeled reviews, while the \"Watches\" category has under 10k.", "labels": [], "entities": [{"text": "Amazon.com Review Dataset", "start_pos": 51, "end_pos": 76, "type": "DATASET", "confidence": 0.8958219885826111}]}, {"text": "For domains with limited data, labeled samples maybe too few to build good estimators and the \"out-of-vocabulary\" (OOV) problem is often observed.", "labels": [], "entities": []}, {"text": "To alleviate the aforementioned issues, in this work, we propose an end-to-end approach for review helpfulness prediction requiring no prior knowledge nor manual feature crafting.", "labels": [], "entities": [{"text": "review helpfulness prediction", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.7149941623210907}]}, {"text": "In recent years, convolutional neural networks (CNNs), able to extract deep features from raw text contents, have demonstrated remarkable results in many tasks of natural language processing, for its high efficiency and performance comparable to Recurrent Neural Networks (RNNs)).", "labels": [], "entities": []}, {"text": "We thus employ CNNs as the basis of this work.", "labels": [], "entities": []}, {"text": "As character-level representations are notably beneficial for alleviating the OOV problem for tasks such as text classification and machine translation (, we specifically enrich the word-level representation of CNNs by adding character-based representation.", "labels": [], "entities": [{"text": "text classification", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7583364546298981}, {"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.8111919164657593}]}, {"text": "Experiments show that our CNNbased method significantly outperforms those using hand-crafted features and yields better results than the ensemble models.", "labels": [], "entities": []}, {"text": "To tackle the problem of insufficient data in some domains, we develop a cross-domain transfer learning (TL) approach to leverage knowledge from a domain with sufficient data.", "labels": [], "entities": [{"text": "cross-domain transfer learning (TL)", "start_pos": 73, "end_pos": 108, "type": "TASK", "confidence": 0.8293043474356333}]}, {"text": "It is worth noting that, existing studies on this task only focus on a single product category or largely ignore the inter-domain correlations.", "labels": [], "entities": []}, {"text": "Previous works also show that some features are domain-specific while others are sharable across domains.", "labels": [], "entities": []}, {"text": "For example, image quality features are only useful for categories covering products like cameras (, while semantic features and argumentbased features usually work for all domains).", "labels": [], "entities": []}, {"text": "Thus it is important fora TL approach to learn shared features for different domains.", "labels": [], "entities": []}, {"text": "A typical TL model uses both a shared neural network (NN) and domainspecific NNs to derive shared and domain-specific features (.", "labels": [], "entities": [{"text": "TL", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9542916417121887}]}, {"text": "Recently, and apply adversarial loss and domain discriminators to specific shared models using RNNs for text classification and word segmentation tasks, respectively.", "labels": [], "entities": [{"text": "text classification", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7795453071594238}, {"text": "word segmentation tasks", "start_pos": 128, "end_pos": 151, "type": "TASK", "confidence": 0.8038268784681956}]}, {"text": "Inspired by them, we study the crossdomain review helpfulness task with both adversarial loss and domain discriminators in a specific shared framework.", "labels": [], "entities": [{"text": "crossdomain review helpfulness task", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.7701865434646606}]}, {"text": "Ina nutshell, our main novelty is in the first endto-end cross-domain model for review helpfulness prediction.", "labels": [], "entities": [{"text": "review helpfulness prediction", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.7445197304089864}]}, {"text": "Our model consists of two components: a feature transformation network (CNN) to represent the input reviews and a transfer learning module to adapt domain knowledge.", "labels": [], "entities": []}, {"text": "In addition, shared and specific-shared features are confined with adversarial and domain discrimination losses.", "labels": [], "entities": []}, {"text": "Extensive experiments show that our model is able to transfer knowledge between domains, and outperforms the state of the arts.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 formally defines the problem and presents our model.", "labels": [], "entities": []}, {"text": "Section 3 illustrates the effectiveness of the proposed model in the experiments.", "labels": [], "entities": []}, {"text": "Section 4 presents related work, and finally Section 5 concludes our paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following previous work (), experiments are done on reviews from five categories of products in Amazon review dataset  The lookup table E is initialized with pretrained vectors from GloVe () by setting l = 100.", "labels": [], "entities": [{"text": "Amazon review dataset", "start_pos": 96, "end_pos": 117, "type": "DATASET", "confidence": 0.9441489775975546}]}, {"text": "For CNNs, the activation function is ReLU, and the channel size is set to 128.", "labels": [], "entities": [{"text": "CNNs", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.956494927406311}, {"text": "ReLU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9945014715194702}]}, {"text": "We also set \u03bb 1 = \u03bb 2 = \u03bb 3 = \u03bb 4 = 0.05, and \u03bb 5 = 0.0008.", "labels": [], "entities": []}, {"text": "AdaGrad () is used in training with an initial learning rate of 0.08.", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8253157138824463}]}, {"text": "Fowllowing the previous work, ten-fold cross-validation is performed for all experiments and all the results are evaluated in correlation coefficients between the predicted helpfulness score and the ground truth score computed by \"a of b approach\" from the dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Amazon reviews from 5 different categories.", "labels": [], "entities": []}, {"text": " Table 2: Comparison with linguistic features.", "labels": [], "entities": []}, {"text": " Table 3.2, our CNN-based model  consistently outperforms the models based on en- semble features.", "labels": [], "entities": []}, {"text": " Table 3: Comparison with ensemble features.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of TL models.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of TL with respect to the amount  of training data of the \"Outdoor\" category.", "labels": [], "entities": [{"text": "TL", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.7394716143608093}]}]}