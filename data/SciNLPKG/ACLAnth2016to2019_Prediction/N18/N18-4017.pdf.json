{"title": [{"text": "Training a Ranking Function for Open-Domain Question Answering", "labels": [], "entities": [{"text": "Open-Domain Question Answering", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.5737081269423167}]}], "abstractContent": [{"text": "In recent years, there have been amazing advances in deep learning methods for machine reading.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.7993537485599518}]}, {"text": "In machine reading, the machine reader has to extract the answer from the given ground truth paragraph.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 3, "end_pos": 18, "type": "TASK", "confidence": 0.8142794668674469}]}, {"text": "Recently, the state-of-the-art machine reading models achieve human level performance in SQuAD which is a reading comprehension-style question answering (QA) task.", "labels": [], "entities": [{"text": "reading comprehension-style question answering (QA) task", "start_pos": 106, "end_pos": 162, "type": "TASK", "confidence": 0.7599781155586243}]}, {"text": "The success of machine reading has inspired researchers to combine information retrieval with machine reading to tackle open-domain QA.", "labels": [], "entities": []}, {"text": "However, these systems perform poorly compared to reading comprehension-style QA because it is difficult to retrieve the pieces of paragraphs that contain the answer to the question.", "labels": [], "entities": []}, {"text": "In this study, we propose two neural network rankers that assign scores to different passages based on their likelihood of containing the answer to a given question.", "labels": [], "entities": []}, {"text": "Additionally, we analyze the relative importance of semantic similarity and word level relevance matching in open-domain QA.", "labels": [], "entities": [{"text": "word level relevance matching", "start_pos": 76, "end_pos": 105, "type": "TASK", "confidence": 0.6657114699482918}]}], "introductionContent": [{"text": "The goal of a question answering (QA) system is to provide a relevant answer to a natural language question.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.8537193655967712}]}, {"text": "In reading comprehension-style QA, the ground truth paragraph that contains the answer is given to the system whereas no such information is available in open-domain QA setting.", "labels": [], "entities": [{"text": "reading comprehension-style QA", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.5698538521925608}]}, {"text": "Open-domain QA systems have generally been built upon large-scale structured knowledge bases, such as Freebase or DBpedia.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.736894965171814}]}, {"text": "The drawback of this approach is that these knowledge bases are not complete, and are expensive to construct and maintain.", "labels": [], "entities": []}, {"text": "Another method for open-domain QA is a corpus-based approach where the QA system looks for the answer in the unstructured text corpus ().", "labels": [], "entities": []}, {"text": "This approach eliminates the need to build and update knowledge bases by taking advantage of the large amount of text data available on the web.", "labels": [], "entities": []}, {"text": "Complex parsing rules and information extraction methods are required to extract answers from unstructured text.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7715339362621307}]}, {"text": "As machine readers are excellent at this task, there have been attempts to combine search engines with machine reading for corpus-based open-domain QA.", "labels": [], "entities": []}, {"text": "To achieve high accuracy in this setting, the top documents retrieved by the search engine must be relevant to the question.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9980973601341248}]}, {"text": "As the top ranked documents returned from search engine might not contain the answer that the machine reader is looking for, reranking the documents based on the likelihood of containing answer will improve the overall QA performance.", "labels": [], "entities": []}, {"text": "Our focus is on building a neural network ranker to re-rank the documents retrieved by a search engine to improve overall QA performance.", "labels": [], "entities": []}, {"text": "Semantic similarity is crucial in QA as the passage containing the answer maybe semantically similar to the question but may not contain the exact same words in the question.", "labels": [], "entities": []}, {"text": "For example, the answer to \"What country did world cup 1998 take place in?\" can be found in \"World cup 1998 was held in France.\"", "labels": [], "entities": [{"text": "World cup 1998", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.8170469800631205}]}, {"text": "Therefore, we evaluate the performance of the fixed size distributed representations that encode the general meaning of the whole sentence on ranking.", "labels": [], "entities": []}, {"text": "We use a simple feed-forward neural network with fixed size question and paragraph representations for this purpose.", "labels": [], "entities": []}, {"text": "In ad-hoc retrieval, the system aims to return a list of documents that satisfies the user's information need described in the query.", "labels": [], "entities": []}, {"text": "1  show that, in ad-hoc retrieval, rele-", "labels": [], "entities": []}], "datasetContent": [{"text": "The QUestion Answering by Search And Reading (QUASAR) dataset ( includes QUASAR-S and QUASAR-T, each designed to address the combination of retrieval and machine reading.", "labels": [], "entities": [{"text": "Answering by Search And Reading (QUASAR)", "start_pos": 13, "end_pos": 53, "type": "TASK", "confidence": 0.8382063284516335}]}, {"text": "QUASAR-S consists of fill-inthe-gaps questions collected from Stackoverflow using software entity tags.", "labels": [], "entities": [{"text": "QUASAR-S", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7532816529273987}]}, {"text": "As our model is not designed for fill-in-the-gaps questions, we do not use QUASAR-S.", "labels": [], "entities": [{"text": "QUASAR-S", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.7832121849060059}]}, {"text": "QUASAR-T, which we use, consists of 43,013 open-domain questions based on trivia, collected from various internet sources.", "labels": [], "entities": [{"text": "QUASAR-T", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8885458707809448}]}, {"text": "The candidate passages in this dataset are collected from a Lucene based search engine built upon ClueWeb09.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Recall of ranker on QUASAR-T test dataset.  The recall is calculated by checking whether the ground  truth answer appears in top-N paragraphs. IR is the  search engine ranking given in QUASAR-T dataset.", "labels": [], "entities": [{"text": "QUASAR-T test dataset", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.9681207537651062}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9994109869003296}, {"text": "IR", "start_pos": 153, "end_pos": 155, "type": "METRIC", "confidence": 0.9166278839111328}, {"text": "QUASAR-T dataset", "start_pos": 195, "end_pos": 211, "type": "DATASET", "confidence": 0.986822634935379}]}, {"text": " Table 3: Exact Match(EM) and F-1 scores of different  models on QUASAR-T test dataset. Our InferSent +  DrQA model is as competitive as SR 2 which is a su- pervised variant of the state-of-the-art model, R 3", "labels": [], "entities": [{"text": "Exact Match(EM)", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9485667943954468}, {"text": "F-1", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9690253138542175}, {"text": "QUASAR-T test dataset", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.9631164073944092}]}]}