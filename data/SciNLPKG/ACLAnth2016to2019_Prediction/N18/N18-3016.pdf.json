{"title": [{"text": "Benchmarks and models for entity-oriented polarity detection", "labels": [], "entities": []}], "abstractContent": [{"text": "We address the problem of determining entity-oriented polarity in business news.", "labels": [], "entities": []}, {"text": "This can be viewed as classifying the polarity of the sentiment expressed toward a given mention of a company in a news article.", "labels": [], "entities": []}, {"text": "We present a complete , end-to-end approach to the problem.", "labels": [], "entities": []}, {"text": "We introduce anew dataset of over 17,000 manually labeled documents, which is substantially larger than any currently available resources.", "labels": [], "entities": []}, {"text": "We propose a benchmark solution based on convolutional neural networks for classifying entity-oriented polarity.", "labels": [], "entities": [{"text": "classifying entity-oriented polarity", "start_pos": 75, "end_pos": 111, "type": "TASK", "confidence": 0.7894492546717325}]}, {"text": "Although our dataset is much larger than those currently available, it is small on the scale of datasets commonly used for training robust neural network models.", "labels": [], "entities": []}, {"text": "To compensate for this, we use transfer learning-pre-train the model on a much larger dataset, annotated fora related but different classification task, in order to learn a good representation for business text, and then fine-tune it on the smaller polarity dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "We report on research done in the context of PULS-a project for monitoring business news media ().", "labels": [], "entities": []}, {"text": "The system gathers 8,000-10,000 documents daily; each document is processed by a cascade of classifiers, including a named entity (NE) recognizer.", "labels": [], "entities": []}, {"text": "A key NE in business news type is company or organization, which can be mentioned in a positive or negative context.", "labels": [], "entities": []}, {"text": "For example, launching anew product or signing anew contract is viewed as a positive event; involvement in a product recall, bankruptcy or fraud is considered negative.", "labels": [], "entities": []}, {"text": "We focus on determining the polarity of a mention of a given company in news media.", "labels": [], "entities": []}, {"text": "Polarity classification is important, since if a company appears in negative contexts frequently, it may affect its reputation, impact its stock price, etc.", "labels": [], "entities": [{"text": "Polarity classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7227686494588852}]}, {"text": "Polarity prediction, as defined here, is similar to sentiment analysis (: both require the system to classify a span of text as positive or negative.", "labels": [], "entities": [{"text": "Polarity prediction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.674252912402153}, {"text": "sentiment analysis", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9462339282035828}]}, {"text": "However, there are crucial differences.", "labels": [], "entities": []}, {"text": "Business news articles typically do not aim to express emotion or subjectivity-positive and negative events are usually described in a neutral tone.", "labels": [], "entities": []}, {"text": "Thus, vocabularies of affective terms-e.g., amazing or terrific-commonly used in sentiment analysis, are not helpful for business polarity.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.9539120495319366}]}, {"text": "Analysis should rather focus on affective events, i.e., stereotypically positive or negative events.", "labels": [], "entities": []}, {"text": "Further, business news employs genre-specific word usage; words seen as negative in \"generic\" contexts, may indicate a positive context here, and vice versa.Negative terms in (), e.g., include \"cancer\", which in business often appears in positive contexts, as when a pharmaceuticals unveil novel treatments.", "labels": [], "entities": []}, {"text": "While most work in sentiment analysis is done at the document level, we aim to classify entity mentions in text.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.9419644176959991}]}, {"text": "This requires changes to documentbased classification models.", "labels": [], "entities": [{"text": "documentbased classification", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.6370645761489868}]}, {"text": "We explore two convolutional neural network (CNN) architectures, initially proposed for document-level classification, and adapt them for entity-oriented classification.", "labels": [], "entities": [{"text": "document-level classification", "start_pos": 88, "end_pos": 117, "type": "TASK", "confidence": 0.7301059067249298}, {"text": "entity-oriented classification", "start_pos": 138, "end_pos": 168, "type": "TASK", "confidence": 0.7114511132240295}]}, {"text": "The modified models have an additional input channel: the focus-the position(s) in text where a target company is mentioned.", "labels": [], "entities": []}, {"text": "Focus helps the model distinguish among different companies mentioned in text and assign them polarity independently.", "labels": [], "entities": []}, {"text": "As far as we aware no suitable datasets exist for training models for entity-oriented polarity classification.", "labels": [], "entities": [{"text": "entity-oriented polarity classification", "start_pos": 70, "end_pos": 109, "type": "TASK", "confidence": 0.7069769501686096}]}, {"text": "We annotated a dataset of over 17,000 business news articles, which we release for public use, to provide a foundation for an eventual standard evaluation.", "labels": [], "entities": []}, {"text": "Despite being much larger than any existing datasets for business polarity detection, it is still small compared to what is typically used when training CNNs for text classification.", "labels": [], "entities": [{"text": "business polarity detection", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.8297325174013773}, {"text": "text classification", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.7837056517601013}]}, {"text": "We attempt to compensate for the small training data by transferring knowledge from a different corpus.", "labels": [], "entities": []}, {"text": "The second corpus is large, but annotated fora different task: each document has a set of event labels; some of these maybe mapped to polarity labels.", "labels": [], "entities": []}, {"text": "We explore two strategies for knowledge transfer: i) manually mapping from event labels to polarity labels, and ii) pre-training CNNs for the event classification task, followed by unsupervised transfer of high-level features from event classification to polarity.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7496419250965118}, {"text": "event classification task", "start_pos": 142, "end_pos": 167, "type": "TASK", "confidence": 0.7997549970944723}]}, {"text": "We demonstrate that unsupervised transfer improves performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present experiments with focus and knowledge transfer variants.", "labels": [], "entities": []}, {"text": "ity between the model output and the annotation.", "labels": [], "entities": []}, {"text": "To compute accuracy as follows.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9992119073867798}]}, {"text": "In annotation we treat polarity detection as a three-way classification task; values inside [\u22120.1, 0.1] are considered neutral; values further from 0.0 are positive or negative.", "labels": [], "entities": [{"text": "polarity detection", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7840814292430878}]}, {"text": "However, for reasons presented below, the models do not do well on identifying neutral instances.", "labels": [], "entities": []}, {"text": "Thus, in the experiments presented here, we evaluate prediction of binary polarity 4 : negative vs. positive or neutral.", "labels": [], "entities": []}, {"text": "Accuracy measures how often a model blunders, and predicts negative polarity rather than positive or neutral, or vice versa.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9885225296020508}]}, {"text": "Cosine similarity 5 is computed by collecting all of the model's polarity probabilities into one vector and one for the manually assigned polarities, and measuring the cosine between the vectors; also, polarities are mapped into the interval.", "labels": [], "entities": [{"text": "Cosine similarity 5", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.6538605888684591}]}, {"text": "This gives a measure of closeness between model prediction and the ground truth, including differences between \"positive\" and \"very positive\" classes.", "labels": [], "entities": [{"text": "model prediction", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7194312661886215}]}, {"text": "As the results show, accuracy and cosine similarity do not produce consistent rankings, because they measure different aspects of performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9995065927505493}, {"text": "cosine similarity", "start_pos": 34, "end_pos": 51, "type": "METRIC", "confidence": 0.7310974597930908}]}, {"text": "From a practical, user-oriented point of view, it maybe more important that a model avoid gross errors, rather than capturing subtle shades of polarity.", "labels": [], "entities": []}, {"text": "In manual annotation we noticed that some distinctions (\"positive\" vs. \"very positive\") is far from clear for human annotators.", "labels": [], "entities": []}, {"text": "Thus, we are interested in the models that yield the best accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9978571534156799}]}, {"text": "In addition, we used a SVM classifier as a baseline.", "labels": [], "entities": []}, {"text": "The baseline does not use any information about the target company.", "labels": [], "entities": []}, {"text": "We use a one-vs-all strategy to obtain three-way classification.", "labels": [], "entities": []}, {"text": "For the baseline we report only the accuracy, since this method does not directly produce probabilities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9996558427810669}]}], "tableCaptions": [{"text": " Table 1: Class distribution in annotated data.", "labels": [], "entities": []}, {"text": " Table 2: Event classification results", "labels": [], "entities": [{"text": "Event classification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7747918367385864}]}, {"text": " Table 3: Experimental results (multiplied by 100, for readability.)", "labels": [], "entities": []}, {"text": " Table 4: Model comparison: CNN with/without focus using transfer. Company in focus is in bold", "labels": [], "entities": []}]}