{"title": [{"text": "Igbo Diacritic Restoration using Embedding Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Igbo is a low-resource language spoken by approximately 30 million people worldwide.", "labels": [], "entities": []}, {"text": "It is the native language of the Igbo people of southeastern Nigeria.", "labels": [], "entities": []}, {"text": "In Igbo language, diacritics-orthographic and tonal-play a huge role in the distinction of the meaning and pronunciation of words.", "labels": [], "entities": []}, {"text": "Omitting dia-critics in texts often leads to lexical ambiguity.", "labels": [], "entities": []}, {"text": "Diacritic restoration is a pre-processing task that replaces missing diacritics on words from which they have been removed.", "labels": [], "entities": [{"text": "Diacritic restoration", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8149110674858093}]}, {"text": "In this work, we applied embedding models to the diacritic restoration task and compared their performances to those of n-gram models.", "labels": [], "entities": [{"text": "diacritic restoration", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.8374680280685425}]}, {"text": "Although word embedding models have been successfully applied to various NLP tasks, it has not been used, to our knowledge, for di-acritic restoration.", "labels": [], "entities": [{"text": "di-acritic restoration", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.7148567736148834}]}, {"text": "Two classes of word em-beddings models were used: those projected from the English embedding space; and those trained with Igbo bible corpus (\u2248 1m).", "labels": [], "entities": [{"text": "Igbo bible corpus", "start_pos": 123, "end_pos": 140, "type": "DATASET", "confidence": 0.8950692415237427}]}, {"text": "Our best result, 82.49%, is an improvement on the baseline n-gram models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical disambiguation is at the heart of a variety of NLP tasks and systems, ranging from grammar and spelling checkers to machine translation systems.", "labels": [], "entities": [{"text": "Lexical disambiguation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8592813014984131}, {"text": "grammar and spelling checkers", "start_pos": 91, "end_pos": 120, "type": "TASK", "confidence": 0.6204314604401588}, {"text": "machine translation", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7400639653205872}]}, {"text": "In Igbo language, diacritics -orthographic and tonal -play a huge role in the distinction of the meaning and pronunciation of words (.", "labels": [], "entities": []}, {"text": "Therefore, effective restoration of diacritics not only improves the quality of corpora for training NLP systems but often improves the performance of existing ones).", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the English-Igbo parallel bible corpora, available from the Jehova Witness website 3 , for jw.org our experiments.", "labels": [], "entities": [{"text": "Jehova Witness website 3", "start_pos": 68, "end_pos": 92, "type": "DATASET", "confidence": 0.9078999310731888}]}, {"text": "The basic statistics are presented in shows that both the total corpus words and its word types constitute over 50% diacritic words i.e. words with at least one diacritic character.", "labels": [], "entities": []}, {"text": "Over 97% of the ambiguous wordkeys have 2 or 3 variants.", "labels": [], "entities": []}, {"text": "We chose 29 wordkeys which have several variants occurring in our corpus, the wordkey itself occurring too . For each wordkey, we keep a list of sentences (excluding punctuations and numbers), each with a blank (see) to be filled with the correct variant of the wordkey.", "labels": [], "entities": []}, {"text": "The experimental pipeline, as illustrated in, follows three fundamental stages:  A major subtask of this project is building the dataset for training the embedding and other language models.", "labels": [], "entities": []}, {"text": "For all of the 29 wordkeys 9 used in the project, we extracted 38,911 instances each with the correct variant and no diacritics on all words in context.", "labels": [], "entities": []}, {"text": "The dataset was used to optimise the parameters in the training of the Basic embedding model.", "labels": [], "entities": []}, {"text": "Simple unigram and bigram methods were were used as the baseline for the restoration task.", "labels": [], "entities": []}, {"text": "10-fold cross-validation was applied in the evaluation of each of the models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Igbo and English models: vocabulary, vector  and training data sizes", "labels": [], "entities": []}, {"text": " Table 6: Accuracy Scores for the Baselines, Trained and Projected embedding models [Bolds indicate best tweak- ing method].", "labels": [], "entities": [{"text": "Accuracy Scores", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.977260559797287}]}]}