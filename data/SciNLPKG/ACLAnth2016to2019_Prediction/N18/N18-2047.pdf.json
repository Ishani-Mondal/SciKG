{"title": [{"text": "Strong Baselines for Simple Question Answering over Knowledge Graphs with and without Neural Networks", "labels": [], "entities": [{"text": "Simple Question Answering", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.5976166625817617}]}], "abstractContent": [{"text": "We examine the problem of question answering over knowledge graphs, focusing on simple questions that can be answered by the lookup of a single fact.", "labels": [], "entities": [{"text": "question answering", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8130548000335693}]}, {"text": "Adopting a straightforward decomposition of the problem into entity detection, entity linking, relation prediction , and evidence combination, we explore simple yet strong baselines.", "labels": [], "entities": [{"text": "entity detection", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7672737836837769}, {"text": "entity linking", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.7622328698635101}, {"text": "relation prediction", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7977949678897858}]}, {"text": "On the popular SIMPLEQUESTIONS dataset, we find that basic LSTMs and GRUs plus a few heuristics yield accuracies that approach the state of the art, and techniques that do not use neural networks also perform reasonably well.", "labels": [], "entities": [{"text": "SIMPLEQUESTIONS dataset", "start_pos": 15, "end_pos": 38, "type": "DATASET", "confidence": 0.7168158888816833}]}, {"text": "These results show that gains from sophisticated deep learning techniques proposed in the literature are quite modest and that some previous models exhibit unnecessary complexity.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been significant recent interest in simple question answering over knowledge graphs, where a natural language question such as \"Where was Sasha Vujacic born?\" can be answered via the lookup of a simple fact-in this case, the \"place of birth\" property of the entity \"Sasha Vujacic\".", "labels": [], "entities": [{"text": "simple question answering", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.7068170110384623}]}, {"text": "Analysis of an existing benchmark dataset and real-world user questions ( show that such questions cover abroad range of users' needs.", "labels": [], "entities": []}, {"text": "Most recent work on the simple QA task involves increasingly complex neural network (NN) architectures that yield progressively smaller gains over the previous state of the art (see \u00a72 for more details).", "labels": [], "entities": [{"text": "QA task", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.892964094877243}]}, {"text": "Lost in this push, we argue, is an understanding of what exactly contributes to the effectiveness of a particular NN architecture.", "labels": [], "entities": []}, {"text": "In many cases, the lack of rigorous ablation studies further compounds difficulties in interpreting results and credit assignment.", "labels": [], "entities": []}, {"text": "To give two related examples: reported that standard LSTM architectures, when properly tuned, outperform some more recent models; showed that the dominant approach to sequence transduction using complex encoder-decoder networks with attention mechanisms work just as well with the attention module only, yielding networks that are far simpler and easier to train.", "labels": [], "entities": []}, {"text": "In line with an emerging thread of research that aims to improve empirical rigor in our field by focusing on knowledge and insights, as opposed to simply \"winning\", we take the approach of peeling away unnecessary complexity until we arrive at the simplest model that works well.", "labels": [], "entities": []}, {"text": "On the SIMPLEQUESTIONS dataset, we find that baseline NN architectures plus simple heuristics yield accuracies that approach the state of the art.", "labels": [], "entities": [{"text": "SIMPLEQUESTIONS dataset", "start_pos": 7, "end_pos": 30, "type": "DATASET", "confidence": 0.7867736220359802}]}, {"text": "Furthermore, we show that a combination of simple techniques that do not involve neural networks can still achieve reasonable accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9976027607917786}]}, {"text": "These results suggest that while NNs do indeed contribute to meaningful advances on this task, some models exhibit unnecessary complexity and that the best models yield at most modest gains over strong baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted evaluations on the SIMPLEQUES-TIONS dataset, comprised of 75.9k/10.8k/21.7k training/validation/test questions.", "labels": [], "entities": [{"text": "SIMPLEQUES-TIONS dataset", "start_pos": 32, "end_pos": 56, "type": "DATASET", "confidence": 0.7390384376049042}]}, {"text": "Each question is associated with a (subject, predicate, object) triple from a Freebase subset that answers the question.", "labels": [], "entities": []}, {"text": "The subject is given as an MID, but the dataset does not identify the entity in the question, which is needed for our formulation of entity detection.", "labels": [], "entities": [{"text": "MID", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9229100346565247}, {"text": "entity detection", "start_pos": 133, "end_pos": 149, "type": "TASK", "confidence": 0.7116594165563583}]}, {"text": "For this, we used the names file by to backproject the entity names onto the questions to annotate each token as either ENTITY or NOTENTITY.", "labels": [], "entities": [{"text": "ENTITY", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9902715086936951}, {"text": "NOTENTITY", "start_pos": 130, "end_pos": 139, "type": "DATASET", "confidence": 0.554694652557373}]}, {"text": "This introduces some noise, as in some cases there are no exact matches-for these, we back off to fuzzy matching and project the entity onto the n-gram sequence with the smallest Levenshtein Distance to the entity name.", "labels": [], "entities": []}, {"text": "As with previous work, we report results over the 2M-subset of Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.6015944480895996}]}, {"text": "For entity detection, we evaluate by extracting every sequence of contiguous ENTITY tags and compute precision, recall, and F1 against the ground truth.", "labels": [], "entities": [{"text": "entity detection", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8556368947029114}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.999487042427063}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9992578625679016}, {"text": "F1", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.9997958540916443}]}, {"text": "For both entity linking and relation prediction, we evaluate recall at N (R@N ), i.e., whether the correct answer appears in the top N results.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7530807256698608}, {"text": "relation prediction", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8417549729347229}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9986594915390015}]}, {"text": "For end-to-end evaluation, we follow the approach of and mark a prediction as correct if both the entity and the relation exactly match the ground truth.", "labels": [], "entities": []}, {"text": "The main metric is accuracy, which is equivalent to R@1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9997153878211975}, {"text": "R", "start_pos": 52, "end_pos": 53, "type": "METRIC", "confidence": 0.9906544089317322}]}, {"text": "Our models were implemented in PyTorch v0.2.0 with CUDA 8.0 running on an NVIDIA GeForce GTX 1080 GPU.", "labels": [], "entities": []}, {"text": "GloVe embeddings () of size 300 served as the input to our models.", "labels": [], "entities": []}, {"text": "We used negative log likelihood loss to optimize model parameters using Adam, with an initial learning rate of 0.0001.", "labels": [], "entities": []}, {"text": "We performed random search over hyperparameters, exploring a range that is typical of NNs for NLP applications; the hyperparameters were selected based on the development set.", "labels": [], "entities": []}, {"text": "In our final model, all LSTM and GRU hidden states sizes and MLP hidden sizes were set to 300.", "labels": [], "entities": []}, {"text": "For the CNNs, we used a size 300 output channel.", "labels": [], "entities": [{"text": "CNNs", "start_pos": 8, "end_pos": 12, "type": "TASK", "confidence": 0.7775692939758301}]}, {"text": "Dropout rate for the CNNs was 0.5 and 0.3 for the RNNs.", "labels": [], "entities": [{"text": "Dropout", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9633569717407227}, {"text": "RNNs", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9294770956039429}]}, {"text": "For the CRF implementation, we used the Stanford NER tagger (Finkel et al.,", "labels": [], "entities": [{"text": "CRF", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9263139367103577}, {"text": "Stanford NER tagger", "start_pos": 40, "end_pos": 59, "type": "DATASET", "confidence": 0.7879766821861267}]}], "tableCaptions": [{"text": " Table 1: Results for entity linking on the validation set,  given the underlying entity detection model.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7107551395893097}]}, {"text": " Table 2: Results for relation prediction on the valida- tion set using different models.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8982131779193878}]}, {"text": " Table 3: End-to-end answer accuracy on the test set  with different model combinations, compared to a se- lection of previous results reported in the literature.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9163495898246765}]}]}