{"title": [{"text": "Can Network Embedding of Distributional Thesaurus be Combined with Word Vectors for Better Representation?", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributed representations of words learned from text have proved to be successful in various natural language processing tasks in recent times.", "labels": [], "entities": [{"text": "Distributed representations of words learned from text", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7836903248514447}]}, {"text": "While some methods represent words as vectors computed from text using predictive model (Word2vec) or dense count based model (GloVe), others attempt to represent these in a distributional thesaurus network structure where the neighborhood of a word is a set of words having adequate context overlap.", "labels": [], "entities": []}, {"text": "Being motivated by recent surge of research in network embedding techniques (DeepWalk, LINE, node2vec etc.), we turn a distributional thesaurus network into dense word vectors and investigate the usefulness of distributional thesaurus embedding in improving overall word representation.", "labels": [], "entities": [{"text": "word representation", "start_pos": 266, "end_pos": 285, "type": "TASK", "confidence": 0.715034544467926}]}, {"text": "This is the first attempt where we show that combining the proposed word representation obtained by distributional thesaurus embedding with the state-of-the-art word representations helps in improving the performance by a significant margin when evaluated against NLP tasks like word similarity and relatedness, synonym detection , analogy detection.", "labels": [], "entities": [{"text": "synonym detection", "start_pos": 312, "end_pos": 329, "type": "TASK", "confidence": 0.9435349106788635}, {"text": "analogy detection", "start_pos": 332, "end_pos": 349, "type": "TASK", "confidence": 0.9046126902103424}]}, {"text": "Additionally, we show that even without using any handcrafted lexical resources we can come up with representations having comparable performance in the word similarity and relatedness tasks compared to the representations where a lexical resource has been used.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language understanding has always been a primary challenge in natural language processing (NLP) domain.", "labels": [], "entities": [{"text": "Natural language understanding", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7217690944671631}]}, {"text": "Learning word representations is one of the basic and primary steps in understanding text and nowadays there are predominantly two views of learning word representations.", "labels": [], "entities": [{"text": "Learning word representations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6761293808619181}, {"text": "learning word representations", "start_pos": 140, "end_pos": 169, "type": "TASK", "confidence": 0.6564155419667562}]}, {"text": "In one realm of representation, words are vectors of distributions obtained from analyzing their contexts in the text and two words are considered meaningfully similar if the vectors of those words are close in the euclidean space.", "labels": [], "entities": []}, {"text": "In recent times, attempts have been made for dense representation of words, be it using predictive model like or count-based model like GloVe () which are computationally efficient as well.", "labels": [], "entities": []}, {"text": "Another stream of representation talks about network like structure where two words are considered neighbors if they both occur in the same context above a certain number of times.", "labels": [], "entities": []}, {"text": "The words are finally represented using these neighbors.", "labels": [], "entities": []}, {"text": "Distributional Thesaurus is one such instance of this type, which gets automatically produced from a text corpus and identifies words that occur in similar contexts; the notion of which was used in early work about distributional semantics).", "labels": [], "entities": []}, {"text": "One such representation is JoBimText proposed by that contains, for each word, a list of words that are similar with respect to their bigram distribution, thus producing a network representation.", "labels": [], "entities": []}, {"text": "Later, introduced a highly scalable approach for computing this network.", "labels": [], "entities": []}, {"text": "We mention this representation as a DT network throughout this article.", "labels": [], "entities": []}, {"text": "With the emergence of recent trend of embedding large networks into dense low-dimensional vector space efficiently () which are focused on capturing different properties of the network like neighborhood structure, community structure, etc., we explore representing DT network in a dense vector space and evaluate its useful application in various NLP tasks.", "labels": [], "entities": []}, {"text": "There has been attempt to turn distributional thesauri into word vectors for synonym extraction and expansion but the full utilization of DT embedding has not yet been explored.", "labels": [], "entities": [{"text": "synonym extraction", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.9374872446060181}]}, {"text": "In this paper, as a main contribution, we investigate the best way of turning a Distributional Thesaurus (DT) network into word embeddings by applying efficient network embedding methods and analyze how these embeddings generated from DT network can improve the representations generated from prediction-based model like Word2vec or dense count based semantic model like GloVe.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 321, "end_pos": 329, "type": "DATASET", "confidence": 0.9600803256034851}]}, {"text": "We experiment with several combination techniques and find that DT network embedding can be combined with Word2vec and GloVe to outperform the performances when used independently.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9740073084831238}]}, {"text": "Further, we show that we can use DT network embedding as a proxy of WordNet embedding in order to improve the already existing state-of-the-art word representations as both of them achieve comparable performance as far as word similarity and word relatedness tasks are concerned.", "labels": [], "entities": []}, {"text": "Considering the fact that the vocabulary size of WordNet is small and preparing WordNet like lexical resources needs huge human engagement, it would be useful to have a representation which can be generated automatically from corpus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9537238478660583}]}, {"text": "We also attempt to combine both WordNet and DT embeddings to improve the existing word representations and find that DT embedding still has some extra information to bring in leading to better performance when compared to combination of only WordNet embedding and state-of-theart word embeddings.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.937086820602417}]}, {"text": "While most of our experiments are focused on word similarity and relatedness tasks, we show the usefulness of DT embeddings on synonym detection and analogy detection as well.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.7062232792377472}, {"text": "synonym detection", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.9239622950553894}, {"text": "analogy detection", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.8975001871585846}]}, {"text": "In both the tasks, combined representation of GloVe and DT embeddings shows promising performance gain over state-of-the-art embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the quality of the word representations, we first conduct qualitative analysis of the joint representation.", "labels": [], "entities": []}, {"text": "Next, we follow the most acceptable way of applying on different NLP tasks like word similarity and word relatedness, synonym detection and word analogy as described next.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.7165976911783218}, {"text": "synonym detection", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.9276954531669617}, {"text": "word analogy", "start_pos": 140, "end_pos": 152, "type": "TASK", "confidence": 0.7848964333534241}]}], "tableCaptions": [{"text": " Table 1: Comparison of individual performances of different vector representation models w.r.t. word similarity  and relatedness tasks. The performance metric is Spearman's rank correlation coefficient (\u03c1). Best result of each  row in bold showing the best vector representation for each dataset.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient (\u03c1)", "start_pos": 163, "end_pos": 206, "type": "METRIC", "confidence": 0.7646679803729057}]}, {"text": " Table 2: Comparison of performances (Spearman's \u03c1) of GloVe against the combined representation of word rep- resentations obtained from DT network using network embeddings (DeepWalk, node2vec) with GloVe. Two com- bination methods -concatenation (CC) and PCA -are used among which PCA performs better than concatenation  (CC) in most of the cases. Also the results show that the combined representation leads to better performance in  almost all the cases.", "labels": [], "entities": []}, {"text": " Table 3: A similar experiment as Table 2 with Word2vec (W2V) instead of GloVe.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9327071905136108}]}, {"text": " Table 4: Comparison of performances (Spearman's  \u03c1) between GloVe combined with Word2vec (W2V)  against GloVe combined with DT embedding obtained  using node2vec (D2V-N). PCA has been taken as com- bination method. Clearly, DT embedding outperforms  Word2vec in terms of enhancing the performance of  GloVe.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9258056879043579}]}, {"text": " Table 6: Comparison of performances (Spearman's \u03c1)  between GloVe representation and retrofitted (by DT  network) GloVe representation. Clearly, DT retrofitting  is not helping much to improve the performance of  GloVe.", "labels": [], "entities": []}, {"text": " Table 7: Comparison of accuracies between GloVe rep- resentation, DT embedding using node2vec and com- bination of both where PCA is the combination tech- nique. Clearly DT embedding is helping to improve the  performance of GloVe for synonym detection as well as  analogy detection.", "labels": [], "entities": [{"text": "synonym detection", "start_pos": 236, "end_pos": 253, "type": "TASK", "confidence": 0.956312358379364}, {"text": "analogy detection", "start_pos": 266, "end_pos": 283, "type": "TASK", "confidence": 0.9662868082523346}]}]}