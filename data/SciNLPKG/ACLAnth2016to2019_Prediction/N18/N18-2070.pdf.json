{"title": [{"text": "Pragmatically Informative Image Captioning with Character-Level Inference", "labels": [], "entities": [{"text": "Informative Image Captioning", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.6161384781201681}]}], "abstractContent": [{"text": "We combine a neural image captioner with a Rational Speech Acts (RSA) model to make a system that is pragmatically informative: its objective is to produce captions that are not merely true but also distinguish their inputs from similar images.", "labels": [], "entities": []}, {"text": "Previous attempts to combine RSA with neural image captioning require an inference which normalizes over the entire set of possible utterances.", "labels": [], "entities": [{"text": "RSA", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9830465912818909}]}, {"text": "This poses a serious problem of efficiency, previously solved by sampling a small subset of possible utterances.", "labels": [], "entities": [{"text": "efficiency", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9690640568733215}]}, {"text": "We instead solve this problem by implementing aversion of RSA which operates at the level of characters (\"a\",\"b\",\"c\",.", "labels": [], "entities": []}, {"text": ".) during the unrolling of the caption.", "labels": [], "entities": []}, {"text": "We find that the utterance-level effect of referential captions can be obtained with only character-level decisions.", "labels": [], "entities": [{"text": "referential captions", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.828289657831192}]}, {"text": "Finally, we introduce an automatic method for testing the performance of pragmatic speaker models, and show that our model outperforms a non-pragmatic baseline as well as a word-level RSA captioner.", "labels": [], "entities": []}], "introductionContent": [{"text": "The success of automatic image captioning demonstrates compellingly that end-to-end statistical models can align visual information with language.", "labels": [], "entities": [{"text": "automatic image captioning", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.6575721303621928}]}, {"text": "However, high-quality captions are not merely true, but also pragmatically informative in the sense that they highlight salient properties and help distinguish their inputs from similar images.", "labels": [], "entities": []}, {"text": "Captioning systems trained on single images struggle to be pragmatic in this sense, producing either very general or hyper-specific descriptions.", "labels": [], "entities": []}, {"text": "In this paper, we present a neural image captioning system 1 that is a pragmatic speaker as defined by the Rational Speech Acts (RSA) model The code is available at https://github.com/ reubenharry/Recurrent-RSA 2013).", "labels": [], "entities": [{"text": "Rational Speech Acts (RSA)", "start_pos": 107, "end_pos": 133, "type": "TASK", "confidence": 0.6863162815570831}]}, {"text": "Given a set of images, of which one is the target, its objective is to generate a natural language expression which identifies the target in this context.", "labels": [], "entities": []}, {"text": "For instance, the literal caption in Figure 1 could describe both the target and the top two distractors, whereas the pragmatic caption mentions something that is most salient of the target.", "labels": [], "entities": []}, {"text": "Intuitively, the RSA speaker achieves this by reasoning not only about what is true but also about what it's like to be a listener in this context trying to identify the target.", "labels": [], "entities": [{"text": "RSA speaker", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.908603847026825}]}, {"text": "This core idea underlies much work in referring expression generation) and image captioning (, but these models do not fully confront the fact that the agents must reason about all possible utterances, which is intractable.", "labels": [], "entities": [{"text": "referring expression generation)", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.7880368232727051}, {"text": "image captioning", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7480841279029846}]}, {"text": "We fully address this problem by implementing RSA at the level of characters rather than the level of utterances or words: the neural language model emits individual characters, choosing them to balance pragmatic informativeness with overall well-formedness.", "labels": [], "entities": [{"text": "RSA", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9620566964149475}]}, {"text": "Thus, the agents reason not about full utterances, but rather only about all possible character choices, a very small space.", "labels": [], "entities": []}, {"text": "The result is that the information encoded recurrently in the neural model allows us to obtain global pragmatic effects from local decisions.", "labels": [], "entities": []}, {"text": "We show that such character-level RSA speakers are more effective than literal captioning systems at the task of helping a reader identify the target image among close competitors, and outperform word-level RSA captioners in both efficiency and accuracy.", "labels": [], "entities": [{"text": "RSA speakers", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.868547260761261}, {"text": "literal captioning", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7088285535573959}, {"text": "RSA captioners", "start_pos": 207, "end_pos": 221, "type": "TASK", "confidence": 0.7628188729286194}, {"text": "accuracy", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.9978952407836914}]}], "datasetContent": [{"text": "Qualitatively, show how the S 1 captions are more informative than the S 0 , as a result of pragmatic considerations.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of our method quantitatively, we implement an automatic evaluation.", "labels": [], "entities": []}, {"text": "To evaluate the success of S 1 as compared to S 0 , we define a listener L eval (image|caption) / PS 0 (caption|image), where PS 0 (caption|image) is the total probability of S 0 incrementally generating caption given image.", "labels": [], "entities": []}, {"text": "In other words, L eval uses Bayes' rule to obtain from S 0 the posterior probability of each image w given a full caption u.", "labels": [], "entities": []}, {"text": "The neural S 0 used in the definition of L eval must be trained on separate data to the neural S 0 used for the S 1 model which produces captions, since otherwise this S 1 production model effectively has access to the system evaluating it.", "labels": [], "entities": []}, {"text": "As note, \"a model might 'com-municate' better with itself using its own language than with others\".", "labels": [], "entities": []}, {"text": "In evaluation, we therefore split the training data in half, with one part for training the S 0 used in the caption generation model S 1 and one part for training the S 0 used in the caption evaluation model L eval . We say that the caption succeeds as a referring expression if the target has more probability mass under the distribution L eval (image|caption) than any distractor.", "labels": [], "entities": []}, {"text": "Dataset We train our production and evaluation models on separate sets consisting of regions in the Visual Genome dataset ( and full images in MSCOCO (.", "labels": [], "entities": [{"text": "Visual Genome dataset", "start_pos": 100, "end_pos": 121, "type": "DATASET", "confidence": 0.7323731084664663}, {"text": "MSCOCO", "start_pos": 143, "end_pos": 149, "type": "DATASET", "confidence": 0.9039922952651978}]}, {"text": "Both datasets consist of over 100,000 images of common objects and scenes.", "labels": [], "entities": []}, {"text": "MSCOCO provides captions for whole images, while Visual Genome provides captions for regions within images.", "labels": [], "entities": [{"text": "MSCOCO", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9164344072341919}]}, {"text": "Our test sets consist of clusters of 10 images.", "labels": [], "entities": []}, {"text": "For a given cluster, we set each image in it as the target, in turn.", "labels": [], "entities": []}, {"text": "We use two test sets.", "labels": [], "entities": []}, {"text": "Test set 1 (TS1) consists of 100 clusters of images, 10 for each of the 10 most common objects in Visual Genome.", "labels": [], "entities": []}, {"text": "Test set 2 (TS2) consists of regions in Visual Genome images whose ground truth captions have high word overlap, an indicator that they are similar.", "labels": [], "entities": []}, {"text": "We again select 100 clusters of 10.", "labels": [], "entities": []}, {"text": "Both test sets have 1,000 items in total (10 potential target images for each of 100 clusters).", "labels": [], "entities": []}, {"text": "Captioning System Our neural image captioning system is a CNN-RNN architecture 4 adapted to use a character-based LSTM for the language model.", "labels": [], "entities": [{"text": "neural image captioning", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.786672830581665}]}, {"text": "Hyperparameters We use abeam search with width 10 to produce captions, and a rationality parameter of \u21b5 = 5.0 for the S 1 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy on both test sets.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992727637290955}]}]}