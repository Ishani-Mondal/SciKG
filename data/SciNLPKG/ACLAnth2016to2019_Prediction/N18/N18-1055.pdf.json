{"title": [{"text": "Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task", "labels": [], "entities": [{"text": "Approaching Neural Grammatical Error Correction", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8028639376163482}, {"text": "Low-Resource Machine Translation", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.7485405802726746}]}], "abstractContent": [{"text": "Previously, neural methods in grammatical error correction (GEC) did not reach state-of-the-art results compared to phrase-based statistical machine translation (SMT) baselines.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.7839493006467819}, {"text": "phrase-based statistical machine translation (SMT)", "start_pos": 116, "end_pos": 166, "type": "TASK", "confidence": 0.7340253676686969}]}, {"text": "We demonstrate parallels between neural GEC and low-resource neural MT and successfully adapt several methods from low-resource MT to neural GEC.", "labels": [], "entities": []}, {"text": "We further establish guidelines for trustable results in neural GEC and propose a set of model-independent methods for neural GEC that can be easily applied inmost GEC settings.", "labels": [], "entities": []}, {"text": "Proposed methods include adding source-side noise, domain-adaptation techniques, a GEC-specific training-objective, transfer learning with monolingual data, and ensembling of independently trained GEC models and language models.", "labels": [], "entities": []}, {"text": "The combined effects of these methods result in better than state-of-the-art neural GEC models that out-perform previously best neural GEC systems by more than 10% M 2 on the CoNLL-2014 benchmark and 5.9% on the JFLEG test set.", "labels": [], "entities": [{"text": "M", "start_pos": 164, "end_pos": 165, "type": "METRIC", "confidence": 0.9707614779472351}, {"text": "CoNLL-2014 benchmark", "start_pos": 175, "end_pos": 195, "type": "DATASET", "confidence": 0.9171196222305298}, {"text": "JFLEG test set", "start_pos": 212, "end_pos": 226, "type": "DATASET", "confidence": 0.9456512331962585}]}, {"text": "Non-neural state-of-the-art systems are outper-formed by more than 2% on the CoNLL-2014 benchmark and by 4% on JFLEG.", "labels": [], "entities": [{"text": "CoNLL-2014 benchmark", "start_pos": 77, "end_pos": 97, "type": "DATASET", "confidence": 0.9070298373699188}, {"text": "JFLEG", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.9425124526023865}]}], "introductionContent": [{"text": "Most successful approaches to automated grammatical error correction (GEC) are based on methods from statistical machine translation (SMT), especially the phrase-based variant.", "labels": [], "entities": [{"text": "automated grammatical error correction (GEC)", "start_pos": 30, "end_pos": 74, "type": "TASK", "confidence": 0.8037866013390678}, {"text": "statistical machine translation (SMT)", "start_pos": 101, "end_pos": 138, "type": "TASK", "confidence": 0.796065111955007}]}, {"text": "For the CoNLL 2014 benchmark on grammatical error correction ( ), Junczys-Dowmunt and Grundkiewicz (2016) established a set of methods for GEC by SMT that remain state-of-the-art.", "labels": [], "entities": [{"text": "CoNLL 2014 benchmark", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.8523167570432028}, {"text": "grammatical error correction", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.6197171111901602}, {"text": "GEC", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.9856406450271606}, {"text": "SMT", "start_pos": 146, "end_pos": 149, "type": "TASK", "confidence": 0.6822184324264526}]}, {"text": "Systems) that improve on results by use their set-up as a backbone for more complex systems.", "labels": [], "entities": []}, {"text": "The view that GEC can be approached as a machine translation problem by translating from erroneous to correct text originates from and resulted in many systems (e.g.) that represented the current state-of-the-art at the time.", "labels": [], "entities": [{"text": "GEC", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.8957961797714233}, {"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7040331661701202}]}, {"text": "In the field of machine translation proper, the emergence of neural sequence-to-sequence methods and their impressive results have lead to a paradigm shift away from phrase-based SMT towards neural machine translation (NMT).) authors of pure phrase-based systems offered \"unconditional surrender\" 1 to NMT-based methods.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7701799869537354}, {"text": "phrase-based SMT", "start_pos": 166, "end_pos": 182, "type": "TASK", "confidence": 0.5214094072580338}]}, {"text": "Based on these developments, one would expect to see arise of state-of-the-art neural methods for GEC, but as Junczys-Dowmunt and Grundkiewicz (2016) already noted, this is not the case.", "labels": [], "entities": [{"text": "GEC", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.8934798240661621}]}, {"text": "Interestingly, even today, the top systems on established GEC benchmarks are still mostly phrase-based or hybrid systems).", "labels": [], "entities": [{"text": "GEC benchmarks", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.8237473666667938}]}, {"text": "The best \"pure\" neural systems () are several percent behind.", "labels": [], "entities": []}, {"text": "If we look at recent MT work with this in mind, we find one area where phrased-based SMT dominates over NMT: low-resource machine translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.98915034532547}, {"text": "SMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.7600035071372986}, {"text": "low-resource machine translation", "start_pos": 109, "end_pos": 141, "type": "TASK", "confidence": 0.6055265764395396}]}, {"text": "analyze the behavior of NMT versus SMT for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data, illustrated in.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9072265028953552}]}, {"text": "Quality for NMT  starts low for small corpora, outperforms SMT at a corpus size of about 15 million words, and with increasing size beats SMT with a large in-domain language model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9754598140716553}, {"text": "SMT", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.9341744184494019}]}, {"text": "lists existing training resources for the English as-a-second-language (ESL) grammatical error correction task.", "labels": [], "entities": [{"text": "English as-a-second-language (ESL) grammatical error correction task", "start_pos": 42, "end_pos": 110, "type": "TASK", "confidence": 0.7397701409127977}]}, {"text": "Publicly available resources, NUS Corpus of Learner English (NUCLE) by, Lang-8 NAIST () and CLC FCE) amount to about 27M tokens.", "labels": [], "entities": [{"text": "NUS Corpus of Learner English (NUCLE)", "start_pos": 30, "end_pos": 67, "type": "DATASET", "confidence": 0.9435361176729202}, {"text": "Lang-8 NAIST", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.7887352108955383}, {"text": "CLC FCE", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.8186356425285339}]}, {"text": "Among these the Lang-8 corpus is quite noisy and of low quality.", "labels": [], "entities": [{"text": "Lang-8 corpus", "start_pos": 16, "end_pos": 29, "type": "DATASET", "confidence": 0.9711236655712128}]}, {"text": "The Cambridge Learner Corpus (CLC) by -probably the best resource in this listis non-public and we would strongly discourage reporting results that include it as training data as this makes comparisons difficult.", "labels": [], "entities": [{"text": "Cambridge Learner Corpus (CLC)", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.9397842486699423}]}, {"text": "Contrasting this with, we see that for about 20M tokens NMT systems start outperforming SMT models without additional large language models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9811044931411743}]}, {"text": "Current state-of-the-art GEC systems based on SMT, however, all include large-scale indomain language models either following the steps outlined in Junczys-Dowmunt and or directly re-using their domain-adapted Common-Crawl language model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9652336239814758}]}, {"text": "It seems that the current state of neural methods in GEC reflects the behavior for NMT systems trained on smaller data sets.", "labels": [], "entities": []}, {"text": "Based on this, we conclude that we can think of GEC as a lowresource, or at most mid-resource, machine translation problem.", "labels": [], "entities": [{"text": "GEC", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.796409547328949}, {"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.751556009054184}]}, {"text": "This means that techniques proposed for low-resource (neural) MT should be applicable to improving neural GEC results.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.8244901895523071}, {"text": "GEC", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.8917741179466248}]}, {"text": "In this work we show that adapting techniques from low-resource (neural) MT and SMT-based GEC methods allows neural GEC systems to catch up to and outperform SMT-based systems.", "labels": [], "entities": [{"text": "SMT-based GEC", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.800175279378891}]}, {"text": "We improve over the previously best-reported neural GEC system () on the CoNLL 2014 test set by more than 10% M 2 , over a comparable pure SMT system by Junczys-Dowmunt and Grundkiewicz (2016) by 6%, and outperform the state-of-the-art result of by 2%.", "labels": [], "entities": [{"text": "CoNLL 2014 test set", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.9631369858980179}, {"text": "M", "start_pos": 110, "end_pos": 111, "type": "METRIC", "confidence": 0.9946252703666687}, {"text": "SMT", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.9660838842391968}]}, {"text": "On the JFLEG data set, we report the currently best results, outperforming the previously best pure neural system ( ) by 5.9% GLEU and the best reported results) by 3% GLEU.", "labels": [], "entities": [{"text": "JFLEG data set", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.9602003296216329}, {"text": "GLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9953923225402832}, {"text": "GLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9904910326004028}]}, {"text": "In Section 2, we describe our NMT-based baseline for GEC, and follow recommendations from the MT community fora trustable neural GEC system.", "labels": [], "entities": [{"text": "GEC", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9741455316543579}]}, {"text": "In Section 3, we adapt neural models to make better use of sparse error-annotated data, transferring low-resource MT and GEC-specific SMT methods to neural GEC.", "labels": [], "entities": [{"text": "MT and GEC-specific SMT", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.5104945972561836}]}, {"text": "This includes a novel training objective for GEC.", "labels": [], "entities": [{"text": "GEC", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.7751022577285767}]}, {"text": "We investigate how to leverage monolingual data for neural GEC by transfer learning in Section 4 and experiment with language model ensembling in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 explores deep NMT architectures.", "labels": [], "entities": []}, {"text": "In Section 7, we provide an overview of the experiments and how results relate to the JFLEG benchmark.", "labels": [], "entities": [{"text": "JFLEG benchmark", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.8602341413497925}]}, {"text": "We also recommend a model-independent toolbox for neural GEC.", "labels": [], "entities": [{"text": "neural GEC", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.5139267444610596}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics for test and development data.", "labels": [], "entities": []}, {"text": " Table 3: Instable results for multiple baseline runs ver- sus average and ensemble -for the CoNLL bench- mark.", "labels": [], "entities": [{"text": "Instable", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9808565378189087}, {"text": "CoNLL bench- mark", "start_pos": 93, "end_pos": 110, "type": "DATASET", "confidence": 0.6934166997671127}]}, {"text": " Table 4: Results (M 2 ) on the CoNLL benchmark for  GEC-specific adaptations.", "labels": [], "entities": [{"text": "CoNLL benchmark", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.8149321675300598}, {"text": "GEC-specific adaptations", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.8896644711494446}]}, {"text": " Table 5: Results for model type +Tied-Emb. trained  with edit-weighted MLE and chosen \u039b.", "labels": [], "entities": [{"text": "MLE", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.8096117377281189}]}, {"text": " Table 6: Results (M 2 ) on the CoNLL benchmark set for  GEC-specific adaptations.", "labels": [], "entities": [{"text": "CoNLL benchmark set", "start_pos": 32, "end_pos": 51, "type": "DATASET", "confidence": 0.9053433736165365}, {"text": "GEC-specific adaptations", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.7986656725406647}]}, {"text": " Table 8: Shallow (Pretrain-Dec.) versus deep ensem- bles, with and without corresponding language models.", "labels": [], "entities": []}]}