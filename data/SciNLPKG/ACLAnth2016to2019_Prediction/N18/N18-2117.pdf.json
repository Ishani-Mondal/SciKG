{"title": [{"text": "Role-specific Language Models for Processing Recorded Neuropsychological Exams", "labels": [], "entities": []}], "abstractContent": [{"text": "Neuropsychological examinations are an important screening tool for the presence of cog-nitive conditions (e.g. Alzheimer's, Parkin-son's Disease), and require a trained tester to conduct the exam through spoken interactions with the subject.", "labels": [], "entities": []}, {"text": "While audio is relatively easy to record, it remains a challenge to automatically diarize (who spoke when?), decode (what did they say?), and assess a subject's cognitive health.", "labels": [], "entities": []}, {"text": "This paper demonstrates a method to determine the cognitive health (im-paired or not) of 92 subjects, from audio that was diarized using an automatic speech recognition system trained on TED talks and on the structured language used by testers and subjects.", "labels": [], "entities": []}, {"text": "Using leave-one-out cross validation and logistic regression modeling we show that even with noisily decoded data (81% WER) we can still perform accurate enough diariza-tion (0.02 % confusion rate) to determine the cognitive state of a subject (0.76 AUC).", "labels": [], "entities": [{"text": "WER", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9844542145729065}, {"text": "confusion rate)", "start_pos": 182, "end_pos": 197, "type": "METRIC", "confidence": 0.8888843456904093}, {"text": "AUC", "start_pos": 250, "end_pos": 253, "type": "METRIC", "confidence": 0.9608805179595947}]}], "introductionContent": [{"text": "Cognitive impairment is a decline in mental abilities that is severe enough to interfere with daily life.", "labels": [], "entities": []}, {"text": "Such conditions are particularly debilitating, with costs of up to $200 billion in the USA alone, and come second only to spinal-cord injuries and terminal cancer in the severity of their effects).", "labels": [], "entities": []}, {"text": "Several methods exist to screen for cognitive conditions (e.g. Alzheimer's, Parkinson's), ranging from laboratory measures to brain imaging scans (; Van Himbergen et al., 2012), with the baseline being set by neuropsychological examinations.", "labels": [], "entities": []}, {"text": "These exams are composed of multiple components that measure a specific domain of cognition such as: thinking, recall, speech, and physical movement.", "labels": [], "entities": [{"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9979494214057922}]}, {"text": "Each exam component is assigned a score by the tester according to the established rubric.", "labels": [], "entities": []}, {"text": "While this exam can be comprehensive, there is an additional dimension of information that can be passively recorded -the audio of the spoken interactions.", "labels": [], "entities": []}, {"text": "Utilizing such data would allow for the identification of spoken language biomarkers of cognitive impairment.", "labels": [], "entities": []}, {"text": "However, with richer information comes additional complexity.", "labels": [], "entities": []}, {"text": "The application of automatic speech processing technologies to medical domains requires a pipeline with multiple stages.", "labels": [], "entities": []}, {"text": "Such a system requires audio pre-processing to locate speech and speaker segments (i.e. diarization) (, the transcription of spoken utterances, and feature representation and modeling of the speaker's latent condition to determine disease biomarkers for classification purposes.", "labels": [], "entities": []}, {"text": "Research in this domain can be categorized into two areas.", "labels": [], "entities": []}, {"text": "First is the utilization of acoustic and linguistic information to perform speaker diarization and verification using standard corpora (e.g. Switchboard, NIST) (.", "labels": [], "entities": []}, {"text": "The second category of work seeks to evaluate speech and language biomarkers for the detection of cognitive impairment utilizing measures such as speaking rate, pauses, n-grams, and Word Error Rates (WERs) (, as well as Automatic Speech Recognition (ASR) for phonetic alignment and acoustic feature extraction.", "labels": [], "entities": [{"text": "Word Error Rates (WERs)", "start_pos": 182, "end_pos": 205, "type": "METRIC", "confidence": 0.8503425121307373}, {"text": "Automatic Speech Recognition (ASR", "start_pos": 220, "end_pos": 253, "type": "TASK", "confidence": 0.7153087079524993}, {"text": "phonetic alignment", "start_pos": 259, "end_pos": 277, "type": "TASK", "confidence": 0.717967689037323}, {"text": "acoustic feature extraction", "start_pos": 282, "end_pos": 309, "type": "TASK", "confidence": 0.6852175196011862}]}, {"text": "However, systems from the speech community are developed using well-curated data with healthy speakers, while the clinical community develops systems using manually transcribed data, with some exceptions.", "labels": [], "entities": []}, {"text": "Our paper seeks to bridge the two areas by automating data curation for clinical use.", "labels": [], "entities": []}, {"text": "We hypothesize that it is possible to automate data curation for clinical use by conditioning on speaker roles, because speakers (subject/tester) during neuropsychological exams have different word usage and speaking patterns due to the question and answer nature of the evaluation.", "labels": [], "entities": []}, {"text": "We also hypothesize that not all segments of the exam will be equally valuable in evaluating for cognitive conditions, due to potential confusion between speakers when automatically annotating speaker segments, polluting the features used for modeling cognitive conditions.", "labels": [], "entities": []}, {"text": "Our study differentiates itself from prior work by combining speaker-specific language modeling and ASR for speaker diarization, with the ultimate goal of assessing the cognitive condition of the subjects using the acoustic information contained in the hypothesized (and less than ideal) segments.", "labels": [], "entities": [{"text": "speaker diarization", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7035459280014038}]}, {"text": "This is an extension of work by Alhanai et al. that used gold standard speaker segmentations and transcriptions to evaluate cognitive outcomes.", "labels": [], "entities": []}, {"text": "Further details on feature selection, modeling, and the relation to previous work in that domain are described in.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7916319072246552}]}, {"text": "This approach captures real-world scenarios where automatically diarized and transcribed data may not beat human parity but its usage is necessary for deploying screening technologies at scale.", "labels": [], "entities": []}, {"text": "Moreover, audio recordings are often sub-optimal, using digital recorders on a desk, which is the case of the data used in this study.", "labels": [], "entities": []}, {"text": "Therefore the ability to detect cognitive conditions must accommodate the presence of noisy data, of which we sought to evaluate.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate speaker diarization we used the Diarization Error Rate (DER) metric, as well as the percentage of speech classified as non-speech (Miss), the percentage of non-speech classified as speech (False Alarm), and the percentage of speech misclassified as belonging to the other speaker (Confusion Rate)).", "labels": [], "entities": [{"text": "Diarization Error Rate (DER) metric", "start_pos": 44, "end_pos": 79, "type": "METRIC", "confidence": 0.8441043325832912}, {"text": "False Alarm)", "start_pos": 201, "end_pos": 213, "type": "METRIC", "confidence": 0.9022538065910339}]}, {"text": "We used a time-based diarization approach, ignoring segments less than 250ms in duration.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the ASR system we used the Word Error Rate (WER) metric.", "labels": [], "entities": [{"text": "ASR", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9897541403770447}, {"text": "Word Error Rate (WER) metric", "start_pos": 58, "end_pos": 86, "type": "METRIC", "confidence": 0.8966855236462185}]}, {"text": "Given the importance of model interpretability for detecting spoken language biomarkers, logistic regression was chosen as our modeling framework.", "labels": [], "entities": []}, {"text": "The evaluation metrics we used for detecting cognitive impairment was the Area Under the Receiver Operating Characteristic Curve (AUC) which has the advantage of evaluating model performance across the whole range of probability cutoffs, rather than a single point estimate such as accuracy or F1 score).", "labels": [], "entities": [{"text": "Receiver Operating Characteristic Curve (AUC)", "start_pos": 89, "end_pos": 134, "type": "METRIC", "confidence": 0.6188573283808572}, {"text": "accuracy", "start_pos": 282, "end_pos": 290, "type": "METRIC", "confidence": 0.9993823766708374}, {"text": "F1 score", "start_pos": 294, "end_pos": 302, "type": "METRIC", "confidence": 0.9768044352531433}]}, {"text": "To assess the generalizability and robustness of our modeling techniques, we performed leave-one-out cross-validation.", "labels": [], "entities": []}, {"text": "Our results from the first experiment showed that language usage between the subject and tester differed significantly, and that each speaker's language style was consistent across recordings (i.e. subjects consistently spoke like other subjects, and testers consistently spoke like other testers).", "labels": [], "entities": []}, {"text": "Therefore, with the availability of highly accurate transcriptions of the same structure (neuropsychological exams), a highly accurate text-based speaker diarization can be conducted.", "labels": [], "entities": []}, {"text": "Our second set of experiments validated the observation from the previous experiment on language usage patterns across speaker roles (i.e. subjects consistently spoke like other subjects, testers consistently spoke like other testers, and subjects and testers did not speak like each other).", "labels": [], "entities": []}, {"text": "Also, seemingly high WERs (between 66.7% and 81.3%) still contained information that was robust enough for further usage in diarization and modeling of cognitive impairment.", "labels": [], "entities": [{"text": "WERs", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9937224984169006}]}, {"text": "Our last experiment showed that it was possible to perform modeling of cognitive impairment utilizing automatically segmented subject speaker turns that was on par with the oracle speaker segmentation, and that 9 segments was sufficient for evaluation.", "labels": [], "entities": []}, {"text": "As shown in, we also found that not all diarization was equal, nor were all segment lengths equally powerful at modeling subjects' cognitive state.", "labels": [], "entities": []}, {"text": "In the case where no oracle segmentation was available, and automatic segmentation was utilized, longer segments contained information that was more discriminative (AUC 0.68 vs. 0.76).", "labels": [], "entities": [{"text": "AUC", "start_pos": 165, "end_pos": 168, "type": "METRIC", "confidence": 0.7722800374031067}]}, {"text": "For the oracle system, the longest system was the most and equally predictive of cognitive impairment, as all segments taken together.", "labels": [], "entities": []}, {"text": "This highlights that tests that elicited longer responses allowed for more robust diarization, were evaluating cognitive performance that was (via speech) most strongly associated with the outcome, and/or that longer spoken segments provided more opportunity to capture patterns associated with cognitive impairment.", "labels": [], "entities": []}, {"text": "Furthermore, the modeling paradigm we explored was robust enough that neither the underlying neuropsychological test need be explicitly modeled (, nor do the features utilized require word or phone alignments (alignments which require accurate transcriptions in order to generate) ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ASR, Speaker ID, and Cognitive ID", "labels": [], "entities": [{"text": "ASR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8400062918663025}, {"text": "Cognitive ID", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.7204365730285645}]}]}