{"title": [{"text": "KBGAN: Adversarial Learning for Knowledge Graph Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce KBGAN, an adversarial learning framework to improve the performances of a wide range of existing knowledge graph embedding models.", "labels": [], "entities": []}, {"text": "Because knowledge graphs typically only contain positive facts, sampling useful negative training examples is a non-trivial task.", "labels": [], "entities": []}, {"text": "Replacing the head or tail entity of a fact with a uniformly randomly selected entity is a conventional method for generating negative facts, but the majority of the generated negative facts can be easily discriminated from positive facts, and will contribute little towards the training.", "labels": [], "entities": []}, {"text": "Inspired by genera-tive adversarial networks (GANs), we use one knowledge graph embedding model as a negative sample generator to assist the training of our desired model, which acts as the dis-criminator in GANs.", "labels": [], "entities": []}, {"text": "This framework is independent of the concrete form of generator and discriminator, and therefore can utilize a wide variety of knowledge graph embedding models as its building blocks.", "labels": [], "entities": []}, {"text": "In experiments, we adversarially train two translation-based models , TRANSE and TRANSD, each with assistance from one of the two probability-based models, DISTMULT and COMPLEX.", "labels": [], "entities": [{"text": "TRANSE", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.8630176782608032}, {"text": "TRANSD", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.942557692527771}]}, {"text": "We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR.", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.8642494479815165}, {"text": "FB15k-237", "start_pos": 115, "end_pos": 124, "type": "DATASET", "confidence": 0.8901234269142151}, {"text": "WN18", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.8821551203727722}, {"text": "WN18RR", "start_pos": 135, "end_pos": 141, "type": "DATASET", "confidence": 0.9083114862442017}]}, {"text": "Experimental results show that ad-versarial training substantially improves the performances of target embedding models under various settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge graph () is a powerful graph structure that can provide direct access of knowledge to users via various applications such as structured search, question answering, and intelligent virtual assistant.", "labels": [], "entities": [{"text": "question answering", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.8571586608886719}]}, {"text": "A common representation of knowledge graph beliefs is in the form of a discrete relational triple such as LocatedIn.", "labels": [], "entities": [{"text": "LocatedIn", "start_pos": 106, "end_pos": 115, "type": "DATASET", "confidence": 0.9279781579971313}]}, {"text": "A main challenge for using discrete representation of knowledge graph is the lack of capability of accessing the similarities among different entities and relations.", "labels": [], "entities": []}, {"text": "Knowledge graph embedding (KGE) techniques (e.g.,), TRANSE (, DIST-MULT ( , and COMPLEX) have been proposed in recent years to deal with the issue.", "labels": [], "entities": [{"text": "TRANSE", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.958989679813385}]}, {"text": "The main idea is to represent the entities and relations in a vector space, and one can use machine learning technique to learn the continuous representation of the knowledge graph in the latent space.", "labels": [], "entities": []}, {"text": "However, even steady progress has been made in developing novel algorithms for knowledge graph embedding, there is still a common challenge in this line of research.", "labels": [], "entities": [{"text": "knowledge graph embedding", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.6375859876473745}]}, {"text": "For space efficiency, common knowledge graphs such as Freebase (,, and NELL () by default only stores beliefs, rather than disbeliefs.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9637899398803711}]}, {"text": "Therefore, when training the embedding models, there is only the natural presence of the positive examples.", "labels": [], "entities": []}, {"text": "To use negative examples, a common method is to remove the correct tail entity, and randomly sample from a uniform distribution ().", "labels": [], "entities": []}, {"text": "Unfortunately, this approach is not ideal, because the sampled entity could be completely unrelated to the head and the target relation, and thus the quality of randomly generated negative examples is often poor (e.g, LocatedIn(NewOrleans,BarackObama)).", "labels": [], "entities": []}, {"text": "Other approach might leverage external ontological constraints such as entity types () to generate negative examples, but such resource does not always exist or accessible.", "labels": [], "entities": []}, {"text": "In this work, we provide a generic solution to improve the training of a wide range of knowl-", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our proposed framework, we test its performance for the link prediction task with different generators and discriminators.", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.8510237336158752}]}, {"text": "For the generator, we choose two classical probability-based KGE model, DISTMULT and COMPLEX, and for the discriminator, we also choose two classical translation-based KGE model, TRANSE and TRANSD, resulting in four possible combinations of generator and discriminator in total.", "labels": [], "entities": [{"text": "DISTMULT", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.7988707423210144}, {"text": "TRANSD", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.8117821216583252}]}, {"text": "See fora brief summary of these models.", "labels": [], "entities": []}, {"text": "We use three common knowledge base completion datasets for our experiment: FB15k-237, WN18 and WN18RR.", "labels": [], "entities": [{"text": "FB15k-237", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.8543372750282288}, {"text": "WN18", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.9087703227996826}, {"text": "WN18RR", "start_pos": 95, "end_pos": 101, "type": "DATASET", "confidence": 0.9436335563659668}]}, {"text": "FB15k-237 is a subset of FB15k introduced by, which removed redundant relations in FB15k and greatly reduced the number of relations.", "labels": [], "entities": [{"text": "FB15k-237", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9658840298652649}, {"text": "FB15k", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.9573459625244141}, {"text": "FB15k", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.967008113861084}]}, {"text": "Likewise, WN18RR is a subset of WN18 introduced by) which removes reversing relations and dramatically increases the difficulty of reasoning.", "labels": [], "entities": [{"text": "WN18RR", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.8175526261329651}]}, {"text": "Both FB15k and WN18 are first introduced by) and have been commonly used in knowledge graph researches.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.9019702076911926}, {"text": "WN18", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.9130449891090393}]}, {"text": "Statistics of datasets we used are shown in.", "labels": [], "entities": []}, {"text": "Following previous works like (  and (, for each run, we report two common metrics, mean reciprocal ranking (MRR) and hits at 10 (H@10).", "labels": [], "entities": [{"text": "mean reciprocal ranking (MRR)", "start_pos": 84, "end_pos": 113, "type": "METRIC", "confidence": 0.9561860263347626}]}, {"text": "We only report scores under the filtered setting (, which removes all triples appeared in training, validating, and testing sets from candidate triples before obtaining the rank of the ground truth triple.", "labels": [], "entities": []}, {"text": "In the pre-training stage, we train every model to convergence for 1000 epochs, and divide every epoch into 100 mini-batches.", "labels": [], "entities": []}, {"text": "To avoid overfitting, we adopt early stopping by evaluating MRR on the validation set every 50 epochs.", "labels": [], "entities": [{"text": "MRR", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.4806220531463623}]}, {"text": "We tried \u03b3 = 0.5, 1, 2, 3, 4, 5 and L 1 , L 2 distances for TRANSE and TRANSD, and \u03bb = 0.01, 0.1, 1, 10 for DISTMULT and COMPLEX, and determined the best hyperparameters listed on table 2, based on their performances on the validation set after pre-training.", "labels": [], "entities": [{"text": "TRANSD", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.7135507464408875}]}, {"text": "Due to limited computation resources, we deliberately limit the dimensions of embeddings to k = 50, similar to the one used in earlier works, to save time.", "labels": [], "entities": []}, {"text": "We also apply certain constraints or regularizations to these models, which are mostly the same as those described in their original publications, and also listed on table 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Hyperparameter settings of the 4 models we used. For DISTMULT and COMPLEX, \u03bb = 1 is  used for FB15k-237 and \u03bb = 0.1 is used for WN18 and WN18RR. All other hyperparameters are shared  among all datasets. L is the global loss defined in Equation (2). \u0398 represents all parameters in the model.", "labels": [], "entities": [{"text": "FB15k-237", "start_pos": 104, "end_pos": 113, "type": "DATASET", "confidence": 0.9666164517402649}, {"text": "WN18", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.9489436745643616}, {"text": "WN18RR", "start_pos": 147, "end_pos": 153, "type": "DATASET", "confidence": 0.8521406650543213}]}, {"text": " Table 3: Statistics of datasets we used in the exper- iments. \"r\": relations.", "labels": [], "entities": []}, {"text": " Table 4: Experimental results. Results of KBGAN are results of its discriminator (on the left of the \"+\"  sign). Underlined results are the best ones among our implementations. Results marked with  \u2020 are pro- duced by running Fast-TransX (Lin et al., 2015) with its default parameters. Results marked with  \u2021 are  copied from (Dettmers et al., 2017). All other baseline results are copied from their original papers.", "labels": [], "entities": [{"text": "KBGAN", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.8137213587760925}]}]}