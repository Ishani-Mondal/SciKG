{"title": [{"text": "Improving Lexical Choice in Neural Machine Translation", "labels": [], "entities": [{"text": "Improving Lexical Choice", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8898311853408813}, {"text": "Neural Machine Translation", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.6732082962989807}]}], "abstractContent": [{"text": "We explore two solutions to the problem of mistranslating rare words in neural machine translation.", "labels": [], "entities": [{"text": "mistranslating rare words in neural machine translation", "start_pos": 43, "end_pos": 98, "type": "TASK", "confidence": 0.63132386973926}]}, {"text": "First, we argue that the standard output layer, which computes the inner product of a vector representing the context with all possible output word embeddings, rewards frequent words disproportionately, and we propose to fix the norms of both vectors to a constant value.", "labels": [], "entities": []}, {"text": "Second, we integrate a simple lexical module which is jointly trained with the rest of the model.", "labels": [], "entities": []}, {"text": "We evaluate our approaches on eight language pairs with data sizes ranging from 100k to 8M words, and achieve improvements of up to +4.3 BLEU, surpassing phrase-based translation in nearly all settings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9985864162445068}, {"text": "phrase-based translation", "start_pos": 154, "end_pos": 178, "type": "TASK", "confidence": 0.7256444990634918}]}], "introductionContent": [{"text": "Neural network approaches to machine translation () are appealing for their single-model, end-to-end training process, and have demonstrated competitive performance compared to earlier statistical approaches (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.8060125112533569}]}, {"text": "However, there are still many open problems in NMT (.", "labels": [], "entities": []}, {"text": "One particular issue is mistranslation of rare words.", "labels": [], "entities": []}, {"text": "For example, consider the Uzbek sentence: Source: Ammo muammolar hali ko'p, deydi amerikalik olim Entoni Fauchi.", "labels": [], "entities": []}, {"text": "Reference: But still there are many problems, says American scientist Anthony Fauci.", "labels": [], "entities": []}, {"text": "Baseline NMT: But there is still a lot of problems, says James Chan.", "labels": [], "entities": []}, {"text": "At the position where the output should be Fauci, the NMT model's top three candidates are Chan, The code for this work can be found at https://github.com/tnq177/improving_lexical_ choice_in_nmt Fauci, and Jenner.", "labels": [], "entities": []}, {"text": "All three surnames occur in the training data with reference to immunologists: Fauci is the director of the National Institute of Allergy and Infectious Diseases, Margaret (not James) Chan is the former director of the World Health Organization, and Edward Jenner invented smallpox vaccine.", "labels": [], "entities": [{"text": "smallpox vaccine", "start_pos": 273, "end_pos": 289, "type": "TASK", "confidence": 0.6522132456302643}]}, {"text": "But Chan is more frequent in the training data than Fauci, and James is more frequent than either Anthony or Margaret.", "labels": [], "entities": []}, {"text": "Because NMT learns word representations in continuous space, it tends to translate words that \"seem natural in the context, but do not reflect the content of the source sentence\" (.", "labels": [], "entities": [{"text": "NMT learns word representations", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.7324823439121246}]}, {"text": "This coincides with other observations that NMT's translations are often fluent but lack accuracy (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9989317059516907}]}, {"text": "At each time step, the model's distribution over output words e is where W e and be area vector and a scalar depending only one, and\u02dchand\u02dc and\u02dch is a vector depending only on the source sentence and previous output words.", "labels": [], "entities": []}, {"text": "We propose two modifications to this layer.", "labels": [], "entities": []}, {"text": "First, we argue that the term W e \u00b7 \u02dc h, which measures how well e fits into the context\u02dchcontext\u02dc context\u02dch, favors common words disproportionately, and show that it helps to fix the norm of both vectors to a constant.", "labels": [], "entities": []}, {"text": "Second, we add anew term representing a more direct connection from the source sentence, which allows the model to better memorize translations of rare words.", "labels": [], "entities": []}, {"text": "Below, we describe our models in more detail.", "labels": [], "entities": []}, {"text": "Then we evaluate our approaches on eight language pairs, with training data sizes ranging from 100k words to 8M words, and show improvements of up to +4.3 BLEU, surpassing phrasebased translation in nearly all settings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9985318183898926}, {"text": "phrasebased translation", "start_pos": 172, "end_pos": 195, "type": "TASK", "confidence": 0.7451494634151459}]}, {"text": "Finally, we provide some analysis to better understand why our modifications work well.: Preliminary experiments show that tying target embeddings with output layer weights performs as well as or better than the baseline, and that normalizing\u02dchnormalizing\u02dc normalizing\u02dch is better than not normalizing\u02dchnormalizing\u02dc normalizing\u02dch.", "labels": [], "entities": []}, {"text": "All numbers are BLEU scores on development sets, scored against tokenized references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9980545043945312}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Preliminary experiments show that tying target  embeddings with output layer weights performs as well  as or better than the baseline, and that normalizing\u02dchnormalizing\u02dc normalizing\u02dch is  better than not normalizing\u02dchnormalizing\u02dc normalizing\u02dch. All numbers are BLEU  scores on development sets, scored against tokenized  references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 271, "end_pos": 275, "type": "METRIC", "confidence": 0.9985963702201843}]}, {"text": " Table 2: Statistics of data and models: effective number  of training source/target tokens, source/target vocabu- lary sizes, number of hidden layers and number of units  per layer.", "labels": [], "entities": []}, {"text": " Table 3: Test BLEU of all models. Differences shown in parentheses are relative to tied, with a dagger ( \u2020) indicating  an insignificant difference in BLEU (p > 0.01). While the method of Arthur et al. (2016) does not always help,  fixnorm and fixnorm+lex consistently achieve significant improvements over tied (p < 0.01) except for English- Japanese (BTEC). Our models also outperform the method of Arthur et al. on all tasks and outperform Moses on  all tasks but Urdu-English and Hausa-English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9937560558319092}, {"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9994044303894043}]}, {"text": " Table 4: Example translations, in which untied and tied generate incorrect, but often semantically related, words,  but fixnorm and/or fixnorm+lex generate the correct ones.", "labels": [], "entities": []}, {"text": " Table 5: Top five translations for some entries of the lexical tables extracted from fixnorm+lex. Probabilities are  shown in parentheses.", "labels": [], "entities": []}, {"text": " Table 6: When r is too small, high train perplexity  and low dev BLEU indicate underfitting; when r is too  large, low train perplexity and low dev BLEU indicate  overfitting.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9910295009613037}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9901554584503174}]}, {"text": " Table 7: Test BLEU for all BPE-based systems. Our models significantly improve over the baseline (p < 0.01) for  both high and low resource when using BPE.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9984264373779297}, {"text": "BPE", "start_pos": 152, "end_pos": 155, "type": "DATASET", "confidence": 0.8513328433036804}]}]}