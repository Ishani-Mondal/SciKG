{"title": [{"text": "Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer", "labels": [], "entities": [{"text": "Sentiment and Style Transfer", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.8227268308401108}]}], "abstractContent": [{"text": "We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., changing \"screen is just the right size\" to \"screen is too small\").", "labels": [], "entities": [{"text": "text attribute transfer", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.658827672402064}]}, {"text": "Our training data includes only sentences labeled with their attribute (e.g., positive or negative), but not pairs of sentences that differ only in their attributes, so we must learn to disentangle attributes from attribute-independent content in an unsupervised way.", "labels": [], "entities": []}, {"text": "Previous work using adversarial methods has struggled to produce high-quality outputs.", "labels": [], "entities": []}, {"text": "In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., \"too small\").", "labels": [], "entities": []}, {"text": "Our strongest method extracts content words by deleting phrases associated with the sentence's original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output.", "labels": [], "entities": []}, {"text": "On human evaluation, our best method generates grammatical and appropriate responses on 22% more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous .", "labels": [], "entities": [{"text": "Yelp", "start_pos": 217, "end_pos": 221, "type": "DATASET", "confidence": 0.8594114184379578}]}], "introductionContent": [{"text": "The success of natural language generation (NLG) systems depends on their ability to carefully control not only the topic of produced utterances, but also attributes such as sentiment and style.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.8159995277722677}]}, {"text": "The desire for more sophisticated, controllable NLG has led to increased interest in text attribute transferthe task of editing a sentence to alter specific attributes, such as style, sentiment, and tense (Hu * Work done while the author was a visiting researcher at Stanford University.", "labels": [], "entities": [{"text": "text attribute transferthe", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.6123906870683035}]}, {"text": "In each of these cases, the goal is to convert a sentence with one attribute (e.g., negative sentiment) to one with a different attribute (e.g., positive sentiment), while preserving all attribute-independent content 1 (e.g., what properties of a restaurant are being discussed).", "labels": [], "entities": []}, {"text": "Typically, aligned sentences with the same content but different attributes are not available; systems must learn to disentangle attributes and content given only unaligned sentences labeled with attributes.", "labels": [], "entities": []}, {"text": "Previous work has attempted to use adversarial networks for this task, but-as we demonstrate-their outputs tend to be low-quality, as judged by human raters.", "labels": [], "entities": []}, {"text": "These models are also difficult to train.", "labels": [], "entities": []}, {"text": "In this work, we propose a set of simpler, easierto-train systems that leverage an important observation: attribute transfer can often be accomplished by changing a few attribute markerswords or phrases in the sentence that are indicative of a particular attribute-while leaving the rest of the sentence largely unchanged.", "labels": [], "entities": [{"text": "attribute transfer", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7288670390844345}]}, {"text": "shows an example in which the sentiment of a sentence can be altered by changing a few sentiment-specific phrases but keeping other words fixed.", "labels": [], "entities": []}, {"text": "With this intuition, we first propose a simple baseline that already outperforms prior adversarial approaches.", "labels": [], "entities": []}, {"text": "Consider a sentiment transfer (negative to positive) task.", "labels": [], "entities": [{"text": "sentiment transfer (negative to positive) task", "start_pos": 11, "end_pos": 57, "type": "TASK", "confidence": 0.814031608402729}]}, {"text": "First, from unaligned corpora of positive and negative sentences, we identify attribute markers by finding phrases that occur much more often within sentences of one attribute than the other (e.g., \"worst\" and \"very disppointed\" are negative markers).", "labels": [], "entities": []}, {"text": "Second, given a sentence, we delete any negative markers in it, and regard the remaining words as its content.", "labels": [], "entities": []}, {"text": "Third, we retrieve a sentence with similar content from the positive corpus.", "labels": [], "entities": []}, {"text": "We further improve upon this baseline by incorporating a neural generative model, as shown in.", "labels": [], "entities": []}, {"text": "Our neural system extracts content words in the same way as our baseline, then generates the final output with an RNN decoder that conditions on the extracted content and the target attribute.", "labels": [], "entities": []}, {"text": "This approach has significant benefits at training time, compared to adversarial networks: having already separated content and attribute, we simply train our neural model to reconstruct sentences in the training data as an auto-encoder.", "labels": [], "entities": []}, {"text": "We test our methods on three text attribute transfer datasets: altering sentiment of Yelp reviews, altering sentiment of Amazon reviews, and altering image captions to be more romantic or humorous.", "labels": [], "entities": []}, {"text": "Averaged across these three datasets, our simple baseline generated grammatical sentences with appropriate content and attribute 23% of the time, according to human raters; in contrast, the best adversarial method achieved only 12%.", "labels": [], "entities": []}, {"text": "Our best neural system in turn outperformed our baseline, achieving an average success rate of 34%.", "labels": [], "entities": []}, {"text": "Our code and data, including newly collected human reference outputs for the Yelp and Amazon domains, can be found at https://github.com/lijuncen/ Sentiment-and-Style-Transfer.", "labels": [], "entities": [{"text": "Yelp", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9839653968811035}]}], "datasetContent": [{"text": "We evaluated our approach on three domains: flipping sentiment of Yelp reviews (YELP) and Amazon reviews (AMAZON), and changing image captions to be romantic or humorous (CAPTIONS).", "labels": [], "entities": [{"text": "flipping sentiment of Yelp reviews (YELP)", "start_pos": 44, "end_pos": 85, "type": "TASK", "confidence": 0.6799826584756374}, {"text": "Amazon reviews (AMAZON)", "start_pos": 90, "end_pos": 113, "type": "METRIC", "confidence": 0.577400517463684}]}, {"text": "We compared our four systems to human references and three previously published adversarial approaches.", "labels": [], "entities": []}, {"text": "As judged by human raters, both of our two baselines outperform all three adversarial methods.", "labels": [], "entities": []}, {"text": "Moreover, DELETEANDRETRIEVE outperforms all other automatic approaches.", "labels": [], "entities": [{"text": "DELETEANDRETRIEVE", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9937322735786438}]}, {"text": "First, we describe the three datasets we use, which are commonly used in prior works too.", "labels": [], "entities": []}, {"text": "All datasets are randomly split into train, development, and test sets.", "labels": [], "entities": []}, {"text": "YELP Each example is a sentence from a business review on Yelp, and is labeled as having either positive or negative sentiment.", "labels": [], "entities": [{"text": "YELP", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8427709341049194}, {"text": "Yelp", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8320351839065552}]}, {"text": "AMAZON Similar to YELP, each example is a sentence from a product review on Amazon, and is labeled as having either positive or negative sentiment (.", "labels": [], "entities": [{"text": "AMAZON", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9018005132675171}, {"text": "YELP", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8159380555152893}]}, {"text": "CAPTIONS In the CAPTIONS dataset (, each example is a sentence that describes an image, and is labeled as either factual, romantic, or humorous.", "labels": [], "entities": [{"text": "CAPTIONS dataset", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.8273936212062836}]}, {"text": "We focus on the task of converting factual sentences into romantic and humorous ones.", "labels": [], "entities": []}, {"text": "Unlike YELP and AMAZON, CAP-TIONS is actually an aligned corpus-it contains captions for the same image in different styles.", "labels": [], "entities": [{"text": "YELP", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.8954152464866638}]}, {"text": "Our systems do not use these alignments, but we use them as gold references for evaluation.", "labels": [], "entities": []}, {"text": "CAPTIONS is also unique in that we reconstruct romantic and humorous sentences during training, whereas attest time we are given factual captions.", "labels": [], "entities": [{"text": "CAPTIONS", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.4296719431877136}]}, {"text": "We assume these factual captions carry only content, and therefore do not look for and delete factual attribute markers; The model essentially only inserts romantic or humorous attribute markers as appropriate.", "labels": [], "entities": []}, {"text": "For our methods, we use 128-dimensional word vectors and a single-layer GRU with 512 hidden units for both encoders and the decoder.", "labels": [], "entities": []}, {"text": "We use the maxout activation function (.", "labels": [], "entities": []}, {"text": "All parameters are initialized by sampling from a uniform distribution between \u22120.1 and 0.1.", "labels": [], "entities": []}, {"text": "For optimization, we use Adadelta (Zeiler, 2012) with a minibatch size of 256.", "labels": [], "entities": [{"text": "Adadelta (Zeiler, 2012)", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.697263111670812}]}, {"text": "For attribute marker extraction, we consider spans up to 4 words, and the smoothing parameter \u03bb is set to 1.", "labels": [], "entities": [{"text": "attribute marker extraction", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.8553838133811951}]}, {"text": "We set the attribute marker threshold \u03b3, which controls the precision and recall of our attribute markers, to 15, 5.5 and 5 for YELP, AMAZON, and CAPTIONS.", "labels": [], "entities": [{"text": "attribute marker threshold \u03b3", "start_pos": 11, "end_pos": 39, "type": "METRIC", "confidence": 0.8191323727369308}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9990847110748291}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9919048547744751}, {"text": "YELP", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9952444434165955}, {"text": "AMAZON", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9566962122917175}]}, {"text": "These values were set by manual inspection of the resulting markers and tuning slightly on the dev set.", "labels": [], "entities": []}, {"text": "For retrieval, we used the TF-IDF weighted word overlap score for DELETEANDRETRIEVE and TEMPLATEBASED, and the Euclidean distance of content embeddings for RETRIEVEONLY.", "labels": [], "entities": [{"text": "TF-IDF weighted word overlap score", "start_pos": 27, "end_pos": 61, "type": "METRIC", "confidence": 0.8625397086143494}, {"text": "DELETEANDRETRIEVE", "start_pos": 66, "end_pos": 83, "type": "METRIC", "confidence": 0.930667519569397}, {"text": "TEMPLATEBASED", "start_pos": 88, "end_pos": 101, "type": "METRIC", "confidence": 0.8928991556167603}, {"text": "RETRIEVEONLY", "start_pos": 156, "end_pos": 168, "type": "METRIC", "confidence": 0.8941692113876343}]}, {"text": "We find the two scoring functions give similar results.", "labels": [], "entities": []}, {"text": "For all neural models, we do beam search with abeam size of 10.", "labels": [], "entities": []}, {"text": "For DELETEANDRETRIEVE, similar to, we retrieve the top-10 sentences and generate results using markers from each sentence.", "labels": [], "entities": [{"text": "DELETEANDRETRIEVE", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.8407220840454102}]}, {"text": "We then select the output with the lowest perplexity given by a separately-trained neural language model on the target-domain training data.", "labels": [], "entities": []}, {"text": "We hired workers on Amazon Mechanical Turk to rate the outputs of all systems.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 20, "end_pos": 42, "type": "DATASET", "confidence": 0.9557633399963379}]}, {"text": "For each source sentence and target attribute, the same worker was shown the output of each tested system.", "labels": [], "entities": []}, {"text": "Workers were asked to rate each output on three criteria on a Likert scale from 1 to 5: grammaticality, similarity to the target attribute, and preservation of the source content.", "labels": [], "entities": []}, {"text": "Finally, we consider a generated output \"successful\" if it is rated 4 or 5 on all three criteria.", "labels": [], "entities": []}, {"text": "For each dataset, we evaluated 400 randomly sampled examples (200 for each target attribute).", "labels": [], "entities": []}, {"text": "shows the human evaluation results.", "labels": [], "entities": []}, {"text": "On all three datasets, both of our baselines have a higher success rate than the previously published models, and DELETEANDRETRIEVE achieves the best performance among all systems.", "labels": [], "entities": [{"text": "DELETEANDRETRIEVE", "start_pos": 114, "end_pos": 131, "type": "METRIC", "confidence": 0.9899044632911682}]}, {"text": "Additionally, we see that human raters strongly preferred the human references to all systems, suggesting there is still significant room for improvement on this task.", "labels": [], "entities": []}, {"text": "We find that a human evaluator's judgment of a sentence is largely relative to other sentences being evaluated together and examples given in the instruction (different for each dataset/task).", "labels": [], "entities": []}, {"text": "There-fore, evaluating all system outputs in one batch is important and results on different datasets are not directly comparable.", "labels": [], "entities": []}, {"text": "Following previous work (, we also compute automatic evaluation metrics, and compare these numbers to our human evaluation results.", "labels": [], "entities": []}, {"text": "We use an attribute classifier to assess whether outputs have the desired attribute (.", "labels": [], "entities": []}, {"text": "We define the classifier score as the fraction of outputs classified as having the target attribute.", "labels": [], "entities": []}, {"text": "For each dataset, we train an attribute classifier on the same training data.", "labels": [], "entities": []}, {"text": "Specifically, we encode the sentence into a vector by a bidirectional LSTM with an average pooling layer over the outputs, and train the classifier by minimizing the logistic loss.", "labels": [], "entities": []}, {"text": "We also compute BLEU between the output and the human references, similar to.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9989369511604309}]}, {"text": "A high BLEU score primarily indicates that the system can correctly preserve content by retaining the same words from the source sentence as the reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9992996454238892}]}, {"text": "One might also hope that it has some correlation with fluency, though we expect this correlation to be much weaker.", "labels": [], "entities": []}, {"text": "shows the classifier and BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9993151426315308}]}, {"text": "In, we compute the system-level correlation between classifier score and human judgments of attribute transfer, and between BLEU and human judgments of content preservation and grammaticality.", "labels": [], "entities": [{"text": "attribute transfer", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7474634647369385}, {"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9968594312667847}, {"text": "content preservation", "start_pos": 152, "end_pos": 172, "type": "TASK", "confidence": 0.6897779554128647}]}, {"text": "We also plot scores given by the automatic metrics and humans in.", "labels": [], "entities": []}, {"text": "While the scores are sometimes well-correlated, the results vary significantly between datasets; on AMAZON, there is no correlation between the classifier score and the human evaluation.", "labels": [], "entities": [{"text": "AMAZON", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.8459556698799133}]}, {"text": "Manual inspection shows that on AMAZON, some product genres are associated with either mostly positive or mostly negative reviews.", "labels": [], "entities": [{"text": "AMAZON", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.8124743700027466}]}, {"text": "However, our systems produce, for example, negative reviews about products that are mostly discussed positively in the training set.", "labels": [], "entities": []}, {"text": "Therefore, the classifier often gives unreliable predictions on system outputs.", "labels": [], "entities": []}, {"text": "As expected, BLEU does not correlate well with human grammaticality ratings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9985034465789795}]}, {"text": "The lack of automatic fluency evaluation artificially favors systems like TEMPLATEBASED, which make more grammatical mistakes.", "labels": [], "entities": [{"text": "TEMPLATEBASED", "start_pos": 74, "end_pos": 87, "type": "METRIC", "confidence": 0.7764593362808228}]}, {"text": "We conclude that while these automatic evaluation methods are useful for model development, they cannot replace human evaluation.", "labels": [], "entities": [{"text": "model development", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7692343890666962}]}], "tableCaptions": [{"text": " Table 2: Human evaluation results on all three datasets. We show average human ratings for grammaticality (Gra),  content preservation (Con), and target attribute match (Att) on a 1 to 5 Likert scale, as well as overall success  rate (Suc). On all three datasets, DELETEANDRETRIEVE is the best overall system, and all four of our methods  outperform previous work.", "labels": [], "entities": [{"text": "grammaticality (Gra)", "start_pos": 92, "end_pos": 112, "type": "METRIC", "confidence": 0.708026647567749}, {"text": "content preservation (Con)", "start_pos": 115, "end_pos": 141, "type": "METRIC", "confidence": 0.6520816743373871}, {"text": "target attribute match (Att)", "start_pos": 147, "end_pos": 175, "type": "METRIC", "confidence": 0.7857200006643931}, {"text": "overall success  rate (Suc)", "start_pos": 213, "end_pos": 240, "type": "METRIC", "confidence": 0.7625468323628107}, {"text": "DELETEANDRETRIEVE", "start_pos": 265, "end_pos": 282, "type": "METRIC", "confidence": 0.9756372570991516}]}, {"text": " Table 3: Example outputs on YELP, CAPTIONS, and AMAZON. Additional examples for transfer from opposite  directions are given in Table 6. Added or changed words are in italic. Attribute markers are colored.", "labels": [], "entities": [{"text": "YELP", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9897220730781555}, {"text": "CAPTIONS", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.8188841938972473}, {"text": "AMAZON", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9742789268493652}]}, {"text": " Table 4: Automatic evaluation results. \"Classifier\" shows the percentage of sentences labeled as the target attribute  by the classifier. BLEU measures content similarity between the output and the human reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9976367950439453}]}, {"text": " Table 5: Spearman correlation between two automatic evaluation metrics and related human evaluation scores.  While some correlations are strong, the classifier exhibits poor correlation on AMAZON, and BLEU only measures  content, not grammaticality.", "labels": [], "entities": [{"text": "AMAZON", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.8241639137268066}, {"text": "BLEU", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.998026430606842}]}]}