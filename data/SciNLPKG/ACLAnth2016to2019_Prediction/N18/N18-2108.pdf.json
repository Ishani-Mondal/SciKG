{"title": [{"text": "Higher-order Coreference Resolution with Coarse-to-fine Inference", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.9785837531089783}]}], "abstractContent": [{"text": "We introduce a fully differentiable approximation to higher-order inference for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.981471598148346}]}, {"text": "Our approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations.", "labels": [], "entities": []}, {"text": "This enables the model to softly consider multiple hops in the predicted clusters.", "labels": [], "entities": []}, {"text": "To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilin-ear factor, enabling more aggressive pruning without hurting accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 222, "end_pos": 230, "type": "METRIC", "confidence": 0.9976824522018433}]}, {"text": "Compared to the existing state-of-the-art span-ranking approach, our model significantly improves accuracy on the English OntoNotes benchmark, while being far more computationally efficient.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.999200165271759}, {"text": "English OntoNotes benchmark", "start_pos": 114, "end_pos": 141, "type": "DATASET", "confidence": 0.7948548992474874}]}], "introductionContent": [{"text": "Recent coreference resolution systems have heavily relied on first order models, where only pairs of entity mentions are scored by the model.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.9629853963851929}]}, {"text": "These models are computationally efficient and scalable to long documents.", "labels": [], "entities": []}, {"text": "However, because they make independent decisions about coreference links, they are susceptible to predicting clusters that are locally consistent but globally inconsistent.", "labels": [], "entities": []}, {"text": "shows an example from that illustrates this failure case.", "labels": [], "entities": []}, {"text": "The plurality of is underspecified, making it locally compatible with both and [all of you], while the full cluster would have mixed plurality, resulting in global inconsistency.", "labels": [], "entities": []}, {"text": "We introduce an approximation of higher-order inference that uses the span-ranking architecture from in an iterative manner.", "labels": [], "entities": []}, {"text": "At each iteration, the antecedent distribution is used as an attention mechanism to optionally update existing span representations, enabling later coreferSpeaker 1: Um and think that is what's -Go ahead Linda.", "labels": [], "entities": [{"text": "Um", "start_pos": 166, "end_pos": 168, "type": "METRIC", "confidence": 0.9494046568870544}]}, {"text": "Speaker 2: Welland uh thanks goes to and to the media to help us...", "labels": [], "entities": []}, {"text": "So our hat is off to [all of you] as well.", "labels": [], "entities": []}, {"text": "ence decisions to softly condition on earlier coreference decisions.", "labels": [], "entities": []}, {"text": "For the example in, this enables the linking of and [all of you] to depend on the linking of and.", "labels": [], "entities": []}, {"text": "To alleviate computational challenges from this higher-order inference, we also propose a coarseto-fine approach that is learned with a single endto-end objective.", "labels": [], "entities": []}, {"text": "We introduce a less accurate but more efficient coarse factor in the pairwise scoring function.", "labels": [], "entities": []}, {"text": "This additional factor enables an extra pruning step during inference that reduces the number of antecedents considered by the more accurate but inefficient fine factor.", "labels": [], "entities": []}, {"text": "Intuitively, the model cheaply computes a rough sketch of likely antecedents before applying a more expensive scoring function.", "labels": [], "entities": []}, {"text": "Our experiments show that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.9733142852783203}, {"text": "English OntoNotes benchmark", "start_pos": 115, "end_pos": 142, "type": "DATASET", "confidence": 0.8212122122446696}]}, {"text": "We observe a significant increase in average F1 with a second-order model, but returns quickly diminish with a third-order model.", "labels": [], "entities": [{"text": "F1", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9973514080047607}]}, {"text": "Additionally, our analysis shows that the coarse-to-fine approach makes the model performance relatively insensitive to more aggressive antecedent pruning, compared to the distance-based heuristic pruning from previous work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the English coreference resolution data from the CoNLL-2012 shared task () in our experiments.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.8117167949676514}, {"text": "CoNLL-2012 shared task", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.8352899750073751}]}, {"text": "The code for replicating these results is publicly available.", "labels": [], "entities": [{"text": "replicating", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9716049432754517}]}, {"text": "Our models reuse the hyperparameters from, with a few exceptions mentioned below.", "labels": [], "entities": []}, {"text": "In our results, we report two improvements that are orthogonal to our contributions.", "labels": [], "entities": []}, {"text": "\u2022 We used embedding representations from a language model () at the input to the LSTMs (ELMo in the results).", "labels": [], "entities": [{"text": "ELMo", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.943310022354126}]}, {"text": "\u2022 We changed several hyperparameters: 1.", "labels": [], "entities": []}, {"text": "increasing the maximum span width from 10 to 30 words.", "labels": [], "entities": [{"text": "maximum span width", "start_pos": 15, "end_pos": 33, "type": "METRIC", "confidence": 0.5756113529205322}]}, {"text": "2. using 3 highway LSTMs instead of 1. 3. using GloVe word embeddings) with a window size 1 https://github.com/kentonl/e2e-coref of 2 for the headword embeddings and a window size of 10 for the LSTM inputs.", "labels": [], "entities": []}, {"text": "The baseline model considers up to 250 antecedents per span.", "labels": [], "entities": []}, {"text": "As shown in, the coarse-to-fine model is quite insensitive to more aggressive pruning.", "labels": [], "entities": []}, {"text": "Therefore, our final model considers only 50 antecedents per span.", "labels": [], "entities": []}, {"text": "On the development set, the second-order model (N = 2) outperforms the first-order model by 0.8 F1, but the third order model only provides an additional 0.1 F1 improvement.", "labels": [], "entities": [{"text": "F1", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9969251751899719}, {"text": "F1", "start_pos": 158, "end_pos": 160, "type": "METRIC", "confidence": 0.9973779916763306}]}, {"text": "Therefore, we only compute test results for the secondorder model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the test set on the English CoNLL-2012 shared task. The average F1 of MUC, B 3 , and  CEAF \u03c64 is the main evaluation metric. We show only non-ensembled models for fair comparison.", "labels": [], "entities": [{"text": "English CoNLL-2012 shared task", "start_pos": 41, "end_pos": 71, "type": "DATASET", "confidence": 0.9261082112789154}, {"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9973784685134888}, {"text": "CEAF \u03c64", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.4879833161830902}]}]}