{"title": [], "abstractContent": [{"text": "Co-training is a popular semi-supervised learning framework to utilize a large amount of unlabeled data in addition to a small labeled set.", "labels": [], "entities": []}, {"text": "Co-training methods exploit predicted labels on the unlabeled data and select samples based on prediction confidence to augment the training.", "labels": [], "entities": []}, {"text": "However, the selection of samples in existing co-training methods is based on a predetermined policy, which ignores the sampling bias between the unlabeled and the labeled subsets, and fails to explore the data space.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel method, Reinforced Co-Training, to select high-quality unlabeled samples to better co-train on.", "labels": [], "entities": []}, {"text": "More specifically, our approach uses Q-learning to learn a data selection policy with a small labeled dataset, and then exploits this policy to train the co-training classifiers automatically.", "labels": [], "entities": []}, {"text": "Experimental results on click-bait detection and generic text classification tasks demonstrate that our proposed method can obtain more accurate text classification results .", "labels": [], "entities": [{"text": "click-bait detection", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7614942193031311}, {"text": "generic text classification", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.7121494710445404}, {"text": "text classification", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.6930189728736877}]}], "introductionContent": [{"text": "Large labeled datasets are often required to obtain satisfactory performance for natural language processing tasks.", "labels": [], "entities": []}, {"text": "However, it is time-consuming to label text corpus manually.", "labels": [], "entities": []}, {"text": "In the meanwhile, there are abundant unlabeled text corpora available on the web.", "labels": [], "entities": []}, {"text": "Semi-supervised methods permit learning improved supervised models by jointly train on a small labeled dataset and a large unlabeled dataset.", "labels": [], "entities": []}, {"text": "Co-training is one of the widely used semisupervised methods, where two complementary classifiers utilize large amounts of unlabeled examples to bootstrap the performance of each other iteratively).", "labels": [], "entities": []}, {"text": "Co-training can be readily applied to NLP tasks since data in these tasks naturally", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our proposed Reinforced Co-training method in two settings: (1) Clickbait detection, where obtaining the labeled data is very timeconsuming and labor-intensive in this real-world problem; (2) Generic text classification, where we randomly set some of the labeled data as unlabeled and train our model in a controlled setting.", "labels": [], "entities": [{"text": "Clickbait detection", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.795396476984024}, {"text": "Generic text classification", "start_pos": 204, "end_pos": 231, "type": "TASK", "confidence": 0.766198476155599}]}, {"text": "Following the settings in , we use large-scale datasets to train and test our model.", "labels": [], "entities": []}, {"text": "To maintain the two-view setting of the co-training method, we choose the following two datasets.", "labels": [], "entities": []}, {"text": "The original annotated training set is then split into three sets, 10% labeled training set, 10% labeled validation set and 80% unlabeled set.", "labels": [], "entities": []}, {"text": "The original proportion of different classes remains the same after the partition.", "labels": [], "entities": []}, {"text": "The statistics of these two datasets are listed in AG's news corpus.", "labels": [], "entities": [{"text": "AG's news corpus", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.9576855152845383}]}, {"text": "The AGs corpus of news articles is obtained from the web and each sample has the title and description fields.", "labels": [], "entities": []}, {"text": "This dataset is constructed by picking 14 non-overlapping classes from DBpedia 2014.", "labels": [], "entities": [{"text": "DBpedia 2014", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.9732355773448944}]}, {"text": "Each sample contains the title and abstract of a Wikipedia article.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of Clickbait Dataset.", "labels": [], "entities": [{"text": "Clickbait Dataset", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.9323932230472565}]}, {"text": " Table 2: The experimental results on clickbait dataset.  Prec.: precision.", "labels": [], "entities": [{"text": "clickbait dataset", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.8487672507762909}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9984878301620483}]}, {"text": " Table 3: The robustness analysis on clickbait dataset.", "labels": [], "entities": [{"text": "clickbait dataset", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.8998086750507355}]}, {"text": " Table 4: Statistics of the Text Classification Datasets.", "labels": [], "entities": [{"text": "Text Classification Datasets", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.8225249449412028}]}, {"text": " Table 5: The experimental results on generic text clas- sification datasets. * Adversarial-SSL is trained on full  labeled data after pre-training.", "labels": [], "entities": []}, {"text": " Table 6: The robustness analysis on generic text classi- fication. Metric: test error rate (%).", "labels": [], "entities": [{"text": "Metric", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9755995869636536}, {"text": "test error rate", "start_pos": 76, "end_pos": 91, "type": "METRIC", "confidence": 0.6879062453905741}]}]}