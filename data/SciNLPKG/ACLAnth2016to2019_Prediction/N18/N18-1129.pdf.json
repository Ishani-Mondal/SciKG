{"title": [{"text": "Simple Models for Word Formation in English Slang", "labels": [], "entities": [{"text": "Word Formation", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7447566539049149}, {"text": "Slang", "start_pos": 44, "end_pos": 49, "type": "TASK", "confidence": 0.6744601726531982}]}], "abstractContent": [{"text": "We propose generative models for three types of extra-grammatical word formation phenomena abounding in English slang: Blends, Clip-pings, and Reduplicatives.", "labels": [], "entities": [{"text": "extra-grammatical word formation", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.7165507674217224}, {"text": "Blends", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9899725914001465}]}, {"text": "Adopting a data-driven approach coupled with linguistic knowledge , we propose simple models with state of the art performance on human annotated gold standard datasets.", "labels": [], "entities": []}, {"text": "Overall, our models reveal insights into the generative processes of word formation in slang-insights which are increasingly relevant in the context of the rising prevalence of slang and non-standard varieties on the Internet.", "labels": [], "entities": [{"text": "generative processes of word formation", "start_pos": 45, "end_pos": 83, "type": "TASK", "confidence": 0.6624625444412231}]}], "introductionContent": [{"text": "Linguistic analysis of slang has traditionally received little attention with some arguing that research on slang be assigned to an \"extra-linguistic darkness\".", "labels": [], "entities": [{"text": "Linguistic analysis of slang", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8621365129947662}]}, {"text": "However, argues that the emergence of social media has predominantly increased the usage of slang and nonstandard forms and \"slang is now worldwide the vocabulary of choice of young people\" 1 . This increasing pervasiveness has recently motivated research on slang and the linguistic phenomena it manifests.", "labels": [], "entities": []}, {"text": "Most notable are the works of, who argues that slang exhibits extragrammatical properties that distinguish it from the standard form.", "labels": [], "entities": []}, {"text": "Specifically, linguistic phenomena like alphabetisms, blending, clippings, and reduplicatives abound in slang (see) . Note the rich and varied word formation patterns ranging from simple abbreviations like dink to more complex combinations like lambortini, a blend of lamborghini and martini.", "labels": [], "entities": []}, {"text": "Note further, that While the definition of slang is a controversial issue, we adopt abroad definition including non-standard expressions.", "labels": [], "entities": [{"text": "definition of slang", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.8107460737228394}]}, {"text": "While such phenomena are likely present in several languages, in this work we restrict ourselves to slang in English.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here, we define the extra-grammatical morphological phenomena modeled and describe the datasets used for our experiments and analysis.", "labels": [], "entities": []}, {"text": "argues that slang exhibits extra-grammatical morphological properties that distinguish them from the standard variety and identified four broad word formation phenomena 5 described below: 1.", "labels": [], "entities": [{"text": "word formation", "start_pos": 144, "end_pos": 158, "type": "TASK", "confidence": 0.7644307315349579}]}, {"text": "Alphabetisms are shortenings of a multiword sequence.", "labels": [], "entities": []}, {"text": "Examples include lol from laugh out loud or YOLO from you only live once.", "labels": [], "entities": [{"text": "YOLO", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9928268790245056}]}, {"text": "They can be further sub-categorized into two types based on their pronunciation although the distinction may not always be clear: (a) Acronyms are pronounced using the regular reading rules (for example.", "labels": [], "entities": []}, {"text": "YOLO) (b) Initialisms are pronounced letter by letter (for example. BBC).", "labels": [], "entities": [{"text": "YOLO", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8244810104370117}, {"text": "BBC", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.9156069755554199}]}, {"text": "2. Blends or portmanteaus, are formed by merging parts of existing words.", "labels": [], "entities": []}, {"text": "For example, edutainment is a blend of education and entertainment.", "labels": [], "entities": []}, {"text": "Prior work notes that blend formation does not exhibit rigid rules but only demonstrates affinities towards certain patterns of formation suggesting learning based approaches to modeling blends).", "labels": [], "entities": [{"text": "blend formation", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.8229973614215851}]}, {"text": "3. Clippings are constructed by shortening words (lexemes).", "labels": [], "entities": []}, {"text": "For example, berg is a clipping of iceberg, gym is a clipping of gymnasium and ammo is a clipping of ammunition.", "labels": [], "entities": []}, {"text": "Based on the portion that is being clipped, clippings are sub-categorized into three types: (a) BACK clipping where the beginning of the word (lexeme) is retained (like brill from brilliant) (b) a FORE clipping, where the end of a word is retained (like choke from artichoke) and (c) A COMPOUND clipping (adman), a clipping of a compound word (advertisment man).", "labels": [], "entities": [{"text": "BACK", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9735677242279053}, {"text": "FORE", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.9965052604675293}]}, {"text": "We compare our model to previous methods namely and (   Most clippings have at-most 2 syllables but it is a challenge to infer whether a given word has a one or two syllable clipping.", "labels": [], "entities": []}, {"text": "instances) . For evaluating on D knight , we use 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "For evaluating on D blind we train our model on D knight and report the mean score on the test-set obtained using 10 random splits of the training data.", "labels": [], "entities": []}, {"text": "As in previous work, our metric is the edit distance between the predicted blend and the true blend.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.9350979328155518}]}, {"text": "Having described our models and evaluation metrics in the previous section, we proceed to evaluate our models empirically and describe our results.", "labels": [], "entities": []}, {"text": "Blends We train our model on D knight and evaluate on the D blind dataset as in () comparing against previous methods.", "labels": [], "entities": []}, {"text": "We set the number of hidden units to 50 with a dropout probability of 0.5 11 . We use ADAM () optimizer with an initial learning rate of 0.001 to train the model for 500 epochs with early stopping over a validation set.", "labels": [], "entities": []}, {"text": "show the mean edit distance of our predictions from the target blend.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 14, "end_pos": 27, "type": "METRIC", "confidence": 0.8966454565525055}]}, {"text": "First, the evaluation on the D knight dataset compares the performance of our models against previous baselines.", "labels": [], "entities": [{"text": "D knight dataset", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.6522397100925446}]}, {"text": "Note that just using our basic model COPYCAT -(LSTM + LM) outperforms the baseline proposed by.", "labels": [], "entities": [{"text": "COPYCAT -(LSTM + LM", "start_pos": 37, "end_pos": 56, "type": "METRIC", "confidence": 0.6973524451255798}]}, {"text": "Furthermore, observe that even our vanilla model (LSTM) significantly outperforms the equivalent \"FOR-WARD\" models by ( that use greedy and beam-search decoding (1.90 and 2.37 vs 1.75).", "labels": [], "entities": [{"text": "FOR-WARD", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9901334047317505}]}, {"text": "Moreover, observe that our model even achieves almost equivalent performance to the \"FORWARD\" state-of-art model which uses exhaustive decoding.", "labels": [], "entities": [{"text": "FORWARD", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.982339084148407}]}, {"text": "Furthermore, our model achieves competitive performance with the more complex models proposed by GANGAL-BACKWARD which use exhaustive decoding.", "labels": [], "entities": [{"text": "GANGAL-BACKWARD", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.8265262246131897}]}, {"text": "We emphasize that we can achieve competitive performance using a simpler model without using exhaustive decoding.", "labels": [], "entities": []}, {"text": "Similar observations can also be made for the evaluation of the D blind data set (our best model yields a score of 1.91 vs 1.77).", "labels": [], "entities": [{"text": "D blind data set", "start_pos": 64, "end_pos": 80, "type": "DATASET", "confidence": 0.6926327422261238}]}, {"text": "Altogether these observations suggest that even simple models with effective modeling of linguistic structure can perform competitively and even outperform overly complex models (see fora few example predictions).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Set of examples demonstrating the effect of incorporating the language model and length model when  ranking candidates revealing insights into blend formation. Incorporating the language model generally improves  the fluency of the blend while the length model helps generate blends with lengths close to the target.", "labels": [], "entities": []}, {"text": " Table 4: Performance of our blending model COPY- CAT in terms of edit distance (lower is better) on  D blind dataset.  \u2020 indicates ensemble approach using  sub-samples of training data consistent with previous  work. Our simpler model yields competitive perfor- mance without the need for exhaustive decoding, uses  a smaller learn-able parameter set while effectively us- ing linguistic insights into the blending process. To en- sure fair comparison, numbers for the baselines were  obtained by filtering the released predictions for these  models to the same set of words our models were eval- uated on.", "labels": [], "entities": [{"text": "COPY- CAT", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.8883667786916097}]}, {"text": " Table 5: Exemplary predictions from clippings models.  We can generate one/two syllable clippings, as well as  compound clippings. Note the effect of incorrect P2G  conversion for amelia as umm which is pronounced  similar to ame. The current G2P model does not in- corporate stress. It is possible that incorporating stress  into the model can address this scenario.", "labels": [], "entities": []}, {"text": " Table 6: Exemplary predictions from our simple blends  model which suggests our model effectively captures  blending phenomena by incorporating linguistic con- straints.", "labels": [], "entities": []}]}