{"title": [{"text": "NEWSROOM: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies", "labels": [], "entities": [{"text": "NEWSROOM: A Dataset", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.7616107761859894}]}], "abstractContent": [{"text": "We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications.", "labels": [], "entities": []}, {"text": "Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles.", "labels": [], "entities": [{"text": "summarization", "start_pos": 130, "end_pos": 143, "type": "TASK", "confidence": 0.9673439264297485}]}, {"text": "In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates.", "labels": [], "entities": []}, {"text": "We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges.", "labels": [], "entities": [{"text": "NEWSROOM summaries", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.5830288231372833}]}, {"text": "The dataset is available online at summari.es.", "labels": [], "entities": [{"text": "summari.es", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.9263628125190735}]}], "introductionContent": [{"text": "The development of learning methods for automatic summarization is constrained by the limited high-quality data available for training and evaluation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.7864584922790527}]}, {"text": "Large datasets have driven rapid improvement in other natural language generation tasks, such as machine translation, where data size and diversity have proven critical for modeling the alignment between source and target texts.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.7527985771497091}, {"text": "machine translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.8241422176361084}]}, {"text": "Similar challenges exist in summarization, with the additional complications introduced by the length of source texts and the diversity of summarization strategies used by writers.", "labels": [], "entities": [{"text": "summarization", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9935278296470642}, {"text": "summarization", "start_pos": 139, "end_pos": 152, "type": "TASK", "confidence": 0.9613797068595886}]}, {"text": "Access to large-scale high-quality data is an essential prerequisite for making substantial progress in summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.9836174249649048}]}, {"text": "In this paper, we present NEWSROOM, a dataset with 1.3 million news articles and human-written summaries.", "labels": [], "entities": [{"text": "NEWSROOM", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.5565602779388428}]}, {"text": "NEWSROOM's summaries were written by authors and editors in the newsrooms of news, sports, entertainment, financial, and other publications.", "labels": [], "entities": [{"text": "NEWSROOM's summaries were written by authors and editors in the newsrooms of news, sports, entertainment, financial, and other publications", "start_pos": 0, "end_pos": 139, "type": "TASK", "confidence": 0.6691032474239668}]}, {"text": "The summaries were published with articles as HTML metadata for social media services and Abstractive Summary: South African photographer Anton Hammerl, missing in Libya since April 4th, was killed in Libya more than a month ago.", "labels": [], "entities": [{"text": "Abstractive", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.9971362352371216}]}, {"text": "NEWSROOM summaries are written by humans, for common readers, and with the explicit purpose of summarization.", "labels": [], "entities": [{"text": "NEWSROOM summaries", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7373561263084412}, {"text": "summarization", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.9874024391174316}]}, {"text": "As a result, NEWSROOM is a nearly two decade-long snapshot representing how singledocument summarization is used in practice across a variety of sources, writers, and topics.", "labels": [], "entities": [{"text": "NEWSROOM", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.7841455340385437}]}, {"text": "Identifying large, high-quality resources for summarization has called for creative solutions in the past.", "labels": [], "entities": [{"text": "summarization", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.9933733940124512}]}, {"text": "This includes using news headlines as summaries of article prefixes, concatenating bullet points as summaries (, or using librarian archival summaries.", "labels": [], "entities": []}, {"text": "While these solutions provide large scale data, it comes at the cost of how well they reflect the summarization problem or their focus on very specific styles of summarizations, as we discuss in Section 4.", "labels": [], "entities": [{"text": "summarization", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.9848040342330933}]}, {"text": "NEWSROOM is distinguished from these resources in its combination of size and diversity.", "labels": [], "entities": [{"text": "NEWSROOM", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.915944516658783}]}, {"text": "The summaries were written with the explicit goal of concisely summarizing news articles over almost two decades.", "labels": [], "entities": [{"text": "summarizing news articles", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.8705670237541199}]}, {"text": "Rather than rely on a single source, the dataset includes summaries from 38 major publishers.", "labels": [], "entities": []}, {"text": "This diversity of sources and time span translate into a diversity of summarization styles.", "labels": [], "entities": []}, {"text": "We explore NEWSROOM to better understand the dataset and how summarization is used in practice by newsrooms.", "labels": [], "entities": [{"text": "summarization", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9785404205322266}]}, {"text": "Our analysis focuses on a key dimension, extractivenss and abstractiveness: extractive summaries frequently borrow words and phrases from their source text, while abstractive summaries describe the contents of articles primarily using new language.", "labels": [], "entities": []}, {"text": "We develop measures designed to quantify extractiveness and use these measures to subdivide the data into extractive, mixed, and abstractive subsets, as shown in, displaying the broad set of summarization techniques practiced by different publishers.", "labels": [], "entities": []}, {"text": "Finally, we analyze the performance of three summarization models as baselines for NEWSROOM to better understand the challenges the dataset poses.", "labels": [], "entities": [{"text": "NEWSROOM", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.5847943425178528}]}, {"text": "In addition to automated ROUGE evaluation, we design and execute a benchmark human evaluation protocol to quantify the output summaries relevance and quality.", "labels": [], "entities": [{"text": "ROUGE evaluation", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.8839642405509949}]}, {"text": "Our experiments demonstrate that NEWS-ROOM presents an open challenge for summarization systems, while providing a large resource to enable data-intensive learning methods.", "labels": [], "entities": [{"text": "NEWS-ROOM", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.6909740567207336}, {"text": "summarization", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.9902665019035339}]}, {"text": "The dataset and evaluation protocol are available online at summari.es.", "labels": [], "entities": []}], "datasetContent": [{"text": "There area several frequently used summarization datasets.", "labels": [], "entities": [{"text": "summarization", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9684767127037048}]}, {"text": "Listed in are examples from four datasets.", "labels": [], "entities": []}, {"text": "The examples are chosen to be representative: they have scores within 5% of their dataset average across our analysis measures (Section 4).", "labels": [], "entities": []}, {"text": "To illustrate the extractive and abstractive nature of summaries, we underline multi-word phrases shared between the article and summary, and italicize words used only in the summary.", "labels": [], "entities": [{"text": "summaries", "start_pos": 55, "end_pos": 64, "type": "TASK", "confidence": 0.9646411538124084}]}, {"text": "Our scraping and extraction process resulted in a set of 1,321,995 article-summary pairs.", "labels": [], "entities": []}, {"text": "Simple dataset statistics are shown in.", "labels": [], "entities": []}, {"text": "The data are divided into training (76%), development (8%), test (8%), and unreleased test (8%) datasets using a hash function of the article URL.", "labels": [], "entities": []}, {"text": "We use the articles' Archive.org URLs for lightweight distribution of the data.", "labels": [], "entities": []}, {"text": "Archive.org is an ideal platform for distributing the data, encouraging its users to scrape its resources.", "labels": [], "entities": [{"text": "Archive.org", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9007125496864319}]}, {"text": "We provide the extraction and analysis scripts used during data collection for reproducing the full dataset from the URL list.", "labels": [], "entities": []}, {"text": "We use density, coverage, and compression to understand the distribution of human summarization techniques across different sources.", "labels": [], "entities": []}, {"text": "shows the distributions of summaries for different domains in the NEWSROOM dataset, along with three major existing summarization datasets:, CNN / Daily Mail, and the New York Times Corpus.", "labels": [], "entities": [{"text": "NEWSROOM dataset", "start_pos": 66, "end_pos": 82, "type": "DATASET", "confidence": 0.9713203012943268}, {"text": "CNN / Daily Mail", "start_pos": 141, "end_pos": 157, "type": "DATASET", "confidence": 0.9448758512735367}, {"text": "New York Times Corpus", "start_pos": 167, "end_pos": 188, "type": "DATASET", "confidence": 0.723897248506546}]}, {"text": "Publication Diversity Each NEWSROOM publication shows a unique distribution of summaries mixing extractive and abstractive strategies in varying amounts.", "labels": [], "entities": [{"text": "NEWSROOM publication", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.8557252585887909}]}, {"text": "For example, the third entry on the top row shows the summarization strategy used by BuzzFeed.", "labels": [], "entities": [{"text": "summarization", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.9835904240608215}]}, {"text": "The density (y-axis) is relatively low, meaning BuzzFeed summaries are unlikely to include long extractive fragments.", "labels": [], "entities": [{"text": "BuzzFeed summaries", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.9210380613803864}]}, {"text": "While the coverage (x-axis) is more varied, BuzzFeed's coverage tends to be lower, indicating that it frequently uses novel words in summaries.", "labels": [], "entities": []}, {"text": "The publication plots in the figure are sorted by median compression ratio.", "labels": [], "entities": [{"text": "median compression ratio", "start_pos": 50, "end_pos": 74, "type": "METRIC", "confidence": 0.7693988879521688}]}, {"text": "We observe that publications with lower compression ratio (top-left of the figure) exhibit higher diversity along both dimensions of extractiveness.", "labels": [], "entities": [{"text": "compression ratio", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.9727623760700226}]}, {"text": "However, as the median compression ratio increases, the distributions become more con-  We study model performance of NEWSROOM, CNN / Daily Mail, and the combined DUC 2003 and 2004 datasets.", "labels": [], "entities": [{"text": "NEWSROOM", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.9594840407371521}, {"text": "CNN / Daily Mail", "start_pos": 128, "end_pos": 144, "type": "DATASET", "confidence": 0.8921589106321335}, {"text": "DUC 2003 and 2004 datasets", "start_pos": 163, "end_pos": 189, "type": "DATASET", "confidence": 0.9487594127655029}]}, {"text": "We use the five systems described in Section 5, including the extractive oracle.", "labels": [], "entities": []}, {"text": "We also evaluate the systems using subsets of: ROUGE-1, ROUGE-2, and ROUGE-L scores for baselines and systems on two common existing datasets, the combined datasets and CNN / Daily Mail dataset, and the released (T) and unreleased (U) test sets of NEWSROOM.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9640450477600098}, {"text": "ROUGE-2", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.89151531457901}, {"text": "ROUGE-L", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9727822542190552}, {"text": "CNN / Daily Mail dataset", "start_pos": 169, "end_pos": 193, "type": "DATASET", "confidence": 0.902859878540039}, {"text": "NEWSROOM", "start_pos": 248, "end_pos": 256, "type": "DATASET", "confidence": 0.9336404204368591}]}, {"text": "The best results for non-baseline systems in the lower parts of the table are in bold.", "labels": [], "entities": []}, {"text": "NEWSROOM to characterize the sensitivity of systems to different levels of extractiveness in reference summaries.", "labels": [], "entities": [{"text": "NEWSROOM", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7808055877685547}]}, {"text": "We use the F 1 -score variants of ROUGE-1, ROUGE-2, and ROUGE-L to account for different summary lengths.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9584133177995682}, {"text": "ROUGE-1", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9791141152381897}, {"text": "ROUGE-2", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.8817855715751648}, {"text": "ROUGE-L", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.964870810508728}]}, {"text": "ROUGE scores are computed with the default configuration of the Lin (2004b) ROUGE v1.5.5 reference implementation.", "labels": [], "entities": [{"text": "Lin (2004b) ROUGE v1.5.5 reference", "start_pos": 64, "end_pos": 98, "type": "DATASET", "confidence": 0.7979695669242314}]}, {"text": "Input article text and reference summaries for all systems are tokenized using the Stanford CoreNLP tokenizer ().", "labels": [], "entities": [{"text": "Stanford CoreNLP tokenizer", "start_pos": 83, "end_pos": 109, "type": "DATASET", "confidence": 0.9283344944318136}]}, {"text": "shows results for summarization systems on DUC, CNN / Daily Mail, and NEWS-ROOM.", "labels": [], "entities": [{"text": "summarization", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.9696379899978638}, {"text": "DUC", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.9787738919258118}, {"text": "CNN / Daily Mail", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.9139807373285294}, {"text": "NEWS-ROOM", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.9410811066627502}]}, {"text": "In nearly all cases, the fully extractive Lede-3 baseline produces the most successful summaries, with the exception of the relatively extractive DUC.", "labels": [], "entities": [{"text": "Lede-3 baseline", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.891544371843338}, {"text": "DUC", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.8746944069862366}]}, {"text": "Among models, NEWSROOMtrained Pointer-N performs best on all datasets other than CNN / Daily Mail, an out-of-domain dataset.", "labels": [], "entities": [{"text": "CNN / Daily Mail", "start_pos": 81, "end_pos": 97, "type": "DATASET", "confidence": 0.9403957426548004}]}, {"text": "Pointer-C, which has access to only a limited subset of NEWSROOM, performs worse than Pointer-N on average.", "labels": [], "entities": [{"text": "Pointer-C", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8393006920814514}, {"text": "NEWSROOM", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.8294369578361511}]}, {"text": "However, despite not being trained on CNN / Daily Mail, Pointer-S outperforms Pointer-C on its own data under ROUGE-N and is competitive under ROUGE-L.", "labels": [], "entities": [{"text": "CNN / Daily Mail", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.9360930621623993}, {"text": "ROUGE-N", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.7938807606697083}, {"text": "ROUGE-L", "start_pos": 143, "end_pos": 150, "type": "DATASET", "confidence": 0.9220108985900879}]}, {"text": "Finally, both Pointer-N and Pointer-S outperform other systems and baselines on DUC, whereas Pointer-C does not outperform Lede-3.", "labels": [], "entities": [{"text": "DUC", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.8404046893119812}]}, {"text": "shows development results on the NEWSROOM data for different level of extractiveness.", "labels": [], "entities": [{"text": "NEWSROOM data", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9516631066799164}]}, {"text": "Pointer-N outperforms the remaining models across all extractive subsets of NEWSROOM and, in the case of the abstractive subset, exceeds the performance of Lede-3.", "labels": [], "entities": [{"text": "NEWSROOM", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.8173443675041199}]}, {"text": "The success of Pointer-N and Pointer-S in generalizing and outperforming models on DUC and CNN / Daily Mail indicates the usefulness of NEWSROOM in generalizing to out-of-domain data.", "labels": [], "entities": [{"text": "DUC and CNN / Daily Mail", "start_pos": 83, "end_pos": 107, "type": "DATASET", "confidence": 0.8653662800788879}]}, {"text": "Similar subset analysis for our other two measures, coverage and compression, are included in the supplementary material.", "labels": [], "entities": [{"text": "coverage", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9959074258804321}, {"text": "compression", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.9872952103614807}]}, {"text": "ROUGE scores systems using frequencies of shared n-grams.", "labels": [], "entities": []}, {"text": "Evaluating systems with ROUGE alone biases scoring against abstractive systems, which rely more on paraphrasing.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.8669794797897339}]}, {"text": "To overcome this limitation, we provide human evaluation of the different systems on NEWSROOM.", "labels": [], "entities": [{"text": "NEWSROOM", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.9314638376235962}]}, {"text": "While human evaluation is still uncommon in summarization work, developing a benchmark dataset presents an opportunity for developing an accompanying protocol for human evaluation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9871980547904968}]}, {"text": "Our evaluation method is centered around three objectives: (1) distinguishing between syntactic and semantic summarization quality, (2) providing a reliable (consistent and replicable) measurement, and    measure can be applied to other models or summarization datasets.", "labels": [], "entities": []}, {"text": "We select two semantic and two syntactic dimensions for evaluation based on experiments with evaluation tasks by and.", "labels": [], "entities": []}, {"text": "The two semantic dimensions, summary informativeness (INF) and relevance (REL), measure whether the systemgenerated text is useful as a summary, and appropriate for the source text, respectively.", "labels": [], "entities": [{"text": "relevance (REL)", "start_pos": 63, "end_pos": 78, "type": "METRIC", "confidence": 0.9641222655773163}]}, {"text": "The two syntactic dimensions, fluency (FLU) and coherence (COH), measure whether individual sentences or phrases of the summary are well-written and whether the summary as a whole makes sense respectively.", "labels": [], "entities": [{"text": "FLU)", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.8963097929954529}, {"text": "coherence (COH)", "start_pos": 48, "end_pos": 63, "type": "METRIC", "confidence": 0.8794769644737244}]}, {"text": "Evaluation was performed on 60 summaries, 20 from each extractive NEWSROOM subset.", "labels": [], "entities": [{"text": "NEWSROOM subset", "start_pos": 66, "end_pos": 81, "type": "DATASET", "confidence": 0.8040096163749695}]}, {"text": "Each system-article pair was evaluated by three unique raters.", "labels": [], "entities": []}, {"text": "Exact prompts given to raters for each dimension are shown in. shows the mean score given to each system under each of the four dimensions, as well as the mean overall score (rightmost column).", "labels": [], "entities": []}, {"text": "No summarization system exceeded the scores given to the Lede-3 baseline.", "labels": [], "entities": [{"text": "Lede-3 baseline", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.9802940487861633}]}, {"text": "However, the extractive oracle designed to maximize n-gram based evaluation performed worse than the majority of systems under human evaluation.", "labels": [], "entities": []}, {"text": "While the fully abstractive Abs-N model performed very poorly under automatic evaluation, it fared slightly better when scored by humans.", "labels": [], "entities": []}, {"text": "TextRank received the highest overall score.", "labels": [], "entities": [{"text": "TextRank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9748519659042358}]}, {"text": "TextRank generates full sentences extracted from the article, and raters preferred TextRank primarily for its fluency and coherence.", "labels": [], "entities": [{"text": "TextRank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8982520699501038}, {"text": "TextRank", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.9253427982330322}]}, {"text": "The pointer-generator models do not have this advantage, and raters did not find the pointer-generator models to be as syntactically sound as TextRank.", "labels": [], "entities": [{"text": "TextRank", "start_pos": 142, "end_pos": 150, "type": "DATASET", "confidence": 0.9273521900177002}]}, {"text": "However, raters preferred the informativeness and relevance of the Pointer-S and Pointer-N models, though not the Pointer-C model, over TextRank.", "labels": [], "entities": [{"text": "TextRank", "start_pos": 136, "end_pos": 144, "type": "DATASET", "confidence": 0.9171152114868164}]}, {"text": "In Section 4, we discuss three measures of summarization diversity: coverage, density, and compression.", "labels": [], "entities": [{"text": "summarization diversity", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.9150018393993378}, {"text": "coverage", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.970535159111023}, {"text": "compression", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.9536001682281494}]}, {"text": "In addition to quantifying diversity of summarization strategies, these measures are helpful for system error analysis.", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9582887887954712}, {"text": "system error analysis", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.6599121391773224}]}, {"text": "We use the density measurement to understand how system performance varies when compared against references using different extractive strategies by subdividing NEWSROOM into three subsets by extractiveness and evaluating using ROUGE on each.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 228, "end_pos": 233, "type": "METRIC", "confidence": 0.988235354423523}]}, {"text": "We show here a similar analysis using the remaining two measures, coverage and compression.", "labels": [], "entities": [{"text": "coverage", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9970566034317017}, {"text": "compression", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9701031446456909}]}, {"text": "Results for subsets based on coverage and compression are shown in LOW COVERAGE MEDIUM HIGH COVERAGE: Performance of the baselines and systems on the three compression subsets of the NEWSROOM development set.", "labels": [], "entities": [{"text": "LOW COVERAGE MEDIUM HIGH COVERAGE", "start_pos": 67, "end_pos": 100, "type": "METRIC", "confidence": 0.7607190489768982}, {"text": "NEWSROOM development set", "start_pos": 183, "end_pos": 207, "type": "DATASET", "confidence": 0.9287163416544596}]}, {"text": "Article-summary pairs with low compression have longer reference summaries with respect to their texts.", "labels": [], "entities": []}, {"text": "Article-summary pairs with high compression have shorter reference summaries with respect to their texts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: ROUGE-1, ROUGE-2, and ROUGE-L scores for baselines and systems on two common existing datasets,  the combined", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9555744528770447}, {"text": "ROUGE-2", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9273852705955505}, {"text": "ROUGE-L", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9744459986686707}]}, {"text": " Table 3: Performance of the baselines and systems on the three extractiveness subsets of the NEWSROOM devel- opment set, and the overall scores of systems on the full development set (D). The best results for non-baseline  systems in the lower parts of the table are in bold.", "labels": [], "entities": [{"text": "NEWSROOM devel- opment set", "start_pos": 94, "end_pos": 120, "type": "DATASET", "confidence": 0.845967960357666}]}, {"text": " Table 4: The prompts given to Amazon Mechanical  Turk crowdworkers for evaluating each summary.", "labels": [], "entities": []}, {"text": " Table 5: Average performance of systems as scored by  human evaluators. Each summary was scored by three  different evaluators. Dimensions, from left to right: in- formativeness, relevance, fluency, and coherence, and  a mean of the four dimensions for each system.", "labels": [], "entities": []}, {"text": " Table 6: Performance of the baselines and systems on the three coverage subsets of the NEWSROOM development  set. Article-summary pairs with low coverage have reference summaries that borrow words less frequently from  their texts and contain more novel words and phrases. Article-summary pairs with high coverage borrow more  words from their text and include fewer novel words and phrases.", "labels": [], "entities": [{"text": "NEWSROOM development  set", "start_pos": 88, "end_pos": 113, "type": "DATASET", "confidence": 0.9182801445325216}]}, {"text": " Table 7: Performance of the baselines and systems on the three compression subsets of the NEWSROOM devel- opment set. Article-summary pairs with low compression have longer reference summaries with respect to their  texts. Article-summary pairs with high compression have shorter reference summaries with respect to their texts.", "labels": [], "entities": [{"text": "NEWSROOM devel- opment set", "start_pos": 91, "end_pos": 117, "type": "DATASET", "confidence": 0.8786911845207215}]}]}