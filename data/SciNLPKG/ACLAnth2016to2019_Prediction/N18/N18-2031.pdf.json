{"title": [{"text": "Frustratingly Easy Meta-Embedding -Computing Meta-Embeddings by Averaging Source Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Creating accurate meta-embeddings from pre-trained source embeddings has received attention lately.", "labels": [], "entities": []}, {"text": "Methods based on global and locally-linear transformation and concatena-tion have shown to produce accurate meta-embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we show that the arithmetic mean of two distinct word embedding sets yields a performant meta-embedding that is comparable or better than more complex meta-embedding learning methods.", "labels": [], "entities": []}, {"text": "The result seems counter-intuitive given that vector spaces in different source embeddings are not comparable and cannot be simply averaged.", "labels": [], "entities": []}, {"text": "We give insight into why averaging can still produce accurate meta-embedding despite the incomparability of the source vector spaces.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed vector representations of words, henceforth referred to as word embeddings, have been shown to exhibit strong performance on a variety of NLP tasks.", "labels": [], "entities": []}, {"text": "Methods for producing word embedding sets exploit the distributional hypothesis to infer semantic similarity between words within large bodies of text, in the process they have been found to additionally capture more complex linguistic regularities, such as analogical relationships (.", "labels": [], "entities": []}, {"text": "A variety of methods now exist for the production of word embeddings).", "labels": [], "entities": []}, {"text": "Comparative work has illustrated a variation in performance between methods across evaluative tasks.", "labels": [], "entities": []}, {"text": "Methods of \"meta-embedding\", as first proposed by, aim to conduct a complementary combination of information from an ensemble of distinct word embedding sets, each trained using different methods, and resources, to yield an embedding set with improved overall quality.", "labels": [], "entities": []}, {"text": "Several such methods have been proposed.", "labels": [], "entities": []}, {"text": "1TON, takes an ensemble of K pre-trained word embedding sets, and employs a linear neural network to learn a set of meta-embeddings along with K global projection matrices, such that through projection, for every word in the meta-embedding set, we can recover its corresponding vector within each source word embedding set.", "labels": [], "entities": [{"text": "1TON", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8458189368247986}]}, {"text": "1TON+, extends this method by predicting embeddings for words not present within the intersection of the source word embedding sets.", "labels": [], "entities": []}, {"text": "An unsupervised locally linear meta-embedding approach has since been taken (, for each source embedding set, for each word; a representation as a linear combination of its nearest neighbours is learnt.", "labels": [], "entities": []}, {"text": "The local reconstructions within each source embedding set are then projected to a common meta-embedding space.", "labels": [], "entities": []}, {"text": "The simplest approach considered to date, has been to concatenate the word embeddings across the source sets.", "labels": [], "entities": []}, {"text": "Despite its simplicity, concatenation has been used to provide a good baseline of performance for metaembedding.", "labels": [], "entities": []}, {"text": "A method which has not yet been proposed is to conduct a direct averaging of embeddings.", "labels": [], "entities": []}, {"text": "The validity of this approach may perhaps not seem obvious, owing to the fact that no correspondence exists between the dimensions of separately trained word embedding sets.", "labels": [], "entities": []}, {"text": "In this paper we first provide some analysis and justification that, despite this dimensional disparity, averaging can provide an approximation of the performance of concatenation without increasing the dimension of the embeddings.", "labels": [], "entities": []}, {"text": "We give empirical results demonstrating the quality of average meta-embeddings.", "labels": [], "entities": []}, {"text": "We make a point of comparison to concatenation since it is the most comparable in terms of simplicity, whilst also providing a good baseline of performance on evaluative tasks.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.985438346862793}]}, {"text": "Our aim is to highlight the validity of averaging across distinct word embedding sets, such that it maybe considered as a tool in future meta-embedding endeavours.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first empirically test our theory that word embeddings are sufficiently random and high dimensional, such that they are approximately all orthogonal to each other.", "labels": [], "entities": []}, {"text": "We then present an empirical evaluation of the performance of the meta-embeddings produced through averaging, and compare against concatenation.", "labels": [], "entities": []}, {"text": "We use the following pre-trained embedding sets that have been used in prior work on metaembedding learning) for experimentation.", "labels": [], "entities": []}, {"text": "1,917,494 word embeddings of dimension 300.", "labels": [], "entities": []}, {"text": "Phrase embeddings discarded, leaving 929,922 word embeddings of dimension 300.", "labels": [], "entities": []}, {"text": "246,122 hierarchical log-bilinear word embeddings of dimension 100.", "labels": [], "entities": []}, {"text": "Note that the purpose of this experiment is not to compare against previously proposed metaembedding learning methods, but to empirically verify averaging as a meta-embedding method and validate the assumptions behind the theoretical analysis.", "labels": [], "entities": []}, {"text": "By using three pre-trained word embeddings with different dimensionalities and empirical accuracies, we can evaluate the averagingbased meta-embeddings in a robust manner.", "labels": [], "entities": []}, {"text": "We pad HLBL embeddings to the rear with 200 zero-entries to bring their dimension up to 300.", "labels": [], "entities": [{"text": "dimension", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9832547903060913}]}, {"text": "For GloVe, we 2 normalise each dimension of the embedding across the vocabulary, as recommended by the authors.", "labels": [], "entities": []}, {"text": "Every individual word embedding from each embedding set is then 2 -normalised.", "labels": [], "entities": []}, {"text": "The proposed averaging operation, as well as concatenation, operate only on the intersection of these embeddings.", "labels": [], "entities": []}, {"text": "The intersectional vocabularies GloVe \u2229 CBOW, GloVe \u2229 HLBL, and CBOW \u2229 HLBL contain 154,076; 90,254; and 140,479 word embeddings respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Observed distribution parameters.", "labels": [], "entities": [{"text": "Observed", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.941936731338501}]}, {"text": " Table 2: Results on word similarity, and analogical  tasks. Best performances bolded per task. Dimension- ality of the meta embedding is shown next to the source  embedding names.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7531333267688751}]}]}