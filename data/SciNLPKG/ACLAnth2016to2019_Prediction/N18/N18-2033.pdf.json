{"title": [{"text": "Lexical Substitution for Evaluating Compositional Distributional Models", "labels": [], "entities": [{"text": "Lexical Substitution", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.784397691488266}, {"text": "Evaluating Compositional Distributional Models", "start_pos": 25, "end_pos": 71, "type": "TASK", "confidence": 0.8958150446414948}]}], "abstractContent": [{"text": "Compositional Distributional Semantic Models (CDSMs) model the meaning of phrases and sentences in vector space.", "labels": [], "entities": []}, {"text": "They have been predominantly evaluated on limited, artificial tasks such as semantic sentence similarity on hand-constructed datasets.", "labels": [], "entities": [{"text": "semantic sentence similarity", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.5998834172884623}]}, {"text": "This paper argues for lexical substitution as a means to evaluate CDSMs.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7232597768306732}]}, {"text": "Lexical substitution is a more natural task, enables us to evaluate meaning composition at the level of individual words, and provides a common ground to compare CDSMs with dedicated lexical substitution models.", "labels": [], "entities": [{"text": "Lexical substitution", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.897076815366745}]}, {"text": "We create a lexical substitution dataset for CDSM evaluation from an English-language corpus with manual \"all-words\" lexical substitution annotation.", "labels": [], "entities": [{"text": "CDSM evaluation", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.699864000082016}]}, {"text": "Our experiments indicate that the Practical Lexical Function CDSM outperforms simple component-wise CDSMs and performs on par with the con-text2vec lexical substitution model using the same context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Compositional Distributional Semantics Models (CDSMs) compute phrase meaning in semantic space as a function of the meanings of the phrase constituents ( ).", "labels": [], "entities": []}, {"text": "The most basic CDSMs represent words as vectors and compose phrase vectors by component-wise operations of the constituent vectors (.", "labels": [], "entities": []}, {"text": "More complex models represent predicates with matrices and tensors (.", "labels": [], "entities": []}, {"text": "Given the large number of different CDSMs proposed in the literature, meaningful evaluation becomes crucial.", "labels": [], "entities": []}, {"text": "The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity).", "labels": [], "entities": []}, {"text": "Only a handful of studies pursued other evaluation tasks, such as textual entailment or sentiment analysis.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.6980372071266174}, {"text": "sentiment analysis", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.9435053467750549}]}, {"text": "Arguably, phrase similarity evaluation has three major problems.", "labels": [], "entities": [{"text": "phrase similarity evaluation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8986221353212992}]}, {"text": "First, the task is affected by the general limitations of rating scales, such as inconsistencies in annotations, scale region bias, and fixed granularity (.", "labels": [], "entities": []}, {"text": "Phrase similarity datasets used for CDSM evaluation demonstrate slight to fair inter-annotator agreement, as well as overlap between groups of items rated as low and high in similarity.", "labels": [], "entities": [{"text": "CDSM evaluation", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.8632691204547882}]}, {"text": "Secondly, phrase similarity is a task that is rather difficult to put down precisely, especially for long phrases.", "labels": [], "entities": [{"text": "phrase similarity", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7895360887050629}]}, {"text": "Generally, phrases can be (dis)similar in any number of ways.", "labels": [], "entities": []}, {"text": "Annotators commonly agree that some sentence pairs are semantically highly similar (private company files annual account and private company registers annual account, Pickering and Frisson 2001), and others are semantically unrelated (man waves hand vs. employee leaves company).", "labels": [], "entities": []}, {"text": "In contrast, their assessments become less confident for cases like delegate buys land and agent sells property (, where there is a semantic relation other than synonymy.", "labels": [], "entities": []}, {"text": "Similarity is also arguably not a useful measure when sentences are semantically deviant, as it is often the casein the datasets: how similar are private company files annual account and private company smooths annual account?", "labels": [], "entities": [{"text": "Similarity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7746941447257996}]}, {"text": "The third problem is that the most widely used phrase similarity datasets are constructed in a balanced fashion along psycholinguistic principles.", "labels": [], "entities": []}, {"text": "For instance, the adjective-noun-verb-adjectivenoun (\"ANVAN\") dataset, from which the examples above are drawn, was constructed from a set of particularly ambiguous verbs paired with strongly disambiguating contexts.", "labels": [], "entities": []}, {"text": "Such setups often do not 206 correlate well with usefulness on more natural data.", "labels": [], "entities": []}, {"text": "Ina previous study on lexical substitution, found that the advantage of machine learning-based models oversimple baselines was much harder to show on a real-world corpus than on the previously used manually constructed benchmark dataset.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7116473466157913}]}, {"text": "In this paper, we pursue the idea that lexical substitution) is a more suitable evaluation task for CDSMs.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7065222412347794}]}, {"text": "Lexical substitution is the task of finding meaning-preserving substitutes fora target word in context: e.g., the word submits is a legitimate substitute for files in private company files annual account, but not in office clerk files old papers.", "labels": [], "entities": [{"text": "Lexical substitution", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.880389004945755}]}, {"text": "Lexical substitution provides a frame for comparing synonyms in context, and disambiguating the context-appropriate sense of polysemous words.", "labels": [], "entities": [{"text": "Lexical substitution", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9055993258953094}]}, {"text": "Since this is a problem CDSMs can account for, lexical substitution seems a suitable task for testing and comparing different CDSMs.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7472975254058838}]}, {"text": "Additionally, lexical substitution is a more natural task than similarity ratings, it makes it possible to evaluate meaning composition at the level of individual words, and provides a common ground to compare CDSMs with dedicated lexical substitution models.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.8252033293247223}]}], "datasetContent": [{"text": "To perform more realistic evaluations for CDSMs, we would like to test them on a sample of actual human utterances rather than hand-selected examples.", "labels": [], "entities": []}, {"text": "That being said, for the evaluation to be useful, the structures on which we evaluate should be relatively uniform and limited, at least initially while moving from artificial to natural datasets.", "labels": [], "entities": []}, {"text": "We thus construct a dataset 1 for English that strikes a balance between these factors: we maintain the adjective-noun-verb-adjective-noun (ANVAN) format, but our phrases are based on corpus sentences, and some or all words in the ANVAN can form the targets of lexical substitution ranking tasks.", "labels": [], "entities": []}, {"text": "The starting point of our corpus construction is the English CoInCo corpus (), which consists of roughly 2,500 sentences taken from the news and fiction section of the MASC corpus and provides manually annotated lexical substitutes for all content words (nouns, adjectives, verbs, and adverbs).", "labels": [], "entities": [{"text": "English CoInCo corpus", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.8284309506416321}, {"text": "MASC corpus", "start_pos": 168, "end_pos": 179, "type": "DATASET", "confidence": 0.9359239637851715}]}, {"text": "We extracted all clauses from CoInCo that met the following requirements: (1) the dependency structure of the clause includes an ANVAN structure; (2) at least one constituent word of the AN-VAN sub-clause has at least two human-provided single-word substitutes that exceeded a minimal frequency threshold (more than 5 occurrences in a large corpus, cf. Section 3); (3) the POS tags were correct.", "labels": [], "entities": []}, {"text": "This resulted in 165 ANVAN phrases, which contained an average of 4.4 (out of 5) target words with substitutes, fora total of 732 target words for lexical substitution.", "labels": [], "entities": []}, {"text": "An issue that required additional consideration was the adjective positions of the ANVANs.", "labels": [], "entities": [{"text": "ANVANs", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.7003977298736572}]}, {"text": "In the CoInCo, we found a substantial number of nounnoun compounds whose modifiers were tagged as adjectives.", "labels": [], "entities": [{"text": "CoInCo", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.9613147974014282}]}, {"text": "Conversely, many adjective modifiers in the corpus were substituted with nouns by human annotators.", "labels": [], "entities": []}, {"text": "To account for this variability, we extended the ANVAN schema conservatively by allowing nouns to fill the A position if it was observed in the large corpus robustly as a modifier (i.e., as part of at least 100 N-N bigram types, each of which occurred at least 300 times).", "labels": [], "entities": []}, {"text": "For each of the 732 target words, we constructed a lexical substitution query by pairing the target with two correct substitutes and two confounders (see).", "labels": [], "entities": []}, {"text": "Since substitutes provided by more annotators in CoInCo tend to be more reliable, we picked the two most frequently given substitutes for each target.", "labels": [], "entities": [{"text": "CoInCo", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.9175172448158264}]}, {"text": "In the case of ties, we chose the lemma with the highest corpus frequency.", "labels": [], "entities": []}, {"text": "To acquire challenging confounders, we retrieved the 20 most similar lemmas with the same part of speech for each target (according to the unigram space; cf. Section 3) and then eliminated all annotator-provided substitutes for this target.", "labels": [], "entities": []}, {"text": "From the remainder, we chose the two most closely matched by corpus frequency to the frequencies of the two chosen annotator-provided substitutes.", "labels": [], "entities": []}, {"text": "Given the relatively high number of human substitutes in CoInCo, this results in highly similar, but contextually inappropriate confounders.", "labels": [], "entities": []}, {"text": "Finally, the acquired confounders were manually checked to make sure that the automatic selection process did not yield a context-appropriate substitute or a semantically unrelated term.", "labels": [], "entities": []}, {"text": "In such cases, the next best candidate (by lemma similarity and frequency) was chosen.", "labels": [], "entities": [{"text": "similarity", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.701188325881958}]}, {"text": "207 target construction arm build large airfield substitute 1 construction branch build large airfield substitute 2 construction part build large airfield confounder 1 construction back build large airfield confounder 2 construction hand build large airfield: ANVAN-LS lexical substitution example  Task and Evaluation.", "labels": [], "entities": [{"text": "ANVAN-LS lexical substitution", "start_pos": 260, "end_pos": 289, "type": "TASK", "confidence": 0.563833604256312}]}, {"text": "We evaluate models on ANVAN-LS in the form of a ranking task for each query: models are supposed to rank the correct substitutes for each target higher than its confounders.", "labels": [], "entities": []}, {"text": "Our evaluation measure is the mean average precision (MAP) of all queries.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 30, "end_pos": 58, "type": "METRIC", "confidence": 0.9276265601317087}]}, {"text": "In our case, where k is the rank in the 4-item list l of substitution candidates, Pl (k) is the precision at cut-off point kin list l, the \u2206r l (k) is the change in recall from items k\u22121 to kin l, and N is the total number of ANVAN queries.", "labels": [], "entities": [{"text": "Pl (k)", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.926936998963356}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9990204572677612}, {"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9755810499191284}]}, {"text": "We calculate MAP both overall, and by target positions in ANVAN, to obtain more detailed insights into performance depending on part of speech and word position.", "labels": [], "entities": [{"text": "MAP", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.8489614725112915}, {"text": "ANVAN", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.5541296601295471}]}, {"text": "Following Baroni and Zamparelli (2010) and, we use a concatenation of the ukWaC, BNC, and English Wikipedia corpora (around 2.8G words).", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9856268763542175}, {"text": "BNC", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.674687385559082}, {"text": "English Wikipedia corpora", "start_pos": 90, "end_pos": 115, "type": "DATASET", "confidence": 0.8800195058186849}]}, {"text": "We build a square co-occurrence matrix, using the complete vocabulary of our ANVAN sentence dataset, and a set of the most frequent content words in our corpus, fora total of 30K words.", "labels": [], "entities": [{"text": "ANVAN sentence dataset", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.786281406879425}]}, {"text": "In addition, we built two co-occurrence matrices of bigrams, composed of all predicates (adjectives and verbs) in our vocabulary and their most frequent noun co-occurrents, as observed in the corpus, with a threshold of 300.", "labels": [], "entities": []}, {"text": "The bigrams were observed in co-occurrence with the aforementioned 30K vocabulary and most frequent corpus terms, resulting in a 650K-by-30K matrix for A-N and N-N bigrams and a 620K-by-30K matrix for V-N bigrams.", "labels": [], "entities": []}, {"text": "For all three matrices, the 3-word-window cooccurrence counts were transformed with PPMI and reduced to 300 dimensions with SVD.", "labels": [], "entities": []}, {"text": "All models were built with DISSECT ().", "labels": [], "entities": [{"text": "DISSECT", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.7435685992240906}]}, {"text": "We consider two component-wise CDSMs: the simple additive and multiplicative models, defined as), which represents predicates (verbs and adjectives in our case) as matrices to be multiplied with vector representations of their nominal arguments.", "labels": [], "entities": []}, {"text": "More specifically, when applied to an ANVAN sentence (such as pointed ear catch sharp sound), the PLF model incorporates vector representations for each of the five constituent words, along with an adjective matrix for pointed and sharp, as well as verb subject and verb object matrices for the verb and its subject and object arguments.", "labels": [], "entities": []}, {"text": "An example of composition fora verb with its subject and object arguments is given in.", "labels": [], "entities": []}, {"text": "Formally, the standard PLF (PLF Paperno ) defines the composition for ANVAN-style sentences as: All CDSMs rank the four candidates for each target (cf.) by comparing the vector for the original sentence against four sentences in which the target is replaced by the two correct and two incorrect substitutes.", "labels": [], "entities": [{"text": "PLF (PLF Paperno )", "start_pos": 23, "end_pos": 41, "type": "DATASET", "confidence": 0.6909047901630402}]}, {"text": "We use the raw dot product as similarity measure, following, to boost frequent candidates.", "labels": [], "entities": []}, {"text": "As competitor, we consider a dedicated lexical substitution model, namely context2vec (.", "labels": [], "entities": []}, {"text": "Since it has demonstrated state-of-the-art performance on lexical substitution and word sense disambiguation tasks, it is a suitable competitor model for CDSMs on a similar problem.", "labels": [], "entities": [{"text": "lexical substitution and word sense disambiguation tasks", "start_pos": 58, "end_pos": 114, "type": "TASK", "confidence": 0.7022005617618561}]}, {"text": "Context2vec uses word embeddings to compute a set of viable substitutes given a context, using a bidirectional LSTM recurrent neural network to build a sentential context representation.", "labels": [], "entities": []}, {"text": "We work with two instantiations: first, using only ANVAN as context (C2V ANVAN ), and second, using the full CoInCo sentence from which the ANVAN was extracted (C2V Sent ).", "labels": [], "entities": []}, {"text": "The first model is directly comparable to the CDSMs in that it uses the same context information, while the second one enables us to gauge to what extent the models can benefit from a richer context.", "labels": [], "entities": []}, {"text": "We let context2vec generate 1000 substitutions for each target word.", "labels": [], "entities": []}, {"text": "If any substitution candidates were not included in this list, the missing items were defined to be tied at the last position, and the AP was defined as the MAP of all permutations of the missing items with respect to their ranking.", "labels": [], "entities": [{"text": "AP", "start_pos": 135, "end_pos": 137, "type": "METRIC", "confidence": 0.998171329498291}, {"text": "MAP", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.9983822107315063}]}, {"text": "Two baselines are defined to compare our models against.", "labels": [], "entities": []}, {"text": "The first is the random baseline (Random): the MAP of all possible rankings of two relevant and two irrelevant items, equal to 0.680.", "labels": [], "entities": [{"text": "random baseline (Random)", "start_pos": 17, "end_pos": 41, "type": "METRIC", "confidence": 0.7353960633277893}, {"text": "MAP", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9941447377204895}]}, {"text": "The second baseline (LemmaSim) ranks the lemma-level similarities of the target word and its substitutes candidates without context.", "labels": [], "entities": []}, {"text": "lists the performance of our experiments on ANVAN-LS, first evaluated for each target word position (top rows) and then averaged across all target positions (bottom row).", "labels": [], "entities": [{"text": "ANVAN-LS", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.6060993075370789}]}, {"text": "The rightmost column shows the average performance of all tested models.", "labels": [], "entities": []}, {"text": "Even though there is some variance in the numbers between subject-position targets and object-position targets, we see consistent patterns by part of speech: adjectives are easiest to substitute, followed by nouns, while verbs are substantially more difficult.", "labels": [], "entities": []}, {"text": "The difficulty with verbs corresponds to the findings of Medi\u00b4c for Croatian, who proposed a correlation between the syntactic valence of words and their difficulty (but see below).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation results on ANVAN-LS (mean average precision)", "labels": [], "entities": [{"text": "ANVAN-LS", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.7554044723510742}, {"text": "mean average precision", "start_pos": 42, "end_pos": 64, "type": "METRIC", "confidence": 0.8491269946098328}]}]}