{"title": [], "abstractContent": [{"text": "Humor is an essential but most fascinating element in personal communication.", "labels": [], "entities": [{"text": "Humor", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.9614078998565674}]}, {"text": "How to build computational models to discover the structures of humor, recognize humor and even generate humor remains a challenge and there have been yet few attempts on it.", "labels": [], "entities": []}, {"text": "In this paper , we construct and collect four datasets with distinct joke types in both English and Chi-nese and conduct learning experiments on humor recognition.", "labels": [], "entities": [{"text": "humor recognition", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.736117959022522}]}, {"text": "We implement a Convolu-tional Neural Network (CNN) with extensive filter size, number and Highway Networks to increase the depth of networks.", "labels": [], "entities": []}, {"text": "Results show that our model outperforms in recognition of different types of humor with benchmarks collected in both English and Chinese languages on accuracy, precision, and recall in comparison to previous works.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9993411898612976}, {"text": "precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9985529780387878}, {"text": "recall", "start_pos": 175, "end_pos": 181, "type": "METRIC", "confidence": 0.9992777705192566}]}], "introductionContent": [{"text": "Humor, a highly intelligent communicative activity, provokes laughter or provides amusement.", "labels": [], "entities": [{"text": "Humor", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.928246796131134}]}, {"text": "The role that humor plays in life can be viewed as a sociological phenomenon and function.", "labels": [], "entities": []}, {"text": "Proper use of it can help eliminate embarrassment, establish social relationships, create positive affection inhuman social interactions.", "labels": [], "entities": []}, {"text": "If computers can understand humor to some extent, it would facilitate predicting human's intention inhuman conversation, and thereby enhance the proficiency of many machine-human interaction systems.", "labels": [], "entities": [{"text": "predicting human's intention", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.8611376285552979}]}, {"text": "However, to automate the humor recognition is also a very challenging research topic in natural language understanding.", "labels": [], "entities": [{"text": "humor recognition", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7717544138431549}, {"text": "natural language understanding", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.6599163512388865}]}, {"text": "The extent to which a person may sense humor depends on his/her personal background.", "labels": [], "entities": []}, {"text": "For example, young children may favor cartoons while the grownups may feel the humor in cartoons boring.", "labels": [], "entities": []}, {"text": "Also, many types of humor require substantial such external knowledge as irony, wordplay, metaphor and sarcasm.", "labels": [], "entities": []}, {"text": "These factors make the task of automated humor recognition difficult.", "labels": [], "entities": [{"text": "automated humor recognition", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6575183471043905}]}, {"text": "Recently, with the advance of deep learning that allows end-to-end training with big data without human intervention of feature selection, humor recognition becomes promising.", "labels": [], "entities": [{"text": "humor recognition", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.8878656923770905}]}, {"text": "In this work, we propose a convolutional neural network (CNN) with augmentation of both the filter sizes and filter numbers.", "labels": [], "entities": []}, {"text": "We use the architecture called highway network to implement a much more proficient model for humor recognition.", "labels": [], "entities": [{"text": "humor recognition", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.9058662354946136}]}, {"text": "The performance on many benchmarks shows a significant improvement in detecting different humor context genre.", "labels": [], "entities": []}], "datasetContent": [{"text": "Short Jokes dataset, which collected the most amount of jokes among four datasets, are from an open database on a Kaggle project 1 . It contains 231,657 short jokes with no restriction on joke types scraped from various joke websites and length ranging from 10 to 200 characters.", "labels": [], "entities": [{"text": "Short Jokes dataset", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.828151285648346}, {"text": "Kaggle project 1", "start_pos": 114, "end_pos": 130, "type": "DATASET", "confidence": 0.9386784235636393}]}, {"text": "We use it as our positive samples.", "labels": [], "entities": []}, {"text": "For the negative samples, we choose WMT16 2 English news crawl as our non-humorous data resource.", "labels": [], "entities": [{"text": "WMT16 2 English news crawl", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.9336689472198486}]}, {"text": "However, simply treating sentences from the resource as negative samples could result in deceptively high performance of classification due to the domain differences between positive and negative data.", "labels": [], "entities": []}, {"text": "So we try to minimize such domain differences by selecting negative samples whose words all appear in the positive samples and whose average text length being close to the humorous ones.", "labels": [], "entities": []}, {"text": "In this section, we describe how we formulate humor recognition as a text classification problem and conduct experiments on four datasets which we mentioned in Section 3.", "labels": [], "entities": [{"text": "formulate humor recognition", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.7400592764218649}, {"text": "text classification", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.727619007229805}]}, {"text": "We validate the performance of different network structure with 10 fold cross validation and compare with the performance of previous work.", "labels": [], "entities": []}, {"text": "shows the experiments on both 16000 One-Liners and Pun of the Day.", "labels": [], "entities": [{"text": "Pun of the Day", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.7753046303987503}]}, {"text": "We set the baseline on the previous works of by Random Forest with Word2Vec + Human Centric Feature (Word2Vec + HCF) and by Convolutional Neural Networks.", "labels": [], "entities": [{"text": "Word2Vec + Human Centric Feature (Word2Vec + HCF)", "start_pos": 67, "end_pos": 116, "type": "DATASET", "confidence": 0.9004224538803101}]}, {"text": "We choose a dropout rate at 0.5 and test our model's performance with two factors F and HN.", "labels": [], "entities": [{"text": "F", "start_pos": 82, "end_pos": 83, "type": "METRIC", "confidence": 0.9926403164863586}]}, {"text": "F means the increase of filter size and number as we mentioned in section 4.", "labels": [], "entities": [{"text": "F", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9937083721160889}]}, {"text": "Otherwise, the window sizes would be (5, 6, 7) and filter number is 100 that is the same with Chen and Lee presents the result of Short Jokes and PTT Jokes datasets.", "labels": [], "entities": [{"text": "PTT Jokes datasets", "start_pos": 146, "end_pos": 164, "type": "DATASET", "confidence": 0.8160314957300822}]}, {"text": "As we can see, for the datasets was construed, it achieve 0.924 on Short Jokes and 0.943 on PTT Jokes in terms of F1 score respectively.", "labels": [], "entities": [{"text": "PTT Jokes", "start_pos": 92, "end_pos": 101, "type": "DATASET", "confidence": 0.8023128509521484}, {"text": "F1 score", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9859599173069}]}, {"text": "It shows that the deep learning model can, to some extent learn the humorous meaning and structure embedded in the text automatically without human selection of features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of four datasets", "labels": [], "entities": []}, {"text": " Table 2: Comparison of Different Methods of Humor Recognition", "labels": [], "entities": [{"text": "Humor Recognition", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.9459021985530853}]}, {"text": " Table 3: Result of Short Jokes and PTT Jokes datasets", "labels": [], "entities": [{"text": "PTT Jokes datasets", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.7482137084007263}]}]}