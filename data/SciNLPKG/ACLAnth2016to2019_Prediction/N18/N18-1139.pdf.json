{"title": [{"text": "Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work, we focus on the task of generating natural language descriptions from a struc-tured table of facts containing fields (such as nationality, occupation, etc) and values (such as Indian, {actor, director}, etc).", "labels": [], "entities": []}, {"text": "One simple choice is to treat the table as a sequence of fields and values and then use a standard seq2seq model for this task.", "labels": [], "entities": []}, {"text": "However, such a model is too generic and does not exploit task-specific characteristics.", "labels": [], "entities": []}, {"text": "For example, while generating descriptions from a table, a human would attend to information at two levels: (i) the fields (macro level) and (ii) the values within the field (micro level).", "labels": [], "entities": []}, {"text": "Further, a human would continue attending to afield fora few timesteps till all the information from that field has been rendered and then never return back to this field (because there is nothing left to say about it).", "labels": [], "entities": []}, {"text": "To capture this behavior we use (i) a fused bifocal attention mechanism which exploits and combines this micro and macro level information and (ii) a gated orthogonalization mechanism which tries to ensure that afield is remembered fora few time steps and then forgotten.", "labels": [], "entities": []}, {"text": "We experiment with a recently released dataset which contains fact tables about people and their corresponding one line biographical descriptions in English.", "labels": [], "entities": []}, {"text": "In addition , we also introduce two similar datasets for French and German.", "labels": [], "entities": []}, {"text": "Our experiments show that the proposed model gives 21% relative improvement over a recently proposed state of the art method and 10% relative improvement over basic seq2seq models.", "labels": [], "entities": []}, {"text": "The code and the datasets developed as apart of this work are publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rendering natural language descriptions from structured data is required in a wide variety of commercial applications such as generating descriptions of products, hotels, furniture, etc., from a corresponding table of facts about the entity.", "labels": [], "entities": [{"text": "Rendering natural language descriptions", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.89095738530159}]}, {"text": "Such a table typically contains {field, value} pairs where the field is a property of the entity (e.g., color) and the value is a set of possible assignments to this property (e.g., color = red).", "labels": [], "entities": []}, {"text": "Another example of this is the recently introduced task of generating one line biography descriptions from a given Wikipedia infobox (.", "labels": [], "entities": []}, {"text": "The Wikipedia infobox serves as a table of facts about a person and the first sentence from the corresponding article serves as a one line description of the person.", "labels": [], "entities": [{"text": "Wikipedia infobox", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9643585383892059}]}, {"text": "illustrates an example input infobox which contains fields such as Born, Residence, Nationality, Fields, Institutions and Alma Mater.", "labels": [], "entities": []}, {"text": "Each field further contains some words (e.g., particle physics, many-body theory, etc.).", "labels": [], "entities": []}, {"text": "The corresponding description is coherent with the information contained in the infobox.", "labels": [], "entities": []}, {"text": "Note that the number of fields in the infobox and the ordering of the fields within the infobox varies from person to person.", "labels": [], "entities": []}, {"text": "Given the large size (700K examples) and heterogeneous nature of the dataset which contains biographies of people from different backgrounds (sports, politics, arts, etc.), it is hard to come up with simple rule-based templates for generating natural language descriptions from infoboxes, thereby making a case for datadriven models.", "labels": [], "entities": []}, {"text": "Based on the recent success of data-driven neural models for various other NLG tasks (, one simple choice is to treat the infobox as Balakrishnan (born 1943 as Venkataraman Balakrishnan) is an Indian theoretical physicist who has worked in a number of fields of areas, including particle physics, many-body theory, the mechanical behavior of solids, dynamical systems, stochastic processes, and quantum dynamics.", "labels": [], "entities": [{"text": "many-body theory", "start_pos": 297, "end_pos": 313, "type": "TASK", "confidence": 0.7212618440389633}]}, {"text": "a sequence of {field, value} pairs and use a standard seq2seq model for this task.", "labels": [], "entities": []}, {"text": "However, such a model is too generic and does not exploit the specific characteristics of this task as explained below.", "labels": [], "entities": []}, {"text": "First, note that while generating such descriptions from structured data, a human keeps track of information at two levels.", "labels": [], "entities": []}, {"text": "Specifically, at a macro level, she would first decide which field to mention next and then at a micro level decide which of the values in the field needs to be mentioned next.", "labels": [], "entities": []}, {"text": "For example, she first decides that at the current step, the field occupation needs attention and then decides which is the next appropriate occupation to attend to from the set of occupations (actor, director, producer, etc.).", "labels": [], "entities": []}, {"text": "To enable this, we use a bifocal attention mechanism which computes an attention over fields at a macro level and over values at a micro level.", "labels": [], "entities": []}, {"text": "We then fuse these attention weights such that the attention weight fora field also influences the attention over the values within it.", "labels": [], "entities": []}, {"text": "Finally, we feed a fused context vector to the decoder which contains both field level and word level information.", "labels": [], "entities": []}, {"text": "Note that such two-level attention mechanisms ( have been used in the context of unstructured data (as opposed to structured data in our case), whereat a macro level one needs to pay attention to sentences and at a micro level to words in the sentences.", "labels": [], "entities": []}, {"text": "Next, we observe that while rendering the output, once the model pays attention to afield (say, occupation) it needs to stay on this field fora few timesteps (till all the occupations are produced in the output).", "labels": [], "entities": []}, {"text": "We refer to this as the stay on behavior.", "labels": [], "entities": []}, {"text": "Further, we note that once the tokens of afield are referred to, they are usually not referred to later.", "labels": [], "entities": []}, {"text": "For example, once all the occupations have been listed in the output we will never visit the occupation field again because there is nothing left to say about it.", "labels": [], "entities": []}, {"text": "We refer to this as the never look back behavior.", "labels": [], "entities": []}, {"text": "To model the stay on behaviour, we introduce a forget (or remember) gate which acts as a signal to decide when to forget the current field (or equivalently to decide till when to remember the current field).", "labels": [], "entities": [{"text": "forget", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9730227589607239}, {"text": "remember", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.8927243947982788}]}, {"text": "To model the never look back behaviour we introduce a gated orthogonalization mechanism which ensures that once afield is forgotten, subsequent field context vectors fed to the decoder are orthogonal to (or different from) the previous field context vectors.", "labels": [], "entities": []}, {"text": "We experiment with the WIKIBIO dataset () which contains around 700K {infobox, description} pairs and has a vocabulary of around 400K words.", "labels": [], "entities": [{"text": "WIKIBIO dataset", "start_pos": 23, "end_pos": 38, "type": "DATASET", "confidence": 0.9699257016181946}]}, {"text": "We show that the proposed model gives a relative improvement of 21% and 20% as compared to current state of the art models ( on this dataset.", "labels": [], "entities": []}, {"text": "The proposed model also gives a relative improvement of 10% as compared to the basic seq2seq model.", "labels": [], "entities": []}, {"text": "Further, we introduce new datasets for French and German on the same lines as the English WIKIBIO dataset.", "labels": [], "entities": [{"text": "WIKIBIO dataset", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.8699822425842285}]}, {"text": "Even on these two datasets, our model outperforms the state of the art methods mentioned above.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe our experimental setup:  We use the WIKIBIO dataset introduced by.", "labels": [], "entities": [{"text": "WIKIBIO dataset", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.948811411857605}]}, {"text": "It consists of 728, 321 biography articles from English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.9067495465278625}]}, {"text": "A biography article corresponds to a person (sportsman, politician, historical figure, actor, etc.).", "labels": [], "entities": []}, {"text": "Each Wikipedia article has an accompanying infobox which serves as the structured input and the task is to generate the first sentence of the article (which typically is a one-line description of the person).", "labels": [], "entities": []}, {"text": "We used the same train, valid and test sets which were made publicly available by.", "labels": [], "entities": []}, {"text": "We also introduce two new biography datasets, one in French and one in German.", "labels": [], "entities": [{"text": "biography datasets", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.7608663439750671}]}, {"text": "These datasets were created and pre-processed using the same procedure as outlined in.", "labels": [], "entities": []}, {"text": "Specifically, we extracted the infoboxes and the first sentence from the corresponding Wikipedia article.", "labels": [], "entities": []}, {"text": "As with the English dataset, we split the French and German datasets randomly into train (80%), test (10%) and valid (10%).", "labels": [], "entities": [{"text": "English dataset", "start_pos": 12, "end_pos": 27, "type": "DATASET", "confidence": 0.8502545952796936}]}, {"text": "The French and German datasets extracted by us has been made publicly available.", "labels": [], "entities": [{"text": "French and German datasets extracted", "start_pos": 4, "end_pos": 40, "type": "DATASET", "confidence": 0.7482005476951599}]}, {"text": "The number of examples was 170K and 50K and the vocabulary size was 297K and 143K for French and German respectively.", "labels": [], "entities": [{"text": "vocabulary size", "start_pos": 48, "end_pos": 63, "type": "METRIC", "confidence": 0.9568846523761749}]}, {"text": "Although in this work we focus only on generating descriptions in one language, we hope that this dataset will also be useful for developing models which jointly learn to generate descriptions from structured data in multiple languages.", "labels": [], "entities": []}, {"text": "To make a qualitative assessment of the generated sentences, we conducted a human study on a sample of 500 Infoboxes which were sampled from English dataset.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 141, "end_pos": 156, "type": "DATASET", "confidence": 0.8863852620124817}]}, {"text": "The annotators for this task were undergraduate and graduate students.", "labels": [], "entities": []}, {"text": "For each of these infoboxes, we generated summaries using the basic seq2seq model and our final model with bifocal attention and gated orthogonalization.", "labels": [], "entities": []}, {"text": "For each description and for each model, we asked three annotators to rank the output of the systems based on i) adequacy (i.e. does it capture relevant information from the infobox), (ii) fluency (i.e. grammar) and (iii) relative preference (i.e., which of the two outputs would be preferred).", "labels": [], "entities": []}, {"text": "Overall the average fluency/adequacy (on a scale of 5) for basic seq2seq model was 4.04/3.6 and 4.19/3.9 for our model respectively.", "labels": [], "entities": []}, {"text": "The results from suggest that in general gated orthogonalization model performs better than the basic seq2seq model.", "labels": [], "entities": []}, {"text": "Additionally, annotators were asked to verify if the generated summaries look natural (i.e, as if they were generated by humans).", "labels": [], "entities": []}, {"text": "In 423 out of 500 cases, the annotators said \"Yes\" suggesting that gated orthogonalization model indeed produces good descriptions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of different models on the  English WIKIBIO dataset", "labels": [], "entities": [{"text": "English WIKIBIO dataset", "start_pos": 49, "end_pos": 72, "type": "DATASET", "confidence": 0.7563165426254272}]}, {"text": " Table 2: Examples of generated descriptions from different models. For the last two examples, name  generated by Basic Seq2Seq model is incorrect because it attended to preceded by field.", "labels": [], "entities": []}, {"text": " Table 3: Qualitative Comparison of Model A  (Seq2Seq) and Model B (our model)", "labels": [], "entities": []}, {"text": " Table 4: Comparison of different models on the  French WIKIBIO dataset", "labels": [], "entities": [{"text": "French WIKIBIO dataset", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.8766976793607076}]}, {"text": " Table 5: Comparison of different models on the  German WIKIBIO dataset", "labels": [], "entities": [{"text": "German WIKIBIO dataset", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.806205670038859}]}, {"text": " Table 6: Out of domain results(BLEU-4)", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9964998960494995}]}]}