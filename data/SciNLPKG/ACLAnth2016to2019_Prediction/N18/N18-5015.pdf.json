{"title": [{"text": "Vis-Eval Metric Viewer: A Visualisation Tool for Inspecting and Evaluating Metric Scores of Machine Translation Output", "labels": [], "entities": [{"text": "Vis-Eval Metric Viewer", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5847405989964803}]}], "abstractContent": [{"text": "Machine Translation systems are usually evaluated and compared using automated evaluation metrics such as BLEU and METEOR to score the generated translations against human translations.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7914274632930756}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9974149465560913}, {"text": "METEOR", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9356172680854797}]}, {"text": "However, the interaction with the output from the metrics is relatively limited and results are commonly a single score along with a few additional statistics.", "labels": [], "entities": []}, {"text": "Whilst this maybe enough for system comparison it does not provide much useful feedback or a means for inspecting translations and their respective scores.", "labels": [], "entities": []}, {"text": "Vis-Eval Metric Viewer (VEMV) is a tool designed to provide visualisation of multiple evaluation scores so they can be easily interpreted by a user.", "labels": [], "entities": [{"text": "Vis-Eval Metric Viewer (VEMV)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7220969895521799}]}, {"text": "VEMV takes in the source, reference, and hypothesis files as parameters, and scores the hypotheses using several popular evaluation metrics simultaneously.", "labels": [], "entities": [{"text": "VEMV", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8740586042404175}]}, {"text": "Scores are produced at both the sentence and dataset level and results are written locally to a series of HTML files that can be viewed on a web browser.", "labels": [], "entities": []}, {"text": "The individual scored sentences can easily be inspected using powerful search and selection functions and results can be vi-sualised with graphical representations of the scores and distributions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic evaluation of Machine Translation (MT) hypotheses is key for system development and comparison.", "labels": [], "entities": [{"text": "evaluation of Machine Translation (MT) hypotheses", "start_pos": 10, "end_pos": 59, "type": "TASK", "confidence": 0.7773731052875519}]}, {"text": "Even though human assessment ultimately provides more reliable and insightful information, automatic evaluation is faster, cheaper, and often considered more consistent.", "labels": [], "entities": []}, {"text": "Many metrics have been proposed for MT that compare system translations against human references, with the most popular being BLEU), METEOR), TER (), and, more recently, BEER).", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9923142790794373}, {"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9988676309585571}, {"text": "METEOR", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9902151823043823}, {"text": "TER", "start_pos": 142, "end_pos": 145, "type": "METRIC", "confidence": 0.995412290096283}, {"text": "BEER", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9976826906204224}]}, {"text": "These and other automatic metrics are often criticised for providing scores that can be non-intuitive and uninformative, especially at the sentence level ().", "labels": [], "entities": []}, {"text": "Additionally, scores across different metrics can be inconsistent with each other.", "labels": [], "entities": []}, {"text": "This inconsistency can bean indicator of linguistic properties of the translations which should be further analysed.", "labels": [], "entities": []}, {"text": "However, multiple metrics are not always used and any discrepancies among them tend to be ignored.", "labels": [], "entities": []}, {"text": "Vis-Eval Metric Viewer (VEMV) was developed as a tool bearing in mind the aforementioned issues.", "labels": [], "entities": [{"text": "Vis-Eval Metric Viewer (VEMV)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.718210349480311}]}, {"text": "It enables rapid evaluation of MT output, currently employing up to eight popular metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9944774508476257}]}, {"text": "Results can be easily inspected (using atypical web browser) especially at the segment level, with each sentence (source, reference, and hypothesis) clearly presented in interactive score tables, along with informative statistical graphs.", "labels": [], "entities": []}, {"text": "No server or internet connection is required.", "labels": [], "entities": []}, {"text": "Only readily available packages or libraries are used locally.", "labels": [], "entities": []}, {"text": "Ultimately VEMV is an accessible utility that can be run quickly and easily on all the main platforms.", "labels": [], "entities": [{"text": "VEMV", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.8890653252601624}]}, {"text": "Before describing the technical specification of the VEMV tool and its features in Section 3, we give an overview of existing metric visualisation tools in Section 2.", "labels": [], "entities": [{"text": "VEMV tool", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.7650017142295837}]}], "datasetContent": [{"text": "Currently VEMV uses eight evaluation metrics to score individual sentences and the whole document.", "labels": [], "entities": [{"text": "VEMV", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8597165942192078}]}, {"text": "All results are shown side by side for comparison purposes and can be inspected at a granular level.", "labels": [], "entities": []}, {"text": "A glance at the two sentences in already provides numerous points for analysis.", "labels": [], "entities": []}, {"text": "For example, the MT in sentence 2 is along way from the reference and receives low metric scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9100037813186646}]}, {"text": "However, whilst not identical to the reference, the MT is correct and could be interchanged with the reference without losing meaning.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.37901008129119873}]}, {"text": "For sentence 1 the MT is only a single word away from the reference and receives good scores, (much higher than sentence 2) although the meaning is incorrect.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9265411496162415}]}, {"text": "The interactive display enables the user to easily examine such phenomena in a given dataset.", "labels": [], "entities": []}], "tableCaptions": []}