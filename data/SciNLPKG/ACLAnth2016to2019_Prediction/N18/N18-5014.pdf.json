{"title": [{"text": "Generating Continuous Representations of Medical Texts", "labels": [], "entities": [{"text": "Generating Continuous Representations of Medical Texts", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7848399976889292}]}], "abstractContent": [{"text": "We present an architecture that generates medical texts while learning an informative, continuous representation with discriminative features.", "labels": [], "entities": []}, {"text": "During training the input to the system is a dataset of captions for medical X-Rays.", "labels": [], "entities": []}, {"text": "The acquired continuous representations are of particular interest for use in many machine learning techniques where the discrete and high-dimensional nature of textual input is an obstacle.", "labels": [], "entities": []}, {"text": "We use an Adversarially Reg-ularized Autoencoder to create realistic text in both an unconditional and conditional setting.", "labels": [], "entities": []}, {"text": "We show that this technique is applicable to medical texts which often contain syntactic and domain-specific shorthands.", "labels": [], "entities": []}, {"text": "A quantitative evaluation shows that we achieve a lower model perplexity than a traditional LSTM generator .", "labels": [], "entities": []}], "introductionContent": [{"text": "The main focus of this paper is the generation of realistic samples with a similar quality to those in a training set of medical texts.", "labels": [], "entities": []}, {"text": "At the same time, an informative, continuous representation is created from the textual input.", "labels": [], "entities": []}, {"text": "Obtaining a good representation for medical texts may prove vital to building more sophisticated generative, discriminative or semantic models for the field.", "labels": [], "entities": [{"text": "generative, discriminative or semantic", "start_pos": 97, "end_pos": 135, "type": "TASK", "confidence": 0.8220250725746154}]}, {"text": "One of the obstacles is the discrete nature of text that makes it difficult to employ in many machine learning algorithms.", "labels": [], "entities": []}, {"text": "This is the case for Generative Adversarial Networks (GANs), which are not adequate to generate text as it is difficult to backpropagate the error to discrete symbols.", "labels": [], "entities": []}, {"text": "The ability of GANs to learn the underlying distribution, rather than repeating examples in the training data, has led to the successful generation of intricate high-resolution samples in computer vision ( . Conditional GANs in particular, where the class or label is passed to both generator and discriminator, implicitly learn relevant ancillary information which leads to more detailed outputs).", "labels": [], "entities": []}, {"text": "If we had a better understanding of how to train GANs with discrete data, some of those developments might be directly applicable to detailed text generation applications-such as image caption generation, machine translation, simplification of text, and text summarizationespecially when dealing with noisy texts.", "labels": [], "entities": [{"text": "text generation", "start_pos": 142, "end_pos": 157, "type": "TASK", "confidence": 0.7363064587116241}, {"text": "image caption generation", "start_pos": 179, "end_pos": 203, "type": "TASK", "confidence": 0.8336877028147379}, {"text": "machine translation", "start_pos": 205, "end_pos": 224, "type": "TASK", "confidence": 0.8132529258728027}, {"text": "text summarizationespecially", "start_pos": 254, "end_pos": 282, "type": "TASK", "confidence": 0.730267196893692}]}, {"text": "Another impediment is the nature of clinical data, which is often unstructured and not wellformed, yet commonly has a high and important information density.", "labels": [], "entities": []}, {"text": "Textual reports often don't follow regular syntax rules and contain very specific medical terminology.", "labels": [], "entities": []}, {"text": "Moreover, the amount of training data is often limited and each physician has a personal writing style.", "labels": [], "entities": []}, {"text": "Simply reusing pretrained continuous representations, such as vectorbased word embeddings (, is therefore not always feasible for medical datasets.", "labels": [], "entities": []}, {"text": "The approach to text generation has mainly been dominated by Long Short-Term Memory networks (LSTMs).", "labels": [], "entities": [{"text": "text generation", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.8623877763748169}]}, {"text": "While LSTMs are successful in creating realistic samples, no actionable smooth representation is created of the text and thus there are limited possibilities to manipulate or employ the representations in additional applications that require continuous inputs.", "labels": [], "entities": []}, {"text": "While the creation of continuous representations of text usually involves an autoencoder, the results mostly lack enough semantic information to be particularly useful in an alternate task.", "labels": [], "entities": []}, {"text": "have shown how to achieve text generation with a continuous representation by implementing an Adversarially Regularized Autoencoder (ARAE).", "labels": [], "entities": [{"text": "text generation", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7822944521903992}]}, {"text": "They combine the training of a rich discrete space autoencoder with recurrent neural networks (RNNs) and the training of more simple, fully connected networks to generate samples in the continuous space.", "labels": [], "entities": []}, {"text": "With adversarial (GAN) training, both the distribution of the generated as well as the encoded samples are encouraged to converge.", "labels": [], "entities": []}, {"text": "The outcome is that a smooth representation is learned as well as a generator that can build realistic samples in the continuous space.", "labels": [], "entities": []}, {"text": "In this paper, we explore this methodology in the context of medical texts, more specifically captions for chest X-Rays.", "labels": [], "entities": []}, {"text": "Analogous to conditional GANs, we also extend the network of by generating samples conditioned on categorical, medical labels (for example 'healthy').", "labels": [], "entities": [{"text": "GANs", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.8945398926734924}]}, {"text": "We refer to this method as conditional ARAE.", "labels": [], "entities": [{"text": "ARAE", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.877103865146637}]}, {"text": "Ina quantitative evaluation, the perplexity of the conditional ARAE outperforms both the unconditional ARAE as well as a traditional LSTM.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Examples of captions generated by the the conditional ARAE from a random vector z and a class  label. For each label an example of a correct (+) caption and a wrong (-) caption is given respectively.", "labels": [], "entities": []}, {"text": " Table 2. Perplexity scores for each of the models.", "labels": [], "entities": [{"text": "Perplexity scores", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9631272256374359}]}]}