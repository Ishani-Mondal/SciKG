{"title": [{"text": "Neural Text Generation in Stories Using Entity Representations as Context", "labels": [], "entities": [{"text": "Neural Text Generation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.808374027411143}]}], "abstractContent": [{"text": "We introduce an approach to neural text generation that explicitly represents entities mentioned in the text.", "labels": [], "entities": [{"text": "neural text generation", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7453813354174296}]}, {"text": "Entity representations are vectors that are updated as the text proceeds; they are designed specifically for narrative text like fiction or news stories.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that modeling entities offers a benefit in two automatic evaluations: mention generation (in which a model chooses which entity to mention next and which words to use in the mention) and selection between a correct next sentence and a distractor from later in the same story.", "labels": [], "entities": [{"text": "mention generation", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.6898468136787415}]}, {"text": "We also conduct a human evaluation on automatically generated text in story contexts; this study supports our emphasis on entities and suggests directions for further research .", "labels": [], "entities": []}], "introductionContent": [{"text": "We consider the problem of automatically generating narrative text, a challenging problem at the junction of computational creativity and language technologies.", "labels": [], "entities": []}, {"text": "We are motivated in particular by potential applications in personalized education and assistive tools for human authors, though we believe narrative might also play a role in social conversational agents.", "labels": [], "entities": []}, {"text": "In this work, the term \"narrative text\" refers primarily to fiction but might also include news and other kinds of stories.", "labels": [], "entities": []}, {"text": "A notable difference between longstanding work in natural language generation and recent \"neural\" models is in the treatment of entities and the words used to refer to them.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6965863307317098}]}, {"text": "Particularly in the generation of narrative text, character-centered generation has been shown important in character dialogue generation) and story planning).", "labels": [], "entities": [{"text": "generation of narrative text", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.8173127174377441}, {"text": "character-centered generation", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.7050167620182037}, {"text": "character dialogue generation", "start_pos": 108, "end_pos": 137, "type": "TASK", "confidence": 0.7858268817265829}, {"text": "story planning", "start_pos": 143, "end_pos": 157, "type": "TASK", "confidence": 0.7234181612730026}]}, {"text": "Neural models, on the other hand, treat mentions as just more words, relying on representation learning to relate the people in a story through the words alone.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our first experiment is to investigate each model's capacity to mention an entity in context.", "labels": [], "entities": []}, {"text": "For example, in, Emily and her are both possible mentions of EMILY's character, but the two cannot be used interchangeably.", "labels": [], "entities": []}, {"text": "Inspired by early work on referring expression generation ( and recent work on entity prediction (, we propose anew task we call mention generation.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.8027093609174093}, {"text": "entity prediction", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7529925107955933}]}, {"text": "Given a text and a slot to be filled with an entity mention, a model must choose among all preceding entity mentions and the correct mention.", "labels": [], "entities": []}, {"text": "So if the model was choosing the next entity mention to be generated in, it would select between all the previous entity mentions (Emily, the dragon, Seth, and her) and the correct mention (she).", "labels": [], "entities": []}, {"text": "In our model, each candidate mention is augmented with the index of its entity.", "labels": [], "entities": []}, {"text": "Therefore, performing well on this task requires choosing both the entity and the words used to refer to it; this notion of quality is our most stringent evaluation measure.", "labels": [], "entities": []}, {"text": "It requires the greatest precision, as it is model cluster and mention cluster only mention only  possible to select the correct mention but not the correct cluster and vice versa.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9991928935050964}]}, {"text": "Since S2SA does not model entities, we also compare systems on quality of mentions alone (without entity clusters).", "labels": [], "entities": []}, {"text": "For completeness, we include cluster quality for the entity-aware models.", "labels": [], "entities": []}, {"text": "Candidate lists for each task to generate the next mention in the example in are shown in.", "labels": [], "entities": []}, {"text": "The experiment setup does not require manual creation of candidate lists.", "labels": [], "entities": []}, {"text": "However, it makes the mention generation task even more challenging, because the size of a candidate list can exceed 100 mention candidates.", "labels": [], "entities": [{"text": "mention generation task", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.8334270119667053}]}, {"text": "We note that the difficulty of this task increases as we consider mention slots later and later in the document.", "labels": [], "entities": []}, {"text": "The first mention generation choice is a trivial one, with a single candidate that is by definition correct.", "labels": [], "entities": []}, {"text": "As more entity mentions are observed, the number of options will increase.", "labels": [], "entities": []}, {"text": "To enable aggregation across contexts of all lengths, we report the mean average precision (MAP) of the correct candidates, where the language model scores are used to rank candidates.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 68, "end_pos": 96, "type": "METRIC", "confidence": 0.9282671411832174}]}, {"text": "Baselines Along with the two ablated models (S2SA and ENTITYNLM), we include a \"reverse order\" baseline, which ranks mentions by recency Note that the list of candidates may include duplicate entries with the same mention words and cluster.", "labels": [], "entities": [{"text": "ENTITYNLM", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9208776354789734}]}, {"text": "These are collapsed since they will have the same score under a language model.", "labels": [], "entities": []}, {"text": "(the first element in the ranking is the most recent mention, then the second-most-recent, and so on).", "labels": [], "entities": []}, {"text": "Results The ranking results of ENGEN and other systems are reported in.", "labels": [], "entities": [{"text": "ENGEN", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.7876334190368652}]}, {"text": "A higher MAP score implies a better system.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9475919306278229}]}, {"text": "We measure the overall performance of all the systems, along with their performance on selecting the mention only and entity cluster only.", "labels": [], "entities": []}, {"text": "Across all the evaluation measures, ENGEN gives the highest MAP numbers.", "labels": [], "entities": [{"text": "ENGEN", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9817885756492615}, {"text": "MAP", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9814903736114502}]}, {"text": "Recall that S2SA does not have a component for entity prediction, therefore we only compare it with ENGEN in the mention only case.", "labels": [], "entities": [{"text": "entity prediction", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.8253873586654663}, {"text": "ENGEN", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.8919049501419067}]}, {"text": "The difference between line 4 and line 2 on the mention only column shows the benefit of adding entity representations for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.7737011313438416}]}, {"text": "The difference between lines 3 and 4 shows that local context also gives a small boost.", "labels": [], "entities": []}, {"text": "Although the distance between the current slot and previous entity mention has been shown as a useful feature in coreference resolution), line 1 shows distance alone is not an effective heuristic for mention generation.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.96955206990242}, {"text": "mention generation", "start_pos": 200, "end_pos": 218, "type": "TASK", "confidence": 0.7286115139722824}]}, {"text": "The sentence selection task is inspired by tests of coherence used to assess text generation components automatically, without human evaluation (.", "labels": [], "entities": [{"text": "sentence selection task", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8320753574371338}]}, {"text": "It serves as a sanity check, as it was conducted prior to full generation and human evaluations ( \u00a77).", "labels": [], "entities": []}, {"text": "Since the models under consideration are generative, they can be used to assign scores to candidate sentences, given a context.", "labels": [], "entities": []}, {"text": "In our version of this task, we provide a model with n \u2212 1 = 49 sentences of preceding context, and offer two choices for the nth (50th) sentence: the actual 50th sentence or a distractor sentence randomly chosen from the next 50 sentences.", "labels": [], "entities": []}, {"text": "A random baseline would achieve 50% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9987945556640625}]}, {"text": "Because the distractor comes from the same  story (with similar language, characters, and topics) and relatively nearby (in 2% cases, the very next sentence), this is not a trivial task.", "labels": [], "entities": []}, {"text": "All of the sentences share lexical and entity information with the last line of the context.", "labels": [], "entities": []}, {"text": "However, the first sentence immediately follows the context, while the second and third sentences are 10 lines and 48 lines away from the context, respectively.", "labels": [], "entities": []}, {"text": "These entity and lexical similarities make distinguishing the actual sentence from the random sentence a challenging problem for the model.", "labels": [], "entities": []}, {"text": "To select the sentence, the model scores each of the two candidate sentences based on its probability on words and all entity-related information as defined in Equation 6.", "labels": [], "entities": []}, {"text": "(Both candidate sentences come from the preprocessed data and have the entity annotations described in \u00a74.)", "labels": [], "entities": []}, {"text": "The sentence that receives the higher probability is chosen.", "labels": [], "entities": []}, {"text": "For each of the 4,037 segments of context in the test set, we calculated the accuracy of each model at distinguishing the gold sentence from a distractor sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9994576573371887}]}, {"text": "We ran this pairwise decision 5 times, each time with a different set of randomly selected distractor sentences and averaged their performance across all 5 rounds.", "labels": [], "entities": []}, {"text": "Results The accuracy of each of the models is reported in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9996912479400635}]}, {"text": "The best performance is obtained by ENGEN, which is significantly better than the other two models (p < 0.05, binomial test).", "labels": [], "entities": [{"text": "ENGEN", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9917702674865723}]}, {"text": "Unlike the mention generation task, S2SA beats ENTITYNLM at this task; this difference in performance shows the importance of local context.", "labels": [], "entities": [{"text": "mention generation task", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.8690639138221741}, {"text": "ENTITYNLM", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9837108254432678}]}, {"text": "Although we performed five different rounds of random sampling to choose a sentence from the following segment as the distractor sentence, the standard deviations in show the results are generally consistent across rounds, regardless of model mean accuracy s.d.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.6795262098312378}]}, {"text": "1. S2SA 0.546 0.01 2.", "labels": [], "entities": []}, {"text": "ENGEN * 0.566 0.008 * signficantly better than lines 1 and 2 with p < 0.05.", "labels": [], "entities": [{"text": "ENGEN * 0.566 0.008", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.9088959693908691}]}, {"text": "the distractor's distance from the gold sentence.", "labels": [], "entities": []}, {"text": "The task motivating the work in this paper is narrative text generation.", "labels": [], "entities": [{"text": "narrative text generation", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.6427267889181772}]}, {"text": "As such, evaluation by human judges of the quality of generated text is the best measure of our methods' quality.", "labels": [], "entities": []}, {"text": "This study simplifies that evaluation by distilling the judgment down to a forced choice between contextually generated sentences generated by two different models.", "labels": [], "entities": []}, {"text": "We use this task to investigate the strengths and weaknesses of our model in a downstream application.", "labels": [], "entities": []}, {"text": "By asking humans to decide which sentences they prefer (in a given context) and to explain why, we can analyze where our model is helping and where text generation for stories still needs to improve, both with respect to entities and to other aspects of language.", "labels": [], "entities": [{"text": "text generation", "start_pos": 148, "end_pos": 163, "type": "TASK", "confidence": 0.7489380240440369}]}, {"text": "Here we control for training data and assess the benefit of including entity information for generating sentences to continue a story.", "labels": [], "entities": []}, {"text": "We presented Amazon Mechanical Turkers 5 with a short excerpt from a story and two generated sentences, one generated by ENGEN and one generated by the entity-unaware S2SA.", "labels": [], "entities": [{"text": "Amazon Mechanical Turkers 5", "start_pos": 13, "end_pos": 40, "type": "DATASET", "confidence": 0.8961061090230942}]}, {"text": "We asked them to \"choose a sentence to continue the story\" and to briefly explain why they made the choice they did, an approach similar to that in other storybased work such as.", "labels": [], "entities": []}, {"text": "Note that we did not prime Turkers to focus on entities.", "labels": [], "entities": []}, {"text": "Rather, the purpose of this experiment was to examine the performance of the model in a story generation setting and to get feedback on what people generally notice in generated text, not only with regard to entities.", "labels": [], "entities": []}, {"text": "By keeping the task open-ended, we can better analyze what people value in generated text for stories, and where our model supports that and where it doesn't.", "labels": [], "entities": []}, {"text": "We used a subset of 50 randomly selected text segments from the test set described in \u00a74.", "labels": [], "entities": []}, {"text": "However, for the human evaluation, we only used the final 60 words 6 of the story segments to keep the amount of reading and context manageable for Turkers.", "labels": [], "entities": []}, {"text": "The models had access to the same subset of the context that the evaluator saw, not all 50 sentences from the original segment as in earlier experiments.", "labels": [], "entities": []}, {"text": "For each context, we randomly sampled a sentence to continue the document, using each of two models: ENGEN and S2SA.", "labels": [], "entities": [{"text": "ENGEN", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9430086612701416}]}, {"text": "These two models allowed us to see if adding the entity information noticeably improved the quality of the generation to evaluators.", "labels": [], "entities": []}, {"text": "Initial experiments showed that fluency remains a problem for neural text generation.", "labels": [], "entities": [{"text": "neural text generation", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.6183209319909414}]}, {"text": "To reduce the effect of fluency on Turkers' judgments, we generated 100 samples for each context/model pair and then reranked them with a 5-gram language model) that was trained on the same training data.", "labels": [], "entities": []}, {"text": "The two top ranked sentences (one for ENGEN and one for S2SA) were presented in random order and without reference to the models that generated them.", "labels": [], "entities": [{"text": "ENGEN", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.513542652130127}]}, {"text": "For each of the 50 contexts, we had 11 Turkers pick a candidate sentence to continue the story passage.", "labels": [], "entities": []}, {"text": "Turkers were paid $0.10 for each evaluation they completed.", "labels": [], "entities": []}, {"text": "In total, 93 Turkers completed the task.", "labels": [], "entities": []}, {"text": "The number of passages Turkers completed ranged from 1 to all 50 story segments (with an average of 6.1).", "labels": [], "entities": []}, {"text": "While the quantitative portion of this task would be easy to scale, the qualitative portion is not; we kept the human evaluation small, running it until reaching saturation.", "labels": [], "entities": []}, {"text": "Results Each pair of sentences was evaluated by 11 Turkers, so each of the passages could receive up to 11 votes for ENGEN.", "labels": [], "entities": [{"text": "ENGEN", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.84172123670578}]}, {"text": "For 27 of the passages, the majority of Turkers (6 or more) chose the sentence from ENGEN, versus 23 passages that went to the baseline model, S2SA.", "labels": [], "entities": [{"text": "ENGEN", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.6820893883705139}]}, {"text": "The scores were close in many cases, and for several passages, Turkers noted in their explanations that while they were required to choose one sentence, both would have worked.", "labels": [], "entities": []}, {"text": "Examples of the context and sentence pairs that were strongly in favor of EN-GEN, strongly in favor of S2SA, and that received We included the whole sentence that contained the 60th word, so most documents were slightly over 60 words.", "labels": [], "entities": []}, {"text": "mixed reviews are shown in.", "labels": [], "entities": []}, {"text": "When asked to explain why they selected the sentence they did, a few Turkers attributed their choices to connections between pronouns in EN-GEN's suggestions to characters mentioned in the story excerpt.", "labels": [], "entities": []}, {"text": "However, a more frequent occurrence was Turkers citing a mismatch in entities as their reason for rejecting an option.", "labels": [], "entities": []}, {"text": "For example, one Turker said they chose ENGEN's sentence because the S2SA sentence began with \"she,\" and there were no female characters in the context.", "labels": [], "entities": []}, {"text": "Interestingly, while pronouns not mentioned in the context were cited as a reason for rejecting candidate sentences, new proper noun entity mentions were seen as an asset by some.", "labels": [], "entities": []}, {"text": "One Turker chose a S2SA sentence that referenced \"Richard,\" a character not present in the context, saying, \"I believe including Richard as a name gives some context of the characters of the story.\"", "labels": [], "entities": []}, {"text": "This demonstrates the importance of the ability to generate new entities, in addition to referring back to exisiting entities.", "labels": [], "entities": []}, {"text": "However, due to the open-ended nature of the task, the reasons Turkers cited for selecting sentences extended far beyond characters and entity mentions.", "labels": [], "entities": []}, {"text": "In fact, most of the responses credited other aspects of stories and language for their choice.", "labels": [], "entities": []}, {"text": "Some chose sentences based on their potential to move the plot forward or because they fit better with \"the theme\" or \"the tone\" of the context.", "labels": [], "entities": []}, {"text": "Others made decisions based on whether they thought a sentence of dialogue or a descriptive sentence was more appropriate, or a statement versus a question.", "labels": [], "entities": []}, {"text": "Many made their decisions using deeper knowledge about the story's context.", "labels": [], "entities": []}, {"text": "For example, in the second story listed in, one Turker used social knowledge to choose the S2SA sentence because \"the introduction makes the man sound like he is a stranger, so 'I'm proud of you' seems out of place.\"", "labels": [], "entities": []}, {"text": "In this case, even though the sentence from ENGEN correctly generated pronouns that refer to entities in the context, the mismatch in the social aspects of the context and ENGEN's sentence contributed to 7 out of 11 Turkers choosing the vaguer S2SA sentence.", "labels": [], "entities": []}, {"text": "While neither S2SA nor ENGEN explicitly encodes these types of information, these qualities are important to human evaluators of generated text and should influence future work on narrative text generation.", "labels": [], "entities": [{"text": "narrative text generation", "start_pos": 180, "end_pos": 205, "type": "TASK", "confidence": 0.6353183587392172}]}], "tableCaptions": [{"text": " Table 1: MAP on the mention generation task. Note that these results can only be compared between models, not  between tasks, as there are a different number of candidates for each of the tasks.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.690689742565155}, {"text": "mention generation task", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7639635304609934}]}]}