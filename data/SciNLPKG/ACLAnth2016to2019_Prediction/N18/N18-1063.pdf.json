{"title": [{"text": "Semantic Structural Evaluation for Text Simplification", "labels": [], "entities": [{"text": "Semantic Structural Evaluation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8088144858678182}, {"text": "Text Simplification", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7764459848403931}]}], "abstractContent": [{"text": "Current measures for evaluating text simplification systems focus on evaluating lexical text aspects, neglecting its structural aspects.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.6846101582050323}]}, {"text": "In this paper we propose the first measure to address structural aspects of text simplification, called SAMSA.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7671152949333191}]}, {"text": "It leverages recent advances in semantic parsing to assess simplification quality by decomposing the input based on its semantic structure and comparing it to the output.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7355587482452393}]}, {"text": "SAMSA provides a reference-less automatic evaluation procedure, avoiding the problems that reference-based methods face due to the vast space of valid simplifications fora given sentence.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7203429341316223}]}, {"text": "Our human evaluation experiments show both SAMSA's substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 43, "end_pos": 48, "type": "TASK", "confidence": 0.8408451676368713}]}], "introductionContent": [{"text": "Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences.", "labels": [], "entities": [{"text": "Text simplification (TS)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8693206489086152}]}, {"text": "It is a useful preprocessing step for several NLP tasks, such as machine translation) and relation extraction, and has also been shown useful in the development of reading aids, e.g., for people with dyslexia () or non-native speakers).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7722185850143433}, {"text": "relation extraction", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.9028086066246033}]}, {"text": "The task has attracted much attention in the past decade (), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments.", "labels": [], "entities": []}, {"text": "This is in part due to the difficulty to combine the effects of different simplification operations (e.g., deletion, splitting and substitution). has recently made considerable progress towards that goal, and proposed to tackle it both by using an improved reference-based measure, named SARI, and by increasing the number of references.", "labels": [], "entities": [{"text": "SARI", "start_pos": 288, "end_pos": 292, "type": "METRIC", "confidence": 0.6953102946281433}]}, {"text": "However, their research focused on lexical, rather than structural simplification, which provides a complementary view of TS quality as this paper will show.", "labels": [], "entities": [{"text": "TS quality", "start_pos": 122, "end_pos": 132, "type": "TASK", "confidence": 0.86424121260643}]}, {"text": "This paper focuses on the evaluation of the structural aspects of the task.", "labels": [], "entities": []}, {"text": "We introduce the semantic measure SAMSA (Simplification Automatic evaluation Measure through Semantic Annotation), the first structure-aware measure for TS in general, and the first to use semantic structure in this context in particular.", "labels": [], "entities": [{"text": "TS", "start_pos": 153, "end_pos": 155, "type": "TASK", "confidence": 0.9599381685256958}]}, {"text": "SAMSA stipulates that an optimal split of the input is one where each predicate-argument structure is assigned its own sentence, and measures to what extent this assertion holds for the input-output pair in question, by using semantic structure.", "labels": [], "entities": []}, {"text": "SAMSA focuses on the core semantic components of the sentence, and is tolerant towards the deletion of other units.", "labels": [], "entities": []}, {"text": "For example, SAMSA will assign a high score to the output split \"John got home.", "labels": [], "entities": [{"text": "John got home", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9377821087837219}]}, {"text": "John gave Mary a call.\" for the input sentence \"John got home and gave Mary a call.\", as it splits each of its predicate-argument structures to a different sentence.", "labels": [], "entities": []}, {"text": "Splits that alter predicate-argument relations such as \"John got home and gave.", "labels": [], "entities": []}, {"text": "Mary called.\" are penalized by SAMSA.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.5960529446601868}]}, {"text": "SAMSA's use of semantic structures for TS evaluation has several motivations.", "labels": [], "entities": [{"text": "TS evaluation", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9698654413223267}]}, {"text": "First, it provides means to measure the extent to which the meaning of the source is preserved in the output.", "labels": [], "entities": []}, {"text": "Second, it provides means for measuring whether the input sentence was split to semantic units of the right granularity.", "labels": [], "entities": []}, {"text": "Third, defining a semantic measure that does not require references avoids the difficulties incurred by their non-uniqueness, and the difficulty in collecting high quality references, as reported by and by with respect to the Parallel Wikipedia Corpus (PWKP;.", "labels": [], "entities": [{"text": "Parallel Wikipedia Corpus (PWKP", "start_pos": 226, "end_pos": 257, "type": "DATASET", "confidence": 0.7654867112636566}]}, {"text": "SAMSA is further motivated by its use of semantic annotation only on the source side, which allows to evaluate multiple systems using same source-side annotation, and avoids the need to parse system outputs, which can be garbled.", "labels": [], "entities": []}, {"text": "In this paper we use the UCCA scheme for defining semantic structure.", "labels": [], "entities": [{"text": "UCCA", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8759728670120239}]}, {"text": "UCCA has been shown to be preserved remarkably well across translations and has also been successfully used for machine translation evaluation () (Section 2).", "labels": [], "entities": [{"text": "UCCA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.913776695728302}, {"text": "machine translation evaluation", "start_pos": 112, "end_pos": 142, "type": "TASK", "confidence": 0.8766064445177714}]}, {"text": "We note, however, that SAMSA can be adapted to work with any semantic scheme that captures predicate-argument relations, such as AMR ( or Discourse Representation Structures, as used by.", "labels": [], "entities": []}, {"text": "We experiment with SAMSA both where semantic annotation is carried out manually, and where it is carried out by a parser.", "labels": [], "entities": []}, {"text": "We conduct human rating experiments and compare the resulting system rankings with those predicted by SAMSA.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.8904403448104858}]}, {"text": "We find that SAMSA's rankings obtain high correlations with human rankings, and compare favorably to existing referencebased measures for TS.", "labels": [], "entities": []}, {"text": "Moreover, our results show that existing measures, which mainly target lexical simplification, are ill-suited to predict human judgments where structural simplification is involved.", "labels": [], "entities": []}, {"text": "Finally, we apply SAMSA to the dataset of the QATS shared task on simplification evaluation.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.6330326795578003}, {"text": "QATS shared task", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.5574509898821512}, {"text": "simplification evaluation", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.9207453429698944}]}, {"text": "We find that SAMSA obtains comparative correlation with human judgments on the task, despite operating in a more restricted setting, as it does not use human ratings as training data and focuses only on structural aspects of simplicity.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 13, "end_pos": 18, "type": "TASK", "confidence": 0.9246550798416138}]}, {"text": "Section 2 presents previous work.", "labels": [], "entities": []}, {"text": "Section 5 details the collection of human judgments.", "labels": [], "entities": [{"text": "collection of human judgments", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.8787345290184021}]}, {"text": "Our experimental setup for comparing our human and automatic rankings is given in Section 6, and results are given in Section 7, showing superior results for SAMSA.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 158, "end_pos": 163, "type": "DATASET", "confidence": 0.8539993762969971}]}, {"text": "A discussion on the results is presented in Section 8.", "labels": [], "entities": []}, {"text": "Section 9 presents experiments with SAMSA on the QATS evaluation benchmark.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.6515148282051086}, {"text": "QATS evaluation benchmark", "start_pos": 49, "end_pos": 74, "type": "DATASET", "confidence": 0.7677916685740153}]}], "datasetContent": [{"text": "For testing the automatic metric, we first build a human evaluation benchmark, using 100 sentences from the complex part of the PWKP corpus and the outputs of six recent simplification systems for these sentences: (1) TSM (Zhu et al., 2010) using Tree-Based SMT, (2) RevILP (Woodsend and Lapata, 2011) using Quasi-Synchronous Grammars, (3) PBMT-R () using PhraseBased SMT, (4) Hybrid), a supervised system using DRS, (5) UN-SUP (, an unsupervised system using DRS, and (6) Split-Deletion (Narayan and Gardent, 2016), the unsupervised system with only structural operations.", "labels": [], "entities": [{"text": "PWKP corpus", "start_pos": 128, "end_pos": 139, "type": "DATASET", "confidence": 0.9550579786300659}, {"text": "PhraseBased SMT", "start_pos": 356, "end_pos": 371, "type": "TASK", "confidence": 0.6049748212099075}]}, {"text": "All these systems explicitly address at least one type of structural simplification operation.", "labels": [], "entities": []}, {"text": "The last system, Split-Deletion, performs only structural (i.e., no lexical) operations.", "labels": [], "entities": []}, {"text": "It is thus an interesting test case for SAMSA since here the aligner can be replaced by a simple match between identical words.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 40, "end_pos": 45, "type": "TASK", "confidence": 0.9185814261436462}]}, {"text": "In total we obtain 600 system outputs from the six systems, as well as 100 sentences from the simple Wikipedia side of the corpus, which serve as references.", "labels": [], "entities": []}, {"text": "Five in-house annotators with high proficiency in English evaluated the resulting 700 input-output pairs by answering the questions in.", "labels": [], "entities": []}, {"text": "Qa addresses grammaticality, Qb and Qc capture two complementary aspects of meaning preservation (the addition and the removal of information) and Qd addresses structural simplicity.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7289407700300217}]}, {"text": "Possible answers are: 1 (\"no\"), 2 (\"maybe\") and 3 (\"yes\").", "labels": [], "entities": []}, {"text": "Following Glavas and\u0160tajnerand\u02c7and\u0160tajner (2013), we used a 3 point Likert scale, which has recently been shown to be preferable over a 5 point scale through human studies on sentence compression (.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 175, "end_pos": 195, "type": "TASK", "confidence": 0.7407415509223938}]}, {"text": "Question Qd was accompanied by a negative example showing a case of lexical simplification, where a complex word is replaced by a simple one.", "labels": [], "entities": [{"text": "lexical simplification", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.6777823567390442}]}, {"text": "A positive example was not included so as not to bias the annotators by revealing the nature of the operations our experiments focus on (i.e., splitting and deletion).", "labels": [], "entities": []}, {"text": "The PWKP test corpus () was selected for our experiments over the development and test sets used in (, as the latter's selection process was explicitly biased towards input-output pairs that mainly contain lexical simplifications.", "labels": [], "entities": [{"text": "PWKP test corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9541450341542562}]}, {"text": "We further compute SAMSA for the 100 sentences of the PWKP test set and the corresponding system outputs.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.7735001444816589}, {"text": "PWKP test set", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.9592330654462179}]}, {"text": "Experiments are conducted in two settings: (1) a semi-automatic setting where UCCA annotation was carried out manually by a single expert UCCA annotator using the UCCAApp annotation software ( , and according to the standard annotation guidelines; 8 (2) an automatic setting where the UCCA annotation was carried out by the TUPA parser (.", "labels": [], "entities": [{"text": "TUPA", "start_pos": 324, "end_pos": 328, "type": "DATASET", "confidence": 0.9067105650901794}]}, {"text": "Sentence segmentation of the outputs was carried out using the NLTK package).", "labels": [], "entities": [{"text": "Sentence segmentation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9241603910923004}, {"text": "NLTK package", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9498946964740753}]}, {"text": "For word alignments, we used the aligner of.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7536201179027557}]}, {"text": "We compare the system rankings obtained by SAMSA and by the four human parameters.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.8295987248420715}]}, {"text": "We find that the two leading systems according to AvgHuman and SAMSA turnout to be the same: Split-Deletion and RevILP.", "labels": [], "entities": [{"text": "RevILP", "start_pos": 112, "end_pos": 118, "type": "DATASET", "confidence": 0.9312065243721008}]}, {"text": "This is the case both for the semi-automatic and the automatic implementations of the metric.", "labels": [], "entities": []}, {"text": "A Spearman \u03c1 correlation between the human and SAMSA scores (comparing their rankings) is presented in.", "labels": [], "entities": [{"text": "Spearman \u03c1 correlation", "start_pos": 2, "end_pos": 24, "type": "METRIC", "confidence": 0.9270395636558533}]}, {"text": "We compare SAMSA and SAMSA abl to the reference-based measures SARI 10 ( and BLEU, as well as to the negative Levenshtein distance to the reference (-LD SR ).", "labels": [], "entities": [{"text": "SAMSA abl", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.6403048634529114}, {"text": "SARI 10", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.8216112852096558}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9977284073829651}, {"text": "Levenshtein distance", "start_pos": 110, "end_pos": 130, "type": "METRIC", "confidence": 0.7514935731887817}, {"text": "LD SR )", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.9131475885709127}]}, {"text": "We use the only available reference for this corpus, in accordance with the standard practice.", "labels": [], "entities": []}, {"text": "SARI is a reference-based measure, based on n-gram overlap between the source, output and reference, and focuses on lexical (rather than structural) simplification.", "labels": [], "entities": []}, {"text": "For completeness, we include the other two measures reported in, which are measures of similarity to the input (i.e., they quantify the tendency of the systems to introduce changes to the input): the negative Levenshtein distances between the output and input compared to the original complex corpus (-LD SC ), and the number of sentences split by each of the systems.", "labels": [], "entities": []}, {"text": "The highest correlation with AvgHuman and grammaticality is obtained by semi-automatic SAMSA (0.58 and 0.54), a high correlation especially in comparison to the inter-annotator agreement on AvgHuman (0.64,).", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.5769280195236206}]}, {"text": "The automatic version obtains high correlation with human judgments in these settings, where for struc-tural simplicity, it scores somewhat higher than the semi-automatic SAMSA.", "labels": [], "entities": []}, {"text": "The highest correlation with structural simplicity is obtained by the number of sentences with splitting, where SAMSA (automatic and semi-automatic) is second and third highest, although when restricted to multiScene sentences, the correlation for SAMSA (semi-automatic) is higher (0.89, p = 0.009 and 0.77, p = 0.04).", "labels": [], "entities": []}, {"text": "The highest correlation for meaning preservation is obtained by SAMSA abl which provides further evidence that the retainment of semantic structures is a strong predictor of meaning preservation (.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8866779208183289}, {"text": "SAMSA abl", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.7954826653003693}, {"text": "meaning preservation", "start_pos": 174, "end_pos": 194, "type": "TASK", "confidence": 0.8637862205505371}]}, {"text": "SAMSA in itself does not correlate with meaning preservation, probably due to its penalization of under-splitting sentences.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.893130362033844}]}, {"text": "Note that the standard reference-based measures for simplification, BLEU and SARI, obtain low and often negative correlation with human ratings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9954094290733337}, {"text": "SARI", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.970848560333252}]}, {"text": "We believe that this is the case because SARI and BLEU admittedly focus on lexical simplification, and are difficult to use to rank systems which also perform structural simplification.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.981684148311615}]}, {"text": "Our results thus suggest that SAMSA provides additional value in predicting the quality of a simplification system and should be reported in tandem with more lexically-oriented measures.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 30, "end_pos": 35, "type": "TASK", "confidence": 0.4895240366458893}]}, {"text": "In order to provide further validation for SAMSA predictive value for quality of simplification systems, we report SAMSA's correlation with a recently proposed benchmark, used for the QATS (Quality Assessment for Text Simplification) shared task by three different lexical simplification systems) (48 pairs).", "labels": [], "entities": [{"text": "SAMSA predictive", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.8256862461566925}, {"text": "QATS (Quality Assessment for Text Simplification) shared task", "start_pos": 184, "end_pos": 245, "type": "TASK", "confidence": 0.5967766225337983}]}, {"text": "Human evaluation is also provided by this resource, with scores for overall quality, grammaticality, meaning preservation and simplicity.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.713474228978157}, {"text": "simplicity", "start_pos": 126, "end_pos": 136, "type": "METRIC", "confidence": 0.9943785071372986}]}, {"text": "Importantly, the simplicity score does not explicitly refer to the output's structural simplicity, but rather to its readability.", "labels": [], "entities": [{"text": "simplicity score", "start_pos": 17, "end_pos": 33, "type": "METRIC", "confidence": 0.9809988439083099}]}, {"text": "We focus on the overall human score, and compare it to SAMSA.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.7973313331604004}]}, {"text": "Since different systems were used to simplify different portions of the input, correlation is taken at the sentence level.", "labels": [], "entities": []}, {"text": "We use the same implementations of SAMSA.", "labels": [], "entities": []}, {"text": "Manual UCCA annotation is here performed by one of the authors of this paper.", "labels": [], "entities": [{"text": "UCCA annotation", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.5178630352020264}]}, {"text": "system in QATS obtained a correlation of 0.23.", "labels": [], "entities": [{"text": "QATS", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8304969668388367}]}, {"text": "This correlation by SAMSA was obtained in more restricted conditions, compared to the measures that competed in QATS.", "labels": [], "entities": [{"text": "SAMSA", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.4833388030529022}, {"text": "QATS", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.8497637510299683}]}, {"text": "First, SAMSA computes its score by only considering the UCCA structure of the source, and an automatic wordto-word alignment between the source and output.", "labels": [], "entities": []}, {"text": "Most QATS systems, including OSVCML and OSVCML2 which scored highest on the shared task, use an ensemble of classifiers based on bag-of-words, POS tags, sentiment information, negation, readability measures and other resources.", "labels": [], "entities": [{"text": "OSVCML", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9114823341369629}, {"text": "OSVCML2", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.931032657623291}]}, {"text": "Second, the systems participating in the shared task had training data available to them, annotated by the same annotators as the test data.", "labels": [], "entities": []}, {"text": "This was used to train classifiers for predicting their score.", "labels": [], "entities": [{"text": "predicting their", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8840292692184448}]}, {"text": "This gives the QATS measures much predictive strength, but hampers their interpretability.", "labels": [], "entities": []}, {"text": "SAMSA on the other hand is conceptually simple and interpretable.", "labels": [], "entities": []}, {"text": "Third, the QATS shared task does not focus on structural simplification, but experiments on different types of systems.", "labels": [], "entities": [{"text": "QATS shared task", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.5556822220484415}]}, {"text": "Indeed, some of the data was annotated by systems that exclusively perform lexical simplification, which is orthogonal to SAMSA's structural focus.", "labels": [], "entities": []}, {"text": "Given these factors, SAMSA's competitive correlation with the participating systems in QATS suggests that structural simplicity, as reflected by the correct splitting of UCCA Scenes, captures a major component in overall simplification quality, underscoring SAMSA's value.", "labels": [], "entities": [{"text": "QATS", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.8573384881019592}]}, {"text": "These promising results also motivate a future combination of SAMSA with classifier-based metrics.", "labels": [], "entities": []}], "tableCaptions": []}