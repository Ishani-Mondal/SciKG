{"title": [{"text": "Using Word Vectors to Improve Word Alignments for Low Resource Machine Translation", "labels": [], "entities": [{"text": "Improve Word Alignments", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.8354874650637308}, {"text": "Low Resource Machine Translation", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.6217888966202736}]}], "abstractContent": [{"text": "We present a method for improving word alignments using word similarities.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.7387414872646332}]}, {"text": "This method is based on encouraging common alignment links between semantically similar words.", "labels": [], "entities": []}, {"text": "We use word vectors trained on mono-lingual data to estimate similarity.", "labels": [], "entities": []}, {"text": "Our experiments on translating fifteen languages into En-glish show consistent BLEU score improvements across the languages.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9781557023525238}]}], "introductionContent": [{"text": "Word alignments are essential for statistical machine translation (MT), especially in low-resource settings where neural MT systems often do not compete with phrase-based and syntax-based MT (.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6897574663162231}, {"text": "statistical machine translation (MT)", "start_pos": 34, "end_pos": 70, "type": "TASK", "confidence": 0.8040660818417867}]}, {"text": "The most widely used word alignment method ( works by estimating the parameters of IBM models from training data using the Expectation Maximization (EM) algorithm.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.8773160874843597}, {"text": "Expectation Maximization (EM)", "start_pos": 123, "end_pos": 152, "type": "TASK", "confidence": 0.738379031419754}]}, {"text": "However, EM works poorly for low-frequency words as they do not appear enough in the training data for confident parameter estimation.", "labels": [], "entities": [{"text": "EM", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.6349482536315918}, {"text": "confident parameter estimation", "start_pos": 103, "end_pos": 133, "type": "TASK", "confidence": 0.619520346323649}]}, {"text": "This problem is even worse in low-resource settings where a large portion of word types appear infrequently in the parallel data.", "labels": [], "entities": []}, {"text": "In this paper we improve word alignments and consequently machine translation in low resource settings by improving the alignments of infrequent tokens.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7738513052463531}, {"text": "machine translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.8172785341739655}]}, {"text": "Works that deal with the rare-word problem in word alignment include those that alter the probability distribution of IBM models' parameters by adding prior distributions (, smoothing the probabilities; Van Bui and Le, 2016) or introducing symmetrization ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7982774078845978}]}, {"text": "These works, although effective, merely rely on the information extracted from the parallel data.", "labels": [], "entities": []}, {"text": "Another branch adds linguistic knowledge like word stems, orthography) morphological analysis), syntactic constraints () or a mixture of such clues).", "labels": [], "entities": []}, {"text": "These methods need languagespecific knowledge or tools like morphological analyzers or syntax parsers that is costly and time consuming to obtain for any given language.", "labels": [], "entities": []}, {"text": "A less explored branch that can help aligning rare words is adding semantic information.", "labels": [], "entities": []}, {"text": "The motivation behind this branch is simple: Words with similar meanings should have similar translations.", "labels": [], "entities": []}, {"text": "Previously, cluster words using monolingual data and substitute each word with its cluster representative to get alignments.", "labels": [], "entities": []}, {"text": "They then duplicate their parallel data and use both regular alignments and alignments on word classes for training MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.8757684230804443}]}, {"text": "Ko\u010disk` simultaneously learn alignments and word representations from bilingual data.", "labels": [], "entities": []}, {"text": "Their method does not benefit from monolingual data and requires large parallel data for training.", "labels": [], "entities": []}, {"text": "define a word-similarity model that can be trained from monolingual data using a feed-forward neural network, and alter the implementation of IBM models in Giza++ ( to use the word similarity inside their EM.", "labels": [], "entities": []}, {"text": "They require large monolingual data for both source language and English.", "labels": [], "entities": []}, {"text": "While English monolingual data is abundant, availability of large and reliable monolingual data for many low resource languages is not guaranteed.", "labels": [], "entities": []}, {"text": "All these previous works define their own word similarity models, which similar to the more widely used distributed word representation methods (), assign high similarity to substitutable words in a given context; however, substitutability does not always imply synonymy.", "labels": [], "entities": []}, {"text": "For instance tea and coffee, or Pakistan and Afghanistan will be similar in these models but do not share translations.", "labels": [], "entities": []}, {"text": "In this paper we propose a simple method to use off-the-shelf distributed representation methods to improve word alignments for low-resource machine translation (Section 2).", "labels": [], "entities": [{"text": "word alignments", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.7562128901481628}, {"text": "low-resource machine translation", "start_pos": 128, "end_pos": 160, "type": "TASK", "confidence": 0.6637506981690725}]}, {"text": "Our model is based on encouraging common alignment links between semantically similar words.", "labels": [], "entities": []}, {"text": "We do this by extracting a bilingual lexicon, as a subset of the translation tables trained by IBM models and adding it to the parallel data.", "labels": [], "entities": []}, {"text": "For instance, the rare word obliterated and its semantically similar word destroyed, have a common entry destruida in the English/Spanish translation table.", "labels": [], "entities": []}, {"text": "We add anew (obliterated, destruida) pair to the parallel data to encourage aligning obliterated to destruida.", "labels": [], "entities": []}, {"text": "The simplicity of our method makes it easy to be widely used.", "labels": [], "entities": []}, {"text": "Our work addresses a major problem of previous works, which is taking substitutability for synonymy without discrimination.", "labels": [], "entities": []}, {"text": "Finally, the lexicon can be extracted either with or without help of word vectors trained on foreign language monolingual data.", "labels": [], "entities": []}, {"text": "Large and reliable foreign monolingual data can help our alignments, but we still get good improvements over baseline for languages with small monolingual data where we only use English word vectors (Section 4).", "labels": [], "entities": []}, {"text": "We test our method on both alignment fscore and machine translation BLEU (Section 4).", "labels": [], "entities": [{"text": "alignment fscore", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.8945245146751404}, {"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.5988676846027374}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9623651504516602}]}, {"text": "Alignment accuracy is tested on Arabic-English, Chinese-English and Farsi-English gold alignments.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.6885598301887512}, {"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9558529853820801}]}, {"text": "Machine translation accuracy is tested on fifteen languages were we show a consistent BLEU score improvement.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7319584786891937}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9332255721092224}, {"text": "BLEU score", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9710599184036255}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Machine translation experiments (BLEU).  For languages with less than 10M monolingual tokens  (first five) we only use L e , otherwise we use both lexi- cons L e +L f . This way we improve baseline for almost  all languages.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7858262360095978}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.8424708843231201}]}]}