{"title": [{"text": "Early Text Classification using Multi-Resolution Concept Representations", "labels": [], "entities": [{"text": "Early Text Classification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7136562665303549}, {"text": "Multi-Resolution Concept Representations", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.630825271209081}]}], "abstractContent": [{"text": "This paper proposes a novel document representation , called Multi-Resolution Representation (MulR), to improve the early detection of risks in social media sources.", "labels": [], "entities": [{"text": "early detection of risks in social media sources", "start_pos": 116, "end_pos": 164, "type": "TASK", "confidence": 0.7874795272946358}]}, {"text": "The goal is to effectively identify the potential risk using as little evidence as possible and with as much anticipation as possible.", "labels": [], "entities": []}, {"text": "MulR allows us to generate multiple \"views\" of the text.", "labels": [], "entities": [{"text": "MulR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5079861879348755}]}, {"text": "These views capture different semantic meanings for words and documents at different levels of granularity, which is very useful in early scenarios to model the variable amounts of evidence.", "labels": [], "entities": []}, {"text": "The experimental evaluation shows that MulR using low resolution is better suited for modeling short documents (very early stages), whereas large documents (medium/late stages) are better modeled with higher resolutions.", "labels": [], "entities": [{"text": "MulR", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.7774641513824463}]}, {"text": "We evaluate the proposed ideas in two different tasks where anticipation is critical: sexual predator detection and depression detection.", "labels": [], "entities": [{"text": "sexual predator detection", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.6251164376735687}, {"text": "depression detection", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.694005012512207}]}, {"text": "The experimental evaluation for these early tasks revealed that the proposed approach outperforms previous method-ologies by a considerable margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Everyday there is a huge amount of people interacting in many social media sites.", "labels": [], "entities": []}, {"text": "Unfortunately this immense cyber-world has been misused by cyber-criminals, who hide in the depths of the web.", "labels": [], "entities": []}, {"text": "For this reason, the social media information has been increasingly studied in the context of applications related to security, forensics and e-commerce.", "labels": [], "entities": []}, {"text": "Recently the early prediction scenarios have attracted the attention of the scientific community (, which aims to prevent major threats in a number of practical situations by analyzing the text as evidence (e.g., sexual harassment, cyberbullying, etc).", "labels": [], "entities": []}, {"text": "In Natural Language Processing this emerging field is called early text classification and the goal is to identify risky-target categories by using as few text as possible and with as much anticipation as possible.", "labels": [], "entities": [{"text": "early text classification", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6455416480700175}]}, {"text": "In real scenarios the amount of evidence available from users under analysis is continuously growing.", "labels": [], "entities": []}, {"text": "Consider for instance chat rooms, or posts and comments in social networks, these text sources comprise cumulative evidence for early prediction that can be used to better capture the phenomenon understudy.", "labels": [], "entities": []}, {"text": "This scenario has challenging particularities.", "labels": [], "entities": []}, {"text": "For example, in early stages where 10% or 20% of the information is available it is necessary to model very short length documents, which tend to produce sparse and low discriminative representations.", "labels": [], "entities": []}, {"text": "On the other hand late stages require to exploit as much evidence as possible to make accurate predictions.", "labels": [], "entities": []}, {"text": "This dynamism between the document length and classification stages makes necessary an adequate representation, that naturally copes with the dynamic amount of evidence in short and long texts generated by users at each stage.", "labels": [], "entities": []}, {"text": "Traditional textual representations, such as Bag-of-Words (BoW)), have problems dealing with social media short texts since they cause the representation to be high dimensional and very sparse.", "labels": [], "entities": []}, {"text": "Moreover, in the particular case of early risk prediction, class unbalance and noisy text also represent a challenge.", "labels": [], "entities": [{"text": "early risk prediction", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7901671528816223}]}, {"text": "In this paper we propose a representation that deals with these challenges by taking advantage of word vectors into a novel methodology for representing documents.", "labels": [], "entities": []}, {"text": "This representation generates high-level features, that we called meta-words, which capture concepts at different resolution levels.", "labels": [], "entities": []}, {"text": "A meta-word is a primitive construction represented by a vector that summarizes the information of semantically related words.", "labels": [], "entities": []}, {"text": "Our methodology associates words with similar semantic meaning to the same meta-words.", "labels": [], "entities": []}, {"text": "These meta-words are obtained by applying clustering techniques to word representations, where the resultant \"centroids\" comprise the meta-words.", "labels": [], "entities": []}, {"text": "Documents are then represented by a Bag-of-Centroids (BoC), that is, a histogram accounting for the occurrence of coarse thematic/semantic primitives, i.e., the meta-words.", "labels": [], "entities": []}, {"text": "This part of the work is inspired by the Bag-of-Visual-Words (BoVW), which is widely used in computer vision to represent images ().", "labels": [], "entities": []}, {"text": "The key aspect for early scenarios is that the number and size of meta-words, allow us to manipulate the level of granularity or the resolution of the representation.", "labels": [], "entities": []}, {"text": "This property is very useful to capture discriminative information along the growing amount of available evidence at each early stage.", "labels": [], "entities": []}, {"text": "We thus propose a multi-resolution approach, in which primitives at different resolutions are combined to capture feature concepts at multiple levels of detail.", "labels": [], "entities": []}, {"text": "The contributions of this paper are twofold: (i) anew Multi-Resolution (MulR) document representation, a generalization to represent documents by exploiting wordvectors at different levels of resolutions; (ii) an empirical validation of the usefulness of multiple resolution levels for early risk detection on social media documents.", "labels": [], "entities": [{"text": "Multi-Resolution (MulR) document representation", "start_pos": 54, "end_pos": 101, "type": "TASK", "confidence": 0.5515652646621069}, {"text": "early risk detection on social media documents", "start_pos": 286, "end_pos": 332, "type": "TASK", "confidence": 0.764196936573301}]}, {"text": "Our experimental results show that this approach is a promising alternative for early text classification scenarios, where there is a need to make predictions as soon as possible, with little evidence, while at the same time, being robust to incorporate more evidence as it becomes available.", "labels": [], "entities": [{"text": "text classification", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7039624154567719}]}, {"text": "We recorded experimental results of an extensive evaluation of our proposed techniques over two benchmarks for early scenarios: sexual predator detection and depression detection.", "labels": [], "entities": [{"text": "sexual predator detection", "start_pos": 128, "end_pos": 153, "type": "TASK", "confidence": 0.6690135399500529}, {"text": "depression detection", "start_pos": 158, "end_pos": 178, "type": "TASK", "confidence": 0.7280249297618866}]}, {"text": "Results showed that in all cases our methodology outperforms state-of-the-art methodologies.", "labels": [], "entities": []}, {"text": "Interestingly, document representations based on partitioning the word-embedding space, like ours, are somewhat similar to topic modeling based representations.", "labels": [], "entities": []}, {"text": "In the experimental section we also compare the performance of our method to different topic-based representations like Latent Semantic Analysis (LSA)) and Latent Dirichlet Allocation (LDA) (.", "labels": [], "entities": []}, {"text": "Experimental results showed that our method outperformed the reference techniques.", "labels": [], "entities": []}, {"text": "We elaborate on the benefits and limitations of our proposed techniques later in this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we lowercase the text in documents and use words and punctuation marks as terms . The representation obtained for each document is then processed by a Support Vector Machine (SVM) with a linear kernel.", "labels": [], "entities": []}, {"text": "For the evaluation of the earliness performance, we report the performance of the different methods when using increasing amounts of textual evidence (chunk by chunk evaluation).", "labels": [], "entities": []}, {"text": "This evaluation allows to quantify prediction performance when using partial information in documents, and it is a strategy that has been used to evaluate early classification ().", "labels": [], "entities": [{"text": "early classification", "start_pos": 155, "end_pos": 175, "type": "TASK", "confidence": 0.7188987135887146}]}, {"text": "For the evaluation of performance we used the f 1 = 2\u00d7precision\u00d7recall precision+recall measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9850047826766968}, {"text": "recall precision", "start_pos": 64, "end_pos": 80, "type": "METRIC", "confidence": 0.8910455405712128}, {"text": "recall measure", "start_pos": 81, "end_pos": 95, "type": "METRIC", "confidence": 0.9271823167800903}]}, {"text": "This decision was made in agreement with previous work that reports this metric for the positive class (.", "labels": [], "entities": []}, {"text": "Please note that, contrary to other measures, such as accuracy, f 1 measure accounts for the class imbalance problem when only the positive class is analyzed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.999362051486969}]}, {"text": "This is desirable for the data sets we consider as they are highly unbalanced.", "labels": [], "entities": []}, {"text": "Word-vector representations: As previously mentioned, the proposed MulR representation generalizes word-vector representations and thus can extend any representation that models each term in the vocabulary using a vector.", "labels": [], "entities": []}, {"text": "For this purpose a wide variety of word embeddings or distributional term representations could be used.", "labels": [], "entities": []}, {"text": "Both of them exploit the distributional hypothesis to build word vectors, nonetheless they differ in the strategy to capture the relevant information.", "labels": [], "entities": []}, {"text": "In this work we use the widely used word2vec, but also other representations that have been used in recent works for these collections.", "labels": [], "entities": []}, {"text": "In we describe each of the word vector representations considered for this work 2 . Baselines: The main baselines in this work are methods based on the idea of topic modeling for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.7967366278171539}]}, {"text": "Topic-based representations group words into topics defined by a set of related words . Given the strong relation to our method we compare our proposal against Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).", "labels": [], "entities": []}, {"text": "Furthermore, we also compare with Bag-of-Words using Term Frequency Inverse Document Frequency, since it is a traditional baseline in text categorization tasks.", "labels": [], "entities": []}, {"text": "In this section we report the experimental results for the MulR representation and the selected approaches from the state-of-the-art.", "labels": [], "entities": [{"text": "MulR representation", "start_pos": 59, "end_pos": 78, "type": "DATASET", "confidence": 0.7104077637195587}]}, {"text": "In all the experiments we trained the reference classifier (SVM) using full-length documents in the training dataset.", "labels": [], "entities": []}, {"text": "In the testing phase, each approach uses all the available information in each of the ten chunks (each chunk increases the available text in 10%).", "labels": [], "entities": []}, {"text": "More specifically, we generate document representations starting with the first chunk, and then incrementally adding one chunk at a time.", "labels": [], "entities": []}, {"text": "The Word Representation Description W2V ( Word2Vec uses the Skip-ngram model to find word representations that are useful to predict the surrounding words of a sentence or a document.", "labels": [], "entities": [{"text": "Word Representation Description W2V", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.8237041905522346}]}, {"text": "The method is efficient for learning high-quality vector representations of words from large amounts of text data.", "labels": [], "entities": []}, {"text": "We empirically set to 200 the vector dimension.", "labels": [], "entities": []}, {"text": "DOR () Document Occurrence Representation (DOR) captures the semantics of a word by observing occurrence distribution over documents in the corpus.", "labels": [], "entities": [{"text": "Document Occurrence Representation (DOR) captures the semantics of a word", "start_pos": 7, "end_pos": 80, "type": "TASK", "confidence": 0.7109066545963287}]}, {"text": "DOR represents each word vi as a vector ti = ti,1, . .", "labels": [], "entities": []}, {"text": ", t i,|D| , where |D| is the number of documents in the training collection, and t i,k indicates the relevance of the document D k to characterize vi.", "labels": [], "entities": []}, {"text": "TCOR( In Term Co-occurrence Representation (TCOR) the semantics of a word is captured by observing its co-occurrences with other words across documents in the corpus.", "labels": [], "entities": []}, {"text": "Thus, each word vi is associated to a vector ti = ti,1, . .", "labels": [], "entities": []}, {"text": ", t i,|V| , where |V| indicates the vocabulary size, and t i,k denotes the contribution of the word v k to the semantic description of vi.", "labels": [], "entities": []}, {"text": "PBR ( Profile Based Representation exploits occurrence-statistics of words over a set of documents in target categories.", "labels": [], "entities": [{"text": "PBR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6330341696739197}]}, {"text": "PBR represents each word vi \u2208 V with a vector ti = ti,1, . .", "labels": [], "entities": []}, {"text": ", ti,q, where the t i,k is the degree of association between word viand category C k . The target categories can betaken from the task or artificially created by means of clustering such as in ().", "labels": [], "entities": []}, {"text": "TVT ( Temporal Variation Terms (TVT) is an adapted version of PBR for early scenarios.", "labels": [], "entities": [{"text": "TVT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8630303740501404}]}, {"text": "TVT builds new artificial target classes/labels in the training set simulating a text stream to generate enriched representations oft i . The idea is to exploit the positive category to create a set of new artificial categories using text-fragments.: Data sets considered for early experimentation.", "labels": [], "entities": [{"text": "TVT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9519596099853516}]}, {"text": "There are only two classes in each dataset.", "labels": [], "entities": []}, {"text": "models will then make predictions incrementally as well.", "labels": [], "entities": []}, {"text": "We report f 1 performance when using different amounts of text from test documents.", "labels": [], "entities": []}, {"text": "For the proposed MulR representation, we build 5 different resolutions: 10, 50, 100, 500, and 1000.", "labels": [], "entities": []}, {"text": "The goal was to generate meta-words at different levels of granularity, and we plan to further explore the impact of these resolutions in our future research.", "labels": [], "entities": []}, {"text": "In the following experiments, we used the word representations in to build our proposed MulR document representation.", "labels": [], "entities": [{"text": "MulR document representation", "start_pos": 88, "end_pos": 116, "type": "DATASET", "confidence": 0.8774719834327698}]}, {"text": "For comparison purposes we also generate an alternative document representation by averaging (Avg) term-vectors of words in each document, which is a popular strategy to build document representations.", "labels": [], "entities": []}, {"text": "Finally, we also compare against several traditional baselines such as the Bag-of-Words, LSA, and specialized methods in each collection (.", "labels": [], "entities": [{"text": "Bag-of-Words", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.7236591577529907}]}, {"text": "We evaluate the usefulness of all these different representations in the two early classification tasks mentioned earlier.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data sets considered for early experimenta- tion. There are only two classes in each dataset.", "labels": [], "entities": []}, {"text": " Table 3: F 1 results for the chunk by chunk evaluation of different approaches in Sexual Predator Detection. The  proposed MulR is evaluated using different word vector representations in the literature.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9364756345748901}, {"text": "Sexual Predator Detection", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.6916703482468923}]}, {"text": " Table 4: F 1 results for the chunk by chunk evaluation of different approaches in Depression Detection. The  proposed MulR is evaluated using different word vector representations in the literature.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9572218656539917}, {"text": "Depression Detection", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.839575856924057}]}, {"text": " Table 5: F 1 results for the chunk by chunk evaluation of different approaches in Sexual Predator Detection. The  best MulR(TVT) is separately evaluated under each resolution.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9747432470321655}, {"text": "Sexual Predator Detection", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.6327473620573679}, {"text": "MulR(TVT)", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.891484409570694}]}, {"text": " Table 6: Post-analysis in test dataset. Distribution of  the top ten meta-words according to each resolution R i  at different chunks. We used Information Gain (", "labels": [], "entities": []}]}