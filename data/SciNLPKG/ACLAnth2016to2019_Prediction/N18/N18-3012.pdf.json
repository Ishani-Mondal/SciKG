{"title": [{"text": "Can Neural Machine Translation be Improved with User Feedback?", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.8493551214536031}]}], "abstractContent": [{"text": "We present the first real-world application of methods for improving neural machine translation (NMT) with human reinforcement , based on explicit and implicit user feedback collected on the eBay e-commerce platform.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 69, "end_pos": 101, "type": "TASK", "confidence": 0.8418903748194376}]}, {"text": "Previous work has been confined to simulation experiments, whereas in this paper we work with real logged feedback for offline bandit learning of NMT parameters.", "labels": [], "entities": []}, {"text": "We conduct a thorough analysis of the available explicit user judgments-five-star ratings of translation quality-and show that they are not reliable enough to yield significant improvements in bandit learning.", "labels": [], "entities": [{"text": "translation", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.9532016515731812}]}, {"text": "In contrast , we successfully utilize implicit task-based feedback collected in a cross-lingual search task to improve task-specific and machine translation quality metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "In commercial scenarios of neural machine translation (NMT), the one-best translation of a text is shown to multiple users who can reinforce highquality (or penalize low-quality) translations by explicit feedback (e.g., on a Likert scale) or implicit feedback (by clicking on a translated page).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.8805864850680033}]}, {"text": "In such settings this type of feedback can be easily collected in large amounts.", "labels": [], "entities": []}, {"text": "While bandit feedback inform of user clicks on displayed ads is the standard learning signal for response prediction in online advertising (, bandit learning for machine translation has so far been restricted to simulation experiments (; * The work for this paper was done while the first author was an intern at eBay.", "labels": [], "entities": [{"text": "response prediction", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7380917966365814}, {"text": "machine translation", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.7399702966213226}]}, {"text": "The fact that only feedback fora single translation is collected constitutes the \"bandit feedback\" scenario where the name is inspired by \"one-armed bandit\" slot machines.).", "labels": [], "entities": []}, {"text": "The goal of our work is to show that the goldmine of cheap and abundant real-world human bandit feedback can be exploited successfully for machine learning in NMT.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.7057816386222839}]}, {"text": "We analyze and utilize human reinforcements that have been collected from users of the eBay e-commerce platform.", "labels": [], "entities": [{"text": "eBay e-commerce platform", "start_pos": 87, "end_pos": 111, "type": "DATASET", "confidence": 0.8888285358746847}]}, {"text": "We show that explicit user judgments inform of fivestar ratings are not reliable and do not lead to downstream BLEU improvements in bandit learning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9946314096450806}]}, {"text": "In contrast, we find that implicit task-based feedback that has been gathered in a cross-lingual search task can be used successfully to improve task-specific metrics and BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9982302784919739}]}, {"text": "Another crucial difference of our work to previous research is the fact that we assume a counterfactual learning scenario where human feedback has been given to a historic system different from the target system.", "labels": [], "entities": []}, {"text": "Learning is done offline from logged data, which is desirable in commercial settings where system updates need to be tested before deployment and the risk of showing inferior translations to users needs to be avoided.", "labels": [], "entities": []}, {"text": "Our offline learning algorithms range from a simple bandit-to-supervised conversion (i.e., using translations with good feedback for supervised tuning) to transferring the counterfactual learning techniques presented by from statistical machine translation (SMT) to NMT models.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 225, "end_pos": 262, "type": "TASK", "confidence": 0.7661709040403366}]}, {"text": "To our surprise, the bandit-to-supervised conversion proved to be very hard to beat, despite theoretical indications of poor generalization for exploration-free learning from logged data.", "labels": [], "entities": []}, {"text": "However, we show that we can further improve over this method by computing a task-specific reward scoring function, resulting in significant improvements in both BLEU and in task-specific metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9992334842681885}]}, {"text": "introduced learning from bandit feedback for SMT models in an interactive online learning scenario: the MT model receives a source sentence from the user, provides a translation, receives feedback from the user for this translation, and performs a stochastic gradient update proportional to the feedback quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9934207797050476}]}, {"text": "showed that the objectives proposed for log-linear models can be transferred to neural sequence learning and found that standard control variate techniques do not only reduce variance but also help to produce best BLEU results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 214, "end_pos": 218, "type": "METRIC", "confidence": 0.9974641799926758}]}, {"text": "proposed a very similar approach using a learned word-based critic in an advantage actor-critic reinforcement learning framework.", "labels": [], "entities": []}, {"text": "A comparison of current approaches was recently performed in a shared task where participants had to build translation models that learn from the interaction with a service that provided e-commerce product descriptions and feedback for submitted translations (. were the first to address the more realistic problem of offline learning from logged bandit feedback, with special attention to the problem of exploration-free deterministic logging as is done in commercial MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 469, "end_pos": 471, "type": "TASK", "confidence": 0.956786572933197}]}, {"text": "They show that variance reduction techniques used in counterfactual bandit learning) and off-policy reinforcement learning can be used to avoid degenerate behavior of estimators under deterministic logging.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Examples for averaged five-star user ratings, five-star expert ratings and expert judgments on the user ratings.", "labels": [], "entities": []}, {"text": " Table 3: Results for the reward estimators trained and eval- uated on human star ratings and simulated sBLEU.", "labels": [], "entities": []}, {"text": " Table 4: Results for models trained on explicit user ratings  evaluated on the product titles test set. 'small' indicates a  random subset of logged translations of the same size as the  filtered log that only contains translations with an average rat- ing of five stars ('stars = 5'). The differences in BLEU are  not significant at p \u2264 0.05 between MIX models, but over  other models.", "labels": [], "entities": [{"text": "product titles test set", "start_pos": 80, "end_pos": 103, "type": "DATASET", "confidence": 0.7593115121126175}, {"text": "rat- ing", "start_pos": 249, "end_pos": 257, "type": "METRIC", "confidence": 0.7512414952119192}, {"text": "BLEU", "start_pos": 306, "end_pos": 310, "type": "METRIC", "confidence": 0.9988058805465698}]}, {"text": " Table 5: Results for models trained on implicit task-based  feedback data evaluated on the product titles test set. 'small'  indicates a random subset of logged translations of the same  size as the filtered log that only contains translations that con- tain all the query words ('recall = 1'). The BLEU score of  MIX (small) significantly differs from MIX (all) at p \u2264 0.05,  the score of MIX (recall = 1) does not. Other differences are  significant.", "labels": [], "entities": [{"text": "product titles test set", "start_pos": 92, "end_pos": 115, "type": "DATASET", "confidence": 0.7092210948467255}, {"text": "recall", "start_pos": 282, "end_pos": 288, "type": "METRIC", "confidence": 0.9912119507789612}, {"text": "BLEU score", "start_pos": 300, "end_pos": 310, "type": "METRIC", "confidence": 0.9810247123241425}, {"text": "recall", "start_pos": 396, "end_pos": 402, "type": "METRIC", "confidence": 0.6370985507965088}]}, {"text": " Table 6: Query recall results on the query test set, comparing  the logged translations, the baseline and the best MIX mod- els trained on logged translations (MIX (all) from Tables 4  and 5) with the W-MIX model trained via word-based query  matching (W-MIX from Table 5).", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.7186743021011353}]}, {"text": " Table 7: Corpus statistics for the out-of domain training data  and in-domain dev and test data.", "labels": [], "entities": []}, {"text": " Table 8: Data set sizes for collected feedback in number of  sentences. The in-domain title translations are only used for  simulation experiments.", "labels": [], "entities": []}, {"text": " Table 9: Hyperparameter settings for training of the models.", "labels": [], "entities": []}, {"text": " Table 10: Results for simulation experiments evaluated on  the product titles test set.", "labels": [], "entities": [{"text": "product titles test set", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.7130637988448143}]}, {"text": " Table 11: BLEU results for simulation models evaluated on  the News Commentary test set (nc-test2007) with beam  search and greedy decoding. SMT results are from Lawrence  et al. (2017b).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9982909560203552}, {"text": "News Commentary test set", "start_pos": 64, "end_pos": 88, "type": "DATASET", "confidence": 0.9744234681129456}, {"text": "SMT", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.9671096801757812}]}]}