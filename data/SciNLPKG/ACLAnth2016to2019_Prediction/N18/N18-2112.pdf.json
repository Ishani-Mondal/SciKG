{"title": [{"text": "Feudal Reinforcement Learning for Dialogue Management in Large Domains", "labels": [], "entities": [{"text": "Feudal Reinforcement Learning", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7538543144861857}, {"text": "Dialogue Management", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.8678100109100342}]}], "abstractContent": [{"text": "Reinforcement learning (RL) is a promising approach to solve dialogue policy optimisa-tion.", "labels": [], "entities": [{"text": "Reinforcement learning (RL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8668290257453919}]}, {"text": "Traditional RL algorithms, however, fail to scale to large domains due to the curse of dimensionality.", "labels": [], "entities": [{"text": "RL", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9538691639900208}]}, {"text": "We propose a novel Dialogue Management architecture, based on Feudal RL, which decomposes the decision into two steps; a first step where a master policy selects a subset of primitive actions, and a second step where a primitive action is chosen from the selected subset.", "labels": [], "entities": [{"text": "Feudal RL", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.8589425981044769}]}, {"text": "The structural information included in the domain ontology is used to abstract the dialogue state space, taking the decisions at each step using different parts of the abstracted state.", "labels": [], "entities": []}, {"text": "This, combined with an information sharing mechanism between slots, increases the scalability to large domains.", "labels": [], "entities": []}, {"text": "We show that an implementation of this approach, based on Deep-Q Networks, significantly outperforms previous state of the art in several dialogue domains and environments , without the need of any additional reward signal.", "labels": [], "entities": []}], "introductionContent": [{"text": "Task-oriented Spoken Dialogue Systems (SDS), in the form of personal assistants, have recently gained much attention in both academia and industry.", "labels": [], "entities": [{"text": "Task-oriented Spoken Dialogue Systems (SDS)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7133974730968475}]}, {"text": "One of the most important modules of a SDS is the Dialogue Manager (DM) (or policy), the module in charge of deciding the next action in each dialogue turn.", "labels": [], "entities": [{"text": "SDS", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9545990824699402}]}, {"text": "Reinforcement Learning (RL)) has been studied for several years as a promising approach to model dialogue management (.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL))", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.694140625}, {"text": "model dialogue management", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.6505570212999979}]}, {"text": "However, as the dialogue state space increases, the number of possible trajectories needed to be ex- * Currently at PolyAI, inigo@poly-ai.com plored grows exponentially, making traditional RL methods not scalable to large domains.", "labels": [], "entities": [{"text": "PolyAI", "start_pos": 116, "end_pos": 122, "type": "DATASET", "confidence": 0.9525986909866333}]}, {"text": "Hierarchical RL (HRL), in the form of temporal abstraction, has been proposed in order to mitigate this problem.", "labels": [], "entities": [{"text": "Hierarchical RL (HRL)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7335684776306153}]}, {"text": "However, proposed HRL methods require that the task is defined in a hierarchical structure, which is usually handcrafted.", "labels": [], "entities": []}, {"text": "In addition, they usually require additional rewards for each subtask.", "labels": [], "entities": []}, {"text": "Space abstraction, instead, has been successfully applied to dialogue tasks such as Dialogue State Tracking (DST) (, and policy transfer between domains.", "labels": [], "entities": [{"text": "Dialogue State Tracking (DST)", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.8009552657604218}, {"text": "policy transfer between domains", "start_pos": 121, "end_pos": 152, "type": "TASK", "confidence": 0.8325936049222946}]}, {"text": "For DST, a set of binary classifiers can be defined for each slot, with shared parameters, learning a general way to track slots.", "labels": [], "entities": [{"text": "DST", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9392890334129333}]}, {"text": "The policy transfer method presented in (, named Domain Independent Parametrisation (DIP), transforms the belief state into a slot-dependent fixed size representation using a handcrafted feature function.", "labels": [], "entities": [{"text": "policy transfer", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7241038978099823}]}, {"text": "This idea could also be applied to large domains, since it can be used to learn a general way to act in any slot.", "labels": [], "entities": []}, {"text": "In slot-filling dialogues, a HRL method that relies on space abstraction, such as Feudal RL (FRL), should allow RL scale to domains with a large number of slots.", "labels": [], "entities": []}, {"text": "FRL divides a task spatially rather than temporally, decomposing the decisions in several steps and using different abstraction levels in each sub-decision.", "labels": [], "entities": [{"text": "FRL divides a task spatially", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6757230520248413}]}, {"text": "This framework is especially useful in RL tasks with large discrete action spaces, making it very attractive for large domain dialogue management.", "labels": [], "entities": [{"text": "RL tasks", "start_pos": 39, "end_pos": 47, "type": "TASK", "confidence": 0.9429686665534973}, {"text": "large domain dialogue management", "start_pos": 113, "end_pos": 145, "type": "TASK", "confidence": 0.7000962346792221}]}, {"text": "In this paper, we introduce a Feudal Dialogue Policy which decomposes the decision in each turn into two steps.", "labels": [], "entities": []}, {"text": "Ina first step, the policy decides if it takes a slot independent or slot dependent action.", "labels": [], "entities": []}, {"text": "Then, the state of each slot sub-policy is abstracted to account for features related to that slot, and a primitive action is chosen from the previously selected subset.", "labels": [], "entities": []}, {"text": "Our model does not require any modification of the reward function and the hierarchical architecture is fully specified by the structured database representation of the system (i.e. the ontology), requiring no additional design.", "labels": [], "entities": []}], "datasetContent": [{"text": "The models used in the experiments have been implemented using the PyDial toolkit (Ultes et al., 2017) and evaluated on the PyDial benchmarking environment . This environment presents a set of tasks which span different size domains, different Semantic Error Rates (SER), and different configurations of action masks and user model parameters (Standard (Std.) or Unfriendly (Unf.)).", "labels": [], "entities": [{"text": "PyDial benchmarking environment", "start_pos": 124, "end_pos": 155, "type": "DATASET", "confidence": 0.8775731523831686}]}, {"text": "shows a summarised description of the tasks.", "labels": [], "entities": []}, {"text": "The models developed in this paper are compared to the state-ofthe-art RL algorithms and to the handcrafted policy presented in the benchmarks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sumarised description of the domains and  environments used in the experiments. Refer to  (Casanueva et al., 2017) for a detailed description.", "labels": [], "entities": []}, {"text": " Table 2: Success rate and reward for Feudal-DQN and  DIP-DQN in the 18 benchmarking tasks, compared  with the reward of the best performing algorithm in  each task (Bnch.) and the handcrafted policy (Hdc.)  presented in (Casanueva et al., 2017).", "labels": [], "entities": [{"text": "Success rate", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9591736197471619}, {"text": "Bnch.", "start_pos": 166, "end_pos": 171, "type": "METRIC", "confidence": 0.8656956553459167}]}]}