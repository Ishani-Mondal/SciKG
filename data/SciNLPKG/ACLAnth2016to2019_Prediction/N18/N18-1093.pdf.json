{"title": [{"text": "Learning Word Embeddings for Low-resource Languages by PU Learning", "labels": [], "entities": [{"text": "PU Learning", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9654840230941772}]}], "abstractContent": [{"text": "Word embedding is a key component in many downstream applications in processing natural languages.", "labels": [], "entities": []}, {"text": "Existing approaches often assume the existence of a large collection of text for learning effective word embedding.", "labels": [], "entities": []}, {"text": "However, such a corpus may not be available for some low-resource languages.", "labels": [], "entities": []}, {"text": "In this paper, we study how to effectively learn a word embedding model on a corpus with only a few million tokens.", "labels": [], "entities": []}, {"text": "In such a situation , the co-occurrence matrix is sparse as the co-occurrences of many word pairs are unob-served.", "labels": [], "entities": []}, {"text": "In contrast to existing approaches often only sample a few unobserved word pairs as negative samples, we argue that the zero entries in the co-occurrence matrix also provide valuable information.", "labels": [], "entities": []}, {"text": "We then design a Positive-Unlabeled Learning (PU-Learning) approach to factorize the co-occurrence matrix and validate the proposed approaches in four different languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning word representations has become a fundamental problem in processing natural languages.", "labels": [], "entities": [{"text": "Learning word representations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6068503161271414}]}, {"text": "These semantic representations, which map a word into a point in a linear space, have been widely applied in downstream applications, including named entity recognition (), document ranking (, sentiment analysis (), question answering (, and image captioning (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 144, "end_pos": 168, "type": "TASK", "confidence": 0.6471827228864034}, {"text": "document ranking", "start_pos": 173, "end_pos": 189, "type": "TASK", "confidence": 0.7766233086585999}, {"text": "sentiment analysis", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.9050570130348206}, {"text": "question answering", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.9027679562568665}, {"text": "image captioning", "start_pos": 242, "end_pos": 258, "type": "TASK", "confidence": 0.7154181003570557}]}, {"text": "Over the past few years, various approaches have been proposed to learn word vectors (e.g.,;) based on co-occurrence information between words observed on the training corpus.", "labels": [], "entities": []}, {"text": "The intuition behind this is to represent words with similar vectors if they have similar contexts.", "labels": [], "entities": []}, {"text": "To learn a good word embedding, most approaches assume a large collection of text is freely available, such that the estimation of word co-occurrences is accurate.", "labels": [], "entities": []}, {"text": "For example, the Google Word2Vec model) is trained on the Google News dataset, which contains around 100 billion tokens, and the GloVe embedding () is trained on a crawled corpus that contains 840 billion tokens in total.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.9165970881779989}]}, {"text": "However, such an assumption may not hold for low-resource languages such as Inuit or Sindhi, which are not spoken by many people or have not been put into a digital format.", "labels": [], "entities": []}, {"text": "For those languages, usually, only a limited size corpus is available.", "labels": [], "entities": []}, {"text": "Training word vectors under such a setting is a challenging problem.", "labels": [], "entities": []}, {"text": "One key restriction of the existing approaches is that they often mainly rely on the word pairs that are observed to co-occur on the training data.", "labels": [], "entities": []}, {"text": "When the size of the text corpus is small, most word pairs are unobserved, resulting in an extremely sparse co-occurrence matrix (i.e., most entries are zero) . For example, the text8 2 corpus has about 17,000,000 tokens and 71,000 distinct words.", "labels": [], "entities": [{"text": "text8 2 corpus", "start_pos": 178, "end_pos": 192, "type": "DATASET", "confidence": 0.7476316094398499}]}, {"text": "The corresponding co-occurrence matrix has more than five billion entries, but only about 45,000,000 are non-zeros (observed on the training corpus).", "labels": [], "entities": []}, {"text": "Most existing approaches, such as Glove and Skip-gram, cannot handle avast number of zero terms in the co-occurrence matrix; therefore, they only sub-sample a small subset of zero entries during the training.", "labels": [], "entities": []}, {"text": "In contrast, we argue that the unobserved word pairs can provide valuable information for training a word embedding model, especially when the co-occurrence matrix is very sparse.", "labels": [], "entities": []}, {"text": "Inspired by the success of Positive-Unlabeled Learning (PU-Learning) in collaborative filtering applications (, we design an algorithm to effectively learn word embeddings from both positive (observed terms) and unlabeled (unobserved/zero terms) examples.", "labels": [], "entities": []}, {"text": "Essentially, by using the square loss to model the unobserved terms and designing an efficient update rule based on linear algebra operations, the proposed PULearning framework can be trained efficiently and effectively.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the proposed approach in English 3 and other three resourcescarce languages.", "labels": [], "entities": []}, {"text": "We collected unlabeled language corpora from Wikipedia and compared the proposed approach with popular approaches, the Glove and the Skip-gram models, for training word embeddings.", "labels": [], "entities": []}, {"text": "The experimental results show that our approach significantly outperforms the baseline models, especially when the size of the training corpus is small.", "labels": [], "entities": []}, {"text": "Our key contributions are summarized below.", "labels": [], "entities": []}, {"text": "\u2022 We propose a PU-Learning framework for learning word embedding.", "labels": [], "entities": []}, {"text": "\u2022 We tailor the coordinate descent algorithm () for solving the corresponding optimization problem.", "labels": [], "entities": [{"text": "coordinate descent", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.6611510664224625}]}, {"text": "\u2022 Our experimental results show that PULearning improves the word embedding training in the low-resource setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our goal in this paper is to train word embedding models for low-resource languages.", "labels": [], "entities": []}, {"text": "In this section, we describe the experimental designs to evaluate the proposed PU-learning approach.", "labels": [], "entities": []}, {"text": "We first describe the data sets and the evaluation metrics.", "labels": [], "entities": []}, {"text": "Then, we provide details of parameter tuning.", "labels": [], "entities": []}, {"text": "We consider two widely used tasks for evaluating word embeddings, the word similarity task and the word analogy task.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 99, "end_pos": 111, "type": "TASK", "confidence": 0.7634253799915314}]}, {"text": "In the word similarity task, each question contains a word pairs and an annotated similarity score.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.8174185156822205}]}, {"text": "The goal is to predict the similarity score between two words based on the inner product between the corresponding word vectors.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 27, "end_pos": 43, "type": "METRIC", "confidence": 0.9577884376049042}]}, {"text": "The performance is then measured by the Spearmans rank correlation coefficient, which estimates the correlation between the model predictions and human annotations.", "labels": [], "entities": [{"text": "Spearmans rank correlation coefficient", "start_pos": 40, "end_pos": 78, "type": "METRIC", "confidence": 0.6876641064882278}]}, {"text": "In the word analogy task, we aim at solving analogy puzzles like \"man is to woman as king is to ?\", where the expected answer is \"queen.\"", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7557130455970764}]}, {"text": "We consider two approaches for generating answers to the puzzles, namely 3CosAdd and 3Cos-Mul (see () for details).", "labels": [], "entities": []}, {"text": "We evaluate the performances on Google analogy dataset (Mikolov et al., 2013a) which contains 8,860 semantic and 10,675 syntactic questions.", "labels": [], "entities": [{"text": "Google analogy dataset", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.796069324016571}]}, {"text": "For the analogy task, only the answer that exactly matches the annotated answer is counted as correct.", "labels": [], "entities": []}, {"text": "As a result, the analogy task is more difficult than the similarity task because the evaluation metric is stricter and it requires algorithms to differentiate words with similar meaning and find the right answer.", "labels": [], "entities": []}, {"text": "To evaluate the performances of models in the low-resource setting, we train word embedding models on Dutch, Danish, Czech and, English data sets collected from Wikipedia.", "labels": [], "entities": [{"text": "English data sets collected from Wikipedia", "start_pos": 128, "end_pos": 170, "type": "DATASET", "confidence": 0.8667023976643881}]}, {"text": "The original Wikipedia corpora in Dutch, Danish, Czech and English contain 216 million, 47 million, 92 million, and 1.8 billion tokens, respectively.", "labels": [], "entities": []}, {"text": "To simulate the low-resource setting, we sub-sample the Wikipedia corpora and create a subset of 64 million tokens for Dutch and Czech and a subset of 32 million tokens for English.", "labels": [], "entities": []}, {"text": "We will demonstrate how the size of the corpus affects the performance of embedding models in the experiments.", "labels": [], "entities": []}, {"text": "To evaluate the performance of word embeddings in Czech, Danish, and Dutch, we translate the English similarity and analogy test sets to the other languages by using Google Cloud Translation API . However, an English word maybe translated to multiple words in another language (e.g., compound nouns).", "labels": [], "entities": []}, {"text": "We discard questions containing such words (see for details).", "labels": [], "entities": []}, {"text": "Because all approaches are compared on the same test set for each language, the comparisons are fair.", "labels": [], "entities": []}, {"text": "We compared the proposed PU-Learning framework with two popular word embedding models -SGNS () and Glove) on English and three other languages.", "labels": [], "entities": []}, {"text": "The experimental results are reported in.", "labels": [], "entities": []}, {"text": "The results show that the proposed PULearning framework outperforms the two baseline approaches significantly inmost datasets.", "labels": [], "entities": []}, {"text": "This re-: Performance change as the corpus size growing (a) on the Google word analogy task (on the left-hand side) and (b) on the WS353 word similarity task (on the right-hand side).", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7787627776463827}, {"text": "WS353 word similarity task", "start_pos": 131, "end_pos": 157, "type": "TASK", "confidence": 0.7845208048820496}]}, {"text": "We demonstrate the performance on four languages, Dutch, Danish, Czech and English datasets.", "labels": [], "entities": []}, {"text": "Results show that PULearning model consistently outperforms SGNS and GloVe when the size of corpus is small..", "labels": [], "entities": []}, {"text": "As the corpus size grows, the performance of all models improves, and the PU-learning model consistently outperforms other methods in all the tasks.", "labels": [], "entities": []}, {"text": "However, with the size of the corpus increases, the difference becomes smaller.", "labels": [], "entities": []}, {"text": "This is reasonable as when the corpus size increases the number of nonzero terms becomes smaller and the PU-learning approach is resemblance to Glove.", "labels": [], "entities": []}, {"text": "Impacts of \u03c1 and \u03bb We investigate how sensitive the model is to the hyper-parameters, \u03c1 and \u03bb. shows the performance along with various values of \u03bb and \u03c1 when training on the text8 corpus, respectively.", "labels": [], "entities": [{"text": "text8 corpus", "start_pos": 175, "end_pos": 187, "type": "DATASET", "confidence": 0.8855517506599426}]}, {"text": "Note that the x-axis is in log scale.", "labels": [], "entities": []}, {"text": "When \u03c1 is fixed, a big \u03bb degrades the performance of the model significantly.", "labels": [], "entities": []}, {"text": "This is because when \u03bb is too big the model suffers from underfitting.", "labels": [], "entities": []}, {"text": "The model is less sensitive when \u03bb is small and in general, \u03bb = 2 \u221211 achieves consistently good performance.", "labels": [], "entities": []}, {"text": "When \u03bb is fixed, we observe that large \u03c1 (e.g., \u03c1 \u2248 2 \u22124 ) leads to better performance.", "labels": [], "entities": []}, {"text": "As \u03c1 represents the weight assigned to the unobserved term, this result confirms that the model benefits from using the zero terms in the co-occurrences matrix.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of the best SGNS, GloVe, PU-Learning models, trained on the text8 corpus. Results  show that our proposed model is better than SGNS and GloVe. Star indicates it is significantly better  than the second best algorithm in the same column according to Wilcoxon signed-rank test. (p < 0.05)", "labels": [], "entities": [{"text": "text8 corpus", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.8106924295425415}]}, {"text": " Table 3: The size of the test sets. The data sets in English are the original test sets. To evaluate other  languages, we translate the data sets from English.", "labels": [], "entities": []}, {"text": " Table 4: Performance of SGNS, GloVe, and the proposed PU-Learning model in four different languages.", "labels": [], "entities": []}]}