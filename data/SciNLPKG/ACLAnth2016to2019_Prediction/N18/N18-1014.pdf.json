{"title": [{"text": "A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation", "labels": [], "entities": [{"text": "Slot Alignment", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7523540258407593}]}], "abstractContent": [{"text": "Natural language generation lies at the core of generative dialogue systems and conversational agents.", "labels": [], "entities": [{"text": "Natural language generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6431769529978434}, {"text": "generative dialogue", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.9336420595645905}]}, {"text": "We describe an ensemble neural language generator, and present several novel methods for data representation and augmentation that yield improved results in our model.", "labels": [], "entities": [{"text": "data representation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7446996867656708}]}, {"text": "We test the model on three datasets in the restaurant, TV and laptop domains, and report both objective and subjective evaluations of our best model.", "labels": [], "entities": []}, {"text": "Using a range of automatic metrics, as well as human evaluators, we show that our approach achieves better results than state-of-the-art models on the same datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has recently been a substantial amount of research in natural language processing (NLP) in the context of personal assistants, such as Cortana or Alexa.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.8211576739947001}]}, {"text": "The capabilities of these conversational agents are still fairly limited and lacking in various aspects, one of the most challenging of which is the ability to produce utterances with humanlike coherence and naturalness for many different kinds of content.", "labels": [], "entities": []}, {"text": "This is the responsibility of the natural language generation (NLG) component.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.790623664855957}]}, {"text": "Our work focuses on language generators whose inputs are structured meaning representations (MRs).", "labels": [], "entities": [{"text": "structured meaning representations (MRs)", "start_pos": 57, "end_pos": 97, "type": "TASK", "confidence": 0.7516144812107086}]}, {"text": "An MR describes a single dialogue act with a list of key concepts which need to be conveyed to the human user during the dialogue.", "labels": [], "entities": []}, {"text": "Each piece of information is represented by a slotvalue pair, where the slot identifies the type of information and the value is the corresponding content.", "labels": [], "entities": []}, {"text": "Dialogue act (DA) types vary depending on the dialogue manager, ranging from simple ones, such as a goodbye DA with no slots at all, to complex ones, such as an inform DA containing multiple slots with various types of values (see example in).", "labels": [], "entities": []}, {"text": "Located near The Bakers, kid-friendly restaurant, The Golden Curry, offers Japanese cuisine with a moderate price range.", "labels": [], "entities": [{"text": "The Bakers", "start_pos": 13, "end_pos": 23, "type": "DATASET", "confidence": 0.9199854135513306}]}, {"text": "A natural language generator must produce a syntactically and semantically correct utterance from a given MR.", "labels": [], "entities": []}, {"text": "The utterance should express all the information contained in the MR, in a natural and conversational way.", "labels": [], "entities": [{"text": "MR", "start_pos": 66, "end_pos": 68, "type": "DATASET", "confidence": 0.6700774431228638}]}, {"text": "In traditional language generator architectures, the assembling of an utterance from an MR is performed in two stages: sentence planning, which enforces semantic correctness and determines the structure of the utterance, and surface realization, which enforces syntactic correctness and produces the final utterance form.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7048570364713669}]}, {"text": "Earlier work on statistical NLG approaches were typically hybrids of a handcrafted component and a statistical training method.", "labels": [], "entities": []}, {"text": "The handcrafted aspects, however, lead to decreased portability and potentially limit the variability of the outputs.", "labels": [], "entities": []}, {"text": "New corpusbased approaches emerged that used semantically aligned data to train language models that output utterances directly from their MRs ().", "labels": [], "entities": []}, {"text": "The alignment provides valuable information during training, but the semantic annotation is costly.", "labels": [], "entities": []}, {"text": "The most recent methods do not require aligned data and use an end-to-end approach to training, performing sentence planning and surface realization simultaneously (.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7331924140453339}, {"text": "surface realization", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7546498477458954}]}, {"text": "The most successful systems trained on unaligned data use recurrent neural networks (RNNs) paired with an encoder-decoder system design (, but also other concepts, such as imitation learning.", "labels": [], "entities": []}, {"text": "These NLG models, however, typically require greater amount of data for training due to the lack of semantic alignment, and they still have problems producing syntactically and semantically correct output, as well as being limited in naturalness.", "labels": [], "entities": []}, {"text": "Here we present a neural ensemble natural language generator, which we train and test on three large unaligned datasets in the restaurant, television, and laptop domains.", "labels": [], "entities": []}, {"text": "We explore novel ways to represent the MR inputs, including novel methods for delexicalizing slots and their values, automatic slot alignment, as well as the use of a semantic reranker.", "labels": [], "entities": [{"text": "slot alignment", "start_pos": 127, "end_pos": 141, "type": "TASK", "confidence": 0.6612338721752167}]}, {"text": "We use automatic evaluation metrics to show that these methods appreciably improve the performance of our model.", "labels": [], "entities": []}, {"text": "On the largest of the datasets, the E2E dataset () with nearly 50K samples, we also demonstrate that our model significantly outperforms the baseline E2E NLG Challenge 1 system inhuman evaluation.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.9753516316413879}, {"text": "E2E NLG Challenge 1", "start_pos": 150, "end_pos": 169, "type": "DATASET", "confidence": 0.7034014239907265}]}, {"text": "Finally, after augmenting our model with stylistic data selection, subjective evaluations reveal that it can still produce overall better results despite a significantly reduced training set.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the models on three datasets from different domains.", "labels": [], "entities": []}, {"text": "The primary one is the recently released E2E restaurant dataset () with 48K samples.", "labels": [], "entities": [{"text": "E2E restaurant dataset", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.8633050123850504}]}, {"text": "For benchmarking we use the TV dataset and the Laptop dataset) with 7K and 13K samples, respectively.", "labels": [], "entities": [{"text": "benchmarking", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9562326073646545}, {"text": "TV dataset", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.9675124883651733}, {"text": "Laptop dataset", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.8990974426269531}]}, {"text": "summarizes the proportions of the training, validation, and test sets for each dataset.", "labels": [], "entities": []}, {"text": "The E2E dataset is by far the largest one available for task-oriented language generation in the restaurant domain.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9072366952896118}, {"text": "task-oriented language generation", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.72140504916509}]}, {"text": "The human references were Note that the number of MRs in the E2E dataset was cutoff at 10K for the sake of visibility of the small differences between other column pairs.", "labels": [], "entities": [{"text": "MRs", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.6728675961494446}, {"text": "E2E dataset", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.9821900129318237}]}, {"text": "collected using pictures as the source of information, which was shown to inspire more informative and natural utterances ().", "labels": [], "entities": []}, {"text": "With nearly 50K samples, it offers almost 10 times more data than the San Francisco restaurant dataset introduced in, which has frequently been used for benchmarks.", "labels": [], "entities": []}, {"text": "The reference utterances in the E2E dataset exhibit superior lexical richness and syntactic variation, including more complex discourse phenomena.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.9526906311511993}]}, {"text": "It aims to provide higher-quality training data for end-to-end NLG systems to learn to produce more naturally sounding utterances.", "labels": [], "entities": []}, {"text": "The dataset was released as apart of the E2E NLG Challenge.", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9423990845680237}]}, {"text": "Although the E2E dataset contains a large number of samples, each MR is associated on average with 8.65 different reference utterances, effectively offering less than 5K unique MRs in the training set (.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.9645387828350067}]}, {"text": "Explicitly providing the model with multiple ground truths, it offers multiple alternative utterance structures the model can learn to apply for the same type of MR.", "labels": [], "entities": []}, {"text": "The delexicalization, as detailed later in Section 5.1, improves the ability of the model to share the concepts across different MRs.", "labels": [], "entities": []}, {"text": "The dataset contains only 8 different slot types, which are fairly equally distributed.", "labels": [], "entities": []}, {"text": "The number of slots in each MR ranges between 3 and 8, but the majority of MRs consist of 5 or 6 slots.", "labels": [], "entities": []}, {"text": "Even though most of the MRs contain many slots, the majority of the corresponding human utterances, however, consist of one or two sentences only (Table 3), suggesting a reasonably high level of sentence complexity in the references.", "labels": [], "entities": []}, {"text": "The reference utterances in the TV and the Laptop datasets were collected using Amazon Mechani-: Average number of sentences in the reference utterance fora given number of slots in the corresponding MR, along with the proportion of MRs with specific slot counts.", "labels": [], "entities": [{"text": "TV and the Laptop datasets", "start_pos": 32, "end_pos": 58, "type": "DATASET", "confidence": 0.8330315351486206}]}, {"text": "cal Turk (AMT), one utterance per MR.", "labels": [], "entities": [{"text": "Turk (AMT)", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.7950443923473358}]}, {"text": "These two datasets are similar in structure, both using the same 14 DA types.", "labels": [], "entities": []}, {"text": "The Laptop dataset, however, is almost twice as large and contains 25% more slot types.", "labels": [], "entities": [{"text": "Laptop dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9537353515625}]}, {"text": "Although both of these datasets contain more than a dozen different DA types, the vast majority (68% and 80% respectively) of the MRs describe a DA of either type inform or recommend (, which inmost cases have very similarly structured realizations, comparable to those in the E2E dataset.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 277, "end_pos": 288, "type": "DATASET", "confidence": 0.956751674413681}]}, {"text": "DAs such as suggest, ?request, or goodbye are represented by less than a dozen samples, but are significantly easier to learn to generate an utterance from because the corresponding MRs contain three slots at the most.", "labels": [], "entities": []}, {"text": "Researchers in NLG have generally used both automatic and human evaluation.", "labels": [], "entities": []}, {"text": "Our results report the standard automatic evaluation metrics: BLEU (), NIST (), METEOR, and ROUGE-L).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.999222993850708}, {"text": "NIST", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.5836665630340576}, {"text": "METEOR", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9882689714431763}, {"text": "ROUGE-L", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9966984391212463}]}, {"text": "For the E2E dataset experiments, we additionally report the results of the human evaluation carried out on the CrowdFlower platform as apart of the E2E NLG Challenge.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.9429817199707031}, {"text": "CrowdFlower platform", "start_pos": 111, "end_pos": 131, "type": "DATASET", "confidence": 0.9029195308685303}, {"text": "E2E NLG Challenge", "start_pos": 148, "end_pos": 165, "type": "DATASET", "confidence": 0.9296934207280477}]}, {"text": "We built our ensemble model using the seq2seq framework () for TensorFlow.", "labels": [], "entities": []}, {"text": "Our individual LSTM models use a bidirectional LSTM encoder with 512 cells per layer, and the CNN models use a pooling encoder as in.", "labels": [], "entities": []}, {"text": "The decoder in all models was a 4-layer RNN decoder with 512 LSTM cells per layer and with attention.", "labels": [], "entities": []}, {"text": "The hyperparameters were determined empirically.", "labels": [], "entities": []}, {"text": "After experimenting with different beam search parameters, we settled on the beam width of 10.", "labels": [], "entities": []}, {"text": "Moreover, we employed the length normalization of the beams as defined in, in order to encourage the decoder to favor longer sequences.", "labels": [], "entities": []}, {"text": "The length penalty providing the best results on the E2E dataset was 0.6, whereas for the TV and Laptop datasets it was 0.9 and 1.0, respectively.", "labels": [], "entities": [{"text": "length", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9950340986251831}, {"text": "E2E dataset", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.9844148755073547}, {"text": "TV and Laptop datasets", "start_pos": 90, "end_pos": 112, "type": "DATASET", "confidence": 0.71145299077034}]}, {"text": "We start by evaluating our system on the E2E dataset.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9839091897010803}]}, {"text": "Since the reference utterances in the test set were kept secret for the E2E NLG Challenge, we carried out the metric evaluation using the validation set.", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.9438024560610453}]}, {"text": "This was necessary to narrow down the models that perform well compared to the baseline.", "labels": [], "entities": []}, {"text": "The final model selection was done based on a human evaluation of the models' outputs on the test set.", "labels": [], "entities": []}, {"text": "In the first experiment, we assess what effect the augmenting of the training set via utterance splitting has on the performance of different models.", "labels": [], "entities": [{"text": "utterance splitting", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7985151410102844}]}, {"text": "The results in show that both the LSTM and the CNN models clearly benefit from additional pseudo-samples in the training set.", "labels": [], "entities": []}, {"text": "This can likely be attributed to the model having access to  more granular information about which parts of the utterance correspond to which slots in the MR.", "labels": [], "entities": []}, {"text": "This may assist the model in sentence planning and building a stronger association between parts of the utterance and certain slots, such as that \"it\" is a substitute for the name.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7236135900020599}]}, {"text": "Testing our ensembling approach reveals that reranking predictions pooled from different models produces an ensemble model that is overall more robust than the individual submodels.", "labels": [], "entities": []}, {"text": "The submodels fail to perform well in all four metrics at once, whereas the ensembling creates anew model that is more consistent across the different metric types.", "labels": [], "entities": []}, {"text": "While the ensemble model decreases the proportion of incorrectly realized slots compared to its individual submodels on the validation set, on the test set it only outperforms two of the submodels in this aspect (Table 8).", "labels": [], "entities": []}, {"text": "Analyzing the outputs, we also observed that the CNN model surpassed the two LSTM models in the ability to realize the \"fast food\" and \"pub\" values reliably, both of which were hardly present in the validation set but very frequent in the test set.", "labels": [], "entities": []}, {"text": "On the official E2E test set, our ensemble model performs comparably to the baseline model, TGen, in terms of automatic metrics).", "labels": [], "entities": [{"text": "E2E test set", "start_pos": 16, "end_pos": 28, "type": "DATASET", "confidence": 0.96867835521698}]}, {"text": "It is known that automatic metrics function only as a general and vague indication of the quality of an utterance in a dialogue ().", "labels": [], "entities": []}, {"text": "Systems which score similarly according to these metrics could produce utterances that are significantly different because automatic  Validation set Test set Ensem.", "labels": [], "entities": [{"text": "Ensem", "start_pos": 158, "end_pos": 163, "type": "METRIC", "confidence": 0.9467175602912903}]}, {"text": "0.087% 0.965% metrics fail to capture many of the characteristics of natural sounding utterances.", "labels": [], "entities": []}, {"text": "Therefore, to better assess the structural complexity of the predictions of our model, we present the results of a human evaluation of the models' outputs in terms of both naturalness and quality, carried out by the E2E NLG Challenge organizers.", "labels": [], "entities": [{"text": "E2E NLG Challenge organizers", "start_pos": 216, "end_pos": 244, "type": "DATASET", "confidence": 0.9513557851314545}]}, {"text": "Quality examines the grammatical correctness and adequacy of an utterance given an MR, whereas naturalness assesses whether a predicted utterance could have been produced by a native speaker, irrespective of the MR.", "labels": [], "entities": []}, {"text": "To obtain these scores, crowd workers ranked the outputs of 5 randomly selected systems from worst to best.", "labels": [], "entities": []}, {"text": "The final scores were produced using the TrueSkill algorithm () through pairwise comparisons of the human evaluation scores among the 20 competing systems.", "labels": [], "entities": []}, {"text": "Our system, trained on the E2E dataset without stylistic selection (Section 5.3), achieved the highest quality score in the E2E NLG Challenge, and was ranked second in naturalness.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.966207355260849}, {"text": "E2E NLG Challenge", "start_pos": 124, "end_pos": 141, "type": "DATASET", "confidence": 0.9142784476280212}]}, {"text": "The system's performance in quality (the primary metric) was significantly better than the competition according to the TrueSkill evaluation, which used bootstrap resampling with a p-level of p \u2264 0.05.", "labels": [], "entities": [{"text": "TrueSkill evaluation", "start_pos": 120, "end_pos": 140, "type": "DATASET", "confidence": 0.8956167697906494}]}, {"text": "Comparing these results with the scores achieved by the baseline model in quality and naturalness (5th and 6th: Automatic metric scores of our ensemble model compared against TGen (the baseline model), tested on the test set of the E2E dataset.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 232, "end_pos": 243, "type": "DATASET", "confidence": 0.9873004853725433}]}, {"text": "After filtering the E2E training set as described in Section 5.3, the new training set consisted of approximately 20K pairs of MRs and utterances.", "labels": [], "entities": [{"text": "E2E training set", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.8203941782315572}]}, {"text": "Interestingly, despite this drastic reduction in training samples, the model was able to learn more complex utterances that contained the natural variations of the human language.", "labels": [], "entities": []}, {"text": "The generated utterances exhibited discourse phenomena such as contrastive cues (see Example #1 in), as well as a more conversational style.", "labels": [], "entities": []}, {"text": "Nevertheless, the model also failed to realize slots more frequently.", "labels": [], "entities": []}, {"text": "In order to observe the effect of stylistic data selection, we conducted a human evaluation where we assessed the utterances based on error rate and naturalness.", "labels": [], "entities": []}, {"text": "The error rate is calculated as the percentage of slots the model failed to realize divided by the total number of slots present among all samples.", "labels": [], "entities": [{"text": "error rate", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9840477406978607}]}, {"text": "The annotators ranked samples of utterance triples -corresponding to three different ensemble models -by naturalness from 1 to 3 (3 being the most natural, with possible ties).", "labels": [], "entities": []}, {"text": "The conservative model combines three submodels all trained on the full training set, the progressive one combines submodels solely trained on the filtered dataset, and finally, the hybrid is an ensemble of three models only one of which is trained on the full training set, so as to serve as a fallback.", "labels": [], "entities": []}, {"text": "The impact of the reduction of the number of  training samples becomes evident by looking at the score of the progressive model, where this model trained solely on the reduced dataset had the highest error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 200, "end_pos": 210, "type": "METRIC", "confidence": 0.9679967164993286}]}, {"text": "We observe, however, that a hybrid ensemble model manages to perform the best in terms of the error rate, as well as the naturalness.", "labels": [], "entities": [{"text": "error rate", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9569086730480194}]}, {"text": "These results suggest that filtering the dataset through careful data selection can help to achieve better and more natural sounding utterances.", "labels": [], "entities": []}, {"text": "It significantly improves the model's ability to produce more elegant utterances beyond the \" is...", "labels": [], "entities": []}, {"text": "It is/has...\" format, which is only too common in neural language generators in this domain.", "labels": [], "entities": []}, {"text": "In order to provide a better frame of reference for the performance of our proposed model, we utilize the RNNLG benchmark toolkit 5 to evaluate our system on two additional, widely used datasets in NLG, and compare our results with those of a state-of-the-art model, SCLSTM).", "labels": [], "entities": [{"text": "RNNLG benchmark toolkit 5", "start_pos": 106, "end_pos": 131, "type": "DATASET", "confidence": 0.9000463485717773}]}, {"text": "As shows, our ensemble model performs competitively with the baseline on the TV dataset, and it outperforms it on the Laptop dataset by a wide margin.", "labels": [], "entities": [{"text": "TV dataset", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.9655585587024689}, {"text": "Laptop dataset", "start_pos": 118, "end_pos": 132, "type": "DATASET", "confidence": 0.9554758965969086}]}, {"text": "We believe the higher error rate of our model can be explained by the significantly less aggressive slot delexicalization than the one used in SCLSTM.", "labels": [], "entities": [{"text": "error rate", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9796754121780396}]}, {"text": "That, however, gives our model a greater lexical freedom and, with it, the ability to produce more natural utterances.", "labels": [], "entities": []}, {"text": "The model trained on the Laptop dataset is also a prime example of how an ensemble model is capable of extracting the best learned concepts from each individual submodel.", "labels": [], "entities": [{"text": "Laptop dataset", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9446752667427063}]}, {"text": "By combining their knowledge and compensating thus for each other's weaknesses, the ensemble model can achieve a lower error rate, as well as a better overall quality, than any of the submodels individually.", "labels": [], "entities": [{"text": "error rate", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.975467175245285}]}, {"text": "0.5226 1.67% 0.5238 1.55%: Automatic metric scores of our ensemble model evaluated on the test sets of the TV and Laptop datasets, and compared against SCLSTM.", "labels": [], "entities": [{"text": "TV and Laptop datasets", "start_pos": 107, "end_pos": 129, "type": "DATASET", "confidence": 0.7865120768547058}]}, {"text": "The ERR column indicates the slot error rate, as computed by the RNNLG toolkit (for our models calculated in postprocessing).", "labels": [], "entities": [{"text": "ERR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9447265863418579}, {"text": "slot error rate", "start_pos": 29, "end_pos": 44, "type": "METRIC", "confidence": 0.8824569980303446}, {"text": "RNNLG toolkit", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.951037734746933}]}], "tableCaptions": [{"text": " Table 2: Overview of the number of samples, as well  as different DA and slot types, in each dataset .", "labels": [], "entities": []}, {"text": " Table 3: Average number of sentences in the reference  utterance for a given number of slots in the correspond- ing MR, along with the proportion of MRs with specific  slot counts.", "labels": [], "entities": []}, {"text": " Table 6: Automatic metric scores of different mod- els tested on the E2E dataset, both unmodified (s) and  augmented (s) through the utterance splitting. The  symbols  \u2020 and  \u2021 indicate statistically significant im- provement over the s counterpart with p < 0.05 and  p < 0.01, respectively, based on the paired t-test.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.988897055387497}]}, {"text": " Table 7: Automatic metric scores of three different  models and their ensemble, tested on the validation set  of the E2E dataset. LSTM2 differs from LSTM1 in that  it was trained longer.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 118, "end_pos": 129, "type": "DATASET", "confidence": 0.9769430756568909}]}, {"text": " Table 9: Automatic metric scores of our ensemble  model compared against TGen (the baseline model),  tested on the test set of the E2E dataset.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.9850570559501648}]}, {"text": " Table 11: Average error rate and naturalness metrics  obtained from six annotators for different ensemble  models.", "labels": [], "entities": [{"text": "Average error rate", "start_pos": 11, "end_pos": 29, "type": "METRIC", "confidence": 0.8550326625506083}]}, {"text": " Table 12: Automatic metric scores of our ensemble  model evaluated on the test sets of the TV and Lap- top datasets, and compared against SCLSTM. The ERR  column indicates the slot error rate, as computed by  the RNNLG toolkit (for our models calculated in post- processing).", "labels": [], "entities": [{"text": "TV and Lap- top datasets", "start_pos": 92, "end_pos": 116, "type": "DATASET", "confidence": 0.8054071168104807}, {"text": "ERR", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9933372735977173}, {"text": "slot error rate", "start_pos": 177, "end_pos": 192, "type": "METRIC", "confidence": 0.9144498705863953}, {"text": "RNNLG toolkit", "start_pos": 214, "end_pos": 227, "type": "DATASET", "confidence": 0.9360900521278381}]}]}