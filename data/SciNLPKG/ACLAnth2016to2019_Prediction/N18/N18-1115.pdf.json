{"title": [{"text": "The Context-dependent Additive Recurrent Neural Net", "labels": [], "entities": []}], "abstractContent": [{"text": "Contextual sequence mapping is one of the fundamental problems in Natural Language Processing.", "labels": [], "entities": [{"text": "Contextual sequence mapping", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7889580130577087}]}, {"text": "Instead of relying solely on the information presented in a text, the learning agents have access to a strong external signal given to assist the learning process.", "labels": [], "entities": []}, {"text": "In this paper , we propose a novel family of Recurrent Neural Network unit: the Context-dependent Additive Recurrent Neural Network (CARNN) that is designed specifically to leverage this external signal.", "labels": [], "entities": []}, {"text": "The experimental results on public datasets in the dialog problem (Babi dialog Task 6 and Frame), contextual language model (Switchboard and Penn Discourse Tree Bank) and question answering (TrecQA) show that our novel CARNN-based architectures outper-form previous methods.", "labels": [], "entities": [{"text": "Babi dialog Task 6", "start_pos": 67, "end_pos": 85, "type": "DATASET", "confidence": 0.8761035203933716}, {"text": "Penn Discourse Tree Bank", "start_pos": 141, "end_pos": 165, "type": "DATASET", "confidence": 0.9287939965724945}, {"text": "question answering (TrecQA", "start_pos": 171, "end_pos": 197, "type": "TASK", "confidence": 0.6722576543688774}]}], "introductionContent": [{"text": "Sequence mapping is one of the most prominent class of problems in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Sequence mapping", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9474599957466125}, {"text": "Natural Language Processing (NLP)", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.7245513300100962}]}, {"text": "This is due to the fact that written language is sequential in nature.", "labels": [], "entities": []}, {"text": "In English, a word is a sequence of characters, a sentence is a sequence of words, a paragraph is a sequence of sentences, and soon.", "labels": [], "entities": []}, {"text": "However, understanding apiece of text may require far more than just extracting the information from that piece itself.", "labels": [], "entities": []}, {"text": "If the piece of text is a paragraph of a document, the reader may have to consider it together with other paragraphs in the document and the topic of the document.", "labels": [], "entities": []}, {"text": "To understand an utterance in a conversation, the utterance has to be put into the context of the conversation, which includes the goals of the participants and the dialog history.", "labels": [], "entities": []}, {"text": "Hence the notion of context is an intrinsic component of language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.7093480229377747}]}, {"text": "Inspired by recent works in dialog systems (, we formalize the contextual sequence mapping problem as a sequence mapping problem with a strong controlling contextual element that regulates the flow of information.", "labels": [], "entities": []}, {"text": "The system has two sources of signals: (i) the main text input, for example, the history utterance sequence in dialog systems or the sequence of words in language modelling; and (ii) the context signal, e.g., the previous utterance in a dialog system, the discourse information in contextual language modelling or the question in question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 330, "end_pos": 348, "type": "TASK", "confidence": 0.6709538996219635}]}, {"text": "Our contribution in this work is two-fold.", "labels": [], "entities": []}, {"text": "First, we propose anew family of recurrent unit, the Context-dependent Additive Recurrent Neural Network (CARNN), specifically constructed for contextual sequence mapping.", "labels": [], "entities": [{"text": "contextual sequence mapping", "start_pos": 143, "end_pos": 170, "type": "TASK", "confidence": 0.6026340226332346}]}, {"text": "Second, we design novel neural network architectures based on CARNN for dialog systems and contextual language modelling, and enhance the state of the art architecture () on question answering.", "labels": [], "entities": [{"text": "contextual language modelling", "start_pos": 91, "end_pos": 120, "type": "TASK", "confidence": 0.6563722292582194}, {"text": "question answering", "start_pos": 174, "end_pos": 192, "type": "TASK", "confidence": 0.8999013006687164}]}, {"text": "Our novel building block, the CARNN, draws inspiration from the Recurrent Additive Network (, which showed that most of the non-linearity in the successful Long Short Term Memory (LSTM) network) is not necessary.", "labels": [], "entities": [{"text": "CARNN", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.7756327390670776}]}, {"text": "In the same spirit, our CARNN unit minimizes the use of non-linearity in the model to facilitate the ease of gradient flow.", "labels": [], "entities": []}, {"text": "We also seek to keep the number of parameters to a minimum to improve trainability.", "labels": [], "entities": []}, {"text": "We experiment with our models on abroad range of problems: dialog systems, contextual language modelling and question answering.", "labels": [], "entities": [{"text": "contextual language modelling", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.6273272136847178}, {"text": "question answering", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.8801420331001282}]}, {"text": "Our systems outperform previous methods on several public datasets, which include the Babi Task 6 ( and the Frame dataset () for dialog, the Switchboard () and Penn Discourse Tree Bank () for contextual language modelling, and the TrecQA dataset () for question answering.", "labels": [], "entities": [{"text": "Babi Task 6", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.9215145111083984}, {"text": "Frame dataset", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.87225741147995}, {"text": "Penn Discourse Tree Bank", "start_pos": 160, "end_pos": 184, "type": "DATASET", "confidence": 0.9293081313371658}, {"text": "contextual language modelling", "start_pos": 192, "end_pos": 221, "type": "TASK", "confidence": 0.65972371896108}, {"text": "TrecQA dataset", "start_pos": 231, "end_pos": 245, "type": "DATASET", "confidence": 0.8594890832901001}, {"text": "question answering", "start_pos": 253, "end_pos": 271, "type": "TASK", "confidence": 0.8932557106018066}]}, {"text": "We propose a different architecture for each task, but all models share the basic building block, the CARNN.", "labels": [], "entities": [{"text": "CARNN", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.8214658498764038}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Perplexity on Switchboard and Penn Dis- course Tree Bank.", "labels": [], "entities": [{"text": "Penn Dis- course Tree Bank", "start_pos": 40, "end_pos": 66, "type": "DATASET", "confidence": 0.7889818946520487}]}, {"text": " Table 3: MAP and MRR for question answering. *  indicates statistical significance with \u03b1 < 0.05 in t-test  compared to IWAN (our implementation).", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9700919389724731}, {"text": "MRR", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9790210723876953}, {"text": "question answering", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8729334473609924}, {"text": "IWAN", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.6230182647705078}]}]}