{"title": [{"text": "Learning Domain Representation for Multi-Domain Sentiment Classification", "labels": [], "entities": [{"text": "Learning Domain Representation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5516286194324493}, {"text": "Multi-Domain Sentiment Classification", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.6680343250433604}]}], "abstractContent": [{"text": "Training data for sentiment analysis are abundant in multiple domains, yet scarce for other domains.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.9748519062995911}]}, {"text": "It is useful to leveraging data available for all existing domains to enhance performance on different domains.", "labels": [], "entities": []}, {"text": "We investigate this problem by learning domain-specific representations of input sentences using neural network.", "labels": [], "entities": []}, {"text": "In particular, a descriptor vector is learned for representing each domain, which is used to map adversarially trained domain-general Bi-LSTM input representations into domain-specific representations.", "labels": [], "entities": []}, {"text": "Based on this model, we further expand the input representation with exemplary domain knowledge, collected by attending over a memory network of domain training data.", "labels": [], "entities": []}, {"text": "Results show that our model outperforms existing methods on multi-domain sentiment analysis significantly, giving the best accuracies on two different benchmarks .", "labels": [], "entities": [{"text": "multi-domain sentiment analysis", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.7451671163241068}, {"text": "accuracies", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.9712192416191101}]}], "introductionContent": [{"text": "Sentiment analysis has received constant research attention due to its importance to business).", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9571253955364227}]}, {"text": "For multiple domains, such as movies, restaurants and digital products, manually annotated datasets have been made available.", "labels": [], "entities": []}, {"text": "A useful research question is how to leverage resources available across all domains to improve sentiment classification on a certain domain.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.9646952748298645}]}, {"text": "One naive domain-agnostic baseline is to combine all training data, ignoring domain differences.", "labels": [], "entities": []}, {"text": "However, domain knowledge is one valuable source of information available.", "labels": [], "entities": []}, {"text": "To utilize this, there has been recent work on domain-aware models via multi-task learning (, building an output layer for each domain while sharing a representation network.", "labels": [], "entities": []}, {"text": "Given an input sentence and a specific test domain, the output layer of the test domain is chosen for calculating the output.", "labels": [], "entities": []}, {"text": "These methods have been shown to improve over the naive domain-agnostic baseline.", "labels": [], "entities": []}, {"text": "However, a limitation is that outputs for different domains are constructed using the same domainagnostic input representation, which leads to weak utilization of domain knowledge.", "labels": [], "entities": []}, {"text": "For different domains, sentiment words can differ.", "labels": [], "entities": []}, {"text": "For example, the word \"beast\" can be a positive indicator of camera quality, but irrelevant to restaurants or movies.", "labels": [], "entities": []}, {"text": "Also, \"easy\" is frequently used in the electronics domain to express positive sentiment (e.g. the camera is easy to use), while expressing negative sentiment in the movie domain (e.g. the ending of this movie is easy to guess).", "labels": [], "entities": []}, {"text": "We address this issue by investigating a model that learns domain-specific input representations for multi-domain sentiment analysis.", "labels": [], "entities": [{"text": "multi-domain sentiment analysis", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.7362880706787109}]}, {"text": "In particular, given an input sentence, our model first uses a bidirectional LSTM to learn a general sentence-level representation.", "labels": [], "entities": []}, {"text": "For better utilizing data from all domains, we use adversarial training () on the Bi-LSTM representation.", "labels": [], "entities": []}, {"text": "The general sentence representation is then mapped into a domain-specific representation by attention over the input sentence using explicitly learned domain descriptors, so that the most salient parts of the input are selected for the specific domain for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 256, "end_pos": 280, "type": "TASK", "confidence": 0.9478830099105835}]}, {"text": "Some examples are shown in, where our model pays attention to word \"engaging\" for movie reviews, but not for laptops, restaurants or cameras.", "labels": [], "entities": []}, {"text": "Similarly, the word \"beast\" receives attention for laptops and cameras, but not for restaurants or movies.", "labels": [], "entities": []}, {"text": "In addition to the domain descriptors, we further introduce a memory network for explicitly representing domain knowledge.", "labels": [], "entities": []}, {"text": "Here domain knowl-  edge refers to example training data in a specific domain, which can offer useful background context.", "labels": [], "entities": []}, {"text": "For example, given a sentence 'Keep cool if you think it's a wonderful life will be a heartwarming tale about lifelike finding nemo', algorithms can mistakenly classify it as positive based on 'wonderful' and 'heartwarming', ignoring the fact that 'it's a wonderful life' is a movie.", "labels": [], "entities": []}, {"text": "In this case, necessary domain knowledge revealed in other sentences, such as 'The last few minutes of the movie: it's a wonderful life don't cancel out all the misery the movie contained' is helpful.", "labels": [], "entities": []}, {"text": "Given a domain-specific input representation, we make attention over the domain knowledge memory network to obtain a background context vector, which is used in conjunction with the input representation for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 207, "end_pos": 231, "type": "TASK", "confidence": 0.9553587436676025}]}, {"text": "Results on two real-world datasets show that our model outperforms the aforementioned multi-task learning methods for domain-aware training, and also generalizes to unseen domains.", "labels": [], "entities": []}, {"text": "Our code is released 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the effectiveness of the model both indomain and cross-domain.", "labels": [], "entities": []}, {"text": "The former refers to the setting where the domain of the test data falls into one of them training data domains, and the latter refers to the setting where the test data comes from one unknown domain.", "labels": [], "entities": []}, {"text": "We conduct experiments on two benchmark datasets.", "labels": [], "entities": []}, {"text": "The datasets are balanced, so we use accuracy as the evaluation metric in the experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9992256164550781}]}, {"text": "The dataset 1 contains four domains.", "labels": [], "entities": []}, {"text": "The statistics are shown in , which also shows the accuracies using baseline method Mix trained and tested on each domain.", "labels": [], "entities": []}, {"text": "Camera 2 consists of reviews with respect to digital products such as cameras and MP3 players ().", "labels": [], "entities": []}, {"text": "Laptop and Restaurant are laptop and restaurant reviews, respectively, obtained from SemEval 2015 Task 12 . Movie 4 are movie reviews provided byproduct reviews taken from Amazon.com, including 25 product types (domains) such as books, beauty and music.", "labels": [], "entities": [{"text": "SemEval 2015 Task 12", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.5482435747981071}]}, {"text": "More statistics can be found at its official website . Given each dataset, we randomly select 80%, 10% and 10% of the instances as training, development and testing sets, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset 1 statistics.", "labels": [], "entities": []}, {"text": " Table 2: Results using two training domains on dataset  1. * denotes p < 0.01 VS. the second best using Mc- Nemar's test.", "labels": [], "entities": [{"text": "VS", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9750669002532959}]}, {"text": " Table 3: In-domain learning and cross-domain results on dataset 1. * denotes p < 0.01 VS. the second best.", "labels": [], "entities": [{"text": "VS", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.960921049118042}]}, {"text": " Table 4: In-domain learning and cross-domain results on dataset 2. * denotes p < 0.01 VS. the second best.", "labels": [], "entities": [{"text": "VS", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9587198495864868}]}]}