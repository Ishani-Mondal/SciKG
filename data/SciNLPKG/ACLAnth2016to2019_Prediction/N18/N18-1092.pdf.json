{"title": [{"text": "Deep Generative Model for Joint Alignment and Word Representation", "labels": [], "entities": [{"text": "Joint Alignment", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7720925509929657}, {"text": "Word Representation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7238414138555527}]}], "abstractContent": [{"text": "This work exploits translation data as a source of semantically relevant learning signal for models of word representation.", "labels": [], "entities": [{"text": "word representation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7209452688694}]}, {"text": "In particular, we exploit equivalence through translation as a form of distributional context and jointly learn how to embed and align with a deep gener-ative model.", "labels": [], "entities": []}, {"text": "Our EMBEDALIGN model embeds words in their complete observed context and learns by marginalisation of latent lexical alignments.", "labels": [], "entities": []}, {"text": "Besides, it embeds words as posterior probability densities, rather than point estimates, which allows us to compare words in context using a measure of overlap between distributions (e.g. KL divergence).", "labels": [], "entities": []}, {"text": "We investigate our model's performance on a range of lexical semantics tasks achieving competitive results on several standard benchmarks including natural language inference, paraphrasing, and text similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing applications often count on the availability of word representations trained on large textual data as a means to alleviate problems such as data sparsity and lack of linguistic resources.", "labels": [], "entities": [{"text": "Natural language processing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6284523705641428}]}, {"text": "Traditional approaches to inducing word representations circumvent the need for explicit semantic annotation by capitalising on some form of indirect semantic supervision.", "labels": [], "entities": []}, {"text": "A typical example is to fit a binary classifier to detect whether or not a target word is likely to co-occur with neighbouring words (.", "labels": [], "entities": []}, {"text": "If the binary classifier represents a word as a continuous vector, that vector will be trained to be discriminative of the contexts it co-occurs with, and thus words in similar contexts will have similar representations.", "labels": [], "entities": []}, {"text": "Code available from https://github.com/ uva-slpl/embedalign MR and WA contributed equally.", "labels": [], "entities": [{"text": "WA", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.8707214593887329}]}, {"text": "The underlying assumption is that context (e.g. neighbouring words) stands for the meaning of the target word.", "labels": [], "entities": []}, {"text": "The success of this distributional hypothesis hinges on the definition of context and different models are based on different definitions.", "labels": [], "entities": []}, {"text": "Importantly, the nature of the context determines the range of linguistic properties the representations may capture ().", "labels": [], "entities": []}, {"text": "For example, propose to use syntactic context derived from dependency parses.", "labels": [], "entities": [{"text": "dependency parses", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7926184833049774}]}, {"text": "They show that their representations are much more discriminative of syntactic function than models based on immediate neighbourhood (.", "labels": [], "entities": []}, {"text": "In this work, we take lexical translation as indirect semantic supervision ().", "labels": [], "entities": [{"text": "lexical translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7024288028478622}]}, {"text": "Effectively we make two assumptions.", "labels": [], "entities": []}, {"text": "First, that every word has a foreign equivalent that stands for its meaning.", "labels": [], "entities": []}, {"text": "Second, that we can find this equivalent in translation data through lexical alignments.", "labels": [], "entities": []}, {"text": "For that we induce both a latent mapping between words in a bilingual sentence pair and distributions over latent word representations.", "labels": [], "entities": []}, {"text": "To summarise our contributions: \u2022 we model a joint distribution over sentence pairs that generates data from latent word representations and latent lexical alignments; \u2022 we embed words in context mining positive correlations from translation data; \u2022 we find that foreign observations are necessary for generative training, but test time predictions can be made monolingually; \u2022 we apply our model to a range of semantic natural language processing tasks showing its usefulness. is generated conditioned on a sequence of random embeddings z m 1 ; generating the foreign sequence y n 1 further requires latent lexical alignments an 1 .", "labels": [], "entities": [{"text": "generative training", "start_pos": 302, "end_pos": 321, "type": "TASK", "confidence": 0.9356758296489716}]}], "datasetContent": [{"text": "We start the section describing the data used to estimate our model's parameters as well as details about the optimiser.", "labels": [], "entities": []}, {"text": "The remainder of the section presents results on various benchmarks.", "labels": [], "entities": []}, {"text": "Training data We train our model on bilingual parallel data.", "labels": [], "entities": []}, {"text": "In particular, we use parliament proceedings (Europarl-v7) () from two language pairs: English-French and EnglishGerman.", "labels": [], "entities": [{"text": "Europarl-v7", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9501864314079285}]}, {"text": "We employed very minimal preprocessing, namely, tokenisation and lowercasing using scripts from MOSES (, and have discarded sentences longer than 50 tokens.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.934050977230072}]}, {"text": "Optimiser For all architectures, we use the Adam optimiser () with a learning rate of 10 \u22123 . Except where explicitly indicated, we \u2022 train our models for 30 epochs using mini batches of 100 sentence pairs; \u2022 use validation alignment error rate for model selection; \u2022 train every model 10 times with random Glorot initialisation ( and report mean and standard deviation; \u2022 anneal the KL terms using the following schedule: we use a scalar \u03b1 from 0 to 1 with additive steps of size 10 \u22123 every 500 updates.", "labels": [], "entities": []}, {"text": "This means that at the beginning of the training, we allow the model to overfit to the likelihood terms, but towards the end we are optimising the true ELBO (.", "labels": [], "entities": [{"text": "ELBO", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9808427691459656}]}, {"text": "It is also important to highlight that we do not employ regularisation techniques (such as batch normalisation, dropout, or L 2 penalty) for they did not seem to yield consistent results.", "labels": [], "entities": [{"text": "batch normalisation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.6244719475507736}, {"text": "L 2 penalty", "start_pos": 124, "end_pos": 135, "type": "METRIC", "confidence": 0.8842419187227885}]}, {"text": "The work of Ko\u010disk\u00b4 is closer to ours in that they also learn embeddings by marginalising alignments, however, their model is conditional-much like IBM models-and their embeddings are not part of the probabilistic model, but rather part of the architecture design.", "labels": [], "entities": []}, {"text": "The joint formulation allows our latent embeddings to harvest learning signal from L2 while still being driven by the learning signal from L1-in a conditional model the representations can become specific to alignment deviating from the purpose of well representing the original language.", "labels": [], "entities": []}, {"text": "In \u00a73 we show substantial evidence that our model performs better when using both learning signals.", "labels": [], "entities": []}, {"text": "first propose to map words into Gaussian densities instead of point estimates for better word representation.", "labels": [], "entities": [{"text": "word representation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7409296929836273}]}, {"text": "For example, a distribution can capture asymmetric relations that a point estimate cannot.", "labels": [], "entities": []}, {"text": "recast the skip-gram model as a conditional variational auto-encoder.", "labels": [], "entities": []}, {"text": "They induce a Gaussian density for each occurrence of a word in context, and for that their model is the closest to ours.", "labels": [], "entities": []}, {"text": "Additionally, they estimate a Gaussian prior per word type thus representing both types and occurrences.", "labels": [], "entities": []}, {"text": "Unlike our model, the Bayesian skip-gram is not trained generatively by reconstructing the data, but rather discriminatively by prediction of overlapping sets of neighbouring words.", "labels": [], "entities": []}, {"text": "(2017) developed a framework to evaluate unsupervised sentence level representations trained on large amounts of data on a range of supervised NLP tasks.", "labels": [], "entities": []}, {"text": "We assess our induced representations using their framework on the following benchmarks evaluated on classification \u2191accuracy (MRPC is further evaluated on \u2191F1) MR classification of positive or negative movie reviews; SST fined-grained labelling of movie reviews from the Stanford sentiment treebank (SST); TREC classification of questions into k-classes; CR classification of positive or negative product reviews; SUBJ classification of a sentence into subjective or objective; MPQA classification of opinion polarity; SICK-E textual entailment classification; MRPC paraphrase identification in the Microsoft paraphrase corpus; as well as the following benchmarks evaluated on the indicated correlation metric(s) SICK-R semantic relatedness between two sentences (\u2191Pearson); SST-14 semantic textual similarity (\u2191Pearson/Spearman).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.996705949306488}, {"text": "SST fined-grained labelling of movie reviews", "start_pos": 218, "end_pos": 262, "type": "TASK", "confidence": 0.8326600392659506}, {"text": "TREC classification of questions", "start_pos": 307, "end_pos": 339, "type": "TASK", "confidence": 0.8060838580131531}, {"text": "SUBJ classification of a sentence", "start_pos": 415, "end_pos": 448, "type": "TASK", "confidence": 0.8957761287689209}, {"text": "SICK-E textual entailment classification", "start_pos": 520, "end_pos": 560, "type": "TASK", "confidence": 0.6406782194972038}, {"text": "MRPC paraphrase identification", "start_pos": 562, "end_pos": 592, "type": "TASK", "confidence": 0.6406746308008829}, {"text": "Microsoft paraphrase corpus", "start_pos": 600, "end_pos": 627, "type": "DATASET", "confidence": 0.7143706281979879}]}, {"text": "Prediction We use EMBEDALIGN to annotate every word in the training set of the benchmarks above with the posterior mean embedding in context.", "labels": [], "entities": [{"text": "EMBEDALIGN", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9163775444030762}]}, {"text": "We then average embeddings in a sentence and give that as features to a logistic regression classifier trained with 5-fold cross validation.", "labels": [], "entities": []}, {"text": "For comparison, we report a SKIPGRAM model (here indicated as as well as a model that uses the encoder of a neural machine translation system (NMT) trained on English-French Europarl data.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 174, "end_pos": 187, "type": "DATASET", "confidence": 0.9363221526145935}]}, {"text": "In both cases, we report results by. shows the results for all benchmarks.", "labels": [], "entities": []}, {"text": "We report EMBEDALIGN trained on either EN-FR or EN-DE.", "labels": [], "entities": [{"text": "EMBEDALIGN", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9188351035118103}, {"text": "EN-FR", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9457358717918396}, {"text": "EN-DE", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.7787678241729736}]}, {"text": "The last line (COMBO) shows what happens if we train logistic regression on the concatenation of embeddings inferred by both EM-BEDALIGN models, that is, EN-FR and EN-DE.", "labels": [], "entities": [{"text": "COMBO", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9659150242805481}]}, {"text": "Note that these two systems perform sometimes better sometimes worse depending on the benchmark.", "labels": [], "entities": []}, {"text": "There is no clear pattern, but differences may well come from some qualitative difference in the induced latent space.", "labels": [], "entities": []}, {"text": "It is a known fact that different languages realise lexical ambiguities differently, thus representations induced towards different languages are likely to capture different generalisations.", "labels": [], "entities": []}, {"text": "As COMBO results show, the representations induced from different corpora are somewhat complementary.", "labels": [], "entities": [{"text": "COMBO", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.7348819375038147}]}, {"text": "That same observation has guided paraphrasing models based on pivoting).", "labels": [], "entities": []}, {"text": "Once more we report a monolingual variant of EMBEDALIGN (indicated by EN) in an attempt to illustrate how crucial the 6 http://scikit-learn.org/stable/ 7 In Appendix A we provide bar plots marked with error bars (2 standard deviations).", "labels": [], "entities": [{"text": "EMBEDALIGN", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.8695043921470642}]}, {"text": "We also acknowledge that our treatment of German is likely suboptimal due to the lack of subword features, as it can also be seen in AER results.", "labels": [], "entities": [{"text": "AER", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9463609457015991}]}], "tableCaptions": [{"text": " Table 1  lists more information about the training data, in- cluding the English-French Giga web corpus (Bojar  et al., 2014) which we use in  \u00a73.4. 5", "labels": [], "entities": [{"text": "Giga web corpus", "start_pos": 89, "end_pos": 104, "type": "DATASET", "confidence": 0.8896488746007284}]}, {"text": " Table 2: English-French validation \u2191accuracy and \u2193  AER results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9983843564987183}, {"text": "AER", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9956978559494019}]}, {"text": " Table 3: English-German validation \u2191accuracy and  \u2193AER results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.995897650718689}, {"text": "AER", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9895283579826355}]}, {"text": " Table 5: English-French \u2191accuracy in test set", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9533356428146362}]}, {"text": " Table 6: English-German \u2191accuracy in test set", "labels": [], "entities": [{"text": "\u2191", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.8969883322715759}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9424757361412048}]}, {"text": " Table 7: English \u2191GAP on LST test data.", "labels": [], "entities": [{"text": "GAP", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9152703285217285}, {"text": "LST test data", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.7844316363334656}]}, {"text": " Table 8: English sentence evaluation results: the last four rows correspond to the mean of 10 runs with EMBE- DALIGN models. All models, but W2VEC, employ bidirectional encoders.", "labels": [], "entities": [{"text": "English sentence evaluation", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6014047563076019}]}, {"text": " Table 9: Evaluation of English word embeddings out of  context in terms of Spearman's rank correlation coeffi- cient (\u2191). The first column is from (Faruqui and Dyer,  2014a).", "labels": [], "entities": [{"text": "Spearman's rank correlation coeffi- cient", "start_pos": 76, "end_pos": 117, "type": "METRIC", "confidence": 0.678977234022958}]}]}