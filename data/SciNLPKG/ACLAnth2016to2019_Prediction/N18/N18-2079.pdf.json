{"title": [{"text": "Incremental Decoding and Training Methods for Simultaneous Translation in Neural Machine Translation", "labels": [], "entities": [{"text": "Incremental Decoding", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8624951839447021}, {"text": "Simultaneous Translation", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.9665609002113342}, {"text": "Neural Machine Translation", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.6377076009909312}]}], "abstractContent": [{"text": "We address the problem of simultaneous translation by modifying the Neural MT decoder to operate with dynamically built encoder and attention.", "labels": [], "entities": [{"text": "simultaneous translation", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.6406164765357971}, {"text": "Neural MT decoder", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.7553666234016418}]}, {"text": "We propose a tunable agent which decides the best segmentation strategy fora user-defined BLEU loss and Average Proportion (AP) constraint.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.9627077579498291}, {"text": "BLEU loss and Average Proportion (AP)", "start_pos": 90, "end_pos": 127, "type": "METRIC", "confidence": 0.9273206293582916}]}, {"text": "Our agent outperforms previously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova, 2016) on BLEU with a lower latency.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.5651639699935913}]}, {"text": "Secondly we proposed data-driven changes to Neural MT training to better match the incremental decoding framework.", "labels": [], "entities": [{"text": "Neural MT", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.7512707710266113}]}], "introductionContent": [{"text": "Simultaneous translation is a desirable attribute in Spoken Language Translation, where the translator is required to keep up with the speaker.", "labels": [], "entities": [{"text": "Simultaneous translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7320242524147034}, {"text": "Spoken Language Translation", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.8828382889429728}]}, {"text": "Ina lecture or meeting translation scenario where utterances are long, or the end of sentence is not clearly marked, the system must operate on a buffered sequence.", "labels": [], "entities": [{"text": "meeting translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7937631011009216}]}, {"text": "Generating translations for such incomplete sequences presents a considerable challenge for machine translation, more so in the case of syntactically divergent language pairs (such as German-English), where the context required to correctly translate a sentence, appears much later in the sequence, and prematurely committing to a translation leads to significant loss in quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7908902764320374}]}, {"text": "Various strategies to select appropriate segmentation points in a streaming input have been proposed.", "labels": [], "entities": []}, {"text": "A downside of this approach is that the MT system translates sequences independent of each other, ignoring the context.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9650774002075195}]}, {"text": "Even if the segmenter decides perfect points to segment the input stream, an MT system requires lexical history to make the correct decision.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9811120629310608}]}, {"text": "The end-to-end nature of the Neural MT architecture () provides a natural mechanism 1 to integrate stream decoding.", "labels": [], "entities": [{"text": "Neural MT architecture", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.5419207513332367}]}, {"text": "Specifically, the recurrent property of the encoder and decoder components provide an easy way to maintain historic context in a fixed size vector.", "labels": [], "entities": []}, {"text": "We modify the neural MT architecture to operate in an online fashion where i) the encoder and the attention are updated dynamically as new input words are added, through a READ operation, and ii) the decoder generates output from the available encoder states, through a WRITE operation.", "labels": [], "entities": [{"text": "READ", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9605051279067993}, {"text": "WRITE", "start_pos": 270, "end_pos": 275, "type": "METRIC", "confidence": 0.7528514266014099}]}, {"text": "The decision of when to WRITE is learned through a tunable segmentation agent, based on user-defined thresholds.", "labels": [], "entities": [{"text": "WRITE", "start_pos": 24, "end_pos": 29, "type": "TASK", "confidence": 0.5433675646781921}]}, {"text": "Our incremental decoder significantly outperforms the chunk-based decoder and restores the oracle performance with a deficit of \uf8ff 2 BLEU points across 4 language pairs with a moderate delay.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9987792372703552}]}, {"text": "We additionally explore whether modifying the Neural MT training to match the decoder can improve performance.", "labels": [], "entities": [{"text": "Neural MT training", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.6227356890837351}]}, {"text": "While we observed significant restoration in the case of chunk decoding matched with chunk-based NMT training, the same was not found true with our proposed incremental training to match the incremental decoding framework.", "labels": [], "entities": []}, {"text": "The remaining paper is organized as follow: Section 2 describes modifications to the NMT decoder to enable stream decoding.", "labels": [], "entities": [{"text": "NMT decoder", "start_pos": 85, "end_pos": 96, "type": "DATASET", "confidence": 0.9005531668663025}]}, {"text": "Section 3 describes various agents to learn a READ/WRITE strategy.", "labels": [], "entities": [{"text": "READ/WRITE", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.7184183200200399}]}, {"text": "Section 4 presents evaluation and results.", "labels": [], "entities": []}, {"text": "Section 5 describes modifications to the NMT training to mimic corresponding decoding strategy, and Section 6 concludes the paper.", "labels": [], "entities": [{"text": "NMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.8823115825653076}]}, {"text": "Iteration 4 n w = 0 n w = 2 n w = 0 n w = 4 Figure 1: A decoding pass over a 4-word source sentence.", "labels": [], "entities": []}, {"text": "n w denotes the number of words the agent chose to commit.", "labels": [], "entities": []}, {"text": "Green nodes = committed words, Blue nodes = newly generated words in the current iteration.", "labels": [], "entities": []}, {"text": "Words marked in red are discarded, as the agent chooses to not commit them.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data: We trained systems for 4 language pairs: German-, Arabic-, Czech-and Spanish-English pairs using the data made available for IWSLT (.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.9259369373321533}]}, {"text": "See supplementary material for data stats.", "labels": [], "entities": []}, {"text": "These language pairs present a diverse set of challenges for this problem, with Arabic and Czech being morphologically rich, German being syntactically divergent, and Spanish introducing local reorderings with respect to English.", "labels": [], "entities": []}, {"text": "NMT System: We trained a 2-layered LSTM encoder-decoder models with attention using the seq2seq-attn implementation.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8825862407684326}]}, {"text": "Please see supplementary material for settings.", "labels": [], "entities": []}, {"text": "Results: shows the results of various streaming agents.", "labels": [], "entities": []}, {"text": "Our proposed STATIC-RW agent outperforms other methods while maintaining an AP < 0.75 with a loss of less than 0.5 BLEU points on Arabic, Czech and Spanish.", "labels": [], "entities": [{"text": "AP", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9979636669158936}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9990690350532532}]}, {"text": "This was found to be consistent for all test-sets 2011-2014 (See under \"small\" models in).", "labels": [], "entities": []}, {"text": "In the case of German the loss at AP < 0.75 was around 1.5 BLEU points.", "labels": [], "entities": [{"text": "German", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.9393557906150818}, {"text": "AP < 0.75", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.967907985051473}, {"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9993008375167847}]}, {"text": "The syntactical divergence and rich morphology of German posits a Scalability: The preliminary results were obtained using models trained on the TED corpus only.", "labels": [], "entities": [{"text": "Scalability", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.922120213508606}, {"text": "TED corpus", "start_pos": 145, "end_pos": 155, "type": "DATASET", "confidence": 0.9084004163742065}]}, {"text": "We conducted further experiments by training models on larger data-sets (See the supplementary section again for data sizes) to see if our findings are scalable.", "labels": [], "entities": []}, {"text": "We fine-tuned () our models with the in-domain data to avoid domain disparity.", "labels": [], "entities": []}, {"text": "We then re-ran our agents with the best S,RW values (with an AP under 0.75) for each language pair.", "labels": [], "entities": [{"text": "AP", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9913522005081177}]}, {"text": "(\"large\" models) shows that the BLEU loss from the respective oracle increased when the models were trained with bigger data sizes.", "labels": [], "entities": [{"text": "BLEU loss", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9836959540843964}]}, {"text": "This could be attributed to the increased lexical ambiguity from the large amount of out-domain data, which can only be resolved with additional contextual information.", "labels": [], "entities": []}, {"text": "However our results were still better than the WIW agent, which also has an AP value above 0.8.", "labels": [], "entities": [{"text": "WIW agent", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.875443160533905}, {"text": "AP", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9976840019226074}]}, {"text": "Allowing similar AP, our STATIC-RW agents were able to restore the BLEU loss to be \uf8ff 1.5 for all language pairs except German-English.", "labels": [], "entities": [{"text": "AP", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9993250370025635}, {"text": "STATIC-RW", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.8409606218338013}, {"text": "BLEU loss", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9803776741027832}]}, {"text": "Detailed test results are available in the suplementary material.", "labels": [], "entities": []}], "tableCaptions": []}