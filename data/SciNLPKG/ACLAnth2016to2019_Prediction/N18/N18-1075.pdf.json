{"title": [{"text": "Global Relation Embedding for Relation Extraction", "labels": [], "entities": [{"text": "Global Relation Embedding", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8262837926546732}, {"text": "Relation Extraction", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8102489709854126}]}], "abstractContent": [{"text": "We study the problem of textual relation embedding with distant supervision.", "labels": [], "entities": [{"text": "textual relation embedding", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.7097075581550598}]}, {"text": "To combat the wrong labeling problem of distant supervision , we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus.", "labels": [], "entities": []}, {"text": "This approach turns out to be more robust to the training noise introduced by distant supervision.", "labels": [], "entities": []}, {"text": "On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7284023761749268}, {"text": "relation extraction", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7640353143215179}]}, {"text": "Most remarkably , for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9997707009315491}]}], "introductionContent": [{"text": "Relation extraction requires deep understanding of the relation between entities.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9663098752498627}]}, {"text": "Early studies mainly use hand-crafted features), and later kernel methods are introduced to automatically generate features ().", "labels": [], "entities": []}, {"text": "Recently neural network models have been introduced to embed words, relations, and sentences into continuous feature space, and have shown a remarkable success in relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.925485372543335}]}, {"text": "In this work, we study the problem of embedding textual relations, defined as the shortest dependency path 1 between two entities in the dependency graph of a sentence, to improve relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.7975267171859741}]}, {"text": "Textual relations are one of the most discriminative textual signals that lay the foundation of many relation extraction models (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7909449636936188}]}, {"text": "A number of recent studies have explored textual relation embedding under the supervised setting (, but the reliance on supervised training data limits their scalability.", "labels": [], "entities": []}, {"text": "In contrast, we embed textual relations with distant supervision (, which provides much larger-scale training data without the need of manual annotation.", "labels": [], "entities": []}, {"text": "However, the assertion of distant supervision, \"any sentence containing a pair of entities that participate in a knowledge base (KB) relation is likely to express the relation,\" can be violated more often than not, resulting in many wrongly labeled training examples.", "labels": [], "entities": []}, {"text": "A representative example is shown in 1.", "labels": [], "entities": []}, {"text": "Embedding quality is thus compromised by the noise in training data.", "labels": [], "entities": []}, {"text": "Our main contribution is a novel way to combat the wrong labeling problem of distant supervision.", "labels": [], "entities": []}, {"text": "Traditional embedding methods () are based on local statistics, i.e., individual textual-KB relation pairs like in (Left).", "labels": [], "entities": []}, {"text": "Our key hypothesis is that global statistics is more robust to noise than local statistics.", "labels": [], "entities": []}, {"text": "For individual examples, the relation label from distant supervision maybe wrong from time to time.", "labels": [], "entities": []}, {"text": "But when we zoom out to consider the entire corpus, and collect the global co-occurrence statistics of textual and KB relations, we will have a more comprehensive view of relation semantics: The semantics of a textual relation can then be represented by its cooccurrence distribution of KB relations.", "labels": [], "entities": []}, {"text": "For example, the distribution in  is also a good indicator of nationality, but not place of death.", "labels": [], "entities": []}, {"text": "Although it is still wrongly labeled with place of death a number of times, the negative impact becomes negligible.", "labels": [], "entities": []}, {"text": "Similarly,  We augment existing relation extractions using the learned textual relation embedding.", "labels": [], "entities": [{"text": "relation extractions", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.7916838824748993}]}, {"text": "On a popular dataset introduced by, we show that a number of recent relation extraction models, which are based on local statistics, can be greatly improved using our textual relation embedding.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7829820513725281}]}, {"text": "Most remarkably, anew best performance is achieved when augmenting the previous best model with our relation embedding: The precision of the top 1,000 relational facts discovered by the model is improved from 83.9% to 89.3%, a 33.5% decrease in error rate.", "labels": [], "entities": [{"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.999017596244812}, {"text": "error rate", "start_pos": 245, "end_pos": 255, "type": "METRIC", "confidence": 0.984906017780304}]}, {"text": "The results suggest that relation embedding with global statistics can capture complementary information to existing local statistics based models.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we discuss related work.", "labels": [], "entities": []}, {"text": "For the modeling part, we first describe how to collect global co-occurrence statistics of relations in Section 3, then introduce a neural network based embedding model in Section 4, and finally discuss how to combine the learned textual relation embedding with existing relation extraction models in Section 5.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 271, "end_pos": 290, "type": "TASK", "confidence": 0.7564332187175751}]}, {"text": "We empirically evaluate the proposed method in Section 6, and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experimental study, we show that GloRE can greatly improve the performance of several recent relation extraction models, including the previous best model on a standard dataset.", "labels": [], "entities": [{"text": "GloRE", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9192248582839966}, {"text": "relation extraction", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7785913944244385}]}, {"text": "There are 53 target KB relations, including a special relation NA indicating that there is no target relation between entities.", "labels": [], "entities": []}, {"text": "We follow the approach described in Section 3 to construct the relation graph from the NYT training data.", "labels": [], "entities": [{"text": "NYT training data", "start_pos": 87, "end_pos": 104, "type": "DATASET", "confidence": 0.9230225880940756}]}, {"text": "The constructed relation graph contains 321,447 edges with non-zero weight.", "labels": [], "entities": []}, {"text": "We further obtain a training set and a validation set from the edges of the relation graph.", "labels": [], "entities": []}, {"text": "We have observed that using a validation set totally disjoint from the training set leads to unstable validation loss, so we randomly sample 300K edges as the training set, and another 60K as the validation set.", "labels": [], "entities": []}, {"text": "The two sets can have some overlap.", "labels": [], "entities": []}, {"text": "For the merging model (Eq. 8), 10% of the edges are reserved as the validation set.", "labels": [], "entities": []}, {"text": "We evaluate with four recent relation extraction models whose source code is publicly available 3 . We use the optimized parameters provided by the authors.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7813050746917725}]}, {"text": "used to handle the three pieces of a contextual sentence (split by the two entities) separately.", "labels": [], "entities": []}, {"text": "\u2022 CNN+ATT and PCNN+ATT (: Different from the at-least-one assumption which loses information in the neglected sentences, these models learn soft attention weights (ATT) over contextual sentences and thus can use the information of all the contextual sentences.", "labels": [], "entities": [{"text": "CNN+ATT", "start_pos": 2, "end_pos": 9, "type": "DATASET", "confidence": 0.7392376661300659}, {"text": "PCNN+ATT", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.7755205035209656}, {"text": "soft attention weights (ATT)", "start_pos": 140, "end_pos": 168, "type": "METRIC", "confidence": 0.73057521879673}]}, {"text": "PCNN+ATT is the best-performing model on the NYT dataset.", "labels": [], "entities": [{"text": "PCNN+ATT", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7260251641273499}, {"text": "NYT dataset", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9340978860855103}]}, {"text": "Similar to previous work (, we use two settings for evaluation: (1) Held-out evaluation, where a subset of relational facts in KB is held out from training, and is later used to compare against newly discovered rela- tional facts.", "labels": [], "entities": []}, {"text": "This setting avoids human labor but can introduce some false negatives because of the incompleteness of the KB.", "labels": [], "entities": []}, {"text": "(2) Manual evaluation, where the discovered relational facts are manually judged by human experts.", "labels": [], "entities": []}, {"text": "For held-out evaluation, we report the precision-recall curve.", "labels": [], "entities": [{"text": "precision-recall curve", "start_pos": 39, "end_pos": 61, "type": "METRIC", "confidence": 0.9788188636302948}]}, {"text": "For manual evaluation, we report P recision@N , i.e., the precision of the top N discovered relational facts.", "labels": [], "entities": [{"text": "P recision", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.8834915459156036}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9991076588630676}]}, {"text": "Hyper-parameters of our model are selected based on the validation set.", "labels": [], "entities": []}, {"text": "For the embedding model, the mini-batch size is set to 128, and the state size of the GRU cells is 300.", "labels": [], "entities": []}, {"text": "For the merging model, the mini-batch size is set to 1024.", "labels": [], "entities": []}, {"text": "We use Adam with parameters recommended by the authors for optimization.", "labels": [], "entities": []}, {"text": "Word embeddings are initialized with the 300-dimensional word2vec vectors pre-trained on the Google News corpus . Early stopping based on the validation set is employed.", "labels": [], "entities": [{"text": "Google News corpus", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.8735995491345724}]}, {"text": "Our model is implemented using Tensorflow (, and the source code is available at https://github.com/ ppuliu/GloRE.", "labels": [], "entities": []}, {"text": "We first show that our approach, GloRE, can improve the performance of the previous best-performing model, PCNN+ATT, leading to anew state of the art on the NYT dataset.", "labels": [], "entities": [{"text": "GloRE", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.8634505867958069}, {"text": "PCNN+ATT", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.6435317595799764}, {"text": "NYT dataset", "start_pos": 157, "end_pos": 168, "type": "DATASET", "confidence": 0.9721984267234802}]}, {"text": "As shown in, when PCNN+ATT is augmented with GloRE, a consistent improvement along the precision-recall curve is observed.", "labels": [], "entities": [{"text": "ATT", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.7962641716003418}, {"text": "GloRE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9926778078079224}, {"text": "precision-recall", "start_pos": 87, "end_pos": 103, "type": "METRIC", "confidence": 0.9950366616249084}]}, {"text": "It is worth noting that although PCNN+ATT+GloRE seems to be inferior to PCNN+ATT when recall < 0.05, as we will show via manual evaluation, it is actually due to false negatives.", "labels": [], "entities": [{"text": "GloRE", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.6186007857322693}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.996482253074646}]}, {"text": "We also show in that the improvement brought by GloRE is general and not specific to PCNN+ATT; the other models also get a consistent improvement when augmented with GloRE.", "labels": [], "entities": [{"text": "GloRE", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.8436941504478455}, {"text": "PCNN+ATT", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.7143598198890686}]}, {"text": "To investigate whether the improvement brought by GloRE is simply from ensemble, we also augment PCNN+ATT with the other three base models in the same way as described in Section 5.", "labels": [], "entities": [{"text": "GloRE", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.7502849102020264}, {"text": "ATT", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.8352454304695129}]}, {"text": "The results in show that pairwise ensemble of existing relation extraction models does not yield much improvement, and GloRE brings much larger improvement than the other models.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7192734181880951}, {"text": "GloRE", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.9196866154670715}]}, {"text": "In summary, the held-out evaluation results suggest that GloRE captures useful information for relation extraction that is not captured by these local statistics based models.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.8604719936847687}]}, {"text": "the usefulness of directly embedding textual relations in addition to sentences.", "labels": [], "entities": []}, {"text": "Due to the incompleteness of the knowledge base, held-out evaluation introduces some false negatives.", "labels": [], "entities": []}, {"text": "The precision from held-out evaluation is therefore a lower bound of the true precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9983041286468506}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.8919471502304077}]}, {"text": "To get a more accurate evaluation of model performance, we have human experts to manually check the false relational facts judged by heldout evaluation in the top 1,000 predictions of three models, PCNN+ATT, PCNN+ATT+LoRE and PCNN+ATT+GloRE, and report the corrected results in.", "labels": [], "entities": [{"text": "GloRE", "start_pos": 235, "end_pos": 240, "type": "METRIC", "confidence": 0.6190183758735657}]}, {"text": "Each prediction is examined by two human experts who reach agreement with discussion.", "labels": [], "entities": []}, {"text": "To ensure fair comparison, the experts are not aware of the provenance of the predictions.", "labels": [], "entities": []}, {"text": "Under manual evaluation, PCNN+ATT+GloRE achieves the best performance in the full range of N . In particular, for the top 1,000 predictions, GloRE improves the precision of the previous best model PCNN+ATT from 83.9% to 89.3%.", "labels": [], "entities": [{"text": "GloRE", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.7623489499092102}, {"text": "GloRE", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.9866668581962585}, {"text": "precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.999427080154419}]}, {"text": "The manual evaluation results reinforce the previous observations from held-out evaluation.", "labels": [], "entities": []}, {"text": "For better illustration, we choose entity pairs that have only one contextual sentence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the NYT dataset.", "labels": [], "entities": [{"text": "NYT dataset", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.8500444889068604}]}, {"text": " Table 2: Manual evaluation: false negatives from held- out evaluation are manually corrected by human ex- perts.", "labels": [], "entities": []}]}