{"title": [{"text": "A Simple and Effective Approach to the Story Cloze Test", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.8830357392628988}]}], "abstractContent": [{"text": "In the Story Cloze Test, a system is presented with a 4-sentence prompt to a story, and must determine which one of two potential endings is the 'right' ending to the story.", "labels": [], "entities": []}, {"text": "Previous work has shown that ignoring the training set and training a model on the validation set can achieve high accuracy on this task due to stylistic differences between the story endings in the training set and validation and test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9978085160255432}]}, {"text": "Following this approach, we present a simpler fully-neural approach to the Story Cloze Test using skip-thought embeddings of the stories in a feed-forward network that achieves close to state-of-the-art performance on this task without any feature engineering.", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 75, "end_pos": 91, "type": "DATASET", "confidence": 0.7516910036404928}]}, {"text": "We also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9993041753768921}]}], "introductionContent": [{"text": "introduced the Story Cloze Test: given a four-sentence story prompt (or 'context'), the task is to pick the 'right' commonsense ending from two options.", "labels": [], "entities": []}, {"text": "The Cloze Test is intended to be a general framework for evaluating story understanding, since it ostensibly requires combining semantic understanding and commonsense knowledge of our world.", "labels": [], "entities": [{"text": "story understanding", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7953434586524963}]}, {"text": "The task is accompanied by the Rochester story (ROCstory) corpus.", "labels": [], "entities": [{"text": "Rochester story (ROCstory) corpus", "start_pos": 31, "end_pos": 64, "type": "DATASET", "confidence": 0.9643658995628357}]}, {"text": "The training set consists of crowdsourced five-sentence stories designed to capture common events in daily life.", "labels": [], "entities": []}, {"text": "The validation and testing sets consist of four-sentence prompts and labeled 'right' and 'wrong' story endings.", "labels": [], "entities": []}, {"text": "shows such a sample story from the Rochester corpus validation set with a labeled right and wrong ending.", "labels": [], "entities": [{"text": "Rochester corpus validation set", "start_pos": 35, "end_pos": 66, "type": "DATASET", "confidence": 0.9702769964933395}]}, {"text": "Many previous approaches to the Cloze Test have ignored the training set entirely and trained on the validation set since the former lacks 'negative' examples; although this greatly reduces the available training data, it circumvents the issue of obtaining negative examples during training.", "labels": [], "entities": []}, {"text": "Our contribution to this task is two-fold.", "labels": [], "entities": []}, {"text": "First, we achieve near state-of-the-art performance (within 1.1%) but with a much simpler, fullyneural approach.", "labels": [], "entities": []}, {"text": "Where previous approaches rely on feature engineering or involved neural network architectures, we achieve high accuracy with a fully neural approach involving only a single feedforward network and pre-trained skip-thought embeddings (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9979242086410522}]}, {"text": "Second, we find that considering only the last sentence of the context outperforms models that consider the full context.", "labels": [], "entities": []}, {"text": "Previous approaches focused on the accuracy achieved by either considering the whole context or ignoring the whole context of the story.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9985017776489258}]}, {"text": "In sum, our approach differs from previous efforts in the joint use of three strategies: (1) using skip-thought embeddings () for sentences in the story in a feed-forward neural network, (2) training the model on the provided validation set, and (3) considering the two endings with only the last sentence in the prompt.", "labels": [], "entities": []}, {"text": "This paper is structured as follows: we will discuss previous approaches to the problem and how they compare to our approach, describe our model and the experiments we ran in detail, and finally discuss reasons for our model's superior performance and why ignoring the first three sentences of the story produces better accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 320, "end_pos": 328, "type": "METRIC", "confidence": 0.9924476146697998}]}, {"text": "92 presented the original Story Cloze Test, and showed that while humans could achieve 100% accuracy on the task, a deep structured semantic model ( was the best performing artificial baseline, with a test-set accuracy of 58.5%.", "labels": [], "entities": [{"text": "Story Cloze Test", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.7471145391464233}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9977044463157654}, {"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9752030372619629}]}, {"text": "While they do consider using skip-thought embeddings for this task, they do so by choosing the ending whose embedding was closer to the average skip-thought embedding of the context.", "labels": [], "entities": []}, {"text": "This only achieves a test-set accuracy of 55.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9967676401138306}]}, {"text": "On the other hand, we train a feed-forward network using skip-thought embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all our experiments, we use the ROCStory corpus ().", "labels": [], "entities": [{"text": "ROCStory corpus", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.8329159021377563}]}, {"text": "The corpus consists of a training set of 98,161 five-sentence stories, a validation set consisting of 1,871 foursentence stories, and a test set of 1,871 foursentence stories, with the validation and test sets providing labeled 'right' and 'wrong' story endings for each story.", "labels": [], "entities": []}, {"text": "( crowdsourced the collection of stories on Amazon Mechanical Turk; workers were asked to compose five-sentence stories about common daily situations with a clear beginning and end.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9732262293497721}]}, {"text": "To create the validation and testing sets, endings were removed from stories and an additional group of workers on Mechanical Turk were asked to provide a 'right' ending or a 'wrong' ending.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 115, "end_pos": 130, "type": "DATASET", "confidence": 0.8765523135662079}]}, {"text": "Although models trained on the validation set score higher than those trained on the training set as previously discussed, we provide the results for the same model trained on the validation set (denoted val) as well as the training set (denoted trn) in for comparison.", "labels": [], "entities": []}, {"text": "When training on the training set, we tuned hyperparameters using the validation set.", "labels": [], "entities": []}, {"text": "When training on the validation set, we holdout 10% of the validation set, and tune hyper-parameters to find a configuration that maximizes the accuracy on the  held out data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9981600642204285}]}, {"text": "We use cross-entropy loss and SGD with learning rate of 0.01.", "labels": [], "entities": [{"text": "SGD", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.8576644659042358}, {"text": "learning rate", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.9726741313934326}]}, {"text": "During training, we save the model every 3000 iterations, and calculate the validation accuracy.", "labels": [], "entities": [{"text": "validation", "start_pos": 76, "end_pos": 86, "type": "TASK", "confidence": 0.9293482899665833}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.8901873826980591}]}, {"text": "We train each model five times (except the FC models, which we train once due to time considerations), and report the average test set accuracy of the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.986585259437561}]}, {"text": "We use the model with the highest validation accuracy in each round to calculate the test set accuracy for that round.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.5354041457176208}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.8731423616409302}]}, {"text": "We present our results in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracies for various models on the Story  Cloze Test", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9984013438224792}, {"text": "Story  Cloze Test", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9837450782457987}]}]}