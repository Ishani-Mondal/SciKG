{"title": [{"text": "Mittens: An Extension of GloVe for Learning Domain-Specialized Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a simple extension of the GloVe representation learning model that begins with general-purpose representations and updates them based on data from a specialized domain.", "labels": [], "entities": []}, {"text": "We show that the resulting representations can lead to faster learning and better results on a variety of tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many NLP tasks have benefitted from the public availability of general-purpose vector representations of words trained on enormous datasets, such as those released by the GloVe () and fastText ( teams.", "labels": [], "entities": []}, {"text": "These representations, when used as model inputs, have been shown to lead to faster learning and better results in a wide variety of settings.", "labels": [], "entities": []}, {"text": "However, many domains require more specialized representations but lack sufficient data to train them from scratch.", "labels": [], "entities": []}, {"text": "We address this problem with a simple extension of the GloVe model) that synthesizes general-purpose representations with specialized data sets.", "labels": [], "entities": []}, {"text": "The guiding idea comes from the retrofitting work of, which updates a space of existing representations with new information from a knowledge graph while also staying faithful to the original space (see also.", "labels": [], "entities": []}, {"text": "We show that the GloVe objective is amenable to a similar retrofitting extension.", "labels": [], "entities": []}, {"text": "We call the resulting model 'Mittens', evoking the idea that it is 'GloVe with a warm start' or a 'warmer GloVe'.", "labels": [], "entities": []}, {"text": "Our hypothesis is that Mittens representations synthesize the specialized data and the generalpurpose pretrained representations in away that gives us the best of both.", "labels": [], "entities": []}, {"text": "To test this, we conducted a diverse set of experiments.", "labels": [], "entities": []}, {"text": "In the first, we learn GloVe and Mittens representations on IMDB movie reviews and test them on separate IMDB reviews using simple classifiers.", "labels": [], "entities": []}, {"text": "In the second, we learn our representations from clinical text and apply them to a sequence labeling task using recurrent neural networks, and to edge detection using simple classifiers.", "labels": [], "entities": [{"text": "sequence labeling task", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.736095150311788}, {"text": "edge detection", "start_pos": 146, "end_pos": 160, "type": "TASK", "confidence": 0.7180434763431549}]}, {"text": "These experiments support our hypothesis about Mittens representations and help identify where they are most useful.", "labels": [], "entities": [{"text": "Mittens representations", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.8944668769836426}]}], "datasetContent": [{"text": "For our sentiment experiments, we train our representations on the unlabeled part of the IMDB review dataset released by.", "labels": [], "entities": [{"text": "IMDB review dataset released", "start_pos": 89, "end_pos": 117, "type": "DATASET", "confidence": 0.9069172441959381}]}, {"text": "This simulates a common use-case: Mittens should enable us to achieve specialized representations for these reviews while benefiting from the large datasets used to train External GloVe.", "labels": [], "entities": []}, {"text": "Our clinical text experiments begin with 100K clinical notes (transcriptions of the reports healthcare providers create summarizing their interactions with patients during appointments) from Real Health Data.", "labels": [], "entities": [{"text": "Real Health Data", "start_pos": 191, "end_pos": 207, "type": "DATASET", "confidence": 0.7875903050104777}]}, {"text": "These notes are divided into informal segments that loosely follow the 'SOAP' convention for such reporting (Subjective, Objective, Assessment, Plan).", "labels": [], "entities": []}, {"text": "The sample has 1.3 million such segments, and these segments provide our notion of 'document'.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Speed comparisons. The values are seconds per iteration, averaged over 10 iterations each on 5 simulated  corpora that produced count matrices with about 10% non-zero cells. Only the training step for each model is  timed. The CPU experiments were done on a machine with a 3.1 GHz Intel Core i7 chip and 16 GB of memory,  and the GPU experiments were done on machine with a 16 GB NVIDIA Tesla V100 GPU and 61 GB of memory.  Dashes mark tests that aren't applicable because the implementation doesn't perform GPU computations.", "labels": [], "entities": []}, {"text": " Table 3: Disease diagnosis examples.", "labels": [], "entities": [{"text": "Disease diagnosis", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8188454210758209}]}, {"text": " Table 4: SNOMED subgraphs and results. For the 'disorder' graph (the largest), a difference of 0.1% corresponds  to 408 examples. For the 'substance' graph (the smallest), it corresponds to 46 examples.", "labels": [], "entities": [{"text": "SNOMED subgraphs", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8387736678123474}]}]}