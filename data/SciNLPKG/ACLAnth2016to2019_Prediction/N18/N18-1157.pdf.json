{"title": [{"text": "Provable Fast Greedy Compressive Summarization with Any Monotone Submodular Function", "labels": [], "entities": []}], "abstractContent": [{"text": "Submodular maximization with the greedy algorithm has been studied as an effective approach to extractive summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.7196894586086273}]}, {"text": "This approach is known to have three advantages: its applicability to many useful sub-modular objective functions, the efficiency of the greedy algorithm, and the provable performance guarantee.", "labels": [], "entities": []}, {"text": "However, when it comes to compressive summarization, we are currently missing a counterpart of the extractive method based on submodular-ity.", "labels": [], "entities": [{"text": "compressive summarization", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.5755112767219543}]}, {"text": "In this paper, we propose a fast greedy method for compressive summarization.", "labels": [], "entities": [{"text": "compressive summarization", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.6225537657737732}]}, {"text": "Our method is applicable to any monotone submodular objective function, including many functions well-suited for document summarization.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.6048634201288223}]}, {"text": "We provide an approximation guarantee of our greedy algorithm.", "labels": [], "entities": []}, {"text": "Experiments show that our method is about 100 to 400 times faster than an existing method based on integer-linear-programming (ILP) formulations and that our method empirically achieves more than 95%-approximation.", "labels": [], "entities": [{"text": "approximation", "start_pos": 200, "end_pos": 213, "type": "METRIC", "confidence": 0.9799512624740601}]}], "introductionContent": [{"text": "Automatic document summarization continues to be a seminal subject of study in natural language processing and information retrieval).", "labels": [], "entities": [{"text": "Automatic document summarization", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6676658093929291}, {"text": "natural language processing", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.6281163593133291}, {"text": "information retrieval", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.7332737743854523}]}, {"text": "Owing to the recent advances in data collection, the size of document data to be summarized has been exploding, which has been bringing a drastic increase in the demand for fast summarization systems.", "labels": [], "entities": [{"text": "data collection", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.8108689188957214}]}, {"text": "Extractive summarization is a widely used approach to designing fast summarization systems.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8788596391677856}]}, {"text": "With this approach, we construct a summary by extracting some sentences from the original document(s).", "labels": [], "entities": []}, {"text": "The extractive approach is not only fast but also has the potential to achieve state-of-theart ROUGE scores), which was revealed by.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.9497378766536713}]}, {"text": "In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem, maximum coverage problem (, budgeted median problem (, and submodular maximization problem (.", "labels": [], "entities": []}, {"text": "Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (; examples of such functions include the coverage function, diversity reward function, and ROUGE.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.6640583574771881}, {"text": "ROUGE", "start_pos": 283, "end_pos": 288, "type": "METRIC", "confidence": 0.9420977830886841}]}, {"text": "Therefore, the method can deliver high performance by using monotone submodular objective functions that are suitable for the given tasks.", "labels": [], "entities": []}, {"text": "(2) The efficient greedy algorithm is effective for the submodular maximization problem, which provides fast summarization systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 109, "end_pos": 122, "type": "TASK", "confidence": 0.9729368686676025}]}, {"text": "(3) Theoretical performance guarantees of the greedy algorithm can be proved; for example, a 1 2 (1 \u2212 e \u22121 )-approximation guarantee can be obtained.", "labels": [], "entities": []}, {"text": "Although the above extractive methods successfully obtain summaries with high ROUGE scores, they have the following shortcoming: A long sentence typically has redundant parts, which means a summary constructed simply by extracting some sentences often includes many redundant parts.", "labels": [], "entities": [{"text": "summaries", "start_pos": 58, "end_pos": 67, "type": "TASK", "confidence": 0.9621610641479492}, {"text": "ROUGE", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9956244826316833}]}, {"text": "As a result, if the limitation placed on summary length is tight, the extractive approach cannot yield an informative summary.", "labels": [], "entities": []}, {"text": "Compressive summarization is known to be effective in overcoming this problem.", "labels": [], "entities": [{"text": "summarization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.8758604526519775}]}, {"text": "With this approach, a summary is constructed with some compressed sentences, and thus we can obtain a concise and informative summary.", "labels": [], "entities": []}, {"text": "To make compressed sentences, the dependency-tree-based approach ( is often used, which is advantageous in that each compressed sentence preserves its original dependency relations.", "labels": [], "entities": []}, {"text": "Specifically, given a set of dependency trees constructed for sentences in the original documents, a summary is obtained by extracting some rooted subtrees; each subtree corresponds to a compressed sentence.", "labels": [], "entities": []}, {"text": "Different from the extractive summarization, the dependency relations in each sentence must betaken into account, and hence the aforementioned extractive methods cannot be applied to compressive summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.5556252896785736}]}, {"text": "A number of methods have been proposed for compressive summarization).", "labels": [], "entities": [{"text": "compressive summarization", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.9341500401496887}]}, {"text": "These methods formulate summarization as a type of combinatorial optimization problem with a tree constraint, and they obtain summaries by solving the problem.", "labels": [], "entities": [{"text": "summarization", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.6383649706840515}]}, {"text": "Unfortunately, the existing methods have two drawbacks: (1) The class of objective functions to which they are applicable is limited; for example, they work only with the linear function or coverage function.", "labels": [], "entities": []}, {"text": "As a result, the performance of these methods cannot be improved by elaborating the objective functions.", "labels": [], "entities": []}, {"text": "(2) They contain costly procedures as their building blocks: integer-linear-programming (ILP) solvers, dynamic programming (DP) algorithms, and soon.", "labels": [], "entities": [{"text": "integer-linear-programming (ILP) solvers", "start_pos": 61, "end_pos": 101, "type": "TASK", "confidence": 0.6554426908493042}]}, {"text": "Therefore, they are not fast enough to be applied to large-scale document data.", "labels": [], "entities": []}, {"text": "Ina nutshell, compressive summarization is currently missing a fast method that is applicable to a wide variety of objective functions.", "labels": [], "entities": [{"text": "compressive summarization", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.6888887882232666}]}], "datasetContent": [{"text": "We applied our method to compressive summarization tasks with the three kinds of objective functions: the coverage function, the one with rewards, and ROUGE 1 . To benchmark our method, we also applied the ILP-based method to the tasks.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8299590647220612}, {"text": "ROUGE 1", "start_pos": 151, "end_pos": 158, "type": "METRIC", "confidence": 0.985969603061676}]}, {"text": "These two methods were compared in terms of achieved approximation ratios, ROUGE 1 scores, and running times.", "labels": [], "entities": [{"text": "ROUGE 1 scores", "start_pos": 75, "end_pos": 89, "type": "METRIC", "confidence": 0.9847843845685323}]}, {"text": "1.00 0.494 92.1: Approximation ratios, ROUGE 1 scores, and running times for our method (Greedy) and the ILP-based method (ILP); the average values over the 50 topics are presented.", "labels": [], "entities": [{"text": "Approximation ratios", "start_pos": 17, "end_pos": 37, "type": "METRIC", "confidence": 0.974094420671463}, {"text": "ROUGE 1 scores", "start_pos": 39, "end_pos": 53, "type": "METRIC", "confidence": 0.9833447734514872}]}, {"text": "The two methods are applied to compressive summarization tasks with three types of objective functions: Coverage, Coverage with rewards, and ROUGE 1 . Summaries obtained with the ILP-based method and ROUGE 1 objective function are oracle summaries.", "labels": [], "entities": [{"text": "compressive summarization tasks", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.6716864903767904}, {"text": "ROUGE 1", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.9837846755981445}]}], "tableCaptions": [{"text": " Table 1: Approximation ratios, ROUGE 1 scores, and running times for our method (Greedy) and the  ILP-based method (ILP); the average values over the 50 topics are presented. The two methods are applied  to compressive summarization tasks with three types of objective functions: Coverage, Coverage with  rewards, and ROUGE 1 . Summaries obtained with the ILP-based method and ROUGE 1 objective function  are oracle summaries.", "labels": [], "entities": [{"text": "Approximation", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9990028738975525}, {"text": "ROUGE 1 scores", "start_pos": 32, "end_pos": 46, "type": "METRIC", "confidence": 0.9753071467081705}, {"text": "summarization tasks", "start_pos": 220, "end_pos": 239, "type": "TASK", "confidence": 0.7685068845748901}, {"text": "ROUGE 1", "start_pos": 319, "end_pos": 326, "type": "METRIC", "confidence": 0.9740329384803772}]}]}