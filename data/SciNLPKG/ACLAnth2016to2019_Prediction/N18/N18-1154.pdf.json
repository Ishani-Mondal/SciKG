{"title": [{"text": "Generative Bridging Network for Neural Sequence Prediction", "labels": [], "entities": [{"text": "Neural Sequence Prediction", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.8422875801722208}]}], "abstractContent": [{"text": "In order to alleviate data sparsity and over-fitting problems in maximum likelihood estimation (MLE) for sequence prediction tasks, we propose the Generative Bridging Network (GBN), in which a novel bridge module is introduced to assist the training of the sequence prediction model (the generator network).", "labels": [], "entities": [{"text": "maximum likelihood estimation (MLE)", "start_pos": 65, "end_pos": 100, "type": "TASK", "confidence": 0.6399648686250051}, {"text": "sequence prediction tasks", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.810646116733551}, {"text": "sequence prediction", "start_pos": 257, "end_pos": 276, "type": "TASK", "confidence": 0.6985153257846832}]}, {"text": "Unlike MLE directly maximizing the conditional likelihood, the bridge extends the point-wise ground truth to abridge distribution conditioned on it, and the generator is optimized to minimize their KL-divergence.", "labels": [], "entities": []}, {"text": "Three different GBNs, namely uniform GBN, language-model GBN and coaching GBN, are proposed to penalize confidence, enhance language smoothness and relieve learning burden.", "labels": [], "entities": []}, {"text": "Experiments conducted on two recognized sequence prediction tasks (machine translation and abstractive text summarization) show that our proposed GBNs can yield significant improvements over strong baselines.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7203714549541473}, {"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.772385984659195}, {"text": "abstractive text summarization", "start_pos": 91, "end_pos": 121, "type": "TASK", "confidence": 0.6193295021851858}]}, {"text": "Furthermore, by analyzing samples drawn from different bridges, expected influences on the generator are verified.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence prediction has been widely used in tasks where the outputs are sequentially structured and mutually dependent.", "labels": [], "entities": [{"text": "Sequence prediction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9720035493373871}]}, {"text": "Recently, massive explorations in this area have been made to solve practical problems, such as machine translation, syntactic parsing (, spelling correction ( ), image captioning ( and speech recognition (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8283010423183441}, {"text": "syntactic parsing", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.7785520255565643}, {"text": "spelling correction", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.868398517370224}, {"text": "image captioning", "start_pos": 163, "end_pos": 179, "type": "TASK", "confidence": 0.7770551145076752}, {"text": "speech recognition", "start_pos": 186, "end_pos": 204, "type": "TASK", "confidence": 0.7670325338840485}]}, {"text": "Armed with modern computation power, deep LSTM (Hochreiter and Schmidhuber, 1997) or GRU () based neural sequence prediction models have achieved the state-of-the-art performance.", "labels": [], "entities": []}, {"text": "The typical training algorithm for sequence prediction is Maximum Likelihood Estimation  (MLE), which maximizes the likelihood of the target sequences conditioned on the source ones: Despite the popularity of MLE or teacher forcing) in neural sequence prediction tasks, two general issues are always haunting: 1).", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7864437699317932}, {"text": "neural sequence prediction tasks", "start_pos": 236, "end_pos": 268, "type": "TASK", "confidence": 0.7796701043844223}]}, {"text": "data sparsity and 2).", "labels": [], "entities": []}, {"text": "tendency for overfitting, with which can both harm model generalization.", "labels": [], "entities": []}, {"text": "To combat data sparsity, different strategies have been proposed.", "labels": [], "entities": []}, {"text": "Most of them try to take advantage of monolingual data.", "labels": [], "entities": []}, {"text": "Others try to modify the ground truth target based on derived rules to get more similar examples for training (.", "labels": [], "entities": []}, {"text": "To alleviate overfitting, regularization techniques, such as confidence penalization and posterior regularization (, are proposed recently.", "labels": [], "entities": []}, {"text": "As shown in, we propose a novel learning architecture, titled Generative Bridging Network (GBN), to combine both of the benefits from synthetic data and regularization.", "labels": [], "entities": []}, {"text": "Within the architecture, the bridge module (bridge) first transforms the point-wise ground truth into abridge distribution, which can be viewed as a target proposer from whom more target examples are drawn to train the generator.", "labels": [], "entities": []}, {"text": "By introducing different constraints, the bridge can beset or trained to possess specific property, with which the drawn samples can augment target-side data (alleviate data sparsity) while regularizing the training (avoid overfitting) of the generator network (generator).", "labels": [], "entities": []}, {"text": "In this paper, we introduce three different constraints to build three bridge modules.", "labels": [], "entities": []}, {"text": "Together with the generator network, three GBN systems are constructed: 1).", "labels": [], "entities": []}, {"text": "a uniform GBN, instantiating the constraint as a uniform distribution to penalize confidence; 2).", "labels": [], "entities": [{"text": "confidence", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.7883092761039734}]}, {"text": "a language-model GBN, instantiating the constraint as a pre-trained neural language model to increase language smoothness; 3).", "labels": [], "entities": []}, {"text": "a coaching GBN, instantiating the constraint as the generator's output distribution to seek a closeto-generator distribution, which enables the bridge to draw easy-to-learn samples for the generator to learn.", "labels": [], "entities": []}, {"text": "Without any constraint, our GBN degrades to MLE.", "labels": [], "entities": [{"text": "GBN", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.6042436361312866}, {"text": "MLE", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9958879351615906}]}, {"text": "The uniform GBN is proved to minimize KL-divergence with a so-called payoff distribution as in reward augmented maximum likelihood or RAML (.", "labels": [], "entities": [{"text": "RAML", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.8119089007377625}]}, {"text": "Experiments are conducted on two sequence prediction tasks, namely machine translation and abstractive text summarization.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7580489218235016}, {"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7910407185554504}, {"text": "abstractive text summarization", "start_pos": 91, "end_pos": 121, "type": "TASK", "confidence": 0.5927905340989431}]}, {"text": "On both of them, our proposed GBNs can significantly improve task performance, compared with strong baselines.", "labels": [], "entities": []}, {"text": "Among them, the coaching GBN achieves the best.", "labels": [], "entities": [{"text": "GBN", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.8263112902641296}]}, {"text": "Samples from these three different bridges are demonstrated to confirm the expected impacts they have on the training of the generator.", "labels": [], "entities": []}, {"text": "In summary, our contributions are: \u2022 A novel GBN architecture is proposed for sequence prediction to alleviate the data sparsity and overfitting problems, where the bridge module and the generator network are integrated and jointly trained.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8738236427307129}]}, {"text": "\u2022 Different constraints are introduced to build GBN variants: uniform GBN, languagemodel GBN and coaching GBN.", "labels": [], "entities": []}, {"text": "Our GBN architecture is proved to be a generalized form of both MLE and RAML.", "labels": [], "entities": [{"text": "RAML", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.799875795841217}]}, {"text": "\u2022 All proposed GBN variants outperform the MLE baselines on machine translation and abstractive text summarization.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7721028029918671}, {"text": "abstractive text summarization", "start_pos": 84, "end_pos": 114, "type": "TASK", "confidence": 0.622260848681132}]}, {"text": "Similar relative improvements are achieved compared to recent state-of-the-art methods in the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.9385577142238617}]}, {"text": "We also demonstrate the advantage of our GBNs qualitatively by comparing ground truth and samples from bridges.", "labels": [], "entities": []}], "datasetContent": [{"text": "We select machine translation and abstractive text summarization as benchmarks to verify our GBN framework.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7581745088100433}, {"text": "abstractive text summarization", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.6429608762264252}, {"text": "GBN framework", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.7791381776332855}]}], "tableCaptions": [{"text": " Table 1: Comparison with existing works on IWSLT- 2014 German-English Machine Translation Task.", "labels": [], "entities": [{"text": "IWSLT- 2014 German-English Machine Translation Task", "start_pos": 44, "end_pos": 95, "type": "TASK", "confidence": 0.8045658554349627}]}, {"text": " Table 2: Full length ROUGE F1 evaluation results on  the English Gigaword test set used by (Rush et al.,  2015). RG in the Table denotes ROUGE. Results  for comparison are taken from SAEASS (Zhou et al.,  2017).", "labels": [], "entities": [{"text": "ROUGE F1", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.6919682025909424}, {"text": "English Gigaword test set", "start_pos": 58, "end_pos": 83, "type": "DATASET", "confidence": 0.8203700631856918}, {"text": "RG", "start_pos": 114, "end_pos": 116, "type": "METRIC", "confidence": 0.9526146054267883}, {"text": "ROUGE", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9149513840675354}, {"text": "SAEASS", "start_pos": 184, "end_pos": 190, "type": "METRIC", "confidence": 0.7431461811065674}]}]}