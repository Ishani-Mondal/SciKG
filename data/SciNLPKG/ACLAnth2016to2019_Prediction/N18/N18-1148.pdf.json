{"title": [{"text": "The Importance of Calibration for Estimating Proportions from Annotations", "labels": [], "entities": [{"text": "Importance", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8356364965438843}]}], "abstractContent": [{"text": "Estimating label proportions in a target corpus is a type of measurement that is useful for answering certain types of social-scientific questions.", "labels": [], "entities": []}, {"text": "While past work has described a number of relevant approaches, nearly all are based on an assumption which we argue is invalid for many problems, particularly when dealing with human annotations.", "labels": [], "entities": []}, {"text": "In this paper, we identify and differentiate between two relevant data generating scenarios (intrinsic vs. ex-trinsic labels), introduce a simple but novel method which emphasizes the importance of calibration, and then analyze and experimentally validate the appropriateness of various methods for each of the two scenarios.", "labels": [], "entities": []}], "introductionContent": [{"text": "A methodological tool often used in the social sciences and humanities (and practical settings like journalism) is content analysis -the manual categorization of pieces of text into a set of categories which have been developed to answer a substantive research question.", "labels": [], "entities": [{"text": "content analysis -the manual categorization of pieces of text into a set of categories which have been developed to answer a substantive research question", "start_pos": 115, "end_pos": 269, "type": "Description", "confidence": 0.7359171819686889}]}, {"text": "Automated content analysis holds great promise for augmenting the efforts of human annotators.", "labels": [], "entities": [{"text": "Automated content analysis", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6112835903962454}]}, {"text": "While this task bears similarity to text categorization problems such as sentiment analysis, the quantity of real interest is often the proportion of documents in a dataset that should receive each label.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.9432879388332367}]}, {"text": "This paper tackles the problem of estimating label proportions in a target corpus based on a small sample of human annotated data.", "labels": [], "entities": []}, {"text": "As an example, consider the hypothetical question (not explored in this work) of whether hate speech is increasingly prevalent in social media posts in recent years.", "labels": [], "entities": []}, {"text": "\"Hate speech\" is a difficultto-define category only revealed (at least initially) through human judgments (.", "labels": [], "entities": [{"text": "Hate speech\"", "start_pos": 1, "end_pos": 13, "type": "TASK", "confidence": 0.7955940365791321}]}, {"text": "Note that the goal would not be to identify individual instances, but rather to estimate a proportion, as away of measuring the prevalence of asocial phenomenon.", "labels": [], "entities": []}, {"text": "Although we assume that trained annotators could recognize this phenomenon with some acceptable level of agreement, relying solely on manual annotation would restrict the number of messages that could be considered, and would limit the analysis to the messages available at the time of annotation.", "labels": [], "entities": []}, {"text": "We thus treat proportion estimation as a measurement problem, and seek away to train an instrument from a limited number of human annotations to measure label proportions in an unannotated target corpus.", "labels": [], "entities": [{"text": "proportion estimation", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.8533414006233215}]}, {"text": "This problem can be cast within a supervised learning framework, and past work has demonstrated that it is possible to improve upon a na\u00a8\u0131vena\u00a8\u0131ve classification-based approach, even without access to any labeled data from the target corpus.", "labels": [], "entities": []}, {"text": "However, as we argue ( \u00a72), most of this work is based on a set of assumptions that we believe are invalid in a significant portion of text-based research projects in the social sciences and humanities.", "labels": [], "entities": []}, {"text": "Our contributions in this paper include: \u2022 identifying two different data-generating scenarios for text data (intrinsic vs. extrinsic labels) and and establishing their importance to the problem of estimating proportions ( \u00a72); \u2022 analyzing which methods are suitable for each setting, and proposing a simple alternative approach for extrinsic labels ( \u00a73); and \u2022 an empirical comparison of methods that validates our analysis ( \u00a74).", "labels": [], "entities": []}, {"text": "Complicating matters somewhat is the fact that annotation may take place before the entire collection is available, so that the subset of instances that are manually annotated may represent a biased sample ( \u00a72).", "labels": [], "entities": []}, {"text": "Because this is so frequently the case, all of the results in this paper assume that we must confront the challenges of transfer learning or domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.7084457725286484}]}, {"text": "(The simpler case, where we can sample from the true population of interest, is revisited in \u00a75.)", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we focus on the case of binary classification where the difference between the source and target corpora results from a difference in time-that is, the training documents are sampled from onetime period, and the goal is to estimate label proportions on documents from a future time period.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7628682553768158}]}, {"text": "We include examples of both intrinsic and extrinsic labels to demonstrate the importance of this distinction to the effectiveness of different methods.", "labels": [], "entities": []}, {"text": "As described below, we create multiple subtasks from each dataset by using different partitions of the data.", "labels": [], "entities": []}, {"text": "In all cases, we report absolute error (AE) on the proportion of positive instances, averaged across the subtasks of each dataset.", "labels": [], "entities": [{"text": "absolute error (AE)", "start_pos": 24, "end_pos": 43, "type": "METRIC", "confidence": 0.925995409488678}]}, {"text": "Although we do not have access to the true annotation function, we approximate the expected label proportions in the target corpus by averaging the available labels, which should be a very close approximation when the number of available labels is large (which informed our choice of datasets for these experiments).", "labels": [], "entities": []}, {"text": "For a single subtask, the absolute error is thus evaluated as For all experiments, we also report the AE we would obtain from using the observed label proportions in the training sample as a prediction (labeled \"Train\").", "labels": [], "entities": [{"text": "absolute error", "start_pos": 26, "end_pos": 40, "type": "METRIC", "confidence": 0.9466691613197327}, {"text": "AE", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9992724061012268}]}, {"text": "Although this does not correspond to an interesting prediction (as it only says the future will always look exactly like the past), it does represent a fundamental baseline.", "labels": [], "entities": []}, {"text": "If a method is unable to do better than this, it suggests that the method has too much measurement error to be useful.", "labels": [], "entities": []}, {"text": "To test for statistically significant differences between methods, we use an omnibus application of the Wilcoxon signed-rank test to compare one method against all others, including a Bonferroni correction for the total number of tests per hypothesis.", "labels": [], "entities": []}, {"text": "With 4 datasets, each with 2 sample sizes, comparing against 6 other methods this results in a significance threshold of approximately 0.001.", "labels": [], "entities": [{"text": "significance threshold", "start_pos": 95, "end_pos": 117, "type": "METRIC", "confidence": 0.9787470400333405}]}, {"text": "Finally, in order to connect this work with past literature on estimating proportions, we also include aside experiment with one intrinsicallylabeled dataset where we have artificially modified the label proportions in the target corpus by dropping positive or negatively-labeled instances in order to simulate a large prior probability shift between the source and target domains.", "labels": [], "entities": []}, {"text": "We briefly describe the datasets we have used here and provide additional details in the supplementary material.", "labels": [], "entities": []}, {"text": "Note that although this work is primarily focused on applications in which the amount of human-annotated data is likely to be small, fair evaluation of these methods requires datasets that are large enough that we can approximate the expected label proportion in the target corpus using the available labels; as such, the following datasets were chosen so as to have a representative sample of sufficiently large intrinsically and extrinsically-labeled data, where documents were time-stamped, with label proportions that differ between time periods.", "labels": [], "entities": []}, {"text": "Media Frames Corpus (MFC): As a primary example of extrinsic labels, we use a dataset of several thousand news articles that have been annotated in terms of a set of broad-coverage framing dimensions (such as economics, morality, etc.).", "labels": [], "entities": [{"text": "Media Frames Corpus (MFC)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6098298331101736}]}, {"text": "We treat annotations as indicating the presence or absence of each dimension, and consider each one as a separate sub-task.", "labels": [], "entities": []}, {"text": "As with all datasets, we create a source and target corpus by dividing the datasets by year.", "labels": [], "entities": []}, {"text": "Particularly for this dataset, it seems reasonable to posit that the annotation function was relatively constant between source and target, as the annotators worked without explicit knowledge of the article's date).", "labels": [], "entities": []}, {"text": "Amazon reviews: As a secondary example of extrinsic labels, we make use of a subset of Amazon reviews for five different product categories, each of which has tens of thousands of reviews.", "labels": [], "entities": []}, {"text": "For this dataset, we ignore the star rating associated with the review, and instead focus on predicting the proportion of people that would rate the review as helpful.", "labels": [], "entities": []}, {"text": "Here we create separate subtasks for each product category by considering each pair of adjacent years as a source and target corpus, respectively ().", "labels": [], "entities": []}, {"text": "Yelp reviews: As a primary example of a large dataset with intrinsic labels, we make use of the Yelp10 dataset, treating the source location of the review as the label of interest.", "labels": [], "entities": [{"text": "Yelp reviews", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9155368506908417}, {"text": "Yelp10 dataset", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.974766880273819}]}, {"text": "Specifically, we create binary classification tasks by choosing pairs of cities with approximately the same number of reviews, and again use year of publication to divide the data into source and target corpora, creating multiple subtasks per pair of cities.", "labels": [], "entities": []}, {"text": "Twitter sentiment: Finally, we include a Twitter sentiment analysis dataset which was collected and automatically labeled, using the presence of certain emoticons as implicit labels indicating positive or negative sentiment (with the emoticons then removed from the text).", "labels": [], "entities": []}, {"text": "Because of the way this data was collected, and the relatively narrow time coverage, it seems plausible to treat the sentiment as an intrinsic label.", "labels": [], "entities": []}, {"text": "As with the above datasets, we create subtasks by considering all pairs of temporally adjacent days with sufficient tweets, and treating them as a paired source and target corpora, respectively.", "labels": [], "entities": []}], "tableCaptions": []}