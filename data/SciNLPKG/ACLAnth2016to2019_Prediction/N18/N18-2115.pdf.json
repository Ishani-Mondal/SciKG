{"title": [{"text": "Natural Language to Structured Query Generation via Meta-Learning", "labels": [], "entities": [{"text": "Structured Query Generation", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6426746944586436}]}], "abstractContent": [{"text": "In conventional supervised training, a model is trained to fit all the training examples.", "labels": [], "entities": []}, {"text": "However, having a monolithic model may not always be the best strategy, as examples could vary widely.", "labels": [], "entities": []}, {"text": "In this work, we explore a different learning protocol that treats each example as a unique pseudo-task, by reducing the original learning problem to a few-shot meta-learning scenario with the help of a domain-dependent relevance function.", "labels": [], "entities": []}, {"text": "When evaluated on the WikiSQL dataset, our approach leads to faster convergence and achieves 1.1%-5.4% absolute accuracy gains over the non-meta-learning counterparts.", "labels": [], "entities": [{"text": "WikiSQL dataset", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.921095609664917}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.8911619186401367}]}], "introductionContent": [{"text": "Conventional supervised training is a pervasive paradigm for NLP problems.", "labels": [], "entities": []}, {"text": "In this setting, a model is trained to fit all the training examples and their corresponding targets.", "labels": [], "entities": []}, {"text": "However, while sharing the same surface form of the prediction task, examples of the same problem may vary widely.", "labels": [], "entities": []}, {"text": "For instance, recognizing textual entailment is a binary classification problem on whether the hypothesis follows a given textual statement, but the challenge datasets consist of a huge variety of inference categories and genres.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 14, "end_pos": 44, "type": "TASK", "confidence": 0.8005545337994894}]}, {"text": "Similarly, fora semantic parsing problem that maps natural language questions to SQL statements, the number of conditions in a SQL query or the length of a question can vary substantially (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7870871126651764}]}, {"text": "The inherently high variety of the examples suggests an alternative training protocol: instead of learning a monolithic, one-size-fits-all model, it could be more effective to learn multiple models, where each one is designed fora specific \"task\" that covers a group of similar examples.", "labels": [], "entities": []}, {"text": "How- * Work performed while XH was at Microsoft Research.", "labels": [], "entities": []}, {"text": "ever, this strategy is faced with at least two difficulties.", "labels": [], "entities": []}, {"text": "As the number of tasks increases, each task will have much fewer training examples for learning a robust model.", "labels": [], "entities": []}, {"text": "In addition, the notion of \"task\", namely the group of examples, is typically not available in the dataset.", "labels": [], "entities": []}, {"text": "In this work, we explore this alternative learning setting and address the two difficulties by adapting the meta-learning framework.", "labels": [], "entities": []}, {"text": "Motivated by the few-shot learning scenario (, meta-learning aims to learn a general model that can quickly adapt to anew task given very few examples without retraining the model from scratch (.", "labels": [], "entities": []}, {"text": "We extend this framework by effectively creating pseudo-tasks with the help of a relevance function.", "labels": [], "entities": []}, {"text": "During training, each example is viewed as the test example of an individual \"task\", where its top-K relevant instances are used as training examples for this specific task.", "labels": [], "entities": []}, {"text": "A general model is trained for all tasks in aggregation.", "labels": [], "entities": []}, {"text": "Similarly during testing, instead of applying the general model directly, the top-K relevant instances (in the training set) to the given test example are first selected to update the general model, which then makes the final prediction.", "labels": [], "entities": []}, {"text": "The overview of the proposed framework is shown in.", "labels": [], "entities": []}, {"text": "When empirically evaluated on a recently proposed, large semantic parsing dataset, WikiSQL (, our approach leads to faster convergence and achieves 1.1%-5.4% absolute accuracy gain over the non-meta-learning counterparts, establishing anew state-of-the-art result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.8915238380432129}]}, {"text": "More importantly, we demonstrate how to design a relevance function to successfully reduce a regular supervised learning problem to a meta-learning problem.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first successful attempt in adapting meta-learning to a semantic task.", "labels": [], "entities": []}, {"text": "K from all training datapoints given a datapoint D j for constructing a pseudo-task T j as in the few-shot meta-learning setup.", "labels": [], "entities": []}, {"text": "(Bottom) We optimize the model parameters \u03b8 such that the model can learn to adapt anew task with parameters \u03b8 j via a few gradient stepson the training examples of the new task.", "labels": [], "entities": []}, {"text": "The model is updated by considering the test error on the test example of the new task.", "labels": [], "entities": []}, {"text": "See Section 2 for detail.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we introduce the WikiSQL dataset and preprocessing steps, the learner model in our meta-learning setup, and the experimental results.", "labels": [], "entities": [{"text": "WikiSQL dataset", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.8922476470470428}]}, {"text": "We evaluate our model on the WikiSQL dataset (.", "labels": [], "entities": [{"text": "WikiSQL dataset", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9690743386745453}]}, {"text": "We follow the data preprocessing in (.", "labels": [], "entities": []}, {"text": "Specifically, we first preprocess the dataset by running both tables and question-query pairs through Stanford Stanza ( ) using the script included with the WikiSQL dataset, which normalizes punctuations and cases of the dataset.", "labels": [], "entities": [{"text": "WikiSQL dataset", "start_pos": 157, "end_pos": 172, "type": "DATASET", "confidence": 0.887321263551712}]}, {"text": "We further normalize each question based on its corresponding table: for table entries and columns occurring in questions or queries, we normalize their format to be consistent with the table.", "labels": [], "entities": []}, {"text": "After preprocessing, we filter the training set by removing pairs whose ground truth solution contains constants not mentioned in the question, as our model requires the constants to be copied from the question.", "labels": [], "entities": []}, {"text": "We train and tune our model only on the filtered training and filtered development set, but we report our evaluation on the full development and test sets.", "labels": [], "entities": []}, {"text": "We obtain 59,845 (originally 61,297) training pairs, 8,928 (originally 9,145) development pairs and 17,283 test pairs (the test set is not filtered).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental Results on the WikiSQL dataset,  where Acc lf represents the logical form accuracy and  Acc ex represents the SQL execution accuracy. \"Pointer  loss\", \"Max loss\", and \"Sum loss\" are the non-meta- learning counterpart from", "labels": [], "entities": [{"text": "WikiSQL dataset", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9168955981731415}, {"text": "Acc", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9636265635490417}, {"text": "Pointer  loss", "start_pos": 158, "end_pos": 171, "type": "METRIC", "confidence": 0.9051767289638519}]}]}