{"title": [{"text": "Implicit Argument Prediction with Event Knowledge", "labels": [], "entities": [{"text": "Implicit Argument Prediction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.812341570854187}]}], "abstractContent": [{"text": "Implicit arguments are not syntactically connected to their predicates, and are therefore hard to extract.", "labels": [], "entities": []}, {"text": "Previous work has used models with large numbers of features, evaluated on very small datasets.", "labels": [], "entities": []}, {"text": "We propose to train models for implicit argument prediction on a simple cloze task, for which data can be generated automatically at scale.", "labels": [], "entities": [{"text": "implicit argument prediction", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6198603510856628}]}, {"text": "This allows us to use a neural model, which draws on narrative coherence and entity salience for predictions.", "labels": [], "entities": []}, {"text": "We show that our model has superior performance on both synthetic and natural data.", "labels": [], "entities": []}], "introductionContent": [{"text": "When parts of an event description in a text are missing, this event cannot be easily extracted, and it cannot easily be found as the answer to a question.", "labels": [], "entities": []}, {"text": "This is the case with implicit arguments, as in this example from the reading comprehension dataset of: Text: More than 2,600 people have been infected by Ebola in Liberia, Guinea, Sierra Leone and Nigeria since the outbreak began in December, according to the World Health Organization.", "labels": [], "entities": []}, {"text": "Question: The X outbreak has killed nearly 1,500.", "labels": [], "entities": []}, {"text": "In this example, it is Ebola that broke out, and Ebola was also the cause of nearly 1,500 people dying, but the text does not state this explicitly.", "labels": [], "entities": []}, {"text": "Ebola is an implicit argument of both outbreak and die, which is crucial to answering the question.", "labels": [], "entities": [{"text": "outbreak", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9614921808242798}]}, {"text": "We are particularly interested in implicit arguments that, like Ebola in this case, do appear in the text, but not as syntactic arguments of their predicates.", "labels": [], "entities": []}, {"text": "Event knowledge is key to determining implicit arguments.", "labels": [], "entities": []}, {"text": "In our example, diseases are maybe the single most typical things to breakout, and diseases also typically kill people.", "labels": [], "entities": []}, {"text": "The task of identifying implicit arguments was first addressed by and.", "labels": [], "entities": []}, {"text": "However, the datasets for the task were very small, and to our knowledge there has been very little further development on the task since then.", "labels": [], "entities": []}, {"text": "In this paper, we address the data issue by training models for implicit argument prediction on a simple cloze task, similar to the narrative cloze task, for which data can be generated automatically at scale.", "labels": [], "entities": [{"text": "implicit argument prediction", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.6674537062644958}]}, {"text": "This allows us to train a neural network to perform the task, building on two insights.", "labels": [], "entities": []}, {"text": "First, event knowledge is crucial for implicit argument detection.", "labels": [], "entities": [{"text": "implicit argument detection", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.7026876409848531}]}, {"text": "Therefore we build on models for narrative event prediction, using them to judge how coherent the narrative would be when we fill in a particular entity as the missing (implicit) argument.", "labels": [], "entities": [{"text": "narrative event prediction", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.6269261439641317}]}, {"text": "Second, the omitted arguments tend to be salient, as Ebola is in the text from which the above example is taken.", "labels": [], "entities": []}, {"text": "So in addition to narrative coherence, our model takes into account entity salience (.", "labels": [], "entities": []}, {"text": "In an evaluation on a large automatically generated dataset, our model clearly outperforms even strong baselines, and we find salience features to be important to the success of the model.", "labels": [], "entities": []}, {"text": "We also evaluate against a variant of the model that does not rely on gold features, finding that our simple neural model outperforms their much more complex model.", "labels": [], "entities": []}, {"text": "Our paper thus makes two major contributions.", "labels": [], "entities": []}, {"text": "1) We propose an argument cloze task to generate synthetic training data at scale for implicit argument prediction.", "labels": [], "entities": [{"text": "argument prediction", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7227134704589844}]}, {"text": "2) We show that neural event models for narrative schema prediction can be used on implicit argument prediction, and that a straightforward combination of event knowledge and entity salience can do well on the task.", "labels": [], "entities": [{"text": "narrative schema prediction", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.717031995455424}, {"text": "implicit argument prediction", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.6423500776290894}]}], "datasetContent": [{"text": "Previous implicit argument datasets were very small.", "labels": [], "entities": []}, {"text": "To overcome that limitation, we automatically create a large and comprehensive evaluation dataset, following the argument cloze task setting in Section 3.", "labels": [], "entities": []}, {"text": "Since the events and entities are extracted from dependency labels and coreference chains, we do not want to introduce systematic error into the evaluation from imperfect parsing and coreference algorithms.", "labels": [], "entities": []}, {"text": "Therefore, we create the evaluation set from OntoNotes (), which contains human-labeled dependency and coreference annotation fora large corpus.", "labels": [], "entities": []}, {"text": "So the extracted events and entities in the evaluation set are gold.", "labels": [], "entities": []}, {"text": "Note that this is only for evaluation; in training we do not rely on any gold annotations (Section 6.1).", "labels": [], "entities": []}, {"text": "There are four English sub-corpora in OntoNotes Release 5.0 4 that are annotated with dependency labels and coreference chains.", "labels": [], "entities": []}, {"text": "Three of them, which are mainly from broadcast news, share similar statistics in document length, so we combine them into a single dataset and name it ON-SHORT as it consists mostly of short documents.", "labels": [], "entities": [{"text": "ON-SHORT", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9300559759140015}]}, {"text": "The fourth subcorpus is from the Wall Street Journal and has significantly longer documents.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.9453562696774801}]}, {"text": "We call this subcorpus ON-LONG and evaluate on it separately.", "labels": [], "entities": [{"text": "ON-LONG", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9611918926239014}]}, {"text": "Some statistics are shown in  The implicit argument dataset from Gerber and Chai (2010) (referred as G&C henceforth) consists of 966 human-annotated implicit argument instances on 10 nominal predicates.", "labels": [], "entities": []}, {"text": "To evaluate our model on G&C, we convert the annotations to the input format of our model as follows: We map nominal predicates to their verbal form, and semantic role labels to syntactic argument types based on the NomBank frame definitions.", "labels": [], "entities": []}, {"text": "One of the examples (after mapping semantic role labels) is as follows: [Participants] subj will be able to transfer  For the nominal predicate investment, there are three arguments missing (subj, dobj, prep to).", "labels": [], "entities": []}, {"text": "The model first needs to determine that each of those argument positions in fact has an implicit filler.", "labels": [], "entities": []}, {"text": "Then, from a list of candidates (not shown here), it needs to select Participants as the implicit subj argument, money as the implicit dobj argument, and either other investment funds or a stock fund and a money-market fund as the implicit prep to.", "labels": [], "entities": []}, {"text": "For the synthetic argument cloze task, we compare our model with 3 baselines.", "labels": [], "entities": []}, {"text": "RANDOM Randomly select one entity from the candidate list.", "labels": [], "entities": []}, {"text": "MOSTFREQ Always select the entity with highest number of mentions.", "labels": [], "entities": [{"text": "MOSTFREQ", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.5203708410263062}]}, {"text": "EVENTWORD2VEC Use the event-based word embeddings described in Section 4.2 for predicates and arguments.", "labels": [], "entities": []}, {"text": "The representation of an event e is the sum of the embeddings of its components, i.e., where v, s, o, pare the embeddings of verb, subject, object, and prepositional object, respectively.", "labels": [], "entities": []}, {"text": "The coherence score of two events in this baseline model is their cosine similarity.", "labels": [], "entities": []}, {"text": "Like in our main model, the coherence score of the candidate is then the maximum pairwise coherence score, as described in Section 4.1.", "labels": [], "entities": []}, {"text": "The evaluation results on the ON-SHORT dataset are shown in.", "labels": [], "entities": [{"text": "ON-SHORT dataset", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.8846628665924072}]}, {"text": "The EVENT-WORD2VEC baseline is much stronger than the other two, achieving an accuracy of 38.40%.", "labels": [], "entities": [{"text": "EVENT-WORD2VEC baseline", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.9340063631534576}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9996724128723145}]}, {"text": "In fact, EVENTCOMP-8M by itself does not do better than EVENTWORD2VEC, but adding entity salience greatly boosts performance.", "labels": [], "entities": []}, {"text": "Using more training data (EVENTCOMP-40M) helps by a substantial margin both with and without entity salience features.", "labels": [], "entities": [{"text": "EVENTCOMP-40M", "start_pos": 26, "end_pos": 39, "type": "METRIC", "confidence": 0.8597160577774048}]}, {"text": "To see which of the entity salience features are important, we conduct an ablation test with the EVENTCOMP-8M model on ON-SHORT.", "labels": [], "entities": [{"text": "EVENTCOMP-8M", "start_pos": 97, "end_pos": 109, "type": "METRIC", "confidence": 0.8561022877693176}, {"text": "ON-SHORT", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.8603863716125488}]}, {"text": "From the results in, we can see that in our task, as in, the entity mentions features, i.e., the numbers of named, nominal, pronominal, and total mentions of the entity, are most helpful.", "labels": [], "entities": []}, {"text": "In fact, the other two features even decrease performance slightly.", "labels": [], "entities": []}, {"text": "We take a closer look at several of the models in. breaks down the results by the argument type of the removed argument.", "labels": [], "entities": []}, {"text": "On subjects, the EVENTWORD2VEC baseline matches the performance of EVENTCOMP, but not on direct objects and prepositional objects.", "labels": [], "entities": [{"text": "EVENTWORD2VEC baseline", "start_pos": 17, "end_pos": 39, "type": "METRIC", "confidence": 0.8845697641372681}, {"text": "EVENTCOMP", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9060332179069519}]}, {"text": "Subjects are semantically much less diverse than the other argument types, as they are very often animate.", "labels": [], "entities": []}, {"text": "A similar pattern is apparent in, which has results by the part-of-speech tag of the headword of the removed entity.", "labels": [], "entities": []}, {"text": "Note that an entity is a coreference chain, not a single mention; so when the headword is a pronoun, this is an entity which has only pronoun mentions.", "labels": [], "entities": []}, {"text": "A pronoun entity provides little semantic content beyond, again, animacy.", "labels": [], "entities": []}, {"text": "And again, EVENTWORD2VEC performs well on pronoun entities, but less soon entities described by a noun.", "labels": [], "entities": [{"text": "EVENTWORD2VEC", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.9316527843475342}]}, {"text": "It seems that EVENT-WORD2VEC can pickup on a coarse-grained pattern such as animate/inanimate, but not on more fine-grained distinctions needed to select the right noun, or to select a fitting direct objector prepositional object.", "labels": [], "entities": []}, {"text": "This matches the fact that EVENT-WORD2VEC gets a less clear signal on the task, in two respects: It gets much less information than EVENTCOMP on the distinction between argument positions, 8 and it only looks at overall event similarity while EVENTCOMP is trained to detect narrative coherence.", "labels": [], "entities": []}, {"text": "Entity salience contributes greatly across all argument types and parts of speech, but more strongly on subjects and pronouns.", "labels": [], "entities": [{"text": "Entity salience", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7693786323070526}]}, {"text": "This is again because subjects, and pronouns, are semantically less distinct, so they can only be distinguished by relative salience.", "labels": [], "entities": []}, {"text": "analyzes results by the frequency of the removed entity, that is, by its number of mentions.", "labels": [], "entities": []}, {"text": "The MOSTFREQ baseline, unsurprisingly, only does well when the removed entity is a highly frequent one.", "labels": [], "entities": [{"text": "MOSTFREQ", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.7020698189735413}]}, {"text": "The EVENTCOMP model is much better than MOSTFREQ at picking out the right entity when it is a rare one, as it can look at the semantic content of the entity as well as its frequency.", "labels": [], "entities": [{"text": "EVENTCOMP", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.8243723511695862}]}, {"text": "Entity salience boosts the performance of EVENTCOMP in particular for frequent entities.", "labels": [], "entities": []}, {"text": "The ON-LONG dataset, as discussed in Section 5.1, consists of OntoNotes data with much longer documents than found in ON-SHORT.", "labels": [], "entities": [{"text": "ON-LONG dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9655123949050903}, {"text": "OntoNotes data", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.7828373610973358}, {"text": "ON-SHORT", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.9702082872390747}]}, {"text": "Evaluation results on ON-LONG are shown in Table 5.", "labels": [], "entities": [{"text": "ON-LONG", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.6008031368255615}]}, {"text": "Although the overall numbers are lower than those for ON-SHORT, we are selecting from 36.95 candidates on average, more than 3 times more than for ON-SHORT.", "labels": [], "entities": [{"text": "ON-SHORT", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.8985316753387451}, {"text": "ON-SHORT", "start_pos": 147, "end_pos": 155, "type": "DATASET", "confidence": 0.9512750506401062}]}, {"text": "Considering that the accuracy of randomly selecting an entity is as low as 2.71%, the performance of our best performing model, with an accuracy of 27.87%, is quite good.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.999117910861969}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9987711310386658}]}, {"text": "The G&C data differs from the Argument Cloze data in two respects.", "labels": [], "entities": [{"text": "G&C data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.6409478932619095}, {"text": "Argument Cloze data", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.756335993607839}]}, {"text": "First, not every argument position that seems to be open needs to be filled: The model must additionally make a fill / no-fill decision.", "labels": [], "entities": []}, {"text": "Whether a particular argument position is typically filled is highly predicate-specific.", "labels": [], "entities": []}, {"text": "As the small G&C dataset does not provide enough data to train our neural model on this task, we instead train a simple logistic classifier, the fill / no-fill classifier, with a small subset of shallow lexical features used in, to make the decision.", "labels": [], "entities": [{"text": "G&C dataset", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.5964005962014198}]}, {"text": "These features describe the syntactic context of the predicate.", "labels": [], "entities": []}, {"text": "We use only 14 features; the original Gerber and Chai model had more than 80 features, and our re-implementation, described below, has around 60.", "labels": [], "entities": []}, {"text": "The second difference is that in G&C, an event may have multiple open argument positions.", "labels": [], "entities": []}, {"text": "In that case, the task is not just to select a candidate entity, but also to determine which of the open argument positions it should fill.", "labels": [], "entities": []}, {"text": "So the model must do multi implicit argument prediction.", "labels": [], "entities": [{"text": "multi implicit argument prediction", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.6886780261993408}]}, {"text": "We can flexibly adapt our method for training data generation to this case.", "labels": [], "entities": [{"text": "training data generation", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.6995578408241272}]}, {"text": "In particular, we create extra negative training events, in which an argument of the positive event has been moved to another argument position in the same event, as shown in.", "labels": [], "entities": []}, {"text": "We can then simply train our EVENTCOMP model on this extended training data.", "labels": [], "entities": [{"text": "EVENTCOMP", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.957216203212738}]}, {"text": "We refer to the extra training process as multi-arg training.", "labels": [], "entities": []}, {"text": "x 0 = The company x 1 = mill x 2 = power plant Context: ( build-pred, x 0 -subj, x 2 -dobj, -) Positive: ( reduce-pred, x 2 -subj, cost-dobj, -) Negative: ( reduce-pred, -, cost-dobj, x 2 -prep ) We compare our models to that of  We present the evaluation results in.", "labels": [], "entities": []}, {"text": "The original EVENTCOMP models do not perform well, which is as expected since the model is not designed to do the fill / no-fill decision and multi implicit argument prediction tasks as described above.", "labels": [], "entities": [{"text": "multi implicit argument prediction", "start_pos": 142, "end_pos": 176, "type": "TASK", "confidence": 0.6391507610678673}]}, {"text": "With the fill / no-fill classifier, precision rises by around 13 points because this classifier prevents many false positives.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9996054768562317}]}, {"text": "With additional multi-arg training, F 1 score improves by another 22-23 points.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9869161248207092}]}, {"text": "At this point, our model achieves a performance comparable to the much more complex G&C reimplementation GCAUTO.", "labels": [], "entities": [{"text": "G&C reimplementation GCAUTO", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.42851245403289795}]}, {"text": "Adding entity salience features further boosts both precision and recall, showing that implicit arguments do tend to be filled by salient entities, as we had hypothesized.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9994812607765198}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9993981122970581}]}, {"text": "Again, more training data substantially benefits the task.", "labels": [], "entities": []}, {"text": "Our best performing model, at 49.6 F 1 , clearly outperforms GCAUTO, and is comparable with the original Gerber and Chai (2012) model trained with gold features.", "labels": [], "entities": [{"text": "49.6 F 1", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.7790834506352743}, {"text": "GCAUTO", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.5638129115104675}]}], "tableCaptions": [{"text": " Table 2: Statistics on argument cloze datasets.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation on ON-SHORT.", "labels": [], "entities": [{"text": "ON-SHORT", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.5917534232139587}]}, {"text": " Table 4: Ablation test on entity salience features.  (Using EVENTCOMP-8M on ON-SHORT.)", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.972480058670044}]}, {"text": " Table 5: Evaluation on ON-LONG.", "labels": [], "entities": [{"text": "ON-LONG", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.7529988288879395}]}, {"text": " Table 6: Evaluation on G&C dataset.", "labels": [], "entities": [{"text": "G&C dataset", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.6420531943440437}]}, {"text": " Table 6.  The original EVENTCOMP models do not per- form well, which is as expected since the model  is not designed to do the fill / no-fill decision and  multi implicit argument prediction tasks as de- scribed above. With the fill / no-fill classifier,  precision rises by around 13 points because this  classifier prevents many false positives. With ad- ditional multi-arg training, F 1 score improves by  another 22-23 points. At this point, our model", "labels": [], "entities": [{"text": "multi implicit argument prediction", "start_pos": 157, "end_pos": 191, "type": "TASK", "confidence": 0.6814714074134827}, {"text": "precision", "start_pos": 257, "end_pos": 266, "type": "METRIC", "confidence": 0.9993946552276611}, {"text": "F 1 score", "start_pos": 387, "end_pos": 396, "type": "METRIC", "confidence": 0.9870811104774475}]}]}