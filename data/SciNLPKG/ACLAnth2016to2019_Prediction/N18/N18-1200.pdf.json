{"title": [], "abstractContent": [{"text": "We propose anew model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework.", "labels": [], "entities": [{"text": "speaker naming", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.828366756439209}]}, {"text": "To evaluate the performance of our model, we introduce anew dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres.", "labels": [], "entities": [{"text": "Big Bang Theory TV show", "start_pos": 102, "end_pos": 125, "type": "DATASET", "confidence": 0.7446955263614654}]}, {"text": "Our experiments show that our multimodal model significantly out-performs several competitive baselines on the average weighted F-score metric.", "labels": [], "entities": [{"text": "F-score", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.9795321822166443}]}, {"text": "To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.", "labels": [], "entities": [{"text": "speaker naming", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.7526973783969879}, {"text": "MovieQA 2017 Challenge", "start_pos": 203, "end_pos": 225, "type": "DATASET", "confidence": 0.9011924465497335}]}], "introductionContent": [{"text": "Identifying speakers and their names in movies, and videos in general, is a primary task for many video analysis problems, including automatic subtitle labeling (, content-based video indexing and retrieval (), video summarization (, and video storyline understanding ().", "labels": [], "entities": [{"text": "Identifying speakers and their names in movies", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8997505903244019}, {"text": "automatic subtitle labeling", "start_pos": 133, "end_pos": 160, "type": "TASK", "confidence": 0.5363622506459554}, {"text": "content-based video indexing and retrieval", "start_pos": 164, "end_pos": 206, "type": "TASK", "confidence": 0.7570754528045655}, {"text": "video summarization", "start_pos": 211, "end_pos": 230, "type": "TASK", "confidence": 0.7269185781478882}, {"text": "video storyline understanding", "start_pos": 238, "end_pos": 267, "type": "TASK", "confidence": 0.6325607200463613}]}, {"text": "It is a very challenging task, as the visual appearance of the characters changes over the course of the movie due to several factors such as scale, clothing, illumination, and so forth).", "labels": [], "entities": []}, {"text": "The annotation of movie data with speakers' names can be helpful in a number of applications, such as movie question answering ( , automatic identification of character relationships (, or automatic movie captioning (.", "labels": [], "entities": [{"text": "movie question answering", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.6676508088906606}, {"text": "automatic identification of character relationships", "start_pos": 131, "end_pos": 182, "type": "TASK", "confidence": 0.7662380278110504}, {"text": "automatic movie captioning", "start_pos": 189, "end_pos": 215, "type": "TASK", "confidence": 0.6007998287677765}]}, {"text": "Most previous studies relied primarily on visual information), and aimed for the slightly different task of face track labeling; 01:02:00 --> 01:02:01 Jack, must you go?", "labels": [], "entities": [{"text": "face track labeling", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7019811073939005}]}, {"text": "01:02:01 --> 01:02:04 Time for me to go row with other slaves.", "labels": [], "entities": [{"text": "Time", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9303032755851746}]}], "datasetContent": [{"text": "Our dataset consists of a mix of TV show episodes and full movies.", "labels": [], "entities": []}, {"text": "For the TV show, we use six full episodes of season one of the BBT.", "labels": [], "entities": [{"text": "BBT", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.908069908618927}]}, {"text": "The number of named characters in the BBT episodes varies between 5 to 8 characters per episode, and the background noise level is low.", "labels": [], "entities": [{"text": "BBT episodes", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9293130934238434}]}, {"text": "Additionally, we also acquired a set of eighteen full movies from different genres, to evaluate how our model works under different conditions.", "labels": [], "entities": []}, {"text": "In this latter dataset, the number of named characters ranges between 6 and 37, and it has varied levels of background noise.", "labels": [], "entities": []}, {"text": "We manually annotated this dataset with the character name of each subtitle segment.", "labels": [], "entities": []}, {"text": "To facilitate the annotation process, we built an interface that parses the movies subtitles files, collects the cast list from IMDB for each movie, and then shows one subtitle segment at a time along with the cast list so that the annotator can choose the correct character.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.9102082848548889}]}, {"text": "Using this tool, human annotators watched the movies and assigned a speaker name to each subtitle segment.", "labels": [], "entities": []}, {"text": "If a character name was not mentioned in the dialogue, the annotators labeled it as \"unknown.\"", "labels": [], "entities": []}, {"text": "To evaluate the quality of the annotations, five movies in our dataset were double annotated.", "labels": [], "entities": []}, {"text": "The Cohen's Kappa interannotator agreement score for these five movies is 0.91, which shows a strong level of agreement.", "labels": [], "entities": [{"text": "Cohen's Kappa interannotator agreement score", "start_pos": 4, "end_pos": 48, "type": "DATASET", "confidence": 0.6156288037697474}]}, {"text": "To clean the data, we removed empty segments, as well as subtitle description parts written between brackets such as \"\" and \"\".", "labels": [], "entities": []}, {"text": "We also removed segments with two speakers at the same time.", "labels": [], "entities": []}, {"text": "We intentionally avoided using any automatic means to split these segments, to preserve the high-quality of our gold standard.", "labels": [], "entities": []}, {"text": "shows the statistics of the collected data.", "labels": [], "entities": []}, {"text": "Overall, the dataset consists of 24 videos with a total duration of 40.28 hours, a net dialogue duration of 21.99 hours, and a total of 31,019 turns spoken by 463 different speakers.", "labels": [], "entities": []}, {"text": "Four of the movies in this dataset are used as a development set to develop supplementary systems and to fine tune our model's parameters; the remaining movies are used for evaluation.", "labels": [], "entities": []}, {"text": "We model our task as a classification problem, and use the unified optimization framework described earlier to assign a character name to each subtitle.", "labels": [], "entities": []}, {"text": "Since our dataset is highly unbalanced, with a few main characters usually dominating the entire dataset, we adopt the weighted F-score as our evaluation metric, instead of using an accuracy metric or a micro-average F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.9709223508834839}, {"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9977120161056519}, {"text": "F-score", "start_pos": 217, "end_pos": 224, "type": "METRIC", "confidence": 0.937770664691925}]}, {"text": "This allows us to take into account that most of the characters have only a few spoken subtitle segments, while at the same time placing emphasis on the main characters.", "labels": [], "entities": []}, {"text": "This leads sometimes to an average weighted F-score that is not between the average precision and recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9814994931221008}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9841586351394653}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9967942833900452}]}, {"text": "One aspect that is important to note is that characters are often referred to using different names.", "labels": [], "entities": []}, {"text": "For example, in the movie \"The Devil's Advocate,\" the character Kevin Lomax is also referred to as Kevin or Kev.", "labels": [], "entities": []}, {"text": "In more complicated situations, characters may even have multiple identities, such as the character Saul Bloom in the movie \"Ocean's Eleven,\" who pretends to be another character named Lyman Zerga.", "labels": [], "entities": []}, {"text": "Since our  Comparison between the average of macro-weighted average of precision, recall and fscore of the baselines and our model.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9800399541854858}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9996026158332825}, {"text": "fscore", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9979352951049805}]}, {"text": "* means statistically significant (t-test p-value < 0.05) when compared to baseline B3.", "labels": [], "entities": []}, {"text": "goal is to assign names to speakers, and not necessarily solve this coreference problem, we consider the assignment of the subtitle segments to any of the speaker's aliases to be correct.", "labels": [], "entities": []}, {"text": "Thus, during the evaluation, we map all the characters' aliases from our model's output to the names in the ground truth annotations.", "labels": [], "entities": []}, {"text": "Our mapping does not include other referent nouns such as \"Dad,\" \"Buddy,\" etc.; if a segment gets assigned to any such terms, it is considered a misprediction.", "labels": [], "entities": []}, {"text": "We compare our model against three baselines: B1: Most-frequently mentioned character consists of selecting the most frequently mentioned character in the dialogue as the speaker for all the subtitles.", "labels": [], "entities": []}, {"text": "Even though it is a simple baseline, it achieves an accuracy of 27.1%, since the leading characters tend to speak the most in the movies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9994725584983826}]}, {"text": "B2: Distribution-driven random assignment consists of randomly assigning character names according to a distribution that reflects their fraction of mentions in all the subtitles.", "labels": [], "entities": [{"text": "Distribution-driven random assignment", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.6190435787041982}]}, {"text": "B3: Gender-based distribution-driven random assignment consists of selecting the speaker names based on the voice-based gender detection classifier.", "labels": [], "entities": [{"text": "Gender-based distribution-driven random assignment", "start_pos": 4, "end_pos": 54, "type": "TASK", "confidence": 0.5807286128401756}]}, {"text": "This baseline randomly selects the character name that matches the speaker's gender according to the distribution of mentions of the names in the matching gender category.", "labels": [], "entities": []}, {"text": "The results obtained with our proposed unified optimization framework and the three baselines are shown in.", "labels": [], "entities": []}, {"text": "We also report the performance of the optimization framework using different combinations of the three modalities.", "labels": [], "entities": []}, {"text": "The model that uses all three modalities achieves the best results, and outperforms the strongest baseline (B3) by more than 6% absolute in average  weighted F-score.", "labels": [], "entities": [{"text": "B3)", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9193811416625977}, {"text": "F-score", "start_pos": 158, "end_pos": 165, "type": "METRIC", "confidence": 0.9069624543190002}]}, {"text": "It also significantly outperforms the usage of the visual and acoustic features combined, which have been frequently used together in previous work, suggesting the importance of textual features in this setting.", "labels": [], "entities": []}, {"text": "The ineffectiveness of the iVectors might be a result of the background noise and music, which are difficult to remove from the speech signal.", "labels": [], "entities": []}, {"text": "shows the t-Distributed Stochastic Neighbor Embedding (t-SNE) (Van Der Maaten, 2014), which is a nonlinear dimensionality reduction technique that models points in such away that similar vectors are modeled by nearby points and dissimilar objects are modeled by distant points, visualization of the iVectors over the whole BBT show and the movie \"Titanic.\"", "labels": [], "entities": [{"text": "BBT show", "start_pos": 323, "end_pos": 331, "type": "DATASET", "confidence": 0.9770847856998444}]}, {"text": "In the BBT there is almost no musical background or background noise, while, Titanic has musical background in addition to the background noise such as the screams of the drowning people.", "labels": [], "entities": [{"text": "BBT", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.9550991058349609}]}, {"text": "From the graph, the difference between the quality of the iVectors clusters on different noise-levels is clear.", "labels": [], "entities": []}, {"text": "shows the effect of adding components of our loss function to the initial loss L init function.", "labels": [], "entities": []}, {"text": "The performance of the model using only L init without the other parts is very low due to the sparsity of first person references and errors that the person reference classifier introduces.", "labels": [], "entities": []}, {"text": "In order to analyze the effect of the errors that several of the modules (e.g., gender and name reference classifiers) propagate into the system, we also test our framework by replacing each one of the components with its ground truth information.", "labels": [], "entities": []}, {"text": "As seen in, the results obtained in this setting show significant improvement with the replacement of each component in our framework, which suggests that additional work on these components will have positive implications on the overall system.", "labels": [], "entities": []}, {"text": "Given that for many of the movies in the dataset the videos are not completely available, we develop our initial system so that it only relies on the subtitles; we thus participate in the challenge subtitles task, which includes the dialogue (without the speaker information) as the only source of information to answer questions.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of our speaker naming approach, we design a model based on an end-to-end memory network (, namely Speaker-based Convolutional Memory Network (SC-MemN2N), which relies on the MovieQA dataset, and integrates the speaker naming approach as a component in the network.", "labels": [], "entities": [{"text": "speaker naming", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.857236385345459}, {"text": "MovieQA dataset", "start_pos": 207, "end_pos": 222, "type": "DATASET", "confidence": 0.9647763967514038}, {"text": "speaker naming", "start_pos": 243, "end_pos": 257, "type": "TASK", "confidence": 0.8039543032646179}]}, {"text": "Specifically, we use our speaker naming framework to infer the name of the speaker for each segment of the subtitles, and prepend the predicted speaker name to each turn in the subtitles.", "labels": [], "entities": []}, {"text": "To represent the movie subtitles, we represent each turn in the subtitles as the mean-pooling of a 300-dimension pretrained) representation of each word in the sentence.", "labels": [], "entities": []}, {"text": "We similarly represent the input questions and their corresponding answers.", "labels": [], "entities": []}, {"text": "Given a question, we use the SC-MemN2N memory to find an answer.", "labels": [], "entities": []}, {"text": "For questions asking about specific characters, we keep the memory slots that have the characters in question as speakers or mentioned in, and mask out the rest of the memory slots.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the annotated movie dataset.", "labels": [], "entities": [{"text": "annotated movie dataset", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.617531289656957}]}, {"text": " Table 2: Performance metrics of the reference clas- sifier on the test data.", "labels": [], "entities": []}, {"text": " Table 5: Comparison between our model while  replacing different components with their ground  truth information.", "labels": [], "entities": []}, {"text": " Table 7: Performance comparison for the subti- tles task on the MovieQA 2017 Challenge on both  validation and test sets. We compare our models  with the best existing models (from the challenge  leaderboard).", "labels": [], "entities": [{"text": "MovieQA 2017 Challenge", "start_pos": 65, "end_pos": 87, "type": "DATASET", "confidence": 0.9548267920811971}]}]}