{"title": [{"text": "Annotation Artifacts in Natural Language Inference Data", "labels": [], "entities": []}], "abstractContent": [{"text": "Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to.", "labels": [], "entities": [{"text": "natural language inference", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.6677397688229879}]}, {"text": "We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise.", "labels": [], "entities": []}, {"text": "Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et al., 2015) and 53% of MultiNLI (Williams et al., 2018).", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.947451651096344}]}, {"text": "Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes.", "labels": [], "entities": []}, {"text": "Our findings suggest that the success of natural language inference models to date has been overestimated , and that the task remains a hard open problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language inference (NLI; also known as recognizing textual entailment, or RTE) is a widely-studied task in natural language processing, to which many complex semantic tasks, such as question answering and text summarization, can be reduced ().", "labels": [], "entities": [{"text": "Natural language inference (NLI", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7044148683547974}, {"text": "RTE)", "start_pos": 82, "end_pos": 86, "type": "TASK", "confidence": 0.7687743008136749}, {"text": "question answering", "start_pos": 190, "end_pos": 208, "type": "TASK", "confidence": 0.8353223204612732}, {"text": "text summarization", "start_pos": 213, "end_pos": 231, "type": "TASK", "confidence": 0.675706684589386}]}, {"text": "Given a pair of sentences, a premise p and a hypothesis h, the goal is to determine whether or not p semantically entails h.", "labels": [], "entities": []}, {"text": "The problem of acquiring large amounts of labeled inference data was addressed by, who devised a method for crowdsourcing high-agreement entailment annotations en masse, creating the SNLI and later the genrediverse MultiNLI () datasets.", "labels": [], "entities": []}, {"text": "In this process, crowd workers are presented with These authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "a premise p drawn from some corpus (e.g., image captions), and are required to generate three new sentences (hypotheses) based on p, according to one of the following criteria:", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: An instance from SNLI that illustrates the artifacts that arise from the annotation protocol. A  common strategy for generating entailed hypotheses is to remove gender or number information. Neutral  hypotheses are often constructed by adding a purpose clause. Negations are often introduced to generate  contradictions.", "labels": [], "entities": []}, {"text": " Table 2: Performance of a premise-oblivious text  classifier on NLI. The MultiNLI benchmark con- tains two test sets: matched (in-domain exam- ples) and mismatched (out-of-domain examples).  A majority baseline is presented for reference.", "labels": [], "entities": []}, {"text": " Table 5: Performance of high-performing NLI models on the full, Hard, and Easy NLI test sets.", "labels": [], "entities": [{"text": "Easy NLI test sets", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.6475800946354866}]}]}