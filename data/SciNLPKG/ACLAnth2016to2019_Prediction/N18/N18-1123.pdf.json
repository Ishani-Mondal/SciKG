{"title": [{"text": "Neural Machine Translation for Bilingually Scarce Scenarios: A Deep Multi-task Learning Approach", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7267669240633646}]}], "abstractContent": [{"text": "Neural machine translation requires large amounts of parallel training text to learn a reasonable-quality translation model.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7045624752839407}]}, {"text": "This is particularly inconvenient for language pairs for which enough parallel text is not available.", "labels": [], "entities": []}, {"text": "In this paper, we use monolingual linguistic resources in the source side to address this challenging problem based on a multi-task learning approach.", "labels": [], "entities": []}, {"text": "More specifically, we scaffold the machine translation task on auxiliary tasks including semantic parsing, syntactic parsing, and named-entity recognition.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.7995269099871317}, {"text": "semantic parsing", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.722296953201294}, {"text": "syntactic parsing", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7330116182565689}, {"text": "named-entity recognition", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.7306192517280579}]}, {"text": "This effectively injects semantic and/or syntactic knowledge into the translation model, which would otherwise require a large amount of training bitext.", "labels": [], "entities": [{"text": "translation", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.9675523042678833}]}, {"text": "We empirically evaluate and show the effectiveness of our multi-task learning approach on three translation tasks: English-to-French, English-to-Farsi, and English-to-Vietnamese.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) with attentional encoder-decoder architectures () has revolutionised machine translation, and achieved state-of-the-art for several language pairs.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7447977066040039}, {"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.8019632995128632}]}, {"text": "However, NMT is notorious for its need for large amounts of bilingual data) to achieve reasonable translation quality.", "labels": [], "entities": []}, {"text": "Leveraging existing monolingual resources is a potential approach for compensating this requirement in bilingually scarce scenarios.", "labels": [], "entities": []}, {"text": "Ideally, semantic and syntactic knowledge learned from existing linguistic resources provides NMT with proper inductive biases, leading to increased generalisation and better translation quality.", "labels": [], "entities": []}, {"text": "Multi-task learning (MTL) is an effective approach to inject knowledge into a task, which is learned from other related tasks.", "labels": [], "entities": [{"text": "Multi-task learning (MTL)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8673998951911926}]}, {"text": "Various recent works have attempted to improve NMT with an MTL approach (; however, they either do not make use of curated linguistic resources (, or their MTL architectures are restrictive yielding mediocre improvements.", "labels": [], "entities": [{"text": "NMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9036845564842224}]}, {"text": "The current research leaves open how to best leverage curated linguistic resources in a suitable MTL framework to improve NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.8660036325454712}]}, {"text": "In this paper, we make use of curated monolingual linguistic resources in the source side to improve NMT in bilingually scarce scenarios.", "labels": [], "entities": [{"text": "NMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9160994291305542}]}, {"text": "More specifically, we scaffold the machine translation task on auxiliary tasks including semantic parsing, syntactic parsing, and named-entity recognition.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.7995265126228333}, {"text": "semantic parsing", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.722296878695488}, {"text": "syntactic parsing", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7330121248960495}, {"text": "named-entity recognition", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.7306192517280579}]}, {"text": "This is achieved by casting the auxiliary tasks as sequence-to-sequence (SEQ2SEQ) transduction tasks, and tie the parameters of their encoders and/or decoders with those of the main translation task.", "labels": [], "entities": []}, {"text": "Our MTL architectures makes use of deep stacked encoders and decoders, where the parameters of the top layers are shared across the tasks.", "labels": [], "entities": []}, {"text": "We further make use of adversarial training to prevent contamination of common knowledge with task-specific information.", "labels": [], "entities": []}, {"text": "We present empirical results on translating from English into French, Vietnamese, and Farsi; three target languages with varying degree of divergence compared to English.", "labels": [], "entities": []}, {"text": "Our extensive empirical results demonstrate the effectiveness of our MTL approach in substantially improving the translation quality for these three translation tasks in bilingually scarce scenarios.", "labels": [], "entities": [{"text": "MTL", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9540826082229614}]}, {"text": "Encoder The encoder is a bi-directional RNN whose hidden states represent tokens of the input sequence.", "labels": [], "entities": []}, {"text": "These representations capture information not only of the corresponding token, but also other tokens in the sequence to leverage the context.", "labels": [], "entities": []}, {"text": "The bi-directional RNN consists of two RNNs running in the left-to-right and right-to-left directions over the input sequence: where E E E S [x i ] is the embedding of the token xi from the embedding table E E E S of the input (source) space, and \u2212 \u2192 hi and \u2190 \u2212 hi are the hidden states of the forward and backward RNNs which can be based on the LSTM (long-short term memory)) or GRU (gated-recurrent unit) () units.", "labels": [], "entities": []}, {"text": "Each source token is then represented by the concatenation of the corresponding bidirectional hidden states, Decoder.", "labels": [], "entities": []}, {"text": "The backbone of the decoder is a unidirectional RNN which generates the token of the output one-by-one from left to right.", "labels": [], "entities": []}, {"text": "The generation of each token y j is conditioned on all of the previously generated tokens y <j via the state of the RNN decoder s j , and the input sequence via a dynamic context vector c j (explained shortly): where E E ET [y j ] is the embedding of the token y j from the embedding table E E ET of the output (target) space, and the W matrices and b r vector are the parameters.", "labels": [], "entities": []}, {"text": "A crucial element of the decoder is the attention mechanism which dynamically attends to relevant parts of the input sequence necessary for generating the next token in the output sequence.", "labels": [], "entities": []}, {"text": "Before generating the next token t j , the decoder computes the attention vector \u03b1 j over the input token: which intuitively is similar to the notion of alignment in word/phrase-based statistical MT ().", "labels": [], "entities": [{"text": "word/phrase-based statistical MT", "start_pos": 166, "end_pos": 198, "type": "TASK", "confidence": 0.4993205785751343}]}, {"text": "The attention vector is then used to compute a fixed-length dynamic representation of the source sentence which is conditioned upon in the RNN decoder when computing the next state or generating the output word (as mentioned above).", "labels": [], "entities": []}, {"text": "The model parameters are trained end-to-end by maximising the (regularised) log-likelihood of the training data arg max where the above conditional probability is defined according to eqn (1).", "labels": [], "entities": []}, {"text": "Usually drop-out is employed to prevent over-fitting on the training data.", "labels": [], "entities": [{"text": "training data", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.6712999194860458}]}, {"text": "In the decoding time, the best output sequence fora given input sequence is produced by arg max Usually greedy decoding or beam search algorithms are employed to find an approximate solution, since solving the above optimisation problem exactly is computationally hard.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The statistics of bilingual corpora.", "labels": [], "entities": []}, {"text": " Table 2: BLEU scores and perplexities of the baseline vs our MTL architecture with various auxiliary  tasks on the full bilingual datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984772801399231}]}, {"text": " Table 3: Our method (partial parameter sharing)  against Baseline 2 (full parameter sharing).", "labels": [], "entities": []}, {"text": " Table 4: Example of translations on Farsi test set. In this examples each Farsi word is replaced with its  English translation, and the order of words is reversed (Farsi is written right-to-left). The structure of  Farsi is Subject-Object-Verb (SOV), leading to different word orders in English and Reference sentences.", "labels": [], "entities": [{"text": "Farsi test set", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8175220489501953}]}]}