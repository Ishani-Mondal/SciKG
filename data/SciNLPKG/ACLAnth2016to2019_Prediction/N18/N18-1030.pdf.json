{"title": [], "abstractContent": [{"text": "Building a taxonomy from the ground up involves several sub-tasks: selecting terms to include , predicting semantic relations between terms, and selecting a subset of relational instances to keep, given constraints on the tax-onomy graph.", "labels": [], "entities": [{"text": "predicting semantic relations between terms", "start_pos": 96, "end_pos": 139, "type": "TASK", "confidence": 0.8571279168128967}]}, {"text": "Methods for this final step-taxonomic organization-vary both in terms of the constraints they impose, and whether they enable discovery of synonymous terms.", "labels": [], "entities": []}, {"text": "It is hard to isolate the impact of these factors on the quality of the resulting taxonomy because organization methods are rarely compared directly.", "labels": [], "entities": []}, {"text": "In this paper, we present a head-to-head comparison of six taxonomic organization algorithms that vary with respect to their structural and transitivity constraints, and treatment of synonymy.", "labels": [], "entities": []}, {"text": "We find that while transitive algorithms out-perform their non-transitive counterparts, the top-performing transitive algorithm is prohibitively slow for taxonomies with as few as 50 entities.", "labels": [], "entities": []}, {"text": "We propose a simple modification to a non-transitive optimum branching algorithm to explicitly incorporate synonymy, resulting in a method that is substantially faster than the best transitive algorithm while giving complementary performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "Many words and phrases fit within a natural semantic hierarchy: a mobile is a type of telephone, which in turn is a communications device and an object.", "labels": [], "entities": []}, {"text": "Taxonomies, which encode this knowledge, are important resources for natural language understanding systems.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.6448371609052023}]}, {"text": "There is ongoing interest in developing methods to build taxonomic resources automatically.", "labels": [], "entities": []}, {"text": "Although several widelyused general ontologies (e.g. WordNet) and domain-specific ontologies (e.g. Unified Medical Language System (UMLS) (Boden-", "labels": [], "entities": [{"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9428212642669678}]}], "datasetContent": [{"text": "In order to directly compare the organization algorithms described, we organize our experiments as follows.", "labels": [], "entities": []}, {"text": "We first run entity extraction (Section 4.1) and relation prediction (Section 4.2) as a common initialization for all algorithms.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7582148611545563}, {"text": "relation prediction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.873875230550766}]}, {"text": "Then, we take the edge scores output by the relation prediction step and feed them to each taxonomic organization algorithm (Section 4.3).", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8029610514640808}]}, {"text": "Finally, we compare the output from each algorithm.", "labels": [], "entities": []}, {"text": "Here we describe the initialization steps in more detail.", "labels": [], "entities": []}, {"text": "We conduct experiments aimed at addressing three primary research questions: (1) How does each taxonomic organization algorithm perform?", "labels": [], "entities": []}, {"text": "In particular, how do DAG algorithms compare to tree-constrained ones, and how do transitive algorithms compare to their non-transitive counterparts?", "labels": [], "entities": []}, {"text": "(2) Are any algorithms, particularly the ILP methods, too slow to use on large sets of terms?", "labels": [], "entities": []}, {"text": "(3) Given that hypernym relation prediction is far from perfect, how robust is each algorithm to noise in the predicted relations?", "labels": [], "entities": [{"text": "hypernym relation prediction", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.6254497369130453}]}], "tableCaptions": [{"text": " Table 1: Evaluation of the HypeNET hypernym clas- sifier and the PARAGRAM synonym classifier on the  PPDB test set and four benchmark test sets. We report  micro-averaged F1-scores for positive and negative in- stances in the test sets.", "labels": [], "entities": [{"text": "PARAGRAM", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.8423294425010681}, {"text": "PPDB test set", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9821694294611613}, {"text": "F1-scores", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9775511622428894}]}, {"text": " Table 3: Precision, recall, and F1 of hypernym, synonym, and all (relation-specific) edges for each method. Metrics  are weighted averages over the 45 local taxonomies in the test set, where each taxonomy's result is weighted by its  number of nodes. Starred methods indicate an oracle, where the weight of edges appearing in WordNet is set to 1  at input.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.985298752784729}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9986650943756104}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9990113973617554}, {"text": "WordNet", "start_pos": 327, "end_pos": 334, "type": "DATASET", "confidence": 0.9586184024810791}]}, {"text": " Table 4: Average runtime (seconds) over all 45 targets,  and percent of targets for which runtime exceeded 5  minutes, by algorithm.", "labels": [], "entities": [{"text": "Average runtime (seconds)", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8630084156990051}]}]}