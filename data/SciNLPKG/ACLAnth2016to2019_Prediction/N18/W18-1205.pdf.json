{"title": [{"text": "Subword-level Composition Functions for Learning Word Embeddings", "labels": [], "entities": [{"text": "Subword-level Composition Functions", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8535985946655273}]}], "abstractContent": [], "introductionContent": [{"text": "Word embeddings are used in many natural language processing tasks).", "labels": [], "entities": []}, {"text": "In word embedding models, words are mapped or \"embedded\" into low-dimensional real-valued vectors.", "labels": [], "entities": []}, {"text": "Such mapping is based, implicitly or explicitly, on word co-occurrence statistics (.", "labels": [], "entities": []}, {"text": "Naturally, frequent words provide a better representation of their distributional properties; thus the quality of word embeddings is indirect relation to the frequency of words (.", "labels": [], "entities": []}, {"text": "However, even in large corpora, most words occur very few times.", "labels": [], "entities": []}, {"text": "For example, shows that the words occurring 3 times or less constitute almost 70% of the vocabulary.", "labels": [], "entities": []}, {"text": "Consequently, most of the in-vocabulary words (for a given task/corpora) have to be discarded or embedded into low-quality vectors.", "labels": [], "entities": []}, {"text": "Therefore, wordlevel models suffer from data sparsity.", "labels": [], "entities": []}, {"text": "Another issue with word-level models is that they do not make use of morphological information.", "labels": [], "entities": []}, {"text": "Different forms of the same word are treated as completely unrelated entities.", "labels": [], "entities": []}, {"text": "For example, as shown in Section 4.2, we find that the word \"physicist\" and \"physicists\" are not close to each other in a well-know word embedding model Skip-Gram (.", "labels": [], "entities": []}, {"text": "These two issues are addressed by the emerging methodology of subword-level representations, as discussed in Section 2.", "labels": [], "entities": []}, {"text": "The most notable example of such representations is FastText (.", "labels": [], "entities": []}, {"text": "It represents each word as a bagof-character n-grams.", "labels": [], "entities": []}, {"text": "Representations for character n-grams, once they are learned, can be combined (via simple summation) to represent out-ofvocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "This paper contributes to the discussion of composition functions for constructing subword-level embeddings and their evaluation.", "labels": [], "entities": []}, {"text": "We propose and evaluate several models (including convolutional and recurrent neural networks) that can embed arbitrary character sequences into vectors.", "labels": [], "entities": []}, {"text": "Our models do not rely on any external resource.", "labels": [], "entities": []}, {"text": "We also propose a hybrid training scheme, which makes these neural networks directly integrated into Skip-Gram model.", "labels": [], "entities": []}, {"text": "We train two sets of word embeddings simultaneously: one is from a lookup table as in traditional Skip-Gram, and another is from convolutional or recurrent neural network.", "labels": [], "entities": []}, {"text": "The former is better at capturing semantic similarity.", "labels": [], "entities": [{"text": "capturing semantic similarity", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.7751042246818542}]}, {"text": "The latter is more focused on morphology and can learn embeddings for OOV words.", "labels": [], "entities": []}, {"text": "We conduct experiments on five tasks, and compare our models with original Skip-Gram and the state-ofthe-art performer FastText.", "labels": [], "entities": [{"text": "FastText", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.9500519037246704}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Results on word similarity and word analogy datasets. For hybrid training scheme, we denote  the embeddings that come from word vector lookup table as \"Model word \", and the embeddings which  come from the composition function as \"Model subword \". We denote the vanilla (non-hybrid) models as  \"Model vanilla \". The \"FastText external \" is the public available FastText embeddings, which are trained  on the full Wikipedia corpus. We also test the version where OOV words are expanded, and denote as  \"Model +OOV \". Model combinations are denoted as gray rows , and best results among them are marked", "labels": [], "entities": [{"text": "word similarity", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.727385938167572}, {"text": "word analogy", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.7329193204641342}]}, {"text": " Table 4: Results on affix prediction (AP) and sequence labeling (SL) tasks. Sequence labeling tasks have  16.5%, 27.1%, 28.5% OOV rate respectively.", "labels": [], "entities": [{"text": "affix prediction (AP)", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8148227453231811}, {"text": "sequence labeling (SL) tasks", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.8094882865746816}, {"text": "Sequence labeling tasks", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.8443604509035746}, {"text": "OOV rate", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9870013892650604}]}, {"text": " Table 5: Results on text classification datasets.", "labels": [], "entities": [{"text": "text classification", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7042393088340759}]}]}