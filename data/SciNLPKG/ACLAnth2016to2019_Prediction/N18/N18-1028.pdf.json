{"title": [{"text": "Variable Typing: Assigning Meaning to Variables in Mathematical Text", "labels": [], "entities": [{"text": "Variable Typing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7594056725502014}]}], "abstractContent": [{"text": "Information about the meaning of mathematical variables in text is useful in NLP/IR tasks such as symbol disambiguation, topic mod-eling and mathematical information retrieval (MIR).", "labels": [], "entities": [{"text": "symbol disambiguation", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.7623830139636993}, {"text": "mathematical information retrieval (MIR)", "start_pos": 141, "end_pos": 181, "type": "TASK", "confidence": 0.7889439562956492}]}, {"text": "We introduce variable typing, the task of assigning one mathematical type (multi-word technical terms referring to mathematical concepts) to each variable in a sentence of mathematical text.", "labels": [], "entities": [{"text": "variable typing, the task of assigning one mathematical type (multi-word technical terms referring to mathematical concepts) to each variable in a sentence of mathematical text", "start_pos": 13, "end_pos": 189, "type": "Description", "confidence": 0.7757422913398061}]}, {"text": "As part of this work, we also introduce anew annotated data set composed of 33,524 data points extracted from scientific documents published on arXiv.", "labels": [], "entities": [{"text": "annotated data set composed of 33,524 data points extracted from scientific documents published on arXiv.", "start_pos": 45, "end_pos": 150, "type": "DATASET", "confidence": 0.7243847362697124}]}, {"text": "Our intrinsic evaluation demonstrates that our data set is sufficient to successfully train and evaluate current classifiers from three different model architectures.", "labels": [], "entities": []}, {"text": "The best performing model is evaluated on an extrinsic task: MIR, by producing a typed formula index.", "labels": [], "entities": [{"text": "MIR", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.7017448544502258}]}, {"text": "Our results show that the best performing MIR models make use of our typed index, compared to a formula index only containing raw symbols, thereby demonstrating the usefulness of variable typing.", "labels": [], "entities": [{"text": "MIR", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9834135174751282}]}], "introductionContent": [{"text": "Scientific documents, such as those from Physics and Computer Science, rely on mathematics to communicate ideas and results.", "labels": [], "entities": []}, {"text": "Written mathematics, unlike general text, follows strong domainspecific conventions governing how content is presented.", "labels": [], "entities": []}, {"text": "According to, the sense of mathematical text is conveyed through the interaction of two contexts: the textual context (flowing text) and the mathematical (or symbolic) context (mathematical formulae).", "labels": [], "entities": []}, {"text": "In this work, we introduce anew task that focuses on one particular interaction: the assignment of meaning to variables by surrounding text in the same sentence . For example, in the sentence the variables P and N in the symbolic context are assigned the meaning \"parabolic subgroup\" and \"unipotent radical\" by the textual context surrounding them respectively.", "labels": [], "entities": []}, {"text": "We will refer to the task of assigning one mathematical type to each variable in a sentence as variable typing.", "labels": [], "entities": []}, {"text": "We use mathematical types as variable denotation labels.", "labels": [], "entities": []}, {"text": "Types are multi-word phrases drawn from the technical terminology of the mathematical discourse that label mathematical objects (e.g., \"set\"), algebraic structures (e.g., \"monoid\") and instantiable notions (e.g., \"cardinality of a set\").", "labels": [], "entities": []}, {"text": "In the sentence presented earlier, the phrases \"parabolic subgroup\", \"Levi decomposition\" and \"unipotent radical\" are examples of types.", "labels": [], "entities": []}, {"text": "Typing variables maybe beneficial to other natural language processing (NLP) tasks, such as topic modeling, to group documents that assign meaning to variables consistently (e.g., \"E\" is \"energy\" consistently in some branches of Physics).", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.7791279554367065}]}, {"text": "In mathematical information retrieval (MIR), for instance, enriching formulae with types may improve precision.", "labels": [], "entities": [{"text": "mathematical information retrieval (MIR)", "start_pos": 3, "end_pos": 43, "type": "TASK", "confidence": 0.7961395333210627}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9964575171470642}]}, {"text": "For example, the formulae x + y and a + b can be considered \u03b1-equivalent matches.", "labels": [], "entities": []}, {"text": "However, if a and bare matrices while x and y are vectors, the match is likely to be a false positive.", "labels": [], "entities": []}, {"text": "Typing information maybe helpful in reducing such instances and improving retrieval precision.", "labels": [], "entities": [{"text": "Typing information", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8713767230510712}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9863945245742798}]}, {"text": "Variable typing differs from similar tasks in three fundamental ways.", "labels": [], "entities": [{"text": "Variable typing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9153488576412201}]}, {"text": "First, meaning -in the form of mathematical types -is explicitly assigned to variables, rather than arbitrary mathematical expressions.", "labels": [], "entities": []}, {"text": "Second, variable typing is carried out at the sentential level, with valid type assignments for variables drawn from the sentences in which they occur, rather than from larger contexts, such as documents.", "labels": [], "entities": [{"text": "variable typing", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7603499591350555}]}, {"text": "Third, denotations are drawn from a pre-determined list of types, rather than from free-form text in the surrounding context of each variable.", "labels": [], "entities": []}, {"text": "As part of our work, we have constructed anew data set for variable typing that is suitable for machine learning (Section 4) and is distributed under the Open Data Commons license.", "labels": [], "entities": [{"text": "Open Data Commons license", "start_pos": 154, "end_pos": 179, "type": "DATASET", "confidence": 0.7665758579969406}]}, {"text": "We propose and evaluate three models for typing variables in mathematical documents based on current machine learning architectures (Section 5).", "labels": [], "entities": []}, {"text": "Our intrinsic evaluation (Section 6) suggests that our models significantly outperform the state-of-theart SVM model by (originally developed for description extraction) on our data set.", "labels": [], "entities": [{"text": "description extraction", "start_pos": 146, "end_pos": 168, "type": "TASK", "confidence": 0.7420860528945923}]}, {"text": "More importantly, our intrinsic evaluation demonstrates that our data set is sufficient to successfully train and evaluate classifiers from three different architectures.", "labels": [], "entities": []}, {"text": "We also demonstrate that our variable typing task and data are useful in MIR in our extrinsic evaluation (Section 7).", "labels": [], "entities": [{"text": "MIR", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9885308742523193}]}], "datasetContent": [{"text": "We compare three models for variable typing to two baselines: the \"nearest type\" baseline and the SVM proposed by.", "labels": [], "entities": []}, {"text": "One of our models is an extension of the latter baseline with both type and variable-centric features.", "labels": [], "entities": []}, {"text": "The other two models are based on deep neural networks: a convolutional neural network and a bidirectional LSTM.", "labels": [], "entities": []}, {"text": "We treat the task of typing as binary classification: every possible typing in a sentence is presented to a classifier which, in turn, is expected to make a \"type\" or \"not-type\" decision.", "labels": [], "entities": []}, {"text": "We say that an edge is positive if it connects a variable to a type in the sentence and negative otherwise.", "labels": [], "entities": []}, {"text": "Evaluation is performed over edges, rather than sentences, in the test set.", "labels": [], "entities": []}, {"text": "We measure performance using precision, recall and F 1 -score.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9997498393058777}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9997478127479553}, {"text": "F 1 -score", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9869778901338577}]}, {"text": "We use the non-parametric paired randomisation test to detect significant differences in performance across classifiers.", "labels": [], "entities": []}, {"text": "The convnet and BiLSTM models are trained and evaluated with as many sentences as there are edges: the source sentence is copied for each input edge, with inputs modified to reflect the relation of interest.", "labels": [], "entities": []}, {"text": "We employed early stopping and dropout to avoid overfitting with these models.", "labels": [], "entities": []}, {"text": "shows the performance results of all classifiers considered.", "labels": [], "entities": []}, {"text": "All three proposed models have significantly outperformed the NT baseline and) state-of-the-art SVM.", "labels": [], "entities": [{"text": "NT baseline", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.8462833166122437}, {"text": "SVM", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.5784478783607483}]}, {"text": "The best performing model is the bidirectional LSTM (F 1 = 78.98%) which has significantly outperformed all other models (\u03b1 = 0.01).", "labels": [], "entities": [{"text": "F 1", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9876929819583893}]}, {"text": "According to the results in, both deep neural network models have significantly outperformed classifiers based on other paradigms.", "labels": [], "entities": []}, {"text": "This is consistent with the intuition that the language of mathematics is formulaic: we expect deep neural networks to effectively recognise patterns and identify correlations between tokens.", "labels": [], "entities": []}, {"text": "The neural models outperform SVM+ despite the fact that the latter is a product of laborious manual feature engineering.", "labels": [], "entities": []}, {"text": "In contrast, no man-  ual feature engineering has been performed on the Convnet model (or indeed on any of the deep neural network models).", "labels": [], "entities": []}, {"text": "The nearest type (NT) baseline demonstrates high recall but low precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.999610960483551}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9981131553649902}]}, {"text": "This is not surprising since the NT baseline is not capable of making a negative decision: it always assigns some type to all variables in a given sentence.", "labels": [], "entities": []}, {"text": "We demonstrate that our data set and variable typing task are useful using a mathematical information retrieval (MIR) experiment.", "labels": [], "entities": [{"text": "mathematical information retrieval (MIR)", "start_pos": 77, "end_pos": 117, "type": "TASK", "confidence": 0.7545981456836065}]}, {"text": "The hypothesis for our MIR experiment is two-fold: (a) types identified in the textual context for the variable typing task are also useful for text-based mathematical retrieval and (b) substituting raw symbols with types in mathematical expressions will have an observable effect to MIR.", "labels": [], "entities": [{"text": "MIR", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9941949248313904}, {"text": "text-based mathematical retrieval", "start_pos": 144, "end_pos": 177, "type": "TASK", "confidence": 0.5780069331328074}, {"text": "MIR", "start_pos": 284, "end_pos": 287, "type": "TASK", "confidence": 0.9881777167320251}]}, {"text": "In order to motivate the second hypothesis, consider the following natural language query: Let x be a vector.", "labels": [], "entities": []}, {"text": "Is there another vector y such that x + y will produce the zero element?", "labels": [], "entities": []}, {"text": "In the context of MIR, mathematical expressions are represented using SLTs) that are constructed by parsing presentation MathML.", "labels": [], "entities": [{"text": "MIR", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9888916015625}, {"text": "parsing presentation MathML", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.6935810446739197}]}, {"text": "The expression \"x + y\" is represented by the SLT in.", "labels": [], "entities": []}, {"text": "The variable typing classifier and the type disambiguation algorithm determine the types of the variables x and y as \"vector\".", "labels": [], "entities": []}, {"text": "Thus, the variable nodes in(a) will be substituted with their type, producing the SLT in.", "labels": [], "entities": []}, {"text": "The example query can be satisfied by identifying a vector y such that when added to x will produce the zero vector.", "labels": [], "entities": []}, {"text": "This operation is abstract in mathematics and extends to objects beyond vectors, including integers.", "labels": [], "entities": []}, {"text": "In an untyped formula index, there is no distinction between instances of x + y where the variables are integers or vectors.", "labels": [], "entities": []}, {"text": "As a result, documents where both variables are integers might also be returned.", "labels": [], "entities": []}, {"text": "In contrast, a typed formula index will return instances of the typed SLT in where the variables are vectors, as opposed to integers.", "labels": [], "entities": []}, {"text": "Therefore, a typed index can reduce the number of false positives and increase precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9985352754592896}]}, {"text": "Four MIR retrieval models are introduced in Section 7.3 designed to control for text indexing/retrieval so that the effects of type-aware vs type-agnostic formula indexing and scoring can be isolated.", "labels": [], "entities": [{"text": "MIR retrieval", "start_pos": 5, "end_pos": 18, "type": "TASK", "confidence": 0.9331350326538086}, {"text": "text indexing/retrieval", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7623214423656464}, {"text": "type-agnostic formula indexing", "start_pos": 141, "end_pos": 171, "type": "TASK", "confidence": 0.5797978341579437}]}, {"text": "These models make use of the Tangent formula indexing and scoring functions), which we have implemented.", "labels": [], "entities": [{"text": "Tangent formula indexing", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.7577380339304606}]}, {"text": "We use the Cambridge University Math IR Test Collection (CUMTC) ( which is composed of 120 research-level mathematical information needs and 160 queries.", "labels": [], "entities": [{"text": "Cambridge University Math IR Test Collection (CUMTC)", "start_pos": 11, "end_pos": 63, "type": "DATASET", "confidence": 0.8786048889160156}]}, {"text": "The CUMTC is ideal for our evaluation for two reasons.", "labels": [], "entities": [{"text": "CUMTC", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.9505171775817871}]}, {"text": "First, topics in the CUMTC are expressed in natural language and are rich in mathematical types.", "labels": [], "entities": [{"text": "CUMTC", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.8610592484474182}]}, {"text": "This allows us to directly apply our best performing variable typing model (BiLSTM) in our retrieval experiment in order to extract variable typings for documents and queries.", "labels": [], "entities": []}, {"text": "Second, the CUMTC uses the MREC as its underlying document collection, which enables downstream evaluation in an optimal setting for variable typing.", "labels": [], "entities": [{"text": "CUMTC", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.9313059449195862}, {"text": "MREC", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.48490655422210693}]}], "tableCaptions": [{"text": " Table 1: Data set statistics.", "labels": [], "entities": []}, {"text": " Table 5: Model performance summary. All figures are  statistically significant (p < 0.01) according to the ran- domisation test.", "labels": [], "entities": []}, {"text": " Table 6: MIR model performance summary.", "labels": [], "entities": [{"text": "MIR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8746213316917419}]}, {"text": " Table 6. The best performing model  is TY/typed which significantly outperforms all  other baselines (p \u2212 value < 0.05 for compari- son with BM25 and p \u2212 value < 0.01 with all  other models). The TY/typed model yields al- most double the MAP performance of its untyped  counterpart (TY/untyped, .083 MAP). In con- trast, the RT/typed and RT/untyped models per- form comparably (no significant difference) but  poorly. This drop in MAP performance suggests  that type phrases are beneficial for text-based re- trieval of mathematics. Retrieval models employ- ing formula indexing seem to be affected by both  the presence of types in the text as well as in the  formula index. The TY/typed model outperforms  the TY/untyped model, which in turn outperforms  RT/untyped. This suggests that gains in retrieval  performance are strongest when types are used in  both text and formula retrieval -models using ei- ther approach alone do not perform as well. These  results demonstrate that variable typing is a valu- able task in MIR.", "labels": [], "entities": [{"text": "BM25", "start_pos": 142, "end_pos": 146, "type": "DATASET", "confidence": 0.7957025170326233}, {"text": "text-based re- trieval of mathematics", "start_pos": 495, "end_pos": 532, "type": "TASK", "confidence": 0.6958961486816406}, {"text": "MIR", "start_pos": 1025, "end_pos": 1028, "type": "TASK", "confidence": 0.9746946096420288}]}]}