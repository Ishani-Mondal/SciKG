{"title": [{"text": "Neural Machine Translation Decoding with Terminology Constraints", "labels": [], "entities": [{"text": "Neural Machine Translation Decoding", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8061675131320953}]}], "abstractContent": [{"text": "Despite the impressive quality improvements yielded by neural machine translation (NMT) systems, controlling their translation output to adhere to user-provided terminology constraints remains an open problem.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.849468727906545}]}, {"text": "We describe our approach to constrained neural decoding based on finite-state machines and multi-stack decoding which supports target-side constraints as well as constraints with corresponding aligned input text spans.", "labels": [], "entities": []}, {"text": "We demonstrate the performance of our framework on multiple translation tasks and motivate the need for constrained decoding with attentions as a means of reducing misplacement and duplication when translating user constraints.", "labels": [], "entities": []}], "introductionContent": [{"text": "Adapting an NMT system with domain-specific data is one way to adjust its output vocabulary to better match the target domain.", "labels": [], "entities": []}, {"text": "Another way to encourage the beam decoder to produce certain words in the output is to explicitly reward n-grams provided by an SMT system ( or language model ( or to modify the vocabulary distribution of the decoder with suggestions from a terminology.", "labels": [], "entities": [{"text": "SMT", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.9624729156494141}]}, {"text": "While providing lexical guidance to the decoder, these methods do not strictly enforce a terminology.", "labels": [], "entities": []}, {"text": "This is a requisite, however, for companies wanting to ensure that brandrelated information is rendered correctly and consistently when translating web content or manuals and is often more important than translation quality alone.", "labels": [], "entities": []}, {"text": "Although domain adaptation and guided decoding can help to reduce errors in these use cases, they do not provide reliable solutions.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7660538852214813}]}, {"text": "Another recent line of work strictly enforces a given set of words in the output ().", "labels": [], "entities": []}, {"text": "Anderson et al. address the task of image captioning with constrained beam search where constraints are given by image tags and constraint permutations are encoded in a finite-state acceptor (FSA).", "labels": [], "entities": [{"text": "image captioning", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.777550995349884}]}, {"text": "Hokamp and Liu propose grid beam search to enforce target-side constraints for domain adaptation via terminology.", "labels": [], "entities": [{"text": "grid beam search", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7519976298014323}, {"text": "domain adaptation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7547627687454224}]}, {"text": "However, since there is no correspondence between constraints and the source words they cover, correct constraint placement is not guaranteed and the corresponding source words maybe translated more than once.", "labels": [], "entities": []}, {"text": "Crego et al. replace entities with special tags that remain unchanged during translation and are replaced in a post-processing step using attention weights.", "labels": [], "entities": []}, {"text": "Given good alignments, this method can translate entities correctly but it requires training data with entity tags and excludes the entities from model scoring.", "labels": [], "entities": []}, {"text": "We address decoding with constraints to produce translations that respect the terminologies of corporate customers while maintaining the high quality of unconstrained translations.", "labels": [], "entities": []}, {"text": "To this end, we apply the constrained beam search of Anderson et al. to machine translation and propose to employ alignment information between target-side constraints and their corresponding source words.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7816173434257507}]}, {"text": "The lack of explicit alignments in NMT systems poses an extra challenge compared to statistical MT where alignments are given by translation rules.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9522147178649902}]}, {"text": "We address the problem of constraint placement by expanding constraints when the NMT model is attending to the correct source span.", "labels": [], "entities": [{"text": "constraint placement", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7498729228973389}]}, {"text": "We also reduce output duplication by masking covered constraints in the NMT attention model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We build attention-based neural machine translation models ( ) using the Blocks implementation of van Merri\u00ebnboer et al.", "labels": [], "entities": [{"text": "attention-based neural machine translation", "start_pos": 9, "end_pos": 51, "type": "TASK", "confidence": 0.6132880449295044}]}, {"text": "(2015) for English-German and English-Chinese translation in both directions.", "labels": [], "entities": []}, {"text": "We combine three models per language pair as ensembles and further combine the NMT systems with n-grams extracted from SMT lattices using Lattice minimum Bayesrisk as described by, referred to as LNMT.", "labels": [], "entities": []}, {"text": "We decode with abeam size of 12 and length normalization  and back off to constrained decoding without attentions when decoding with attentions fails.", "labels": [], "entities": [{"text": "length normalization", "start_pos": 36, "end_pos": 56, "type": "METRIC", "confidence": 0.9638030827045441}]}, {"text": "We report lowercase BLEU using mteval-v13.pl.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9953357577323914}]}], "tableCaptions": [{"text": " Table 1: BLEU scores and dev length ratios for decoding with gold constraints (without attentions) followed  by results for dictionary constraints without (v1) or with (v2) attentions. The column rep shows the number of  character 7-grams that occur more than once within a sentence of the dev set, see Section 4.3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998052716255188}]}, {"text": " Table 3: BLEU scores and speed ratios relative to un- constrained LNMT for production system with up to c  constraints per sentence (newstest2017). A: secondary  attention, B, C: allow 1 or 2 extra tokens, respectively  (Section 2.3). Dict (v2  *  ) refers to decoding with atten- tions but without A, B or C.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987269043922424}, {"text": "newstest2017", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.9376117587089539}]}]}