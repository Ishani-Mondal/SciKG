{"title": [{"text": "A Predictive Model for Notional Anaphora in English", "labels": [], "entities": [{"text": "Notional Anaphora", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8090096414089203}]}], "abstractContent": [{"text": "Notional anaphors are pronouns which disagree with their antecedents' grammatical categories for notional reasons, such as plural to singular agreement in: \"the government ...", "labels": [], "entities": []}, {"text": "Since such cases are rare and conflict with evidence from strictly agreeing cases (\"the government ...", "labels": [], "entities": []}, {"text": "it\"), they present a substantial challenge to both coreference resolution and referring expression generation.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.9637070000171661}, {"text": "referring expression generation", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.768210788567861}]}, {"text": "Using the OntoNotes corpus, this paper takes an ensemble approach to predicting English no-tional anaphora in context on the basis of the largest empirical data to date.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.8920271396636963}, {"text": "predicting English no-tional anaphora in context", "start_pos": 69, "end_pos": 117, "type": "TASK", "confidence": 0.831536332766215}]}, {"text": "In addition to state of the art prediction accuracy, the results suggest that theoretical approaches positing a plural construal at the antecedent's utterance are insufficient, and that circumstances at the anaphor's utterance location, as well as global factors such as genre, have a strong effect on the choice of referring expression.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9443060755729675}]}], "introductionContent": [{"text": "In notional agreement, nouns which ostensibly belong to one agreement category are referred back to using a different category, as in (1), with singular/plural verb and pronoun.", "labels": [], "entities": []}, {"text": "(1)The government] has/have voted and has/ have announced the decision Although examples such as (1) are often taken to represent a single phenomenon, subject-verb (SV) agreement and pronoun number represent distinct agreement phenomena and can disagree in some cases, as shown in (2) and (3), taken from the OntoNotes corpus ().", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 309, "end_pos": 325, "type": "DATASET", "confidence": 0.9315036833286285}]}, {"text": "(2) is my wire service;'re on top of everything.", "labels": [], "entities": []}, {"text": "[One hospital] in Ramallah tells us have treated seven people While previous studies have focused on SV agreement (den, there have been few corpus studies of notional pronouns, due at least in part to the lack of sizable corpora reliably annotated for coreference, and the low accuracy of automatic systems on difficult cases.", "labels": [], "entities": [{"text": "SV agreement", "start_pos": 101, "end_pos": 113, "type": "TASK", "confidence": 0.6581812500953674}, {"text": "coreference", "start_pos": 252, "end_pos": 263, "type": "TASK", "confidence": 0.9590864181518555}, {"text": "accuracy", "start_pos": 277, "end_pos": 285, "type": "METRIC", "confidence": 0.9989868998527527}]}, {"text": "In this paper we take advantage of the OntoNotes corpus, the largest corpus manually annotated for coreference in English (about 1.59 million tokens with coreference annotations), to build a predictive model of the phenomenon, which can be used for both coreference resolution and referring expression generation (see for an overview).", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.837740570306778}, {"text": "coreference resolution", "start_pos": 254, "end_pos": 276, "type": "TASK", "confidence": 0.9447734951972961}, {"text": "referring expression generation", "start_pos": 281, "end_pos": 312, "type": "TASK", "confidence": 0.7420207063357035}]}], "datasetContent": [{"text": "In this paper we focus exclusively on plural pronouns referring back to singular headed phrases, but the exact nature of cases included requires some decisions.", "labels": [], "entities": []}, {"text": "Since the number for second person pronouns (you, your, etc.) is ambiguous, we omit all second person cases.", "labels": [], "entities": []}, {"text": "First person cases are rare but possible, especially in reference to organizations, as in, taken from OntoNotes.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 102, "end_pos": 111, "type": "DATASET", "confidence": 0.9425808787345886}]}], "tableCaptions": [{"text": " Table 2: Confusion matrix for test data classification", "labels": [], "entities": []}, {"text": " Table 3. However we  note that individual genres do behave differently:  data from the Web is closer to spoken language.  The most restrictive genre in avoiding notional  agreement is translations. Both of these facts may  reflect a combination of modality, genre and ed- itorial practice effects. However the strong dif- ferences suggest that genre is likely crucial to any  model attempting to predict this phenomenon.", "labels": [], "entities": []}, {"text": " Table 3: Agreement patterns across genres", "labels": [], "entities": []}, {"text": " Table 4: Agreement by anaphor governor POS", "labels": [], "entities": [{"text": "POS", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.7596622109413147}]}]}