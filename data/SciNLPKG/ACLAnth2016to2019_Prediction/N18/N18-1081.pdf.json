{"title": [], "abstractContent": [{"text": "We present data and methods that enable a supervised learning approach to Open Information Extraction (Open IE).", "labels": [], "entities": [{"text": "Open Information Extraction (Open IE)", "start_pos": 74, "end_pos": 111, "type": "TASK", "confidence": 0.7807387581893376}]}, {"text": "Central to the approach is a novel formulation of Open IE as a sequence tagging problem, addressing challenges such as encoding multiple extractions fora predicate.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.5105634331703186}, {"text": "sequence tagging", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.709341287612915}]}, {"text": "We also develop a bi-LSTM transducer, extending recent deep Semantic Role Labeling models to extract Open IE tuples and provide confidence scores for tuning their precision-recall tradeoff.", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.6332433720429739}]}, {"text": "Furthermore , we show that the recently released Question-Answer Meaning Representation dataset can be automatically converted into an Open IE corpus which significantly increases the amount of available training data.", "labels": [], "entities": [{"text": "Question-Answer Meaning Representation", "start_pos": 49, "end_pos": 87, "type": "TASK", "confidence": 0.7651864091555277}, {"text": "Open IE corpus", "start_pos": 135, "end_pos": 149, "type": "DATASET", "confidence": 0.5727255940437317}]}, {"text": "Our supervised model, made publicly available , 1 outperforms the state-of-the-art in Open IE on benchmark datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see).", "labels": [], "entities": [{"text": "Open Information Extraction (Open IE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7879672901971}]}, {"text": "They have been used fora wide variety of tasks, such as textual entailment), question answering, and knowledge base population (.", "labels": [], "entities": [{"text": "textual entailment)", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7778031826019287}, {"text": "question answering", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.9511121809482574}]}, {"text": "However, perhaps due to limited data, existing methods use semisupervised approaches (, or rule-based algorithms.", "labels": [], "entities": []}, {"text": "In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.7013587653636932}]}, {"text": "Mercury filling, particularly prevalent in the USA, was banned in the EU, partly because it causes antibiotic resistance.", "labels": [], "entities": [{"text": "Mercury filling", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6823529750108719}]}, {"text": "We build on recent work that studies other natural-language driven representations of predicate argument structure, which can be annotated by non-experts.", "labels": [], "entities": [{"text": "predicate argument structure", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.6937092145284017}]}, {"text": "Recently,  created the first labeled corpus for evaluation of Open IE by an automatic translation from question-answer driven semantic role labeling (QA-SRL) annotations).", "labels": [], "entities": [{"text": "Open IE", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.5420375019311905}]}, {"text": "We extend these techniques and apply them to the QAMR corpus (, an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5).", "labels": [], "entities": [{"text": "QAMR corpus", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.8951498568058014}]}, {"text": "The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor.", "labels": [], "entities": []}, {"text": "To train on this data, we formulate Open IE as a sequence labeling problem.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.4974534958600998}, {"text": "sequence labeling", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.5952529609203339}]}, {"text": "We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 150, "end_pos": 172, "type": "TASK", "confidence": 0.6513823767503103}]}, {"text": "We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9995239973068237}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9982549548149109}]}, {"text": "Experiments demonstrate that our approach out-performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independently of our work (.", "labels": [], "entities": []}, {"text": "This shows that for Open IE, careful data curation and model design can push the state of the art using supervised learning.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.5849013030529022}]}], "datasetContent": [{"text": "We evaluate the performance of our model on the four test sets discussed in Section 2.", "labels": [], "entities": []}, {"text": "Metrics We evaluate each system according to three metrics.", "labels": [], "entities": []}, {"text": "First, as is typical for Open IE, we compute a precision-recall (PR) curve by evaluating the systems' performance at different extraction confidence thresholds.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.6426937580108643}, {"text": "precision-recall (PR) curve", "start_pos": 47, "end_pos": 74, "type": "METRIC", "confidence": 0.970432460308075}]}, {"text": "This curve is useful for downstream applications which can set the threshold according to their specific needs (i.e., recall oriented versus precision oriented).", "labels": [], "entities": [{"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9834152460098267}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9425563216209412}]}, {"text": "Second, we compute the area under the PR curve (AUC) as a scalar measurement of the overall system performance.", "labels": [], "entities": [{"text": "PR curve (AUC)", "start_pos": 38, "end_pos": 52, "type": "METRIC", "confidence": 0.8543904662132263}]}, {"text": "Finally, for each system, we report a single F1 score using a confidence threshold optimized on the development set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9830694496631622}]}, {"text": "This can serve as a preset threshold for out-of-the-box use.", "labels": [], "entities": [{"text": "preset threshold", "start_pos": 20, "end_pos": 36, "type": "METRIC", "confidence": 0.9456627070903778}]}, {"text": "Matching function Similar to other cases in NLP, we would like to allow some variability in the predicted tuples.", "labels": [], "entities": [{"text": "Matching", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9708230495452881}]}, {"text": "For example, for the sentence The sheriff standing against the wall spoke in a very soft voice we would want to treat both (The Sheriff; spoke; in a soft voice) and (The sheriff standing against the wall; spoke; in a very soft voice) as acceptable extractions.", "labels": [], "entities": []}, {"text": "To that end, we follow which judge an argument as correct if and only if it includes the syntactic head of the gold argument (and similarly for predicates).", "labels": [], "entities": []}, {"text": "For OIE2016, we use the available Penn Treebank gold syntactic trees, while for the other test sets, we use predicted trees instead.", "labels": [], "entities": [{"text": "OIE2016", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9156761169433594}, {"text": "Penn Treebank gold syntactic trees", "start_pos": 34, "end_pos": 68, "type": "DATASET", "confidence": 0.9769332766532898}]}, {"text": "While this metric may sometimes be too lenient, it does allow a more balanced and fair comparison between systems which can make different, but equally valid, span boundary decisions.", "labels": [], "entities": []}, {"text": "Baselines We compare our model (RnnOIE) against the top-performing systems of those evaluated most recently in  and in: Open IE4,, and PropS ( ).", "labels": [], "entities": [{"text": "RnnOIE", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.8625751733779907}, {"text": "Open IE4", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.8419112861156464}, {"text": "PropS", "start_pos": 135, "end_pos": 140, "type": "DATASET", "confidence": 0.9487344026565552}]}, {"text": "reports the AUC and F1 scores of all of the systems on the 4 test sets.", "labels": [], "entities": [{"text": "AUC", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.7189709544181824}, {"text": "F1 scores", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9583816230297089}]}, {"text": "In addition, the PR curves for the two largest test sets (OIE2016 and WEB) are depicted in Figures 5a and 5b.", "labels": [], "entities": [{"text": "PR", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.6822059750556946}, {"text": "OIE2016", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9403584003448486}, {"text": "WEB", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.49925413727760315}]}, {"text": "We report results for two versions of our model: one trained on the OIE2016 training set containing only verbal predicates, and another on the extended training set that includes the automatic conversion of QAMR outlined in Section 5 (RnnOIE-aw).", "labels": [], "entities": [{"text": "OIE2016 training set", "start_pos": 68, "end_pos": 88, "type": "DATASET", "confidence": 0.9518965482711792}, {"text": "RnnOIE-aw", "start_pos": 235, "end_pos": 244, "type": "METRIC", "confidence": 0.9158377647399902}]}], "tableCaptions": [{"text": " Table 1: Datasets used in this work, follow- ing (Schneider et al., 2017).  *  AW-OIE (All Words  Open IE) was created in the course of this work,  see Section 5 for details.", "labels": [], "entities": [{"text": "AW-OIE", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9814199805259705}]}, {"text": " Table 5: Performance of the OIE extractors on our test sets. Each system is tested in terms of Area Under  the PR Curve (AUC), and F1 (precision and recall in parenthesis).", "labels": [], "entities": [{"text": "OIE extractors", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.6648196280002594}, {"text": "PR Curve (AUC)", "start_pos": 112, "end_pos": 126, "type": "METRIC", "confidence": 0.8510512828826904}, {"text": "F1", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9995823502540588}, {"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.995388388633728}, {"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.963084876537323}]}, {"text": " Table 6: Output statistics of the different systems  on OIE2016, versus the gold data.", "labels": [], "entities": [{"text": "Output", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9919618964195251}, {"text": "OIE2016", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9538799524307251}]}, {"text": " Table 7: Analysis of frequently-occuring recall errors for all tested systems on a random sample of  100 sentences. For each phenomenon we list the percentage of sentences in which it occurs (possibly  overlapping with other phenomena), and a protoypical example, taken from the WEB corpus.", "labels": [], "entities": [{"text": "recall errors", "start_pos": 42, "end_pos": 55, "type": "METRIC", "confidence": 0.8936027586460114}, {"text": "WEB corpus", "start_pos": 280, "end_pos": 290, "type": "DATASET", "confidence": 0.9750139713287354}]}, {"text": " Table 8: Runtime analysis, measured in sentences  per second, of the different systems on 3200  sentences from the OIE2016 corpus on Xeon  2.3GHz CPU (top) and on an NVIDIA GeForce  GTX 1080 Ti (bottom). Baselines were only run  on CPU as they are currently not optimized for  GPU.", "labels": [], "entities": [{"text": "OIE2016 corpus", "start_pos": 116, "end_pos": 130, "type": "DATASET", "confidence": 0.9697887599468231}]}]}