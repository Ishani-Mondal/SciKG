{"title": [{"text": "Using exemplar responses for training and evaluating automated speech scoring systems", "labels": [], "entities": [{"text": "speech scoring", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.7125176340341568}]}], "abstractContent": [{"text": "Automated scoring engines are usually trained and evaluated against human scores and compared to the benchmark of human-human agreement.", "labels": [], "entities": []}, {"text": "In this paper we compare the performance of an automated speech scoring engine using two corpora: a corpus of almost 700,000 randomly sampled spoken responses with scores assigned by one or two raters during operational scoring, and a corpus of 16,500 exemplar responses with scores reviewed by multiple expert raters.", "labels": [], "entities": [{"text": "speech scoring engine", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.7539926469326019}]}, {"text": "We show that the choice of corpus used for model evaluation has a major effect on estimates of system performance with r varying between 0.64 and 0.80.", "labels": [], "entities": []}, {"text": "Surprisingly , this is not the case for the choice of corpus for model training: when the training corpus is sufficiently large, the systems trained on different corpora showed almost identical performance when evaluated on the same corpus.", "labels": [], "entities": []}, {"text": "We show that this effect is consistent across several learning algorithms.", "labels": [], "entities": []}, {"text": "We conclude that evaluating the model on a corpus of exemplar responses if one is available provides additional evidence about system validity; at the same time, investing effort into creating a corpus of exemplar responses for model training is unlikely to lead to a substantial gain in model performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Systems that automatically score constructed responses in an assessment -such as essays or spoken responses -are typically trained and evaluated on a corpus of such test taker responses with scores assigned by trained human raters, considered to be the \"gold standard\" for both training and evaluation of the automated scoring system).", "labels": [], "entities": []}, {"text": "Human raters follow certain agreed-upon scoring guidelines (\"rubrics\") that define the characteristics of a response for each discrete score level of the scoring scale.", "labels": [], "entities": []}, {"text": "For instance, in the case of speech scoring, human raters may evaluate certain aspects of a test taker's speech production, such as fluency, pronunciation, prosody, vocabulary diversity, grammatical accuracy, content correctness, or discourse organization when determining their score fora given spoken response (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.8301632404327393}]}, {"text": "Even as assessment companies try their best to ensure high quality of human scores, human raters do not always agree in the scores they assign to a constructed response.", "labels": [], "entities": []}, {"text": "One reason is related to properties of the responses themselves: the raters use a unidimensional (holistic) scale to score a multidimensional performance.", "labels": [], "entities": []}, {"text": "In this situation different raters may differently weight various aspects of performance resulting in disagreement.", "labels": [], "entities": []}, {"text": "The second reason is related to various imperfections of human raters, e.g., rater fatigue (, differences between novice and experienced raters, and the effect of raters' linguistic background on their evaluation of the language skill being measured.", "labels": [], "entities": []}, {"text": "To guard against such rater inconsistencies, in addition to extensive rater training and monitoring, responses for high-stakes tests are often scored by multiple raters and scores from responses to multiple test questions are used to compute the final score reported to the test taker and other stakeholders, with different responses scored by different raters (.", "labels": [], "entities": []}, {"text": "As a result, the final score remains highly reliable despite variation inhuman agreement at the level of the individual question.", "labels": [], "entities": []}, {"text": "However, since automated scoring engines are usually trained using response-level scores, any inconsistencies in such scores due to the variety of reasons outlined above may negatively affect the system 1 performance.", "labels": [], "entities": []}, {"text": "To monitor rater performance, testing programs sometimes use previously scored responses that are intermixed with the operational responses.", "labels": [], "entities": []}, {"text": "These responses are selected from operational responses to represent exemplar cases of each score level and the scores are further reviewed by multiple raters to ensure their accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9971686005592346}]}, {"text": "In this paper we are examining the effect of using such \"exemplar\" responses for scoring model training and evaluation in the context of automated speech scoring.", "labels": [], "entities": [{"text": "automated speech scoring", "start_pos": 137, "end_pos": 161, "type": "TASK", "confidence": 0.63834015528361}]}, {"text": "In particular, we aim to address the following research questions: 1.", "labels": [], "entities": []}, {"text": "How do automated speech scoring models perform when trained on a corpus with randomly selected responses vs. a corpus with exemplar responses?", "labels": [], "entities": [{"text": "speech scoring", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.6940030604600906}]}, {"text": "2. How is performance affected by the choice of evaluation corpus (random response selection vs. exemplar responses)?", "labels": [], "entities": []}, {"text": "Our initial hypothesis about research question (1) is that if the size and score distribution for the training corpora are comparable, we would expect to seethe scoring model perform better when trained on the exemplar responses since the model is trained on clear-cut examples (less noise in the data).", "labels": [], "entities": []}, {"text": "Similarly, as for research question (2), we hypothesize that when evaluating on clear-cut exemplar responses, scoring model performance should be better than in the default case (random selection) since the machine would likely benefit from the same response properties that also result in more consistent and reliable human scores.", "labels": [], "entities": []}, {"text": "Constructing large corpora of exemplar responses is a very resource-intensive task and therefore little is known about the possible impact of the use of such corpora for training and evaluation of automated scoring models.", "labels": [], "entities": []}, {"text": "Our paper uses a very large corpus of spoken responses and an exemplar corpus constructed by experts over the course of multiple years to address this gap and improve our understanding of the effect of training data on the performance of automated scoring models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a linear mixed-effect model () fitted using the statsmodels Python package) to identify statistically significant differences among the various models.", "labels": [], "entities": []}, {"text": "We used prediction squared error for each response (N =3,124,338) as a dependent variable, response as a random factor, and learner, training set and test set as fixed effects.", "labels": [], "entities": [{"text": "prediction squared error", "start_pos": 8, "end_pos": 32, "type": "METRIC", "confidence": 0.8233108520507812}]}, {"text": "We included both the main effects of training and test set as well as their interaction and used the Linear Regression and MAIN corpus as the reference categories.", "labels": [], "entities": [{"text": "MAIN corpus", "start_pos": 123, "end_pos": 134, "type": "DATASET", "confidence": 0.8819941878318787}]}, {"text": "The average model performance for each model is shown in.", "labels": [], "entities": []}, {"text": "While the model was fitted using squared prediction error, for ease of interpretation and comparison with other studies, we report Pearson's correlation coefficient in the table and in the body of the paper.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 131, "end_pos": 164, "type": "METRIC", "confidence": 0.9306325316429138}]}, {"text": "Corresponding values of root mean squared error (RMSE) are given in the Appendix.", "labels": [], "entities": [{"text": "root mean squared error (RMSE)", "start_pos": 24, "end_pos": 54, "type": "METRIC", "confidence": 0.8271132707595825}, {"text": "Appendix", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9664738178253174}]}, {"text": "Unless stated otherwise, p < .0001 for all effects is reported as significant.", "labels": [], "entities": []}, {"text": "The effect of the choice of learner on model performance was statistically significant but very small.", "labels": [], "entities": []}, {"text": "Most of the more complex models resulted in higher prediction error than OLS linear regression.", "labels": [], "entities": [{"text": "prediction error", "start_pos": 51, "end_pos": 67, "type": "METRIC", "confidence": 0.5918647348880768}, {"text": "OLS linear regression", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.8246141672134399}]}, {"text": "Huber regression (p = 0.007) and MLP regression gave a slight boost in performance.", "labels": [], "entities": []}, {"text": "Random Forest and Linear SVR gave the highest prediction error.", "labels": [], "entities": [{"text": "prediction error", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.9292043149471283}]}, {"text": "In all cases the differences in performance were very small: for RF and SVR the difference between these learners and OLS was 0.03%; in other cases the differences were around 0.01%.", "labels": [], "entities": []}, {"text": "The choice of the evaluation set had the strongest effect on the estimates of model performance.", "labels": [], "entities": []}, {"text": "The best model trained on the MAIN corpus of randomly selected responses achieved r = 0.66 (MLP) when evaluated on the MAIN corpus.", "labels": [], "entities": [{"text": "MAIN corpus", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.8835196495056152}, {"text": "MLP)", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9864050447940826}, {"text": "MAIN corpus", "start_pos": 119, "end_pos": 130, "type": "DATASET", "confidence": 0.9456155002117157}]}, {"text": "This is consistent with other results reported for similar corpora:  cite values between 0.60 and 0.67 depending on the question type and system used.", "labels": [], "entities": []}, {"text": "This model achieved substantially higher performance on the EXEM-PLAR corpus with r = 0.80.", "labels": [], "entities": [{"text": "EXEM-PLAR corpus", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.9291473627090454}]}, {"text": "In other words, the corpus that contained typical responses that could be accurately scored by human raters was also accurately scored by the automated engine.", "labels": [], "entities": []}, {"text": "Disappointingly, we did not see any improvement in performance when the models were trained on the EXEMPLAR corpus: the performance on the MAIN corpus was in fact slightly worse than when the models were trained on the MAIN corpus, with the highest correlation being r = 0.64 (vs. r = 0.66).", "labels": [], "entities": [{"text": "EXEMPLAR corpus", "start_pos": 99, "end_pos": 114, "type": "DATASET", "confidence": 0.9166311025619507}, {"text": "MAIN corpus", "start_pos": 139, "end_pos": 150, "type": "DATASET", "confidence": 0.937946617603302}, {"text": "MAIN corpus", "start_pos": 219, "end_pos": 230, "type": "DATASET", "confidence": 0.9221154451370239}]}, {"text": "The performance of these models was also no better than the performance of the models trained on the same amount of randomly sampled responses (MAIN*).", "labels": [], "entities": [{"text": "MAIN", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9570314288139343}]}, {"text": "As expected, models trained on EXEMPLAR responses reached high agreement when evaluated on EXEMPLAR responses (r = 0.79).", "labels": [], "entities": []}, {"text": "The performance of this model was also better than the performance of the model trained on MAIN*.", "labels": [], "entities": [{"text": "MAIN*", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.9042476117610931}]}, {"text": "That is, training on EXEMPLAR responses gives an advantage over training on the same number of randomly sampled responses when the model is evaluated on EXEMPLAR responses.", "labels": [], "entities": []}, {"text": "However, there was no difference between the model trained on the full training set of the MAIN corpus and the model trained on the EXEMPLAR corpus.", "labels": [], "entities": [{"text": "MAIN corpus", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.9215363264083862}, {"text": "EXEMPLAR corpus", "start_pos": 132, "end_pos": 147, "type": "DATASET", "confidence": 0.876986026763916}]}], "tableCaptions": [{"text": " Table 1: Characteristics of the corpora used in this  study. The table shows the total number of responses  in each partition across all 6 question types and the av- erage number of responses used to train/evaluate the  model for each question type.", "labels": [], "entities": []}, {"text": " Table 2: Average performance (Pearsons's r) across 6 question types from the two corpora in these studies using  different combinations of learners and training sets.", "labels": [], "entities": [{"text": "Pearsons's r)", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9523016065359116}]}, {"text": " Table 3: Corresponding RMSE coefficients for values reported in Table 2.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.582697868347168}]}]}