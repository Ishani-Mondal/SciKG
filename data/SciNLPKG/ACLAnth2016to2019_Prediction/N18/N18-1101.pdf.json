{"title": [{"text": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "labels": [], "entities": [{"text": "Sentence Understanding", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.9522369205951691}]}], "abstractContent": [{"text": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding.", "labels": [], "entities": [{"text": "sentence understanding", "start_pos": 176, "end_pos": 198, "type": "TASK", "confidence": 0.7680520415306091}]}, {"text": "At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty.", "labels": [], "entities": [{"text": "natural language inference", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.6868396004041036}, {"text": "recognizing textual entailment)", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.680316336452961}]}, {"text": "MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation.", "labels": [], "entities": [{"text": "cross-genre domain adaptation", "start_pos": 237, "end_pos": 266, "type": "TASK", "confidence": 0.6955548326174418}]}, {"text": "In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.", "labels": [], "entities": [{"text": "Stanford NLI corpus", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.8826091488202413}]}], "introductionContent": [{"text": "Many of the most actively studied problems in NLP, including question answering, translation, and dialog, depend in large part on natural language understanding (NLU) for success.", "labels": [], "entities": [{"text": "NLP", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9582260847091675}, {"text": "question answering", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.835647314786911}, {"text": "translation", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.9362223148345947}]}, {"text": "While there has been a great deal of work that uses representation learning techniques to pursue progress on these applied NLU problems directly, in order fora representation learning model to fully succeed atone of these problems, it must simultaneously succeed both at NLU, and atone or more additional hard machine learning problems like structured prediction or memory access.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 341, "end_pos": 362, "type": "TASK", "confidence": 0.7341873943805695}]}, {"text": "This makes it difficult to accurately judge the degree to which current models extract reasonable representations of language meaning in these settings.", "labels": [], "entities": []}, {"text": "The task of natural language inference (NLI) is well positioned to serve as a benchmark task for research on NLU.", "labels": [], "entities": [{"text": "natural language inference (NLI)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.7894244889418284}]}, {"text": "In this task, also known as recognizing textual entailment), a model is presented with a pair of sentences-like one of those in-and asked to judge the relationship between their meanings by picking a label from a small set: typically ENTAILMENT, NEUTRAL, and CONTRADICTION.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.8226760427157084}, {"text": "ENTAILMENT", "start_pos": 234, "end_pos": 244, "type": "METRIC", "confidence": 0.834854006767273}, {"text": "NEUTRAL", "start_pos": 246, "end_pos": 253, "type": "METRIC", "confidence": 0.6458202004432678}]}, {"text": "Succeeding at NLI does not require a system to solve any difficult machine learning problems except, crucially, that of extracting effective and thorough representations for the meanings of sentences (i.e., their lexical and compositional semantics).", "labels": [], "entities": []}, {"text": "In particular, a model must handle phenomena like lexical entailment, quantification, coreference, tense, belief, modality, and lexical and syntactic ambiguity.", "labels": [], "entities": []}, {"text": "As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI;) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (, memory, and the use of parse structure (.", "labels": [], "entities": [{"text": "Stanford NLI Corpus (SNLI", "start_pos": 74, "end_pos": 99, "type": "DATASET", "confidence": 0.9142046570777893}, {"text": "sentence understanding", "start_pos": 204, "end_pos": 226, "type": "TASK", "confidence": 0.7262707650661469}]}, {"text": "However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.7450335025787354}]}, {"text": "Met my first girlfriend that way.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Key validation statistics for SNLI (copied  from Bowman et al., 2015) and MultiNLI.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.49863165616989136}, {"text": "MultiNLI", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9407975077629089}]}, {"text": " Table 3: Key statistics for the corpus by genre. The first five genres represent the matched section of the develop- ment and test sets, and the remaining five represent the mismatched section. The first three statistics provide the  number of examples in each genre. #Wds. Prem. is the mean token count among premise sentences. 'S' parses  is the percentage of sentences for which the Stanford Parser produced a parse rooted with an 'S' (sentence) node.", "labels": [], "entities": []}, {"text": " Table 4: Test set accuracies (%) for all models; Match.  represents test set performance on the MultiNLI genres  that are also represented in the training set, Mis. repre- sents test set performance on the remaining ones; Most  freq. is a trivial 'most frequent class' baseline.", "labels": [], "entities": [{"text": "Match", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9977295994758606}, {"text": "freq.", "start_pos": 229, "end_pos": 234, "type": "METRIC", "confidence": 0.9233922958374023}]}]}