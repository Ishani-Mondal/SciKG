{"title": [], "abstractContent": [{"text": "Cross-lingual information retrieval (CLIR) is a document retrieval task where the documents are written in a language different from that of the user's query.", "labels": [], "entities": [{"text": "Cross-lingual information retrieval (CLIR)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8113080461819967}, {"text": "document retrieval task", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.7736385762691498}]}, {"text": "This is a challenging problem for data-driven approaches due to the general lack of labeled training data.", "labels": [], "entities": []}, {"text": "We introduce a large-scale dataset derived from Wikipedia to support CLIR research in 25 languages.", "labels": [], "entities": []}, {"text": "Further, we present a simple yet effective neural learning-to-rank model that shares representations across languages and reduces the data requirement.", "labels": [], "entities": []}, {"text": "This model can exploit training data in, for example , Japanese-English CLIR to improve the results of Swahili-English CLIR.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multilingual document collections are becoming prevalent.", "labels": [], "entities": [{"text": "Multilingual document collections", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6408397157986959}]}, {"text": "Thus an important application is crosslingual information retrieval (CLIR), i.e. document retrieval which assumes that the language of the user's query does not match that of the documents.", "labels": [], "entities": [{"text": "crosslingual information retrieval (CLIR)", "start_pos": 33, "end_pos": 74, "type": "TASK", "confidence": 0.7584080199400584}, {"text": "document retrieval", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7030858248472214}]}, {"text": "For example, imagine an investor who wishes to monitor consumer sentiment of an international brand in Twitter conversations around the world.", "labels": [], "entities": []}, {"text": "She might issue a query string in English, and desire all relevant tweets in any language.", "labels": [], "entities": []}, {"text": "There are two main approaches to building CLIR systems.", "labels": [], "entities": []}, {"text": "The modular approach involves a pipeline of two components: translation (machine translation or bilingual dictionary look-up) and monolingual information retrieval (IR).", "labels": [], "entities": [{"text": "translation (machine translation or bilingual dictionary look-up", "start_pos": 60, "end_pos": 124, "type": "TASK", "confidence": 0.7537925578653812}, {"text": "monolingual information retrieval (IR)", "start_pos": 130, "end_pos": 168, "type": "TASK", "confidence": 0.7419415016969045}]}, {"text": "These approaches maybe further divided into the document translation and query translation approaches.", "labels": [], "entities": [{"text": "document translation", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7806928753852844}, {"text": "query translation", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.70914526283741}]}, {"text": "In the former, one translates all foreignlanguage documents to the language of the user query prior to IR indexing; in the latter, one indexes foreign-language documents and translates the query.", "labels": [], "entities": [{"text": "IR indexing", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.8621901869773865}]}, {"text": "In both, the idea is to solve the translation problem separately, so that CLIR becomes document retrieval in the monolingual setting.", "labels": [], "entities": []}, {"text": "A distinctly different way to build CLIR systems is what maybe called the direct modeling approach (.", "labels": [], "entities": []}, {"text": "This assumes the availability of CLIR training examples of the form, where q is an English query, dis a foreign-language document, a r is the corresponding relevance judgment ford with respect to q.", "labels": [], "entities": []}, {"text": "One directly builds a retrieval model S(q, d) that scores the querydocument pair.", "labels": [], "entities": []}, {"text": "While q and dare in different languages, the model directly learns both translation and retrieval relevance on the CLIR training data.", "labels": [], "entities": [{"text": "CLIR training data", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.8425607085227966}]}, {"text": "Compared to the modular approach, direct modeling is advantageous in that it focuses on learning translations that are beneficial for retrieval, rather than translations that preserve sentence meaning/structure in bitext.", "labels": [], "entities": [{"text": "direct modeling", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.6954428404569626}]}, {"text": "However, there exist no large-scale CLIR dataset that can support direct modeling approaches in a wide variety of languages.", "labels": [], "entities": [{"text": "CLIR dataset", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.7613719403743744}]}, {"text": "To obtain relevance judgments, one typically needs a bilingual speaker who can read a foreign-language document and assess whether it is relevant fora given English query.", "labels": [], "entities": []}, {"text": "This can bean expensive process.", "labels": [], "entities": []}, {"text": "Here, we present a large-scale dataset that is automatically constructed from Wikipedia: it can support training and evaluation of CLIR systems between English queries and documents in 25 other languages (Section 2).", "labels": [], "entities": []}, {"text": "The data is of sufficient size for direct modeling, and can also serve as an wide-coverage evaluation data for the modular approaches.", "labels": [], "entities": []}, {"text": "To demonstrate the utility of the data, we further present experiments for CLIR in low-resource languages.", "labels": [], "entities": []}, {"text": "First, we introduce a neural CLIR model based on the direct modeling approach (Section: CLIR data construction process: From an English article (E1), we extract the English query.", "labels": [], "entities": []}, {"text": "Using the inter-language link, we obtain the most relevant foreign-language document (F1).", "labels": [], "entities": [{"text": "F1", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9898772835731506}]}, {"text": "Any article that has mutual links to and from F1 are labeled as slightly relevant (F2).", "labels": [], "entities": [{"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9662700891494751}, {"text": "F2)", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.939178854227066}]}, {"text": "All other articles are not relevant (F3).", "labels": [], "entities": [{"text": "F3", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.8546866178512573}]}, {"text": "The data is a set of tuples: (English query q, foreign document d, relevance judgment r), where r \u2208 {0, 1, 2} represents the three levels of relevance.", "labels": [], "entities": []}, {"text": "We then show how we can bootstrap CLIR models for languages with less training data by an appropriate use of paramater sharing among different language pairs (Section 3.2).", "labels": [], "entities": []}, {"text": "For example, using the training data for Japanese-English CLIR, we can improve the Mean Average Precision (MAP) results of a Swahili-English CLIR system by 5-7 points (Section 4).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 83, "end_pos": 111, "type": "METRIC", "confidence": 0.969834973414739}]}], "datasetContent": [{"text": "We construct a large-scale CLIR data from Wikipedia.", "labels": [], "entities": [{"text": "CLIR data from Wikipedia", "start_pos": 27, "end_pos": 51, "type": "DATASET", "confidence": 0.7620229348540306}]}, {"text": "The idea is to exploit inter-language links: from an English page, we extract a sentence as query, and label the linked foreign-document pages as relevant.", "labels": [], "entities": []}, {"text": "This data construction process is similar to () who made an EnglishGerman CLIR dataset, but ours is at a larger scale.", "labels": [], "entities": [{"text": "EnglishGerman CLIR dataset", "start_pos": 60, "end_pos": 86, "type": "DATASET", "confidence": 0.9464805523554484}]}, {"text": "Specifically, we use Wikipedia dumps released on August 23, 2017.", "labels": [], "entities": [{"text": "Wikipedia dumps released on August 23", "start_pos": 21, "end_pos": 58, "type": "DATASET", "confidence": 0.9058003028233846}]}, {"text": "English queries are obtained by extracting the first sentence of every English Wikipedia article.", "labels": [], "entities": []}, {"text": "The intuition is that the first sentence is usually a well-defined summary of its corresponding article and should be thematically related for articles linked to it from another language.", "labels": [], "entities": []}, {"text": "Similar to (), title words from the query sentences are removed, because they maybe present across different language editions.", "labels": [], "entities": []}, {"text": "This deletion prevents the task from becoming an easy keyword matching task.", "labels": [], "entities": [{"text": "keyword matching task", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.8073476751645406}]}, {"text": "For practical purposes, each document is limited to the first 200 words of the article.", "labels": [], "entities": []}, {"text": "Empty documents and category pages are filtered.", "labels": [], "entities": []}, {"text": "Currently, our dataset consists of more than 2.8 mil-: CLIR dataset statistics.", "labels": [], "entities": [{"text": "CLIR dataset statistics", "start_pos": 55, "end_pos": 78, "type": "DATASET", "confidence": 0.9216263890266418}]}, {"text": "For each language X, we show the total number of documents in language X and the number of English queries.", "labels": [], "entities": []}, {"text": "The number of \"most relevant\" documents is by definition equal to #Query.", "labels": [], "entities": []}, {"text": "The number of \"slightly relevant\" documents is shown in the column #SR.", "labels": [], "entities": [{"text": "SR", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9397138953208923}]}, {"text": "lion English queries and relevant documents from 25 other selected languages (see).", "labels": [], "entities": []}, {"text": "In sum, we have created a CLIR dataset that is large-scale in terms of both the amount of examples as well as the number of languages.", "labels": [], "entities": [{"text": "CLIR dataset", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.8498042821884155}]}, {"text": "This can be used in two scenarios: (1) one mixed-language collection where an English query may retrieve relevant documents in multiple languages.", "labels": [], "entities": []}, {"text": "(2) 25 independent datasets for training and evaluating CLIR on English queries against one foreign language collection.", "labels": [], "entities": []}, {"text": "In the experiments in Section 4, we will utilize the dataset in terms of scenario (2).", "labels": [], "entities": []}, {"text": "3 Direct Modeling for CLIR  Setup: We use datasets of 3 high-resource languages (Japanese [Ja], German, French) and 2 low-resource languages (Tagalog, Swahili).", "labels": [], "entities": []}, {"text": "We also subsample German and French data to be equivalent to the size of Swahili, in order to compare training size effects.", "labels": [], "entities": []}, {"text": "Word embedding with dimension 100 for each language is trained on Wikipedia corpus, using word2vec SGNS (.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 66, "end_pos": 82, "type": "DATASET", "confidence": 0.9692921340465546}]}, {"text": "The size of hidden states in the deep model is {100, 200, 300, 400, 500}.", "labels": [], "entities": []}, {"text": "We adopt Adam () for optimization, train for 20 epochs and pick the best epoch based on development set loss.", "labels": [], "entities": []}, {"text": "For the proposed method of parameter sharing, we use the weight parameters pre-trained on JapaneseEnglish dataset to initialize parameters.", "labels": [], "entities": [{"text": "parameter sharing", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7061873823404312}, {"text": "JapaneseEnglish dataset", "start_pos": 90, "end_pos": 113, "type": "DATASET", "confidence": 0.9930037558078766}]}, {"text": "High-resource results: shows the P@1 (precision at top position) and MAP (mean average precision) for datasets consisting of on the order of 100k+ training queries.", "labels": [], "entities": [{"text": "P@1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9753691554069519}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.8000792860984802}, {"text": "MAP (mean average precision)", "start_pos": 69, "end_pos": 97, "type": "METRIC", "confidence": 0.8272095620632172}]}, {"text": "The deep models outperformed the cosine models under all conditions, suggesting that the fully connected layer can exploit the large training set in learning more expressive scoring functions.", "labels": [], "entities": []}, {"text": "Low-resource results: shows the results on the low resource datasets under two conditions: training on only the language-pair of interest (inlanguage), or additionally sharing parameters using a pre-trained Japanese-English model.", "labels": [], "entities": []}, {"text": "For the in-language case, we observe the cosine model outperforms the deep model.", "labels": [], "entities": []}, {"text": "In contrast to the high-resource results, this implies that deep models, which have a lot of parameters, only become effective if provided with sufficient training data.", "labels": [], "entities": []}, {"text": "For the sharing case, the deep models with parameter sharing outperformed the basic deep models trained only on in-language data under almost all conditions.", "labels": [], "entities": []}, {"text": "This indicates that our sharing method reduces training data requirement.", "labels": [], "entities": []}, {"text": "Importantly, by sharing parameters, the deep models are now able to outperform the cosine model and achieve the best results on all datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: CLIR dataset statistics. For each language X,  we show the total number of documents in language  X and the number of English queries. The number  of \"most relevant\" documents is by definition equal to  #Query. The number of \"slightly relevant\" documents  is shown in the column #SR.", "labels": [], "entities": [{"text": "CLIR dataset", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.7548180520534515}, {"text": "SR", "start_pos": 290, "end_pos": 292, "type": "METRIC", "confidence": 0.9659290313720703}]}, {"text": " Table 2: P@1/MAP performance (0-100 range, in  percent) of the cosine model and the deep model with  different hidden state size on high resource datasets.  Best value in each column is highlighted in bold.", "labels": [], "entities": [{"text": "P", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9608593583106995}, {"text": "MAP", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9437114000320435}]}]}