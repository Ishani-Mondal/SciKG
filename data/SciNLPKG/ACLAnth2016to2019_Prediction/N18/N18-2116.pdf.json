{"title": [{"text": "Smaller Text Classifiers with Discriminative Cluster Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embedding parameters often dominate overall model sizes in neural methods for natural language processing.", "labels": [], "entities": []}, {"text": "We reduce deployed model sizes of text clas-sifiers by learning a hard word clustering in an end-to-end manner.", "labels": [], "entities": []}, {"text": "We use the Gumbel-Softmax distribution to maximize over the latent clustering while minimizing the task loss.", "labels": [], "entities": []}, {"text": "We propose variations that selectively assign additional parameters to words, which further improves accuracy while still remaining parameter-efficient.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9984468817710876}]}], "introductionContent": [{"text": "Word embeddings () form the foundation of most neural methods for natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.8146789073944092}]}, {"text": "However, embeddings typically comprise a large fraction of the total parameters learned by a model, especially when large vocabularies and high dimensions are used.", "labels": [], "entities": []}, {"text": "This can become problematic when seeking to deploy NLP systems on mobile devices where memory and computation time are limited.", "labels": [], "entities": []}, {"text": "We address this issue by proposing alternative parameterizations for word embeddings in text classifiers.", "labels": [], "entities": []}, {"text": "We introduce a latent variable for each word type that represents the (hard) cluster to which it belongs.", "labels": [], "entities": []}, {"text": "An embedding is learned for each cluster.", "labels": [], "entities": []}, {"text": "All parameters (including cluster assignment probabilities for each word and the cluster embeddings themselves) are learned jointly in an end-to-end manner.", "labels": [], "entities": []}, {"text": "This idea is based on the conjecture that most words do not need their own unique embedding parameters, due both to the focused nature of particular text classification tasks and also due to the power law characteristics of word frequen-.", "labels": [], "entities": [{"text": "text classification", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.7493581175804138}]}, {"text": ". is a group of symptoms . .", "labels": [], "entities": []}, {"text": "For a particular task, many word embeddings would be essentially identical, so using clusters lets us avoid learning redundant embedding vectors, making parameter usage more efficient.", "labels": [], "entities": []}, {"text": "For sentiment analysis, for example, the procedure can learn to place all sentiment-neutral words in a single cluster, and then learn distinct clusters for positive and negative words.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9747810065746307}]}, {"text": "During learning, we minimize log loss of the correct classification label while maximizing over the latent variables.", "labels": [], "entities": []}, {"text": "To do so, we use the GumbelSoftmax distribution) as a continuous approximation to hard clustering.", "labels": [], "entities": []}, {"text": "After training, we compute the argmax over cluster assignments for each word type and replace the cluster assignment probabilities with pointers to clusters; see.", "labels": [], "entities": []}, {"text": "This leads to a large reduction in model size attest time.", "labels": [], "entities": [{"text": "size attest time", "start_pos": 41, "end_pos": 57, "type": "METRIC", "confidence": 0.8835045496622721}]}, {"text": "We consider two variations of the above idea which introduce a small number of additional word-specific parameters.", "labels": [], "entities": []}, {"text": "The best variation learns unique embeddings for only the most frequent words and uses hard clustering for the rest.", "labels": [], "entities": []}, {"text": "We evaluate our methods on five text classification datasets, comparing them at several model size budgets.", "labels": [], "entities": [{"text": "text classification", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7139961421489716}]}, {"text": "Our results demonstrate that clustering can maintain or improve performance while offering extremely small deployed models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Test results. Model sizes are in MB.", "labels": [], "entities": [{"text": "MB", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9815748929977417}]}, {"text": " Table 2. The test re- sults are reported based on model performance on  the development set for different model sizes. The  models are consistent between development and  test, as our cluster models with max size 0.05MB  outperform SE across datasets, with ME having  the highest accuracies.", "labels": [], "entities": [{"text": "SE", "start_pos": 233, "end_pos": 235, "type": "METRIC", "confidence": 0.9576221108436584}, {"text": "ME", "start_pos": 258, "end_pos": 260, "type": "METRIC", "confidence": 0.9921659231185913}]}, {"text": " Table 3. Compared with  compositional coding, our models perform much  better with a much smaller set of embedding pa- rameters even when we use a smaller number of  cluster embeddings (e.g., compare 8 \u00d7 8 coding", "labels": [], "entities": []}, {"text": " Table 3: IMDB test results. The four rows above the  dashed line are from Shu and Nakayama (2018); our  results are below it.", "labels": [], "entities": [{"text": "IMDB test results", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.8808517058690389}]}, {"text": " Table 5: IMDB test results for RNN with different em- bedding dimensions.", "labels": [], "entities": [{"text": "IMDB test", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.6528055965900421}]}]}