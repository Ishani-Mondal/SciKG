{"title": [{"text": "Tuning Bayes Baseline for dialect detection", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes an analysis of our submissions to the Dialect Detection Shared Task 2016.", "labels": [], "entities": [{"text": "Dialect Detection Shared Task 2016", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.8828338265419007}]}, {"text": "We proposed three different systems that involved simplistic features, to name: a Naive-bayes system, a Support Vector Machines-based system and a Tree Kernel-based system.", "labels": [], "entities": []}, {"text": "These systems underperform when compared to other submissions in this shared task, since the best one achieved an accuracy of \u223c0.834.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9994779229164124}]}], "introductionContent": [{"text": "The problem of discriminating similar languages has been tackled in previous years in the context of shared tasks ().", "labels": [], "entities": []}, {"text": "Here, there are promising results for dialect detection, being the best results around 95.54% and for an open challenge, the best results yield around 95.65%.", "labels": [], "entities": [{"text": "dialect detection", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.9009558260440826}]}, {"text": "Despite these positive results, some research issues remain to be solved such as: domain adaptation, inclusion of new languages and classifier performance in terms of processing time.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7822248041629791}]}, {"text": "The DSL 2014 Shared Task aimed to discriminate dialects within each of these 6 groups: Group A (Bosnian, Croatian, Serbian), Group B (Indonesian, Malay), Group C (Czech, Slovak), Group D (Brazilian Portuguese, European Portuguese), Group E (Castilian Spanish, Argentine Spanish), and Group F (American English, British English).", "labels": [], "entities": [{"text": "DSL 2014 Shared Task", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8534760922193527}]}, {"text": "The 2015 version of this Shared task considered the first 5 groups plus an additional group comprising Bulgarian and Macedonian.", "labels": [], "entities": []}, {"text": "where our systems were competing differs to previous tasks in the addition of anew variety of Spanish language: Mexican Spanish.", "labels": [], "entities": []}, {"text": "Additionally, a second task aims to test dialect identification systems in Arabic language datasets.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7584587633609772}]}, {"text": "Our submissions mainly addressed the sub-task 1 for automatically discriminating between similar languages and language varieties.", "labels": [], "entities": []}, {"text": "We took into account two principles: a) to design lightweight systems given that such a system should work in an online environment, and b) to design systems that would involve using grammatical information without the recurring to sophisticated parsers.", "labels": [], "entities": []}, {"text": "This paper is structured as follows: Section 2 provides a brief context for our work in the state of the art of language detection.", "labels": [], "entities": [{"text": "language detection", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.7943675220012665}]}, {"text": "Section 3 describes the core of our experiments.", "labels": [], "entities": []}, {"text": "Section 4 outlines our results and an analysis of the relevance of the proposed methods.", "labels": [], "entities": []}, {"text": "Finally, we conclude with Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training datasets provided by the shared task organizers were created based on text from newspapers articles.", "labels": [], "entities": []}, {"text": "One in-domain test set and also two out of domain twitter base data sets were made available for testing purposes; these two twitter data sets were collected in a different manner to the news dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. They correspond to two of the systems  described in Section 3. This table shows the accuracy, micro and macro and weighted F1 for each of the  submitted classifications. Test set A is the in-domain composed by text from newspaper articles. Test  datasets B1 and B2 are composed by text extracted from twitter microposts, in two different ways. Test  set C is composed by Arabic text extracted by Automatic Speech Recognition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9996416568756104}, {"text": "F1", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.9871068596839905}, {"text": "Automatic Speech Recognition", "start_pos": 406, "end_pos": 434, "type": "TASK", "confidence": 0.5774854520956675}]}, {"text": " Table 2: Results for all runs (for the closed track)", "labels": [], "entities": []}, {"text": " Table 3: Language confusion matrix", "labels": [], "entities": [{"text": "Language confusion", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8193996250629425}]}, {"text": " Table 4: Confusion matrix results", "labels": [], "entities": [{"text": "Confusion matrix", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9129607379436493}]}, {"text": " Table 5: Precision, Recall and F1 of each dialect, in domain test set (in percentages)", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9973515272140503}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9976660013198853}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9968120455741882}]}, {"text": " Table 6: Simplified confusion matrix for the twitter test datasets B1 and B2.", "labels": [], "entities": [{"text": "Simplified confusion matrix", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.637486070394516}, {"text": "twitter test datasets B1", "start_pos": 46, "end_pos": 70, "type": "DATASET", "confidence": 0.8589012175798416}]}]}