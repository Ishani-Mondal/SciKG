{"title": [{"text": "A Study of Imitation Learning Methods for Semantic Role Labeling", "labels": [], "entities": [{"text": "Imitation Learning", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.9252515137195587}, {"text": "Semantic Role Labeling", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7155135869979858}]}], "abstractContent": [{"text": "Global features have proven effective in a wide range of structured prediction problems but come with high inference costs.", "labels": [], "entities": []}, {"text": "Imitation learning is a common method for training models when exact inference isn't feasible.", "labels": [], "entities": [{"text": "Imitation learning", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9743222594261169}]}, {"text": "We study imitation learning for Semantic Role Labeling (SRL) and analyze the effectiveness of the Violation Fixing Perceptron (VFP) (Huang et al., 2012) and Locally Optimal Learning to Search (LOLS) (Chang et al., 2015) frameworks with respect to SRL global features.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.8023335138956705}, {"text": "SRL global", "start_pos": 247, "end_pos": 257, "type": "TASK", "confidence": 0.9101675748825073}]}, {"text": "We describe problems in applying each framework to SRL and evaluate the effectiveness of some solutions.", "labels": [], "entities": [{"text": "SRL", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9690306782722473}]}, {"text": "We also show that action ordering, including easy first inference , has a large impact on the quality of greedy global models.", "labels": [], "entities": [{"text": "action ordering", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.751781553030014}]}], "introductionContent": [{"text": "In structured prediction problems, global features express dependencies between related pieces of a label and make inference non-trivial.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.6756522357463837}]}, {"text": "In Semantic Role Labeling (SRL) (), global features and constraints have been studied extensively () inter alia.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.8311082820097605}]}, {"text": "SRL has many phenomenon that relate labels such as syntactic control, role mutual exclusion, and structural constraints like span overlap.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8537191152572632}, {"text": "role mutual exclusion", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.7184696396191915}]}, {"text": "Previous work on inference for models with global features has studied a variety of method including dynamic programming, reranking, and ILP solvers.", "labels": [], "entities": [{"text": "ILP solvers", "start_pos": 137, "end_pos": 148, "type": "TASK", "confidence": 0.7099948823451996}]}, {"text": "Greedy search and beam search are relatively understudied areas due to the difficulty in training models which perform well with the weak guarantees provided by greedy search.", "labels": [], "entities": [{"text": "beam search", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9266723990440369}]}, {"text": "The Violation Fixing Perceptron (VFP) framework) is a notable exception which has been used to great effect in a range of structured problems.", "labels": [], "entities": [{"text": "Violation Fixing Perceptron (VFP)", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.8866365253925323}]}, {"text": "Learning to Search (L2S)) is another line of work for training greedy models with no assumptions about features.", "labels": [], "entities": []}, {"text": "These training methods are appealing because they decouple the definition of (global) features from the (exact) inference and training procedures.", "labels": [], "entities": []}, {"text": "This allows easier specification of models (features not algorithms) and the ability to use inference methods which scale with the difficulty of the problem rather than the type of features used.", "labels": [], "entities": []}, {"text": "In this work, we study VFP and L2S methods for training greedy global SRL models.", "labels": [], "entities": [{"text": "SRL", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9238249659538269}]}, {"text": "We find that both methods are far from ideal.", "labels": [], "entities": []}, {"text": "VFP is inconsistent and often doesn't perform better than unstructured perceptron training.", "labels": [], "entities": [{"text": "VFP", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.4956897497177124}]}, {"text": "L2S leads to models which under-predict arguments and do not perform as well as pipeline training.", "labels": [], "entities": []}, {"text": "We describe the causes of these problems and offer some solutions.", "labels": [], "entities": []}, {"text": "Finally, we study the effect of the transition system on the usefulness of global features.", "labels": [], "entities": []}, {"text": "We find that the order that actions are performed in can be as important as the training method, leading to better models with the same features and computational complexity.", "labels": [], "entities": []}], "datasetContent": [{"text": "We measure performance on two data sets, the Propbank annotations) available in the Ontonotes 5.0 corpus (Pradhan et al., 2012) and FrameNet 1.5 (.", "labels": [], "entities": [{"text": "Ontonotes 5.0 corpus", "start_pos": 84, "end_pos": 104, "type": "DATASET", "confidence": 0.9384304086367289}]}, {"text": "For all learning methods we average the weights across all iterations of training).", "labels": [], "entities": []}, {"text": "This is explicitly called for as apart of LOLS and is also a standard trick used with the structured perceptron.", "labels": [], "entities": [{"text": "LOLS", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9558616280555725}]}, {"text": "We use the local features described in for argument and frame identification, but we did not use their feature embedding method since it performed about as well as the sparse feature method and was slower.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.7162014693021774}]}, {"text": "We use the best refinements using the process described in \u00a73.", "labels": [], "entities": []}, {"text": "We are studying the fully greedy case of inference in this work (i.e. abeam size of 1).", "labels": [], "entities": []}, {"text": "As far as we know, efficient greedy and easy first inference are mutually exclusive goals, and we focus on the latter.", "labels": [], "entities": []}, {"text": "Our implementation uses a heap to store actions in a manner similar to.", "labels": [], "entities": []}, {"text": "This way actions can be generated once, instead of once per transition, and global features perform sparse updates to the actions on the heap.", "labels": [], "entities": []}, {"text": "For beam search, states cannot share a heap (since their histories, and thus global features, would be different), so actions generation, global features, and action sorting would have to occur at every transition.", "labels": [], "entities": [{"text": "beam search", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8271971344947815}, {"text": "actions generation", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7476804256439209}, {"text": "action sorting", "start_pos": 159, "end_pos": 173, "type": "TASK", "confidence": 0.7125872373580933}]}, {"text": "All performance values shown here are measured for the task of frame semantic parsing (FSP), meaning that we measure precision, recall, and F-measure where every index inf and k are considered predictions.", "labels": [], "entities": [{"text": "frame semantic parsing (FSP)", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.7936772008736929}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9994014501571655}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9993659853935242}, {"text": "F-measure", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9966356158256531}]}, {"text": "Predictions ink are not correct unless the frame that they correspond to are also correct.", "labels": [], "entities": [{"text": "Predictions", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9403858184814453}]}, {"text": "We show two scenarios: gold f refers to the case where the frame labels are given and auto f refers to when they are predicted by the model.", "labels": [], "entities": [{"text": "auto f", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9407130479812622}]}, {"text": "All figures and plots are on dev set performance.", "labels": [], "entities": []}, {"text": "Unless specified, we set the loop order over roles by how frequently they occur in the dev data, which will be described as freq in \u00a77.", "labels": [], "entities": []}], "tableCaptions": []}