{"title": [{"text": "Multilingual Code-switching Identification via LSTM Recurrent Neural Networks", "labels": [], "entities": [{"text": "Multilingual Code-switching Identification", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.540713389714559}]}], "abstractContent": [{"text": "This paper describes the HHU-UH-G system submitted to the EMNLP 2016 Second Workshop on Computational Approaches to Code Switching.", "labels": [], "entities": [{"text": "EMNLP 2016 Second Workshop on Computational Approaches to Code Switching", "start_pos": 58, "end_pos": 130, "type": "TASK", "confidence": 0.6479986757040024}]}, {"text": "Our system ranked first place for Arabic (MSA-Egyptian) with an F1-score of 0.83 and second place for Spanish-English with an F1-score of 0.90.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9994773268699646}, {"text": "F1-score", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9983567595481873}]}, {"text": "The HHU-UH-G system introduces a novel unified neural network architecture for language identification in code-switched tweets for both Spanish-English and MSA-Egyptian dialect.", "labels": [], "entities": [{"text": "language identification", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.7240020483732224}]}, {"text": "The system makes use of word and character level representations to identify code-switching.", "labels": [], "entities": []}, {"text": "For the MSA-Egyptian dialect the system does not rely on any kind of language-specific knowledge or linguistic resources such as, Part Of Speech (POS) taggers, morphological analyz-ers, gazetteers or word lists to obtain state-of-the-art performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Code-switching can be defined as the act of alternating between elements of two or more languages or language varieties within the same utterance.", "labels": [], "entities": []}, {"text": "The main language is sometimes referred to as the 'host language', and the embedded language as the 'guest language' (.", "labels": [], "entities": []}, {"text": "Code-switching is a wide-spread linguistic phenomenon in modern informal user-generated data, whether spoken or written.", "labels": [], "entities": []}, {"text": "With the advent of social media, such as Facebook posts, Twitter tweets, SMS messages, user comments on the articles, blogs, etc., this phenomenon is becoming more pervasive.", "labels": [], "entities": []}, {"text": "Code-switching does not only occur across sentences (inter-sentential) but also within the same sentence (intra-sentential), adding a substantial complexity dimension to the automatic processing of natural languages.", "labels": [], "entities": []}, {"text": "This phenomenon is particularly dominant in multilingual societies, migrant communities (), and in other environments due to social changes through education and globalization.", "labels": [], "entities": []}, {"text": "There are also some social, pragmatic and linguistic motivations for code-switching, such as the the intent to express group solidarity, establish authority (), lend credibility, or makeup for lexical gaps.", "labels": [], "entities": []}, {"text": "It is not necessary for code-switching to occur only between two different languages like Spanish-English (, MandarinTaiwanese ( and Turkish-German (\u00d6zlem \u00c7etinoglu, 2016), but it can also happen between three languages, e.g. Bengali, English and Hindi (, and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German).", "labels": [], "entities": []}, {"text": "Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (, or MSA and Moroccan Arabic ().", "labels": [], "entities": []}, {"text": "The current shared task is limited to two scenarios: a) codeswitching between two distinct languages: SpanishEnglish, b) and two language varieties: MSAEgyptian Dialect.", "labels": [], "entities": []}, {"text": "With the massive increase in code-switched writings in user-generated content, it has become imperative to develop tools and methods to handle and process this type of data.", "labels": [], "entities": []}, {"text": "Identification of languages used in the sentence is the first step in doing any kind of text analysis.", "labels": [], "entities": [{"text": "Identification of languages used in the sentence", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8650238684245518}, {"text": "text analysis", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.7029609531164169}]}, {"text": "For example, most data found in social media produced by bilingual people is a mixture of two languages.", "labels": [], "entities": []}, {"text": "In order to processor translate this data to some other language, the first step will be to detect text chunks and identify which language each chunk belongs to.", "labels": [], "entities": []}, {"text": "The other categories like named entities, mixed, ambiguous and other are also important for further language processing.", "labels": [], "entities": []}], "datasetContent": [{"text": "The shared task organizers made available the tagged dataset for Spanish-English and Arabic (MSA-Egyptian) code-switched language pairs.", "labels": [], "entities": []}, {"text": "The Spanish-English dataset consists of 8,733 tweets (139,539 tokens) as training set, 1,587 tweets (33,276 tokens) as development set and 10,716 tweets (121,446 tokens) as final test set.", "labels": [], "entities": [{"text": "Spanish-English dataset", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.8552644848823547}]}, {"text": "Similarly, the Arabic (MSA-Egyptian) dataset consists of 8,862 tweets (185,928 tokens) as training set, 1,117 tweets (20,688 tokens) as development set and 1,262 tweets (20,713 tokens) as final test set.", "labels": [], "entities": [{"text": "Arabic (MSA-Egyptian) dataset", "start_pos": 15, "end_pos": 44, "type": "DATASET", "confidence": 0.6194410562515259}]}, {"text": "For Arabic we trained different word embeddings using word2vec   Data preprocessing: We transformed Arabic scripts to, a characterto-character mapping that replaces Arabic UTF alphabet with Latin characters to reduce size and streamline processing.", "labels": [], "entities": []}, {"text": "Also in order to reduce data sparsity, we converted all Persian numbers (e.g. ) to Arabic numbers (e.g. 1, 2), Arabic punctuation (e.g. '' and '') to Latin punctuation (e.g. ',' and ';'), removed kashida (elongation character) and vowel marks, and separated punctuation marks from words.", "labels": [], "entities": []}, {"text": "We explored different combinations of hand-crafted features (Section 3.3.1), word LSTM and char-word LSTM models with CRF and softmax classifier to identify the best system.", "labels": [], "entities": []}, {"text": "show the results for different settings for Spanish-English and MSA-Egyptian on the development dataset respectively.", "labels": [], "entities": []}, {"text": "For the Spanish-English dataset, we find that combining the character and word representations learned with a char-word LSTM system with hand-crafted features and then using CRF as a sequence classifier gives the highest overall accuracy   of 0.963.", "labels": [], "entities": [{"text": "Spanish-English dataset", "start_pos": 8, "end_pos": 31, "type": "DATASET", "confidence": 0.7090986371040344}, {"text": "accuracy", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.9990590214729309}]}, {"text": "Also, we notice that the addition of character and word representations improves the F1-score for named entity and unknown tokens.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9992617964744568}]}, {"text": "For the MSAEgyptian dataset, we find that a char-word LSTM model with softmax classifier is better than the CRF as this setting gives us the highest overall accuracy of 0.90.", "labels": [], "entities": [{"text": "MSAEgyptian dataset", "start_pos": 8, "end_pos": 27, "type": "DATASET", "confidence": 0.9599445462226868}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9978166818618774}]}, {"text": "Moreover, the addition of character and word representations to hand-crafted features improves the F1 score for named entity.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9894798696041107}]}, {"text": "Based on these results, our final system for Spanish-English uses CRF with hand-crafted features and character and word representations learned with a char-word LSTM model and the MSA-Egyptian uses charword LSTM model with softmax as classifier.", "labels": [], "entities": []}, {"text": "We do not use any kind of hand-crafted features for the MSA-Egyptian dataset.", "labels": [], "entities": [{"text": "MSA-Egyptian dataset", "start_pos": 56, "end_pos": 76, "type": "DATASET", "confidence": 0.9276922345161438}]}, {"text": "Our final system outperformed all other participants' systems for the MSA-Egyptian dialects in terms of tweet level and token level performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1 score results on Spanish-English development", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9725233614444733}]}, {"text": " Table 2: F1 score results on MSA-Egyptian development", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9747904241085052}, {"text": "MSA-Egyptian development", "start_pos": 30, "end_pos": 54, "type": "DATASET", "confidence": 0.8453311920166016}]}, {"text": " Table 3: Tweet level results the test dataset.", "labels": [], "entities": []}, {"text": " Table 4: Token level results on Spanish-English test dataset.", "labels": [], "entities": [{"text": "Spanish-English test dataset", "start_pos": 33, "end_pos": 61, "type": "DATASET", "confidence": 0.8223797678947449}]}, {"text": " Table 5: Token level results on MSA-DA test dataset.", "labels": [], "entities": [{"text": "MSA-DA test dataset", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.8937720457712809}]}]}