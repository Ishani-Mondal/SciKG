{"title": [], "abstractContent": [{"text": "In this work, we evaluate different sentence encoders with emphasis on examining their embedding spaces.", "labels": [], "entities": []}, {"text": "Specifically , we hypothesize that a \"high-quality\" embedding aids in generalization, promoting transfer learning as well as zero-shot and one-shot learning.", "labels": [], "entities": [{"text": "generalization", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.9658780097961426}]}, {"text": "To investigate this, we modify Skipthought vectors to learn a more generalizable space by exploiting a small amount of supervision.", "labels": [], "entities": []}, {"text": "The aim is to introduce an additional notion of similarity in the embeddings, rendering the vectors informative for different tasks requiring less adaptation.", "labels": [], "entities": []}, {"text": "Our embeddings capture human intuition on similarity favorably than competing models, while we also show positive indications of transfer from the task of natural language inference to paraphrase detection and paraphrase ranking.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 185, "end_pos": 205, "type": "TASK", "confidence": 0.8382764756679535}, {"text": "paraphrase ranking", "start_pos": 210, "end_pos": 228, "type": "TASK", "confidence": 0.9035272896289825}]}, {"text": "Further, our model's behaviour on paraphrase detection when trained with an increasing amount of labelled data is indicative of a generalizable model.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.9390243291854858}]}, {"text": "Finally, we support our hypothesis on generaliz-ability of our embeddings through inspection of their statistics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language is an integral part of numerous applications, such as web search, information retrieval, and automatic text summarization, to name just a few.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.8314544856548309}, {"text": "automatic text summarization", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.5965869029362997}]}, {"text": "Therefore, constructing high-quality text representations is very important.", "labels": [], "entities": []}, {"text": "In addition, despite having well-established methods to construct word representations, it remains an open problem to capture the semantics of larger pieces of text in a vector that is useful for different tasks with minimal adaptation.", "labels": [], "entities": []}, {"text": "In this paper we report on our efforts towards building such generalizable sentence representations.", "labels": [], "entities": []}, {"text": "Representing a sentence as a vector can bethought of as \"embedding\" it into a highdimensional space.", "labels": [], "entities": []}, {"text": "Therefore, a meaningful representation relies on a function which sends \"related\" sentences to neighbouring points in this vector space.", "labels": [], "entities": []}, {"text": "There are, however, many possible notions of closeness that maybe desirably reflected in the embeddings.", "labels": [], "entities": []}, {"text": "For instance, two sentences could be considered similar if they are likely to be found in the same context (\"distributional similarity\"), or if the second is entailed from the first, or if they are paraphrases of each other.", "labels": [], "entities": []}, {"text": "We hypothesize that an embedding space which adheres to multiple of these notions can host more generalizable vectors.", "labels": [], "entities": []}, {"text": "For instance our hypothesis is that in a \"generalizable\" space, two sentences that are likely to be found in the same context and also entail each other are closer that two other sentences which are also likely to be found in the same context but contradict each other.", "labels": [], "entities": []}, {"text": "Moreover, we believe that \"supervised evaluation\" of sentence encoders is not informative of the embedding quality: a classifier is trained on top of the sentence embeddings and then the accuracy for the task is computed, and is used as a proxy for the quality of the embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9988102912902832}]}, {"text": "This approach has the disadvantage that it hides the embedding properties due to the extra training which allows to mend its potential shortcomings.", "labels": [], "entities": []}, {"text": "We instead focus our attention on directly inspecting the model space.", "labels": [], "entities": []}, {"text": "In this work, we introduce a sentence encoder that is learned by injecting supervised information from the Stanford Natural Language Inference (SNLI) dataset) in the commonly used Skipthought embeddings.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) dataset", "start_pos": 107, "end_pos": 157, "type": "DATASET", "confidence": 0.5788625776767731}]}, {"text": "The aim is to enhance the embeddings with an additional notion of similarity, rendering them more generalizable.", "labels": [], "entities": []}, {"text": "We experiment with this model and with sentence encoders of different training objectives in order to compare both their performance in the commonly used supervised fashion () for reference, but more importantly their embedding quality.", "labels": [], "entities": []}, {"text": "We perform supervised evaluation on paraphrase detection, semantic relatedness, natural language inference and various classification benchmarks.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.937800943851471}]}, {"text": "We then evaluate the embeddings through paraphrase ranking, correlation of their similarity notion with human judgements (), through paraphrase detection with little or no training, and through examination of embedding statistics.", "labels": [], "entities": [{"text": "paraphrase ranking", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.8100101351737976}, {"text": "paraphrase detection", "start_pos": 133, "end_pos": 153, "type": "TASK", "confidence": 0.9058229923248291}]}], "datasetContent": [{"text": "In this section we present results on a number of supervised tasks.", "labels": [], "entities": []}, {"text": "These results are obtained by encoding the sentence at hand (or each sentence of the pair when applicable) and using this encoding as the features of a logistic regression which is trained for the given task, following the approach in (.", "labels": [], "entities": []}, {"text": "For the tasks involving pairs of sentences, the features that were given to the logistic classifier were computed as follows: the element-wise product and absolute difference between the two sentence embeddings were computed and then concatenated, resulting in a 9600-dimensional vector, as was also done in ().", "labels": [], "entities": []}, {"text": "In the next paragraph we briefly describe the tasks that we report experiments on.", "labels": [], "entities": []}, {"text": "Paraphrase detection (MSRP dataset) is the task where given pairs of sentences, the goal is to assign a binary label indicating whether the sentences of each pair are paraphrases.", "labels": [], "entities": [{"text": "Paraphrase detection (MSRP dataset)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8616449137528738}]}, {"text": "For semantic relatedness we use SICK (, and the objective is to assign a score of relatedness in the range 1-5 to pairs of sentences.", "labels": [], "entities": [{"text": "SICK", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.8242867588996887}]}, {"text": "Natural language inference is the task of predicting a label of \"entailment\", \"contradiction\", or \"neutral\" for each pair.", "labels": [], "entities": [{"text": "Natural language inference", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6788260340690613}]}, {"text": "Note that SNLI is a dataset for this task, but the results we report here are on SICK, which has both relatedness scores as well as these 3-way classifications labels for each pair.", "labels": [], "entities": []}, {"text": "TREC is a dataset for (6-way) question-type classification and finally, MR and SUBJ come from a movie review dataset and they are binary classification tasks for sentiment polarity (MR) and subjectivity status (SUBJ).", "labels": [], "entities": [{"text": "TREC", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6206841468811035}, {"text": "question-type classification", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7067558169364929}, {"text": "MR", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9979619979858398}]}, {"text": "The     could suggest that SNLI information, when injected into an RNN encoder and fed into a logistic regression classifier, is adequate in order to perform reasonably well on paraphrase detection and semantic relatedness.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 177, "end_pos": 197, "type": "TASK", "confidence": 0.9372974634170532}]}, {"text": "But we believe that this observation underlines the weakness of this method of evaluation.", "labels": [], "entities": []}, {"text": "The 2% improvment of SNLI-Finetuned Skipthoughts over skipthoughts for paraphrase detection is an indication of transfer from SNLI to MSRP, on which () presented negative results.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.9010024666786194}]}, {"text": "Our results on transfer to the related task of paraphrase ranking which also uses MSRP (Section 6.1) are even more encouraging.", "labels": [], "entities": [{"text": "transfer", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.9593185782432556}, {"text": "paraphrase ranking", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.973565399646759}, {"text": "MSRP (Section 6.1", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.6517015770077705}]}, {"text": "Further, on natural language inference all SNLI models (slightly) outperform the non-SNLI ones.", "labels": [], "entities": []}, {"text": "This is not surprising given their training objective, and constitutes a less impressive sign of transfer between these two datasets of the same task.", "labels": [], "entities": []}, {"text": "Finally, we note that SNLI-finetuned Skipthoughts perform either better or on par with Skipthoughts on all tasks considered.", "labels": [], "entities": [{"text": "SNLI-finetuned Skipthoughts", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.4748572111129761}]}, {"text": "This shows that the added SNLI information does not hurt Skipthoughts' performance on this evaluation while outperforming it in terms of embedding space quality as demonstrated in the next section, which we argue is more important.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on paraphrase detection (MSRP).", "labels": [], "entities": [{"text": "paraphrase detection (MSRP)", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.8808164119720459}]}, {"text": " Table 2: Results on natural language inference  (SICK). Note that this is the same task as SNLI,  but different dataset.", "labels": [], "entities": [{"text": "natural language inference  (SICK)", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.765288308262825}]}, {"text": " Table 3: Results on semantic relatedness (SICK).  PR, SR, SE: Pearson, Spearman correlation coeffi- cient and mean squared error, resp. between model  scores and human scores.", "labels": [], "entities": [{"text": "semantic relatedness (SICK)", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.8140798449516297}, {"text": "SE: Pearson, Spearman correlation coeffi- cient and mean squared error", "start_pos": 59, "end_pos": 129, "type": "METRIC", "confidence": 0.7082200875649085}]}, {"text": " Table 4: Results on various classification tasks.  Each row stores test accuracies of the correspond- ing dataset.", "labels": [], "entities": []}, {"text": " Table 5: Results on paraphrase ranking. accu- racy@k is the proportion of sentences for which  the true paraphrase received rank at most k. MRC  is the Mean Rank of the Correct paraphrase.", "labels": [], "entities": [{"text": "paraphrase ranking", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8723807334899902}, {"text": "MRC", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9858454465866089}, {"text": "Mean Rank", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9741002321243286}]}, {"text": " Table 6: Very simple baseline for paraphrase rank- ing. random BOW is no way capturing anything  informative about the sentence (see section 5.1  for description). These results suggest that BOW  models may have an unfair advantage for MSRP.", "labels": [], "entities": [{"text": "paraphrase rank- ing", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.8165190517902374}, {"text": "MSRP", "start_pos": 237, "end_pos": 241, "type": "TASK", "confidence": 0.9107707142829895}]}, {"text": " Table 7: Results on semantic relatedness (SICK)  based on cosine distances. PR, SR, SE: Pear- son, Spearman correlation coefficient and mean  squared error, resp. between model scores and hu- man scores.", "labels": [], "entities": [{"text": "semantic relatedness (SICK)", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.7772695302963257}, {"text": "PR", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9669755697250366}, {"text": "SR", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.8638170957565308}, {"text": "SE", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.7758810520172119}, {"text": "Pear- son, Spearman correlation coefficient", "start_pos": 89, "end_pos": 132, "type": "METRIC", "confidence": 0.6826069142137255}, {"text": "mean  squared error", "start_pos": 137, "end_pos": 156, "type": "METRIC", "confidence": 0.8321582674980164}]}, {"text": " Table 8: Results of the ranking task for the reference sentence My soup is not hot anymore. sim refers  to the similarity between the reference sentence and the sentence of the corresponding row in embedding  space. Sentences which are \"relevant\" to this one (Group 2), and thus should receive lower ranks, are  shown in bold.", "labels": [], "entities": []}]}