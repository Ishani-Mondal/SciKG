{"title": [{"text": "The Role of Discourse Units in Near-Extractive Summarization", "labels": [], "entities": [{"text": "Near-Extractive Summarization", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.6678763926029205}]}], "abstractContent": [{"text": "Although human-written summaries of documents tend to involve significant edits to the source text, most automated summa-rizers are extractive and select sentences verbatim.", "labels": [], "entities": [{"text": "summaries of documents", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.8610032995541891}]}, {"text": "In this work we examine how elementary discourse units (EDUs) from Rhetorical Structure Theory can be used to extend extractive summarizers to produce a wider range of human-like summaries.", "labels": [], "entities": []}, {"text": "Our analysis demonstrates that EDU segmentation is effective in preserving human-labeled summarization concepts within sentences and also aligns with near-extractive summaries constructed by news editors.", "labels": [], "entities": [{"text": "EDU segmentation", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.8624269068241119}]}, {"text": "Finally, we show that using EDUs as units of content selection instead of sentences leads to stronger sum-marization performance in near-extractive scenarios, especially under tight budgets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document summarization has a wide variety of practical applications and is consequently a focus of much NLP research.", "labels": [], "entities": [{"text": "Document summarization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9290111660957336}]}, {"text": "When a human summarizes a document, they often edit its constituent sentences in order to succinctly capture the document's meaning.", "labels": [], "entities": [{"text": "summarizes a document", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.8637412786483765}]}, {"text": "For instance, observed that summary authors trimmed extraneous content, combined sentences, replaced phrases or clauses with more general or specific variants, etc.", "labels": [], "entities": []}, {"text": "These abstractive summaries thus involve sentences which deviate from those of the source document in structure or content.", "labels": [], "entities": []}, {"text": "In contrast, automated approaches to summarization generally produce extractive summaries by selecting complete sentences from the source document () in order to ensure that the output is grammatical.", "labels": [], "entities": [{"text": "summarization", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9886283874511719}]}, {"text": "Extractive summarization techniques, which are widely used in practical applications, therefore address a substantially simpler problem than human summarization.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8728643357753754}]}, {"text": "This leads to a natural question: can extractive summarization techniques be used to produce more human-like summaries?", "labels": [], "entities": [{"text": "summarization", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.7649268507957458}]}, {"text": "We hypothesize that automated methods can generate a wider range of summaries by extracting over sub-sentential units of meaning from the source documents rather than whole sentences.", "labels": [], "entities": []}, {"text": "Specifically, in this paper we investigate whether elementary discourse units (EDUs) from Rhetorical Structure Theory () comprise viable textual units for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 155, "end_pos": 168, "type": "TASK", "confidence": 0.9847751259803772}]}, {"text": "Our focus is on recovering salient summary content under ROUGE) while the composition of EDUs into fluent output sentences is left to future work.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.8271610140800476}]}, {"text": "We investigate this hypothesis in two complementary ways: by studying the compatibility of EDUs with human-labeled summarization units from pyramid evaluations ( and by assessing their utility in reconstructing real-world document previews chosen by news editors in the New York Times corpus.", "labels": [], "entities": [{"text": "reconstructing real-world document previews chosen by news editors in the New York Times", "start_pos": 196, "end_pos": 284, "type": "TASK", "confidence": 0.7463014171673701}]}, {"text": "The contributions of this work include: \u2022 A demonstration that EDU segmentation preserves human-identified conceptual units in the context of document summarization.", "labels": [], "entities": [{"text": "EDU segmentation", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.891207754611969}, {"text": "document summarization", "start_pos": 142, "end_pos": 164, "type": "TASK", "confidence": 0.5861613154411316}]}, {"text": "\u2022 New, large datasets proposed for research into extractive and compressive summarization of news articles.", "labels": [], "entities": [{"text": "extractive and compressive summarization of news articles", "start_pos": 49, "end_pos": 106, "type": "TASK", "confidence": 0.7164499972547803}]}, {"text": "\u2022 A study of the lexical omissions made by news editors in real-world compressive summarization.", "labels": [], "entities": []}, {"text": "\u2022 A comparative analysis of supervised singledocument summarization overfull sentences and over a range of budgets in extractive and near-extractive scenarios.", "labels": [], "entities": []}], "datasetContent": [{"text": "The NYT dataset contains editor-produced online lead paragraphs 2 which accompany 284,980 arti-cles featured prominently on the NYT homepage from 2001 onwards.", "labels": [], "entities": [{"text": "NYT dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9856346249580383}, {"text": "NYT homepage from 2001", "start_pos": 128, "end_pos": 150, "type": "DATASET", "confidence": 0.9619406461715698}]}, {"text": "They are explicitly intended for presentation to readers and usually consist of one or more complete sentences which serve as a brief summary or teaser for the full article.", "labels": [], "entities": []}, {"text": "We ensure that these online lead paragraphshenceforth online summaries-are composed of complete sentences by filtering out cases which contain no verbs, omit sentence-terminating punctuation or are all-uppercase, respectively indicating summaries which are caption-like, truncated or merely topic/location descriptors.", "labels": [], "entities": []}, {"text": "We also exclude articles with frequently repeated titles, first sentences and summaries which we observe to be template-like and thus not indicative of editorial input.", "labels": [], "entities": []}, {"text": "Finally, we preprocess the remaining 244,267 summaries by stripping HTML artifacts and structured prefixes (e.g., bureau locations), normalizing Unicode symbols and fixing whitespace inserted within or deleted between tokens.", "labels": [], "entities": []}, {"text": "We have released our data preparation code to facilitate future research on the NYT corpus.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 80, "end_pos": 90, "type": "DATASET", "confidence": 0.9098391830921173}]}, {"text": "Three mutually exclusive datasets 5 are drawn from the processed document collection: \u2022 EX-SENT: 38,921 fully extractive instances in which each summary sentence is drawn whole from the article when ignoring case, punctuation and whitespace.", "labels": [], "entities": [{"text": "EX-SENT", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9635915756225586}]}, {"text": "\u2022 NX-SPAN: 15,646 near-extractive instances where one or more summary sentences form a contiguous span of tokens within an article sentence, and the remaining fit the definition above.", "labels": [], "entities": [{"text": "NX-SPAN", "start_pos": 2, "end_pos": 9, "type": "DATASET", "confidence": 0.9539086818695068}]}, {"text": "\u2022 NX-SUBSEQ: 25,381 near-extractive instances where one or more summary sentences form a non-contiguous token subsequence within an article sentence, and the remaining fit either of the definitions above.: Examples of reference summaries from NX-SPAN and NX-SUBSEQ alongside their source sentences from the article, segmented into EDUs.", "labels": [], "entities": [{"text": "NX-SUBSEQ", "start_pos": 2, "end_pos": 11, "type": "DATASET", "confidence": 0.9507651925086975}]}, {"text": "Tokens omitted by the summary are italicized.", "labels": [], "entities": []}, {"text": "paper but left to future work.", "labels": [], "entities": []}, {"text": "Examples of summaries from the two near-extractive datasets are presented in along with EDU-segmented source sentences from the corresponding articles.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average salience scores of EDUs  overlapping with SCU contributors, stratified by  SCU weight. Differences between scores for  each group are statistically significant under the  Wilcoxon rank-sum test (p < 0.05).", "labels": [], "entities": []}, {"text": " Table 2: Examples of reference summaries from NX-SPAN and NX-SUBSEQ alongside their source sen- tences from the article, segmented into EDUs. Tokens omitted by the summary are italicized.", "labels": [], "entities": [{"text": "NX-SPAN", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.9356667399406433}, {"text": "NX-SUBSEQ", "start_pos": 59, "end_pos": 68, "type": "DATASET", "confidence": 0.827879011631012}]}, {"text": " Table 3: Average #tokens deleted and added from  each type of unit in NX-SPAN and NX-SUBSEQ.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9593788385391235}, {"text": "NX-SPAN", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9564858675003052}, {"text": "NX-SUBSEQ", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.8949318528175354}]}, {"text": " Table 4: ROUGE-1 of lead sentences vs. the su- pervised summarizer under a 200-char budget.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.995372474193573}]}, {"text": " Table 5: ROUGE results for EDU-and sentence- based summarization.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9848459959030151}, {"text": "EDU-and sentence- based summarization", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.5547085523605346}]}]}