{"title": [{"text": "System Description of bjtu_nlp Neural Machine Translation System", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6298748751481374}]}], "abstractContent": [{"text": "This paper presents our machine translation system that developed for the WAT2016 evaluation tasks of ja-en, ja-zh, en-ja, zh-ja, JPCja-en, JPCja-zh, JPCen-ja, JPCzh-ja.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.6713377833366394}, {"text": "WAT2016 evaluation tasks", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.7021007140477499}]}, {"text": "We build our system based on encoder-decoder framework by integrating recurrent neural network (RNN) and gate recurrent unit (GRU), and we also adopt an attention mechanism for solving the problem of information loss.", "labels": [], "entities": [{"text": "information loss", "start_pos": 200, "end_pos": 216, "type": "TASK", "confidence": 0.7419878244400024}]}, {"text": "Additionally, we propose a simple translation-specific approach to resolve the unknown word translation problem.", "labels": [], "entities": [{"text": "word translation", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.766907125711441}]}, {"text": "Experimental results show that our system performs better than the baseline statistical machine translation (SMT) systems in each task.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 76, "end_pos": 113, "type": "TASK", "confidence": 0.8082875311374664}]}, {"text": "Moreover, it shows that our proposed approach of unknown word translation performs effectively improvement of translation results.", "labels": [], "entities": [{"text": "unknown word translation", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.6951001683870951}]}], "introductionContent": [{"text": "Our system is constructed by using the framework of neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 52, "end_pos": 84, "type": "TASK", "confidence": 0.8031533062458038}]}, {"text": "NMT is a recently proposed approach to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8129984140396118}]}, {"text": "Unlike the traditional SMT, the NMT aims at building a single neural network that can be jointly turned to maximize the translation performance).", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9840760827064514}]}, {"text": "Most of the existing NMT models are built based on Encoder-Decoder framework).", "labels": [], "entities": []}, {"text": "The encoder network encodes the source sentence into a vector, the decoder generates a target sentence.", "labels": [], "entities": []}, {"text": "While early models encode the source sentence into a fixed-length vector.", "labels": [], "entities": []}, {"text": "For instance, Bahdanau et al. advocate the attention mechanism to dynamically generate a context vector of the whole source sentence () for improving the performance of the NMT.", "labels": [], "entities": []}, {"text": "Recently, a large amount of research works focus on the attention mechanism (.", "labels": [], "entities": []}, {"text": "In this paper, we adopt RNN, GRU and attention mechanism to build an Encoder-Decoder network as our machine translation system.", "labels": [], "entities": [{"text": "GRU", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.768028736114502}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.6938486695289612}]}, {"text": "shows the framework of our NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7283650040626526}]}, {"text": "x: The framework of NMT.", "labels": [], "entities": []}, {"text": "Where x and y denote embeddings of words in the source vocabulary and target vocabulary respectively, h means the hidden state of Encoder RNN, sis the hidden state of decode RNN, ci is the context vector, a expresses the attention weight of each position.", "labels": [], "entities": [{"text": "Encoder RNN", "start_pos": 130, "end_pos": 141, "type": "DATASET", "confidence": 0.9159302413463593}]}, {"text": "Experiment results show that our system achieved significantly higher BLEU scores compared to the traditional SMT system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.999298095703125}, {"text": "SMT", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9934188723564148}]}, {"text": "shows the structure of our NMT system.: The structure of our system Our system consists three parts: training part, decode part and the post-processing part of our proposed approach of unknown word processing.", "labels": [], "entities": [{"text": "word processing", "start_pos": 193, "end_pos": 208, "type": "TASK", "confidence": 0.6903721839189529}]}], "datasetContent": [{"text": "We participated in all tasks related to Chinese and Japanese and English.", "labels": [], "entities": []}, {"text": "We use the given data of Asian Scientific Paper Excerpt Corpus (ASPEC) and JPO Patent Corpus (JPC)   For long sentence, we discarded all of the sentences which length with more than 50 words on both source and target side.", "labels": [], "entities": [{"text": "Asian Scientific Paper Excerpt Corpus (ASPEC)", "start_pos": 25, "end_pos": 70, "type": "DATASET", "confidence": 0.7983180955052376}, {"text": "JPO Patent Corpus (JPC", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.9053900718688965}]}], "tableCaptions": [{"text": " Table 2: Official automatic evaluation results on ASPEC", "labels": [], "entities": [{"text": "ASPEC", "start_pos": 51, "end_pos": 56, "type": "TASK", "confidence": 0.7683728933334351}]}, {"text": " Table 3: Official automatic evaluation results on JPC", "labels": [], "entities": [{"text": "JPC", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.5018478035926819}]}]}