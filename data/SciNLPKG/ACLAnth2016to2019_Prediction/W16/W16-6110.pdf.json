{"title": [{"text": "Replicability of Research in Biomedical Natural Language Processing: a pilot evaluation fora coding task", "labels": [], "entities": [{"text": "Replicability of Research in Biomedical Natural Language Processing", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.579916175454855}]}], "abstractContent": [{"text": "The scientific community is facing raising concerns about the reproducibility of research in many fields.", "labels": [], "entities": []}, {"text": "To address this issue in Natural Language Processing, the CLEF eHealth 2016 lab offered a replication track together with the Clinical Information Extraction task.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.6581392188866934}, {"text": "CLEF eHealth 2016 lab", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.9602722227573395}, {"text": "Clinical Information Extraction task", "start_pos": 126, "end_pos": 162, "type": "TASK", "confidence": 0.6771589741110802}]}, {"text": "Herein, we report detailed results of the repli-cation experiments carried outwith the three systems submitted to the track.", "labels": [], "entities": []}, {"text": "While all results were ultimately replicated, we found that the systems were poorly rated by analysts on documentation aspects such as \"ease of understanding system requirements\" (33%) and \"provision of information while system is run-ning\" (33%).", "labels": [], "entities": [{"text": "ease", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9904725551605225}]}, {"text": "As a result, simple steps could betaken by system authors to increase the ease of replicability of their work, thereby increasing the ease of re-using the systems.", "labels": [], "entities": [{"text": "ease", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9793881177902222}, {"text": "ease", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.976503312587738}]}, {"text": "Our experiments aim to raise the awareness of the community towards the challenges of replica-tion and community sharing of NLP systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reproducibility, or replicability, is the quality of a scientific experiment that can be performed independently several times and yield the exact same results on each iteration.", "labels": [], "entities": []}], "datasetContent": [{"text": "Early in the history of natural language processing, it was quite difficult for researchers to learn from comparisons of systems because they generally differed on the most basic issues of goals and metrics.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6707034607728323}]}, {"text": "Answering questions that are commonplace today, such as what are the advantages and disadvantages of purely rule-based methods and purely learningbased methods for information extraction?, was not possible when the differences between projects included not only different methods, but also different extraction targets, data, and figures of merit.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 164, "end_pos": 186, "type": "TASK", "confidence": 0.8094018399715424}]}, {"text": "In that context, the idea developed that one could learn more from research by standardizing some of those basic aspects of the work.", "labels": [], "entities": []}, {"text": "The resulting shared task model of evaluation consists of multiple groups agreeing on a shared task definition, a shared data set, and a shared evaluation metric.", "labels": [], "entities": []}, {"text": "Thus, shared tasks provide an opportunity to overcome some of the challenges to replication in natural language processing-in particular, the definitional, data, and scoring issues.", "labels": [], "entities": []}, {"text": "The work reported here explores the question of whether the evaluation of replicability in natural language processing can be pushed forward to the highest level of the replicability hierarchy by taking advantage of these aspects of the shared task model.", "labels": [], "entities": []}, {"text": "The rationale behind this approach is that one can capitalize on the fact that the systems that are used to address a challenge task all accommodate the same input and output formats, as specified in the challenge.", "labels": [], "entities": []}, {"text": "And, the scoring code is open and freely available.", "labels": [], "entities": []}, {"text": "Therefore, system results on a challenge dataset should be very easy to replicate without incurring significant training and effort-or, at least, they should be possible to replicate, if given access to the original system.", "labels": [], "entities": []}, {"text": "Four system analysts committed to spend a maximum of one working day (8 hours) with each system.", "labels": [], "entities": []}, {"text": "The analysts attempted to install and configure the systems according to the instructions supplied.", "labels": [], "entities": []}, {"text": "Participants were also allowed to supply a contact address to make themselves available to address any additional questions.", "labels": [], "entities": []}, {"text": "Two analysts had a Computer Science background with experience developping research systems in the field of bioNLP, and represented the usecase of a colleague trying to reproduce experiments in their field (research-oriented role).", "labels": [], "entities": []}, {"text": "Another analyst had a computer science background, and the fourth analyst had a mixed linguistics/computational linguistics background.", "labels": [], "entities": []}, {"text": "Both had experience using bioNLP applications and represented the use-case of a user trying to leverage an existing tool fora task of interest (user-oriented role).", "labels": [], "entities": []}, {"text": "In contrast with (Zheng et al., 2015), we did not foster a controlled environment (e.g. using a virtual machine with standard configuration for all analysts) for installing the systems evaluated because we wanted the analysts to work in an experimental setting that would be similar to the one they would use for reproducing experiments.", "labels": [], "entities": []}, {"text": "For the same reason, we did not rely on the use of containers.", "labels": [], "entities": []}, {"text": "The analysts independently ran the systems on the appropriate CLEF eHealth task 2 test sets.", "labels": [], "entities": [{"text": "CLEF eHealth task 2 test sets", "start_pos": 62, "end_pos": 91, "type": "DATASET", "confidence": 0.9606742858886719}]}, {"text": "The results obtained were be compared to those submitted by the teams using the same system.", "labels": [], "entities": []}, {"text": "During this process, the analysts took notes on the various aspects of working with the systems (ease of installing and using, ease of understanding supplied instructions, success of the replication attempt), using a specific score sheet developed by the analysts, following some of the criteria evaluated by).", "labels": [], "entities": []}, {"text": "The score sheet comprised 10 questions addressing the experience of analysts at each stage of the experiment: system configuration, system installation, running the system, obtaining results, and overall impressions.", "labels": [], "entities": []}, {"text": "shows the specific questions and answer scales.", "labels": [], "entities": []}, {"text": "The analysts were also encouraged to complete their answer to questions with free text comments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Configuration of the machines used by the analysts to", "labels": [], "entities": []}, {"text": " Table 4: Time (in minutes) spent by each analyst reproducing", "labels": [], "entities": []}, {"text": " Table 5: Aggregated scoring of systems. A star symbol * in-", "labels": [], "entities": []}]}