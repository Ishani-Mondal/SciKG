{"title": [{"text": "SCTB: A Chinese Treebank in Scientific Domain", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.7587321698665619}]}], "abstractContent": [{"text": "Treebanks are curial for natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.7890197535355886}]}, {"text": "In this paper, we present our work for annotating a Chinese treebank in scientific domain (SCTB), to address the problem of the lack of Chinese treebanks in this domain.", "labels": [], "entities": [{"text": "Chinese treebank in scientific domain (SCTB)", "start_pos": 52, "end_pos": 96, "type": "DATASET", "confidence": 0.7619027234613895}]}, {"text": "Chinese analysis and machine translation experiments conducted using this treebank indicate that the annotated treebank can significantly improve the performance on both tasks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7053032666444778}]}, {"text": "This treebank is released to promote Chinese NLP research in scientific domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "A treebank is a text corpus consisting of usually thousands to tens of thousands of sentences annotated with linguistic knowledge such as segmentation, part-of-speech (POS) tags and syntactic structures.", "labels": [], "entities": []}, {"text": "From the initial release of the first treebank of the Penn treebank (PTB), treebanking has remarkably promoted the research of statistical natural language processing (NLP).", "labels": [], "entities": [{"text": "Penn treebank (PTB)", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.972014844417572}, {"text": "statistical natural language processing (NLP)", "start_pos": 127, "end_pos": 172, "type": "TASK", "confidence": 0.7363795978682381}]}, {"text": "Inspired by the success of the English treebank, treebanks for other languages have also been constructed or under construction ().", "labels": [], "entities": [{"text": "English treebank", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.7984429597854614}]}, {"text": "For Chinese, there are several existing treebanks such as the widely used Penn Chinese treebank (CTB) (, and the Peking University (PKU) treebank ( . Chinese language processing has been significantly developed with these treebanks.", "labels": [], "entities": [{"text": "Penn Chinese treebank (CTB)", "start_pos": 74, "end_pos": 101, "type": "DATASET", "confidence": 0.9576537509759268}, {"text": "Peking University (PKU) treebank", "start_pos": 113, "end_pos": 145, "type": "DATASET", "confidence": 0.6541139930486679}]}, {"text": "For example, the F-Measures of Chinese analysis on the benchmark data set CTB version 5 (CTB5) 1 has achieved about 98% for segmentation, 94% for POS tagging, and 80% for syntactic parsing.", "labels": [], "entities": [{"text": "benchmark data set CTB version 5 (CTB5) 1", "start_pos": 55, "end_pos": 96, "type": "DATASET", "confidence": 0.9266073048114777}, {"text": "segmentation", "start_pos": 124, "end_pos": 136, "type": "TASK", "confidence": 0.9678083062171936}, {"text": "POS tagging", "start_pos": 146, "end_pos": 157, "type": "TASK", "confidence": 0.7868154346942902}, {"text": "syntactic parsing", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.7723686099052429}]}, {"text": "One difficulty of statistical NLP is the domain diversity.", "labels": [], "entities": []}, {"text": "As most treebanks such as the PTB, CTB and PKU are constructed mainly in news domain, the performance is not satisfied when analyzing sentences in other distant domains using the models trained on these treebanks.", "labels": [], "entities": [{"text": "PTB", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.9447782635688782}]}, {"text": "In China, the number of scientific documents has been remarkably increased.", "labels": [], "entities": []}, {"text": "For example, the worldwide share of patent documents has increased to 30% (worldwide rank 1) in 2009, and the worldwide share of scientific papers has increased to 13% on the average of 2011-2013 (worldwide rank 2) (.", "labels": [], "entities": []}, {"text": "Therefore, the needs for scientific domain text analyzing such as text mining, knowledge discovery, and translating scientific documents to other languages are increasing.", "labels": [], "entities": [{"text": "scientific domain text analyzing", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.6195826232433319}, {"text": "text mining", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.8081588745117188}, {"text": "knowledge discovery", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7621275782585144}]}, {"text": "However, when applying the Chinese analysis models trained on different domains to scientific domain, the F-Measures of various analysis tasks dramatically decrease to 90% for segmentation, 78% for POS tagging, and 70% for syntactic parsing (Section 3.1).", "labels": [], "entities": [{"text": "F-Measures", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9958498477935791}, {"text": "segmentation", "start_pos": 176, "end_pos": 188, "type": "TASK", "confidence": 0.96442711353302}, {"text": "POS tagging", "start_pos": 198, "end_pos": 209, "type": "TASK", "confidence": 0.7981175482273102}, {"text": "syntactic parsing", "start_pos": 223, "end_pos": 240, "type": "TASK", "confidence": 0.8163462579250336}]}, {"text": "This level of low accuracy analysis could significantly affect the performance of downstream applications such as text mining and machine translation (MT).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9472877383232117}, {"text": "text mining", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.8058222532272339}, {"text": "machine translation (MT)", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.8459444165229797}]}, {"text": "Motivated by this, we decide to construct a Chinese treebank in the scientific domain (SCTB) to promote Chinese NLP research in this domain.", "labels": [], "entities": [{"text": "Chinese treebank in the scientific domain (SCTB)", "start_pos": 44, "end_pos": 92, "type": "DATASET", "confidence": 0.7562429640028212}]}, {"text": "This paper presents the details of our treebank annotation process and the experiments conducted on the annotated treebank.", "labels": [], "entities": []}, {"text": "The raw sentences are selected from Chinese scientific papers.", "labels": [], "entities": []}, {"text": "Our annotation process follows that of CTB () with an exception of the segmentation standard.", "labels": [], "entities": [{"text": "CTB", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.9000093936920166}, {"text": "segmentation", "start_pos": 71, "end_pos": 83, "type": "TASK", "confidence": 0.9559573531150818}]}, {"text": "We apply a Chinese word segmentation standard based on character-level POS patterns, aiming to circumvent inconsistency and address data (the bottom boxes contain words, the pre-terminal boxes contain POS tags, while the upper boxes contain phrasal constituents).", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.6350888113180796}]}, {"text": "sparsity of the annotated treebank.", "labels": [], "entities": []}, {"text": "As the first version of release, we finished the annotation of 5,133 sentences (138,781 words).", "labels": [], "entities": []}, {"text": "To verify the effectiveness of the annotated SCTB, we conducted both instinct Chinese analysis experiments of segmentation, POS tagging and syntactic parsing, and extrinsic MT experiments on Chinese-to-Japanese and Chinese-to-English directions.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.8044089376926422}, {"text": "syntactic parsing", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.7101654708385468}, {"text": "MT", "start_pos": 173, "end_pos": 175, "type": "TASK", "confidence": 0.9536384344100952}]}, {"text": "Experimental results show that the annotated SCTB can significantly improve both Chinese analysis and MT performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9844411611557007}]}], "datasetContent": [{"text": "We conducted both instinct and extrinsic experiments to verify the effectiveness of the annotated treebank.", "labels": [], "entities": []}, {"text": "The instinct experiments were the conventional Chinese analysis tasks including segmentation, POS tagging and syntactic parsing.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 80, "end_pos": 92, "type": "TASK", "confidence": 0.9681793451309204}, {"text": "POS tagging", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.8412883877754211}, {"text": "syntactic parsing", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7338513135910034}]}, {"text": "For the extrinsic experiments, we selected MT as an application of the Chinese analysis tasks, and conducted MT experiments on both the Chinese-to-Japanese and Chinese-to-English directions in scientific paper and patent domains, respectively.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.8501178622245789}, {"text": "MT", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.9810940027236938}]}, {"text": "We conducted segmentation, POS tagging and syntactic parsing experiments.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9717075824737549}, {"text": "POS tagging", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.9018954038619995}, {"text": "syntactic parsing", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7294519990682602}]}, {"text": "Segmentation and POS tagging experiments were conducted using the Chinese analyzing tool KyotoMorph: Joint segmentation and POS tagging results (\" \u2020\" indicates that the result is significantly better than \"Baseline\" at p < 0.01).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.7966654002666473}, {"text": "KyotoMorph", "start_pos": 89, "end_pos": 99, "type": "DATASET", "confidence": 0.809271514415741}, {"text": "Joint segmentation", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.567593976855278}, {"text": "POS tagging", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.7290027439594269}]}, {"text": "Parsing was performed by the Berkeley parser 6 (.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9073886871337891}, {"text": "Berkeley parser 6", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.8399486939112345}]}, {"text": "We compared the Chinese analysis performance of the Chinese analyzers trained on the following two settings.", "labels": [], "entities": []}, {"text": "\u2022 Baseline: Chinese analyzers trained on CTB5 containing 18k sentences in news domain, and a previously created in-house (mainly) NLP domain treebank of 10k sentences.", "labels": [], "entities": []}, {"text": "Note that the Chinese word segmentation of the baseline treebanks originally follows the conventional segmentation standard (), and we manually re-annotated them based on the character-level POS patterns ().", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.6375663181145986}]}, {"text": "\u2022 Baseline+SCTB: Additionally used 4,933 sentences from the newly annotated SCTB for training the Chinese analyzers.", "labels": [], "entities": []}, {"text": "For testing, we used the remaining 200 sentences from the newly annotated SCTB.", "labels": [], "entities": [{"text": "SCTB", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.9225543737411499}]}, {"text": "The significance tests were performed using the bootstrapping method (.", "labels": [], "entities": []}, {"text": "show the word segmentation, and the joint segmentation and POS tagging results, respectively.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.6657322347164154}, {"text": "POS tagging", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.71763975918293}]}, {"text": "We can see that SCTB significantly improves both segmentation and joint segmentation and POS tagging performance by a large margin, i.e., 4.27% and 7.26% F-Measure, respectively.", "labels": [], "entities": [{"text": "SCTB", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.8923850059509277}, {"text": "segmentation", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.9583098888397217}, {"text": "joint segmentation", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.6337207555770874}, {"text": "POS tagging", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.7164548188447952}, {"text": "F-Measure", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9957199692726135}]}, {"text": "We used the Evalb toolkit 7 for the parsing accuracy calculation.", "labels": [], "entities": [{"text": "Evalb toolkit 7", "start_pos": 12, "end_pos": 27, "type": "DATASET", "confidence": 0.936604917049408}, {"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9811490774154663}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9463586211204529}]}, {"text": "As Evalb was originally designed for English, it only can evaluate the sentences that have the same segmentation as the gold data.", "labels": [], "entities": []}, {"text": "For this reason, we showed the results based on gold segmentations in.", "labels": [], "entities": []}, {"text": "As a reference, the parsing F-Measures from scratch for Baseline and Baseline+SCTB are 74.88% and 79.80% for 66 and 107 valid sentences (sentences that have the same segmentation as the gold data), respectively.", "labels": [], "entities": [{"text": "F-Measures", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.8439483046531677}, {"text": "Baseline+SCTB", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.6680888732274374}]}, {"text": "For Chinese-to-Japanese translation, we conducted experiments on the scientific domain MT task on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ) (, which is one subtask of the workshop on Asian translation (WAT) 9 ().", "labels": [], "entities": [{"text": "Chinese-to-Japanese translation", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.6600267887115479}, {"text": "MT", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.8751810789108276}, {"text": "Asian translation (WAT)", "start_pos": 194, "end_pos": 217, "type": "TASK", "confidence": 0.7918389976024628}]}, {"text": "The ASPEC-CJ task uses 672,315, 2,090, and 2,107 sentences for training, development, and testing, respectively.", "labels": [], "entities": [{"text": "ASPEC-CJ task", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.5260188579559326}]}, {"text": "For Chinese-to-English translation, we conducted experiments on the Chinese-English subtask (NTCIR-CE) of the patent MT System ASPEC-CJ NTCIR-CE: BLEU-4 scores for ASPEC-CJ and NTCIR-CE translation tasks (\" \u2020\" indicates that the result is significantly better than \"Baseline\" at p < 0.01).", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.6526943892240524}, {"text": "ASPEC-CJ NTCIR-CE", "start_pos": 127, "end_pos": 144, "type": "DATASET", "confidence": 0.4849008619785309}, {"text": "BLEU-4", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9988271594047546}, {"text": "NTCIR-CE translation tasks", "start_pos": 177, "end_pos": 203, "type": "TASK", "confidence": 0.6727752784887949}]}, {"text": "task at the NTCIR-10 workshop 10 ().", "labels": [], "entities": [{"text": "NTCIR-10 workshop 10", "start_pos": 12, "end_pos": 32, "type": "DATASET", "confidence": 0.8939137061436971}]}, {"text": "The NTCIR-CE task uses 1,000,000, 2,000, and 2,000 sentences for training, development, and testing, respectively.", "labels": [], "entities": [{"text": "NTCIR-CE task", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8527192175388336}]}, {"text": "We used the Moses tree-to-string MT system () for all of our MT experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9087469577789307}, {"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9913434982299805}]}, {"text": "In our experiments, Chinese is in the tree format, and Japanese/English is in the string format.", "labels": [], "entities": []}, {"text": "For Chinese, we used KyotoMorph for segmentations and the Berkeley parser for joint POS tagging and parsing.", "labels": [], "entities": [{"text": "KyotoMorph", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.960314154624939}, {"text": "segmentations", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.959868311882019}, {"text": "POS tagging", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.7055114507675171}]}, {"text": "We binarized the parsing results for better translation rule extraction.", "labels": [], "entities": [{"text": "translation rule extraction", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.9012336333592733}]}, {"text": "We compared the MT performance of the \"Baseline\" and \"Baseline+SCTB\" settings in Section 3.1.", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.6569654941558838}]}, {"text": "For Japanese, we used JUMAN 11 for the segmentation.", "labels": [], "entities": [{"text": "JUMAN 11", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.792977511882782}, {"text": "segmentation", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9807023406028748}]}, {"text": "For English, we tokenized the sentences using a script in Moses.", "labels": [], "entities": []}, {"text": "For the Chinese-to-Japanese MT task, we trained a 5-gram language model for Japanese, on the training data of the ASPEC-CJ corpus using the KenLM toolkit 12 with interpolated Kneser-Ney discounting.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.8928900361061096}, {"text": "ASPEC-CJ corpus", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.9083130955696106}]}, {"text": "For the Chinese-to-English MT task, we trained a 5-gram language model for English, on the training data of the NTCIR-CE corpus using the same method.", "labels": [], "entities": [{"text": "MT task", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.85513636469841}, {"text": "NTCIR-CE corpus", "start_pos": 112, "end_pos": 127, "type": "DATASET", "confidence": 0.9793798923492432}]}, {"text": "In all of our experiments, we used the GIZA++ toolkit 13 for word alignment; tuning was performed by minimum error rate training, and it was re-run for every experiment.", "labels": [], "entities": [{"text": "GIZA++ toolkit 13", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.885715663433075}, {"text": "word alignment", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.819511204957962}]}, {"text": "The significance tests were performed using the bootstrap resampling method).", "labels": [], "entities": []}, {"text": "We can see that the significant improvements on Chinese analysis due to the annotated treebank, also lead to the significant MT performance improvements.", "labels": [], "entities": [{"text": "Chinese analysis", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.8490189611911774}, {"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9347828030586243}]}, {"text": "Despite the language pair and slight domain difference, similar improvements are observed on both the ASPEC-CJ and NTCIR-CE MT tasks.", "labels": [], "entities": [{"text": "ASPEC-CJ", "start_pos": 102, "end_pos": 110, "type": "DATASET", "confidence": 0.7032445669174194}, {"text": "NTCIR-CE MT tasks", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.5294439792633057}]}], "tableCaptions": [{"text": " Table 1: Word segmentation results (\" \u2020\" indicates that the result is significantly better than \"Baseline\" at  p < 0.01).", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.68728107213974}]}, {"text": " Table 2: Joint segmentation and POS tagging results (\" \u2020\" indicates that the result is significantly better  than \"Baseline\" at p < 0.01).", "labels": [], "entities": [{"text": "Joint segmentation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8067739009857178}, {"text": "POS tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.7810044586658478}]}, {"text": " Table 3. As a reference, the parsing F-Measures from scratch for Baseline and Baseline+SCTB are  74.88% and 79.80% for 66 and 107 valid sentences (sentences that have the same segmentation as the  gold data), respectively.", "labels": [], "entities": [{"text": "F-Measures", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.8968573212623596}]}, {"text": " Table 3: Parsing results based on gold segmentations (\" \u2020\" indicates that the result is significantly better  than \"Baseline\" at p < 0.01).", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.8950162529945374}]}, {"text": " Table 4: BLEU-4 scores for ASPEC-CJ and NTCIR-CE translation tasks (\" \u2020\" indicates that the result is  significantly better than \"Baseline\" at p < 0.01).", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9991928935050964}, {"text": "NTCIR-CE translation tasks", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7885364691416422}]}]}