{"title": [{"text": "N-gram and Neural Language Models for Discriminating Similar Languages", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our submission (named clac) to the 2016 Discriminating Similar Languages (DSL) shared task.", "labels": [], "entities": [{"text": "2016 Discriminating Similar Languages (DSL) shared task", "start_pos": 56, "end_pos": 111, "type": "TASK", "confidence": 0.5221124788125356}]}, {"text": "We participated in the closed Sub-task 1 (Set A) with two separate machine learning techniques.", "labels": [], "entities": []}, {"text": "The first approach is a character based Convolution Neural Network with a bidirectional long short term memory (BiLSTM) layer (CLSTM), which achieved an accuracy of 78.45% with minimal tuning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9983848333358765}]}, {"text": "The second approach is a character-based n-gram model.", "labels": [], "entities": []}, {"text": "This last approach achieved an accuracy of 88.45% which is close to the accuracy of 89.38% achieved by the best submission, and allowed us to rank #7 overall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9995504021644592}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9994567036628723}]}], "introductionContent": [{"text": "Discriminating between languages is often the first task to many natural language applications (NLP), such as machine translation or information retrieval.", "labels": [], "entities": [{"text": "Discriminating between languages", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8661860624949137}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.821859210729599}, {"text": "information retrieval", "start_pos": 133, "end_pos": 154, "type": "TASK", "confidence": 0.7709926664829254}]}, {"text": "Current approaches to address this problem achieve impressive results in ideal conditions: a small number of unrelated or dissimilar languages, enough training data and long enough sentences.", "labels": [], "entities": []}, {"text": "For example, Sim\u00f5es et al. achieved an accuracy of 97% on the discrimination of 25 languages in TED talks).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9994114637374878}]}, {"text": "However, in the case of discriminating between similar languages or dialects, such as French Canadian and European French, or Spanish varieties, the task is more challenging.", "labels": [], "entities": []}, {"text": "This problem is addressed specifically in the DSL shared task at VarDial 2016.", "labels": [], "entities": [{"text": "DSL shared task at VarDial 2016", "start_pos": 46, "end_pos": 77, "type": "DATASET", "confidence": 0.708479772011439}]}, {"text": "In comparison to results from Sim\u00f5es et al. who achieved a 97% accuracy, the best performing system at DSL 2016 achieved only an 89.38% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9972983002662659}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9865054488182068}]}, {"text": "This paper describes our system and submission at the DSL 2016 shared task.", "labels": [], "entities": [{"text": "submission at the DSL 2016 shared task", "start_pos": 36, "end_pos": 74, "type": "TASK", "confidence": 0.6328846003328051}]}, {"text": "The shared task is split into two main sub-tasks.", "labels": [], "entities": []}, {"text": "Sub-task 1 aims at discriminating between similar languages and national language varieties; whereas Sub-task 2 focuses on Arabic dialect identification.", "labels": [], "entities": [{"text": "Arabic dialect identification", "start_pos": 123, "end_pos": 152, "type": "TASK", "confidence": 0.6613908608754476}]}, {"text": "We will only describe the specifics of Sub-task 1, for which we submitted results.", "labels": [], "entities": []}, {"text": "For Sub-task 1, participants could chose between the closed submission, where only the use of the DSL Corpus Collection, provided by the organisers (see Section 3), was allowed; or the open task which permitted the use of any external data for training.", "labels": [], "entities": [{"text": "DSL Corpus Collection", "start_pos": 98, "end_pos": 119, "type": "DATASET", "confidence": 0.9842797915140787}]}, {"text": "Participants could also submit runs for two different data sets: Set A, composed of newspaper articles, and Set B, composed of social media data.", "labels": [], "entities": []}, {"text": "We only participated in the closed Sub-task 1 using Set A. Hence, our task was to discriminate between 12 similar languages and national language varieties using only the newspaper articles provided in the DSL corpus as training set.", "labels": [], "entities": [{"text": "DSL corpus", "start_pos": 206, "end_pos": 216, "type": "DATASET", "confidence": 0.9735346436500549}]}, {"text": "For a full description of all sub-tasks, seethe overview paper ( , which also discusses data and results for all participants.", "labels": [], "entities": []}, {"text": "It was our first participation to the DSL task, and registered late to the shared task.", "labels": [], "entities": [{"text": "DSL task", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.6814448535442352}]}, {"text": "Hence our system is the result of a 3 person-week effort.", "labels": [], "entities": []}, {"text": "We started with very little existing code.", "labels": [], "entities": []}, {"text": "We had experimented previously with neural language models (NLM) and wanted to evaluate their applicability to this task.", "labels": [], "entities": []}, {"text": "In addition, we believed that a convolutional plus long-short term memory network (CLSTM) would be appropriate for the task given their success in several other NLP tasks (see Section 2 for details).", "labels": [], "entities": []}, {"text": "In the This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/ end, we managed to submit 3 runs: run1 and run2 consist of standard character-based n-gram models; while run3 is the CLSTM.", "labels": [], "entities": []}, {"text": "Our best performance was achieved by run1, with an accuracy of 88.45% ranking it 7 th among the 17 participants, and arriving very close to the top run which had an accuracy of 89.38%.", "labels": [], "entities": [{"text": "run1", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.7480815052986145}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.999408483505249}, {"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9990322589874268}]}, {"text": "Alas, our run3, the CLSTM, attained an accuracy of 78.45% but benefited from very minimal tuning.", "labels": [], "entities": [{"text": "CLSTM", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.8993093967437744}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9995785355567932}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of DSL 2016 Data set A. We list the number of instances across languages for the  Training, Development and Test sets. The last two columns represent the average number of characters  and average number of tokens of the training set.", "labels": [], "entities": [{"text": "DSL 2016 Data set A", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.9053882956504822}]}, {"text": " Table 2: Accuracy across n-grams of sizes 1 to 8 with the development Set A.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9978079199790955}]}, {"text": " Table 3: Layers used in our neural network. The Features column represents the number of filters for  the convolutional layers and hidden units for LSTM and Dense layers. Layers 4 and 5 are merged by  concatenation to form the BiLSTM layer. Dropout was added between layer 6 and the output layer (not  listed in the table).", "labels": [], "entities": []}, {"text": " Table 4: Results of our 3 submissions on test set A (closed training).", "labels": [], "entities": []}, {"text": " Table 5: Results for all systems, data set A, closed track. Our system \"clac\" ranked 7 th .", "labels": [], "entities": []}, {"text": " Table 6: Confusion matrix for the n-gram of size 7, test Set A. We also add the F1 score in the last  column.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9904691278934479}]}, {"text": " Table 7: Confusion matrix for the CLSTM, test Set A. We also add the F1 score in the last column.", "labels": [], "entities": [{"text": "CLSTM", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.86097651720047}, {"text": "F1 score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9891462922096252}]}]}