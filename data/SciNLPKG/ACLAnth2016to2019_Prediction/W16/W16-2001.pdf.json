{"title": [{"text": "Mining linguistic tone patterns with symbolic representation", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper conceptualizes speech prosody data mining and its potential application in data-driven phonology/phonetics research.", "labels": [], "entities": [{"text": "speech prosody data mining", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8055517822504044}]}, {"text": "We first conceptualize Speech Prosody Mining (SPM) in a time-series data mining framework.", "labels": [], "entities": [{"text": "Speech Prosody Mining (SPM)", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.7974874277909597}]}, {"text": "Specifically, we propose using efficient symbolic representations for speech prosody time-series similarity computation.", "labels": [], "entities": [{"text": "speech prosody time-series similarity computation", "start_pos": 70, "end_pos": 119, "type": "TASK", "confidence": 0.6686458468437195}]}, {"text": "We experiment with both symbolic and numeric representations and distance measures in a series of time-series classification and clustering experiments on a dataset of Mandarin tones.", "labels": [], "entities": []}, {"text": "Evaluation results show that symbolic representation performs comparably with other representations at a reduced cost, which enables us to efficiently mine large speech prosody corpora while opening up to possibilities of using a wide range of algorithms that require discrete valued data.", "labels": [], "entities": [{"text": "symbolic representation", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7721289694309235}]}, {"text": "We discuss the potential of SPM using time-series mining techniques in future works.", "labels": [], "entities": [{"text": "SPM", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9948498606681824}]}], "introductionContent": [{"text": "Current investigations on the phonology of intonation and tones (or pitch accent) typically employ data-driven approaches by building research on top of manual annotations of a large amount of speech prosody data (for example,, and many others).", "labels": [], "entities": []}, {"text": "Meanwhile, researchers are also limited by the amount of resources invested in such expensive endeavor of manual annotations.", "labels": [], "entities": []}, {"text": "Given this paradox, we believe that this type of data driven approach in phonology-phonetics interface can benefit from tools that can efficiently index, query, classify, cluster, summarize, and discover meaningful prosodic patterns from a large speech prosody corpus.", "labels": [], "entities": []}, {"text": "The data mining off 0 1 (pitch) contour patterns from audio data has recently gained success in the domain of Music Information Retrieval (aka MIR, see) for examples).", "labels": [], "entities": [{"text": "Music Information Retrieval (aka MIR", "start_pos": 110, "end_pos": 146, "type": "TASK", "confidence": 0.6597939729690552}]}, {"text": "In contrast, the data mining of speech prosody f 0 data (here on referred to as Speech Prosody Mining (SPM) 2 ) is a less explored research topic.", "labels": [], "entities": [{"text": "data mining of speech prosody f", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.7762015362580618}, {"text": "Speech Prosody Mining (SPM)", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.7558014343182246}]}, {"text": "Fundamentally, SPM in a large prosody corpus aims at discovering meaningful patterns in the f 0 data using efficient time-series data mining techniques adapted to the speech prosody domain.", "labels": [], "entities": [{"text": "SPM", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9888355135917664}]}, {"text": "Such knowledge has many potential applications in prosody-related tasks, including speech prosody modeling and speech recognition.", "labels": [], "entities": [{"text": "speech prosody modeling", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.8102084795633951}, {"text": "speech recognition", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.8601941466331482}]}, {"text": "Moreover, a Speech Prosody Query and Retrieval (SPQR) tool can be also of great utility to researchers in speech science and theoretical phonology/phonetics (tone and intonation).", "labels": [], "entities": [{"text": "Speech Prosody Query and Retrieval (SPQR)", "start_pos": 12, "end_pos": 53, "type": "TASK", "confidence": 0.7712214440107346}]}, {"text": "Due to the nature of speech prosody data, SPM in a large prosody corpus faces classic time-series data mining challenges such as high dimensionality, high feature correlation, and high time complexity in operations such as pair-wise distance computation.", "labels": [], "entities": [{"text": "SPM", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9866456389427185}]}, {"text": "Many of these challenges have been addressed in the time-series data mining literature by proposing heuristics that make use of cheaper and more efficient approximate representations of time-series (e.g., symbolic representations).", "labels": [], "entities": [{"text": "time-series data mining", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.6260483960310618}]}, {"text": "However, a central question to be addressed in SPM is how to adapt these generic techniques to develop the most efficient methods for computing similar-1 ity for the speech prosody time-series data (that also preserves the most meaningful information within this domain).", "labels": [], "entities": [{"text": "SPM", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9842228293418884}]}, {"text": "In this paper, we first conceptualize SPM in a time-series mining framework.", "labels": [], "entities": [{"text": "SPM", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9890908598899841}]}, {"text": "We outline the various components of SPM surrounding the central question of efficiently computing similarity for speech prosody time-series data.", "labels": [], "entities": [{"text": "SPM", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9920503497123718}]}, {"text": "In particular, we propose using Symbolic Aggregate approXimation (SAX) representation for time-series in various SPM tasks.", "labels": [], "entities": [{"text": "Symbolic Aggregate approXimation (SAX)", "start_pos": 32, "end_pos": 70, "type": "METRIC", "confidence": 0.7548484106858572}, {"text": "SPM tasks", "start_pos": 113, "end_pos": 122, "type": "TASK", "confidence": 0.9296418130397797}]}, {"text": "We then evaluate the use of SAX against several alternative representations for f 0 time-series in a series of classic data mining tasks using a data set of Mandarin tones().", "labels": [], "entities": []}, {"text": "Finally we discuss potential challenges and SPM applications to be addressed in future works.", "labels": [], "entities": [{"text": "SPM", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9904402494430542}]}], "datasetContent": [{"text": "Our evaluation data set of Mandarin tones is drawn from the () data used for unsupervised learning of Mandarin tones with the Self-Organizing Map.", "labels": [], "entities": []}, {"text": "This data set contains lab speech (480*4=1920 tones, three speakers each produced 160 instances of each of the four tone  The tonal environments introduce noisy deviation of tone shapes from tone templates, making tone recognition a mildly challenging task.", "labels": [], "entities": [{"text": "tone recognition", "start_pos": 214, "end_pos": 230, "type": "TASK", "confidence": 0.746320515871048}]}, {"text": "The target tones are spoken in a carrier sentence and later extracted as syllable-length subsequences.", "labels": [], "entities": []}, {"text": "shows the time-series representations and distance measures to be evaluated in this paper.", "labels": [], "entities": [{"text": "distance", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9479600191116333}]}, {"text": "Following conventions in time-series data mining literature, we evaluate these combinations using time-series classification and clustering.", "labels": [], "entities": [{"text": "time-series classification", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.6693282276391983}]}, {"text": "For classification, we use k-nearest neighbor (KNN) and Decision Tree, both of which are widely used in time-series classification . We report only accuracy on the classification experiments considering the balanced nature of the data set.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.961534857749939}, {"text": "time-series classification", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.6790326088666916}, {"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.999548614025116}]}, {"text": "All classification experiments are done with 10-fold cross validation with randomized split of data.", "labels": [], "entities": []}, {"text": "Following the convention of using a smaller training size in time-series data mining literature (and considering the optimal time complexity for splitting data size in KNN), the classification ex-  Mandarin tones (%) from 5 iterations.", "labels": [], "entities": []}, {"text": "For numeric representations, Euclidean distance is used by default unless otherwise noted periments are carried out using 1600 samples for testing and 320 samples for training (with total size of the data set being 1920 samples of tone contour time-series).", "labels": [], "entities": [{"text": "Euclidean distance", "start_pos": 29, "end_pos": 47, "type": "METRIC", "confidence": 0.6971336007118225}]}, {"text": "To optimize SAX parameters (with MINDIST distance measure), for w, we search from 6 up to 2n/3 (n is the length of the time series); fora, we search each value between 6 and 20.", "labels": [], "entities": [{"text": "SAX", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.7780430912971497}, {"text": "MINDIST distance measure", "start_pos": 33, "end_pos": 57, "type": "METRIC", "confidence": 0.9095005989074707}]}, {"text": "It is observed that low value fora results in poor classification results (since the MINDIST is a sum of pairwise letter distance between two strings).", "labels": [], "entities": [{"text": "MINDIST", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9505306482315063}]}, {"text": "shows how classification accuracy varies depending on wand a.", "labels": [], "entities": [{"text": "classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9214848279953003}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9635596871376038}]}, {"text": "For clustering experiments we use the k-means clustering algorithm, where accuracy is computed against true tone labels for each instance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9990449547767639}]}], "tableCaptions": [{"text": " Table 2: K-Nearest Neighbor tone classification results, with 10-fold cross validation (CR=compression  rate, TSR=time-series representation, DIST=distance measure, EU=Euclidean Distance, SAX parame- ters (w,a)=(20,17), test size=1600, training size=320)", "labels": [], "entities": [{"text": "K-Nearest Neighbor tone classification", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.5720035880804062}, {"text": "CR=compression  rate", "start_pos": 89, "end_pos": 109, "type": "METRIC", "confidence": 0.8784262984991074}, {"text": "TSR", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9334885478019714}, {"text": "DIST=distance measure", "start_pos": 143, "end_pos": 164, "type": "METRIC", "confidence": 0.7953858524560928}, {"text": "EU=Euclidean Distance", "start_pos": 166, "end_pos": 187, "type": "METRIC", "confidence": 0.9066930860280991}, {"text": "SAX parame- ters", "start_pos": 189, "end_pos": 205, "type": "METRIC", "confidence": 0.8123573511838913}]}, {"text": " Table 3: Decision tree classification results (with  10-fold cross validation), CR=compression rate", "labels": [], "entities": [{"text": "Decision tree classification", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7147182027498881}, {"text": "CR", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9981319308280945}, {"text": "compression rate", "start_pos": 84, "end_pos": 100, "type": "METRIC", "confidence": 0.9764222800731659}]}]}