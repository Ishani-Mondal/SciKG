{"title": [{"text": "Clinical Text Prediction with Numerically Grounded Conditional Language Models", "labels": [], "entities": [{"text": "Clinical Text Prediction", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.547370711962382}]}], "abstractContent": [{"text": "Assisted text input techniques can save time and effort and improve text quality.", "labels": [], "entities": []}, {"text": "In this paper , we investigate how grounded and conditional extensions to standard neural language models can bring improvements in the tasks of word prediction and completion.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 145, "end_pos": 160, "type": "TASK", "confidence": 0.7870710492134094}]}, {"text": "These extensions incorporate a structured knowledge base and numerical values from the text into the context used to predict the next word.", "labels": [], "entities": []}, {"text": "Our automated evaluation on a clinical dataset shows extended models significantly outper-form standard models.", "labels": [], "entities": []}, {"text": "Our best system uses both conditioning and grounding, because of their orthogonal benefits.", "labels": [], "entities": []}, {"text": "For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8320358693599701}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9991825222969055}, {"text": "word completion", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.7849871814250946}]}, {"text": "We also perform a qualitative investigation of how models with lower perplexity occasionally fare better at the tasks.", "labels": [], "entities": []}, {"text": "We found that attest time numbers have more influence on the document level than on individual word probabilities.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text prediction is the task of suggesting the next word, phrase or sentence while the user is typing.", "labels": [], "entities": [{"text": "Text prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7960372865200043}]}, {"text": "It is an assisted data entry function that aims to save time and effort by reducing the number of keystrokes needed and to improve text quality by preventing misspellings, promoting adoption of standard terminologies and allowing for exploration of the vocabulary).", "labels": [], "entities": []}, {"text": "Text prediction originated in augmentative and alternative communication (AAC) to increase text generation rates for people with motor or speech impairments).", "labels": [], "entities": [{"text": "Text prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8221718668937683}, {"text": "augmentative and alternative communication (AAC)", "start_pos": 30, "end_pos": 78, "type": "TASK", "confidence": 0.7102482233728681}, {"text": "text generation", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.6916121542453766}]}, {"text": "Its scope has been extended to a gamut of applications, such as data entry in mobile devices), interactive machine translation (), search term auto-completion () and assisted clinical report compilation (.", "labels": [], "entities": [{"text": "interactive machine translation", "start_pos": 95, "end_pos": 126, "type": "TASK", "confidence": 0.6728443503379822}, {"text": "assisted clinical report compilation", "start_pos": 166, "end_pos": 202, "type": "TASK", "confidence": 0.5990395247936249}]}, {"text": "In this paper, we explore the tasks of word prediction, where a system displays a list of suggestions for the next word before the user starts typing it, and word completion, where the system suggests a single possible completion for the word, while the user is typing its characters.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7929924726486206}, {"text": "word completion", "start_pos": 158, "end_pos": 173, "type": "TASK", "confidence": 0.7987067997455597}]}, {"text": "The former task is relevant when the user has not yet made a firm decision about the intended word, thus any suggestions can have a great impact in the content of the final document.", "labels": [], "entities": []}, {"text": "In the latter case, the user is thinking of a particular word that they want to input and the system's goal is to help them complete the word as quickly as possible.", "labels": [], "entities": []}, {"text": "shows examples for both tasks.", "labels": [], "entities": []}, {"text": "Often, the user's goal is to compose a document describing a particular situation, e.g. a clinical report about a patient's condition.", "labels": [], "entities": []}, {"text": "An intelligent predictive system should be able to account for such contextual information in order to improve the quality of its suggestions.", "labels": [], "entities": []}, {"text": "Challenges to modelling structured contexts include mixed types of values for the different fields an schema inconsistencies across the entries of the structure.", "labels": [], "entities": []}, {"text": "We address these issues by employing numerically grounded conditional language models (.", "labels": [], "entities": []}, {"text": "The contribution of this work is twofold.", "labels": [], "entities": []}, {"text": "First, we show that conditional and numerically grounded models can achieve significant improvements over standard language models in the tasks of word prediction and completion.", "labels": [], "entities": [{"text": "word prediction and completion", "start_pos": 147, "end_pos": 177, "type": "TASK", "confidence": 0.7635693848133087}]}, {"text": "Our best model with a list of 5 suggestions raises recall from 25.03% to 71.28% and keystroke savings from 34.35% to 44.81%.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9995866417884827}, {"text": "keystroke", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9810011982917786}]}, {"text": "Second, we investigate in depth the behaviour of such models and their sensitivity to the numerical values in the text.", "labels": [], "entities": []}, {"text": "We find that the grounded probability for the whole document is more sensitive to numerical configurations than the probabilities of individual words.", "labels": [], "entities": []}], "datasetContent": [{"text": "We run an automated evaluation for both tasks and all systems by simulating a user who types the text character by character.", "labels": [], "entities": []}, {"text": "The character stream comes from a dataset of finalised clinical reports.", "labels": [], "entities": []}, {"text": "For the word prediction task, we assume that the word from the dataset is the correct word.", "labels": [], "entities": [{"text": "word prediction task", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.8396672209103903}]}, {"text": "For the word completion task, we assume that the user types the special key to autocomplete the word as soon as the correct suggestion becomes available.", "labels": [], "entities": [{"text": "word completion task", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.8451253573099772}]}, {"text": "In practice, the two tasks can be tackled at the same time, e.g. a list of suggestions based on a language model is shown as the user types and they can choose to complete the prefix with the word on the top of the list.", "labels": [], "entities": []}, {"text": "However, we chose to decouple the two functions because of their conceptual differences, which call for different evaluation metrics.", "labels": [], "entities": []}, {"text": "For word prediction, the user has not yet started typing and they might seek guidance in the suggestions of the system for their final decision.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8572812080383301}]}, {"text": "A vocabulary exploration system will need to have a high recall.", "labels": [], "entities": [{"text": "vocabulary exploration", "start_pos": 2, "end_pos": 24, "type": "TASK", "confidence": 0.8183772265911102}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9985969662666321}]}, {"text": "To also capture the effect of the length of the suggestions' list, we will report recall at various ranks (Recall@k), where the rank corresponds to the list length.", "labels": [], "entities": [{"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9979992508888245}, {"text": "Recall@k)", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.8855199068784714}]}, {"text": "Because our automated evaluation considers a single correct word, Recall@1 is numerically identical to Precision@1.", "labels": [], "entities": []}, {"text": "We also report the mean reciprocal rank (MRR), which is the multiplicative inverse of the rank of the correct word in the suggestions' list.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 19, "end_pos": 45, "type": "METRIC", "confidence": 0.9050144056479136}]}, {"text": "Finally, per token perplexity is train dev test a common evaluation metric for language models.", "labels": [], "entities": []}, {"text": "For word completion, the main goal of the system should be to reduce input time and effort for the intended word that is being typed by the user.", "labels": [], "entities": [{"text": "word completion", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8344608843326569}]}, {"text": "Keystroke savings (KS) measures the percentage reduction in keys pressed compared to character-bycharacter text entry.", "labels": [], "entities": [{"text": "Keystroke savings (KS)", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.8427192151546479}]}, {"text": "Suggestions that are not taken by the user area source of unnecessary distractions.", "labels": [], "entities": []}, {"text": "We define an unnecessary distractions (UD) metric as average number of unaccepted character suggestions that the user has to scan before completing a word.", "labels": [], "entities": [{"text": "unnecessary distractions (UD) metric", "start_pos": 13, "end_pos": 49, "type": "METRIC", "confidence": 0.7622294674317042}]}], "tableCaptions": [{"text": " Table 1: Statistics for clinical dataset. Counts for non-numeric", "labels": [], "entities": []}, {"text": " Table 2: Word-level evaluation results for next word prediction on the test set. Perplexity (PP), mean reciprocal rank (MRR) and", "labels": [], "entities": [{"text": "Perplexity (PP)", "start_pos": 82, "end_pos": 97, "type": "METRIC", "confidence": 0.9350670278072357}, {"text": "mean reciprocal rank (MRR)", "start_pos": 99, "end_pos": 125, "type": "METRIC", "confidence": 0.9453529814879099}]}, {"text": " Table 3: Character-level evaluation results for word comple-", "labels": [], "entities": []}, {"text": " Table 2. The conditioned  model (+c) achieves double the MRR and quadruple", "labels": [], "entities": [{"text": "MRR", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9948421120643616}]}, {"text": " Table 4: Word prediction for sample document from the development set. Top-5 suggestion lists for <word> (original document", "labels": [], "entities": [{"text": "Word prediction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7306631207466125}]}, {"text": " Table 5: Document probabilities for different <word> choices", "labels": [], "entities": []}]}