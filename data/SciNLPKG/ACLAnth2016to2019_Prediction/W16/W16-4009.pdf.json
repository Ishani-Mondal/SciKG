{"title": [{"text": "Automatic parsing as an efficient pre-annotation tool for historical texts", "labels": [], "entities": [{"text": "Automatic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6480877697467804}]}], "abstractContent": [{"text": "Historical treebanks tend to be manually annotated, which is not surprising, since state-of-the-art parsers are not accurate enough to ensure high-quality annotation for historical texts.", "labels": [], "entities": [{"text": "Historical treebanks", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.8247519135475159}]}, {"text": "We test whether automatic parsing can bean efficient pre-annotation tool for Old East Slavic texts.", "labels": [], "entities": []}, {"text": "We use the TOROT treebank from the PROIEL treebank family.", "labels": [], "entities": [{"text": "TOROT treebank", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.7651097178459167}, {"text": "PROIEL treebank family", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.9176163077354431}]}, {"text": "We convert the PROIEL format to the CONLL format and use MaltParser to create syntactic pre-annotation.", "labels": [], "entities": [{"text": "CONLL format", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9051603674888611}]}, {"text": "Using the most conservative evaluation method, which takes into account PROIEL-specific features, MaltParser by itself yields 0.845 unlabelled attachment score, 0.779 labelled attachment score and 0.741 secondary dependency accuracy (note, though, that the test set comes from a relatively simple genre and contains rather short sentences).", "labels": [], "entities": [{"text": "labelled attachment score", "start_pos": 167, "end_pos": 192, "type": "METRIC", "confidence": 0.7814583579699198}, {"text": "accuracy", "start_pos": 224, "end_pos": 232, "type": "METRIC", "confidence": 0.5191112160682678}]}, {"text": "Experiments with human annotators show that preparsing, if limited to sentences where no changes to word or sentence boundaries are required, increases their annotation rate.", "labels": [], "entities": []}, {"text": "For experienced annotators, the speed gain varies from 5.80% to 16.57%, for inexperienced annotators from 14.61% to 32.17% (using conservative estimates).", "labels": [], "entities": [{"text": "speed gain", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9923497140407562}]}, {"text": "There are no strong reliable differences in the annotation accuracy, which means that there is no reason to suspect that using preparsing might lower the final annotation quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.7004396915435791}]}], "introductionContent": [{"text": "Parsing historical texts is a complicated venture.", "labels": [], "entities": [{"text": "Parsing historical texts", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9150459965070089}]}, {"text": "One challenge is high variation on all levels both across and within texts, in particular the absence of standardised spelling.", "labels": [], "entities": [{"text": "variation", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.967312216758728}]}, {"text": "Another is the small number of texts available in digital form, and the even smaller amount of annotated resources which could facilitate the development of new tools.", "labels": [], "entities": []}, {"text": "Moreover, the overall amount of existing texts can be small too, which means that the gain achieved by developing highly specialised tools can be limited.", "labels": [], "entities": []}, {"text": "In the meantime, historical linguists usually expect their corpora to have high-quality annotation, and tend to be less tolerant towards errors than computational linguists on average, which is probably reasonable, given the relatively small sizes of the corpora.", "labels": [], "entities": []}, {"text": "With this in mind, it is not surprising that historical treebanks tend to be manually annotated.", "labels": [], "entities": []}, {"text": "One way to make use of automatic annotation would be to develop parsers that can handle historical texts.", "labels": [], "entities": []}, {"text": "Another would be use off-the-shelf tools for pre-annotation and then correct their output manually.", "labels": [], "entities": []}, {"text": "In this paper, we test whether the latter approach is efficient for Old East Slavic (also known as Old Russian) texts.", "labels": [], "entities": []}, {"text": "The idea to combine pre-annotation with subsequent manual correction is, of course, not at all new.", "labels": [], "entities": []}, {"text": "It has been used, for instance, in the development of the TIGER treebank of German (), the SynTagRus treebank of Russian () and ICEPAHC, the diachronic treebank of Icelandic ().", "labels": [], "entities": [{"text": "TIGER treebank", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.7210978418588638}, {"text": "SynTagRus treebank", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.7073978334665298}]}, {"text": "We are not, however, aware of any systematic evaluation of whether this routine is more efficient than a fully manual annotation with respect to historical texts.", "labels": [], "entities": []}, {"text": "In section 2, we describe the treebank we use for this purpose.", "labels": [], "entities": []}, {"text": "In section 3, we outline the format conversions we have to perform and the technical details of our parsing experiments.", "labels": [], "entities": [{"text": "format conversions", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8212425410747528}, {"text": "parsing", "start_pos": 100, "end_pos": 107, "type": "TASK", "confidence": 0.9641504287719727}]}, {"text": "In section 4, we describe a parsing experiment in an idealised setting, where the test set has manually corrected morphological annotation and lemmatisation.", "labels": [], "entities": [{"text": "parsing experiment", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8847789466381073}]}, {"text": "In section 5, we move to realistic experiments, where the parser has to deal with texts that have not been manually corrected.", "labels": [], "entities": []}, {"text": "We present the results in section 6 and conclude in section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Parsing accuracy, test set", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9613717794418335}]}, {"text": " Table 3: Sentence length in selected and non-selected sentences", "labels": [], "entities": [{"text": "Sentence length", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7104812264442444}]}, {"text": " Table 4: Overview of selected sentences, accuracy of lemmatisation and morphological preprocessing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9990743398666382}]}, {"text": " Table 5: Annotation speed gain, inexperienced annotators, tokens per minute", "labels": [], "entities": [{"text": "Annotation speed gain", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9407515327135721}]}, {"text": " Table 6: Annotation speed, tokens per minute", "labels": [], "entities": [{"text": "Annotation speed", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9416797757148743}]}, {"text": " Table 7: Parsing accuracy, Batch 1", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.7000365853309631}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.8952398896217346}]}, {"text": " Table 8: Parsing accuracy, Batch 2", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.6524721384048462}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.8849064707756042}, {"text": "Batch", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.762213408946991}]}]}