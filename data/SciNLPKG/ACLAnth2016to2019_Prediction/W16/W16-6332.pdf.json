{"title": [{"text": "Use of Semantic Knowledge Base for Enhancement of Coherence of Code-mixed Topic-Based Aspect Clusters", "labels": [], "entities": []}], "abstractContent": [{"text": "In social media code-mixing is getting very popular due to which there is enormous generation of noisy and sparse multilingual text which exhibits high dispersion of useful topics which people discuss.", "labels": [], "entities": []}, {"text": "Also, the semantics is expressed across random occurrence of code-mixed words.", "labels": [], "entities": []}, {"text": "In this paper, we propose code-mixed knowledge based LDA (cmk-LDA), which infers latent topic based aspects from code-mixed social media data.", "labels": [], "entities": []}, {"text": "We experimented on FIRE 2014, a code-mixed corpus and showed that with the help of semantic knowledge from multilingual external knowledge base, cmk-LDA learns coherent topic-based aspects across languages and improves topic inter-pretibility and topic distinctiveness better than the baseline models.", "labels": [], "entities": [{"text": "FIRE 2014", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.8417837619781494}]}, {"text": "The same is shown to have agreed with human judgment .", "labels": [], "entities": []}], "introductionContent": [{"text": "The huge amount of social media text available online is becoming increasingly popular thereby providing an additional opportunity of mining useful information from it.", "labels": [], "entities": []}, {"text": "Therefore, most of the research on social media text has concentrated on English chat data or on multilingual data where each message as a component is monolingual.", "labels": [], "entities": []}, {"text": "In social media, people often switch between two or more languages, both at conversation level and at message level ().", "labels": [], "entities": []}, {"text": "However, majority of conversational data on social networking forums is informal and occurs in random mix of languages (.", "labels": [], "entities": []}, {"text": "When this code alternation occurs at or above the utterance level, the phenomenon is referred to as codeswitching; when the alternation is utterance internal, the term code-mixing is common.", "labels": [], "entities": []}, {"text": "Thus, code-mixing while chatting has become prevalent in current times.", "labels": [], "entities": []}, {"text": "However, exponentially increasing large volumes of short and long code-mixed messages contain lot of noise and has useful information highly dispersed.", "labels": [], "entities": []}, {"text": "Unfortunately, it is not an easy task to retrieve useful knowledge from such data as code-mixing occurs at different levels of code-complexity and imposes fundamental challenges namely: 1.", "labels": [], "entities": []}, {"text": "Code-mixed social media data is multilingual, usually bilingual.", "labels": [], "entities": []}, {"text": "Therefore, semantics is spread across languages.", "labels": [], "entities": []}, {"text": "2. Social media data do not have specific terminology.", "labels": [], "entities": []}, {"text": "Therefore, using training data from parallel or comparable corpora will not be useful in this context.", "labels": [], "entities": []}, {"text": "Also, availability of pre annotated corpora for all language pairs used in social media may practically be very difficult to obtain.", "labels": [], "entities": []}, {"text": "Our objective is to model unsupervised aspect extraction using topics, from the code-mixed context to obtain useful knowledge.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7336540520191193}]}, {"text": "Probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA)() are popularly recommended unsupervised topic modeling methods for this purpose; but a shortcoming with them is that they result in extracting some incoherent topics ().This is due to the occurrence of some irrelevant and polysemous terms in extraction (, which is likely to get aggregated in multilingual content.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)()", "start_pos": 50, "end_pos": 85, "type": "METRIC", "confidence": 0.8019373814264933}]}, {"text": "Therefore, in code-mixed context extraction of incoherent topics are highly likely to occur.", "labels": [], "entities": [{"text": "code-mixed context extraction of incoherent topics", "start_pos": 14, "end_pos": 64, "type": "TASK", "confidence": 0.7488432625929514}]}, {"text": "In order to crossover the word level language barrier and augment each word with its semantic description we propose to leverage knowledge from external multilingual semantic knowledge base such as BabelNet v3.0 1 () which is created from integration of both.", "labels": [], "entities": []}, {"text": "Our approach of incorporating semantic knowledge in LDA topic model has resemblance with General Knowledge based LDA (GK-LDA)) model.", "labels": [], "entities": []}, {"text": "However, their sets were monolingual and were constructed using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9744826555252075}]}, {"text": "They called their semantic sets as Lexical Relation Sets(LR-sets) and were comprising of synonym, antonym and adjective relations.", "labels": [], "entities": []}, {"text": "They addressed the problem of multiple senses using synonyms and antonyms in LR-Sets.", "labels": [], "entities": []}, {"text": "However, in their work since semantic knowledge is augmented at word level not all LR-sets resulted incorrect context knowledge.", "labels": [], "entities": []}, {"text": "They handled this problem by using explicit word-correlation matrix and also fixed the wrong knowledge explicitly.", "labels": [], "entities": []}, {"text": "But we take a different approach.", "labels": [], "entities": []}, {"text": "Ina single step, we obtain disambiguated synsets fora code-mixed message, retrieving correct multilingual synsets appropriate to the context across languages.", "labels": [], "entities": []}, {"text": "This also resolves language related multiple senses issue.", "labels": [], "entities": []}, {"text": "As per our knowledge, our proposed model is the first model to exploit semantic knowledge from BabelNet in topic models for producing coherent topics from code-mixed data.", "labels": [], "entities": []}, {"text": "The remaining part of the paper is organized as follows: Section 2 presents related work, Section 3 describes our proposed work, Section 4 gives implementation details, Section 5 presents experimental results and Section 6 states the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments on FIRE 2014 3 (Forum for IR Evaluation) for shared task on transliterated search.", "labels": [], "entities": [{"text": "FIRE 2014 3 (Forum for IR Evaluation)", "start_pos": 28, "end_pos": 65, "type": "DATASET", "confidence": 0.8332908815807767}]}, {"text": "This dataset comprises of social media posts in English mixed with six other Indian languages.The English-Hindi corpora from FIRE 2014 was introduced by.", "labels": [], "entities": [{"text": "FIRE 2014", "start_pos": 125, "end_pos": 134, "type": "DATASET", "confidence": 0.8842959105968475}]}, {"text": "It consists of 700 messages with the total of 23,967 words which were taken from Facebook chat group for Indian University students.", "labels": [], "entities": [{"text": "Indian University students", "start_pos": 105, "end_pos": 131, "type": "DATASET", "confidence": 0.9392685294151306}]}, {"text": "The data contained 63.33% of tokens in Hindi.", "labels": [], "entities": []}, {"text": "The overall code-mixing percentage for English-Hindi corpus was as high as 80% due to the frequent slang used in two languages randomly during the chat ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cohens Kappa for inter-rater agreement  Index  1  2  3  4  5  k  3  6  9  12  15  Precision@k 0.899 0.798 0.876 0.712 0.766", "labels": [], "entities": [{"text": "Precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.971051037311554}]}]}