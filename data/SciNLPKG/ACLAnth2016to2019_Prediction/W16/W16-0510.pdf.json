{"title": [{"text": "Unsupervised Modeling of Topical Relevance in L2 Learner Text", "labels": [], "entities": [{"text": "Unsupervised Modeling of Topical Relevance", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.5674742758274078}]}], "abstractContent": [{"text": "The automated scoring of second-language (L2) learner text along various writing dimensions is an increasingly active research area.", "labels": [], "entities": [{"text": "automated scoring of second-language (L2) learner text", "start_pos": 4, "end_pos": 58, "type": "TASK", "confidence": 0.732915523979399}]}, {"text": "In this paper, we focus on determining the topical relevance of an essay to the prompt that elicited it.", "labels": [], "entities": []}, {"text": "Given the burden involved in manually assigning scores for use in training supervised prompt-relevance models, we develop unsupervised models and show that they correlate well with human judgements.", "labels": [], "entities": []}, {"text": "We show that expanding prompts using topically-related words, via pseudo-relevance modelling, is beneficial and outperforms other distributional techniques.", "labels": [], "entities": []}, {"text": "Finally, we incorporate our prompt-relevance models into a supervised essay scoring system that predicts a holistic score and show that it improves its performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given the increase in demand for educational tools and aids for L2 learners of English, the automated scoring of learner texts according to a number of predetermined dimensions (e.g., grammaticality and lexical variety) is an increasingly important research area.", "labels": [], "entities": []}, {"text": "While a number of early approaches and recent competitions 1) have sought to assign a holistic score to an entire essay, it is more informative to give detailed feedback to learners by assigning individual scores across each such writing dimension.", "labels": [], "entities": []}, {"text": "1 https://www.kaggle.com/c/asap-aes This more specific feedback facilitates reflection both on learners' strengths and weaknesses, and focuses attention on the aspects of writing that need improvement.", "labels": [], "entities": []}, {"text": "Recent work outlines a number of broad competencies that systems should assess.", "labels": [], "entities": []}, {"text": "These include morphology, syntax, semantics, discourse, and stylistics, noting that the specific assessment tasks that might aim to measure these areas of competency may vary.", "labels": [], "entities": []}, {"text": "One dimension against which apiece of text is often scored is that of topical relevance.", "labels": [], "entities": []}, {"text": "That is, determining if a learner has understood and responded adequately to the prompt.", "labels": [], "entities": []}, {"text": "This aspect of automated writing assessment has received considerably less attention than holistic scoring.", "labels": [], "entities": [{"text": "automated writing assessment", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.6595373153686523}]}, {"text": "Topical relevance is not so much concerned with whether an L2 learner has constructed grammatically correct and well-organised sentences, as it is concerned with whether the learner has understood the prompt and attempted a response with appropriate vocabulary.", "labels": [], "entities": [{"text": "Topical relevance", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.844408392906189}]}, {"text": "Other reasons for measuring the topical relevance of a text include the detection of malicious submissions, that is, detecting submissions that have been rote-learned or memorised specifically for assessment situations).", "labels": [], "entities": []}, {"text": "In this paper, we employ techniques from the area of distributional semantics and information retrieval (IR) to develop unsupervised promptrelevance models, and demonstrate that they correlate well with human judgements.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.8079632520675659}]}, {"text": "In particu-lar, we study four different methods of expanding a prompt with with topically-related words and show that some are more beneficial than others at overcoming the 'vocabulary mismatch' problem which is typically present in free-text learner writing.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there have been no attempts at a comparative study investigating the effectiveness of such techniques on the automatic prediction of a topical-relevance score in the noisy domain of learner texts, where grammatical errors are common.", "labels": [], "entities": []}, {"text": "In addition, we perform an external evaluation to measure the extent to which prompt-relevance informs () the holistic score.", "labels": [], "entities": []}, {"text": "The remainder of the paper is outlined as follows: Section 2 discusses related work and outlines our contribution.", "labels": [], "entities": []}, {"text": "Section 3 presents our framework and four unsupervised approaches to measuring semantic similarity.", "labels": [], "entities": []}, {"text": "Section 4 presents both quantitative and qualitative evaluations for all of the methods employed in this paper.", "labels": [], "entities": []}, {"text": "Section 5 performs an external evaluation by incorporating the best promptrelevance model as features into a supervised preference ranking approach.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes with a discussion and outline of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present results on the effectiveness of the unsupervised approaches for the task of assessing the prompt relevance of an essay.", "labels": [], "entities": []}, {"text": "For this experiment, we used a dataset consisting of 2,316 essays written for the IELTS (International English Language Testing System) English examination from 2005 to 2010.", "labels": [], "entities": [{"text": "IELTS (International English Language Testing System) English examination from 2005", "start_pos": 82, "end_pos": 165, "type": "DATASET", "confidence": 0.7518086408575376}]}, {"text": "The examination is designed to measure abroad proficiency continuum ranging from an intermediate to a proficient level of English (A2 to C2 in the CEFR levels).", "labels": [], "entities": [{"text": "A2", "start_pos": 131, "end_pos": 133, "type": "METRIC", "confidence": 0.9813051819801331}, {"text": "CEFR levels", "start_pos": 147, "end_pos": 158, "type": "DATASET", "confidence": 0.9390791356563568}]}, {"text": "The essays are associated with 22 prompts that are similar in style (i.e. essay style) to those in the ICLE dataset.", "labels": [], "entities": [{"text": "ICLE dataset", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.9151950478553772}]}, {"text": "Candidates are assigned an overall score on a scale from 1 to 9.", "labels": [], "entities": []}, {"text": "Prompt relevance is an aspect that is present in the marking criteria, and it is identified as a determinant of the overall score.", "labels": [], "entities": [{"text": "Prompt relevance", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9359027147293091}]}, {"text": "We therefore hypothesise that adding prompt-relevance measures to the feature set of a prompt-independent essay scoring system (i.e. that is designed to assess linguistic competence only) would better reflect the evaluation performed by examiners and improve system performance.", "labels": [], "entities": []}, {"text": "The baseline system is a linear preference ranking model and is trained to predict an overall essay score based on the following set of features: -word unigrams, bigrams, and trigrams -POS (part-of-speech) counts -grammatical relations -essay length (# of unique words) -counts of cohesive devices -max-word length and min-sentence length -number of errors based on a presence/absence trigram language model We divided the dataset into 5-folds in two separate ways.", "labels": [], "entities": []}, {"text": "First, we created prompt-dependent folds, where essays associated with all 22 prompts appear in both the training and test data in the appropriate proportions.", "labels": [], "entities": []}, {"text": "This scenario allows the system to learn from essays that were written in response to the prompt.", "labels": [], "entities": []}, {"text": "Second, we created promptindependent folds, where all essays associated with a specific prompt appear in only one fold.", "labels": [], "entities": []}, {"text": "This second dataset is a more realistic real-world scenario (see Section 2) whereby the system learns on one set of prompts (possibly from previous years) and aims to predict the score for essays associated with different prompts.", "labels": [], "entities": []}, {"text": "For both of these supervised experiments, we measured system performance using Spearman's and Pearson's correlation between the output of the system and the gold essay scores (human judgements).", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 94, "end_pos": 115, "type": "METRIC", "confidence": 0.8548256953557333}]}, {"text": "In order to examine the effect of prompt relevance on these datasets, we added to our baseline system two sets of features.", "labels": [], "entities": []}, {"text": "The first set of features labelled PR includes the cosine similarity between the essay and the prompt cos(p, s), the fraction of essays words that appear in the prompt cov(p, s), and the fraction of prompt words that appear in the essay cov(s, p).", "labels": [], "entities": []}, {"text": "The second set of features labelled semPR is the same as the first set except that the prompt is expanded using the PRF method from earlier.", "labels": [], "entities": []}, {"text": "We believe the main reason that the PRF approach outperforms the others is that topicality is a quality that spans larger segments of text (e.g. docu-ments).", "labels": [], "entities": []}, {"text": "For the other approaches, the words that are promoted are very close in proximity to the prompt words (due to the smaller context sizes), and this is more likely to capture local aspects of word usage.", "labels": [], "entities": []}, {"text": "Furthermore, in the PRF approach the most important contexts are those in which all prompt words appear together, and this aids automatic disambiguation.", "labels": [], "entities": []}, {"text": "Regardless, due to the empirical results in the previous section and the perceived topical quality of the terms from the PRF approach, we make use of the PRF approach as a feature in the next experiment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of ICLE essays over score grades.", "labels": [], "entities": [{"text": "Distribution of ICLE essays", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.842298299074173}]}, {"text": " Table 2: Correlation (Spearman's \u03c1) between prompt-essay similarity scores and human annotations for each prompt (higher values", "labels": [], "entities": [{"text": "Spearman's \u03c1)", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.6978253498673439}]}, {"text": " Table 3: The top 10 non-prompt words and their similarity to the prompt in a lemmatised Wikipedia corpus of 4.4M documents.", "labels": [], "entities": []}, {"text": " Table 4: Performance of systems using 5-fold cross-validation", "labels": [], "entities": []}]}