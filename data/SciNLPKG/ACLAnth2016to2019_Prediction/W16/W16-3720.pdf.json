{"title": [{"text": "Learning Indonesian-Chinese Lexicon with Bilingual Word Embedding Models and Monolingual Signals", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a research on learning Indonesian-Chinese bilingual lexicon using monolingual word embedding and bilingual seed lexicons to build shared bilingual word embedding space.", "labels": [], "entities": []}, {"text": "We take the first attempt to examine the impact of different monolingual signals for the choice of seed lexicons on the model performance.", "labels": [], "entities": []}, {"text": "We found that although monolingual signals alone do not seem to outperform signals coverings all words, the significant improvement for learning word translation of the same signal types may suggest that linguistic features possess value for further study in distinguishing the semantic margins of the shared word embedding space.", "labels": [], "entities": [{"text": "learning word translation", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.6549615561962128}]}], "introductionContent": [{"text": "We explore the latest development of bilingual lexicon learning (BLL) research and investigate their application on inducing Indonesian-Chinese lexicon.", "labels": [], "entities": [{"text": "bilingual lexicon learning (BLL)", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.6954066753387451}]}, {"text": "In particular, due to the limitation of parallel and comparable Indonesian-Chinese bilingual corpora, we study the state-of-the-art bilingual word embedding (BWE) models built with seed lexicons and monolingual corpora to project Indonesian and Chinese word pairs onto the same transformed space.", "labels": [], "entities": []}, {"text": "We further explore the impact of Indonesian linguistic signals on these models to provide insights on the implications of monolingual signals and challenges for bilingual lexicon learning Bilingual word embedding models have proven to be effective in many cross-lingual tasks such as document classification, POS tagging, and phrase generation.", "labels": [], "entities": [{"text": "document classification", "start_pos": 284, "end_pos": 307, "type": "TASK", "confidence": 0.7675179243087769}, {"text": "POS tagging", "start_pos": 309, "end_pos": 320, "type": "TASK", "confidence": 0.9251032471656799}, {"text": "phrase generation", "start_pos": 326, "end_pos": 343, "type": "TASK", "confidence": 0.8079476654529572}]}, {"text": "As illustrated in, two sets of words (numbers and animals) in two languages (English and Spanish) have similar geometric arrangements.", "labels": [], "entities": []}, {"text": "This is achieved by constructing wore embedding vectors for both languages and projecting the vectors down into two dimensions, rotated to show similarity.", "labels": [], "entities": []}, {"text": "The Figure demonstrates that the relations between words are similar across languages.", "labels": [], "entities": []}, {"text": "This finding inspired a series of research on generating a bilingual dictionary with cross-lingual word embedding space.", "labels": [], "entities": []}, {"text": "The general steps involve 1) building a word space for each individual language; 2) projecting the two spaces into one shared space or from one to the other; and 3) learning or retrieving the target language word most similar to the source language word in the projection.", "labels": [], "entities": []}, {"text": "Our paper attempts to contribute to this line of research by examining the monolingual signals from Indonesian in building the bilingual word embedding model.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review the latest development in BLL with BWE.", "labels": [], "entities": [{"text": "BLL", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.3537747263908386}, {"text": "BWE", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.49615025520324707}]}, {"text": "We summarize the related research and propose our research questions.", "labels": [], "entities": []}, {"text": "In Section 3, we discuss details of our methodologies.", "labels": [], "entities": []}, {"text": "We present our data preparation, experiment design, results and analysis in Section 4, and conclude with Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "For building monolingual word embedding models, we use Chinese and Indonesian Wikipedia articles as training set.", "labels": [], "entities": []}, {"text": "We collected and processed the Chinese Wikipedia dump of Aug. 1 2016 and the Indonesian Wikipedia dump of July 20, 2016 and generate 727k Chinese word vectors and 190k Indonesian vectors.", "labels": [], "entities": [{"text": "Chinese Wikipedia dump of Aug. 1 2016", "start_pos": 31, "end_pos": 68, "type": "DATASET", "confidence": 0.8597146996429988}, {"text": "Indonesian Wikipedia dump of July 20", "start_pos": 77, "end_pos": 113, "type": "DATASET", "confidence": 0.9191559255123138}]}, {"text": "For bilingual lexicons, we take the complete vocabulary from \"Kamus Besar Bahasa Indonesia\" (the Grand Indonesian Dictionary) and run the Google translation and Bing translation.", "labels": [], "entities": []}, {"text": "Since both translation systems generate a great deal of errors, we take the same translation from both systems hoping for better accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.996651828289032}]}, {"text": "One of our authors (an Indonesian language teaching professor) manually filtered out the correct word-pairs from this translation set.", "labels": [], "entities": []}, {"text": "We also take the vocabulary from the Indonesian language textbooks for Chinese learners to include with the word-pairs from the Grand Dictionary.", "labels": [], "entities": [{"text": "Grand Dictionary", "start_pos": 128, "end_pos": 144, "type": "DATASET", "confidence": 0.9618028402328491}]}, {"text": "Therefore, we have a collection of 10436 Indonesian-Chinese word-pair lexicon.", "labels": [], "entities": []}, {"text": "Out of this base seed lexicon, we select nouns, root words, high-frequency words (as from the basic-level and medium-level Indonesian textbook for Chinese language learners), and highly-unambiguous words.", "labels": [], "entities": []}, {"text": "The statistics are as follows: For each of the above 5 monolingual signals, we experiment with Mikolov's and Dinu's methodologies respectively.", "labels": [], "entities": []}, {"text": "We take 10% of the data as test set, 90% as training set.", "labels": [], "entities": []}, {"text": "We perform two types of experiment designs: Design 1: We build test data by randomly selecting 10% of all-words data.", "labels": [], "entities": []}, {"text": "We build 5 training models with the five signal data without overlapping with the test set; Design 2: For each of 5 signal data, we randomly select 10% for testing, and the rest for training.", "labels": [], "entities": []}, {"text": "In other words, each experiment is performed within the data of the same signal themselves.", "labels": [], "entities": []}, {"text": "These experiments evaluate the impact of signal for learning a general lexicon, and for learning lexicon of their own signal types.", "labels": [], "entities": []}, {"text": "We evaluate performance with the standard Precisions @ 1, 5, and 10.", "labels": [], "entities": [{"text": "Precisions", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.7869613766670227}]}], "tableCaptions": [{"text": " Table 1: Design 1 --Test on All-words Lexicon", "labels": [], "entities": []}, {"text": " Table 2: Design 2 -Test with data from the same signal type", "labels": [], "entities": []}]}