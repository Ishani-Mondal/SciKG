{"title": [{"text": "Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs", "labels": [], "entities": []}], "abstractContent": [{"text": "Recursive neural networks (RNN) and their recently proposed extension recur-sive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree.", "labels": [], "entities": []}, {"text": "Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems.", "labels": [], "entities": [{"text": "RLSTMs", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.7412012815475464}]}, {"text": "We present an artificial learning task that allows us to quantify the severity of these problems for both models.", "labels": [], "entities": []}, {"text": "We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree.", "labels": [], "entities": []}, {"text": "This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.9547394812107086}]}], "introductionContent": [{"text": "The recursive neural network (RNN) model became popular since the work of.", "labels": [], "entities": []}, {"text": "It has been employed to tackle several NLP tasks, such as syntactic parsing), machine translation (), and word embedding learning (.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7110981941223145}, {"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.851114422082901}, {"text": "word embedding learning", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.7169407804807028}]}, {"text": "However, like traditional recurrent neural networks, the RNN seems to suffer from the vanishing gradient problem, in which error signals propagating from the root in a parse tree to the child nodes shrink very quickly.", "labels": [], "entities": []}, {"text": "Moreover, it encounters difficulties in capturing long range dependencies: information propagating from child nodes deep in a parse tree can be obscured before reaching the root node.", "labels": [], "entities": []}, {"text": "In the recurrent neural network world, the long short term memory (LSTM) architecture) is often used as a solution to these two problems.", "labels": [], "entities": []}, {"text": "A natural extension of the LSTM can be defined for tree structures, which we call Recursive LSTM (RLSTM), as proposed independently by,, and.", "labels": [], "entities": []}, {"text": "However, while there is intensive research showing how the LSTM architecture can overcome those two problems compared to traditional recurrent models (e.g.,), such research is, to our knowledge, still absent for the comparison between RNNs and RLSTMs.", "labels": [], "entities": []}, {"text": "Therefore, in the current paper we investigate the following two questions: 1.", "labels": [], "entities": []}, {"text": "Is the RLSTM more capable of capturing long range dependencies than the RNN?", "labels": [], "entities": [{"text": "RLSTM", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.7839392423629761}]}, {"text": "2. Does the RLSTM overcome the vanishing gradient problem more effectively than the RNN?", "labels": [], "entities": [{"text": "RLSTM", "start_pos": 12, "end_pos": 17, "type": "TASK", "confidence": 0.4770589768886566}, {"text": "RNN", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8657474517822266}]}, {"text": "Supervised learning requires annotated data, which is often expensive to collect.", "labels": [], "entities": []}, {"text": "As a result, examining a model on natural data on many different aspects can be difficult because the portion of data that fits a specific aspect could not be sufficient.", "labels": [], "entities": []}, {"text": "Moreover, studying individual aspects separately is hard since many aspects are often correlated with each other.", "labels": [], "entities": []}, {"text": "This, unfortunately, is true in our case: answering those two questions requires us to evaluate the examined models on datasets of dif- ferent tree depths, in which the key nodes which contain decisive information in a parse tree must be identified.", "labels": [], "entities": []}, {"text": "Using available annotated corpora such as the Stanford Sentiment Treebank () and the Penn Treebank is thus inappropriate, as they are too small for this purpose (10k, 40k trees, respectively, compared to 240k trees in our experiments), and key nodes are not marked.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 46, "end_pos": 73, "type": "DATASET", "confidence": 0.8631065487861633}, {"text": "Penn Treebank", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9950558841228485}]}, {"text": "Our solution is an artificial task where sentences and parse trees can be randomly generated under any arbitrary constraints on tree depth and key node's position.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now examine how the two problems, the vanishing gradient problem and the problem of how to capture long range dependencies, affect the RLSTM model and the RNN model.", "labels": [], "entities": []}, {"text": "To do so, we propose the following artificial task, which requires a model to distinguish useful signals from noise.", "labels": [], "entities": []}, {"text": "We define: \u2022 a sentence is a sequence of tokens which are integer numbers in the range [0, 10000]; \u2022 a sentence contains one and only one keyword token which is an integer number smaller than 1000; \u2022 a sentence is labeled with the integer resulting from dividing the keyword by 100.", "labels": [], "entities": []}, {"text": "For instance, if the keyword is 607, the label is 6.", "labels": [], "entities": []}, {"text": "In this way, there are 10 classes, ranging from 0 to 9.", "labels": [], "entities": []}, {"text": "The task is to predict the class of a sentence, given its binary parse tree ().", "labels": [], "entities": []}, {"text": "Because the label of a sentence is determined solely by the keyword, the two models need to identify the keyword in the parse tree and allow only the information from the leaf node of the keyword to affect the root node.", "labels": [], "entities": []}, {"text": "It is worth noting that this task resembles sentiment analysis with simple cases in which the sentiment of a whole sentence is determined by one keyword (e.g. \"I like the movie\").", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9500773847103119}]}, {"text": "Simulating complex cases involving negation, composition, etc. is straightforward and for future work.", "labels": [], "entities": [{"text": "negation, composition", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.715485672156016}]}, {"text": "But here we believe that the current task is adequate to answer our two questions raised in Section 1.", "labels": [], "entities": []}, {"text": "The two models, RLSTM and RNN, were implemented with the dimension of vector representations and vector memories 50.", "labels": [], "entities": []}, {"text": "Following, we used tanh as the activation function, and initialized word vectors by randomly sampling each value from a uniform distribution U (\u22120.0001, 0.0001).", "labels": [], "entities": []}, {"text": "We trained the two models using the AdaGrad method) with a learning rate of 0.05 and a mini-batch size of 20 for the RNN and of 5 for the RLSTM.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.9500462710857391}, {"text": "RNN", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.9031547904014587}, {"text": "RLSTM", "start_pos": 138, "end_pos": 143, "type": "DATASET", "confidence": 0.8827371597290039}]}, {"text": "Development sets were employed for early stopping (training is halted when the accuracy on the development set is not improved after 5 consecutive epochs).", "labels": [], "entities": [{"text": "early stopping", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7387256622314453}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9992471933364868}]}, {"text": "It is worth noting that we also tried other values for the hyper-parameters but did not gain significantly better results on development sets.", "labels": [], "entities": []}, {"text": "We randomly generated 10 datasets.", "labels": [], "entities": []}, {"text": "To generate a sentence of length l, we shuffle a list of randomly chosen l \u2212 1 non-keywords and one keyword.", "labels": [], "entities": []}, {"text": "The i-th dataset contains 12k sentences of lengths from 10i \u2212 9 tokens to 10i tokens, and is split into train, dev, test sets with sizes of 10k, 1k, 1k sentences.", "labels": [], "entities": []}, {"text": "We parsed each sentence by randomly generating a binary tree whose number of leaf nodes equals to the sentence length.", "labels": [], "entities": []}, {"text": "The test accuracies of the two models on the 10 datasets are shown in; For each dataset we run each model 5 times and reported the highest accuracy for the RNN model, and the distribution of accuracies (via boxplot) for the RLSTM model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9983270764350891}]}, {"text": "We can see that the RNN model performs reasonably well on very short sentences (less than 11 tokens).", "labels": [], "entities": []}, {"text": "However, when the sentence length exceeds 10, the RNN's performance drops so quickly that the difference between it and the random guess' performance (10%) is negligible.", "labels": [], "entities": []}, {"text": "Trying different learning rates, mini-batch sizes, and values for n (the dimension of vectors) did not give significant differences.", "labels": [], "entities": []}, {"text": "On the other hand, the RLSTM model achieves more than 90% accuracy on sentences shorter than 31 tokens.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9989571571350098}]}, {"text": "Its performance drops when the sentence length increases, but is still substantially better than the random guess when the sentence length does not exceed 70.", "labels": [], "entities": []}, {"text": "When the sentence length exceeds 70, both the RLSTM and RNN perform similarly.", "labels": [], "entities": [{"text": "RLSTM", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.6101024746894836}, {"text": "RNN", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.7765967845916748}]}, {"text": "In Experiment 1, it is not clear whether the tree size or the keyword depth is the main factor of the rapid drop of the RNN's performance.", "labels": [], "entities": [{"text": "rapid", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9638176560401917}]}, {"text": "In this ex- periment, we kept the tree size fixed and vary the keyword depth.", "labels": [], "entities": []}, {"text": "We generated a pool of sentences of lengths from 21 to 30 tokens and parsed them by randomly generating binary trees.", "labels": [], "entities": []}, {"text": "We then created 10 datasets each of which has 12k trees (10k for training, 1k for development, and 1k for testing).", "labels": [], "entities": []}, {"text": "The i-th dataset consists of only trees in which distances from keywords to roots are i or i + 1 (to stop the networks from exploiting keyword depths directly).", "labels": [], "entities": []}, {"text": "shows test accuracies of the two models on those 10 datasets.", "labels": [], "entities": []}, {"text": "Similarly in Experiment 1, for each dataset we run each model 5 times and reported the highest accuracy for the RNN model, and the distribution of accuracies for the RLSTM model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9991723299026489}]}, {"text": "As we can see, the RNN model achieves very high accuracies when the keyword depth does not exceed 3.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9852098822593689}]}, {"text": "Its performance then drops rapidly and gets close to the performance of the random guess.", "labels": [], "entities": []}, {"text": "This is evidence that the RNN model has difficulty capturing long range dependencies.", "labels": [], "entities": []}, {"text": "By contrast, the RLSTM model performs at above 90% accuracy until the depth of the keyword reaches 8.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9991235136985779}]}, {"text": "It has difficulty dealing with larger depths, but the performance is always better than the random guess.", "labels": [], "entities": []}, {"text": "We now examine whether the two models can encounter the vanishing gradient problem.", "labels": [], "entities": []}, {"text": "To do so, we looked at the the back-propagation phase of each model in Experiment 1 on the third dataset (the one containing sentences of lengths from 21 to 30 tokens).", "labels": [], "entities": []}, {"text": "For each tree, we calculated the ra- where the numerator is the norm of the error vector at the keyword node and the denominator is the norm of the error vector at the root node.", "labels": [], "entities": []}, {"text": "This ratio gives us an intuition how the error signals develop when propagating backward to leaf nodes: if the ratio 1, the vanishing gradient problem occurs; else if the ratio 1, we observe the exploding gradient problem.", "labels": [], "entities": []}, {"text": "reports the ratios w.r.t. the keyword node depth in each epoch of training the RNN model.", "labels": [], "entities": []}, {"text": "The ratios in the first epoch are always very small.", "labels": [], "entities": []}, {"text": "In each following epoch, the RNN model successfully lifts up the ratios steadily (see fora clear picture at the keyword depth 10), but a clear decrease when the depth becomes larger is observable.", "labels": [], "entities": []}, {"text": "For the RLSTM model (see and 7b), the story is somewhat different.", "labels": [], "entities": []}, {"text": "The ratios go up after two epochs so rapidly that there are even some exploding error signals sent back to leaf nodes.", "labels": [], "entities": []}, {"text": "They subsequently go down and remain stable with substantially less exploding error signals.", "labels": [], "entities": []}, {"text": "This is, interestingly, concurrent with the performance of the RLSTM model on the development set (see.", "labels": [], "entities": []}, {"text": "It seems that the RLSTM model, after one epoch, quickly locates the keyword node in a tree and relates it to the root by building a strong bond between them via error signals.", "labels": [], "entities": []}, {"text": "After the correlation between the keyword and the label at the root is found, it tries to stabilize the training by reducing the error signals sent back  to the keyword node.", "labels": [], "entities": []}, {"text": "Comparing the two models by aligning with, and with, we can see that the RLSTM model is more capable of transmitting error signals to leaf nodes.", "labels": [], "entities": []}, {"text": "It is worth noting that we do seethe vanishing gradient problem happening when training the RNN model in; but suggests that the problem can become less serious after along enough training time.", "labels": [], "entities": []}, {"text": "This might be because depth 10 is still manageable for the RNN model.", "labels": [], "entities": []}, {"text": "(Notice that in the Stanford Sentiment Treebank, more than three quarters of leaf nodes are at depths less than 10.)", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 20, "end_pos": 47, "type": "DATASET", "confidence": 0.9323971271514893}]}, {"text": "The fact the the RNN model still doesnot perform better than random guessing can be explained using the arguments given by, who show that there is a trade-off between avoiding the vanishing gradient problem Figure 6: Ratios of norms of error vectors at keyword nodes (at different depths) to norms of error vectors at root nodes, in the RLSTM.", "labels": [], "entities": [{"text": "RLSTM", "start_pos": 337, "end_pos": 342, "type": "DATASET", "confidence": 0.7606970071792603}]}, {"text": "Many gradients explode in epoch 2, but stabilize later.", "labels": [], "entities": []}, {"text": "Gradients do not vanish, even at depth 12 and 13. and capturing long term dependencies when training traditional recurrent networks.", "labels": [], "entities": []}], "tableCaptions": []}