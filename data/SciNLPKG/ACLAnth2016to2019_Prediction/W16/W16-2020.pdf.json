{"title": [{"text": "A Multilinear Approach to the Unsupervised Learning of Morphology", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel approach to the un-supervised learning of morphology.", "labels": [], "entities": []}, {"text": "In particular, we use a Multiple Cause Mixture Model (MCMM), a type of autoen-coder network consisting of two node layers-hidden and surface-and a matrix of weights connecting hidden nodes to surface nodes.", "labels": [], "entities": [{"text": "Multiple Cause Mixture Model (MCMM)", "start_pos": 24, "end_pos": 59, "type": "TASK", "confidence": 0.7259607144764492}]}, {"text": "We show that an MCMM shares crucial graphical properties with autosegmental morphology.", "labels": [], "entities": []}, {"text": "We argue on the basis of this graphical similarity that our approach is theoretically sound.", "labels": [], "entities": []}, {"text": "Experiment results on Hebrew data show that this theoretical soundness bears out in practice.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is well-known that Semitic languages pose problems for the unsupervised learning of morphology (ULM).", "labels": [], "entities": [{"text": "unsupervised learning of morphology (ULM)", "start_pos": 62, "end_pos": 103, "type": "TASK", "confidence": 0.7384786350386483}]}, {"text": "For example, Hebrew morphology exhibits both agglutinative and fusional processes, in addition to non-concatenative root-and-pattern morphology.", "labels": [], "entities": []}, {"text": "This diversity in types of morphological processes presents unique challenges not only for unsupervised morphological learning, but for morphological theory in general.", "labels": [], "entities": [{"text": "morphological theory", "start_pos": 136, "end_pos": 156, "type": "TASK", "confidence": 0.7911198735237122}]}, {"text": "Many previous ULM approaches either handle the concatenative parts of the morpholgy (e.g.,) or, less often, the non-concatenative parts (e.g.,).", "labels": [], "entities": []}, {"text": "We present an approach to clustering morphologically related words that addresses both concatenative and non-concatenative morphology via the same learning mechanism, namely the Multiple Cause Mixture Model (MCMM).", "labels": [], "entities": [{"text": "clustering morphologically related words", "start_pos": 26, "end_pos": 66, "type": "TASK", "confidence": 0.8734473437070847}, {"text": "Multiple Cause Mixture", "start_pos": 178, "end_pos": 200, "type": "TASK", "confidence": 0.6365109284718832}]}, {"text": "This type of learning has direct connections to autosegmental theories of morphology, and at the same time raises questions about the meaning of morphological units (cf..", "labels": [], "entities": []}, {"text": "Consider the Hebrew verbs zwkr 1 ('he remembers') and mzkir ('he reminds'), which share the root z.k.r.", "labels": [], "entities": []}, {"text": "In neither form does this root appear as a continuous string.", "labels": [], "entities": []}, {"text": "Moreover, each form interrupts the root in a different way.", "labels": [], "entities": []}, {"text": "Many ULM algorithms ignore non-concatenative processes, assuming word formation to be a linear process, or handle the non-concatenative processes separately from the concatenative ones (see survey in.", "labels": [], "entities": [{"text": "word formation", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.7200543135404587}]}, {"text": "By separating the units of morphological structure from the surface string of phonemes (or characters), however, the distinction between non-concatenative and concatenative morphological processes vanishes.", "labels": [], "entities": []}, {"text": "We apply the Multiple Cause Mixture Model (MCMM), a type of autoencoder that serves as a disjunctive clustering algorithm, to the problem of morphological learning.", "labels": [], "entities": [{"text": "Multiple Cause Mixture", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.644443283478419}, {"text": "morphological learning", "start_pos": 141, "end_pos": 163, "type": "TASK", "confidence": 0.7683998644351959}]}, {"text": "An MCMM is composed of a layer of hidden nodes and a layer of surface nodes.", "labels": [], "entities": []}, {"text": "Like other generative models, it assumes that some subset of hidden nodes is responsible for generating each instance of observed data.", "labels": [], "entities": []}, {"text": "Here, the surface nodes are features that represent the \"surface\" properties of words, and the hidden nodes represent units of morphological structure.", "labels": [], "entities": []}, {"text": "An MCMM is well-suited to learn nonconcatenative morphology for the same reason that the autosegmental formalism is well-suited to representing it on paper (section 2): the layer of morphological structure is separate from the surface layer of features, and there are no dependencies between nodes within the same layer.", "labels": [], "entities": []}, {"text": "This intra-layer independence allows each hidden node to associate with any subset of features, con-tiguous or discontiguous.", "labels": [], "entities": []}, {"text": "We present details of the MCMM and its application to morphology in section 3.", "labels": [], "entities": []}, {"text": "Our ultimate goal is to find a ULM framework that is theoretically plausible, with the present work being somewhat exploratory.", "labels": [], "entities": [{"text": "ULM", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9465711116790771}]}], "datasetContent": [{"text": "We evaluate our clustering results according to three metrics.", "labels": [], "entities": []}, {"text": "Let U denote the set of M returned clusters and V the set of N gold-standard categories.", "labels": [], "entities": []}, {"text": "The idea behind purity is to compute the proportion of examples assigned to the correct cluster, using the most frequent category within a given cluster as gold.", "labels": [], "entities": [{"text": "purity", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9150643348693848}]}, {"text": "Standard purity assumes each example belongs to only one gold category.", "labels": [], "entities": [{"text": "purity", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.8375521898269653}]}, {"text": "For a dataset like ours consisting of multi-category examples, this can yield purities greater than 1.", "labels": [], "entities": []}, {"text": "We thus modify the calculations slightly to compute average cluster-wise purity, as in, where we divide by M . While this equation yields purities within, even when clusters overlap, it retains the metric's bias toward small clusters.", "labels": [], "entities": []}, {"text": "Given this bias, we incorporate other metrics: BCubed precision and BCubed recall (Bagga and Baldwin, 1998) compare the cluster mappings of x with those of y, for every pair of data points x and y.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.5010443329811096}, {"text": "BCubed", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.6297050714492798}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.7280779480934143}]}, {"text": "These metrics are well-suited to cases of overlapping clusters.", "labels": [], "entities": []}, {"text": "Suppose x and y share m clusters and n categories.", "labels": [], "entities": []}, {"text": "BCubed precision measures the extent to which m \u2264 n.", "labels": [], "entities": [{"text": "BCubed", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5263131260871887}, {"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.6456192135810852}]}, {"text": "It is 1 as long there are not more clusters than gold-standard categories.", "labels": [], "entities": []}, {"text": "BCubed Recall measures the extent to which m \u2265 n.", "labels": [], "entities": [{"text": "BCubed", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5993630290031433}, {"text": "Recall", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.504909873008728}]}, {"text": "See Artiles and Verdejo (2009) for calculation details.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results at K = 100", "labels": [], "entities": [{"text": "K", "start_pos": 21, "end_pos": 22, "type": "METRIC", "confidence": 0.9836148619651794}]}]}