{"title": [{"text": "On the Robustness of Standalone Referring Expression Generation Algorithms Using RDF Data", "labels": [], "entities": []}], "abstractContent": [{"text": "A sub-task of Natural Language Generation (NLG) is the generation of referring expressions (REG).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.7961034079392751}, {"text": "generation of referring expressions (REG)", "start_pos": 55, "end_pos": 96, "type": "TASK", "confidence": 0.6003308509077344}]}, {"text": "REG algorithms are expected to select attributes that unam-biguously identify an entity with respect to a set of distractors.", "labels": [], "entities": []}, {"text": "In previous work we have defined a methodology to evaluate REG algorithms using real life examples.", "labels": [], "entities": [{"text": "REG algorithms", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.913326770067215}]}, {"text": "In the present work, we evaluate REG algorithms using a dataset that contains alterations in the properties of referring entities.", "labels": [], "entities": [{"text": "REG", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9618602395057678}]}, {"text": "We found that naturally occurring ontological re-engineering can have a devastating impact in the performance of REG algorithms, with some more robust in the presence of these changes than others.", "labels": [], "entities": [{"text": "REG", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9453922510147095}]}, {"text": "The ultimate goal of this work is observing the behavior and estimating the performance of a series of REG algorithms as the entities in the data set evolve overtime.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main research focus in NLG is the creation of computer systems capable of generating humanlike language.", "labels": [], "entities": []}, {"text": "According to the consensus Natural Language Generation (NLG) architecture) the NLG task takes as input nonlinguistic data and operates over it as a series of enrichment steps, culminating with fully specified sentences from which output strings can be readout.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.81754998366038}]}, {"text": "Such a generation pipeline mimics, up to a certain extent, a Natural Language Understanding (NLU) pipeline.", "labels": [], "entities": []}, {"text": "In NLU, however, it is expected that the text upon which the system is being run upon might contain a variety of errors.", "labels": [], "entities": [{"text": "NLU", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.9471839666366577}]}, {"text": "These errors include wrongly written text that a human might find it difficult to understand or idiosyncratic deviations from well accepted prose (what is called \"improper grammar\" or \"orthographic mistakes\" by defenders of prescriptive grammar).", "labels": [], "entities": []}, {"text": "The fact that plenty of texts of interest to NLU exhibit poor quality explains the reason behind NLU's focus on robust approaches.", "labels": [], "entities": [{"text": "NLU", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9035420417785645}]}, {"text": "Such approaches attempt to cope gracefully with inputs that do not conform to the standards of the original texts employed for building the system (either as working examples or training data in a machine learning sense).", "labels": [], "entities": []}, {"text": "In NLG, on the other hand, current approaches rarely explore fallback strategies for those cases where the data is not fully compliant with the expected input and, thus, there is little intuition about possible outputs of a system under such circumstances.", "labels": [], "entities": []}, {"text": "In this work we aim to explore robustness for the particular case of Referring Expressions Generation (REG) algorithms by means of different versions of an ontology.", "labels": [], "entities": [{"text": "Referring Expressions Generation (REG)", "start_pos": 69, "end_pos": 107, "type": "TASK", "confidence": 0.7735957155625025}]}, {"text": "Therefore, we can combine REG algorithms with ontologies to study their behavior as the entities in the chosen ontology change.", "labels": [], "entities": []}, {"text": "In our case, we have chosen the ontology built from Wikipedia though different versions of DBpedia and three REG algorithms on which we will measure robustness, defined here as an algorithm's resilience to adapt to changes in the data or its capability to gracefully deal with noisy data.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.9484962224960327}]}, {"text": "Ina sense, we are interested in two different phenomena: (1) whether an NLG subcomponent (REG in particular) can be used with outdated ontological data to fulfill its task and (2) which implementation of the said subcomponent is better suited in this setting.", "labels": [], "entities": []}, {"text": "Our research is driven by the second question but this current work sheds more light on the first one.", "labels": [], "entities": []}, {"text": "See Section 7 for details.", "labels": [], "entities": []}, {"text": "This paper is structured as follows: next section briefly mentions the relevant related work, in Section 3 and Section 4 we describe the algorithms applied and data used, respectively; in Section 5 we describe the setup used in the experiments, then Section 6 presents the results which are further discussed in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We follow an approach similar to ) to extract REG tasks from journalistic text: we extract all people that appear explicitly linked in a given Wikinews article.", "labels": [], "entities": [{"text": "REG tasks from journalistic text", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.8567964196205139}]}, {"text": "By using Wikinews, we ensure all the people are disambiguated to their DBpedia URIs by construction.", "labels": [], "entities": []}, {"text": "We selected a Wikinews dump as closest to our target.", "labels": [], "entities": []}, {"text": "From there, we define all URIs for which DBpedia has a birthDate relation (761,830 entities) as \"people\" and all entities with a foundDate as an \"organization\" (19,694 entities).", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.920719563961029}]}, {"text": "We extract all such people and organizations that appear in the same Wikinews article using the provided inter-wiki SQL links file.", "labels": [], "entities": []}, {"text": "For each article, we randomly chose a person as the referent, turning them into a fully defined REG task.", "labels": [], "entities": []}, {"text": "This approach produced 4,741 different REG tasks, over 9,660 different people and 3,062 over 8,539 We then created a subset of the relevant tuples for these people (291,039 tuples on DBpedia 2014 and 129,782 on DBpedia 3.6, a 224% increase 6 ) and organizations (468,041 tuples on DBpedia 2014 and 216,730 on DBpedia 3.6, a 216% increase) by extracting all tuples were any of the people or organizations were involved, either as subject or object of the statement.", "labels": [], "entities": [{"text": "DBpedia 2014", "start_pos": 183, "end_pos": 195, "type": "DATASET", "confidence": 0.9349032938480377}]}, {"text": "Over these subsets were our algorithms executed.", "labels": [], "entities": []}, {"text": "As we are interested in REs occurring after first mentions, we filter properties from the data that unequivocally identify the entity, such as full name or GPS location of its headquarters.", "labels": [], "entities": [{"text": "REs occurring after first mentions", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.8873775005340576}]}], "tableCaptions": [{"text": " Table 1: Our results, over 3,051 different REG tasks for people and 2,370 for organizations. The error  percentages are computed over the total number of executed tasks.", "labels": [], "entities": [{"text": "error  percentages", "start_pos": 98, "end_pos": 116, "type": "METRIC", "confidence": 0.9564589858055115}]}]}