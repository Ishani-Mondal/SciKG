{"title": [], "abstractContent": [{"text": "We present a simple yet effective approach for learning word sense embeddings.", "labels": [], "entities": []}, {"text": "In contrast to existing techniques, which either directly learn sense representations from corpora or rely on sense inventories from lexical resources, our approach can induce a sense inventory from existing word embeddings via clustering of ego-networks of related words.", "labels": [], "entities": []}, {"text": "An integrated WSD mechanism enables labeling of words in context with learned sense vectors, which gives rise to downstream applications.", "labels": [], "entities": [{"text": "WSD", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.8403881788253784}]}, {"text": "Experiments show that the performance of our method is comparable to state-of-the-art unsupervised WSD systems .", "labels": [], "entities": [{"text": "WSD", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.8835126161575317}]}], "introductionContent": [{"text": "Term representations in the form of dense vectors are useful for many natural language processing applications.", "labels": [], "entities": []}, {"text": "First of all, they enable the computation of semantically related words.", "labels": [], "entities": []}, {"text": "Besides, they can be used to represent other linguistic units, such as phrases and short texts, reducing the inherent sparsity of traditional vector-space representations (.", "labels": [], "entities": []}, {"text": "One limitation of most word vector models, including sparse () and dense ( representations, is that they conflate all senses of a word into a single vector.", "labels": [], "entities": []}, {"text": "Several architectures for learning multiprototype embeddings were proposed that try to address this shortcoming (.", "labels": [], "entities": []}, {"text": "provide indications that such sense vectors improve the performance of text processing applications, such as part-of-speech tagging and semantic relation identification.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.7059736996889114}, {"text": "semantic relation identification", "start_pos": 136, "end_pos": 168, "type": "TASK", "confidence": 0.7293981313705444}]}, {"text": "The contribution of this paper is a novel method for learning word sense vectors.", "labels": [], "entities": [{"text": "learning word sense vectors", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6754985079169273}]}, {"text": "In contrast to previously proposed methods, our approach relies on existing single-prototype word embeddings, transforming them to sense vectors via ego-network clustering.", "labels": [], "entities": []}, {"text": "An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters.", "labels": [], "entities": []}, {"text": "Our method is fitted with a word sense disambiguation (WSD) mechanism, and thus words in context can be mapped to these sense representations.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.7652333527803421}]}, {"text": "An advantage of our method is that one can use existing word embeddings and/or existing word sense inventories to build sense embeddings.", "labels": [], "entities": []}, {"text": "Experiments show that our approach performs comparably to state-of-the-art unsupervised WSD systems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9536949992179871}]}], "datasetContent": [{"text": "We evaluate our method on two complementary datasets: (1) a crowdsourced collection of senselabeled contexts; and (2) a commonly used SemEval dataset.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 134, "end_pos": 149, "type": "DATASET", "confidence": 0.7108678072690964}]}, {"text": "The goal of this evaluation is to test different configurations of our approach on a large-scale dataset, i.e. it is used for development purposes.", "labels": [], "entities": []}, {"text": "This test collection is based on a largescale crowdsourced resource by that comprises 1,012 frequent nouns with average polysemy of 2.26 senses per word.", "labels": [], "entities": []}, {"text": "For these nouns the dataset provides 145,140 annotated sentences sampled from Wikipedia.", "labels": [], "entities": []}, {"text": "Besides, it is accompanied by an explicit sense inventory, where each sense is represented with a list of words that can substitute target noun in a given sentence.", "labels": [], "entities": []}, {"text": "The sense distribution across sentences in the dataset is skewed, resulting in 79% of contexts assigned to the most frequent senses.", "labels": [], "entities": []}, {"text": "Therefore, in addition to the full TWSI dataset, we also use a balanced subset that has no bias towards the Most Frequent Sense (MFS).", "labels": [], "entities": [{"text": "TWSI dataset", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.8200602531433105}]}, {"text": "This dataset features 6,165 contexts with five contexts per sense excluding monosemous words.", "labels": [], "entities": []}, {"text": "To compute WSD performance, we create an explicit mapping between the system-provided sense inventory and the TWSI senses: senses are represented as bag of words vectors, which are compared using cosine similarity.", "labels": [], "entities": [{"text": "WSD", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9854409098625183}]}, {"text": "Every induced sense gets assigned to at most one TWSI sense.", "labels": [], "entities": []}, {"text": "Once the mapping is completed, we can calculate precision and recall of sense prediction with respect to the original TWSI labeling.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9996652603149414}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9993221759796143}, {"text": "sense prediction", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.6352256089448929}]}, {"text": "Performance of a disambiguation model depends on quality of the sense mapping.", "labels": [], "entities": []}, {"text": "These baselines facilitate interpretation of results: \u2022 Upper bound of the induced inventory selects the correct sense for the context, but only if the mapping exist for this sense.", "labels": [], "entities": []}, {"text": "\u2022 MFS of the TWSI inventory assigns the most frequent sense in the TWSI dataset.", "labels": [], "entities": [{"text": "MFS", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9042369723320007}, {"text": "TWSI inventory", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.8614234030246735}, {"text": "TWSI dataset", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.9702570736408234}]}, {"text": "\u2022 MFS of the induced inventory assigns the identifier of the largest sense cluster.", "labels": [], "entities": [{"text": "MFS", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.5900311470031738}]}, {"text": "\u2022 Random sense baseline of the TWSI and induced sense inventories.", "labels": [], "entities": [{"text": "TWSI", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.734272837638855}]}, {"text": "presents examples of the senses induced via clustering of nearest neighbours generated byword embeddings (w2v) and JBT as compared to the inventory produced via crowdsourcing (TWSI).", "labels": [], "entities": []}, {"text": "The TWSI contains more senses (2.26 on average), while induced ones have less senses (1.56 and 1.64, respectively).", "labels": [], "entities": [{"text": "TWSI", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.4780280888080597}]}, {"text": "The senses in the table are arranged in the way they are mapped to TWSI during evaluation.", "labels": [], "entities": []}, {"text": "illustrates how the granularity of the inventory influences WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9847842454910278}]}, {"text": "The more granular the sense inventory, the better the match between the TWSI and the induced inventory can be established (mind that we map every induced sense to at most one TWSI sense).", "labels": [], "entities": []}, {"text": "Therefore, the upper bound of WSD performance is maximal for the most fine-grained inventories.", "labels": [], "entities": [{"text": "WSD", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9112837314605713}]}, {"text": "However, the relation of actual WSD performance to granularity is inverse: the lower the number of senses, the higher the WSD performance (in the limit, we converge to the strong MFS baseline).", "labels": [], "entities": [{"text": "WSD", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9770345687866211}, {"text": "WSD", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.7083727717399597}]}, {"text": "We select a coarse-grained inventory for our further experiments (n=200, k = 15).", "labels": [], "entities": []}, {"text": "illustrates the fact that using context filtering positively impacts disambiguation performance, reaching optimal characteristics when two context words are used.", "labels": [], "entities": []}, {"text": "Finally, presents results of our experiments on the full and sense-balanced TWSI datasets.", "labels": [], "entities": [{"text": "TWSI datasets", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.8612801432609558}]}, {"text": "First of all, our models significantly outperform random sense baseline of both TWSI and induced inventories.", "labels": [], "entities": [{"text": "TWSI", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.5882726311683655}]}, {"text": "Secondly, we observe that pooling vectors using similarity scores as weights is better than unweighted pooling.", "labels": [], "entities": []}, {"text": "Indeed, some clusters may contain irrelevant words and thus their contribution should be discounted.", "labels": [], "entities": []}, {"text": "Third, we observe that using similarity-based disambiguation mechanism yields better results as compared: Upper-bound and actual value of the WSD performance on the sense-balanced TWSI dataset, function of sense inventory used for unweighted pooling of word vectors.", "labels": [], "entities": [{"text": "WSD", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.7955252528190613}, {"text": "TWSI dataset", "start_pos": 180, "end_pos": 192, "type": "DATASET", "confidence": 0.8751100301742554}]}, {"text": "to the mechanism based on probabilities.", "labels": [], "entities": []}, {"text": "Indeed, cosine similarity between embeddings proved to be useful for semantic relatedness, yielding stateof-the-art results (), while there is less evidence about successful use-cases of the CBOW as a language model.", "labels": [], "entities": [{"text": "semantic relatedness", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.7697955667972565}]}, {"text": "Fourth, we confirm our observation that filtering context words positively impacts WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.987231433391571}]}, {"text": "Finally, we note that models based on JBTand w2v-induced sense inventories yield comparable results.", "labels": [], "entities": []}, {"text": "However, the JBT inventory shows higher performance (0.410 vs 0.390) on the balanced TWSI, indicating the importance of a precise sense inventory.", "labels": [], "entities": [{"text": "JBT inventory", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.886875331401825}]}, {"text": "Finally, using the \"gold\" TWSI inventory significantly improves the performance on the balanced dataset outperforming models based on induced inventories.", "labels": [], "entities": [{"text": "TWSI inventory", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.6990940272808075}]}, {"text": "The goal of this evaluation is to compare the performance of our method to state-of-the-art unsupervised WSD systems.", "labels": [], "entities": []}, {"text": "The SemEval-2013 task 13 \"Word Sense Induction for Graded and Non-Graded Senses\" (Jurgens and Klapaftis, 2013) provides 20 nouns, 20 verbs and 10 adjectives in WordNetsense-tagged contexts.", "labels": [], "entities": [{"text": "Word Sense Induction for Graded and Non-Graded Senses\" (Jurgens and Klapaftis, 2013)", "start_pos": 26, "end_pos": 110, "type": "TASK", "confidence": 0.6252865009009838}]}, {"text": "It contains 20-100 contexts per word, and 4,664 contexts in total, which were drawn from the Open American National Corpus.", "labels": [], "entities": [{"text": "Open American National Corpus", "start_pos": 93, "end_pos": 122, "type": "DATASET", "confidence": 0.9068494290113449}]}, {"text": "Participants were asked to cluster these 4,664 instances into groups, with each group corresponding to a distinct word sense.", "labels": [], "entities": []}, {"text": "Performance is measured with three measures that require a mapping of sense inventories (Jaccard Index, Tau and WNDCG) and two cluster comparison measures (Fuzzy NMI and Fuzzy B-Cubed).", "labels": [], "entities": [{"text": "Jaccard Index", "start_pos": 89, "end_pos": 102, "type": "METRIC", "confidence": 0.7172459065914154}, {"text": "WNDCG", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.706787645816803}]}, {"text": "We compare our approach to SemEval participants and the AdaGram sense embeddings.", "labels": [], "entities": [{"text": "AdaGram sense embeddings", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.9020329316457113}]}, {"text": "The AI-KU system () directly clusters test contexts using the k-means algorithm based on lexical substitution features.", "labels": [], "entities": []}, {"text": "The Unimelb system ( uses a hierarchical topic model to induce and dis- Our models: The best configurations of our method selected on the TWSI dataset on the SemEval 2013 Task 13 dataset.", "labels": [], "entities": [{"text": "TWSI dataset on the SemEval 2013 Task 13 dataset", "start_pos": 138, "end_pos": 186, "type": "DATASET", "confidence": 0.8133031494087644}]}, {"text": "The w2v-based methods rely on the CBOW model with 100 dimensions and context window size 3.", "labels": [], "entities": []}, {"text": "The JBT similarities were computed using the Malt parser.", "labels": [], "entities": [{"text": "JBT similarities", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8174470067024231}]}, {"text": "All systems were trained on the ukWaC corpus.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9935345351696014}]}, {"text": "The UoS system) is most similar to our approach: to induce senses it builds an ego-network of a word using dependency relations, which is subsequently clustered using a simple graph clustering algorithm.", "labels": [], "entities": []}, {"text": "The La Sapienza system (, relies on WordNet to get word senses and perform disambiguation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.9366703629493713}]}, {"text": "shows a comparative evaluation of our method on the SemEval dataset.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.7730813324451447}]}, {"text": "Like above, dependency-based (JBT) word similarities yield slightly better results than word embedding similarity (w2v) for inventory induction.", "labels": [], "entities": [{"text": "inventory induction", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7225941717624664}]}, {"text": "In addition to these two configurations, we also built a model based on the TWSI sense inventory (only for nouns as the TWSI contains nouns only).", "labels": [], "entities": []}, {"text": "This model significantly outperforms both JBT-and w2v-based models, thus precise sense inventories greatly influence WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 117, "end_pos": 120, "type": "TASK", "confidence": 0.9685681462287903}]}, {"text": "As one may observe, performance of the best configurations of our method is comparable to the top-ranked SemEval participants, but is not systematically exceeding their results.", "labels": [], "entities": []}, {"text": "AdaGram sometimes outperforms our method, sometimes it is on par, depending on the metric.", "labels": [], "entities": [{"text": "AdaGram", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9311044812202454}]}, {"text": "We interpret these results as an indication of comparability of our method to state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "Finally, note that none of the unsupervised WSD methods discussed in this paper, including the top-ranked SemEval submissions and AdaGram, were able to beat the most frequent sense baselines of the respective datasets (with the exception of the balanced version of TWSI).", "labels": [], "entities": [{"text": "WSD", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9376152157783508}, {"text": "AdaGram", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.8891384601593018}, {"text": "TWSI", "start_pos": 265, "end_pos": 269, "type": "DATASET", "confidence": 0.8865518569946289}]}, {"text": "Similar results are observed for other unsupervised WSD methods).", "labels": [], "entities": [{"text": "WSD", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9309305548667908}]}], "tableCaptions": [{"text": " Table 4: Influence of context filtering on disam- biguation in terms of F-score. The models were  trained on Wikipedia corpus; the w2v is based  on weighted pooling and similarity-based disam- biguation. All differences between filtered and un- filtered models are significant (p < 0.05).", "labels": [], "entities": [{"text": "F-score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9901187419891357}, {"text": "Wikipedia corpus", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.9117815494537354}]}, {"text": " Table 3: Upper-bound and actual value of the WSD performance on the sense-balanced TWSI dataset,  function of sense inventory used for unweighted pooling of word vectors.", "labels": [], "entities": [{"text": "WSD", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9169171452522278}, {"text": "TWSI dataset", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.8914885222911835}]}, {"text": " Table 5: The best configurations of our method selected on the TWSI dataset on the SemEval 2013 Task  13 dataset. The w2v-based methods rely on the CBOW model with 100 dimensions and context window  size 3. The JBT similarities were computed using the Malt parser. All systems were trained on the  ukWaC corpus.", "labels": [], "entities": [{"text": "TWSI dataset", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9776086211204529}, {"text": "SemEval 2013 Task  13 dataset", "start_pos": 84, "end_pos": 113, "type": "DATASET", "confidence": 0.854150366783142}, {"text": "ukWaC corpus", "start_pos": 299, "end_pos": 311, "type": "DATASET", "confidence": 0.9922833442687988}]}]}