{"title": [{"text": "Paraphrase for Open Question Answering: New Dataset and Methods", "labels": [], "entities": [{"text": "Open Question Answering", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.706277201573054}]}], "abstractContent": [{"text": "We propose anew open question answering framework for question answering over a knowledge base (KB).", "labels": [], "entities": [{"text": "question answering", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.766114354133606}, {"text": "question answering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8227332830429077}]}, {"text": "Our system uses both a curated KB, Freebase, and one that is extracted automatically by an open information extraction model, IE KB.", "labels": [], "entities": [{"text": "IE KB", "start_pos": 126, "end_pos": 131, "type": "DATASET", "confidence": 0.8595103621482849}]}, {"text": "Our system consists of only one layer of paraphrase, compared to the three layers used in a previous open question answering system (Fader et al., 2014).", "labels": [], "entities": []}, {"text": "However, because of the more accurately extracted relation triples in IE KB, combined with linked entities from IE KB to Freebase, our system achieves a 7% absolute gain in F 1 score over the previous system.", "labels": [], "entities": [{"text": "IE KB", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.933681845664978}, {"text": "IE KB", "start_pos": 112, "end_pos": 117, "type": "DATASET", "confidence": 0.9356744885444641}, {"text": "Freebase", "start_pos": 121, "end_pos": 129, "type": "DATASET", "confidence": 0.5505267381668091}, {"text": "F 1 score", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9891507228215536}]}], "introductionContent": [{"text": "There are two broad classes of systems that provide question answering (QA) from a knowledge base (KB).", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.8359987497329712}]}, {"text": "One uses semantic parsing, and the other uses information extraction (IE).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.721891924738884}, {"text": "information extraction (IE)", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.8266955375671386}]}, {"text": "Semantic parsing systems depend on highly accurate knowledge bases such as Freebase, which are accurate but incomplete ().", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7750660181045532}, {"text": "Freebase", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9762002825737}]}, {"text": "However, although semantic parsing systems currently achieve higher performance than IE-based systems, we think it is desirable to continue to develop the latter as an alternative or in combination with the former.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7924792170524597}]}, {"text": "One major challenge for question answering from a KB is the many ways in which relations can be expressed.", "labels": [], "entities": [{"text": "question answering from a KB", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.7277969598770142}]}, {"text": "On the one hand, we need to deal with language variability, for example, acknowledging that the following two questions have the same meaning: \"What character did Natalie Portman play in Star Wars?\" and \"What is the role of Natalie Portman in Star Wars?\".", "labels": [], "entities": []}, {"text": "We call this NL-NL paraphrasing, since it requires the identification of a map between two natural language expressions.", "labels": [], "entities": []}, {"text": "On the other hand, we need to bridge the gap between the expression of relations in a curated knowledge base, such as Freebase, and relations conveyed in natural language sentences.", "labels": [], "entities": []}, {"text": "We refer to this as NL-KB paraphrasing.", "labels": [], "entities": []}, {"text": "For instance, general QA will require a mapping between the natural language relation brother and the Freebase relation \"/people/person/sibling s.\"", "labels": [], "entities": []}, {"text": "Our contribution to Open IE question answering is three-fold.", "labels": [], "entities": [{"text": "Open IE question answering", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.7971269637346268}]}, {"text": "First, we provide anew widecoverage store of automatically extracted relation triples, which has a higher precision than that of previous work).", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9973229765892029}]}, {"text": "This Open IE triple store allows us to tackle the NL-NL paraphrasing problem.", "labels": [], "entities": []}, {"text": "Second, the entities in our dataset is linked to Freebase, which allows us to tackle the NL-KB paraphrasing problem.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.965752899646759}]}, {"text": "Third, we propose a simple and effective open QA framework that consists of a single paraphrase layer.", "labels": [], "entities": []}, {"text": "In this framework, we perform searches on both Open IE KB and Freebase.", "labels": [], "entities": [{"text": "Open IE KB", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.9096169074376425}, {"text": "Freebase", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.8012763857841492}]}, {"text": "This single-layered paraphrase model allows us to test and compare a variety of paraphrasing models.", "labels": [], "entities": []}, {"text": "Experiments on the WebQuestion set () shows that our system exhibits better performance than the two IE-based systems,), and is comparable to, where their average F-score is 35.7%, and ours is 37.9%.", "labels": [], "entities": [{"text": "WebQuestion set", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.9410122036933899}, {"text": "F-score", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9973071813583374}]}], "datasetContent": [{"text": "To augment our QA system, we create anew Open IE relation triple dataset that contains sentences and document IDs from which the triples are extracted.", "labels": [], "entities": [{"text": "Open IE relation triple dataset", "start_pos": 41, "end_pos": 72, "type": "DATASET", "confidence": 0.5516059815883636}]}, {"text": "The relation triples are extracted from ClueWeb09 by the Open IE system of.", "labels": [], "entities": []}, {"text": "Each triple contains two arguments, which are entities from Freebase, and one relation phrase.", "labels": [], "entities": []}, {"text": "The arguments' Freebase IDs are provided by FACC1 corpus . The relations are lemmatized and the sentences that contain the triples are provided.", "labels": [], "entities": [{"text": "FACC1 corpus", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.9677200615406036}]}, {"text": "As a result, the dataset contains more than 300 million relation triples 2 . shows one relation triple example, including the originating parsed sentence.", "labels": [], "entities": []}, {"text": "The relation triple is from the 485 th sentence of the document clueweb12-0700tw-51-00204.", "labels": [], "entities": [{"text": "clueweb12-0700tw-51-00204", "start_pos": 64, "end_pos": 89, "type": "DATASET", "confidence": 0.9104019403457642}]}, {"text": "The relation word is director, which is the 4th word in the sentence.", "labels": [], "entities": []}, {"text": "The two arguments Raul Gonzalez and National Council of La Raza have corresponding Freebase IDs: /m/02rmsx3 and /m/085f3n.", "labels": [], "entities": [{"text": "National Council of La Raza", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.9671423196792602}]}, {"text": "Many NLP tasks can potentially benefit from such data.", "labels": [], "entities": []}, {"text": "For example, for question answering task, there are at least three advantages.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.8925973574320475}]}, {"text": "One is that the Relation Triple Example <doc>clueweb12-0700tw-51-00204: Example of a relation triple extracted from ClueWeb09, with its source sentence and document ID.", "labels": [], "entities": []}, {"text": "entities are linked to Freebase, which will identify entities that represent one object but with different instances.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.952406644821167}]}, {"text": "Secondly, the triple is associated with the parsed sentences and the document ID, which can provide better evidence for questions with n-ary relations, such as \"What character did Natalie Portman play in Star Wars?\"", "labels": [], "entities": []}, {"text": "Finally, we can provide explanations, i.e., by identifying sentences that are evidence in support of our answers.", "labels": [], "entities": []}, {"text": "We believe that this large volume of linked triples may not only improve the mapping between the natural language and Freebase relations, but also improve the recall of questions, as we can also search based on entities' Freebase IDs.", "labels": [], "entities": [{"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.989274799823761}]}, {"text": "We intend to make this data publicly available.", "labels": [], "entities": []}, {"text": "The first component of our system is query preprocessing.", "labels": [], "entities": [{"text": "query preprocessing", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8283928632736206}]}, {"text": "We use the Stanford CoreNLP tool for entity extraction and sentence parsing.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 11, "end_pos": 27, "type": "DATASET", "confidence": 0.9055637717247009}, {"text": "entity extraction", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8154670596122742}, {"text": "sentence parsing", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7497055530548096}]}, {"text": "Entities such as persons and organizations have higher priority as target entities than those such as numbers and dates.", "labels": [], "entities": []}, {"text": "We then extract the dependency path between the chosen target entity and the question phrase.", "labels": [], "entities": []}, {"text": "The question phrase can be a single word such as where or multi-word phrases such as which character.", "labels": [], "entities": []}, {"text": "The words on the dependency path are considered to be the relation words, i.e., the predicates.", "labels": [], "entities": []}, {"text": "We evaluate our question answering system based on the question set provided by Frequency of (the type word in the question, answer ) in the ClueWeb 5 Shape of the answer (e.g. has numbers, multiwords) + the question phrase 6 Paraphrase rules 7 Whether the paraphrase rule from the 3000 training sentences 8 Paraphrase rule with the max score, its score, and which paraphrase approach leads to it 9 Context words hit rate 10 Score of the answer with answer retrieval answers are guaranteed to be found in Freebase.", "labels": [], "entities": [{"text": "question answering", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7393293678760529}, {"text": "Freebase", "start_pos": 505, "end_pos": 513, "type": "DATASET", "confidence": 0.9778329730033875}]}, {"text": "Our experiments attempt to answer several questions: \u2022 Is our dataset useful for the paraphrase tasks?", "labels": [], "entities": [{"text": "paraphrase tasks", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.8966617286205292}]}, {"text": "\u2022 Which popular paraphrase approach is more suitable for question answering?", "labels": [], "entities": [{"text": "question answering", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.8761956989765167}]}, {"text": "\u2022 Is our system better than other Information Extraction-based systems?", "labels": [], "entities": [{"text": "Information Extraction-based", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.7046109139919281}]}, {"text": "There are at least two metrics used in the literature: 1.", "labels": [], "entities": []}, {"text": "Top 1 F 1 score, as used by).", "labels": [], "entities": []}, {"text": "Every system outputs only one answer.", "labels": [], "entities": []}, {"text": "The system's answer is the entity with highest score (randomly pick one if there is a tie).", "labels": [], "entities": []}, {"text": "No answer is produced if the highest score is below a certain threshold.", "labels": [], "entities": []}, {"text": "An answer is considered correct if the entity in the system's answer appears in the gold answer.", "labels": [], "entities": []}, {"text": "The precision, recall and F 1 score are calculated globally: Precision = # questions with correct answers # questions with answers Recall = # questions with correct answers # questions 2.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.999728262424469}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.999525785446167}, {"text": "F 1 score", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9909815788269043}, {"text": "Precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9988571405410767}, {"text": "Recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9341992735862732}]}, {"text": "Average F 1 score (accuracy).", "labels": [], "entities": [{"text": "Average F 1 score", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.8733177036046982}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9856223464012146}]}, {"text": "This is used by semantic parsing question answering systems such as).", "labels": [], "entities": [{"text": "semantic parsing question answering", "start_pos": 16, "end_pos": 51, "type": "TASK", "confidence": 0.8746466636657715}]}, {"text": "For every question, the precision, recall and F 1 score are computed between the systems' answer set and the gold answer set.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9997438788414001}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9990240335464478}, {"text": "F 1 score", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9928513566652933}]}, {"text": "Then, the F 1 scores are averaged across all questions in the test set.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9838915665944418}]}, {"text": "This metric is used to reward partially-complete system answers.", "labels": [], "entities": []}, {"text": "In the following experiments, we will compare our system with Fader's with respect to the first metric, and the rest with the second metric.", "labels": [], "entities": []}, {"text": "We demonstrate the effect of different dataset sizes by estimating a paraphrase PMI model from a smaller subset of our data, and then comparing QA systems' performance with these alternative paraphrase sets.", "labels": [], "entities": []}, {"text": "Initially, we use the recall as the comparative metric.", "labels": [], "entities": [{"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9987466335296631}]}, {"text": "Recall is calculated as the percentage of questions that can be answered by the top 30 candidate answers retrieved.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9874457120895386}]}, {"text": "To filter the effect of features and supervised models, results are based on answer retrieval on Freebase, and no re-ranking is given.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.9721894860267639}]}, {"text": "For every question we extract the top 100 Freebase relations as paraphrase of one question relation (100 is set on a development set).", "labels": [], "entities": []}, {"text": "The paraphrase score is used as weight on the answer retrieval phase.", "labels": [], "entities": [{"text": "answer retrieval", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.8301671147346497}]}, {"text": "We use a PMI model as the paraphrase model metric.", "labels": [], "entities": []}, {"text": "Our baseline consists of 800k triples, which is larger than the size of one existing relation triple dataset from.", "labels": [], "entities": []}, {"text": "The whole paraphrase set has 26 million triples.", "labels": [], "entities": []}, {"text": "Note that this is a smaller number of triples but with higher precision, compared with the whole Open IE knowledge base (300 million).", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9995218515396118}, {"text": "Open IE knowledge base", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.8570075482130051}]}, {"text": "When using 800k triples, the recall is 10.7%, whereas we obtain a recall of 34.5% when using 26 million triples.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9997414946556091}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9985802173614502}]}, {"text": "We notice that the performance difference is dramatic.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Comparing different paraphrase models.  Recall on the top 30, based on Freebase.", "labels": [], "entities": [{"text": "Recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9624249935150146}, {"text": "Freebase", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9609805941581726}]}, {"text": " Table 4: Comparing different paraphrase models.  Recall on the top 40, based on Open IE KB.", "labels": [], "entities": [{"text": "Recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9418309926986694}, {"text": "Open IE KB", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.8445278207461039}]}, {"text": " Table 6: Results that compare our system with se- mantic parsing-based question answering systems.", "labels": [], "entities": [{"text": "se- mantic parsing-based question answering", "start_pos": 47, "end_pos": 90, "type": "TASK", "confidence": 0.7674516687790552}]}, {"text": " Table 5: Results that compare our system with other IE-based question answering systems.", "labels": [], "entities": [{"text": "IE-based question answering", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.9052164554595947}]}, {"text": " Table 7: The results of the open question answering  system on the original development set and the one  with expanded answers.", "labels": [], "entities": [{"text": "open question answering", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.6800672809282938}]}]}