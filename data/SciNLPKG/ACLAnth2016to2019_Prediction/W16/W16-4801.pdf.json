{"title": [{"text": "Discriminating Between Similar Languages and Arabic Dialect Identification: A Report on the Third DSL Shared Task", "labels": [], "entities": [{"text": "Discriminating Between Similar Languages and Arabic Dialect Identification", "start_pos": 0, "end_pos": 74, "type": "TASK", "confidence": 0.7543626762926579}, {"text": "Third DSL Shared Task", "start_pos": 92, "end_pos": 113, "type": "DATASET", "confidence": 0.6091286167502403}]}], "abstractContent": [{"text": "We present the results of the third edition of the Discriminating between Similar Languages (DSL) shared task, which was organized as part of the VarDial'2016 workshop at COLING'2016.", "labels": [], "entities": [{"text": "Discriminating between Similar Languages (DSL) shared task", "start_pos": 51, "end_pos": 109, "type": "TASK", "confidence": 0.70681874288453}]}, {"text": "The challenge offered two subtasks: subtask 1 focused on the identification of very similar languages and language varieties in newswire texts, whereas subtask 2 dealt with Arabic dialect identification in speech transcripts.", "labels": [], "entities": [{"text": "identification of very similar languages and language varieties in newswire texts", "start_pos": 61, "end_pos": 142, "type": "TASK", "confidence": 0.7720195596868341}, {"text": "Arabic dialect identification in speech transcripts", "start_pos": 173, "end_pos": 224, "type": "TASK", "confidence": 0.8149217466513315}]}, {"text": "A total of 37 teams registered to participate in the task, 24 teams submitted test results, and 20 teams also wrote system description papers.", "labels": [], "entities": []}, {"text": "High-order character n-grams were the most successful feature, and the best classification approaches included traditional supervised learning methods such as SVM, logistic regression, and language models, while deep learning approaches did not perform very well.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Discriminating between Similar Languages (DSL) shared task on language identification was first organized in 2014.", "labels": [], "entities": [{"text": "Discriminating between Similar Languages (DSL) shared task", "start_pos": 4, "end_pos": 62, "type": "TASK", "confidence": 0.7277543412314521}, {"text": "language identification", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.6725861430168152}]}, {"text": "It provides an opportunity for researchers and developers to test language identification approaches for discriminating between similar languages, language varieties, and dialects.", "labels": [], "entities": [{"text": "language identification", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7446896135807037}]}, {"text": "The task was organized by the workshop series on NLP for Similar Languages, Varieties and Dialects (VarDial), which was collocated in 2014 with COLING, in 2015 with RANLP, and in 2016 again with COLING.", "labels": [], "entities": [{"text": "RANLP", "start_pos": 165, "end_pos": 170, "type": "DATASET", "confidence": 0.6741775274276733}]}, {"text": "In its third edition, the DSL shared task grew in size and scope featuring two subtasks and attracting a record number of participants.", "labels": [], "entities": [{"text": "DSL shared task", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.5700311660766602}]}, {"text": "Below we present the task setup, the evaluation results, and a brief discussion about the features and learning methods that worked best.", "labels": [], "entities": []}, {"text": "More detail about each particular system can be found in the corresponding system description paper, as cited in this report.", "labels": [], "entities": []}], "datasetContent": [{"text": "Regarding evaluation, in the previous editions of the DSL task, we used average accuracy as the main evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.6634536385536194}]}, {"text": "This was because the DSL datasets were balanced with the same number of examples for each language variety.", "labels": [], "entities": [{"text": "DSL datasets", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9218744337558746}]}, {"text": "However, this is not true for this year's Arabic dataset, and thus we added macro-averaged F1-score, which is the official score this year.", "labels": [], "entities": [{"text": "Arabic dataset", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.7606001496315002}, {"text": "F1-score", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9306033849716187}]}, {"text": "Moreover, following common practice in other shared tasks, e.g., at WMT (), this year we carried out statistical significance tests using McNemar's test in order to investigate the variation of performance between the participating systems.", "labels": [], "entities": [{"text": "WMT", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.8856697082519531}, {"text": "McNemar's test", "start_pos": 138, "end_pos": 152, "type": "DATASET", "confidence": 0.7997907201449076}]}, {"text": "Therefore, in all tables with results, we rank teams in groups taking statistical significance into account, 10 rather than using absolute performance only.", "labels": [], "entities": []}, {"text": "A total of 17 teams participated in the shared task.", "labels": [], "entities": []}, {"text": "This means that systems not significantly different to the top system are also assigned rank 1, and soon.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the DSL 2014 shared task: accuracy.", "labels": [], "entities": [{"text": "DSL 2014 shared task", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.5509586781263351}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9994404911994934}]}, {"text": " Table 2: Results for the DSL 2015 shared task: accuracy.", "labels": [], "entities": [{"text": "DSL 2015 shared task", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.5657229945063591}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9994617104530334}]}, {"text": " Table 3: The evolution of the DSL task from 2014 to 2016.", "labels": [], "entities": []}, {"text": " Table 4: DSLCC v3.0: the languages included in the corpus grouped by similarity. Note that a test  example in test set A is an excerpt of text, whereas in test sets B1 and B2 it is a collection of multiple  tweets by the same user (with 98.88 and 50.47 tweets per user on average for B1 and B2, respectively).", "labels": [], "entities": []}, {"text": " Table 6: Teams participating in subtask 1 (here, we group test sets B1 and B2 under B).", "labels": [], "entities": []}, {"text": " Table 8: Results for subtask 1, test set A, open training condition.", "labels": [], "entities": []}, {"text": " Table 9: Results for subtask 1, test set B1, closed training condition.", "labels": [], "entities": []}, {"text": " Table 10: Results for subtask 1, test set B1, open training condition.", "labels": [], "entities": []}, {"text": " Table 11: Results for subtask 1, test set B2, closed training condition.", "labels": [], "entities": []}, {"text": " Table 12: Results for subtask 1, test set B2, open training condition.", "labels": [], "entities": []}, {"text": " Table 13: The teams that participated in subtask 2 (Arabic).", "labels": [], "entities": []}, {"text": " Table 14: Results for subtask 2 (Arabic), closed training condition.", "labels": [], "entities": []}, {"text": " Table 15: Results for subtask 2 (Arabic), open training condition.", "labels": [], "entities": []}]}