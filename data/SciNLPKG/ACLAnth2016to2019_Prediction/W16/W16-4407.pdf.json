{"title": [{"text": "BioMedLAT Corpus: Annotation of the Lexical Answer Type for Biomedical Questions", "labels": [], "entities": [{"text": "BioMedLAT Corpus", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9109584391117096}]}], "abstractContent": [{"text": "Question answering (QA) systems need to provide exact answers for the questions that are posed to the system.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9100612044334412}]}, {"text": "However, this can only be achieved through a precise processing of the question.", "labels": [], "entities": []}, {"text": "During this procedure, one important step is the detection of the expected type of answer that the system should provide by extracting the headword of the questions and identifying its semantic type.", "labels": [], "entities": []}, {"text": "We have annotated the headword and assigned UMLS semantic types to 643 factoid/list questions from the BioASQ training data.", "labels": [], "entities": [{"text": "BioASQ training data", "start_pos": 103, "end_pos": 123, "type": "DATASET", "confidence": 0.9291612903277079}]}, {"text": "We present statistics on the corpus and a preliminary evaluation in baseline experiments.", "labels": [], "entities": []}, {"text": "We also discuss the challenges on both the manual annotation and the automatic detection of the headwords and the semantic types.", "labels": [], "entities": []}, {"text": "We believe that this is a valuable resource for both training and evaluation of biomedical QA systems.", "labels": [], "entities": []}, {"text": "The corpus is available at: https://github.com/mariananeves/BioMedLAT.", "labels": [], "entities": [{"text": "BioMedLAT", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.818432629108429}]}], "introductionContent": [{"text": "Question answering (QA) systems are able of providing exact answers for input questions.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8996367573738098}]}, {"text": "However, coherent answers can only be returned if the system correctly understands the question that is posed.", "labels": [], "entities": []}, {"text": "Ina QA system, the question processing (or understanding) step includes many components, such as linguistic analysis (e.g., tokenization, part-of-speech tagging, semantic role labeling and parsing), question type identification (e.g., yes/no, factoid, definition), lexical answer type (LAT) identification (e.g., protein or disease name) and query construction.", "labels": [], "entities": [{"text": "question processing (or understanding)", "start_pos": 19, "end_pos": 57, "type": "TASK", "confidence": 0.8522544006506602}, {"text": "part-of-speech tagging", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.7288183271884918}, {"text": "semantic role labeling and parsing", "start_pos": 162, "end_pos": 196, "type": "TASK", "confidence": 0.6768655180931091}, {"text": "question type identification", "start_pos": 199, "end_pos": 227, "type": "TASK", "confidence": 0.6318520804246267}, {"text": "lexical answer type (LAT) identification", "start_pos": 265, "end_pos": 305, "type": "TASK", "confidence": 0.5672584261213031}, {"text": "query construction", "start_pos": 342, "end_pos": 360, "type": "TASK", "confidence": 0.8067624568939209}]}, {"text": "In this work we focus on the LAT component of a QA system, i.e., the identification of the expected type of the answer that needs to be returned.", "labels": [], "entities": [{"text": "LAT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8356641530990601}]}, {"text": "This is especially important for factoid questions, i.e., questions that expect an exact and short answer in return, such as a protein or disease name.", "labels": [], "entities": []}, {"text": "The LAT task can be divided into two steps: (i) recognition of the headword, followed by (ii) its classification into predefined type(s).", "labels": [], "entities": [{"text": "LAT task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9224698841571808}]}, {"text": "For instance, in the question \"What hand deformities do patients with Apert syndrome present with?\", \"deformities\" is the headword of the question while \"Sign or Symptom\" is a possible expected type.", "labels": [], "entities": []}, {"text": "Although the field of question answering for biomedicine has evolved in the last years thanks to the many editions of the BioASQ challenges (, researchers still miss important resources to support both development and evaluation of biomedical QA systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7796978950500488}, {"text": "BioASQ challenges", "start_pos": 122, "end_pos": 139, "type": "DATASET", "confidence": 0.7843309938907623}]}, {"text": "BioASQ has provided the community with the most important benchmark in this domain but the dataset does not include information on the expected LAT's.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.886975109577179}, {"text": "LAT", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.8704233169555664}]}, {"text": "The latter is an important detail, which enables both the evaluation of the LAT identification component in biomedical QA systems as well for training in machine-learning-based methods.", "labels": [], "entities": [{"text": "LAT identification", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.9742589890956879}]}, {"text": "We manually annotated a set of 643 questions from the BioASQ training data with the headword and the corresponding UMLS semantic type.", "labels": [], "entities": [{"text": "BioASQ training data", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.9256733655929565}]}, {"text": "We evaluated our annotations using a baseline approach based on dictionary-based matching of UMLS-derived dictionaries.", "labels": [], "entities": []}, {"text": "In this paper, we describe the guidelines and also the results of our annotation process.", "labels": [], "entities": []}, {"text": "We evaluate and discuss on the statistics of the annotations, on the complexity of the annotation task and on the error analysis of our baseline approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe a simple baseline experiment that we performed for evaluation of our corpus.", "labels": [], "entities": []}, {"text": "It included both the extraction of the headword and the identification of the LAT.", "labels": [], "entities": [{"text": "identification of the LAT", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.7059332877397537}]}, {"text": "Similar to previous works, we extracted the headword based on both NER and simple heuristics.", "labels": [], "entities": []}, {"text": "We used the following regular expression to process a question and to extract its headword: ((what |where |which |who) (<(plural) noun> is| are .*)) After the headword extraction, we performed an NER step on the question.", "labels": [], "entities": [{"text": "headword extraction", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.7881743907928467}]}, {"text": "We matched words in the question to UMLS concepts based on various UMLS ontologies.", "labels": [], "entities": []}, {"text": "Given the concepts identified in the question, we checked their overlap with the previously identified headword.", "labels": [], "entities": [{"text": "overlap", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9560423493385315}]}, {"text": "For instance, for the question \"Which genes have been proposed as potential candidates for gene therapy of heart failure?\", we identified \"genes\" as the headword, using the above regular expression.", "labels": [], "entities": []}, {"text": "The same word \"genes\" also matched the UMLS concept \"C0017337\" in the NER step.", "labels": [], "entities": [{"text": "UMLS concept \"C0017337", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.8596333265304565}, {"text": "NER step", "start_pos": 70, "end_pos": 78, "type": "TASK", "confidence": 0.5764518082141876}]}, {"text": "Finally, as the concept \"C0017337\" is linked to the type \"Gene or Genome\" (T028), this is the LAT of the question.", "labels": [], "entities": [{"text": "LAT", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9877259135246277}]}, {"text": "From a total of 643 questions, our baseline experiment correctly detected the semantic types for 184 (28.6%) questions and the semantic groups for 395 (61.4%) of the questions.", "labels": [], "entities": []}, {"text": "The most frequent semantic types that were correctly detected were the following: \"Amino Acid, Peptide, or Protein\" (58), \"Gene or Genome\" (T028) and \"Disease or Syndrome\".", "labels": [], "entities": []}, {"text": "These are also the most frequently annotated types in the corpus, as presented in.", "labels": [], "entities": []}, {"text": "Consequently, the most frequent groups correctly detected by our system were the following: \"Chemicals & Drugs\" (212), \"Disorders\" (54) and \"Concepts & Ideas\" (47).", "labels": [], "entities": []}, {"text": "We could not correctly detect many of the semantic types in our corpus.", "labels": [], "entities": []}, {"text": "summarizes our most frequent errors.", "labels": [], "entities": []}, {"text": "All of our top errors are failures to detect the \"Amino Acid, Peptide, or Protein\" types, given that it contains a variety of headwords.", "labels": [], "entities": []}, {"text": "Finally, many semantic groups that we failed to detect were from the very abstract category \"Concepts & Ideas\".: List of the most frequent errors for the detection of semantic types and groups.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: List of the eleven semantic groups included in the corpus.", "labels": [], "entities": []}, {"text": " Table 5: List of the most frequent errors for the detection of semantic types and groups.", "labels": [], "entities": []}]}