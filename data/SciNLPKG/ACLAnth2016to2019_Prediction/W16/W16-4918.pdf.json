{"title": [{"text": "Grammatical Error Detection Based on Machine Learning for Mandarin as Second Language Learning", "labels": [], "entities": [{"text": "Grammatical Error Detection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8945164084434509}]}], "abstractContent": [{"text": "Mandarin is not simple language for foreigner.", "labels": [], "entities": []}, {"text": "Even using Mandarin as the mother tongue, they have to spend more time to learn when they were child.", "labels": [], "entities": []}, {"text": "The following issues are the reason why causes learning problem.", "labels": [], "entities": []}, {"text": "First, the word is envolved by Hieroglyphic.", "labels": [], "entities": []}, {"text": "So a character can express meanings independently, but become a word has another semantic.", "labels": [], "entities": []}, {"text": "Second, the Mandarin's grammars have flexible rule and special usage.", "labels": [], "entities": []}, {"text": "Therefore, the common grammatical errors can classify to missing, redundant, selection and disorder.", "labels": [], "entities": []}, {"text": "In this paper, we proposed the structure of the Recurrent Neural Networks using Long Short-term memory (RNN-LSTM).", "labels": [], "entities": []}, {"text": "It can detect the error type from the foreign learner writing.", "labels": [], "entities": []}, {"text": "The features based on the word vector and part-of-speech vector.", "labels": [], "entities": []}, {"text": "In the test data found that our method in the detection level of recall better than the others, even as high as 0.9755.", "labels": [], "entities": [{"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9279475808143616}]}, {"text": "That is because we give the possibility of greater choice in detecting errors.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, the rapid development of communication between countries.", "labels": [], "entities": []}, {"text": "Especially the Chinese region, more and more foreign people came to traveling or working.", "labels": [], "entities": []}, {"text": "So the Mandarin become the option as second language learner.", "labels": [], "entities": []}, {"text": "But it is not easy to learn because its grammars are very complexity.", "labels": [], "entities": []}, {"text": "To research Mandarin as second language, we can distinguish two parts: word level and sentence level.", "labels": [], "entities": []}, {"text": "In word level, there have two main aspects are Word Segmentation and Part-of-Speech (POS) Tagging.", "labels": [], "entities": [{"text": "word level", "start_pos": 3, "end_pos": 13, "type": "TASK", "confidence": 0.746386706829071}, {"text": "Word Segmentation", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.753905177116394}, {"text": "Part-of-Speech (POS) Tagging", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.6495212614536285}]}, {"text": "We want to segment the sentence to the basic semantic units and give the correct tagging.", "labels": [], "entities": []}, {"text": "About the research of word segmentation and POS tagging,) the authors proposed using the prefix and suffix query of Chinese word segmentation algorithm for maximum matching.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7738017439842224}, {"text": "POS tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.7851488292217255}]}, {"text": "This structure can choose the best structure of words as the dictionary.", "labels": [], "entities": []}, {"text": "(Li,) the authors proposed joint algorithm to optimize the POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.6590844988822937}, {"text": "dependency parsing", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7748961746692657}]}, {"text": "They use the parsing tree to find the relationship between words and sentence.) the authors proposed the system to word segmentation and POS tagging about Chinese.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.7490744590759277}, {"text": "POS tagging", "start_pos": 137, "end_pos": 148, "type": "TASK", "confidence": 0.7674461305141449}]}, {"text": "They define the 47 class of POS in Chinese and this system is now using in Taiwan Academia Sinica.", "labels": [], "entities": [{"text": "Taiwan Academia Sinica", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.929841419061025}]}, {"text": "And we employ this POS classification in our research.", "labels": [], "entities": [{"text": "POS classification", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.5922262519598007}]}, {"text": "In the word level, the Chinese common grammar error can classify the four parts: Missing, Redundant, Selection, Disorder (see example in).", "labels": [], "entities": []}, {"text": "In the grammar and word order, proposed using latent semi-CRF model on the Chinese phrase classifications.", "labels": [], "entities": []}, {"text": "(Jinjin Zhu and Yangsen Zhang, 2010) the authors proposed auto-detect the Chinese errors by using hybrid algorithm.", "labels": [], "entities": []}, {"text": "They are looking for word, syntax and semantic.", "labels": [], "entities": []}, {"text": "(B. the authors proposed extracting opinion sentence by SVM and syntax template.", "labels": [], "entities": [{"text": "extracting opinion sentence", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.8546289006868998}]}, {"text": "Then in the grammar error detection, (H. H.) the authors proposed Automated Error Detection of ESL (English as a Second Language) Learners.", "labels": [], "entities": [{"text": "grammar error detection", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.5809109608332316}, {"text": "Automated Error Detection of ESL (English as a Second Language) Learners", "start_pos": 66, "end_pos": 138, "type": "TASK", "confidence": 0.8114466025279119}]}, {"text": "And (Chung-Hsien) the authors proposed sentence correction incorporating relative position and parse template language models.", "labels": [], "entities": [{"text": "sentence correction", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7901148796081543}]}, {"text": "They are looking for the English errors.", "labels": [], "entities": [{"text": "English errors", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.7208897769451141}]}, {"text": "Then in Chinese error detection, (Lung-Hao proposed the linguistic rules of Chinese error detection for CFL (Chinese as a For-eign Language).", "labels": [], "entities": [{"text": "Chinese error detection", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.6337783932685852}, {"text": "Chinese error detection", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.6074034372965494}]}, {"text": "And (Chi-Hsin) the authors proposed detect the errors of word order by training the HSK corpus.", "labels": [], "entities": [{"text": "HSK corpus", "start_pos": 84, "end_pos": 94, "type": "DATASET", "confidence": 0.9054895043373108}]}, {"text": "The HSK corpus is simplified Chinese data.", "labels": [], "entities": [{"text": "HSK corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9256363809108734}]}, {"text": "Then (ShukMan Cheng et al., 2014) they also using HSK corpus to proposed word ordering errors detection and correction by SVM to ranking the optimal sentences.: Common grammatical error type In our research, we proposed the architecture for grammatical error detection by recurrent neural network using long-short term memory (RNN-LSTM) as a second language learner.", "labels": [], "entities": [{"text": "HSK corpus", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.8575423061847687}, {"text": "word ordering errors detection", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.7389141321182251}, {"text": "grammatical error detection", "start_pos": 241, "end_pos": 268, "type": "TASK", "confidence": 0.69780433177948}]}, {"text": "We use this architecture to generate the language model and error rule patterns are made based on parsing tree.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we analyse the performance of the proposed architecture.", "labels": [], "entities": []}, {"text": "First, we introduced the corpus in our training model and the evaluation of testing.", "labels": [], "entities": []}, {"text": "Final, we showed the experiment result in training model and the result of NLP-TEA 3 competition.", "labels": [], "entities": []}, {"text": "We used the three datasets from NLP-TEA 1(Yu,) to NLP-TEA 3.", "labels": [], "entities": []}, {"text": "There are two datasets: TOCFL corpus (Traditional Chinese) and HSK corpus (Simplified Chinese), the details are showed in the table 2.", "labels": [], "entities": [{"text": "TOCFL corpus", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.7806776762008667}, {"text": "HSK corpus", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.758269339799881}]}, {"text": "In addition, we used three parameters based on the confusion matrix to evaluate our system.", "labels": [], "entities": []}, {"text": "They are precision, recall, and F1-score and can represented:  First, we wanted to find the optimal class to our language model in the training phase.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9997484087944031}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9994237422943115}, {"text": "F1-score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9995080232620239}]}, {"text": "Therefore, we used the perplexity to evaluate and showed the result in table 3.", "labels": [], "entities": []}, {"text": "In the table, we could seethe 30-class is in average better than other classes.", "labels": [], "entities": []}, {"text": "And we use internal validation and proved the 30-class is better.", "labels": [], "entities": []}, {"text": "Therefore, we chose the 30-class to training and used to the test phase.", "labels": [], "entities": []}, {"text": "Second, we showed the result from NLP-TEA 2016.", "labels": [], "entities": [{"text": "NLP-TEA 2016", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.8921094238758087}]}, {"text": "In detection level (see), our recall is better than other teams.", "labels": [], "entities": [{"text": "detection level", "start_pos": 3, "end_pos": 18, "type": "METRIC", "confidence": 0.7522344291210175}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9997236132621765}]}, {"text": "It means we can find more error rate in dataset.", "labels": [], "entities": []}, {"text": "In addition, our F1-Score is the best in this level.", "labels": [], "entities": [{"text": "F1-Score", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9992177486419678}]}, {"text": "It means our overall is superior to the others, although our precision is less than other teams.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9996453523635864}]}, {"text": "In identification level (see the), it show who can find most error and error type is correct.", "labels": [], "entities": []}, {"text": "In our method, we found that our recall is better than other teams.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9994638562202454}]}, {"text": "It means we find more correct error type than other teams, although our precision is less than other teams.", "labels": [], "entities": [{"text": "correct error type", "start_pos": 22, "end_pos": 40, "type": "METRIC", "confidence": 0.8781314690907797}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9996570348739624}]}, {"text": "Nevertheless, our F1-Score is better than NCTU+NTUT.", "labels": [], "entities": [{"text": "F1-Score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9994235038757324}, {"text": "NCTU+NTUT", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.7931166291236877}]}, {"text": "In Position level, our method that looking for accurate location is not illustrious in this level.", "labels": [], "entities": []}, {"text": "We consider the reasons are our correction is not enough standard.", "labels": [], "entities": [{"text": "correction", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9904796481132507}]}, {"text": "In detection level (see the), our recall is better than other teams.", "labels": [], "entities": [{"text": "detection level", "start_pos": 3, "end_pos": 18, "type": "METRIC", "confidence": 0.8407499492168427}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9997429251670837}]}, {"text": "It means we can find more error rate in dataset.", "labels": [], "entities": []}, {"text": "Although our precision is less than other teams, our F1-Score is better than SKY's method.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9995582699775696}, {"text": "F1-Score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9990766048431396}, {"text": "SKY", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.8444194197654724}]}, {"text": "In Identification level (see the), our recall is better than SKY's method that we can find more correct error type.", "labels": [], "entities": [{"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9996108412742615}, {"text": "SKY", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.8332798480987549}]}, {"text": "However, our precision is less than other teams.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996901750564575}]}, {"text": "In Position level (see the), our method that looking for accurate location is not illustrious in this level.", "labels": [], "entities": []}, {"text": "We consider the reasons are our correction is not enough standard. are the performance with the NLP-TEA 2016 HSK dataset and compare the others team", "labels": [], "entities": [{"text": "correction", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9820539951324463}, {"text": "NLP-TEA 2016 HSK dataset", "start_pos": 96, "end_pos": 120, "type": "DATASET", "confidence": 0.9150025397539139}]}], "tableCaptions": [{"text": " Table 2: The Training Corpus", "labels": [], "entities": []}, {"text": " Table 3: The Perplexity of language model to each type", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9296649098396301}]}]}