{"title": [], "abstractContent": [{"text": "In this paper we present the results of the University of Sheffield (SHEF) submissions for the WMT16 shared task on document-level Quality Estimation (Task 3).", "labels": [], "entities": [{"text": "WMT16 shared task", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.6387847463289896}, {"text": "document-level Quality Estimation", "start_pos": 116, "end_pos": 149, "type": "TASK", "confidence": 0.5224542617797852}]}, {"text": "Our submission explore discourse and document-aware information and word embeddings as features, with Support Vector Regression and Gaussian Process used to train the Quality Estimation models.", "labels": [], "entities": [{"text": "Support Vector Regression", "start_pos": 102, "end_pos": 127, "type": "METRIC", "confidence": 0.9068743586540222}]}, {"text": "The use of word embeddings (combined with baseline features) and a Gaussian Process model with two kernels led to the winning submission in the shared task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of Quality Estimation (QE) of Machine Translation (MT) consists in predicting the quality of unseen data using Machine Learning (ML) models trained on labelled data points.", "labels": [], "entities": [{"text": "Quality Estimation (QE) of Machine Translation (MT)", "start_pos": 12, "end_pos": 63, "type": "TASK", "confidence": 0.7407967312769457}]}, {"text": "Such a scenario does not require reference translations and only uses information from source and target documents.", "labels": [], "entities": []}, {"text": "Therefore, QE is different from traditional automatic evaluation metrics (such as BLEU ().", "labels": [], "entities": [{"text": "QE", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.8353971838951111}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9965974688529968}]}, {"text": "Sentence-level and word-level QE have been widely explored along the years ().", "labels": [], "entities": [{"text": "word-level QE", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.40252430737018585}]}, {"text": "On the other hand, documentlevel QE has only recently started to be addressed, with the first shared task organised last year.", "labels": [], "entities": [{"text": "documentlevel QE", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.6057699918746948}]}, {"text": "Document-level QE is the task of predicting the quality of an entire document and is useful for gisting applications (mainly in cases where the user does not speak the source language) and fully automated uses of MT where post-editing is not an option.", "labels": [], "entities": [{"text": "Document-level QE", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6520808637142181}, {"text": "MT", "start_pos": 213, "end_pos": 215, "type": "TASK", "confidence": 0.896196186542511}]}, {"text": "Predicting the quality of documents is challenging: problems on all linguistic levels need betaken into account, including document-wide issues.", "labels": [], "entities": []}, {"text": "Moreover, defining quality labels for documents is a complex task on itself, as pointed by.", "labels": [], "entities": []}, {"text": "Little previous research has addressed this problem.", "labels": [], "entities": []}, {"text": "explore pseudo-references and document-aware features for document-level ranking, using BLEU as quality label.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9882907867431641}]}, {"text": "apply pseudoreferences, document-aware and discourse-aware features for document-level quality prediction, using BLEU and TER as quality scores.", "labels": [], "entities": [{"text": "document-level quality prediction", "start_pos": 72, "end_pos": 105, "type": "TASK", "confidence": 0.5867784917354584}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9974567294120789}, {"text": "TER", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9834620952606201}]}, {"text": "Last year, a paragraph-level QE shared task was organised for the first time at, using METEOR as quality label.", "labels": [], "entities": [{"text": "QE shared task", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7910042405128479}, {"text": "METEOR", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.7546026706695557}]}, {"text": "Scarton (2015) explore discourse information for paragraph-level prediction.", "labels": [], "entities": [{"text": "paragraph-level prediction", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.8491138517856598}]}, {"text": "They also perform exhaustive search and find out that using only three features from the official baseline set leads to results comparable to those of the full baseline system.", "labels": [], "entities": []}, {"text": "apply referential translation machines for paragraphlevel QE and obtain the best overall results in the shared task.", "labels": [], "entities": [{"text": "referential translation", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.6634575724601746}, {"text": "paragraphlevel QE", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.6975307166576385}]}, {"text": "Finally,,  and analyse the task of document-level QE from the perspective of defining reliable labels.", "labels": [], "entities": [{"text": "QE", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.6456165313720703}]}, {"text": "They also investigate the correlation of discourse phenomena and document-lvel translation quality.", "labels": [], "entities": [{"text": "document-lvel translation", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.6215382069349289}]}, {"text": "In this paper, we focus on feature engineering and the use of different ML techniques for document-level QE in the context of the WMT16 QE shared task (Task 3).", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7856159210205078}, {"text": "WMT16 QE shared task", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.6728109866380692}]}, {"text": "We submitted two systems: \u2022 GRAPH-BASE: counts on pronouns, connectives, Rhetorical Structure Theory (RST) and Elementary Discourse Units (EDUs) information (similar to (), plus scores from an entity graph-based model for the target documents) were used as features.", "labels": [], "entities": [{"text": "GRAPH-BASE", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.990919828414917}]}, {"text": "This system was trained with the Support Vector Regression (SVR) algorithm.", "labels": [], "entities": []}, {"text": "Discourse features were combined with the official baseline features.", "labels": [], "entities": []}, {"text": "\u2022 EMB-BASE-GP: word embeddings from the source documents combined with the official baseline features were used to train a Gaussian Process (GP) 1 with two-kernels: one for word embeddings and one for baseline features.", "labels": [], "entities": [{"text": "EMB-BASE-GP", "start_pos": 2, "end_pos": 13, "type": "METRIC", "confidence": 0.5399200916290283}]}, {"text": "In addition to the official results of our submitted systems, we experiment with other feature combinations, such as scores from graphbased entity grid coherence models extracted from source documents and word embeddings generated for target documents.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the models used in our experiments and in Section 3 we present our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Apart from word embedding features, which use external corpora for training the embeddings, our systems only use the data provided by the task organisers.", "labels": [], "entities": []}, {"text": "Task Our participation is in Task 3 (documentlevel QE) in both scoring and ranking variants.", "labels": [], "entities": [{"text": "documentlevel QE)", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.8179616332054138}]}, {"text": "Pearson r is the official primary evaluation metric for scoring, while Spearman rho is the official primary metric for ranking.", "labels": [], "entities": [{"text": "Pearson r", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.866511344909668}, {"text": "Spearman rho", "start_pos": 71, "end_pos": 83, "type": "METRIC", "confidence": 0.6600991189479828}]}, {"text": "We also experimented with a GP for training QE models using discourse-aware features, but the results were worse than with the SVR model.", "labels": [], "entities": []}, {"text": "An alternative would be to employ Automatic Relevance Determination (ARD), a feature weighting scheme common in GPs and other Bayesian models.", "labels": [], "entities": [{"text": "Automatic Relevance Determination (ARD)", "start_pos": 34, "end_pos": 73, "type": "METRIC", "confidence": 0.7084083259105682}]}, {"text": "However, This would add a large number of hyperparameters in our case (one per feature/dimension), making the model difficult to optimise and prone to overfitting.", "labels": [], "entities": []}, {"text": "Besides RatQuad, we also experimented with RBF, Exponential and Matern32 kernels.", "labels": [], "entities": [{"text": "RBF", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.49517083168029785}]}, {"text": "RatQuad showed the best results.", "labels": [], "entities": [{"text": "RatQuad", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9344002604484558}]}, {"text": "Data The data of Task 3 consists of 208 documents for English-Spanish language pair, extracted from the WMT08-13 translation shared task datasets.", "labels": [], "entities": [{"text": "WMT08-13 translation shared task datasets", "start_pos": 104, "end_pos": 145, "type": "DATASET", "confidence": 0.8448984026908875}]}, {"text": "The machine translation for each source document was randomly picked from the set of all systems that participated in the translation task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7139845341444016}]}, {"text": "The documents were evaluated by following the two-stage post-editing method described in).", "labels": [], "entities": []}, {"text": "In the first stage, sentences are post-edited out of context, whilst in the second stage the post-edited sentences are placed in context and any remaining mistakes are corrected.", "labels": [], "entities": []}, {"text": "The quality scores are, then, a variation of Human-targeted Translation Edit Rate (HTER)) that combines results from both post-editing stages.", "labels": [], "entities": [{"text": "Human-targeted Translation Edit Rate (HTER))", "start_pos": 45, "end_pos": 89, "type": "METRIC", "confidence": 0.750791243144444}]}, {"text": "Baseline We use the 17 QUEST++ baseline features to train our baseline systems ( . We build a baseline system with SVR and another with GP, in order to compare our systems with comparable models.", "labels": [], "entities": [{"text": "GP", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.8606160283088684}]}, {"text": "Models using discourse features and SVR The features sets we experimented with are: \u2022 baseline + PCER + LSA + GRAPH-target + GRAPH-source; \u2022 baseline + PCER + LSA + GRAPH-target.", "labels": [], "entities": [{"text": "GRAPH-target", "start_pos": 110, "end_pos": 122, "type": "METRIC", "confidence": 0.9651131629943848}, {"text": "GRAPH-source", "start_pos": 125, "end_pos": 137, "type": "METRIC", "confidence": 0.8728846311569214}, {"text": "GRAPH-target", "start_pos": 165, "end_pos": 177, "type": "METRIC", "confidence": 0.9595140218734741}]}, {"text": "11 Our models using discourse information were trained with SVR as described in Section 2.1.", "labels": [], "entities": []}, {"text": "Models using word embeddings and GP The features sets we experimented with are: \u2022 baseline + EMB-source + EMB-target; \u2022 baseline + EMB-source; 12 \u2022 EMB-source; Our models using word embeddings were trained using GP as described in Section 2.2.", "labels": [], "entities": []}, {"text": "Model selection The best models for our submissions are selected by applying 10-fold crossvalidation in the training set and choosing the model with the highest averaged Pearson r correlation.", "labels": [], "entities": [{"text": "Pearson r correlation", "start_pos": 170, "end_pos": 191, "type": "METRIC", "confidence": 0.9701982140541077}]}, {"text": "The ranks for the ranking task variant are defined by ordering the predicted values best to worst.", "labels": [], "entities": []}, {"text": "shows the results for our experiments with discourse-aware features and SVR for the scoring sub-task.", "labels": [], "entities": []}, {"text": "We report results of our 10-fold crossvalidation method over the training and the results on the official test set.", "labels": [], "entities": [{"text": "official test set", "start_pos": 97, "end_pos": 114, "type": "DATASET", "confidence": 0.7793573538462321}]}, {"text": "Results in the first column (10-fold) show that both discourse feature combination lead to improvements over the baseline.", "labels": [], "entities": []}, {"text": "However, when testing on the test set, the models do not outperform the baseline.", "labels": [], "entities": []}, {"text": "More investigation with additional data would be necessary to draw any conclusions on the reasons behind this difference.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson r correlation scores of models built with discourse-aware features and SVR.", "labels": [], "entities": [{"text": "Pearson r correlation scores", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9064550250768661}]}, {"text": " Table 2: Spearman rho correlation scores of models built with discourse-aware features and SVR.", "labels": [], "entities": []}, {"text": " Table 4: Spearman rho correlation scores of models built with word-embeddings features and GP.", "labels": [], "entities": [{"text": "Spearman rho correlation scores", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.6099887862801552}, {"text": "GP", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.8586845397949219}]}]}