{"title": [{"text": "Residual Stacking of RNNs for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.7077812353769938}]}], "abstractContent": [{"text": "To enhance Neural Machine Translation models, several obvious ways such as enlarging the hidden size of recurrent layers and stacking multiple layers of RNN can be considered.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.8671303192774454}]}, {"text": "Surprisingly, we observe that using naively stacked RNNs in the decoder slows down the training and leads to degradation in performance.", "labels": [], "entities": []}, {"text": "In this paper, We demonstrate that applying residual connections in the depth of stacked RNNs can help the optimization, which is referred to as residual stacking.", "labels": [], "entities": [{"text": "residual stacking", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.6973975896835327}]}, {"text": "In empirical evaluation, residual stacking of decoder RNNs gives superior results compared to other methods of enhancing the model with a fixed parameter budget.", "labels": [], "entities": [{"text": "residual stacking of decoder RNNs", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.7455966591835022}]}, {"text": "Our submitted systems in WAT2016 are based on a NMT model ensemble with residual stacking in the decoder.", "labels": [], "entities": []}, {"text": "To further improve the performance, we also attempt various methods of system combination in our experiments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, the performance of machine translation is greatly improved by applying neural networks partially in a Statistical Machine Translation (SMT) pipeline ( or training a end-to-end neural network based machine translation model).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.8315868675708771}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 112, "end_pos": 149, "type": "TASK", "confidence": 0.7772340228160223}]}, {"text": "The latter approach, or Neural Machine Translation (NMT), possess several advantages compared to conventional SMT approaches.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.7875301440556844}, {"text": "SMT", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9865967631340027}]}, {"text": "Firstly, as the translation process is done with a single network, the pipeline of NMT training is simpler.", "labels": [], "entities": [{"text": "translation", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.9742185473442078}, {"text": "NMT training", "start_pos": 83, "end_pos": 95, "type": "TASK", "confidence": 0.9207242429256439}]}, {"text": "This advantage also indicates a much lower implementation cost.", "labels": [], "entities": []}, {"text": "Developing a SMT decoder will cost one to three months fora graduate student, while training a NMT model requires merely a script that contains the network definition.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9211264252662659}]}, {"text": "Secondly, the memory consumption is vastly reduced when translating with a NMT model.", "labels": [], "entities": []}, {"text": "While a conventional phrase-based model trained on a large bilingual corpus can easily consumes over 100GB memory space, a trained NMT model typically requires less than 1GB GPU on-board memory in test time.", "labels": [], "entities": []}, {"text": "The most significant difference of NMT approach in contrast with SMT is that the information for producing the translation is held in continuous space but not discrete values.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9831035137176514}]}, {"text": "While this characteristics of neural network introduces huge difficulties in debugging, this allows the model to learn translation knowledge beyond the conventional hand-crafted MT framework.", "labels": [], "entities": [{"text": "MT", "start_pos": 178, "end_pos": 180, "type": "TASK", "confidence": 0.9551204442977905}]}, {"text": "Recently, even the word embeddings obtained from NMT training demonstrates advantages over specific tasks ().", "labels": [], "entities": [{"text": "NMT training", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.8192292153835297}]}, {"text": "The network architectures of NMT models are simple but effective.", "labels": [], "entities": []}, {"text": "It produces a sentence representation with the encoder, then the decoder generates the translation from the vector of sentence representation.", "labels": [], "entities": []}, {"text": "Generally the encoder is composed of a single recurrent neural network (RNN).", "labels": [], "entities": []}, {"text": "The decoder reads the vector representations created by the encoder and produce a series of output vectors.", "labels": [], "entities": []}, {"text": "A linear transformation is applied to each vector in order to create a large softmax layer in size of the whole vocabulary.", "labels": [], "entities": []}, {"text": "Soft attention mechanism introduced in () further boosted the performance by increasing the computational complexity.", "labels": [], "entities": []}, {"text": "With the advance of deep learning, deeper network architectures are favored as the models become more expressive with more layers.", "labels": [], "entities": []}, {"text": "An absolute way to deepening NMT models is to stack multiple layers of RNN in either encoder or decoder side.", "labels": [], "entities": []}, {"text": "However, our experiments show that stacking RNNs naively in the decoder will cause significant slowdown in training and results in degraded performance.", "labels": [], "entities": []}, {"text": "In this paper, we explores the effects of applying residual connection ( ) between stacked RNNs in the decoder.", "labels": [], "entities": []}, {"text": "We found the residual connection successfully helps the deepened NMT model, which leads to a performance gain of evaluation scores.", "labels": [], "entities": []}, {"text": "In our submitted systems for English-Japanese translation task in WAT2016 (), we also attempts to combination the advantages of Tree-to-String (T2S) SMT systems and NMT models.", "labels": [], "entities": [{"text": "English-Japanese translation task", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.7073927819728851}, {"text": "WAT2016", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.6251047849655151}, {"text": "SMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.8734735250473022}]}, {"text": "Specifically, we experimented combination methods with both simple heuristics and Minimum Bayesian Risk (MBR) based approach (Duh et al., 2011).", "labels": [], "entities": [{"text": "Minimum Bayesian Risk (MBR)", "start_pos": 82, "end_pos": 109, "type": "METRIC", "confidence": 0.8085190455118815}]}], "datasetContent": [{"text": "In, we show the empirical evaluation results of the fore-mentioned models.", "labels": [], "entities": []}, {"text": "Enlarging the singlelayer decoder RNN to 1400 hidden units boosted BLEU by 0.92%.", "labels": [], "entities": [{"text": "RNN", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7327077388763428}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.999261200428009}]}, {"text": "Surprisingly, stacking another layer of decoder RNN degraded the performance in automatic evaluation.", "labels": [], "entities": []}, {"text": "This is also confirmed by a significant slowdown of the decreasing of the validation loss.", "labels": [], "entities": [{"text": "validation", "start_pos": 74, "end_pos": 84, "type": "TASK", "confidence": 0.9680383801460266}]}, {"text": "With the residual stacking, we get the best performance in all four model variations.", "labels": [], "entities": []}, {"text": "In, we show the automatic evaluation scores together with pairwise crowdsourcing evaluation scores for our submitted systems 1 in WAT2016.", "labels": [], "entities": [{"text": "WAT2016", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.8881365060806274}]}, {"text": "The first submission, which is a two-model ensemble of NMT models with residual decoder RNNs, achieved a comparably high RIBES () score.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.9918174147605896}]}, {"text": "For system combination, although we gained improvement by applying GMBR method 2 , the naive method of system combination based on the length of input sentence works better in the evaluation with test data.", "labels": [], "entities": [{"text": "system combination", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8270505368709564}, {"text": "GMBR", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.8231979012489319}, {"text": "system combination", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7070372998714447}]}, {"text": "Considering the scores of human evaluation, the system combination does make significant difference compared to the NMT model ensemble.", "labels": [], "entities": []}, {"text": "achieved similar classification performance when using shared weights in a ResNet, which is exactly a RNN.", "labels": [], "entities": []}, {"text": "Pixel Recurrent Neural Networks (van den) demonstrates a novel architecture of neural nets with two-dimensional recurrent neural nets using residual connections.", "labels": [], "entities": []}, {"text": "Their models achieved better log-likelihood on image generation tasks.", "labels": [], "entities": [{"text": "image generation tasks", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.8073897361755371}]}, {"text": "Remarkably, the neural network architecture described in a lecture report is similar to our models in spirit, where they applied stochastic residual learning to both depth and horizontal timesteps, which leads to better classification accuracy in Stanford Sentiment Treebank dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 235, "end_pos": 243, "type": "METRIC", "confidence": 0.9256013035774231}, {"text": "Stanford Sentiment Treebank dataset", "start_pos": 247, "end_pos": 282, "type": "DATASET", "confidence": 0.90798220038414}]}], "tableCaptions": [{"text": " Table 1: Automatic evaluation results in  English-Japanese translation task on ASPEC  corpus. Both baseline model and enlarged de- coder RNN has a single-layer RNN with 1000  and 1400 hidden units respectively in decoder.  Stacked decoder RNN and residual decoder  RNN are both composed of two-layer RNNs  with 1000 hidden units each.", "labels": [], "entities": [{"text": "ASPEC  corpus", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.8259028494358063}]}, {"text": " Table 2: Official evaluation results of WAT2016", "labels": [], "entities": [{"text": "WAT2016", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.8257543444633484}]}]}