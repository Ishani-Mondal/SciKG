{"title": [{"text": "Model Combination for Correcting Preposition Selection Errors *", "labels": [], "entities": []}], "abstractContent": [{"text": "Many grammatical error correction approaches use classifiers with specially-engineered features to predict corrections.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 5, "end_pos": 33, "type": "TASK", "confidence": 0.6496155758698782}]}, {"text": "A simpler alternative is to use n-gram language model scores.", "labels": [], "entities": []}, {"text": "Rozovskaya and Roth (2011) reported that clas-sifiers outperformed a language modeling approach.", "labels": [], "entities": []}, {"text": "Here, we report a more nuanced result: a classifier approach yielded results with higher precision while a language modeling approach provided better recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9962485432624817}, {"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.997588038444519}]}, {"text": "Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we compare methods for correcting grammatical errors.", "labels": [], "entities": [{"text": "correcting grammatical errors", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.8783941864967346}]}, {"text": "Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers).", "labels": [], "entities": [{"text": "grammatical error detection and correction", "start_pos": 29, "end_pos": 71, "type": "TASK", "confidence": 0.8329204320907593}]}, {"text": "In particular, and  found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives)., however, reported that a language modeling approach substantially outperformed a classifier using contextual features.", "labels": [], "entities": [{"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9988228678703308}]}, {"text": "Finally, found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best.", "labels": [], "entities": []}, {"text": "* Michael Heilman is now a data scientist at Civis Analytics.", "labels": [], "entities": []}, {"text": "Much of the previous work has used well-formed text when training contextual classifiers due to the lack of large error-annotated corpora.", "labels": [], "entities": []}, {"text": "conducted experiments with a relatively small error-annotated corpus and showed that it outperformed a contextual classifier trained on well-edited text.", "labels": [], "entities": []}, {"text": "More recently, mined Wikipedia revisions to produce a large, publicly available error-annotated corpus and reported similar results on multiple, publicly available data sets.", "labels": [], "entities": []}, {"text": "Our goal in this paper is not to build a state-ofthe-art system but rather to investigate the following research questions: \u2022 Does a contextual classifier trained on errorannotated data outperform a language modeling approach?", "labels": [], "entities": []}, {"text": "\u2022 Can a classifier trained on error-annotated data and the language modeling approach be effectively combined?", "labels": [], "entities": []}, {"text": "With respect to the second question, previously reported that a combination of a contextual classifier trained on well-edited text and a language modeling approach outperformed each individual method.", "labels": [], "entities": []}, {"text": "However, given that the performance of his classifier was lower than what has been reported on other datasets, we believe it is worth reinvestigating the merits of system combination but with publicly available data sets and with a classifier trained on error-annotated data instead of on well-edited text.", "labels": [], "entities": []}, {"text": "This work differs from in that we are interested in combining statistical models in order to more accurately correct individual preposition errors, while their work combined -at the sentence level -the outputs of multiple systems designed to correct different types of grammatical errors.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use two data sets for evalution: (a) The CLC FCE dataset, which contains exam scripts written by English language learners for the Cambridge ESOL First Certificate in, with 20% held out for development, and (b) The HOO 2011 shared task data set, which contains excerpts of ACL papers manually annotated for grammatical errors (.", "labels": [], "entities": [{"text": "CLC FCE dataset", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9391467769940695}, {"text": "Cambridge ESOL First Certificate", "start_pos": 134, "end_pos": 166, "type": "DATASET", "confidence": 0.9093178808689117}, {"text": "HOO 2011 shared task data set", "start_pos": 218, "end_pos": 247, "type": "DATASET", "confidence": 0.8920005559921265}]}, {"text": "No HOO data was used for development.", "labels": [], "entities": [{"text": "HOO data", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.7377849966287613}]}], "tableCaptions": [{"text": " Table 1: P, R and F1 scores for the FCE and HOO test sets. Bold", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9660196304321289}, {"text": "FCE and HOO test sets", "start_pos": 37, "end_pos": 58, "type": "DATASET", "confidence": 0.7672271370887757}]}]}