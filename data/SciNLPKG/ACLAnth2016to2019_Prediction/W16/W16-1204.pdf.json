{"title": [{"text": "Enhancing Automatic Wordnet Construction Using Word Embeddings", "labels": [], "entities": [{"text": "Enhancing Automatic Wordnet Construction", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8215908855199814}]}], "abstractContent": [{"text": "Researchers have shown that a wordnet fora new language, possibly resource-poor, can be constructed automatically by translating wordnets of resource-rich languages.", "labels": [], "entities": []}, {"text": "The quality of these constructed wordnets is affected by the quality of the resources used such as dictionaries and translation methods in the construction process.", "labels": [], "entities": []}, {"text": "Recent work shows that vector representation of words (word em-beddings) can be used to discover related words in text.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method that performs such similarity computation using word embeddings to improve the quality of automatically constructed wordnets.", "labels": [], "entities": []}], "introductionContent": [{"text": "A wordnet is a lexical ontology of words.", "labels": [], "entities": []}, {"text": "Highquality wordnets have been developed for only a few languages.", "labels": [], "entities": []}, {"text": "Wordnets, other than the Princeton WordNet (PWN)), are typically constructed by one of two approaches.", "labels": [], "entities": [{"text": "Princeton WordNet (PWN))", "start_pos": 25, "end_pos": 49, "type": "DATASET", "confidence": 0.9470160365104675}]}, {"text": "The translation approach translates the PWN to target languages ().", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9603506326675415}]}, {"text": "In contrast, the merge approach builds the semantic taxonomy of a wordnet in a target language, and then aligns it with the Princeton WordNet by generating translations.", "labels": [], "entities": [{"text": "Princeton WordNet", "start_pos": 124, "end_pos": 141, "type": "DATASET", "confidence": 0.8914075493812561}]}, {"text": "In this paper, we propose a method to enhance the translation approach using word embeddings produced by the word2vec algorithm (.", "labels": [], "entities": []}, {"text": "We produce wordnets in several languages although the current paper focuses only on the new Arabic wordnet we construct.", "labels": [], "entities": []}], "datasetContent": [{"text": "To construct the core wordnet, i.e.,   In order to compute the synset similarity threshold value \u03b1 and the threshold for each semantic relation \u03b1 \u03c1 , we use the freely available Arabic wordnet (AWN) (.", "labels": [], "entities": [{"text": "synset similarity threshold value \u03b1", "start_pos": 63, "end_pos": 98, "type": "METRIC", "confidence": 0.7086039364337922}]}, {"text": "AWN was manually constructed in 2006 and has been semiautomatically enhanced and extended several times.", "labels": [], "entities": [{"text": "AWN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7730696201324463}]}, {"text": "We start by extracting synonym words, semantically related words, and non-related words from AWN.", "labels": [], "entities": [{"text": "AWN", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.9314239621162415}]}, {"text": "Then, we use the histogram representation of the cosine similarity of the previous sets of words to set the thresholds.", "labels": [], "entities": []}, {"text": "As shows, more than 67% of the non-related words have cosine similarity less than 0.1, while about 23% of the synonym words in    AWN have a cosine similarity less than 0.1.", "labels": [], "entities": [{"text": "AWN", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.9142481684684753}]}, {"text": "Furthermore, about 34% of the semantically related words in AWN have cosine similarity less than 0.1.", "labels": [], "entities": []}, {"text": "shows the weighted average cosine similarity between synonyms, hypernyms, topic-domain related, part-holonyms, instance-hypernyms, and membermeronyms in AWN where the frequency of the similarity value is the weight.", "labels": [], "entities": [{"text": "AWN", "start_pos": 153, "end_pos": 156, "type": "DATASET", "confidence": 0.8963863253593445}]}, {"text": "We compute cosine similarity between semantically related words extracted from our initial Arabic wordnet produced in Section 4.2.", "labels": [], "entities": []}, {"text": "The language model to calculate the cosine similarity is created using CBOW with vector size=100 and window size=3.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.7887523770332336}]}, {"text": "shows a comparison between the number of Arabic synsets we create and the number of synsets in AWN.", "labels": [], "entities": [{"text": "AWN", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.9471729397773743}]}, {"text": "We notice that the translation method we use produces high number of synsets compared to the manually constructed AWN.", "labels": [], "entities": [{"text": "AWN", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.8790779113769531}]}, {"text": "However, the number of synsets sharply decreases after filtering the initial synonyms using the method described in Section 3.", "labels": [], "entities": []}, {"text": "Although our Arabic wordnet is automatically created, the number of synsets we create is 60% of the number of synsets in the manually created AWN  when filtering the synsets using \u03b1= 0.1.", "labels": [], "entities": [{"text": "AWN", "start_pos": 142, "end_pos": 145, "type": "DATASET", "confidence": 0.9340867400169373}]}, {"text": "We evaluate precision by comparing 600 pairs of synonyms, hypernyms, part-holonyms, and member-meronyms with three ranges of cosine similarity values: 0 to 0.1, 0.1 to 0.288, and 0.288 to 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9991618394851685}]}, {"text": "We asked 3 Arabic speakers to evaluate the pairs using a 0 to 5 scale where 0 represents the minimum score and 5 represents the maximum score.", "labels": [], "entities": []}, {"text": "We compute precision by taking the average score and converting it to a percentage.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9994041919708252}]}, {"text": "The precision of the synonyms, hypernyms, partholonyms, and member-meronyms we produce is 78.4%, 84.4%, 90.4%, and 79.6% respectively, with the threshold set to 0.288.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9992400407791138}]}, {"text": "This is higher than the precision obtained by) which produces synonyms with 76.4% precision when just using PWN.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9992004036903381}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9986514449119568}, {"text": "PWN", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.9191454648971558}]}, {"text": "Our results suggest that using lower precision for producing synsets reduces the quality of the other created semantic relations.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9680027365684509}]}, {"text": "Our results clearly show that pairs with higher cosine similarity are more likely to be semantically related.", "labels": [], "entities": []}, {"text": "It confirms the benefit of combining the translation method with word embeddings in the process of automatically generating new wordnets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: An example of cosine similarity between words in a", "labels": [], "entities": []}, {"text": " Table 4: Comparison between the weighted similarity average", "labels": [], "entities": []}, {"text": " Table 5: Comparison between the number of synsets in AWN", "labels": [], "entities": [{"text": "AWN", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.8582149147987366}]}, {"text": " Table 6: Precision of the Arabic wordnet we create.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.962350606918335}]}]}