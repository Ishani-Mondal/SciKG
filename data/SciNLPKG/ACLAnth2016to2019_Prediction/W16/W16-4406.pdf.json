{"title": [{"text": "Answer Presentation in Question Answering over Linked Data using Typed Dependency Subtree Patterns", "labels": [], "entities": [{"text": "Answer Presentation in Question Answering", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7543665528297424}]}], "abstractContent": [{"text": "In an era where highly accurate Question Answering (QA) systems are being built using complex Natural Language Processing (NLP) and Information Retrieval (IR) algorithms, presenting the acquired answer to the user akin to a human answer is also crucial.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.8404188394546509}]}, {"text": "In this paper we present an answer presentation strategy by embedding the answer in a sentence which is developed by incorporating the linguistic structure of the source question extracted through typed dependency parsing.", "labels": [], "entities": [{"text": "answer presentation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8872664868831635}, {"text": "typed dependency parsing", "start_pos": 197, "end_pos": 221, "type": "TASK", "confidence": 0.7009841998418173}]}, {"text": "The evaluation using human participants proved that the methodology is human-competitive and can result in linguistically correct sentences for more that 70% of the test dataset acquired from QALD question dataset.", "labels": [], "entities": [{"text": "QALD question dataset", "start_pos": 192, "end_pos": 213, "type": "DATASET", "confidence": 0.8452413280804952}]}], "introductionContent": [{"text": "In this research we focus on generating a sentence which formulates the answer as a natural language sentence and presents it in a more natural form.", "labels": [], "entities": []}, {"text": "In particular, if we ask a question to a person, he/she has the ability to answer with a sentence or sentences which has the answer embedded in a context.", "labels": [], "entities": []}, {"text": "This form of answering a question is more natural compared to the bare factoid answer delivered by most QA systems.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses the framework that generates the answer sentences.", "labels": [], "entities": []}, {"text": "Section 3 explains the experimental framework that evaluates the framework and the results.", "labels": [], "entities": []}, {"text": "We also provide a detailed discussion on results in this section.", "labels": [], "entities": []}, {"text": "Related work and comparison of our approach to existing work is discussed in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper with an overview of the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We were able to identify 18 distinct wh-interrogative patterns and 7 polar interrogative patterns.", "labels": [], "entities": []}, {"text": "Using these patterns, answer sentences were generated for the testing dataset with a 78.84% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9956350922584534}]}, {"text": "Except for 11 questions where the framework completely failed to generate answer sentences, all others were syntactically and semantically accurate.", "labels": [], "entities": []}, {"text": "These 11 questions include 5 wh-interrogatives and 6 polar interrogatives.", "labels": [], "entities": []}, {"text": "The framework failed to generate answer sentences for these questions mainly due to the absence of rules (for 10 questions) and the errors in the typed dependency parse (for 1 question).", "labels": [], "entities": []}, {"text": "The top-10 patterns were able successfully cover 69.19% of the questions from the testing dataset.", "labels": [], "entities": []}, {"text": "Furthermore, the coverage of 51.91% of the questions through top-4 patterns shows that the top patterns are highly representative.", "labels": [], "entities": [{"text": "coverage", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.953930675983429}]}, {"text": "We also carried out a human evaluation using three postgraduate students chosen on the basis of having acceptable level of competency in English.", "labels": [], "entities": []}, {"text": "The results show that the participants rated the answer sentences with a Cronbach's Alpha values of 0.842 and 0.771 for accuracy and readability respectively.", "labels": [], "entities": [{"text": "Cronbach's Alpha", "start_pos": 73, "end_pos": 89, "type": "METRIC", "confidence": 0.5771209100882212}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9991897940635681}]}, {"text": "depicts the weighted average of rating values provided for both accuracy and readability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.998927652835846}]}, {"text": "According to the figure it is clear that the ratings reside between 4 and 5 in the 5-point Likert scale.", "labels": [], "entities": [{"text": "Likert scale", "start_pos": 91, "end_pos": 103, "type": "METRIC", "confidence": 0.5578492283821106}]}, {"text": "Furthermore, weighted average rating average for readability is recoded as 5 for 37 cases (90.24% from the test collection) while weighted average rating average for accuracy is recorded as 5 for 31 cases (75.6% from the test collection).", "labels": [], "entities": [{"text": "weighted average rating average", "start_pos": 13, "end_pos": 44, "type": "METRIC", "confidence": 0.8446914404630661}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9965319633483887}]}, {"text": "This shows that the framework has achieved reasonable readability and accuracy levels from the user perspective.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9989737272262573}]}, {"text": "4 Related work present the cooperative question answering approach which generates natural language responses forgiven questions.", "labels": [], "entities": [{"text": "question answering", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.726949617266655}]}, {"text": "In essence, a cooperative QA system moves a few steps further from ordinary question answering systems by providing an explanation of the answer, describing if the system is unable to find an answer or by providing links to the user to get more information for the given question.", "labels": [], "entities": [{"text": "question answering", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7316190898418427}]}, {"text": "A successful attempt to move beyond the exact answer presentation with additional information in sentence form is presented by utilizing summarization techniques.", "labels": [], "entities": [{"text": "summarization", "start_pos": 137, "end_pos": 150, "type": "TASK", "confidence": 0.9644882678985596}]}, {"text": "In this research assumes that a QA system has already extracted a sentence that contains the exact answer.", "labels": [], "entities": []}, {"text": "He coins the term an \"intensive answer\" to refer to the answer generated from the system.", "labels": [], "entities": []}, {"text": "The process of generating intensive answer is based on summarization using rhetorical structures.", "labels": [], "entities": [{"text": "summarization", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9634361863136292}]}, {"text": "Vargas-Vera and Motta (2004) present an ontology based QA system, AQUA.", "labels": [], "entities": [{"text": "AQUA", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.5815768241882324}]}, {"text": "Although AQUA is primarily aimed at extracting answers from a given ontology, it also contributes to answer presentation by providing an enriched answer.", "labels": [], "entities": [{"text": "answer presentation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.9156977832317352}]}, {"text": "The AQUA system extracts ontology concepts from the entities mentioned in the question and present those concepts in aggregated natural language.", "labels": [], "entities": []}, {"text": "However, the benefit that researchers achieved by building the enriching module on top of an ontology is that the related information can be easily acquired using the relations in the ontology.", "labels": [], "entities": []}], "tableCaptions": []}