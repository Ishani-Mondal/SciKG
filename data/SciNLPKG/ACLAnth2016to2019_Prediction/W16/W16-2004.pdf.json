{"title": [{"text": "EHU at the SIGMORPHON 2016 Shared Task. A Simple Proposal: Grapheme-to-Phoneme for Inflection", "labels": [], "entities": [{"text": "EHU at the SIGMORPHON 2016 Shared Task", "start_pos": 0, "end_pos": 38, "type": "DATASET", "confidence": 0.650073664528983}]}], "abstractContent": [{"text": "This paper presents a proposal for learning morphological inflections by a grapheme-to-phoneme learning model.", "labels": [], "entities": []}, {"text": "No special processing is used for specific languages.", "labels": [], "entities": []}, {"text": "The starting point has been our previous research on induction of phonology and morphology for normalization of historical texts.", "labels": [], "entities": [{"text": "normalization of historical texts", "start_pos": 95, "end_pos": 128, "type": "TASK", "confidence": 0.8809991180896759}]}, {"text": "The results show that a very simple method can indeed improve upon some baselines, but does not reach the accuracies of the best systems in the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "In our previous work carried out in the context of normalization of historical texts we proposed an approach based on the induction of phonology.", "labels": [], "entities": [{"text": "normalization of historical texts", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.8811175972223282}]}, {"text": "We obtained good results using only induced phonological weighted finite-state transducers (WFSTs), i.e. by leveraging the phoneme-to-grapheme method to yield a grapheme-to-grapheme model.", "labels": [], "entities": []}, {"text": "The research question now is if the grapheme-to-grapheme model can be extended to handle morphological information instead of words or morphological segmentation.", "labels": [], "entities": []}, {"text": "To assess this, we test a general solution that works without special processing for specific languages (i.e. we do not focus on special treatment of accents in Spanish and other idiosyncracies).", "labels": [], "entities": []}], "datasetContent": [{"text": "We have measured the quality using the metrics and the script provided by the organizers; the baseline figures also originate with the organizers.", "labels": [], "entities": []}, {"text": "In all the languages whole tags were injected as prefixes and suffixes, with the exception of Finnish, wherein the prefix tag position only the first character is included.", "labels": [], "entities": []}, {"text": "For example, for the wordform aakkostot 'alphabets' N+aakkosto+N9 is used instead of N9+aakkosto+N9.", "labels": [], "entities": []}, {"text": "For the submitted final test we retrained the transducer adding the development section to the training corpus.", "labels": [], "entities": []}, {"text": "As can be seen in table 1, a slight improvement was obtained (0.43% on average).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the test corpus using 1-best  accuracy for evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.807515025138855}]}, {"text": " Table 2: Accuracy when using a word list for filter- ing the proposals from the WFST. The first column  shows the results without any external resources  used; in the second column a word list has been  used for filtering the top 3 proposals and in the  third column for filtering with the top 5 proposals.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9934677481651306}, {"text": "WFST", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9321633577346802}]}]}