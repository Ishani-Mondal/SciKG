{"title": [{"text": "Unsupervised Resolution of Acronyms and Abbreviations in Nursing Notes Using Document-Level Context Models", "labels": [], "entities": [{"text": "Unsupervised Resolution of Acronyms and Abbreviations in Nursing Notes", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.7579343650076125}]}], "abstractContent": [{"text": "Automatic simplification of clinical notes continues to bean important challenge for NLP systems.", "labels": [], "entities": []}, {"text": "A frequent obstacle to developing more robust NLP systems for the clinical domain is the lack of annotated training data.", "labels": [], "entities": []}, {"text": "This study investigates unsupervised techniques for one key aspect of medical text simplification, viz.", "labels": [], "entities": [{"text": "medical text simplification", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.6516804397106171}]}, {"text": "the expansion and disam-biguation of acronyms and abbreviations.", "labels": [], "entities": []}, {"text": "Our approach combines statistical machine translation with document-context neural language models for the disambiguation of multi-sense terms.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6305253207683563}]}, {"text": "In addition we investigate the use of mismatched training data and self-training.", "labels": [], "entities": []}, {"text": "These techniques are evaluated on nursing progress notes and obtain a disambiguation accuracy of 71.6% without any manual annotation effort.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 70, "end_pos": 84, "type": "METRIC", "confidence": 0.990500807762146}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.7886658310890198}]}], "introductionContent": [{"text": "As part of a general trend towards patient-centered care many healthcare systems in the U.S. are starting to provide patients with expanded access to clinical notes, often through patient portals connected to their electronic medical record (EMR) systems.", "labels": [], "entities": []}, {"text": "Recent studies, such as the OpenNotes project), have found that that patients with access to their health records are more involved in their care and have a better understanding of their treatment plan (.", "labels": [], "entities": []}, {"text": "However, medical notes often contain complex technical language and medical jargon, requiring patients to seek additional help for linguistic clarification (.", "labels": [], "entities": []}, {"text": "Natural language processing (NLP) has the potential to bridge the gap between increased access to medical information and the lack of domain-specific medical training on the patient side.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7549436340729395}]}, {"text": "However, in spite of previous work in this area, medical text simplification systems are still not sufficiently mature to be routinely deployed in practice.", "labels": [], "entities": [{"text": "medical text simplification", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.7071627279122671}]}, {"text": "One problem is the large variety of medical sub-disciplines and document types that need to be covered; another is the lack of annotated training data, often due to constraints on data sharing for reasons of patient privacy.", "labels": [], "entities": []}, {"text": "In this study we investigate unsupervised statistical NLP techniques to address one key aspect of medical text simplification, viz.", "labels": [], "entities": [{"text": "medical text simplification", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.626584937175115}]}, {"text": "the expansion of medical acronyms and abbreviations (AAs).", "labels": [], "entities": [{"text": "medical acronyms and abbreviations (AAs)", "start_pos": 17, "end_pos": 57, "type": "TASK", "confidence": 0.6878593564033508}]}, {"text": "In addition to text simplification, AA resolution can also help a variety of downstream information extraction tasks.", "labels": [], "entities": [{"text": "AA resolution", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9705203771591187}, {"text": "information extraction", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7106310427188873}]}, {"text": "While AA resolution has been studied extensively in the biomedical domain, studies on clinical text are comparatively rare.", "labels": [], "entities": [{"text": "AA resolution", "start_pos": 6, "end_pos": 19, "type": "TASK", "confidence": 0.9864296913146973}]}, {"text": "Moreover, most previous studies use traditional supervised machine learning techniques, consisting of feature extraction and supervised classifiers such as naive Bayes or Support Vector Machines (SVMs) that utilize a carefully developed AA sense inventory and a large amount of hand-annotated ground-truth data.", "labels": [], "entities": []}, {"text": "In spite of recently developed methods for rapid data acquisition (crowdsourcing), obtaining reliable manual annotations for highly specialized domains is still difficult and acts as a bottleneck in the development of highquality medical text simplification systems.", "labels": [], "entities": [{"text": "rapid data acquisition", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.8017865022023519}, {"text": "highquality medical text simplification", "start_pos": 218, "end_pos": 257, "type": "TASK", "confidence": 0.625891737639904}]}, {"text": "Our proposed approach combines automatic mining of AAs and their possible expansions from medical websites, a first-pass simplification step using statistical machine translation, and a second-pass rescoring step using recently-developed documentlevel neural language models.", "labels": [], "entities": [{"text": "automatic mining of AAs and their possible expansions from medical websites", "start_pos": 31, "end_pos": 106, "type": "TASK", "confidence": 0.7428496236150915}]}, {"text": "To address the data sparsity issue we investigate model training with mismatched training data as well as self-training.", "labels": [], "entities": []}, {"text": "We evaluate our approach on a subset of a publicly available corpus of nursing progress notes from the MIMIC-II database.", "labels": [], "entities": [{"text": "MIMIC-II database", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.8992507457733154}]}, {"text": "Results show an F1 score for AA identification of 0.96, an overall expansion accuracy of 74.3%, and a disambiguation accuracy of 71.6%, all without any supervised annotations used during training.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9835187494754791}, {"text": "AA identification", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8182289898395538}, {"text": "expansion", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9850965142250061}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.6754742860794067}, {"text": "disambiguation accuracy", "start_pos": 102, "end_pos": 125, "type": "METRIC", "confidence": 0.7492073178291321}]}], "datasetContent": [{"text": "The first evaluation criterion for our method is the correct identification of AAs vs. regular words.", "labels": [], "entities": [{"text": "identification of AAs vs. regular words", "start_pos": 61, "end_pos": 100, "type": "TASK", "confidence": 0.7560213605562845}]}, {"text": "Contrary to rule-based or supervised approaches to AA identification () AAs are not identified explicitly but implicitly through the choices made by the SMT system.", "labels": [], "entities": [{"text": "AA identification", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.9421656131744385}, {"text": "SMT", "start_pos": 153, "end_pos": 156, "type": "TASK", "confidence": 0.9748479723930359}]}, {"text": "AA identification can be considered a binary detection problem and can thus be evaluated by precision, recall, and F1 score.", "labels": [], "entities": [{"text": "AA identification", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9473288953304291}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9997735619544983}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9993521571159363}, {"text": "F1 score", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9881574511528015}]}, {"text": "The second evaluation measure is overall accuracy, i.e., the overall percentage of correct AA expansions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9545947313308716}, {"text": "AA", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.7338395118713379}]}, {"text": "Finally, we measure the disambiguation accuracy, i.e., the percentage of correct expansions of ambiguous AAs only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9120855927467346}]}, {"text": "shows precision (P), recall (R), F1-score (F1), overall accuracy (A) and disambiguation accuracy (DA) on the eval set for several baseline systems.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 6, "end_pos": 19, "type": "METRIC", "confidence": 0.942927896976471}, {"text": "recall (R)", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9574060589075089}, {"text": "F1-score (F1)", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.9102702140808105}, {"text": "accuracy (A)", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.933297261595726}, {"text": "disambiguation accuracy (DA)", "start_pos": 73, "end_pos": 101, "type": "METRIC", "confidence": 0.8254424452781677}]}, {"text": "Random is a baseline system where one of the sentence hypotheses produced by the SMT system is selected randomly.", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.983105480670929}]}, {"text": "Precision and recall are high (and generally stable across all different models), since it is only a small number of words not caught by the function word filter that are consistently misinterpreted as AAs.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9922754168510437}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9993621706962585}]}, {"text": "Oracle refers to results obtained by a system that always chooses the hypothesis yielding the highest disambiguation accuracy according to the reference annotation -this represents the upper bound on the accuracy that can be achieved given our automatically collected term mappings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9231374263763428}, {"text": "accuracy", "start_pos": 204, "end_pos": 212, "type": "METRIC", "confidence": 0.9973554611206055}]}, {"text": "The gap between the oracle accuracy and 4 A majority sense baseline system is not available due to the lack of a sense inventory with frequency information for this data set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9625101685523987}]}, {"text": "100% accuracy is due to missing expansions in our term mapping list.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9995637536048889}, {"text": "term mapping list", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.7558261156082153}]}, {"text": "Row 3 in is the result obtained by the first-pass SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9705238342285156}]}, {"text": "The LM for this system was optimized on the development set and consists of a 4-gram back-off model trained using modified Kneser-Ney smoothing on the combined Cases and i2b2 data and the target side of our term mapping list.", "labels": [], "entities": []}, {"text": "Accuracy scores obtained by the SMT model are markedly higher than random scores, though there is still much room for improvement.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9903924465179443}, {"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.991394579410553}]}, {"text": "shows the results obtained by an improved system that utilizes self-training and DCLMs.", "labels": [], "entities": []}, {"text": "For self-training, the amount of automatically expanded MIMIC-II data and the combination with Cases and i2b2 data was optimized on the development set.", "labels": [], "entities": [{"text": "MIMIC-II data", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.7859510183334351}]}, {"text": "Combining the latter two sets with 1,500 expanded documents from MIMIC to train a 4-gram back-off LM was found to be best.", "labels": [], "entities": [{"text": "MIMIC", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.9381490349769592}]}, {"text": "Since new n-best lists are generated using the self-trained models, the Random and Oracle results are different (and improved).", "labels": [], "entities": []}, {"text": "The accuracy of our SMT system's output is also improved by 1.4% absolute.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997542500495911}, {"text": "SMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9891762137413025}]}, {"text": "For rescoring hypotheses with document-level language models we investigated the DCLM architectures described in Section 6, minus the attentionbased model, well as standard RNNLMs and RNNLMs whose context can extend in the past beyond the sentence boundary.", "labels": [], "entities": []}, {"text": "The number of parame-ters for each model (K and H) was optimized on the development set.", "labels": [], "entities": []}, {"text": "Different models trained on different automatically expanded data sources, and MIMIC-II) and their combinations were investigated.", "labels": [], "entities": [{"text": "MIMIC-II", "start_pos": 79, "end_pos": 87, "type": "DATASET", "confidence": 0.5990581512451172}]}, {"text": "It was found that the combined data as well as the Cases and i2b2 data sets in isolation actually resulted in a worse performance of the rescored system compared to the first-pass SMT system.", "labels": [], "entities": [{"text": "Cases and i2b2 data sets", "start_pos": 51, "end_pos": 75, "type": "DATASET", "confidence": 0.7248467445373535}, {"text": "SMT", "start_pos": 180, "end_pos": 183, "type": "TASK", "confidence": 0.9462617039680481}]}, {"text": "While our mismatched data sources did help in training the SMT system, DCLMs, which attempt to model the entire document structure, seem to be very sensitive to mismatched data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9961106181144714}]}, {"text": "By contrast, DCLMs trained on the automatically expanded MIMIC-II data only did achieve an improvement over the firstpass system.", "labels": [], "entities": [{"text": "MIMIC-II data", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.8701685070991516}]}, {"text": "The best model (obtained by development set optimization) was a \"context-to-hidden\" DCLM with a hidden layer size of 48 and a word embedding layer size of 128.", "labels": [], "entities": []}, {"text": "The best final overall accuracy on the evaluation set is 74.3%; the disambiguation accuracy is 71.6%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9623725414276123}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9254550933837891}]}, {"text": "This is fairly close to the topline disambiguation accuracy of 80.2% that can be achieved given our term inventory; however, there is further room for improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.8232142329216003}]}, {"text": "Of the different document context models tested, all performed in a similar range -e.g., the best models with other architectures (\"context-to-output\" and RNNLMs without sentence boundary) achieved between 70.2% and 71.1% disambiguation accuracy on the eval set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9833817481994629}]}, {"text": "Furthermore, an RNNLM model with only the current sentence as context achieves 70.5%.", "labels": [], "entities": []}, {"text": "Thus, while DCLMs seem to provide slight improvements, our text sample is currently too small to assess statistically significant differences between different architectures or context lengths.", "labels": [], "entities": []}, {"text": "Rather, the benefit seems to derive from the neural probability estimation technique used in RNNLM-style models.", "labels": [], "entities": []}, {"text": "shows the automatically expanded version of the sample in.", "labels": [], "entities": []}, {"text": "While most expansions were acceptable, our term mapping list did not contain a domain-appropriate entry for a&a, which was therefore expanded incorrectly to arthroscopy and arthrotomy rather than albuterol and atroven.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Precision (P), recall (R), F1-score (F1), overall accu-", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9391490966081619}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9530344158411026}, {"text": "F1-score (F1)", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.8878120630979538}, {"text": "overall accu-", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.8083223501841227}]}, {"text": " Table 4: Precision (P), recall (R), F1-score (F), overall expan-", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9445459842681885}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9599910974502563}, {"text": "F1-score (F)", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.9648000299930573}, {"text": "overall expan-", "start_pos": 51, "end_pos": 65, "type": "METRIC", "confidence": 0.8234691222508749}]}]}