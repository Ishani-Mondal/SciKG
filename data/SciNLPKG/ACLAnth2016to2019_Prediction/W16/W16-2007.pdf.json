{"title": [{"text": "Improving Sequence to Sequence Learning for Morphological Inflection Generation: The BIU-MIT Systems for the SIGMORPHON 2016 Shared Task for Morphological Reinflection", "labels": [], "entities": [{"text": "Improving Sequence to Sequence Learning", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7672232985496521}, {"text": "Morphological Inflection Generation", "start_pos": 44, "end_pos": 79, "type": "TASK", "confidence": 0.7961004972457886}, {"text": "BIU-MIT", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.8365947008132935}]}], "abstractContent": [{"text": "Morphological reinflection is the task of generating a target form given a source form and the morpho-syntactic attributes of the target (and, optionally, of the source).", "labels": [], "entities": [{"text": "Morphological reinflection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8478779792785645}]}, {"text": "This work presents the submission of Bar Ilan University and the Massachusetts Institute of Technology for the morphological reinflection shared task held at SIGMORPHON 2016.", "labels": [], "entities": [{"text": "morphological reinflection shared task held at SIGMORPHON 2016", "start_pos": 111, "end_pos": 173, "type": "TASK", "confidence": 0.7952907308936119}]}, {"text": "The submission includes two recurrent neural network architectures for learning morphological reinflection from incomplete inflection tables while using several novel ideas for this task: morpho-syntactic attribute embeddings, modeling the concept of templatic morphology, bidirectional input character representations and neural discriminative string transduction.", "labels": [], "entities": []}, {"text": "The reported results for the proposed models over the ten languages in the shared task bring this submission to the second/third place (depending on the language) on all three sub-tasks out of eight participating teams, while training only on the Restricted category data.", "labels": [], "entities": [{"text": "Restricted category data", "start_pos": 247, "end_pos": 271, "type": "DATASET", "confidence": 0.7431192994117737}]}], "introductionContent": [{"text": "Morphological inflection, or reinflection, involves generating a target (surface form) word from a source word (e.g. a lemma), given the morphosyntactic attributes of the target word.", "labels": [], "entities": [{"text": "Morphological inflection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8290871977806091}]}, {"text": "Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers, which are theoretically appealing but require expert knowledge, or machine learning methods for string transduction.", "labels": [], "entities": [{"text": "automatic inflection generation", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.6419457693894705}]}, {"text": "While these studies achieved high accuracies, they also make specific assumptions about the set of possible morphological processes that create the inflection, and require feature engineering over the input.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9693305492401123}]}, {"text": "More recently, used encoder-decoder neural networks for inflection generation inspired by similar approaches for sequence-to-sequence learning for machine translation ().", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.7852819263935089}, {"text": "machine translation", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.7202453017234802}]}, {"text": "The general idea is to use an encoderdecoder network over characters, that encodes the input lemma into a vector and decodes it one character at a time into the inflected surface word.", "labels": [], "entities": []}, {"text": "They factor the data into sets of inflections with identical morpho-syntactic attributes (we refer to each such set as a factor) and try two training approaches: in one they train an individual encoderdecoder RNN per factor, and in the other they train a single encoder RNN overall the lemmas in the dataset and a specific decoder RNN per factor.", "labels": [], "entities": []}, {"text": "An important aspect of previous work on learning inflection generation is the reliance on complete inflection tables -the training data contains all the possible inflections per lemma.", "labels": [], "entities": [{"text": "learning inflection generation", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.7211092114448547}]}, {"text": "In contrast, in the shared task setup ( ) the training is over partial inflection tables that mostly contain only several inflections per lemma, for three different sub-tasks: The first requires morphological inflection generation given a lemma and a set of morpho-syntactic attributes, the second requires morphological re-inflection of an inflected word given the word, its morpho-syntactic attributes and the target inflection's attributes, and the third requires re-inflection of an inflected word given only the target inflection attributes.", "labels": [], "entities": []}, {"text": "The datasets for the different tasks are available on the shared task's website.", "labels": [], "entities": []}, {"text": "The fact that the data is incomplete makes it problematic to use factored models like the ones introduced in, as there maybe insufficient data for training a highquality model per factor of inflections with identical morpho-syntactic attributes.", "labels": [], "entities": []}, {"text": "For example, in the shared task dataset the training data usually contains less than 100 training examples on average per such factor.", "labels": [], "entities": []}, {"text": "Moreover, when the data is factored this way, no information is shared between the different factors even though they may have identical inflection rules.", "labels": [], "entities": []}, {"text": "We propose two neural network architectures for the task.", "labels": [], "entities": []}, {"text": "The first, detailed in Section 2, departs from the architecture of by extending it in three novel ways: representing morpho-syntactic attributes, template-inspired modeling, and bidirectional input character representations.", "labels": [], "entities": []}, {"text": "The second, described in Section 3, is based on an explicit control mechanism we introduce while also making use of the three extensions mentioned above.", "labels": [], "entities": []}, {"text": "Our experimental evaluation overall 10 languages represented in the shared task brings our models to the second or third place, depending on the language.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for inflection generation (first sub-task), measuring accuracy on the development set:  our models vs. the shared task (ST-Base) and Factored (Fact.) baselines, and mean reciprocal rank  (MRR) on the test set: our models vs. the best performing model (", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.7093621492385864}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9993582367897034}, {"text": "mean reciprocal rank  (MRR)", "start_pos": 183, "end_pos": 210, "type": "METRIC", "confidence": 0.8720463116963705}]}, {"text": " Table 2: Results for morphological re-inflection with source attributes (second sub-task) measuring  accuracy over the development set: our models vs. the shared task (ST-Base) baseline, and mean  reciprocal rank (MRR) over the test set: our models vs. the best performing model (Kann and Sch\u00fctze,  2016)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9992689490318298}, {"text": "mean  reciprocal rank (MRR)", "start_pos": 192, "end_pos": 219, "type": "METRIC", "confidence": 0.8080426553885142}]}, {"text": " Table 3: Results for morphological re-inflection without source attributes (third sub-task) measuring  accuracy over the development set: our models vs. the shared task (ST-Base) baseline, and mean  reciprocal rank (MRR) over the test set: our models vs. the best performing model (", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9992414712905884}, {"text": "mean  reciprocal rank (MRR)", "start_pos": 194, "end_pos": 221, "type": "METRIC", "confidence": 0.8119116971890131}]}]}