{"title": [{"text": "Improving Phrase-Based SMT Using Cross-Granularity Embedding Similarity", "labels": [], "entities": [{"text": "Improving Phrase-Based SMT", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8315741817156473}]}], "abstractContent": [{"text": "The phrase-based statistical machine translation (PBSMT) model can be viewed as a log-linear combination of translation and language model features.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (PBSMT)", "start_pos": 4, "end_pos": 56, "type": "TASK", "confidence": 0.697516211441585}]}, {"text": "Such a model typically relies on the phrase table as the main resource for bilingual knowledge, which in its most basic form consists of aligned phrases, along with four probability scores.", "labels": [], "entities": []}, {"text": "These scores only indicate the co-occurrence of phrase pairs in the training corpus, and not necessarily their semantic relatedness.", "labels": [], "entities": []}, {"text": "The basic phrase table is also unable to incorporate contextual information about the segments where a particular phrase tends to occur.", "labels": [], "entities": []}, {"text": "In this paper, we define six new features which express the semantic relatedness of bilingual phrases.", "labels": [], "entities": []}, {"text": "Our method utilizes both source and target side information to enrich the phrase table.", "labels": [], "entities": []}, {"text": "The new features are inferred from a bilingual corpus by a neural network (NN).", "labels": [], "entities": []}, {"text": "We evaluate our model on the English-Farsi (En-Fa) and English-Czech (En-Cz) pairs and observe considerable improvements in the all En\u2194Fa and En\u2194Cz directions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The process of PBSMT can be interpreted as a search problem where the score at each step of exploration is formulated as a log-linear model.", "labels": [], "entities": []}, {"text": "For each candidate phrase, the set of features is combined with a set of learned weights to find the best target counterpart of the provided source sentence.", "labels": [], "entities": []}, {"text": "Because an exhaustive search of the candidate space is not computationally feasible, the space is typically pruned via some heuristic search, such as beam search.", "labels": [], "entities": [{"text": "beam search", "start_pos": 150, "end_pos": 161, "type": "TASK", "confidence": 0.8326379954814911}]}, {"text": "The discriminative log-linear model allows the incorporation of arbitrary context-dependent and context-independent features.", "labels": [], "entities": []}, {"text": "Thus, features such as those in or can be combined to improve translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.9622741937637329}]}, {"text": "The standard baseline bilingual features included in Moses () by default are: the phrase translation probability \u03c6(e|f ), inverse phrase translation probability \u03c6(f |e), direct lexical weighting lex(e|f ) and inverse lexical weighting lex(f |e).", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7083786129951477}, {"text": "phrase translation", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.6289407163858414}]}, {"text": "The scores in the phrase table are computed directly from the co-occurrence of aligned phrases in training corpora.", "labels": [], "entities": []}, {"text": "A large body of recent work evaluates the hypothesis that co-occurrence information alone cannot capture contextual information as well as the semantic relations among phrases (see section 2).", "labels": [], "entities": []}, {"text": "Therefore, many techniques have been proposed to enrich the feature list with semantic information.", "labels": [], "entities": []}, {"text": "In this paper, we define six new features for this purpose.", "labels": [], "entities": []}, {"text": "All of our features indicate the semantic relatedness of source and target phrases.", "labels": [], "entities": []}, {"text": "Our features leverage contextual information which is lost by the traditional phrase extraction operations.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.7522328495979309}]}, {"text": "Specifically, in both sides (source and target) we look for any type of constituents including phrases, sentences or even words which can fortify the semantic information about phrase pairs.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are threefold: a) We define new semantic features and embed into PBSMT to enhance the translation quality.", "labels": [], "entities": []}, {"text": "b) In order to define the new features we train bilingual phrase and sentence embeddings using an NN.", "labels": [], "entities": []}, {"text": "Embeddings are trained in a joint distributed feature space which not only preserves monolingual semantic and syntactic information but also represents cross-lingual relations.", "labels": [], "entities": []}, {"text": "c) We indirectly incorporate external contextual information using the neural features.", "labels": [], "entities": []}, {"text": "We search in the source and target spaces and retrieve the closest constituent to the phrase pair in our bilingual embedding space.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of related work.", "labels": [], "entities": []}, {"text": "Section 3 explains our pipeline and the network architecture in detail.", "labels": [], "entities": []}, {"text": "In Section 4, experimental results are reported.", "labels": [], "entities": []}, {"text": "We also have a separate section to discuss different aspects of embeddings and the model.", "labels": [], "entities": []}, {"text": "Finally, in the last section we present our conclusions along with some avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our new features on two language pairs: En-Fa and En-Cz.", "labels": [], "entities": []}, {"text": "Both Farsi and Czech are morphologically rich languages; therefore, translation to/from these languages can be more difficult than it is for languages where words tend to be discrete semantic units.", "labels": [], "entities": []}, {"text": "Farsi is also a low-resource language, so we are interested in working with these pairs.", "labels": [], "entities": []}, {"text": "For the En-Fa pair we used the TEP++ corpus (Passban et al., 2015a) and for Czech we used the Europarl 4 corpus (.", "labels": [], "entities": [{"text": "TEP++ corpus", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.7369458079338074}, {"text": "Europarl 4 corpus", "start_pos": 94, "end_pos": 111, "type": "DATASET", "confidence": 0.9514887531598409}]}, {"text": "TEP++ is a collection of 600,000 parallel sentences.", "labels": [], "entities": []}, {"text": "We used 1000 and 2000 sentences for testing and tuning, respectively and the rest of the corpus for training.", "labels": [], "entities": []}, {"text": "From the Czech dataset we selected the same number of sentences for training, testing and tuning.", "labels": [], "entities": [{"text": "Czech dataset", "start_pos": 9, "end_pos": 22, "type": "DATASET", "confidence": 0.9366963803768158}]}, {"text": "The baseline system is a PBSMT engine built using Moses () with the default configuration.", "labels": [], "entities": []}, {"text": "We used MERT (Och, 2003) for tuning.", "labels": [], "entities": [{"text": "MERT", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9223943948745728}, {"text": "Och, 2003)", "start_pos": 14, "end_pos": 24, "type": "DATASET", "confidence": 0.7700234055519104}]}, {"text": "In the experiments we trained 5-gram language models on the monolingual parts of the bilingual corpora using SRILM ().", "labels": [], "entities": [{"text": "SRILM", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.4838184714317322}]}, {"text": "We used BLEU () as the evaluation metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9988958835601807}]}, {"text": "We added our features to the phrase table and tuned the translation models.", "labels": [], "entities": []}, {"text": "shows the impact of each feature.", "labels": [], "entities": []}, {"text": "We also estimated the translation quality in the presence of the all features (we run MERT for each row of).", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9387429356575012}, {"text": "MERT", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.8842447400093079}]}, {"text": "Bold numbers are statistically significant according to the results of paired bootstrap re-sampling with p=0.05 for 1000 samples.", "labels": [], "entities": []}, {"text": "Arrows indicate whether the new features increased or decreased the quality over the baseline.", "labels": [], "entities": [{"text": "quality", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.9785276055335999}]}, {"text": "Results show that the new features are useful and positively affect the translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.9683711528778076}]}, {"text": "Some of the features such as sp2tp are always helpful regardless of the translation direction and language pair.", "labels": [], "entities": []}, {"text": "This feature is the most important feature among others.", "labels": [], "entities": []}, {"text": "The sm2tm feature always works effectively in translating into English and the tp2sm feature is effective when translating from English.", "labels": [], "entities": []}, {"text": "In the presence of all features results are significantly better than the baseline system in all cases.", "labels": [], "entities": []}, {"text": "Some of the features are not as strong as the others (tp2tm) and some of them behave differently based on the language (sp2tm).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Impact of the proposed features.", "labels": [], "entities": [{"text": "Impact", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8702343106269836}]}, {"text": " Table 3. The top 10 most similar vectors for the given English query. Recall that the retrieved  vectors could belong to words, phrases or sentences in either English or Farsi and word or phrase  pairs. The items that were originally in Farsi have been translated into English, and are indicated  with italics.", "labels": [], "entities": []}]}