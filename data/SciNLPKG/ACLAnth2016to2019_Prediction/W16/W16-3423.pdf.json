{"title": [{"text": "Re-assessing the Impact of SMT Techniques with Human Evaluation: a Case Study on English\u2194Croatian", "labels": [], "entities": [{"text": "SMT Techniques", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.9182747006416321}]}], "abstractContent": [{"text": "We reassess the impact brought by a set of widely-used SMT models and techniques by means of human evaluation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.992120623588562}]}, {"text": "These include different types of development sets (crowdsourced vs translated professionally), reordering, operation sequence and bilingual neural language models as well as common approaches to data selection and combination.", "labels": [], "entities": []}, {"text": "In some cases our results corroborate previous findings found in the literature, when those approaches were evaluated in terms of automatic metrics, but in some other cases they do not.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the field of statistical machine translation (SMT), when new models and techniques are introduced, it is rather common to assess their performance in terms of automatic metrics solely for just one or a few language pairs.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.8173895329236984}]}, {"text": "Upon showing significant improvement and being implemented as free/open-source software, some of these techniques become then widely used in the community.", "labels": [], "entities": []}, {"text": "In this paper we select a relevant set of such techniques and evaluate the impact they bring by means of a human evaluation.", "labels": [], "entities": []}, {"text": "This paper is part of a wider activity whose goal is to rapidly provide machine translation (MT) for under-resourced languages along with a better insight on how do they work and perform in away that is meaningful not just for researchers but also for industrial adopters of MT.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.8054704546928406}, {"text": "MT", "start_pos": 275, "end_pos": 277, "type": "TASK", "confidence": 0.9794517755508423}]}, {"text": "Our case study is on Croatian given its strategic importance to the EU as the official language of a recent member state.", "labels": [], "entities": []}, {"text": "In this work we aim to assess the impact of the components of several SMT systems (English-Croatian in both directions) built for this purpose.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9905783534049988}]}, {"text": "To meet this aim, we evaluate, both automatically and manually, each component one at a time.", "labels": [], "entities": []}, {"text": "We assess the impact of using different development sets, produced by professional and amateur translators.", "labels": [], "entities": []}, {"text": "2. Compare the use of three reordering models (word-, phrase-based and hierarchical).", "labels": [], "entities": []}, {"text": "3. Measure the impact of using additional models recently introduced in the SMT pipeline.", "labels": [], "entities": [{"text": "SMT pipeline", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.9054973423480988}]}, {"text": "Specifically, the operation sequence model (OSM) and bilingual neural language models (BiNLM).", "labels": [], "entities": []}, {"text": "4. Assess the impact of different ways to select and combine data sets.", "labels": [], "entities": []}, {"text": "5. Compare our best systems to widely-used commercial systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each experiment is evaluated both automatically and manually (except the one in Section 3.4).", "labels": [], "entities": []}, {"text": "We used the widely used automatic metrics BLEU () and TER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.932537853717804}, {"text": "TER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9986275434494019}]}, {"text": "Statistical significance is calculated on BLEU scores with paired bootstrap resampling (1,000 iterations and p = 0.95).", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.49595242738723755}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9968256950378418}]}, {"text": "The human evaluation consists of ranking MT outputs with Appraise.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.8878002762794495}, {"text": "Appraise", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9987504482269287}]}, {"text": "For each experiment 100 randomly selected segments were ranked.", "labels": [], "entities": []}, {"text": "All the annotations were carried out by 2 native Croatian speakers with an advanced level of English.", "labels": [], "entities": []}, {"text": "The following guidelines were provided to the annotators: Given translations by more than two MT systems, the task is to rank them: -Rank system A higher (rank1) than B (rank2), if the output of the first is better than the output of the second.", "labels": [], "entities": []}, {"text": "-Rank both systems equally, A rank1 and B rank1, if the outputs are of the same quality -Use the highest rank possible, e.g. if you've three systems A, B and C, and the quality of A and B is equivalent and both are better than C, then do: A=rank1, B=rank1, C=rank2.", "labels": [], "entities": []}, {"text": "Do NOT use lower rankings, e.g.: A=rank2, B=rank2, C=rank4.", "labels": [], "entities": []}, {"text": "We then derive a human score for each system with the TrueSkill method adapted to MT evaluation () following its usage at WMT15.", "labels": [], "entities": [{"text": "TrueSkill", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.7175735235214233}, {"text": "MT evaluation", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.9468047320842743}, {"text": "WMT15", "start_pos": 122, "end_pos": 127, "type": "DATASET", "confidence": 0.9690885543823242}]}, {"text": "14 Namely, we run 1,000 iterations of rankings followed by clustering (p = 0.95).", "labels": [], "entities": []}, {"text": "If two systems are placed in different clusters (column \"range\" in results' tables) then the one with lower range is considered significantly better.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Results (different development sets) for English\u2192Croatian.", "labels": [], "entities": []}, {"text": " Table 2. Results using different reordering models. Best results shown in bold.", "labels": [], "entities": []}, {"text": " Table 4. Results applying data selection and combination. Best results shown in bold.", "labels": [], "entities": []}, {"text": " Table 5. Results comparing to commercial systems. Best results shown in bold.  \u2020 indicates that a  system is significantly better than the other ones ( p = 0.05).", "labels": [], "entities": []}]}