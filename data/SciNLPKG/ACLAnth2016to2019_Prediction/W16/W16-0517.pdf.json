{"title": [{"text": "Detecting Context Dependence in Exercise Item Candidates Selected from Corpora", "labels": [], "entities": [{"text": "Detecting Context Dependence", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9349715312321981}, {"text": "Corpora", "start_pos": 71, "end_pos": 78, "type": "TASK", "confidence": 0.37701764702796936}]}], "abstractContent": [{"text": "We explore the factors influencing the dependence of single sentences on their larger tex-tual context in order to automatically identify candidate sentences for language learning exercises from corpora which are presentable in isolation.", "labels": [], "entities": []}, {"text": "An in-depth investigation of this question has not been previously carried out.", "labels": [], "entities": []}, {"text": "Understanding this aspect can contribute to a more efficient selection of candidate sentences which, besides reducing the time required for item writing, can also ensure a higher degree of variability and authenticity.", "labels": [], "entities": [{"text": "item writing", "start_pos": 140, "end_pos": 152, "type": "TASK", "confidence": 0.744712233543396}, {"text": "authenticity", "start_pos": 205, "end_pos": 217, "type": "METRIC", "confidence": 0.9602584838867188}]}, {"text": "We present a set of relevant aspects collected based on the qualitative analysis of a smaller set of context-dependent corpus example sentences.", "labels": [], "entities": []}, {"text": "Furthermore , we implemented a rule-based algorithm using these criteria which achieved an average precision of 0.76 for the identification of different issues related to context dependence.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9944968819618225}]}, {"text": "The method has also been evaluated empirically where 80% of the sentences in which our system did not detect context-dependent elements were also considered context-independent by human raters.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extracting single sentences from corpora with the use of Natural Language Processing (NLP) tools can be useful fora number of purposes including the detection of candidate sentences for automatic exercise generation.", "labels": [], "entities": [{"text": "Extracting single sentences from corpora", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8837933301925659}, {"text": "automatic exercise generation", "start_pos": 186, "end_pos": 215, "type": "TASK", "confidence": 0.6053758561611176}]}, {"text": "Such sentences are also known as seed sentences () or carrier sentences () in the Intelligent ComputerAssisted Language Learning (ICALL) literature.", "labels": [], "entities": []}, {"text": "Interest for the use of corpora in language learning has arisen already in the 1980s, since the increasing amount of digital text available enables learning through authentic language use (O'.", "labels": [], "entities": []}, {"text": "However, since sentences in a text form a coherent discourse, it might be the case that for the interpretation of the meaning of certain expressions in a sentence, previously mentioned information, i.e. a context, is required ().", "labels": [], "entities": [{"text": "interpretation of the meaning of certain expressions in a sentence", "start_pos": 96, "end_pos": 162, "type": "TASK", "confidence": 0.8318129301071167}]}, {"text": "Corpus sentences whose meaning is hard to interpret are less optimal to be used as exercise items (, however, having access to a larger linguistic context is not possible due to copy-right issues sometimes (.", "labels": [], "entities": []}, {"text": "In the followings, we explore how we can automatically assess whether a sentence previously belonging to a text can also be used as a stand-alone sentence based on the linguistic information it contains.", "labels": [], "entities": []}, {"text": "We consider a sentence context-dependent if it is not meaningful in isolation due to: (i) the presence of expressions referring to textual content that is external to the sentence, or (ii) the absence of one or more elements which could only be inferred from the surrounding sentences.", "labels": [], "entities": []}, {"text": "Understanding the main factors giving rise to context dependence can improve the trade-off between discarding (or penalizing) sub-optimal candidates and maximizing the variety of examples and thus, their authenticity.", "labels": [], "entities": []}, {"text": "Such a system may not only facilitate teaching professionals' work, but it can also aid the NLP community in a number of ways, e.g. evaluating automatic single-sentence summaries, detecting ill-formed sentences in machine translation output or identifying dictionary examples.", "labels": [], "entities": [{"text": "evaluating automatic single-sentence summaries", "start_pos": 132, "end_pos": 178, "type": "TASK", "confidence": 0.576935201883316}, {"text": "detecting ill-formed sentences in machine translation output", "start_pos": 180, "end_pos": 240, "type": "TASK", "confidence": 0.7704790915761676}]}, {"text": "Although context dependence has been taken into consideration to some extent in previous work, we offer an in-depth investigation of this research problem.", "labels": [], "entities": [{"text": "context dependence", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7020224928855896}]}, {"text": "The theoretical contribution of our work is a set of criteria relevant for assessing context dependence of single sentences based on a qualitative analysis of human evaluators' comments.", "labels": [], "entities": []}, {"text": "This is complemented with a practical contribution in the form of a rule-based system implemented using the proposed criteria which can reliably categorize corpus examples based on context dependence both when evaluated using relevant datasets and according to human raters' judgments.", "labels": [], "entities": []}, {"text": "The current implementation of the system has been tested on Swedish data, but the criteria can be easily applied to other languages as well.", "labels": [], "entities": [{"text": "Swedish data", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.8078635632991791}]}], "datasetContent": [{"text": "Instead of creating a corpus specifically tailored for this task with gold standard labels assigned by human annotators, which can be a rather time-and resource-intensive endeavor, we explored how different types of existing data sources which contained inherently context-(in)dependent sentences could be used for our purposes.", "labels": [], "entities": []}, {"text": "Language learning coursebooks contain not only texts, but also single sentences in the form of exercise items, lists and language examples illustrating a lexical or a grammatical pattern.", "labels": [], "entities": [{"text": "Language learning coursebooks contain not only texts, but also single sentences in the form of exercise items, lists and language examples illustrating a lexical or a grammatical pattern", "start_pos": 0, "end_pos": 186, "type": "Description", "confidence": 0.7711798141400019}]}, {"text": "We collected sentences belonging to these two latter categories from COCTAILL ( ), a corpus of coursebooks for learners of Swedish as a second language.", "labels": [], "entities": []}, {"text": "Most exercises contained gaps which might have misled the automatic linguistic annotation, therefore they have not been included in our dataset.", "labels": [], "entities": []}, {"text": "Dictionaries contain example sentences illustrating the meaning and the usage of an entry.", "labels": [], "entities": []}, {"text": "One of the characteristics of such sentences is the absence of referring expressions which would require a larger context to be understood (), therefore they can be considered suitable representatives of context-independent sentences.", "labels": [], "entities": []}, {"text": "We collected instances of good dictionary example sentences from two Swedish lexical resources: SALDO) and the Swedish FrameNet (SweFN)).", "labels": [], "entities": [{"text": "Swedish FrameNet (SweFN))", "start_pos": 111, "end_pos": 136, "type": "DATASET", "confidence": 0.838770043849945}]}, {"text": "These sentences were manually selected by lexicographers from a variety of corpora.", "labels": [], "entities": []}, {"text": "Sentences explicitly considered dependent on a larger context are less available due to their lack of usefulness inmost application scenarios.", "labels": [], "entities": []}, {"text": "Two previous evaluations of corpus example selection for Swedish are described in and, we will refer to these as EVAL1 and EVAL2 respectively.", "labels": [], "entities": [{"text": "EVAL1", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.5665868520736694}]}, {"text": "In the former case, evaluators including both lexicographers and language teachers had to provide a score for the appropriateness of about 1800 corpus examples on a threepoint scale.", "labels": [], "entities": []}, {"text": "In EVAL2, about 200 corpus examples selected with two different approaches were rated by a similar group of experts based on their understandability (readability) for language learners, as well as their appropriateness as exercise items and as good dictionary examples.", "labels": [], "entities": [{"text": "EVAL2", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9009471535682678}]}, {"text": "The data from both evaluations contained human raters' comments explicitly mentioning that certain sentences were contextdependent.", "labels": [], "entities": []}, {"text": "We gathered these instances to create a negative sample.", "labels": [], "entities": []}, {"text": "Since comments were optional, and context dependence was not the focus of these evaluations, the amount of sentences collected remained rather small, 92 in total.", "labels": [], "entities": []}, {"text": "It is worth noting that this data contains spontaneously occurring mentions based on raters' intuition, rather than being labeled following a description of the phenomenon of context dependence as it would be customary in an annotation task.", "labels": [], "entities": []}, {"text": "The sentences from all data sources mentioned above constituted our development set.", "labels": [], "entities": []}, {"text": "The amount of sentences per data source is presented in  We evaluated our system both on the hand-coded negative example sentences collected from EVAL1 and EVAL2 (CDEP) and the positive samples comprised of the good dictionary examples (CINDEP-D) and the coursebook sentences (CINDEP-LL).", "labels": [], "entities": [{"text": "EVAL1", "start_pos": 146, "end_pos": 151, "type": "DATASET", "confidence": 0.919974684715271}, {"text": "EVAL2 (CDEP", "start_pos": 156, "end_pos": 167, "type": "DATASET", "confidence": 0.7895454168319702}]}, {"text": "The performance when predicting different aspects of context dependence is presented in.", "labels": [], "entities": []}, {"text": "We focused on maximizing precision, i.e. on correctly identifying as many themes as possible in the hand-coded CDEP sentences, recall values were of lower importance since we aimed at avoiding every context-dependent sentence rather than retrieving them all.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9916332960128784}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9980562925338745}]}, {"text": "Most themes were correctly identified, all themes except one was predicted with a precision of at least 0.7 and above.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9967319965362549}]}, {"text": "The only theme that yielded a lower result was that of implicit anaphoras.", "labels": [], "entities": []}, {"text": "The error analysis revealed that these cases were mostly connected to an incorrect dependency parse of the sentences, mainly subjects tagged as objects in sentences with an inverted (predicate-subject) word order.", "labels": [], "entities": []}, {"text": "As mentioned previously, we strived for minimizing sub-optimal sentences in terms of context dependence, while trying to avoid being excessively selective to maintain a varied set of examples.", "labels": [], "entities": []}, {"text": "To assess performance with respect to this latter aspect, we inspected also the percentage of sentences identified as context-dependent in dictionary examples (CIND-D) and coursebook sentences (CIND-LL).", "labels": [], "entities": []}, {"text": "The percentage of predicted themes per dataset is shown in where Total stands for the percentage of sentences with at least one predicted theme.", "labels": [], "entities": [{"text": "Total", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9805843830108643}]}, {"text": "We can observe that even though all sentences are expected to be context-independent, our system labeled as context-dependent about three out often good dictionary examples and coursebook sentences.", "labels": [], "entities": []}, {"text": "The error analysis revealed that some of these instances did indeed contain contextdependent elements, e.g. the conjunction men 'but' in sentence-initial position.", "labels": [], "entities": []}, {"text": "In CIND-LL in the case of some sentences containing anaphoric pronouns an image provided the missing context in the coursebook, thus not all predicted cases were actual false positives, but rather, they indicated some noise in the data.", "labels": [], "entities": [{"text": "CIND-LL", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.8958543539047241}]}, {"text": "As for dictionary examples, the presence of such sentences may also suggest that the criterion of context dependence can vary somewhat depending on the type of lexicon or lexicographers' individual decisions.", "labels": [], "entities": []}, {"text": "Some sentences exhibited more than one phenomenon connected to context dependence.", "labels": [], "entities": []}, {"text": "Multiple themes were predicted in 30.43% of the CDEP sentences, but only 6.54% and 7.25 of the CIND-D and CIND-LL sentences respectively.", "labels": [], "entities": [{"text": "CDEP sentences", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.9531818330287933}, {"text": "CIND-D", "start_pos": 95, "end_pos": 101, "type": "DATASET", "confidence": 0.8639565110206604}]}, {"text": "The algorithm was tested also empirically during an evaluation of automatic candidate sentence selection for the purposes of learning Swedish as a second language.", "labels": [], "entities": []}, {"text": "The evaluation data consisted of 338 3 sentences retrieved from a variety of modern Swedish corpora and classified as not containing context dependence themes according to our algorithm (with the exception of 4 control sentences that were context-dependent).", "labels": [], "entities": []}, {"text": "These were all unseen sentences not present in the datasets described in section 3.", "labels": [], "entities": []}, {"text": "In the evaluation setup, all implemented themes were used as filters, i.e. sentences containing any recognized element connected to context dependence, described in section 6, were discarded.", "labels": [], "entities": []}, {"text": "Besides context dependence, the evaluated system incorporated also other selection criteria (e.g. readability), but for reasons of relevance and space these aspects and the associated results are not discussed here.", "labels": [], "entities": []}, {"text": "The selected sentences were given for evaluation to 5 language teachers who assessed the suitability of these sentences based on 3 criteria: (i) their degree of being independent of context, (ii) their CEFR 4 level and (iii) their overall suitability for language learners.", "labels": [], "entities": [{"text": "CEFR 4 level", "start_pos": 202, "end_pos": 214, "type": "METRIC", "confidence": 0.6892251571019491}]}, {"text": "Teachers were required to assess this latter aspect without a specific exercise type in mind, but considering a learner reading the sentence instead.", "labels": [], "entities": []}, {"text": "Sentences were divided into two subsets, each being rated by at least 2 evaluators.", "labels": [], "entities": []}, {"text": "Teachers had to assign a score between 1 to 4 to each sentence according to the scale definition in.", "labels": [], "entities": []}, {"text": "The results were promising, the average score overall evaluators and sentences for context independence was 3.05, and for overall suitability 3.23.", "labels": [], "entities": [{"text": "context independence", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7213700860738754}]}, {"text": "For context-independence, 61% of the sentences received score 3 or 4 (completely satisfying the criterion) from at least half of the evaluators, and 80% of the sentences received an average score higher than 2.5.", "labels": [], "entities": []}, {"text": "This latter improves significantly on the percentage of context-dependent sentences that we reported previously in, where about 36% of all selected sentences were explicitly considered context-dependent by evaluators.", "labels": [], "entities": []}, {"text": "Furthermore, we computed the Spearman correlation coefficient for teachers' scores of overall suitability and context dependence to gain insight into how strongly associated these two aspects were according to our evaluation data.", "labels": [], "entities": [{"text": "Spearman correlation coefficient", "start_pos": 29, "end_pos": 61, "type": "METRIC", "confidence": 0.7688009738922119}]}, {"text": "The correlation overall sentences was \u03c1=0.53, which indicates that not being context-dependent is positively associated with overall suitability.", "labels": [], "entities": []}, {"text": "Therefore, context dependence is worth targeting when selecting carrier sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of sentences per source.", "labels": [], "entities": []}, {"text": " Table 2: Thematic analysis results.", "labels": [], "entities": []}, {"text": " Table 3: Theme prediction performance in CDEP sentences.", "labels": [], "entities": [{"text": "Theme prediction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8254976570606232}]}, {"text": " Table 4: Percentage of sentences with a predicted theme in the", "labels": [], "entities": []}]}