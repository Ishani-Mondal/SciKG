{"title": [{"text": "Enlarging scarce in-domain English-Croatian corpus for SMT of MOOCs using Serbian", "labels": [], "entities": [{"text": "SMT of MOOCs", "start_pos": 55, "end_pos": 67, "type": "TASK", "confidence": 0.775176465511322}]}], "abstractContent": [{"text": "Massive Open Online Courses have been growing rapidly in size and impact.", "labels": [], "entities": []}, {"text": "Yet the language barrier constitutes a major growth impediment in reaching out all people and educating all citizens.", "labels": [], "entities": []}, {"text": "A vast majority of educational material is available only in English, and state-of-the-art machine translation systems still have not been tailored for this peculiar genre.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7504758238792419}]}, {"text": "In addition, a mere collection of appropriate in-domain training material is a challenging task.", "labels": [], "entities": []}, {"text": "In this work, we investigate statistical machine translation of lecture subtitles from English into Croatian, which is morphologically rich and generally weakly supported, especially for the educational domain.", "labels": [], "entities": [{"text": "statistical machine translation of lecture subtitles from English into Croatian", "start_pos": 29, "end_pos": 108, "type": "TASK", "confidence": 0.8101276516914367}]}, {"text": "We show that results comparable with publicly available systems trained on much larger data can be achieved if a small in-domain training set is used in combination with additional in-domain corpus originating from the closely related Serbian language.", "labels": [], "entities": []}], "introductionContent": [{"text": "Massive Open Online Courses (MOOCs) have been growing rapidly in size and importance, but the language barrier constitutes a major obstacle in reaching out all people and educating all citizens.", "labels": [], "entities": []}, {"text": "A vast majority of materials is available only in English, and state-of-the-art machine translation (MT) systems still have not been tailored for this type of texts: the specific type of spoken language used in lectures, ungrammatical and/or incomplete segments in subtitles, slides and assignments, a number of distinct courses i.e. domains such as various natural sciences, computer science, engineering, philosophy, history, music, etc.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.8652488470077515}]}, {"text": "Machine translation of this genre into an under-resourced morphologically rich target language represents an additional challenge -in this work, we investigate translation into Croatian.", "labels": [], "entities": []}, {"text": "Croatian has recently become the third official South Slavic language in the EU, 1 but it is still rather under-resourced in terms of free/open-source language resources and tools, especially in terms of parallel bilingual corpora.", "labels": [], "entities": []}, {"text": "Finding appropriate parallel educational data is even more difficult.", "labels": [], "entities": []}, {"text": "Therefore, we based our experiments on a small in-domain parallel corpus containing about 12k parallel segments.", "labels": [], "entities": []}, {"text": "We then investigate in what way the translation quality can be improved by an additional in-domain corpus of about 50k segments containing a closely related language, namely Serbian.", "labels": [], "entities": []}, {"text": "In addition, we explore the impact of adding a relatively large (200k) out-of-domain news corpus.", "labels": [], "entities": []}, {"text": "Croatian and Serbian are rather close languages, so one option could be to directly use additional English-Serbian data.", "labels": [], "entities": []}, {"text": "However, previous work has shown a significant drop in translation quality fora similar cross-language translation scenario).", "labels": [], "entities": [{"text": "cross-language translation", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.6808473020792007}]}, {"text": "Therefore we also investigate a high-quality Serbian-to-Croatian rule-based MT system for creating additional artificial English-Croatian data.", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.8920385837554932}]}], "datasetContent": [{"text": "For all set-ups, BLEU scores () and character n-gram F-scores i.e. CHRF3 scores (Popovi\u00b4cPopovi\u00b4c, 2015) are reported.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9988874793052673}, {"text": "CHRF3", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.7497563362121582}]}, {"text": "In addition, five Hjerson error classes (Popovi\u00b4cPopovi\u00b4c, 2011) are reported in order to get a better insight into differences between the systems: inflectional errors, ordering errors, missing words, additions and lexical errors.", "labels": [], "entities": []}, {"text": "presents the obtained automatic scores for all Moses training set-ups described in Section 5 together with the scores for translations generated 7 by two publicly available SMT systems for Englishto-Croatian: Asistent 8 (Ar\u010dan et al., 2016) and Google translate 9 . It can be seen that the most promising set-up according to automatic evaluation metrics is the set-up 5, i.e merging both domains and adding artificial in-domain English-Croatian parallel text where the target Croatian part is generated from Serbian by the rule-based MT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.9267788529396057}]}, {"text": "This set-up even outperforms the Asistent system which is trained on much larger parallel texts, albeit none of them from educational domain.", "labels": [], "entities": []}, {"text": "Furthermore, it can be seen that both SETimes and original Coursera set produce the same percentage of lexical errors -the first one due to the domain discrepance and the other due to data sparsity.", "labels": [], "entities": [{"text": "SETimes", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.6874455213546753}, {"text": "Coursera set", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.9565945565700531}]}, {"text": "Adding in-domain Serbian data reduces the number of lexical errors, which is further reduced by translating Serbian into Croatian.", "labels": [], "entities": []}, {"text": "Merging of two data-sets reduces lexical errors even more, however their number is still larger than for Asistent and Google systems.: Automatic evaluation scores (%) for each of the systems: BLEU score, CHRF3 score and five Hjerson error rates: inflectional, ordering, omission, addition and lexical error rate together with their sum.", "labels": [], "entities": [{"text": "Automatic", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9729728698730469}, {"text": "BLEU score", "start_pos": 192, "end_pos": 202, "type": "METRIC", "confidence": 0.9814577996730804}, {"text": "CHRF3 score", "start_pos": 204, "end_pos": 215, "type": "METRIC", "confidence": 0.9675391316413879}]}, {"text": "Ordering errors and omissions are lower for the set-ups without SEtimes, most probably due to different sentence (i.e. segment) structure in two genres/domains.", "labels": [], "entities": []}, {"text": "Morphological errors are also lower without SEtimes, however they are high in all set-ups which should be generally addressed in future work by using morpho-syntactic analysers and/or generators.", "labels": [], "entities": [{"text": "Morphological errors", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.9412101805210114}, {"text": "SEtimes", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.5944859981536865}]}, {"text": "Apart from this, it can be observed that the main advantage of the Google system is the low number of lexical errors which is probably achieved by using very large training corpora.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Automatic evaluation scores (%) for each of the systems: BLEU score, CHRF3 score and five  Hjerson error rates: inflectional, ordering, omission, addition and lexical error rate together with their  sum.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9834702610969543}, {"text": "CHRF3 score", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9745893776416779}, {"text": "Hjerson error rates", "start_pos": 101, "end_pos": 120, "type": "METRIC", "confidence": 0.6185308496157328}]}]}