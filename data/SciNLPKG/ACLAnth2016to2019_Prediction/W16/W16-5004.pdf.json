{"title": [], "abstractContent": [{"text": "The utilization of social media material in journalistic workflows is increasing, demanding automated methods for the identification of mis-and disinformation.", "labels": [], "entities": [{"text": "identification of mis-and disinformation", "start_pos": 118, "end_pos": 158, "type": "TASK", "confidence": 0.8923002183437347}]}, {"text": "Since textual contradiction across social media posts can be a signal of rumorousness, we seek to model how claims in Twitter posts are being textually contradicted.", "labels": [], "entities": []}, {"text": "We identify two different contexts in which contradiction emerges: its broader form can be observed across independently posted tweets and its more specific form in threaded conversations.", "labels": [], "entities": []}, {"text": "We define how the two scenarios differ in terms of central elements of argumentation: claims and conversation structure.", "labels": [], "entities": []}, {"text": "We design and evaluate models for the two scenarios uniformly as 3-way Recognizing Textual Entailment tasks in order to represent claims and conversation structure implicitly in a generic inference model, while previous studies used explicit or no representation of these properties.", "labels": [], "entities": []}, {"text": "To address noisy text, our classifiers use simple similarity features derived from the string and part-of-speech level.", "labels": [], "entities": []}, {"text": "Corpus statistics reveal distribution differences for these features in contradictory as opposed to non-contradictory tweet relations, and the classifiers yield state of the art performance.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In order to predict the RTE classes based on the features introduced above, we trained two classifiers: Nearest (shrunken) centroids (NC) ( and Random forest (RF)), using the R wrapper package Caret (Kuhn, 2016) with the methods pam and rf, respectively.", "labels": [], "entities": []}, {"text": "To derive the same number of instances for all classes, we applied separately for both datasets resampling without replacement, so that the total data amounts about 4,550 feature vectors equally distributed over the three classes, the majority of 4,130 belonging to the iPosts data set.", "labels": [], "entities": [{"text": "iPosts data set", "start_pos": 270, "end_pos": 285, "type": "DATASET", "confidence": 0.9459074139595032}]}, {"text": "Further, we centered and scaled the feature matrix.", "labels": [], "entities": []}, {"text": "Within the Caret framework we optimized the tunable parameters of both classifiers by maximizing the F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9866523146629333}]}, {"text": "This way the NC shrinkage delta was set to 0, which means that the class reference centroids are not modified.", "labels": [], "entities": [{"text": "NC shrinkage delta", "start_pos": 13, "end_pos": 31, "type": "METRIC", "confidence": 0.6127746899922689}]}, {"text": "For RF the number of variables randomly sampled as candidates at each split was set to 2.", "labels": [], "entities": [{"text": "RF", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.897514820098877}]}, {"text": "The remaining parameters were kept default.", "labels": [], "entities": []}, {"text": "The classifiers were tested on both datasets in a 4-fold event-based held-out setting, training on three events and testing on the remaining one (4-fold cross-validation, CV), quantifying how performance generalizes to new events with unseen claims and unseen targets.", "labels": [], "entities": []}, {"text": "The CV scores are summarized in.", "labels": [], "entities": []}, {"text": "It turns out generally that classifying CON is more difficult than classifying ENT or UNK.", "labels": [], "entities": [{"text": "classifying CON", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7883689999580383}, {"text": "UNK", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.7606362104415894}]}, {"text": "We observe a dependency of the classifier performances on the two contradiction scenarios: for detecting CON, RF achieved higher classification values on Threads, whereas NC performed better on iPosts.", "labels": [], "entities": [{"text": "detecting CON", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.7165765166282654}, {"text": "RF", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9423320889472961}]}, {"text": "General performance across all three classes was better in independent posts than in conversational threads.", "labels": [], "entities": []}, {"text": "Definitions of contradiction, the genre of texts and the features used are dependent on end applications, making performance comparison nontrivial ().", "labels": [], "entities": []}, {"text": "On a different subset of the Threads data in terms of events, size of evidence, 4 stance classes and no resampling, ( report .40 overall F-score using Gaussian processes, cosine similarity on text vector representation and temporal metadata.", "labels": [], "entities": [{"text": "Threads data", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.7839724123477936}, {"text": "F-score", "start_pos": 137, "end_pos": 144, "type": "METRIC", "confidence": 0.9971895813941956}]}, {"text": "Our previous experiments were done using the Excitement Open Platform incorporating syntactico-semantic processing and 4-fold CV.", "labels": [], "entities": []}, {"text": "For the non-resampled Threads data we reported .11 F1 on CON via training on iPosts ().", "labels": [], "entities": [{"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9810708165168762}, {"text": "CON", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8465232849121094}]}, {"text": "On the non-resampled iPosts data we obtained .51 overall F1 score (), F1 on CON being .25 ().", "labels": [], "entities": [{"text": "iPosts data", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.9168903231620789}, {"text": "F1 score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9852328896522522}, {"text": "F1 on CON", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.7461588780085245}]}, {"text": "We proposed to model two types of contradictions: in the first both tweets encode the claim target (iPosts), in the second typically only one of them (Threads).", "labels": [], "entities": []}, {"text": "The Nearest Centroid algorithm performs poorly on the CON class in Threads where textual overlap is typically small especially for the CON and UNK classes, in part due to the absence of the claim target in replies.", "labels": [], "entities": []}, {"text": "However, the Random Forest algorithm's performance is not affected by this factor.", "labels": [], "entities": []}, {"text": "The advantage of RF on the Threads data can be explained by its property of training several weak classifiers on parts of the feature vectors only.", "labels": [], "entities": [{"text": "RF", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9753471612930298}, {"text": "Threads data", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.7630960941314697}]}, {"text": "By this boosting strategy a usually undesirable combination of relatively long feature vectors but few training observations can be tackled, holding for the Threads data that due to its extreme skewedness (cf.) shrunk down to only 420 datapoints after our class balancing technique of resampling without replacement.", "labels": [], "entities": [{"text": "Threads data", "start_pos": 157, "end_pos": 169, "type": "DATASET", "confidence": 0.7221800833940506}]}, {"text": "Results indicate the benefit of RF classifiers in such sparse data cases.", "labels": [], "entities": [{"text": "RF classifiers", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.8311715424060822}]}, {"text": "The good performance of NC on the much larger amount of data in iPosts is inline with the corpus statistics reported in section 4.3, implying a reasonably small amount of class overlap.", "labels": [], "entities": [{"text": "iPosts", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.9277123808860779}]}, {"text": "The classes are thus relatively well represented by their centroids, which is exploited by the NC classifier.", "labels": [], "entities": []}, {"text": "However, as illustrated in, the majority of feature distributions are generally better separated for ENT and UNK, while CON in its mid position shows more overlap to both other classes and is thus overall a less distinct category.", "labels": [], "entities": [{"text": "UNK", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.7596939206123352}]}], "tableCaptions": [{"text": " Table 1: Threads (left) and iPosts (right) RTE datasets compiled from 4 crisis events: amount of pairs per  entailment type (ENT, CON, UNK), amount of unique rumorous claims (#uniq clms) used for creating  the pairs, amount of unique tweets discussing these claims (#uniq tws).", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.8669583797454834}]}, {"text": " Table 2: iPosts dataset. Mean and weighted (wgt) mean results on held-out data after event held-out  cross validation for the Random Forest (RF) and Nearest Centroid (NC) classifiers.", "labels": [], "entities": [{"text": "iPosts dataset", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.9381069242954254}, {"text": "Mean and weighted (wgt) mean", "start_pos": 26, "end_pos": 54, "type": "METRIC", "confidence": 0.8692436814308167}]}, {"text": " Table 3: Threads dataset. Mean and weighted (wgt) mean results on held-out data after event held-out  cross validation for the Random forest and Nearest Centroid classifiers (RF/NC).", "labels": [], "entities": [{"text": "Threads", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9489303827285767}, {"text": "Mean and weighted (wgt) mean", "start_pos": 27, "end_pos": 55, "type": "METRIC", "confidence": 0.8688766786030361}]}]}