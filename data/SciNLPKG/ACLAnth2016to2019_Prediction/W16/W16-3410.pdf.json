{"title": [{"text": "Potential and Limits of Using Post-edits as Reference Translations for MT Evaluation", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.988882303237915}]}], "abstractContent": [{"text": "This work investigates the potential use of post-edited machine translation (MT) outputs as reference translations for automatic machine translation evaluation, focusing mainly on the following important question: Is it necessary to take into account the machine translation system and the source language from which the given post-edits are generated?", "labels": [], "entities": [{"text": "post-edited machine translation (MT) outputs", "start_pos": 44, "end_pos": 88, "type": "TASK", "confidence": 0.8154509748731341}, {"text": "machine translation evaluation", "start_pos": 129, "end_pos": 159, "type": "TASK", "confidence": 0.7753330568472544}]}, {"text": "In order to explore this, we investigated the use of post-edits originating from different machine translation systems (two statistical systems and two rule-based systems), as well as the use of post-edits originating from two different source languages (English and German).", "labels": [], "entities": []}, {"text": "The obtained results shown that for comparison of different systems using automatic evaluation metrics, a good option is to use a post-edit originating from a high-quality (possibly distinct) system.", "labels": [], "entities": []}, {"text": "A better option is to use it together with other references and post-edits, however post-edits originating from poor translation systems should be avoided.", "labels": [], "entities": []}, {"text": "For tuning or development of a particular system, post-edited output of this same system seems to be the best reference translation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The evaluation of the machine translation (MT) output is an important and difficult task.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.8142393946647644}]}, {"text": "The fastest way is to use an automatic evaluation metric, which compares the obtained output with a human translation of the same source text and calculates a numerical score related to their similarity.", "labels": [], "entities": []}, {"text": "Despite all disadvantages and criticisms, such metrics are still irreplaceable for many tasks (such as rapid development of anew system, tuning of a statistical MT system, etc.) and are considered as at least baseline metrics for MT quality evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 161, "end_pos": 163, "type": "TASK", "confidence": 0.9314277768135071}, {"text": "MT quality evaluation", "start_pos": 230, "end_pos": 251, "type": "TASK", "confidence": 0.9038398861885071}]}, {"text": "All these metrics (n-gram based such as BLEU and METEOR, edit-distance based such as TER, etc.) are reference-based, i.e. a human reference translation is needed as a gold standard.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.99796462059021}, {"text": "METEOR", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9435253143310547}, {"text": "TER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.991309642791748}]}, {"text": "Since there is usually not only one single best translation of a text, the best way of evaluating an MT output would be to compare it with many references -nevertheless, creating each reference translation is a time consuming and expensive process.", "labels": [], "entities": [{"text": "MT output", "start_pos": 101, "end_pos": 110, "type": "TASK", "confidence": 0.8789615035057068}]}, {"text": "Therefore, automatic MT evaluation is usually carried out using only a single reference.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.96030193567276}]}, {"text": "On the other hand, MT has considerably improved in the recent years so that the use of MT outputs as a starting point for human translation has become a common practice.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.982218325138092}, {"text": "MT outputs", "start_pos": 87, "end_pos": 97, "type": "TASK", "confidence": 0.8837772905826569}, {"text": "human translation", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.6939242035150528}]}, {"text": "Therefore, ever-increasing amounts of post-edited machine translation outputs (PEs) are being collected.", "labels": [], "entities": [{"text": "machine translation outputs (PEs)", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.7576706111431122}]}, {"text": "These represent very valuable data and are being used fora number of applications, such as automatic quality prediction, adaptation, etc.", "labels": [], "entities": [{"text": "automatic quality prediction", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.6366653641064962}, {"text": "adaptation", "start_pos": 121, "end_pos": 131, "type": "TASK", "confidence": 0.8417503833770752}]}, {"text": "Among other things, post-edits are more similar to MT outputs than \"independent\" references, thus being potentially more useful for automatic evaluation and/or tuning.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.8481214642524719}]}, {"text": "However, their use as reference translations has been scarcely investigated so far.", "labels": [], "entities": []}, {"text": "This work explores two scenarios: comparing four distinct MT systems using PEs originating from these systems, as well as comparing translations from two different source languages using PEs originating from these source languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9615147113800049}]}, {"text": "In addition, the effects of using multiple references are reported in terms of variations and standard deviations of automatic scores for different number of references.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all experiments, BLEU scores] and character n-gram F scores, i.e. CHRF3 scores, calculated using different PEs are reported.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.999015212059021}, {"text": "character n-gram F scores", "start_pos": 38, "end_pos": 63, "type": "METRIC", "confidence": 0.6749877035617828}, {"text": "PEs", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9781715869903564}]}, {"text": "BLEU is used as a well-known and widely used metric, and CHRF3 as a simple tokenisationindependent metric, which has shown very good correlations with human judgements on the WMT-2015 shared metric task, both on the system level as well as on the segment level, especially for morphologically rich(er) languages.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9748327136039734}, {"text": "CHRF3", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.7550662159919739}, {"text": "WMT-2015 shared metric task", "start_pos": 175, "end_pos": 202, "type": "TASK", "confidence": 0.6162676364183426}]}, {"text": "For both scores, Pearson's system-level correlation coefficient r is reported for each PE.", "labels": [], "entities": [{"text": "Pearson's system-level correlation coefficient r", "start_pos": 17, "end_pos": 65, "type": "METRIC", "confidence": 0.8760846356550852}]}, {"text": "For CHRF3, segment-level Kendall's \u03c4 correlation coefficient is presented as well.", "labels": [], "entities": [{"text": "segment-level Kendall's \u03c4 correlation coefficient", "start_pos": 11, "end_pos": 60, "type": "METRIC", "confidence": 0.7468996693690618}]}, {"text": "For both correlation coefficients, the ties inhuman rankings were excluded from calculation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.9815510511398315}]}, {"text": "In all tables, post-edited MT outputs are marked with pe . Initially, for each of the two data sets the scores were calculated separately for each target language.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9863891005516052}]}, {"text": "Nevertheless, since no differences related to the target language were observed, the results were merged.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU (left) and CHRF3 (right) scores calculated on PEs originating from four  distinct MT systems (two SMT and two", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992688298225403}, {"text": "CHRF3", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.936345636844635}, {"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.8880934119224548}]}, {"text": " Table 3: Edit distances between PEs originating from four distinct systems and refer- ence; the PEs of the same system types are slightly closer than those of the two different  system types; the reference is significantly different from all PEs.", "labels": [], "entities": [{"text": "Edit", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9651319980621338}]}, {"text": " Table 4: Example of post-edited German-to-English MT outputs originating from four  distinct translation systems.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.8660975694656372}]}, {"text": " Table 5: BLEU (left) and CHRF3 (right) scores calculated on SMTPEs originating from  two different source languages and on an independent reference translation; the results  are strongly biased towards the source language; the best option is to use PE of a high  performance system or multiple references without PEs of poor systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990087151527405}, {"text": "CHRF3", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.5159717798233032}]}, {"text": " Table 6: Edit distances between post-edits originating from two different source lan- guages and an independent reference translation.", "labels": [], "entities": [{"text": "Edit", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.976635217666626}]}, {"text": " Table 7: Effects of the number of multiple references: average BLEU and CHRF3 scores  with standard deviations for different number of (independent) references ranging from  1 to 12. The results are obtained on the texts used in previous sections (a), (b) as well  as on a small text with a large number (12) of independent reference translations (c).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9979361295700073}]}, {"text": " Table 8: Effects of source language on tuning of an SMT system: MERT tuning on BLEU  using independent reference, post-edit from the corresponding source language and  post-edit from another source language. The best BLEU and METEOR scores are ob- tained when the corresponding source language post-edit is used.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9914060831069946}, {"text": "MERT", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9890282154083252}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9934019446372986}, {"text": "BLEU", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.9946761131286621}, {"text": "METEOR", "start_pos": 227, "end_pos": 233, "type": "METRIC", "confidence": 0.9237627983093262}]}]}