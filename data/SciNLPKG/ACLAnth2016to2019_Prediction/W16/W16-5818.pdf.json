{"title": [{"text": "Codeswitching language identification using Subword Information Enriched Word Vectors", "labels": [], "entities": [{"text": "Codeswitching language identification", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.5809782445430756}]}], "abstractContent": [{"text": "Codeswitching is a widely observed phenomenon among bilingual speakers.", "labels": [], "entities": []}, {"text": "By combining subword information enriched word vectors with linear-chain Conditional Random Field, we develop a supervised machine learning model that identifies languages in a English-Spanish codeswitched tweets.", "labels": [], "entities": []}, {"text": "Our computational method achieves a tweet-level weighted F1 of 0.83 and a token-level accuracy of 0.949 without using any external resource.", "labels": [], "entities": [{"text": "tweet-level weighted F1", "start_pos": 36, "end_pos": 59, "type": "METRIC", "confidence": 0.834059496720632}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.8774197101593018}]}, {"text": "The result demonstrates that named entity recognition remains a challenge in codeswitched texts and warrants further work.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.6741681198279063}]}], "introductionContent": [{"text": "Codeswitching (CS) is a widely observed phenomenon in social media.", "labels": [], "entities": [{"text": "Codeswitching (CS)", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.5963785275816917}]}, {"text": "define CS broadly as a communication act, whether spoken or written, where two or more languages are being used interchangeably.", "labels": [], "entities": []}, {"text": "Codeswitching is common among bilingual speakers, both in speech and in writing.", "labels": [], "entities": []}, {"text": "Identifying the languages in a codeswitched input is a crucial first step before applying other natural language processing algorithms.", "labels": [], "entities": []}, {"text": "The second shared task, like the previous one (), challenges the participants to develop computational method for identifying the language of each word in a dataset of codeswitched tweets.", "labels": [], "entities": []}, {"text": "For each word in the source, the goal is to identify whether the word is lang1, lang2, mixed, other (punctuation, emotion and everything that is not a word in neither lang1 nor lang2), ambiguous,  ne (named entity), unknown or fw (foreign word).", "labels": [], "entities": []}, {"text": "Lang1 and lang2 are the two languages presented in a codeswitched language pair.", "labels": [], "entities": []}, {"text": "There are two language pairs available in this shared task: Modern Standard Arabic-Arabic Dialects (MSA-DA) and English-Spanish (EN-ES).", "labels": [], "entities": []}, {"text": "An example of token language identification is shown in.", "labels": [], "entities": [{"text": "token language identification", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.8147720893224081}]}, {"text": "Our work covers only the EN-ES language pair.", "labels": [], "entities": []}, {"text": "We use FastText () to train a subword information enhanced word vectors model from the datasets of the shared task.", "labels": [], "entities": []}, {"text": "We then use these vectors and, in addition, custom features extracted from the words to train a linear-chain Conditional Random Field model that predicts the language label of each word.", "labels": [], "entities": []}, {"text": "Our system requires only the dataset provided by the shared task, without any external resource.", "labels": [], "entities": []}, {"text": "The final model scores 0.83 in weighted tweet-level F1 and 0.949 in overall tokenlevel accuracy.", "labels": [], "entities": [{"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.7484960556030273}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9813747406005859}]}], "datasetContent": [{"text": "The shared task maintains three sets of dataset: a training dataset, a development dataset and a testing dataset.", "labels": [], "entities": []}, {"text": "Each dataset contains rows of token extracted from EN-ES codeswitched tweets and the respective gold standard label for each token.", "labels": [], "entities": []}, {"text": "We train an unsupervised FastText word vectors model Meaning as well as in English using the training dataset.", "labels": [], "entities": []}, {"text": "Then we train a supervised CRF model using the same dataset.", "labels": [], "entities": []}, {"text": "The supervised model is validated on the development dataset by evaluating standard metrics: precision, recall and F-measure of the predictions (Powers, 2011).", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9994534850120544}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9980642199516296}, {"text": "F-measure", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9987046718597412}]}, {"text": "We make hyper-parameter tuning to the CRF classifier using grid search.", "labels": [], "entities": []}, {"text": "To verify that all our features were contributing to the model's performance, we also did an ablation study, removing one group of features at a time.", "labels": [], "entities": []}, {"text": "Using the final model which consist of all the features, we compute predictions for the tokens in the testing dataset and submit the result to the workshop as final result.", "labels": [], "entities": []}, {"text": "shows the F1 scores on the dev dataset resulting from training with each group of feature removed.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9987909197807312}]}, {"text": "Note that although the removal of word features has no impact on the overall average F1, we decide to keep it because of the extra boost in performance it provides for named entities.", "labels": [], "entities": [{"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9966424703598022}]}], "tableCaptions": [{"text": " Table 2: Feature ablation study. F1 on dev dataset after training with individual feature groups removed. The F1 for mixed, fw and", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9982888102531433}, {"text": "F1", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.9948514103889465}]}, {"text": " Table 3: Tweet-level performance -there are in total 4626", "labels": [], "entities": [{"text": "Tweet-level", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7754517197608948}]}, {"text": " Table 4: Token-level performance -the number of tokens for", "labels": [], "entities": []}]}