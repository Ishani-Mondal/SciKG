{"title": [{"text": "An Information Foraging Approach to Determining the Number of Relevant Features", "labels": [], "entities": []}], "abstractContent": [{"text": "For many types of high-dimensional data, such as natural language corpora, the vast majority of extracted variables or features are essentially noise.", "labels": [], "entities": []}, {"text": "Culling such features cannot only reveal important patterns , but also improve the performance of supervised and unsupervised machine algorithms.", "labels": [], "entities": []}, {"text": "Most research on feature selection has focused on the statistical measures used to rank features.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7623212337493896}]}, {"text": "Meanwhile, little work has been done developing techniques for identifying the optimal subset of features without repeatedly training models.", "labels": [], "entities": []}, {"text": "However, developing such techniques is important, as they can significantly decrease computation time while providing away to determine the features that characterize the classes within a data set, independent of how the data maybe classified in the future.", "labels": [], "entities": []}, {"text": "Here we introduce a novel method based on information foraging that works in conjunction with existing feature ranking methods to automatically determine a subset of important features.", "labels": [], "entities": []}, {"text": "The method is demonstrated on simulated and linguistic data from psychiatric interviews.", "labels": [], "entities": []}, {"text": "We show that the method is able to accurately determine the features that characterize the classes within both data sets.", "labels": [], "entities": []}, {"text": "The method is fast, simple, and independent of any method of classifying the data, and can be extended to any high-dimensional data set.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The method is demonstrated on two kinds of data: simulated data sets and a linguistic data set from a clinical trial.", "labels": [], "entities": []}, {"text": "The goal of the simulated experiments is to show that the method is able to accurately identify subsets of features with inter-class statistical differences.", "labels": [], "entities": []}, {"text": "In these experiments, the performance of the algorithm is evaluated based on its ability to accurately identify these subsets.", "labels": [], "entities": []}, {"text": "The goal of demonstrating the method on clinical trial data is to evaluate the method within a more realistic context of a wrapper method applied to linguistic data.", "labels": [], "entities": []}, {"text": "Evaluating the method's performance on such data also illustrates its behavior on data containing redundant and correlated features.", "labels": [], "entities": []}, {"text": "Each simulated data set is comprised of data points from two classes.", "labels": [], "entities": []}, {"text": "(The number of data are kept small to reflect the small sample sizes typically found in clinically annotated NLP data sets.)", "labels": [], "entities": []}, {"text": "The data from the first class (class A) are generated from a Gaussian distribution with mean 0 and standard deviation \u03c3.", "labels": [], "entities": [{"text": "standard deviation \u03c3", "start_pos": 99, "end_pos": 119, "type": "METRIC", "confidence": 0.8781179189682007}]}, {"text": "The data from the second class (class B) are generated from two Gaussian distributions; f \u00d7 100% of the features are generated with mean 1 and standard deviation \u03c3, while the rest of the features are generated in the same fashion as those from class A, with mean 0 and standard deviation \u03c3.", "labels": [], "entities": [{"text": "standard deviation \u03c3", "start_pos": 143, "end_pos": 163, "type": "METRIC", "confidence": 0.8761433561642965}]}, {"text": "In this way, f \u00d7 100% of the features are generated with interclass differences.", "labels": [], "entities": []}, {"text": "The performance of the algorithm is then evaluated as a function of the definition of gain, sparsity of the data (s), the total number of features (F ), number of features with statistical differences (f ), and statistical differences between features (parameterized by \u03c3).", "labels": [], "entities": []}, {"text": "The gain is define in four ways: as 1-p-value from the Kolmogorov-Smirnov test (1 \u2212 p KS ), 1-p-value from ANOVA (Fisher, 1992) (1 \u2212 p AN OV A ), and the reciprocal of the KS and ANOVA p-values (1/p KS and 1/p AN OV A , respectively).", "labels": [], "entities": [{"text": "ANOVA (Fisher, 1992)", "start_pos": 107, "end_pos": 127, "type": "DATASET", "confidence": 0.7927307486534119}, {"text": "AN OV A )", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.814086526632309}]}, {"text": "The influence of \u03bb i is also studied by setting it to its empirical value and to unity.", "labels": [], "entities": []}, {"text": "When they are not being varied, the default values for F , s, \u03c3 and f are: 1, 000, 0.5, 0.2 and 0.5, respectively.", "labels": [], "entities": []}, {"text": "The data from the clinical trial are derived from the Suicide Thought Markers study (.", "labels": [], "entities": [{"text": "Suicide Thought Markers study", "start_pos": 54, "end_pos": 83, "type": "DATASET", "confidence": 0.7922709882259369}]}, {"text": "In this study, three hundred seventy-nine adults and adolescents from Cincinnati Childrens Hospital Medical Center (CCHMC), University of Cincinnati (UC), and Princeton Community Hospital (PCH) were enrolled during the course of the study between October 2013 and March 2015.", "labels": [], "entities": [{"text": "Princeton Community Hospital (PCH)", "start_pos": 159, "end_pos": 193, "type": "DATASET", "confidence": 0.8926103512446085}]}, {"text": "Participants were evenly divided into three subject groups: suicidal, patients with mental illness, and controls.", "labels": [], "entities": []}, {"text": "Suicidal subjects consisted of patients who presented in the Emergency Department (ED) with suicidal ideation or behaviors; the mental illness group was not suicidal, but had a mental health diagnosis; and the control group had no mental illness diagnosis and was not suicidal.", "labels": [], "entities": []}, {"text": "Subjects were then asked five open-ended, ubiquitous questions (UQs): Do you have hope?, \"Do you have any fear?\", \"Do you have any secrets?\", \"Are you angry?\", and \"Does it hurt emotionally?\".", "labels": [], "entities": []}, {"text": "These questions were intended to stimulate conversation for language sampling, and would later form the basis of the training sample for the machine learning algorithm.", "labels": [], "entities": [{"text": "language sampling", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7841562032699585}]}, {"text": "The interviews were transcribed and the subjects words were extracted in a systematic way.", "labels": [], "entities": []}, {"text": "For classification purposes, each subject was characterized by (1) their subject group and (2) a vector of word (1-gram) frequencies.", "labels": [], "entities": []}, {"text": "Due to the extreme variability of word frequencies and interview lengths, the frequencies were normalized to smooth the frequency distributions and lessen the classifiers sensitivity to interview length.", "labels": [], "entities": []}, {"text": "The word frequencies were therefore logarithmically (log(x+1)) transformed to smooth the frequencies, and further L2-normalized at the subject level as to base the classification on relative word frequencies.", "labels": [], "entities": []}, {"text": "Only suicidal and control patients are used in the present work.", "labels": [], "entities": []}, {"text": "To test the method on various sizes and types of data, the data are split three ways: patients from CCHMC (pediatric patients), patients from PCH and UC (adults patients), and patients from all three hospitals.", "labels": [], "entities": [{"text": "PCH", "start_pos": 142, "end_pos": 145, "type": "DATASET", "confidence": 0.9191981554031372}]}, {"text": "In the end, 2,471, 4,788, and 5,457 unique words were extracted over 84, 169, and 253 suicidal and control subjects from CCHMC, PCH and UC, and all hospitals, respectively.", "labels": [], "entities": [{"text": "CCHMC", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.9651048183441162}, {"text": "PCH", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.8549894094467163}]}, {"text": "The number relevant of features are then evaluated using the method presented in this work, and a wrapper method whereby the performance of Support Vector Machine (SVM) classifiers are evaluated using LOO cross-validation.", "labels": [], "entities": []}, {"text": "Note the classifications here are simplified versions of the classifications in (; for instance, the features here are not partitioned based on the questions.", "labels": [], "entities": []}, {"text": "show the F 1 scores for selecting features, varying the total number of features (F), the matrix sparsity (s), \u03c3, and the fraction of features with statistical differences.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9338148633639017}, {"text": "matrix sparsity (s)", "start_pos": 90, "end_pos": 109, "type": "METRIC", "confidence": 0.8107794165611267}]}, {"text": "The method is able to determine the features with significant features of a large parameter space when 1 \u2212 p X defines the gain.", "labels": [], "entities": []}, {"text": "On the other hand, when the reciprocal pvalues are used, the method fails spectacularly, indicating that p X must be bounded or it must possess a more direct statistical interpretation.", "labels": [], "entities": []}, {"text": "This aside, performance is, to a degree, invariant to the type of statistic used; the KS test p-value performs better when the matrix is sparse, while the ANOVA p-value works better when the statistical differences are small.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.9006500840187073}]}, {"text": "This maybe less of a reflection on the method, and more to do with the KS test's ability to detect differences in small data samples, and ANOVA's ability to detect statistical differences when the distributions are Gaussian.", "labels": [], "entities": [{"text": "KS test", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.7509681284427643}]}, {"text": "shows the same plots with the meantime between patches set to unity (\u03bb i = 1).", "labels": [], "entities": []}, {"text": "The two sets of figures look nearly identical indicating that \u03bb i does not play a significant role in determining the number of features.", "labels": [], "entities": []}, {"text": "shows the area under the crossvalidated receiver operating curve (AROC) of the SVM classifier as a function of the number of topranked features.", "labels": [], "entities": [{"text": "receiver operating curve (AROC)", "start_pos": 40, "end_pos": 71, "type": "METRIC", "confidence": 0.8192769090334574}]}, {"text": "The number of features determined by our method, along with the corresponding AROC, are circled on these plots.", "labels": [], "entities": [{"text": "AROC", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.8869333863258362}]}, {"text": "In these plots, the relevant number of features are the minimal number of features that optimize classifier performance.", "labels": [], "entities": []}, {"text": "When the KS test p-values are used for the gain, the method is unable to predict the optimal features.", "labels": [], "entities": []}, {"text": "However, the oscillating performance as the number of features increase indicate the KS test may not be the best choice for feature ranking for this data set.", "labels": [], "entities": [{"text": "KS test", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.6314794272184372}]}, {"text": "In contrast, the ANOVA p-value is more stable, leading to more monotonic curves, and the method is better able to determine the optimal number of features.: The plots in 1 with the meantime between patches set to unity (\u03bb i = 1).", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9077265858650208}]}], "tableCaptions": []}