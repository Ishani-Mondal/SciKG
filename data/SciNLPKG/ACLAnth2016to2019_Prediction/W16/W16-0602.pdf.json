{"title": [{"text": "Extending Phrase-Based Translation with Dependencies by Using Graphs", "labels": [], "entities": [{"text": "Extending Phrase-Based Translation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7484351396560669}]}], "abstractContent": [{"text": "In this paper, we propose a graph-based translation model which takes advantage of discon-tinuous phrases.", "labels": [], "entities": []}, {"text": "The model segments a graph which combines bigram and dependency relations into subgraphs and produces translations by combining translations of these sub-graphs.", "labels": [], "entities": []}, {"text": "Experiments on Chinese-English and German-English tasks show that our system is significantly better than the phrase-based model.", "labels": [], "entities": []}, {"text": "By explicitly modeling the graph seg-mentation, our system gains further improvement .", "labels": [], "entities": []}], "introductionContent": [{"text": "One significant weakness of conventional phrasebased (PB) models ( is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne.", "labels": [], "entities": []}, {"text": "pas to English not (. Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns) or treelets, handling non-syntactic phrases is still a big challenge.", "labels": [], "entities": []}, {"text": "In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs.", "labels": [], "entities": []}, {"text": "Each subgraph is connected and may cover discontinuous phrases.", "labels": [], "entities": []}, {"text": "Experiments show that our model is significantly better than the PB model.", "labels": [], "entities": []}, {"text": "Explicitly modeling the graph segmentation further improves our system.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our Chinese-English (ZH-EN) training corpus contains 1.5M+ sentence pairs from LDC.", "labels": [], "entities": [{"text": "Chinese-English (ZH-EN) training corpus", "start_pos": 4, "end_pos": 43, "type": "DATASET", "confidence": 0.6352756867806116}]}, {"text": "Our GermanEnglish (DE-EN) training corpus (2M+ sentence pairs) is from WMT 2014.", "labels": [], "entities": [{"text": "GermanEnglish (DE-EN) training corpus", "start_pos": 4, "end_pos": 41, "type": "DATASET", "confidence": 0.8156787753105164}, {"text": "WMT 2014", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9156133234500885}]}, {"text": "GBMT is our graphbased translation system and GSM adds the graph segmentation model into GBMT.", "labels": [], "entities": [{"text": "GBMT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.967088520526886}, {"text": "GSM", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.8509309887886047}, {"text": "GBMT", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9417531490325928}]}, {"text": "DTU extends the PB model by allowing source discontinuous phrases (.", "labels": [], "entities": [{"text": "DTU", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9516624808311462}]}, {"text": "All systems are implemented in Moses ( shows our main results.", "labels": [], "entities": []}, {"text": "Our system GBMT is better than PBMT as measured by all three metrics across all testsets.", "labels": [], "entities": [{"text": "GBMT", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.4830573499202728}, {"text": "PBMT", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.6726192235946655}]}, {"text": "This improvement is reasonable as GBMT allows discontinuous phrases which can reduce data sparsity and handle longdistance relations (.", "labels": [], "entities": [{"text": "GBMT", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.7624043822288513}]}, {"text": "Since phrases from syntactic structures are fewer in number but more reliable (, our system GBMT achieves slightly better performance than DTU but uses significantly fewer rules, as shown in.", "labels": [], "entities": [{"text": "GBMT", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.7176263928413391}]}, {"text": "After integrating the graph segmentation model to help subgraph selection, our system (GSM) achieves significantly better BLEU than DTU on both language pairs.", "labels": [], "entities": [{"text": "subgraph selection", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7714304029941559}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9992146492004395}]}], "tableCaptions": [{"text": " Table 1: BLEU (Papineni et al., 2002) scores for all systems on", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982106685638428}]}]}