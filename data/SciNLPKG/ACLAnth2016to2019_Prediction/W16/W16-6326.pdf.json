{"title": [{"text": "Twitter Named Entity Extraction and Linking Using Differential Evolution", "labels": [], "entities": [{"text": "Entity Extraction", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.6412781924009323}]}], "abstractContent": [{"text": "Systems that simultaneously identify and classify named entities in Twitter typically show poor recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9989540576934814}]}, {"text": "To remedy this, the task is here divided into two parts: i) named entity identification using Conditional Random Fields in a multi-objective framework built on Differential Evolution, and ii) named entity classification using Vector Space Modelling and edit distance techniques.", "labels": [], "entities": [{"text": "entity identification", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7167764157056808}, {"text": "entity classification", "start_pos": 198, "end_pos": 219, "type": "TASK", "confidence": 0.7185911685228348}]}, {"text": "Differential Evolution is an evolutionary algorithm, which not only opti-mises the features, but also identifies the proper context window for each selected feature.", "labels": [], "entities": [{"text": "Differential Evolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8210088014602661}]}, {"text": "The approach obtains F-scores of 70.7% for Twitter named entity extraction and 66.0% for entity linking to the DB-pedia database.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9990488886833191}, {"text": "Twitter named entity extraction", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.5292128548026085}]}], "introductionContent": [{"text": "Twitter has established itself as one of the most popular social networks, with about 320 million active users daily generating almost 500 million short messages, tweets, with a maximum length of 140 characters.", "labels": [], "entities": []}, {"text": "The language used is very noisy, with tweets containing many grammatical and spelling mistakes, short form of the words, multiple words merged together, special symbols and characters inserted into the words, etc.", "labels": [], "entities": []}, {"text": "Hence it is difficult to analyse and monitor all types of tweets, and the vast number of tweets: specific messages may need to be filtered out from millions of tweets.", "labels": [], "entities": []}, {"text": "Named entity extraction plays a vital role when filtering out relevant tweets from a collection.", "labels": [], "entities": [{"text": "Named entity extraction", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6437133252620697}]}, {"text": "It is also useful for pre-processing in many other language processing tasks, such as machine translation and question-answering.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.8142087459564209}]}, {"text": "The paper is organized as follows: Section 2 describes related work on Twitter named entity recognition and linking.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.6346890727678934}]}, {"text": "The actual Twitter name identification methodology and different features used are presented in Section 3.", "labels": [], "entities": [{"text": "Twitter name identification", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.652292529741923}]}, {"text": "Section 4 focuses on classification of the identified named entities and their linking to DBpedia.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9695483446121216}]}, {"text": "Experimental results and a discussion of those appear in Section 5 and Section 6, respectively, while Section 7 addresses future work and concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "The approach was tested on the NEEL2016 datasets ( . The statistics of the datasets are given in.", "labels": [], "entities": [{"text": "NEEL2016 datasets", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9914259910583496}]}, {"text": "This data was used to train a CRF-based classifier as baseline and then to build all the models of the full DE-based system, for Twitter named entity identification as well as for categorization and DBpedia linking, as described in turn below.", "labels": [], "entities": [{"text": "Twitter named entity identification", "start_pos": 129, "end_pos": 164, "type": "TASK", "confidence": 0.5500454232096672}, {"text": "DBpedia linking", "start_pos": 199, "end_pos": 214, "type": "TASK", "confidence": 0.7187002301216125}]}, {"text": "To enhance recall and F-score, Twitter names are first identified using the multi-objective DE-based technique.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9995090961456299}, {"text": "F-score", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9985008239746094}]}, {"text": "Ina second step, the identified Twitter named entities are classified using vector space modelling and edit distances.", "labels": [], "entities": []}, {"text": "A baseline model for identification of Twitter names was built using the features described in Section 3.3.", "labels": [], "entities": [{"text": "identification of Twitter names", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.8555791825056076}]}, {"text": "When trained on the training data and evaluated on the development data, the model shows recall, precision and F 1 -scores of 56.1%, 87.5% and 68.4%, respectively, as given in the 'Dev Data' column of.", "labels": [], "entities": [{"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9997887015342712}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9994931221008301}, {"text": "F 1 -scores", "start_pos": 111, "end_pos": 122, "type": "METRIC", "confidence": 0.9922356307506561}, {"text": "Dev Data' column", "start_pos": 181, "end_pos": 197, "type": "DATASET", "confidence": 0.8372025862336159}]}, {"text": "To improve on these results, a Differential Evolution-based feature selection technique was used to identify named entities in noisy text.", "labels": [], "entities": [{"text": "Differential Evolution-based feature selection", "start_pos": 31, "end_pos": 77, "type": "TASK", "confidence": 0.8109083920717239}]}, {"text": "For Twitter named entity identification, the parameters of the DE were set as follows: \u2022 N (population size) = 100, \u2022 Cr (probability of crossover) = 0.5, \u2022 G max (number of generation) = 100, and \u2022 F mu (mutation factor) = 0.5.", "labels": [], "entities": [{"text": "Twitter named entity identification", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.5462207794189453}, {"text": "Cr (probability of crossover)", "start_pos": 118, "end_pos": 147, "type": "METRIC", "confidence": 0.7863900562127432}, {"text": "G max (number of generation)", "start_pos": 157, "end_pos": 185, "type": "METRIC", "confidence": 0.8179092918123517}, {"text": "F mu (mutation factor)", "start_pos": 199, "end_pos": 221, "type": "METRIC", "confidence": 0.9540323416392008}]}, {"text": "The best feature set along with the context features was determined based on development data.", "labels": [], "entities": []}, {"text": "The selected features along with context features are) and F 20 (\u22121, 0).", "labels": [], "entities": [{"text": "F 20", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9677554368972778}]}, {"text": "This setup achieved recall, precision and F-measure values of 79.9%, 93.8% and 86.3% on the development data.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.999799907207489}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9996645450592041}, {"text": "F-measure", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9993504881858826}]}, {"text": "The performance of the system increases almost 18 Fmeasure points over the baseline model.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9992706179618835}]}, {"text": "To fairly evaluate the approach, it was applied to the unseen test data using the selected features along with the context features, giving the recall, precision and F 1 -scores of 73.9%, 89.2% and 80.8%, respectively; also shown in  When merging the development data with the training data and building a model using selected features, the recall, precision and F-measure values are 81.3%, 90.8% and 85.7%, respectively, also improving on the scores obtained by the baseline model.", "labels": [], "entities": [{"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9995539784431458}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9937096834182739}, {"text": "F 1 -scores", "start_pos": 166, "end_pos": 177, "type": "METRIC", "confidence": 0.9830810874700546}, {"text": "recall", "start_pos": 341, "end_pos": 347, "type": "METRIC", "confidence": 0.9986697435379028}, {"text": "precision", "start_pos": 349, "end_pos": 358, "type": "METRIC", "confidence": 0.9738389849662781}, {"text": "F-measure", "start_pos": 363, "end_pos": 372, "type": "METRIC", "confidence": 0.9850038290023804}]}, {"text": "The results show that the multiobjective DE-based approach efficiently identifies Twitter names from noisy text.", "labels": [], "entities": []}, {"text": "In the next step, the identified Twitter names extracted using the multi-objective DE-based approach were classified.", "labels": [], "entities": []}, {"text": "Three models were built to classify the Twitter named entities into the seven categories described at the beginning of Section 4.", "labels": [], "entities": []}, {"text": "In the first model (Model-1), the identified Twitter named entities are passed through a CRF-based supervised classifier.", "labels": [], "entities": []}, {"text": "The recall, precision and Fmeasure values given in are 38.8%, 40.7% and 39.8%, respectively, showing that the recall is increased over the baseline model (Section 5.1), which simultaneously identifies and classifies the named entities.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9996225833892822}, {"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9993694424629211}, {"text": "Fmeasure", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9994725584983826}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9992658495903015}]}, {"text": "Another Twitter named classification model (Model-2) was built using VSM and edit distance techniques based on training data, giving recall, precision and F-scores of 27.9%, 87.2% and 42.3%.", "labels": [], "entities": [{"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9996032118797302}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9993646740913391}, {"text": "F-scores", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9972555041313171}]}, {"text": "The probable reason for the low performance is that sufficient examples (instances) may not be found in the training data.", "labels": [], "entities": []}, {"text": "When the Model-2 approach was applied to the DBpedia database (Model-3), the recall, precision and F-scores are 67.4%, 86.7% and 75.8%, respectively.", "labels": [], "entities": [{"text": "DBpedia database", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9366475045681}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9998279809951782}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9987397789955139}, {"text": "F-scores", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.997894823551178}]}, {"text": "Model-3 achieves one of the best results among all the models and also outperforms all existing approaches, as shown in.", "labels": [], "entities": []}, {"text": "All the models were blindly evaluated on the test data (the second set of R-P-F 1 scores given in).", "labels": [], "entities": [{"text": "R-P-F 1 scores", "start_pos": 74, "end_pos": 88, "type": "METRIC", "confidence": 0.9636342326800028}]}, {"text": "The Model-3 results show that its performance is far better than all other present state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "It achieves recall, precision and F-measure values of 50.8%, 76.1% and 60.9%, respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9997766613960266}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9995357990264893}, {"text": "F-measure", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9992363452911377}]}, {"text": "Even though the performance of Model-3 on the development data is only slightly better than the best existing system, KEA (, the gap in performance on the test data is a lot larger, since the KEA system tries to increase the recall value as much as possible without controlling the precision value.", "labels": [], "entities": [{"text": "KEA", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.8706169724464417}, {"text": "recall value", "start_pos": 225, "end_pos": 237, "type": "METRIC", "confidence": 0.9810472726821899}, {"text": "precision", "start_pos": 282, "end_pos": 291, "type": "METRIC", "confidence": 0.9986686706542969}]}, {"text": "As a result, KEA's performance on the test data is fairly bad.", "labels": [], "entities": [{"text": "KEA", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.5244359970092773}]}, {"text": "In the last model (Combined Model), the outputs of the three models are merged, increasing the F-score almost 5 points compared to Model-3 on the test data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9989584684371948}]}, {"text": "The precision, recall and F 1 -scores of the Combined Model are 65.3%, 67.1% and 66.2%, respectively (see).", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9998227953910828}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9994563460350037}, {"text": "F 1 -scores", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9902420043945312}, {"text": "Combined Model", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.8951388597488403}]}, {"text": "Hence the Combined Model produces the best results compared to all the other models as well as to all the #Microposts2016 systems.", "labels": [], "entities": []}, {"text": "When merging the development data with the training data (the third set of R-P-F 1 scores in Table 3), the performances of Model-1 and Model-2 are better than Model-3 because many examples in the test data are seen in the development data and these two models are developed based on training instances, while Model-3 classifies the Twitter names based on the DBpedia database.", "labels": [], "entities": [{"text": "DBpedia database", "start_pos": 359, "end_pos": 375, "type": "DATASET", "confidence": 0.9649594724178314}]}, {"text": "Here, the Combined Model also achieves the best results among all the approaches, with precision, recall and F-score of 69.7%, 71.7% and 70.7%.", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9998412132263184}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9997696280479431}, {"text": "F-score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9995823502540588}]}, {"text": "To link the classified Twitter names, we use a knowledge-based approach (utilizing either training data or the DBpedia database).", "labels": [], "entities": [{"text": "DBpedia database", "start_pos": 111, "end_pos": 127, "type": "DATASET", "confidence": 0.9516246318817139}]}, {"text": "The DBpedia links for Models 1 and 2 reported in are based on training data.", "labels": [], "entities": []}, {"text": "These models achieve better precision values than the state-of-the-art systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9983865022659302}]}, {"text": "In Model-3, the links are retrieved from the DBpedia database.", "labels": [], "entities": [{"text": "DBpedia database", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9782876670360565}]}, {"text": "The performance of Model-3 is better than the first and second models.", "labels": [], "entities": []}, {"text": "The recall, precision and F-measure values of Model-3 on the test data are 26.9%, 67.1% and 38.4%.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9998182654380798}, {"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9995610117912292}, {"text": "F-measure", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9990001320838928}]}, {"text": "The Combined Model performs best among our models and produces an F-measure value of 42.1%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9990791082382202}]}, {"text": "All these models outperform the NEEL 2016 systems in terms of precision, but fail get 204   a higher F-score than the KEA system (Waitelonis and Sack, 2016) due to lower recall.", "labels": [], "entities": [{"text": "NEEL 2016", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.7894238829612732}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.999567449092865}, {"text": "F-score", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.9990301132202148}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9988037347793579}]}, {"text": "However, when development data are merged with the training data for the model building, the Combined Model again gives the best test data results among all the systems, achieving recall, precision and Fmeasure values of 57.2%, 78.1% and 66.0%.", "labels": [], "entities": [{"text": "recall", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.9997864365577698}, {"text": "precision", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.9995711445808411}, {"text": "Fmeasure", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.9993793964385986}]}], "tableCaptions": [{"text": " Table 1: Statistics of the NEEL2016 datasets", "labels": [], "entities": [{"text": "NEEL2016 datasets", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.925013393163681}]}, {"text": " Table 2: Twitter Named Entity Identification results", "labels": [], "entities": [{"text": "Twitter Named Entity Identification", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.7119671702384949}]}, {"text": " Table 3: Twitter Named Entity Classification results ('Baseline Model': using all features)", "labels": [], "entities": [{"text": "Twitter Named Entity Classification", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.6522914320230484}]}, {"text": " Table 4: Twitter Named Entity Linking results ('Baseline Model': using all features)", "labels": [], "entities": [{"text": "Twitter Named Entity Linking", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.5840850695967674}]}]}