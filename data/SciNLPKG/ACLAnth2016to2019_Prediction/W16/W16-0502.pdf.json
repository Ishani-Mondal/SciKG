{"title": [{"text": "Text Readability Assessment for Second Language Learners", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper addresses the task of readability assessment for the texts aimed at second language (L2) learners.", "labels": [], "entities": []}, {"text": "One of the major challenges in this task is the lack of significantly sized level-annotated data.", "labels": [], "entities": []}, {"text": "For the present work, we collected a dataset of CEFR-graded texts tailored for learners of English as an L2 and investigated text readability assessment for both native and L2 learners.", "labels": [], "entities": []}, {"text": "We applied a generalization method to adapt models trained on larger native corpora to estimate text readability for learners, and explored domain adaptation and self-learning techniques to make use of the native data to improve system performance on the limited L2 data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.7216241657733917}]}, {"text": "In our experiments, the best performing model for readability on learner texts achieves an accuracy of 0.797 and P CC of 0.938.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9995672106742859}, {"text": "P CC", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9215105772018433}]}], "introductionContent": [{"text": "Developing reading ability is an essential part of language acquisition.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7356669306755066}]}, {"text": "However, finding proper reading materials for training language learners at a specific level of proficiency is a demanding and timeconsuming task for English instructors as well as the readers themselves.", "labels": [], "entities": []}, {"text": "To automate the process of reading material selection and the assessment of reading ability for non-native learners, a system that focuses on text readability analysis for L2 learners can be developed.", "labels": [], "entities": [{"text": "reading material selection", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.6593666474024454}, {"text": "text readability analysis", "start_pos": 142, "end_pos": 167, "type": "TASK", "confidence": 0.6499618887901306}]}, {"text": "Such a system enhances many pedagogical applications by supporting readers in their second language education.", "labels": [], "entities": []}, {"text": "Text readability, which has been formally defined as the sum of all elements in textual material that affect a reader's understanding, reading speed, and level of interest in the material (, is influenced by multiple variables.", "labels": [], "entities": []}, {"text": "These may include the style of writing, its format and organization, reader's background and interest as well as various contextual dimensions of the text, such as its lexical and syntactic complexity, level of conceptual familiarity, logical sophistication and soon.", "labels": [], "entities": [{"text": "soon", "start_pos": 262, "end_pos": 266, "type": "METRIC", "confidence": 0.9914783835411072}]}, {"text": "The choice of the criteria to measure readability often depends upon the need and characteristics of the target readers.", "labels": [], "entities": []}, {"text": "Most of the studies so far have evaluated text difficulty as judged by native speakers, despite the fact that text comprehensibility can be perceived very differently by L2 learners.", "labels": [], "entities": []}, {"text": "In the case of L2 learners, due to the difference in the pace of language acquisition, the focus in readability measures often differs from that for native readers.", "labels": [], "entities": []}, {"text": "For example, the grammatical aspects of readability usually contribute more to text comprehensibility for L2 learners than the conceptual cognition difficulty of the reading material (.", "labels": [], "entities": []}, {"text": "A system that is tailored towards learner's perception of reading difficulty can produce more accurate estimation of text reading difficulty for non-native readers and thus better facilitate language learning.", "labels": [], "entities": []}, {"text": "One of the major challenges fora data-driven approach to text readability assessment for L2 learners is that there is not enough significantly sized, properly annotated data for this task.", "labels": [], "entities": [{"text": "text readability assessment", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.7623709638913473}]}, {"text": "At the same time, text readability assessment in general has been previously studied by many researchers and there area number of existing corpora aimed at native speakers that can be used.", "labels": [], "entities": [{"text": "text readability assessment", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.831059734026591}]}, {"text": "To address the problem, we compiled a collection of texts that are tailored for L2 learners' readability and looked at several approaches to make use of existing native data to estimate readability for L2 learners.", "labels": [], "entities": []}, {"text": "In sum, the contribution of our work is threefold.", "labels": [], "entities": []}, {"text": "First, we develop a system that produces state-ofthe-art estimation of text readability, exploit a range of readability measures and investigate their predictive power.", "labels": [], "entities": []}, {"text": "Second, we focus on readability for L2 learners of English and present a level-graded dataset for non-native readability analysis.", "labels": [], "entities": []}, {"text": "Third, we explore methods that help to make use of the existing native corpora to produce better estimation of readability when there is not enough data aimed at L2 learners.", "labels": [], "entities": []}, {"text": "Specifically, we apply a generalization method to adapt models trained on native data to estimate text readability for learners, and explore domain adaptation and self-training techniques to improve system performance on the data aimed at L2 learners.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.7293861508369446}]}, {"text": "To the best of our knowledge, these approaches have not been applied in readability experiments before.", "labels": [], "entities": []}, {"text": "The best performing model in our experiments achieves an accuracy (ACC) of 0.797 and Pearson correlation coefficient (P CC) of 0.938.", "labels": [], "entities": [{"text": "accuracy (ACC)", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.9545847177505493}, {"text": "Pearson correlation coefficient (P CC)", "start_pos": 85, "end_pos": 123, "type": "METRIC", "confidence": 0.9718500375747681}]}], "datasetContent": [{"text": "Most work on readability assessment has been done on native corpora with age-specific reading levels (  Such texts are aimed not at L2 learners but rather at native-speaking children of different ages.", "labels": [], "entities": []}, {"text": "Therefore, the level annotation in such texts is arrived at using criteria different from those that are relevant for L2 readers.", "labels": [], "entities": []}, {"text": "The lack of significantly sized L2 level-annotated data raises a problem for readability analysis aimed at L2 readers.", "labels": [], "entities": [{"text": "readability analysis", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.8044756948947906}]}, {"text": "To tackle this, we created a dataset with texts tailored for L2 learners' readability specifically.", "labels": [], "entities": []}, {"text": "We have collected a dataset composed of reading passages from the five main suite Cambridge English Exams (KET, PET, FCE, CAE, CPE).", "labels": [], "entities": [{"text": "Cambridge English Exams (KET", "start_pos": 82, "end_pos": 110, "type": "DATASET", "confidence": 0.8570356726646423}]}, {"text": "1 These five exams are targeted at learners at A2-C2 levels of the Common European Framework of Reference (CEFR)).", "labels": [], "entities": [{"text": "A2-C2", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9052693843841553}, {"text": "Common European Framework of Reference (CEFR))", "start_pos": 67, "end_pos": 113, "type": "DATASET", "confidence": 0.8216019794344902}]}, {"text": "The documents are harvested from all the tasks in the past reading papers for each of the exams.", "labels": [], "entities": []}, {"text": "The Cambridge English Exams are designed for L2 learners specifically and the A2-C2 levels assigned to each reading paper can be treated as the level of reading difficulty of the documents for the L2 learners.", "labels": [], "entities": [{"text": "Cambridge English Exams", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.9600636959075928}, {"text": "A2-C2", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9861380457878113}]}, {"text": "3 shows the number of documents at each CEFR level across the dataset.", "labels": [], "entities": [{"text": "CEFR", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.935021162033081}]}, {"text": "The data is available at http://www.", "labels": [], "entities": []}, {"text": "cl.cam.ac.uk/ \u02dc mx223/cedata.html.", "labels": [], "entities": []}, {"text": "Experimenting on the language testing data annotated with the L2 learner readability levels is one of the contributions of this research.", "labels": [], "entities": []}, {"text": "Most previous work on readability assessment for English have relied on the data annotated with readability levels aimed at native speakers.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.8038500845432281}]}, {"text": "In this work, we use language testing data with the levels assigned based on L2 learner levels, and we believe that this level annotation is more appropriate for text readability assessment for L2 learners than using texts with the level annotation aimed at native speakers.", "labels": [], "entities": [{"text": "text readability assessment", "start_pos": 162, "end_pos": 189, "type": "TASK", "confidence": 0.7256480058034261}]}, {"text": "In our experiments, we cast readability assessment as a supervised machine learning problem.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7700094282627106}]}, {"text": "In particular, a pairwise ranking approach is adopted and compared with a classification method.", "labels": [], "entities": []}, {"text": "We believe that the reading difficulty of text is a continuous rather than discrete variable.", "labels": [], "entities": []}, {"text": "Text difficulty within a level can also vary.", "labels": [], "entities": []}, {"text": "Instead of assigning an abso-  Support vector machines (SVM) have been used in the past for readability assessment by many researchers and have consistently yielded better results when compared to other statistical models for the task (.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.775639146566391}]}, {"text": "We use the LIBSVM toolkit (Chang and Lin, 2011) to implement both multiclass classification and pairwise ranking.", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.7879963517189026}]}, {"text": "Five-fold cross validation is used for evaluation.", "labels": [], "entities": []}, {"text": "We report two popular performance metrics, accuracy (ACC) and Pearson correlation coefficient (P CC), and use pairwise accuracy to evaluate ranking models.", "labels": [], "entities": [{"text": "accuracy (ACC)", "start_pos": 43, "end_pos": 57, "type": "METRIC", "confidence": 0.9515442550182343}, {"text": "Pearson correlation coefficient (P CC)", "start_pos": 62, "end_pos": 100, "type": "METRIC", "confidence": 0.974305978843144}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.8112838268280029}]}, {"text": "Pairwise accuracy is defined as the percentage of instance pairs that the model ranked correctly.", "labels": [], "entities": [{"text": "Pairwise", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.881341814994812}, {"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.7776793837547302}]}, {"text": "It should be noted that accuracy and pairwise accuracy are not directly comparable.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9994951486587524}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9698194861412048}]}, {"text": "Thus, P CC is introduced to compare the results of the classification and the ranking models.", "labels": [], "entities": [{"text": "P CC", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.8460395336151123}]}, {"text": "First, we tested the generalization ability of the classification and ranking models trained on the WeeBit corpus on the Cambridge Exams data to see if it is possible to directly apply the models trained on native data to L2 data.", "labels": [], "entities": [{"text": "WeeBit corpus", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.9888556003570557}, {"text": "Cambridge Exams data", "start_pos": 121, "end_pos": 141, "type": "DATASET", "confidence": 0.9046340187390646}]}, {"text": "In the case of the multi-class classification model, the accuracy dropped greatly when the model is applied to the L2 dataset, while the correlation remained relatively high.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.7158648073673248}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9996575117111206}, {"text": "L2 dataset", "start_pos": 115, "end_pos": 125, "type": "DATASET", "confidence": 0.688993513584137}, {"text": "correlation", "start_pos": 137, "end_pos": 148, "type": "METRIC", "confidence": 0.9872146844863892}]}, {"text": "Looking at the confusion matrix of the classifier's predictions on the L2 data (see), we notice that most of the documents in the L2 data are classified into the higher levels of WeeBit by the model.", "labels": [], "entities": [{"text": "L2 data", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.7697703540325165}, {"text": "WeeBit", "start_pos": 179, "end_pos": 185, "type": "DATASET", "confidence": 0.9464049339294434}]}, {"text": "This is because, on average, the Cambridge Exams texts are more difficult than the WeeBit corpus ones which are generally targeted at children of young ages.", "labels": [], "entities": [{"text": "Cambridge Exams texts", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.9753707448641459}, {"text": "WeeBit corpus", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.9721816182136536}]}, {"text": "Thus, the mismatch between the targeted levels has led to poor generalization of the classification model.", "labels": [], "entities": []}, {"text": "In contrast, for the ranking model, both evaluation measures are relatively unharmed when the model is applied to the L2 data.", "labels": [], "entities": []}, {"text": "It shows that, when generalizing to an unseen dataset, the estimation produced by the ranking model is able to maintain a high pairwise accuracy and correlation with the ground truth.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9611010551452637}]}, {"text": "We believe that this is because the ranking model does not try to band the documents into one of the levels on a different basis of difficulty annotation.", "labels": [], "entities": []}, {"text": "Instead, pairwise ranking captures the relative reading difficulty of the documents, and therefore the resulting ranked positions of the documents are closer to the ground truth compared to the classification model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of documents in the original and modified", "labels": [], "entities": []}, {"text": " Table 2: Statistics for the Cambridge English Exams data", "labels": [], "entities": [{"text": "Cambridge English Exams", "start_pos": 29, "end_pos": 52, "type": "DATASET", "confidence": 0.9355358282725016}]}, {"text": " Table 3: Classification and ranking results on the WeeBit cor-", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.974015474319458}, {"text": "WeeBit cor-", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.9853737155596415}]}, {"text": " Table 4: Generalization results of the classification and ranking", "labels": [], "entities": [{"text": "Generalization", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8398696780204773}]}, {"text": " Table 5: Confusion matrix of the classification model on the", "labels": [], "entities": []}, {"text": " Table 6: Results of mapping ranking scores to CEFR levels", "labels": [], "entities": [{"text": "CEFR", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.9737868309020996}]}, {"text": " Table 8: Confusion matrix of the mapped estimation after", "labels": [], "entities": []}, {"text": " Table 9: Results of self-training", "labels": [], "entities": []}]}