{"title": [{"text": "Answering Yes-No Questions by Penalty Scoring in History Subjects of University Entrance Examinations", "labels": [], "entities": [{"text": "Answering Yes-No Questions by Penalty Scoring", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8537681996822357}]}], "abstractContent": [{"text": "Answering yes-no questions is more difficult than simply retrieving ranked search results.", "labels": [], "entities": [{"text": "Answering yes-no questions", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8840185205141703}]}, {"text": "To answer yes-no questions, especially when the correct answer is no, one must find an objectionable keyword that makes the question's answer no.", "labels": [], "entities": []}, {"text": "Existing systems, such as factoid-based ones, cannot answer yes-no questions very well because of insufficient handling of such objectionable keywords.", "labels": [], "entities": []}, {"text": "We suggest an algorithm that answers yes-no questions by assigning an importance to objectionable keywords.", "labels": [], "entities": []}, {"text": "Concretely speaking, we suggest a penalized scoring method that finds and makes lower score for parts of documents that include such objectionable keywords.", "labels": [], "entities": []}, {"text": "We check a keyword distribution for each part of a document such as a paragraph, calculating the keyword density as a basic score.", "labels": [], "entities": []}, {"text": "Then we use an objectionable keyword penalty when a keyword does not appear in a target part but appears in other parts of the document.", "labels": [], "entities": []}, {"text": "Our algorithm is robust for open domain problems because it requires no machine learning.", "labels": [], "entities": []}, {"text": "We achieved 4.45 point better results in F1 scores than the best score of the NTCIR-10 RITE2 shared task, also obtained the best score in 2014 mock university examination challenge of the Todai Robot project.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9790286123752594}, {"text": "NTCIR-10 RITE2 shared task", "start_pos": 78, "end_pos": 104, "type": "DATASET", "confidence": 0.8160491585731506}, {"text": "Todai Robot project", "start_pos": 188, "end_pos": 207, "type": "DATASET", "confidence": 0.8935115337371826}]}], "introductionContent": [{"text": "Although its importance has long been recognized, yes-no question answering (QA) has not been studied well compared to other types of QA such as factoid-style QA () and non-factoid complex QA (, including definition QA (.", "labels": [], "entities": [{"text": "yes-no question answering (QA)", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.8126135865847269}]}, {"text": "As described herein, we propose an approach to answer yes-no questions.", "labels": [], "entities": []}, {"text": "Our main claim is that it is necessary to handle objectionable keywords in no questions that are insufficiently considered in previous studies.", "labels": [], "entities": []}, {"text": "We claim that this is the greatest difference in yes-no QA from other QA tasks.", "labels": [], "entities": [{"text": "QA", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.7137786149978638}]}, {"text": "We suggest a penalized scoring method that finds and makes lower scores for objectionable keywords.", "labels": [], "entities": []}, {"text": "This method can classify yes-no answers more sharply, overcoming the white noise effects described below.", "labels": [], "entities": []}, {"text": "In spite of the apparent simplicity that a yes-no question is a binary decision, it is not easy to answer.", "labels": [], "entities": []}, {"text": "One might consider the following yes-no question.", "labels": [], "entities": []}, {"text": "(1) Is it dangerous to use an acidic cleaner with enzyme bleach?", "labels": [], "entities": []}, {"text": "A slightly different question can be posed by replacing enzyme with chlorine.", "labels": [], "entities": []}, {"text": "(2) Is it dangerous to use an acidic cleaner with chlorine bleach?", "labels": [], "entities": []}, {"text": "Example (1) includes the keywords dangerous, acidic cleaner, and enzyme bleach, while (2) includes chlorine bleach instead of enzyme bleach.", "labels": [], "entities": []}, {"text": "Correct answers are no for (1) and yes for (2).", "labels": [], "entities": []}, {"text": "The standard means of answering yes-no questions would be to ask a search engine using keywords extracted as shown above.", "labels": [], "entities": []}, {"text": "A search engine can return ranked results with confidence values.", "labels": [], "entities": []}, {"text": "Comparing the topmost confidence values of yes and no questions, we can determine yes or no.", "labels": [], "entities": []}, {"text": "However, standard search engines do not expect an objectionable keyword, enzyme bleach in (1).", "labels": [], "entities": []}, {"text": "Therefore, they do not make a sufficient difference between (1) and (2), do not directly function for yes-no questions.", "labels": [], "entities": []}, {"text": "Yes-no QA can also be regarded as an application of factoid-style QA systems.", "labels": [], "entities": [{"text": "Yes-no QA", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.5679076313972473}]}, {"text": "In fact, (2) can be converted into the following.", "labels": [], "entities": []}, {"text": "(3) What is dangerous to use an acidic cleaner with?", "labels": [], "entities": []}, {"text": "By replacing chlorine bleach with What, a factoid-style QA system ( can provide an answer to question (3) such as chlorine bleach.", "labels": [], "entities": []}, {"text": "By comparing the answer with the original question's keyword such as chlorine bleach in (2), yes or no can be assigned for each question).", "labels": [], "entities": []}, {"text": "However, this conversion process includes a large part of the entire solution process as described below.", "labels": [], "entities": []}, {"text": "The next example adds in a washing machine to (2), thereby producing the following question.", "labels": [], "entities": []}, {"text": "(4) Is it dangerous to use an acidic cleaner with chlorine bleach in a washing machine?", "labels": [], "entities": []}, {"text": "This addition does not affect the yes-no answer.", "labels": [], "entities": []}, {"text": "When converting this question into a factoid-style question, which keyword to replace is a critical and difficult issue ().", "labels": [], "entities": []}, {"text": "The best system () in the World History of the Todai Robot project's mock exam challenge combined different methods that make effective features unclear.", "labels": [], "entities": [{"text": "World History of the Todai Robot project's mock exam challenge", "start_pos": 26, "end_pos": 88, "type": "DATASET", "confidence": 0.9237143397331238}]}, {"text": "These previous works leave some issues unresolved, what is the key feature to answer yes-no questions.", "labels": [], "entities": []}, {"text": "In either case, finding an objectionable keyword is the missing issue.", "labels": [], "entities": []}, {"text": "Ideally speaking, all the keywords would co-occur in an evidence description of the knowledge source if the answer is yes.", "labels": [], "entities": []}, {"text": "Unfortunately, keyword extraction is not perfect because it is extremely difficult to determine an unrelated keyword such as washing machine.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.914618581533432}]}, {"text": "Distribution of such an unrelated keyword has no relation to the cooccurrence of relevant and objectionable keywords.", "labels": [], "entities": []}, {"text": "Consequently, it makes a sort of white noise in scoring.", "labels": [], "entities": [{"text": "scoring", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.9424319863319397}]}, {"text": "This effect produces a score difference between relevant and objectionable keywords vague.", "labels": [], "entities": []}, {"text": "Standard frequency-based algorithms will not answer yes-no questions adequately.", "labels": [], "entities": []}, {"text": "Recognition of Textual Entailment (RTE) is another related task to the yes-no QA.", "labels": [], "entities": [{"text": "Recognition of Textual Entailment (RTE)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.9152753608567374}]}, {"text": "RTE has recently been studied intensively, including shared tasks such as RTE tasks of PASCAL (, SemEval-2012 Cross-lingual Textual Entailment (CLTE) (, and NTCIR RITE tasks ().", "labels": [], "entities": [{"text": "RTE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9473875761032104}, {"text": "SemEval-2012 Cross-lingual Textual Entailment (CLTE)", "start_pos": 97, "end_pos": 149, "type": "TASK", "confidence": 0.6373334186417716}, {"text": "NTCIR RITE tasks", "start_pos": 157, "end_pos": 173, "type": "TASK", "confidence": 0.5599221189816793}]}, {"text": "NTCIR-9 RITE (Shima et al., 2011) and NTCIR-10 RITE2's Exam Search tasks () required participants to find an evidence in source documents and to answer a given proposition according to yes or no.", "labels": [], "entities": [{"text": "NTCIR-9", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9565765261650085}, {"text": "RITE", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.6489753127098083}, {"text": "NTCIR-10 RITE2", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.6929094791412354}]}, {"text": "In this most realistic setting, no candidate sentence is given explicitly.", "labels": [], "entities": []}, {"text": "One can consider the following, which is converted from question (1) of an interrogative form into an affirmative form.", "labels": [], "entities": []}, {"text": "(5) It is dangerous to use an acidic cleaner with enzyme bleach.", "labels": [], "entities": []}, {"text": "Judging entailment of (5) in a given source document is equivalent to answering yes-no question (1).", "labels": [], "entities": []}, {"text": "Therefore, this style of RTEs can also be regarded as yes-no questions.", "labels": [], "entities": [{"text": "RTEs", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.9030516743659973}]}, {"text": "We describe details of our proposed method and implementation (Section 2), experiments and results (Section 3), discussion with potential future works (Section 4), and conclude the paper (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "The RITE2 Exam Search subtask was designed originally as an RTE task in which participants return true or false fora given proposition by referring to textual knowledge, such as Wikipedia and textbooks, with no candidate sentence in the knowledge source specified.", "labels": [], "entities": [{"text": "RITE2 Exam Search", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.4609934190909068}]}, {"text": "The RITE2 dataset was developed from past Japanese National Center Test questions for the University Admissions (Center Test).", "labels": [], "entities": [{"text": "RITE2 dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7409603744745255}, {"text": "National Center Test questions", "start_pos": 51, "end_pos": 81, "type": "DATASET", "confidence": 0.8325314968824387}]}, {"text": "The questions were presented originally in a multiple-choice style of questions.", "labels": [], "entities": []}, {"text": "Because each choice corresponds to  true or false, each choice can be regarded as a single yes-no question.", "labels": [], "entities": []}, {"text": "Participant systems are asked to return yes or no with a confidence value for each question.", "labels": [], "entities": []}, {"text": "The dataset consists of a development set of 528 yes-no questions and a test set of 448 yes-no questions.", "labels": [], "entities": []}, {"text": "All of our evaluation results are on the test set using the RITE2 official evaluation tool.", "labels": [], "entities": [{"text": "RITE2 official evaluation tool", "start_pos": 60, "end_pos": 90, "type": "DATASET", "confidence": 0.905550017952919}]}, {"text": "Since our system requires no machine learning, we did not use the development set.", "labels": [], "entities": []}, {"text": "We used knowledge sources of two types: high school textbooks and Wikipedia.", "labels": [], "entities": []}, {"text": "Both are written in Japanese.", "labels": [], "entities": []}, {"text": "We tried three types of snippets: section, subsection, paragraph, larger to smaller in this order.", "labels": [], "entities": []}, {"text": "Boundaries of these snippets are explicitly marked in textbooks by the textbook authors.", "labels": [], "entities": [{"text": "Boundaries", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9551887512207031}]}, {"text": "Wikipedia has its own document structures.", "labels": [], "entities": []}, {"text": "For comparison with textbooks, we regarded a Wikipedia page as a section, sections in a page as subsections, and paragraphs as paragraphs.", "labels": [], "entities": []}, {"text": "For efficiency, we used Wikipedia pages for which titles detected in the test datasets.", "labels": [], "entities": []}, {"text": "This arrangement does not affect results because our keyword extraction is performed using the very same set of Wikipedia titles.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7606699466705322}]}, {"text": "shows results of our proposed model, our baseline, and the best of RITE2 participant.", "labels": [], "entities": [{"text": "RITE2", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.57846999168396}]}, {"text": "The source row shows which knowledge source was used: either textbook or Wikipedia.", "labels": [], "entities": [{"text": "textbook", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.934139609336853}, {"text": "Wikipedia", "start_pos": 73, "end_pos": 82, "type": "DATASET", "confidence": 0.7071181535720825}]}, {"text": "The snippet row shows the snippet unit: section, subsection, or paragraph.", "labels": [], "entities": []}, {"text": "Our baseline model is equivalent to the suggested model, except for dropping the penalty term, to check the effect of the penalty term.", "labels": [], "entities": []}, {"text": "The baseline model becomes s \u00ed \u00b5\u00ed\u00b1\u00b9\u00ed \u00b5\u00ed\u00b1\u00b9 = \u2211 \u00ed \u00b5\u00ed\u00b1\u00a4\u00ed \u00b5\u00ed\u00b1\u00a4 \u00ed \u00b5\u00ed\u00b1\u0099\u00ed \u00b5\u00ed\u00b1\u0099 \u00ed \u00b5\u00ed\u00b1\u0099\u00ed \u00b5\u00ed\u00b1\u0099\u2208\u00ed \u00b5\u00ed\u00b1\u00b9\u00ed \u00b5\u00ed\u00b1\u00b9\u2229\u00ed \u00b5\u00ed\u00b1\u00b2\u00ed \u00b5\u00ed\u00b1\u00b2 . In the Macro F1 score, which was the primary metric in RITE2 balancing yes and no answers, our best system (knowledge source is textbook and snippet is paragraph) performed 5.45 points better than the best result in RITE2.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.8894151449203491}, {"text": "RITE2", "start_pos": 329, "end_pos": 334, "type": "DATASET", "confidence": 0.9553073048591614}]}, {"text": "Our best system performed 3.02 points better than our baseline, showing the effect of a penalty.", "labels": [], "entities": []}, {"text": "Among the snippet units in our suggested method, paragraph using textbook obtained the best score overall.", "labels": [], "entities": []}, {"text": "Results using textbook were better than those using Wikipedia.", "labels": [], "entities": [{"text": "textbook", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.9438071250915527}]}, {"text": "Wikipedia results do not show a clear difference irrespective of the snippet units.", "labels": [], "entities": []}, {"text": "shows a graph of the Macro F1 score with respect to the bias term b, with values of 1.0-3.8.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8716539740562439}]}, {"text": "The notation of \u221e is assigned when no weight is used.", "labels": [], "entities": []}, {"text": "Comparison of pairs of proposed and baseline for each snippet shows that the baseline is almost always lower than proposed, i.e. the penalty term is effective.", "labels": [], "entities": []}, {"text": "corresponds to a bias value of b = 3.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. NTCIR-10 RITE2 Exam Search Results", "labels": [], "entities": [{"text": "NTCIR-10 RITE2 Exam Search", "start_pos": 10, "end_pos": 36, "type": "DATASET", "confidence": 0.7439823225140572}]}]}