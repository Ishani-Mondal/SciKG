{"title": [{"text": "Neural Utterance Ranking Model for Conversational Dialogue Systems", "labels": [], "entities": [{"text": "Neural Utterance Ranking", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7794330517450968}]}], "abstractContent": [{"text": "In this study, we present our neural utterance ranking (NUR) model, an utterance selection model for conversational dialogue agents.", "labels": [], "entities": []}, {"text": "The NUR model ranks candidate utterances with respect to their suitability in relation to a given context using neural networks; in addition, a dialogue system based on the model converses with humans using highly ranked utterances.", "labels": [], "entities": []}, {"text": "Specifically, the model processes word sequences in utterances and utterance sequences in context via recurrent neural networks.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed model ranks utterances with higher precision relative to deep learning and other existing methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9974143505096436}]}, {"text": "Furthermore, we construct a conversational dialogue system based on the proposed method and conduct experiments on human subjects to evaluate performance.", "labels": [], "entities": []}, {"text": "The experimental result indicates that our system can offer a response that does not provoke a critical dialogue breakdown with a probability of 92% and a very natural response with a probability of 58%.", "labels": [], "entities": []}], "introductionContent": [{"text": "The study of conversational dialogue systems (also known as non-task-oriented or chat-oriented dialogue systems) has along history.", "labels": [], "entities": []}, {"text": "To construct such systems, rule-based methods have long been used; however, construction and maintenance costs are very high because these rules are manually created.", "labels": [], "entities": []}, {"text": "Moreover, intuition tells us that the performance of such systems depends on the number of established rules, though reports indicate that performance did not improve much even if the number of rules was doubled (), indicating that performance of rulebased systems is limited.", "labels": [], "entities": []}, {"text": "Recently, the study of statistical-based methods that use statistical processing with large volumes of web data has become increasingly active.", "labels": [], "entities": []}, {"text": "The key benefit of this approach is that manual response creation is not necessary; thus, construction and maintenance costs are low; however, since web data contains noise, this approach has the potential to output grammatically or semantically incorrect sentences.", "labels": [], "entities": [{"text": "manual response creation", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.6794383923212687}]}, {"text": "To tackle this problem, some studies extract correct sentences as utterances for dialogue systems from web data)These studies focus solely on extraction and do not indicate how replies are generated using extracted sentences.", "labels": [], "entities": []}, {"text": "In our study, we propose a neural utterance ranking (NUR) model that ranks candidate utterances by their suitability in a given context using neural networks.", "labels": [], "entities": []}, {"text": "Previously, we proposed an utterance selection model () in the framework same as that of the NUR model, which ranks utterances in order of suitability to given context.", "labels": [], "entities": [{"text": "utterance selection", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.852756917476654}]}, {"text": "In section 4, we experimentally show that the performance of the NUR model exceeds that of our previous model.", "labels": [], "entities": []}, {"text": "Our proposed method processes the word sequences in utterances and utterance sequences in context via multiple recurrent neural networks (RNNs).", "labels": [], "entities": []}, {"text": "More specifically, the RNN encodes both utterances in a given context and candidates into fixed-length vectors.", "labels": [], "entities": []}, {"text": "Such encoding enables suitable feature extraction for ranking.", "labels": [], "entities": []}, {"text": "Next, another RNN receives these utterance-encoded vectors in chronological order, and our proposed NUR model ranks candidates using the output of this RNN.", "labels": [], "entities": []}, {"text": "Our model considers the order of utterances in a given context; this architecture makes it pos-sible to handle distant semantic relationships between context and candidates.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments to verify the performance of ranking given candidate utterances and given contexts.", "labels": [], "entities": []}, {"text": "For comparison, we also tested a few baseline methods.", "labels": [], "entities": []}, {"text": "For our experiments, we used dialogue data between a conversational dialogue system and a user for both training and test data.", "labels": [], "entities": []}, {"text": "We released a conversational dialogue system called KELDIC on Twitter (screen name: @KELDIC) b . KELDIC selects an appropriate response from candidates extracted by the utterance acquisition method of (Inaba et al., 2014) using ListNet().", "labels": [], "entities": []}, {"text": "The utterance acquisition method extracted suitable sentences for system utterances related to given keywords from Twitter data by filtering inappropriate sentences.", "labels": [], "entities": [{"text": "utterance acquisition", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.867151141166687}]}, {"text": "Details of the response algorithm of KELDIC is further described in ( We collected training and test data by first collecting pairs of context and candidate utterances that the system used for reply on Twitter.", "labels": [], "entities": [{"text": "KELDIC", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.767725944519043}]}, {"text": "Next, annotators evaluated the suitability of each candidate utterance in relation to the given context.", "labels": [], "entities": []}, {"text": "Here annotators must evaluate utterances that were actually used by the system on Twitter.", "labels": [], "entities": []}, {"text": "Evaluation criterion was based on the Dialogue Breakdown Detection Challenge (DBDC) (.", "labels": [], "entities": [{"text": "Dialogue Breakdown Detection Challenge (DBDC)", "start_pos": 38, "end_pos": 83, "type": "TASK", "confidence": 0.7507306039333344}]}, {"text": "Each system's utterances were annotated using one of the following three breakdown labels: (NB) Not a breakdown It is easy to continue the conversation.", "labels": [], "entities": []}, {"text": "(PB) Possible breakdown It is difficult to continue the conversation smoothly.", "labels": [], "entities": [{"text": "PB) Possible breakdown", "start_pos": 1, "end_pos": 23, "type": "METRIC", "confidence": 0.8158893138170242}]}, {"text": "(B) Breakdown It is difficult to continue the conversation.", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9097534418106079}]}, {"text": "Annotators evaluated dialogue data on a tool we prepared.", "labels": [], "entities": []}, {"text": "They were first shown a context and 10 candidate utterances, including how KELDIC actually replied on Twitter, as well as labels for each candidate.", "labels": [], "entities": []}, {"text": "We instructed them to assign at least one NB label to given candidate utterances.", "labels": [], "entities": []}, {"text": "If there were no suitable candidates for the NB label, they could optionally add candidate utterances.", "labels": [], "entities": [{"text": "NB label", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.782829612493515}]}, {"text": "If they were still notable to find a suitable response, we allowed them to skip the evaluation.", "labels": [], "entities": []}, {"text": "We recruited annotators on crowd-sourcing site CrowdWorks c . In our evaluation, we regard candidates with 50% or more annotators decided as NB as correct utterances and others as incorrect.", "labels": [], "entities": []}, {"text": "We used 1581 data points (i.e., 1581 contexts and 17533 candidate utterances), each evaluated by three or more annotators.", "labels": [], "entities": []}, {"text": "We choose 300 data points that contained at least one correct candidate for the given test data; the remaining 1057 data points were used for training data.", "labels": [], "entities": []}, {"text": "shows statistics for our data.", "labels": [], "entities": []}, {"text": "In learning the model, we need scores for candidate utterances to define ranking.", "labels": [], "entities": []}, {"text": "Score y c i of candidate utterance ac i is calculated as follows: c https://crowdworks.jp Here, n NB , n PB and n B denote the numbers of annotators assigned as NB, PB and B, respectively, and s NB , s PB and s B denote scoring parameters of NB, PB and B, respectively.", "labels": [], "entities": []}, {"text": "In our experiments, we set (s NB , s PB , s B ) = (10.0, \u22125.0, \u221210.0).", "labels": [], "entities": []}, {"text": "In the word-embedding neural network of our NUR model, we used 1000 embedding cells, a skip-gram window size of five, and learned via 100GB of Twitter data (Other layers were learned by 1281 data points).", "labels": [], "entities": []}, {"text": "In our encoding and ranking RNNs, we used LSTM layers with 1000 hidden cells in each layer.", "labels": [], "entities": []}, {"text": "The dropout rate was set to 0.5, and the model was trained via AdaGrad ().", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9267427921295166}]}, {"text": "To validate our NUR model, we conducted experiments with the following two settings:.", "labels": [], "entities": []}, {"text": "In the previous section, since test data must contain correct candidate utterances, the ability of our NUR model in terms of actual dialogue is uncertain, thus we developed a conversational dialogue system based on our proposed method and conducted dialogue experiments with human subjects.", "labels": [], "entities": []}, {"text": "The dialogue format and rules were fully compliant with the Dialogue Breakdown Detection Challenge (DBDC) (see ().", "labels": [], "entities": [{"text": "Dialogue Breakdown Detection Challenge (DBDC)", "start_pos": 60, "end_pos": 105, "type": "TASK", "confidence": 0.7807164447648185}]}, {"text": "A dialogue is started by a system utterance, then user and the system communicate with one another.", "labels": [], "entities": []}, {"text": "When a system speaks 11 times, the dialogue is finished.", "labels": [], "entities": []}, {"text": "Therefore, a dialogue contains 11 system and 10 human utterances.", "labels": [], "entities": []}, {"text": "Our dialogue system and subjects chat on our website; we collected 120 text chat dialogues.", "labels": [], "entities": []}, {"text": "Annotators then labeled 1200 system utterances (excluding initial greetings) using breakdown labels NB, PB, and B. We again recruited subjects and annotators via CrowdWorks.", "labels": [], "entities": []}, {"text": "For comparison, we used DBDC development/test data d collected by chatting with a system based on NTT Docomo's chat API e (see . Since the DBDC system selects a suitable response from large-scale utterance data, the architecture is similar to our model and therefore suitable as a comparative system.", "labels": [], "entities": [{"text": "NTT Docomo's chat API e", "start_pos": 98, "end_pos": 121, "type": "DATASET", "confidence": 0.9039523700873057}]}, {"text": "DBDC data has been annotated by 30 annotators using the breakdown labels and we use them without any change in this experiment.", "labels": [], "entities": [{"text": "DBDC data", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.930577963590622}]}, {"text": "Therefore, the annotation rule is same but the annotators are different between our dialogue data and DBDC data.", "labels": [], "entities": [{"text": "DBDC data", "start_pos": 102, "end_pos": 111, "type": "DATASET", "confidence": 0.9434202313423157}]}], "tableCaptions": [{"text": " Table 1: Statistics of the datasets  Train Test  All  Data  1281  300  1581  Utterances in context 1.67  2.04  1.74  Candidates per data  11.12 10.94 11.09  Words per candidate 11.17 10.70 11.08  Num of Annotators  3.97  3.88  3.95", "labels": [], "entities": [{"text": "Train Test  All  Data  1281  300  1581", "start_pos": 38, "end_pos": 76, "type": "DATASET", "confidence": 0.8710373640060425}, {"text": "Utterances", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.5398998260498047}]}]}