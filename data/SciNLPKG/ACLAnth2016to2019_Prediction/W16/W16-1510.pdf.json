{"title": [{"text": "How do practitioners, PhD students and postdocs in the social sciences assess topic-specific recommendations?", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we describe a case study where researchers in the social sciences (n=19) assess topical relevance for controlled search terms, journal names and author names which have been compiled by recommender services.", "labels": [], "entities": []}, {"text": "We call these services Search Term Recommender (STR), Journal Name Recommender (JNR) and Author Name Recom-mender (ANR) in this paper.", "labels": [], "entities": []}, {"text": "The researchers in our study (practition-ers, PhD students and postdocs) were asked to assess the top n pre-processed recommendations from each recommender for specific research topics which have been named by them in an interview before the experiment.", "labels": [], "entities": []}, {"text": "Our results show clearly that the presented search term, journal name and author name recommendations are highly relevant to the researchers topic and can easily be integrated for search in Digital Libraries.", "labels": [], "entities": []}, {"text": "The average precision for top ranked recommendations is 0.749 for author names, 0.743 for search terms and 0.728 for journal names.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9989110231399536}]}, {"text": "The relevance distribution di\u21b5ers largely across topics and researcher types.", "labels": [], "entities": []}, {"text": "Practitioners seem to favor author name recommendations while postdocs have rated author name recommendations the lowest.", "labels": [], "entities": []}, {"text": "In the experiment the small postdoc group favors journal name recommendations .", "labels": [], "entities": []}], "introductionContent": [{"text": "In metadata-driven Digital Libraries (DL) typically three major information retrieval (IR) related diculties arise: (1) the vagueness between search and indexing terms, (2) the information overload by the amount of result records obtained by the information retrieval systems, and (3) the problem that pure term frequency based rankings, such as term frequency -inverse document frequency (tf-idf), provide results that often do not meet user needs.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.7411111950874328}, {"text": "term frequency -inverse document frequency (tf-idf)", "start_pos": 346, "end_pos": 397, "type": "METRIC", "confidence": 0.6877641975879669}]}, {"text": "Search term suggestion or other domain-specific recommendation modules can help users -e.g. in the social sciences and humanities -to formulate their queries by mapping the personal vocabularies of the users onto the often highly specialized vocabulary of a digital library.", "labels": [], "entities": []}, {"text": "A recent overview of recommender systems in DL can be found in.", "labels": [], "entities": []}, {"text": "This strongly suggests the introduction of models in IR systems that rely more on the real research process and have therefore a greater potential for closing the gap between information needs of scholarly users and IR systems than conventional system-oriented approaches.", "labels": [], "entities": []}, {"text": "In this paper 1 we will present an approach to utilize specific information retrieval services as enhanced search stratagems and recommendation services within a scholarly IR environment.", "labels": [], "entities": []}, {"text": "Ina typical search scenario a user first formulates his query, which can then be enriched by a Search Term Recommender that adds controlled descriptors from the corresponding document language to the query.", "labels": [], "entities": []}, {"text": "With this new query a search in a database can be triggered.", "labels": [], "entities": []}, {"text": "The search returns a result set which can be re-ranked.", "labels": [], "entities": []}, {"text": "Since search is an iterative procedure this workflow can be repeated many times till the expected result set is retrieved.", "labels": [], "entities": []}, {"text": "These iterative search steps are typically stored in a search sessions log.", "labels": [], "entities": []}, {"text": "The idea of this paper is to assess if topic-specific recommendation services which provide search related thesaurus term, journal name and author name suggestions are accepted by researchers.", "labels": [], "entities": []}, {"text": "In section 2 we will shortly introduce three di\u21b5erent recommendation services: (1) co-word analysis and the derived concept of Search Term Recommendation (STR), (2) coreness of journals and the derived Journal Name Recommender (JNR) and (3) centrality of authors and the derived Author Name Recommender (ANR).", "labels": [], "entities": []}, {"text": "The basic concepts and an evaluation of the top-ranked recommendations are presented in the following sections.", "labels": [], "entities": []}, {"text": "We can show that the proposed recommendation services can easily be implemented within scholarly DLs.", "labels": [], "entities": []}, {"text": "In the conclusion we assume that a users search should improve by using the proposed recommendation services when interacting with a scientific information system, but the relevance of these recommendations in areal interactive search task is still an open question (compare the experiences with the Okapi system).", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following section we describe the evaluation of the recorded assessments.", "labels": [], "entities": []}, {"text": "We calculated average precision AP for each recommender service.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9892798066139221}, {"text": "AP", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.69380784034729}]}, {"text": "The precision P of each service was calculated by for each topic, where r is the number of all relevant assessed recommendations and r+nr is the number of all assessed recommendations (relevant and not relevant).", "labels": [], "entities": [{"text": "precision P", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9142466187477112}]}, {"text": "We wanted to keep the assessment exercise for the researchers very short and hence we limited the list of recommendations of each service to a maximum of 5 controlled terms, journal names and author names.", "labels": [], "entities": []}, {"text": "According to this restriction we decided to calculate AP@1, AP@2, AP@4 for each service.", "labels": [], "entities": [{"text": "AP@1", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9614678621292114}, {"text": "AP@2", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9375030795733134}, {"text": "AP@4", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9512885808944702}]}, {"text": "In very rare case one recommendation service generated just one or two recommendations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Statistics of the assessment study", "labels": [], "entities": []}, {"text": " Table 2. Evaluation of the assessments. AP, AP@1, AP@2 and AP@4 for recommen- dation from STR, JNR and ANR", "labels": [], "entities": [{"text": "AP", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.981939971446991}, {"text": "ANR", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.7007100582122803}]}, {"text": " Table 3. Evaluation of di\u21b5erent researcher types. Average precision for recommenda- tion from STR, JNR and ANR", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9984884262084961}, {"text": "ANR", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.5315749049186707}]}]}