{"title": [{"text": "Comparison of Annotating Methods for Named Entity Corpora", "labels": [], "entities": [{"text": "Named Entity Corpora", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.6262180209159851}]}], "abstractContent": [{"text": "We compared two methods to annotate a corpus via non-expert annotators for named entity (NE) recognition task, which are (1) revising the results of the existing NE recognizer and (2) annotating NEs only by hand.", "labels": [], "entities": [{"text": "named entity (NE) recognition task", "start_pos": 75, "end_pos": 109, "type": "TASK", "confidence": 0.7417927384376526}]}, {"text": "We investigated the annotation time, the degrees of agreement, and the performances based on the gold standard.", "labels": [], "entities": []}, {"text": "As we have two annotators for one file of each method, we evaluated the two performances, which are the averaged performances over the two anno-tators and the performances deeming the annotations correct when either of them is correct.", "labels": [], "entities": []}, {"text": "The experiments revealed that the semi-automatic annotation was faster and showed better agreements and higher performances on average.", "labels": [], "entities": [{"text": "agreements", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9964256882667542}]}, {"text": "However they also indicated that sometimes fully manual annotation should be used for some texts whose genres are far from its training data.", "labels": [], "entities": []}, {"text": "In addition, the experiments using the annotated corpora via semi-automatic and fully manual annotation as training data for machine learning indicated that the F-measures sometimes could be better for some texts when we used manual annotation than when we used semi-automatic annotation.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 161, "end_pos": 171, "type": "METRIC", "confidence": 0.9852403402328491}]}], "introductionContent": [{"text": "The crowdsourcing made annotation of the training data cheaper and faster).", "labels": [], "entities": []}, {"text": "Snow et al. evaluated non-expert annotations but they did not discuss the difference in the annotation qualities depending on how to give them the corpus.", "labels": [], "entities": []}, {"text": "Therefore, we compared the two methods to annotate a corpus, which are semiautomatic and fully manual annotations, to examine the method to generate high quality corpora by non-experts.", "labels": [], "entities": []}, {"text": "We investigate Japanese named entity (NE) recognition task using a corpus that consists of six genres to examine the annotation qualities depending on the genres.", "labels": [], "entities": [{"text": "Japanese named entity (NE) recognition task", "start_pos": 15, "end_pos": 58, "type": "TASK", "confidence": 0.6174312941730022}]}, {"text": "The annotation of NE task is difficult for nonexperts because its definition has many rules, and some of them are complicated.", "labels": [], "entities": []}, {"text": "Therefore, the semi-automatic annotation seems a good way to decrease the annotation errors.", "labels": [], "entities": []}, {"text": "However, sometimes the existing system also can make mistakes, especially on corpora in other genres but newswires, because it is trained only from the newswire corpus.", "labels": [], "entities": []}, {"text": "Therefore, we compare the two methods to annotate a corpus, which are the semiautomatic and fully manual annotations and discuss them, from the point of view of time, agreement, and performance based on the gold standard to generate high quality corpora by non-experts.", "labels": [], "entities": []}, {"text": "We also discuss the difference in performances according to the genres of the target corpus as we used the multi-genre corpus for analysis.", "labels": [], "entities": []}, {"text": "evaluated non-expert annotations through comparing with expert annotations from the point of view of time, quality, and cost.", "labels": [], "entities": []}, {"text": "proposed agile data annotation, which is iterative, and compared it with the traditional linear annotation method.", "labels": [], "entities": []}, {"text": "van der described the method to annotate semantic roles to the French corpus using English template to investigate the cross-lingual validity.", "labels": [], "entities": []}, {"text": "compared the semi-automatic and fully manual annotations to develop the Penn Treebank on the POS tagging task and the bracketing task.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9943636655807495}, {"text": "POS tagging task", "start_pos": 93, "end_pos": 109, "type": "TASK", "confidence": 0.7235266466935476}, {"text": "bracketing task", "start_pos": 118, "end_pos": 133, "type": "TASK", "confidence": 0.918486088514328}]}, {"text": "However, as far as we know, there is no paper which compared the semi-automatic and fully manual annotations to develop high quality corpora via non-expert annotators.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used 136 texts extracted from BCCWJ, which are available as ClassA . BCCWJ consists of six genres, \"Q & A sites\" (OC), \"white papers\" (OW), \"blogs\" (OY), \"books\" (PB), \"magazines\" (PM), and \"newswires\" (PN).", "labels": [], "entities": [{"text": "BCCWJ", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.9362711906433105}]}, {"text": "shows the summary of the numbers of documents and tags of each genre.", "labels": [], "entities": []}, {"text": "Sixteen non-experts assigned the nine types of NE tag of IREX to the plain texts after reading the definitions . Every annotator annotated 34 texts, which is 17 texts via KNP+M and Manual, respectively, which makes two sets of corpus for each method.", "labels": [], "entities": [{"text": "IREX", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.633742094039917}]}, {"text": "Eight annotators began with KNP+M, and the rest began with Manual to address the bias of the proficiency.", "labels": [], "entities": [{"text": "KNP+M", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.7903852462768555}, {"text": "Manual", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9502753615379333}]}, {"text": "Annotation time is recorded for each text.", "labels": [], "entities": [{"text": "Annotation time", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.8884724676609039}]}, {"text": "We calculated the averaged annotation time for one set of corpus, i.e., 136 texts, for each method.", "labels": [], "entities": []}, {"text": "Therefore, the documents matched in size when the annotation times were compared.", "labels": [], "entities": []}, {"text": "We used the newest corpus of BCCWJ by 2016/2/11 ( as the gold standard.", "labels": [], "entities": [{"text": "BCCWJ by 2016/2/11", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.889759532042912}]}, {"text": "7.0 for windows . The performances were evaluated based on the rules defined for IREX.", "labels": [], "entities": [{"text": "IREX", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.8905882239341736}]}, {"text": "In other words, the annotations were deemed correct if and only if both the tag and its extent were correct except for the cases of the optional tags.", "labels": [], "entities": [{"text": "extent", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.968414306640625}]}, {"text": "When the optional tag was assigned to some words in the gold standard, the annotations were deemed correct if (1) the words were not annotated by any tags or (2) a word or some words in that extent were annotated by any tags including the optional tag.", "labels": [], "entities": []}, {"text": "As we have two annotators for one file of each method, we evaluated the two performances based on golden standard, which are the averaged performances over the two annotators and the performances deeming the annotations correct when either of them is correct.", "labels": [], "entities": []}, {"text": "We investigate the latter performances since we usually integrate the results of two annotators when we generate corpora.", "labels": [], "entities": []}, {"text": "In addition, we used the corpora which are annotated via Manual or KNP+M as the training data for supervised learning of NER to test the quality of the annotations for the machine learn-  Every test set of each validation includes the texts from as many genres as possible.", "labels": [], "entities": []}, {"text": "show the micro and macroaveraged observed agreement (Observed) and Kappa coefficients (Kappa) of each method of all the genres.", "labels": [], "entities": [{"text": "macroaveraged observed agreement", "start_pos": 19, "end_pos": 51, "type": "METRIC", "confidence": 0.6092363099257151}, {"text": "Observed", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.8834714293479919}, {"text": "Kappa coefficients (Kappa)", "start_pos": 67, "end_pos": 93, "type": "METRIC", "confidence": 0.9057367324829102}]}, {"text": "show the averaged precisions (P), recalls (R), and F-measures (F) of each method of all the genres.", "labels": [], "entities": [{"text": "precisions (P)", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.900912880897522}, {"text": "recalls (R)", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9207520186901093}, {"text": "F-measures (F)", "start_pos": 51, "end_pos": 65, "type": "METRIC", "confidence": 0.9649793207645416}]}, {"text": "They are average over the two annotators.", "labels": [], "entities": []}, {"text": "Tables 10 and 11 summarize those of each genre.", "labels": [], "entities": []}, {"text": "The fully automatic annotation, which is the results of original KNP without revising are also shown in these tables as KNP.", "labels": [], "entities": [{"text": "KNP", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.8591346740722656}]}, {"text": "in the tables indicates the average of KNP+M and Manual.", "labels": [], "entities": [{"text": "KNP+M", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.8089168071746826}, {"text": "Manual", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9742882251739502}]}, {"text": "The higher observed agreements, Kappa coefficients, precisions, recalls, and F-measures among the two methods are written in bold.", "labels": [], "entities": [{"text": "precisions", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.994443953037262}, {"text": "recalls", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9876331090927124}, {"text": "F-measures", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9975016713142395}]}], "tableCaptions": [{"text": " Table 3: Micro-averaged observed agreement and  Kappa coefficient of each method (All)", "labels": [], "entities": [{"text": "Micro-averaged observed agreement", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.8756739099820455}, {"text": "Kappa coefficient", "start_pos": 49, "end_pos": 66, "type": "METRIC", "confidence": 0.9820863008499146}]}, {"text": " Table 2: Summary of number of documents and tags", "labels": [], "entities": []}, {"text": " Table 4: Macro-averaged observed agreement and  Kappa coefficient of each method (All)", "labels": [], "entities": [{"text": "Macro-averaged observed agreement", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.469006876150767}, {"text": "Kappa coefficient", "start_pos": 49, "end_pos": 66, "type": "METRIC", "confidence": 0.9755328595638275}]}, {"text": " Table 5: Micro-averaged observed agreement and  Kappa coefficient of each method", "labels": [], "entities": [{"text": "Micro-averaged observed agreement", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.8736345171928406}, {"text": "Kappa coefficient", "start_pos": 49, "end_pos": 66, "type": "METRIC", "confidence": 0.9847226440906525}]}, {"text": " Table 8: Micro-averaged precision, recall, and F- measure of each method (All)", "labels": [], "entities": [{"text": "Micro-averaged", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9572441577911377}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9225500226020813}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9995860457420349}, {"text": "F- measure", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9942789475123087}]}, {"text": " Table 9: Macro-averaged precision, recall, and F- measure of each method (All)", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9825043082237244}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9996141195297241}, {"text": "F- measure", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.994951089223226}]}, {"text": " Table 11: Macro-averaged precision, recall, and  F-measure of each method", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9784155488014221}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9996201992034912}, {"text": "F-measure", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9995026588439941}]}, {"text": " Table 12: Micro-averaged precision, recall, and F- measure of each method (All) deeming the annota- tions correct when either of two annotators is cor- rect", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9290024638175964}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.999574601650238}, {"text": "F- measure", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9953060746192932}]}, {"text": " Table 13: Macro-averaged precision, recall, and  F-measure of each method (All) deeming the an- notations correct when either of two annotators is  correct", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9657161235809326}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9995027780532837}, {"text": "F-measure", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9991756081581116}]}, {"text": " Table 14: Micro-averaged precision, recall, and F- measure of each method deeming the annotations  correct when either of two annotators is correct", "labels": [], "entities": [{"text": "Micro-averaged", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.9332001209259033}, {"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9040858149528503}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9995803236961365}, {"text": "F- measure", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9944132169087728}]}, {"text": " Table 15: Macro-averaged precision, recall, and F- measure of each method deeming the annotations  correct when either of two annotators is correct", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9606987237930298}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9994696974754333}, {"text": "F- measure", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9954355557759603}]}, {"text": " Table 16: Micro-averaged precision, recall, and F- measure of each method (All) when the annotated  data were used for training", "labels": [], "entities": [{"text": "Micro-averaged", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.9530519843101501}, {"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9149881601333618}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9994983673095703}, {"text": "F- measure", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9939112067222595}]}, {"text": " Table 17: Macro-averaged precision, recall, and F- measure of each method (All) when the annotated  data were used for training", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9740671515464783}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9994692206382751}, {"text": "F- measure", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9945751031239828}]}, {"text": " Table 18: Micro-averaged precision, recall, and F- measure of each method when the annotated data  were used for training", "labels": [], "entities": [{"text": "Micro-averaged", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.9528729915618896}, {"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9270510673522949}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9994633793830872}, {"text": "F- measure", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9922810991605123}]}, {"text": " Table 19: Macro-averaged precision, recall, and F- measure of each method when the annotated data  were used for training", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9769847393035889}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9994213581085205}, {"text": "F- measure", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9938410520553589}]}]}