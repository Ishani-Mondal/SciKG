{"title": [{"text": "Detecting Grammatical Errors in Machine Translation Output Using Dependency Parsing and Treebank Querying", "labels": [], "entities": [{"text": "Detecting Grammatical Errors in Machine Translation Output", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.7917535347597939}]}], "abstractContent": [{"text": "Despite the recent advances in the field of machine translation (MT), MT systems cannot guarantee that the sentences they produce will be fluent and coherent in both syntax and semantics.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.867400336265564}, {"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9630458354949951}]}, {"text": "Detecting and highlighting errors in machine-translated sentences can help post-editors to focus on the erroneous fragments that need to be corrected.", "labels": [], "entities": []}, {"text": "This paper presents two methods for detecting grammatical errors in Dutch machine-translated text, using dependency parsing and treebank querying.", "labels": [], "entities": [{"text": "detecting grammatical errors in Dutch machine-translated text", "start_pos": 36, "end_pos": 97, "type": "TASK", "confidence": 0.8503962159156799}, {"text": "dependency parsing", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7623911499977112}]}, {"text": "We test our approach on the output of a statistical and a rule-based MT system for English-Dutch and evaluate the performance on sentence and word-level.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9515169858932495}]}, {"text": "The results show that our method can be used to detect grammatical errors with high accuracy on sentence-level in both types of MT output.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9980173110961914}, {"text": "MT output", "start_pos": 128, "end_pos": 137, "type": "TASK", "confidence": 0.9271197021007538}]}], "introductionContent": [{"text": "Despite the continuous progress that has been made over the last decades, Machine Translation (MT) systems are far from perfect.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.8641629099845887}]}, {"text": "Moreover, MT quality not only varies between different domains or language pairs but also from sentence to sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9929872155189514}]}, {"text": "Even though it has been shown that using MT leads to productivity gains in computer-assisted translation (CAT) workflows), to produce high-quality translations, humans still need to intervene in the translation process and do this usually by post-editing (correcting) the MT output.", "labels": [], "entities": [{"text": "computer-assisted translation (CAT) workflows", "start_pos": 75, "end_pos": 120, "type": "TASK", "confidence": 0.840809961160024}]}, {"text": "Post-editing MT output requires post-editors to detect translation errors prior to correcting them.", "labels": [], "entities": [{"text": "MT output", "start_pos": 13, "end_pos": 22, "type": "TASK", "confidence": 0.8919060528278351}]}, {"text": "Hence, automatic quality estimation (QE) systems not only aim to estimate the post-editing effort at segment level to filter low quality translations (), but also to detect the location and the nature of errors at word level ().", "labels": [], "entities": [{"text": "automatic quality estimation (QE)", "start_pos": 7, "end_pos": 40, "type": "TASK", "confidence": 0.5881386895974478}]}, {"text": "MT errors can be analysed as adequacy and fluency errors.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9637088775634766}]}, {"text": "While adequacy is concerned with how much of the source content and meaning is also expressed in the target text, fluency is concerned with to what extent the translation is well formed and adheres to the norms of the target language.", "labels": [], "entities": []}, {"text": "The distinction between adequacy and fluency has been used in different translation error taxonomies (.", "labels": [], "entities": []}, {"text": "Besides the difficulties of transferring source content and meaning to a target sentence, the task of producing grammatically correct sentences remains to be challenging for MT systems, independent of the domain of text to be translated and the type of MT system (.", "labels": [], "entities": [{"text": "MT", "start_pos": 174, "end_pos": 176, "type": "TASK", "confidence": 0.9884717464447021}]}, {"text": "This motivates us to examine the use of dependency structures, which represent the abstract grammatical relations that hold between constituents, for detecting grammatical errors in machine-translated text.", "labels": [], "entities": []}, {"text": "A dependency tree is a rooted, directed acyclic graph, which represents all words in a sentence as nodes and grammatical relations between the words as edges.", "labels": [], "entities": []}, {"text": "A labelled dependency tree, additionally, incorporates the nature of the grammatical relationships between the words as annotations of relation names on the edges of the tree.", "labels": [], "entities": []}, {"text": "Dependency trees are interesting for the QE task due to the fact that the dependents may span discontinuous parts of the input sentence and are suited for representing languages with word order variations and discontinuous constituencies such as Dutch.", "labels": [], "entities": [{"text": "QE task", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9007123708724976}]}, {"text": "In this paper, we present two new approaches for QE on sub-segment level, and more specifically for detecting grammatical errors in Dutch MT output.", "labels": [], "entities": [{"text": "QE", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9741581082344055}, {"text": "detecting grammatical errors in Dutch MT output", "start_pos": 100, "end_pos": 147, "type": "TASK", "confidence": 0.6640244168894631}]}, {"text": "In the first approach, we use the partial dependency parses as an indicator of problematic text fragments when no parse covers the complete input.", "labels": [], "entities": []}, {"text": "In the second approach, we query the sub-trees of the dependency tree of an MT sentence against a treebank of correct Dutch sentences by using dependency relation and syntactic category information on phrase and lexical level.", "labels": [], "entities": [{"text": "MT sentence", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.8740773797035217}]}, {"text": "The number of matching constructions is then considered to bean indicator of possible translation errors fora given sub-tree.", "labels": [], "entities": []}, {"text": "In addition to using these two approaches separately, we combine them together and evaluate the three approaches on the output of an English-to-Dutch statistical and rule-based MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 177, "end_pos": 179, "type": "TASK", "confidence": 0.9355758428573608}]}, {"text": "We evaluate the performance on sentence and word-level by comparing the detected errors to manually annotated errors.", "labels": [], "entities": []}, {"text": "The remainder of this work is as follows: In Section 2, we describe related research.", "labels": [], "entities": []}, {"text": "In Section 3, our approach is outlined in detail and in Section 4, we describe the data sets we used.", "labels": [], "entities": []}, {"text": "In Section 5, we give the results of our experiments.", "labels": [], "entities": []}, {"text": "Finally, in Section 6, we conclude by discussing the results and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using the error detection approaches described in Section 3, we built three different MT error detection systems: a system which uses the partial parses obtained from the Alpino parser (P), a system which uses the matches obtained from the treebank for each sub-tree being queried against (X) and a system which combines the output from the first two systems (P+X).", "labels": [], "entities": [{"text": "error detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6949673444032669}, {"text": "MT error detection", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.9145278135935465}]}, {"text": "We evaluate the output of these three systems both on sentence and word level.", "labels": [], "entities": []}, {"text": "The sentence-level evaluation is used to assess whether the QE systems can detect sentences containing errors, whereas the word-level evaluation is used to assess whether the systems can locate the errors in the sentence.", "labels": [], "entities": []}, {"text": "As the sub-tree extraction method we propose in Section 3.2 does not impose any constraints on the maximum number of child nodes a sub-tree can contain, the X system is subject to data sparsity especially if the queried sub-trees contain a high number of child nodes.", "labels": [], "entities": [{"text": "sub-tree extraction", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7236577421426773}]}, {"text": "This problem is clearly visible in the distribution of sub-trees with different number of child nodes (N) overall the trees in the treebank, in.", "labels": [], "entities": []}, {"text": "As it can be seen from, 96% of all the sub-trees that occur in the treebank contain five of less child nodes.", "labels": [], "entities": []}, {"text": "We can therefore expect the X system to erroneously flag errors in sub-trees consisting of a higher number of child nodes even though they do not contain grammatical errors, but due to the fact that such sub-trees never occur in the treebank.", "labels": [], "entities": []}, {"text": "The first evaluation we make therefore aims to measure the error detection performance of different versions of the X system that query only the sub-trees that consist of equal or less child nodes than the given threshold MAXN.", "labels": [], "entities": []}, {"text": "The precision, recall and F1 scores for each X system are provided in, for the SMT and the RBMT output.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.999646782875061}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9988490343093872}, {"text": "F1 scores", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9764160513877869}, {"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.7394604086875916}, {"text": "RBMT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.4806859493255615}]}, {"text": "As MAXN values can be combined with different threshold values T, various versions of the X system can be built.", "labels": [], "entities": []}, {"text": "To have a better understanding of the impact of different T values on the error detection performance, we choose an X system with high precision (MAXN=3) on both types of MT output and refer to this as the X system in the remainder of this study.", "labels": [], "entities": [{"text": "error detection", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.6967533677816391}, {"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9769533276557922}, {"text": "MAXN", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.8630015254020691}, {"text": "MT", "start_pos": 171, "end_pos": 173, "type": "TASK", "confidence": 0.9578104019165039}]}, {"text": "A simple analysis of the annotations obtained by this system shows us that 10% and 11% of the annotations (SMT and RBMT respectively) marked non-adjacent words and these annotations were able to capture agreement errors such as the determiner-noun agreement problems as in \"onze medisch project (EN: our medical project)\", which should be rephrased as \"ons medisch project\".", "labels": [], "entities": [{"text": "RBMT", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9502566456794739}]}, {"text": "Next, we evaluate the performance of the three types of error detection systems (P, X and P+X) on detecting grammatical errors at the sentence level, and compare the results for the SMT and the RBMT output in.", "labels": [], "entities": [{"text": "error detection", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7309674620628357}, {"text": "SMT", "start_pos": 182, "end_pos": 185, "type": "TASK", "confidence": 0.8878570795059204}]}, {"text": "For the X system, we include the results obtained from the different versions, using different T values as threshold..", "labels": [], "entities": []}, {"text": "Sentence-level evaluation of the two systems P and X on SMT (above) and RBMT (below) output: Precision, Recall and F1 scores for error detection performance on grammar errors We can make a number of observations from.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.7277366518974304}, {"text": "RBMT", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.5108172297477722}, {"text": "Precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9974790215492249}, {"text": "Recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9766460061073303}, {"text": "F1", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.9920917749404907}]}, {"text": "First of all, the P system performs better overall on detecting grammatical errors in the SMT output compared to the RBMT output.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.944564700126648}]}, {"text": "While the X(T=1) system shows a higher precision for the SMT system, it shows higher recall for the RBMT system.", "labels": [], "entities": [{"text": "X(T=1)", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8255755305290222}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9987216591835022}, {"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9788811206817627}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9995711445808411}]}, {"text": "When we evaluate the X system using increasing T values, we see a similar trend for both types of MT output, namely minor losses in precision and major gains on recall, which leads to increased F1 scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9778757095336914}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9993932247161865}, {"text": "recall", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9993921518325806}, {"text": "F1 scores", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.9848610460758209}]}, {"text": "However, with increasing T values, the system potentially annotates more words per sentence, which should betaken into account for error detection on word-level.", "labels": [], "entities": []}, {"text": "We discuss the impact of high T values on the performance of word-level error detection in Section 5.2.", "labels": [], "entities": [{"text": "word-level error detection", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.7223419348398844}]}, {"text": "If we compare, we can see that the third error detection system (P+X) has a higher recall for both types of MT output and for all values of T (for the X system), which is an indication that the two types of error detection systems detect different grammar errors.", "labels": [], "entities": [{"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9986037611961365}]}, {"text": "In, we see that for both types of MT output, the increasing T values brings minor losses on precision and major gains on recall, similar to our observations from.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9894589185714722}, {"text": "T", "start_pos": 60, "end_pos": 61, "type": "METRIC", "confidence": 0.9775322675704956}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9992789626121521}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9994096755981445}]}, {"text": "A final comparison on sentence level performance can be made with a trivial baseline system, which would mark all sentences (698) as erroneous.", "labels": [], "entities": []}, {"text": "Based on the data statistics provided in, this baseline would score 0,66 and 0,63 on precision with respect to the evaluations made on the SMT and the RBMT output and score 1 on recall for both systems, yielding F1 scores of 79,5 on the SMT output and 77 on the RBMT output.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9994204044342041}, {"text": "RBMT output", "start_pos": 151, "end_pos": 162, "type": "DATASET", "confidence": 0.7563863098621368}, {"text": "recall", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9993224143981934}, {"text": "F1", "start_pos": 212, "end_pos": 214, "type": "METRIC", "confidence": 0.9987145662307739}, {"text": "RBMT output", "start_pos": 262, "end_pos": 273, "type": "DATASET", "confidence": 0.8366906940937042}]}, {"text": "Even though these results would be comparable to the F1 scores we observe in, the strength of the error detection systems being evaluated in this study is the high precision scores they achieve.", "labels": [], "entities": [{"text": "F1", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.998738706111908}, {"text": "error detection", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.6559390127658844}, {"text": "precision", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9978177547454834}]}, {"text": "We additionally evaluate the two error detection systems P and X on word level.", "labels": [], "entities": []}, {"text": "In this evaluation, each word is considered either erroneous or not and this binary distinction is used to evaluate the error detection performances of the two systems.", "labels": [], "entities": []}, {"text": "Since choosing different values of n has an impact on the number of errors being marked by the P system, in this evaluation, we include different versions of this system, which are based on different n values.", "labels": [], "entities": []}, {"text": "The evaluation results for detecting errors on word-level are provided in, per system.", "labels": [], "entities": []}, {"text": "In, we see relatively low precision, recall and F1 scores for all systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9996240139007568}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9997066855430603}, {"text": "F1 scores", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9827972054481506}]}, {"text": "Even though it is difficult to compare, given that we only target grammar errors in this study, similar F1 results have been observed in the WMT 2015 word-level QE task () ranging between 16 and 43.", "labels": [], "entities": [{"text": "F1", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9937344193458557}, {"text": "WMT 2015 word-level QE task", "start_pos": 141, "end_pos": 168, "type": "DATASET", "confidence": 0.7617777228355408}]}, {"text": "It seems that even though these systems perform well on sentence-level error detection, they are notable to locate the errors within the MT output with high accuracy.", "labels": [], "entities": [{"text": "sentence-level error detection", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.6500002046426138}, {"text": "MT", "start_pos": 137, "end_pos": 139, "type": "TASK", "confidence": 0.9431159496307373}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9901116490364075}]}, {"text": "So, word-level QE seems to be a more challenging task.", "labels": [], "entities": [{"text": "word-level QE", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.5808660387992859}]}, {"text": "One reason for the poorer performance of these systems on word-level error detection can be due to the parsing issues that surface in other parts of the MT output and not on the sub-trees which contain the erroneous words themselves.", "labels": [], "entities": [{"text": "word-level error detection", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.6719346642494202}]}, {"text": "Figure 9 additionally shows us that the P system performs better on SMT output (for all performance measures and for all values of n).", "labels": [], "entities": [{"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9896383285522461}]}, {"text": "We can also see that the performance of both systems improves, with respect to recall and F1 measures, with increasing n and T values.", "labels": [], "entities": [{"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9993951320648193}, {"text": "F1", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9969955682754517}, {"text": "T", "start_pos": 125, "end_pos": 126, "type": "METRIC", "confidence": 0.9684199094772339}]}, {"text": "However, this increase affects the number of words being marked as error as well as the precision and recall scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.999677300453186}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9990214109420776}]}, {"text": "In, we compare the number of words marked as erroneous between the output of the two error detection systems, P and X, and the reference error annotations in the evaluation set.", "labels": [], "entities": []}, {"text": "From, we can see that except for the P(n=1) system, all versions of both error detection systems mark more words on average than the number of words being annotated as errors in the reference error annotation set.", "labels": [], "entities": [{"text": "error detection", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.6904827803373337}]}, {"text": "The optimal values of n and T would therefore depend on the goal of such an error detection system.", "labels": [], "entities": [{"text": "error detection", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.7024659067392349}]}, {"text": "If the systems are used for highlighting errors in the MT output, higher values of n and T can highlight too many words.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9436014294624329}]}, {"text": "On the other hand, if the goal is to extract features for predicting postediting speed, lower values of these parameters could restrict the amount of useful information.", "labels": [], "entities": [{"text": "predicting postediting speed", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.7715030113855997}]}, {"text": "Depending on the scenario and the optimal values of n and T, these two approaches can be combined (as P+X) in many different ways.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Number of segments containing errors (#segments) and the number of error annotations  (#annotations) in the SCATE corpus of MT errors, per error category, per data set. The same  information is additionally provided for the merged annotation set of grammar and multiple  errors.", "labels": [], "entities": [{"text": "SCATE corpus of MT errors", "start_pos": 118, "end_pos": 143, "type": "DATASET", "confidence": 0.8083637297153473}]}, {"text": " Table 2. Number and the percentage of sub-trees in the treebank, with a specific number of child  nodes.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9722340106964111}]}, {"text": " Table 3. The average ratio of the number of errors marked erroneous by the error detection  systems to the number of words marked in the reference error annotations, per error detection  system, per MT type. This comparison is made on the sentences for which there is at least one  word being marked as an error both by the error detection systems and in the reference error  annotation set.", "labels": [], "entities": []}]}