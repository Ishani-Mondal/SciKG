{"title": [{"text": "Detecting Visually Relevant Sentences for Fine-Grained Classification", "labels": [], "entities": [{"text": "Detecting Visually Relevant Sentences", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8989708572626114}, {"text": "Classification", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.7267276644706726}]}], "abstractContent": [{"text": "Detecting discriminative semantic attributes from text which correlate with image features is one of the main challenges of zero-shot learning for fine-grained image classification.", "labels": [], "entities": [{"text": "Detecting discriminative semantic attributes from text", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8977901240189871}, {"text": "fine-grained image classification", "start_pos": 147, "end_pos": 180, "type": "TASK", "confidence": 0.6507770220438639}]}, {"text": "Particularly, using full-length encyclopedic articles as textual descriptions has had limited success, one reason being that such documents contain many non-visual or unrelated sentences.", "labels": [], "entities": []}, {"text": "We propose a method to automatically extract visually relevant sentences from Wikipedia documents.", "labels": [], "entities": []}, {"text": "Our model, based on a convolutional neural network, is robustly tested through ground truth labeling obtained via Amazon Mechanical Turk, achieving 81.73% F1 measure.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 114, "end_pos": 136, "type": "DATASET", "confidence": 0.9197566707928976}, {"text": "F1 measure", "start_pos": 155, "end_pos": 165, "type": "METRIC", "confidence": 0.976464033126831}]}], "introductionContent": [{"text": "Current research in multimodal fusion and crossmodal mapping relies primarily on pre-aligned datasets of images and their short captions or tags, where the text is known to contain visually descriptive content directly related to its image).", "labels": [], "entities": [{"text": "multimodal fusion", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8024717569351196}, {"text": "crossmodal mapping", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7827032804489136}]}, {"text": "These texts are usually manually collected, and restricted in length to words, phrases, and sentences.", "labels": [], "entities": []}, {"text": "Using full-length documents such as Wikipedia articles would potentially allow automated access to already available rich descriptive content and would greatly aid the task of finegrained classification across numerous domains, many of which have rich image datasets (such as birds (, flowers), aircraft (, and dogs ().)", "labels": [], "entities": [{"text": "finegrained classification", "start_pos": 176, "end_pos": 202, "type": "TASK", "confidence": 0.7415741086006165}]}, {"text": "Unfortunately, most full-length documents contain predominantly non-visual text, making them noisy with respect to visual information and limiting the success of zero-shot learning techniques for fine-grained classification (.", "labels": [], "entities": [{"text": "fine-grained classification", "start_pos": 196, "end_pos": 223, "type": "TASK", "confidence": 0.5636684894561768}]}, {"text": "Furthermore, the visual portion of the text often describes objects outside the classifier's interest, such as the color of a bird's eggs when the task is identifying bird species (see.", "labels": [], "entities": []}, {"text": "Thus, the question we address in this paper is as follows: can we automatically identify visually descriptive sentences relevant to a particular object from documents that may contain predominantly non-visual text?", "labels": [], "entities": []}, {"text": "We refer to this type of sentence as 'visually relevant'.", "labels": [], "entities": []}, {"text": "Answering this question would allow us to automatically build aligned datasets of images with rich sentence-level descriptions, removing the necessity of manually creating aligned image-text datasets.", "labels": [], "entities": []}, {"text": "In this work, we focus on bird species, as this is one of the most well-studied and challenging finegrained classification domains, using Wikipedia articles as our text (Section 2).", "labels": [], "entities": []}, {"text": "To build our computational models, we must first define the notion of 'visually relevant' sentences.", "labels": [], "entities": []}, {"text": "We use the defini-tion of Visually Descriptive Language (VDL) introduced by, with some restrictions.", "labels": [], "entities": []}, {"text": "Like VDL, we aim to identify 'visually confirmed' rather than 'visually concrete' segments of text as our descriptions correspond to a class (the bird species) rather than a particular image.", "labels": [], "entities": []}, {"text": "For example, a sentence describing a bird's feet can be a 'visually relevant' sentence fora bird, though it would not be 'visually concrete' for an image of the bird flying with its feet hidden.", "labels": [], "entities": []}, {"text": "Unlike VDL, for the scope of this paper we are interested only in the sentences which are visually descriptive with respect to the object (i.e., bird species).", "labels": [], "entities": [{"text": "VDL", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.8270832300186157}]}, {"text": "We define such sentences as containing visually relevant language (VRL).", "labels": [], "entities": []}, {"text": "To build our training data, we make a simplifying assumption: a sentence is only considered to contain visually relevant language if it is in the 'Description' section of the article.", "labels": [], "entities": []}, {"text": "While other sections may contain visually descriptive language, we assume they describe other objects such as the eggs.", "labels": [], "entities": []}, {"text": "This simplifying assumption allows us to approach our problem as a sentence classification task (is a sentence VRL or non-VRL), and provides an automatic, though noisy, approach for labeling the training data.", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.7843565543492635}]}, {"text": "We collect a dataset of 1150 Wikipedia articles about birds to train the non-linear, non-consecutive convolution neural network architecture proposed by . The architecture of this particular CNN is well suited to model sentences in our corpus such as \"Adults have upperparts streaked with brown, grey, black and white\" as it captures nonconsecutive grams such as \"upperparts brown\", \"upperparts gray\", \"streaked white\", etc.", "labels": [], "entities": []}, {"text": "To test our model in a robust manner, we use crowdsourcing to manually annotate all sentences as either VRL or non-VRL from an unseen set of 200 Wikipedia articles (for a total of 6342 sentences) (Section 2), corresponding to the bird classes in the Caltech-UCSD Birds-200-2011 dataset.", "labels": [], "entities": [{"text": "Caltech-UCSD Birds-200-2011 dataset", "start_pos": 250, "end_pos": 285, "type": "DATASET", "confidence": 0.7389891544977824}]}, {"text": "Our experiments show that the CNN model trained on the noisy VRL dataset performs very well when tested on a human-labeled VRL dataset: 83.4% Precision, 80.13% Recall, 81.73% F1 measure (Section 4).", "labels": [], "entities": [{"text": "VRL dataset", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.8416455984115601}, {"text": "VRL dataset", "start_pos": 123, "end_pos": 134, "type": "DATASET", "confidence": 0.7280916273593903}, {"text": "Precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9960764050483704}, {"text": "Recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.996131420135498}, {"text": "F1 measure", "start_pos": 175, "end_pos": 185, "type": "METRIC", "confidence": 0.9672084748744965}]}, {"text": "Our analysis highlights several findings: 1) VRL sentences outside of the description section, or in documents with no Description section, are properly labeled by the model as VRL; 2) non-VRL sentences within the Description sec-.", "labels": [], "entities": []}, {"text": "This dataset will be useful to advance research on fine-grained classification, given that the Caltech-UCSD Birds-200-2011 is one of the most highly used datasets for this task.", "labels": [], "entities": [{"text": "fine-grained classification", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7059524059295654}, {"text": "Caltech-UCSD Birds-200-2011", "start_pos": 95, "end_pos": 122, "type": "DATASET", "confidence": 0.7484170794487}]}], "datasetContent": [{"text": "To train our models we collected a set of 1150 Wikipedia articles of bird species.", "labels": [], "entities": []}, {"text": "As a future goal of this work is to correlate the extracted textual information with image data, the training documents were specifically chosen not to correspond to the 200 birds species in the Caltech-UCSD Birds-200-11 dataset, which were set aside as test data.", "labels": [], "entities": [{"text": "Caltech-UCSD Birds-200-11 dataset", "start_pos": 195, "end_pos": 228, "type": "DATASET", "confidence": 0.7505990068117777}]}, {"text": "Of these 1150 documents, 690 of them contained sections labeled \"Description\" or related headings such as \"Appearance\", which allowed us to build our training and development sets.", "labels": [], "entities": []}, {"text": "All sentences in the sections labeled \"Description\", \"Appearance\" and \"Identification\" were considered instances of the VRL class and everything else as instances of the non-VRL class; this labeling scheme we refer to as 'noisy'.", "labels": [], "entities": []}, {"text": "shows the statistics of the number of training and development instances used to build the computational models.", "labels": [], "entities": []}, {"text": "The dataset is highly unbalanced: VRL sentences comprise 19% of both training and development.", "labels": [], "entities": []}, {"text": "This skew is typical of many descriptive documents, and as such provides an appropriate model to train on.", "labels": [], "entities": []}, {"text": "To test our models we use the Wikipedia articles of the 200 birds in the Caltech-UCSD Birds-200-11 previously collected by, consisting of 6342 sentences, which we call 200 V RL . To see whether our computational models trained on the noisy VRL dataset are able to detect VRL sentences as judged by humans, we conducted a crowdsourcing experiment.", "labels": [], "entities": [{"text": "VRL dataset", "start_pos": 240, "end_pos": 251, "type": "DATASET", "confidence": 0.756525993347168}]}, {"text": "We first evaluate the CNN Lei model on the 200 HumV RL dataset described in Section 2, which contains the 6342 sentences labeled by Turkers (class distribution: 1248 sentences in class 1 and 5094 sentences in class 0).", "labels": [], "entities": [{"text": "HumV RL dataset", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.8367208441098531}]}, {"text": "Since our computational model was trained on the noisy visually relevant sentences (where the labels were determined by the 'Description' section of the documents), we wanted to evaluate how the model performed on a similarly constructed test set.", "labels": [], "entities": []}, {"text": "Thus, instead of considering the human labels for the 6342 sentences, a sentence was assigned to class 1 if it belonged to the Description, Appearance or Identification sections and to class 0 otherwise.", "labels": [], "entities": []}, {"text": "We call this dataset 200 N oisyV RL (class distribution: 1258 sentences in class 1 and 5084 sentences in class 0).", "labels": [], "entities": [{"text": "oisyV RL", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9382464289665222}]}, {"text": "Note that while it seems as if only 10 sentences changed, many of the sentences in the 'Description' sections were labeled by humans as class 0, and many sentences outside these sections labeled as class 1.", "labels": [], "entities": []}, {"text": "However, one possible issue with the 200 N oisyV RL dataset is that some documents do not contain any descriptiontype sections and thus all sentences are labeled 0, which might affect measuring the performance of the model.", "labels": [], "entities": [{"text": "oisyV RL dataset", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.5588054656982422}]}, {"text": "Thus, we considered additional test sets containing only the documents that had sections labeled with 'Description', 'Appearance' or 'Identification' (142 documents out of the original 200 documents).", "labels": [], "entities": []}, {"text": "Using these documents, we constructed a dataset 142 N oisyV RL , where class 1 contained sentences that were part of the three description-type sections, and class 0 contained all other sentences (class distribution: 1156 class 1 and 3836 class 0).", "labels": [], "entities": [{"text": "oisyV RL", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.8455705940723419}]}, {"text": "In addition, we also used the Turkers' labels (majority voting) for the corresponding sentences in these 142 documents.", "labels": [], "entities": []}, {"text": "We call this dataset 142 HumV RL (class distribution: 992 class 1 and 4000 class 0).", "labels": [], "entities": [{"text": "HumV RL", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.6395271122455597}]}, {"text": "Since the CNN model was trained on the noisy labeling, a reasonable assumption is that the classification results would be better on the 200 N oisyV RL and 142 N oisyV RL datasets than on the 200 HumV RL and 142 HumV RL datasets.", "labels": [], "entities": [{"text": "HumV RL and 142 HumV RL datasets", "start_pos": 196, "end_pos": 228, "type": "DATASET", "confidence": 0.6594065725803375}]}, {"text": "As baseline, we used the same neural bag-of-words model (nBoW) as . We use the same training and development sets as for the CNN model, along with the same word embeddings.", "labels": [], "entities": []}, {"text": "shows the results of the CNN Lei model and the nBoW model on the four datasets.", "labels": [], "entities": [{"text": "CNN Lei model", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9057734211285909}]}, {"text": "The CNN model performs slightly better than the baseline on all datasets in terms of F1 measure, with a much better Recall but worse Precision.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9888134300708771}, {"text": "Recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9995139837265015}, {"text": "Precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9989978671073914}]}, {"text": "Given that the end goal is to use the extracted visually relevant sentences together with images for fine-grained classification, and that the amount of visually relevant sentences in a document is small with respect to the document length, having high Recall is important.", "labels": [], "entities": [{"text": "fine-grained classification", "start_pos": 101, "end_pos": 128, "type": "TASK", "confidence": 0.5936989784240723}, {"text": "Recall", "start_pos": 253, "end_pos": 259, "type": "METRIC", "confidence": 0.9964227080345154}]}, {"text": "One of the most interesting findings of this study is that both of the computational models perform much better on the human-labeled visually relevant datasets (200 HumV RL , 142 HumV RL ) than on the noisy visually relevant datasets (200 N oisyV RL , 142 N oisyV RL ).", "labels": [], "entities": []}, {"text": "In particular, the recall increases significantly (e.g., from 63.24% on 142 N oisyV RL to 80.15% on 142 HumV RL using the CNN Lei model).", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.99915611743927}, {"text": "CNN Lei model", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.8911516269048055}]}, {"text": "An error analysis highlights that the computational models are more 'conservative' with the classification of VRL than the noisy labeling.", "labels": [], "entities": [{"text": "VRL", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.7418095469474792}]}, {"text": "As mentioned earlier, the Description sections of the Wikipedia articles often (though not always) contain details pertaining to the birds' song.", "labels": [], "entities": []}, {"text": "However, despite being trained on such a labeling, the computational models do not classify most sentences related primarily to the description of birds' song as VRL.", "labels": [], "entities": [{"text": "VRL", "start_pos": 162, "end_pos": 165, "type": "DATASET", "confidence": 0.7186679244041443}]}, {"text": "This result was most likely aided by the fact that some of the training documents contain: Classification results on the four datasets song descriptions outside of the description-type sections, so the words pertaining to sound were not correlated as strongly with the VRL class.", "labels": [], "entities": []}, {"text": "It is also possible that the abundance of appearance descriptions in each description section would encourage the visual words to have a much stronger effect on the 'visualness' of a sentence.", "labels": [], "entities": []}, {"text": "One such example is the sentence \"The song is a series of musical notes which sound like: wheeta wheeta whee-tee-oh, for which a common pneumonic is 'The red, the red T-shirt'\".", "labels": [], "entities": []}, {"text": "Even the repetition of the word 'red' is not enough to make the classifier label the sentence as VRL.", "labels": [], "entities": [{"text": "VRL", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.7417929172515869}]}, {"text": "Another type of example that explains these results are sentences that describe the weight of the birds, such as \"Recorded weights range from 0.69 to 2 kg,[...]\" These sentences were part of the Description section, but were not marked as VRL by either the Turkers or the computational models.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 257, "end_pos": 264, "type": "DATASET", "confidence": 0.9435571432113647}]}, {"text": "We also analyzed some of the false positives of the CNN Lei model on the 142 HumV RL and 200 HumV RL datasets.", "labels": [], "entities": [{"text": "CNN Lei model", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9276494979858398}, {"text": "HumV RL and 200 HumV RL datasets", "start_pos": 77, "end_pos": 109, "type": "DATASET", "confidence": 0.7428740220410484}]}, {"text": "One type of error comes from sentences that are visually descriptive, but not visually relevant, such as sentences that describe other objects like eggs.", "labels": [], "entities": []}, {"text": "For example, the sentence \"The egg shells are of various shades of light or bluish grey with irregular, dark brown spots or greyish-brown splotches\" was labeled as VRL by the model but not by the Turkers.", "labels": [], "entities": [{"text": "VRL", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.7639651894569397}, {"text": "Turkers", "start_pos": 196, "end_pos": 203, "type": "DATASET", "confidence": 0.9690415263175964}]}, {"text": "More interesting are the false positives that contain comparison words such as \"clapping or clicking has been observed more often in females than in males\", and words having to do with appearance that do not specifically describe how the bird looks such as \"this bird is more often seen than heard\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the Training and Dev. Sets", "labels": [], "entities": [{"text": "Dev", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.8269514441490173}]}]}