{"title": [{"text": "Wisdom of Students: A Consistent Automatic Short Answer Grading Technique", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic short answer grading (ASAG) techniques are designed to automatically assess short answers written in natural language having a length of a few words to a few sentences.", "labels": [], "entities": [{"text": "Automatic short answer grading (ASAG)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7198557938848223}]}, {"text": "In this paper, we report an intriguing finding that the set of short answers to a question, collectively, share significant lexical commonalities.", "labels": [], "entities": []}, {"text": "Based on this finding, we propose an unsuper-vised ASAG technique that only requires sequential pattern mining in the first step and an intuitive scoring process in the second step.", "labels": [], "entities": [{"text": "ASAG", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.9299492239952087}, {"text": "pattern mining", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.6908834427595139}]}, {"text": "We demonstrate, using multiple datasets, that the proposed technique effectively exploits wisdom of students to deliver comparable or better performance than prior ASAG techniques as well as distributional semantics-based approaches that require heavy training with a large corpus.", "labels": [], "entities": [{"text": "ASAG", "start_pos": 164, "end_pos": 168, "type": "TASK", "confidence": 0.9652343988418579}]}, {"text": "Moreover, by virtue of being independent of instructor provided model answers, our technique offers consistency by overcoming the limitation of undesired variability in performance exhibited by existing unsupervised techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic grading systems have been in practice in the educational domain for many years now, but primarily for recognition questions where students have to choose the correct answer from given options such as multiple choice questions.", "labels": [], "entities": []}, {"text": "Prior research has shown that such recognition questions are deficient as they do not capture multiple aspects of acquired knowledge such as reasoning and self-explanation (.", "labels": [], "entities": []}, {"text": "In contrast, recall questions that seek students' constructed answers in natural language have been found to be more effective in assessing their acquired knowledge.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9808468222618103}]}, {"text": "However, automating assessment of such answers is non-trivial owing to linguistic variations (a given answer could be articulated in different ways); subjective nature of assessment (multiple possible correct answers or no correct answer); lack of consistency inhuman rating (non-binary scoring on an ordinal scale within a range); etc.", "labels": [], "entities": []}, {"text": "Consequently, this has remained a repetitive and tedious job for teaching instructors and is often seen as an overhead and nonrewarding.", "labels": [], "entities": []}, {"text": "This paper is about a computational technique for automatically grading constructed student answers in natural language.", "labels": [], "entities": []}, {"text": "In particular, we are interested in short answers: a few words to a few sentences long (everything in between fillin-the-gap and essay type answers () and refer to the task as Automatic Short Answer Grading (ASAG).", "labels": [], "entities": [{"text": "Automatic Short Answer Grading (ASAG)", "start_pos": 176, "end_pos": 213, "type": "TASK", "confidence": 0.7158356819834027}]}, {"text": "An example ASAG task is shown in.", "labels": [], "entities": [{"text": "ASAG task", "start_pos": 11, "end_pos": 20, "type": "TASK", "confidence": 0.8891176581382751}]}], "datasetContent": [{"text": "Datasets: The recent survey papers referred to in Section 2 noted that rarely any ASAG work reported results on multiple (standard) datasets).", "labels": [], "entities": [{"text": "ASAG", "start_pos": 82, "end_pos": 86, "type": "TASK", "confidence": 0.935621976852417}]}, {"text": "They emphasized the need for sharing of datasets and structured evaluations on them.", "labels": [], "entities": []}, {"text": "Towards that, we evaluated the proposed technique and compared against multiple similarity-based baseline techniques on three datasets: \u2022 CSD: This is one of the earliest ASAG datasets consisting of 21 questions with 30 student answers evaluated each on a scale of 0-5 from an undergraduate computer science course.", "labels": [], "entities": []}, {"text": "Student answers were independently evaluated by two annotators and automatic techniques are measured against their average.", "labels": [], "entities": []}, {"text": "\u2022 X-CSD: This is an extended version of CSD with 81 questions by the same authors).", "labels": [], "entities": []}, {"text": "\u2022 RCD: We created anew dataset on a reading comprehension assignment for Standard-12 students in Central Board of Secondary Education (CBSE) in India.", "labels": [], "entities": []}, {"text": "The dataset contains 14 questions answered by 58 students.", "labels": [], "entities": []}, {"text": "The answers were graded by two expert human raters based on model answers, again on a scale of 0-5.", "labels": [], "entities": []}, {"text": "All datasets have less than (total number of questions \u00d7 total number of students) answers as presumably some students did not answer some questions.", "labels": [], "entities": []}, {"text": "We mark such missing entries as \"No Answer\" and corresponding groundtruth scores as zero.", "labels": [], "entities": [{"text": "No Answer\"", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.7305202484130859}]}, {"text": "Metrics: A wide variety of evaluation metrics has been used in the literature for measuring goodness of ASAG techniques.", "labels": [], "entities": [{"text": "ASAG", "start_pos": 104, "end_pos": 108, "type": "TASK", "confidence": 0.9637677073478699}]}, {"text": "We use Pearson's r in this paper as it has been one of the most popular metrics though its appropriateness have been questioned).", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 7, "end_pos": 18, "type": "METRIC", "confidence": 0.6877083579699198}]}, {"text": "For every question we compute Pearson's r between groundtruth and predicted scores and average across all questions are reported.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.9683672189712524}]}], "tableCaptions": [{"text": " Table 3: Question wise Pearson's r of the pro- posed technique against unsupervised ASAG tech- niques.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of Pearson's r of unsuper- vised ASAG techniques against the proposed tech- nique.", "labels": [], "entities": []}, {"text": " Table 5: Fluctuation in performance (minimum (Min), maximum (Max) and standard deviation (SD)) of  ASAG techniques with different model answers. The proposed technique does not exhibit any fluctua- tion.", "labels": [], "entities": [{"text": "standard deviation (SD))", "start_pos": 71, "end_pos": 95, "type": "METRIC", "confidence": 0.9790044069290161}, {"text": "ASAG", "start_pos": 100, "end_pos": 104, "type": "TASK", "confidence": 0.9257899522781372}]}]}