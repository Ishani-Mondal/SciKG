{"title": [{"text": "Using Factored Word Representation in Neural Network Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Neural network language and translation models have recently shown their great potentials in improving the performance of phrase-based machine translation.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 122, "end_pos": 154, "type": "TASK", "confidence": 0.6440886755784353}]}, {"text": "At the same time, word representations using different word factors have been translation quality and are part of many state-of-the-art machine translation systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.9682067036628723}, {"text": "machine translation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7705681025981903}]}, {"text": "used in many state-of-the-art machine translation systems, in order to support better translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.712153896689415}]}, {"text": "In this work, we combined these two ideas by investigating the combination of both techniques.", "labels": [], "entities": []}, {"text": "By representing words in neu-ral network language models using different factors, we were able to improve the models themselves as well as their impact on the overall machine translation performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.7352968752384186}]}, {"text": "This is especially helpful for morphologically rich languages due to their large vocabulary size.", "labels": [], "entities": []}, {"text": "Furthermore, it is easy to add additional knowledge, such as source side information, to the model.", "labels": [], "entities": []}, {"text": "Using this model we improved the translation quality of a state-of-the-art phrase-based machine translation system by 0.7 BLEU points.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.6413001716136932}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9992358684539795}]}, {"text": "We performed experiments on three language pairs for the news translation task of the WMT 2016 evaluation.", "labels": [], "entities": [{"text": "news translation task", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.7852344314257304}, {"text": "WMT 2016 evaluation", "start_pos": 86, "end_pos": 105, "type": "DATASET", "confidence": 0.8814327915509542}]}], "introductionContent": [{"text": "Recently, neural network models are deployed extensively for better translation quality of statistical machine translation (.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.6294813652833303}]}, {"text": "For the language model as well as for the translation model, neural network-based models showed improvements when used during decoding as well as when used in re-scoring.", "labels": [], "entities": [{"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9719396233558655}]}, {"text": "In phrase-based machine translation (PBMT), word representation using different factors are commonly used in stateof-the-art systems.", "labels": [], "entities": [{"text": "phrase-based machine translation (PBMT)", "start_pos": 3, "end_pos": 42, "type": "TASK", "confidence": 0.7929134666919708}, {"text": "word representation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7314226776361465}]}, {"text": "Using Part-of-Speech (POS) information or automatic word clusters is especially important for morphologically rich languages which often have a large vocabulary size.", "labels": [], "entities": []}, {"text": "Language models based on these factors are able to consider longer context and therefore improve the modelling of the overall structure.", "labels": [], "entities": []}, {"text": "Furthermore, the POS information can be used to improve the modelling of word agreement, which is often a difficult task when handling morphologically rich languages.", "labels": [], "entities": [{"text": "modelling of word agreement", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.6127470657229424}]}, {"text": "Until now, word factors have been used relatively limited in neural network models.", "labels": [], "entities": []}, {"text": "Automatic word classes have been used to structure the output layer () and as input in feed forward neural network language models.", "labels": [], "entities": []}, {"text": "In this work, we propose a multi-factor recurrent neural network (RNN)-based language model that is able to facilitate all available information about the word in the input as well as in the output.", "labels": [], "entities": []}, {"text": "We evaluated the technique using the surface form, POS-tag and automatic word clusters using different cluster sizes.", "labels": [], "entities": []}, {"text": "Using this model, it is also possible to integrate source side information into the model.", "labels": [], "entities": []}, {"text": "By using the model as a bilingual model, the probability of the translation can be modelled and not only the one of target sentence.", "labels": [], "entities": []}, {"text": "As for the target side, we use a factored representation for the words on the source side.", "labels": [], "entities": []}, {"text": "The remaining of the paper is structured as following: In the following section, we first review the related work.", "labels": [], "entities": []}, {"text": "Afterwards, we will shortly describe the RNN-based language model used in our experiments.", "labels": [], "entities": []}, {"text": "In Section 4, we will introduce the factored RNN-based language model.", "labels": [], "entities": []}, {"text": "In the next section, we will describe the experiments on the WMT 2016 data.", "labels": [], "entities": [{"text": "WMT 2016 data", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9609066049257914}]}, {"text": "Finally, we will end the paper with a conclusion of the work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the factored RNNLM on three different language pairs of the WMT 2016 News Translation Task.", "labels": [], "entities": [{"text": "WMT 2016 News Translation Task", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.7448245048522949}]}, {"text": "In each language pair, we created an n-best list using our phrase-based MT system and used the factored RNNLM as an additional feature in rescoring.", "labels": [], "entities": []}, {"text": "It is worth noting that the POS and word class information are already present during decoding of the baseline system by n-gram-based language models based on each of these factors.", "labels": [], "entities": [{"text": "POS", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.8864892721176147}]}, {"text": "First, we performed a detailed analysis on the English-Romanian task.", "labels": [], "entities": []}, {"text": "In addition, we used the model in a German-English and EnglishGerman translation system.", "labels": [], "entities": []}, {"text": "In all tasks, we used the model in re-scoring of a PBMT system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: English -Romanian Single Score", "labels": [], "entities": [{"text": "English -Romanian Single Score", "start_pos": 10, "end_pos": 40, "type": "DATASET", "confidence": 0.8485270619392395}]}, {"text": " Table 3: English -Romanian Language Models", "labels": [], "entities": []}, {"text": " Table 4: English -Romanian Bilingual Models", "labels": [], "entities": []}, {"text": " Table 5: English -German Single Score", "labels": [], "entities": [{"text": "English -German Single Score", "start_pos": 10, "end_pos": 38, "type": "DATASET", "confidence": 0.84180668592453}]}, {"text": " Table 6: English-German Language Model", "labels": [], "entities": []}]}