{"title": [{"text": "Strategy and Policy Learning for Non-Task-Oriented Conversational Systems", "labels": [], "entities": [{"text": "Strategy and Policy Learning", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7563556432723999}]}], "abstractContent": [{"text": "We propose a set of generic conversational strategies to handle possible system breakdowns in non-task-oriented dialog systems.", "labels": [], "entities": []}, {"text": "We also design policies to select these strategies according to dialog context.", "labels": [], "entities": []}, {"text": "We combine expert knowledge and the statistical findings derived from data in designing these policies.", "labels": [], "entities": []}, {"text": "The policy learned via reinforcement learning out-performs the random selection policy and the locally greedy policy in both simulated and real-world settings.", "labels": [], "entities": []}, {"text": "In addition, we propose three metrics for conversation quality evaluation which consider both the local and global quality of the conversation .", "labels": [], "entities": [{"text": "conversation quality evaluation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.7028957804044088}]}], "introductionContent": [{"text": "Non-task-oriented conversational systems do not have a stated goal to work towards.", "labels": [], "entities": []}, {"text": "Nevertheless, they are useful for many purposes, such as keeping elderly people company and helping second language learners improve conversation and communication skills.", "labels": [], "entities": []}, {"text": "More importantly, they can be combined with task-oriented systems to act as a transition smoother or a rapport builder for complex tasks that require user cooperation.", "labels": [], "entities": []}, {"text": "There area variety of methods to generate responses for nontask-oriented systems, such as machine translation), retrieval-based response selection (, and sequence-tosequence recurrent neural network.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7914545834064484}, {"text": "retrieval-based response selection", "start_pos": 112, "end_pos": 146, "type": "TASK", "confidence": 0.6363827387491862}]}, {"text": "However, these systems still produce utterances that are incoherent or inappropriate from time to time.", "labels": [], "entities": []}, {"text": "To tackle this problem, we propose a set of conversational strategies, such as switching topics, to avoid possible inappropriate responses (breakdowns).", "labels": [], "entities": []}, {"text": "After we have a set of strategies, which strategy to perform according to the conversational context is another critical problem to tackle.", "labels": [], "entities": []}, {"text": "Ina multi-turn conversation, the user experience will be affected if the same strategy is used repeatedly.", "labels": [], "entities": []}, {"text": "We experimented on three policies to control which strategy to use given the context: a random selection policy that randomly selects a policy regardless of the context, a locally greedy policy that focuses on local context, and a reinforcement learning policy that considers conversation quality both locally and globally.", "labels": [], "entities": []}, {"text": "The strategies and policies are applicable for non-taskoriented systems in general.", "labels": [], "entities": []}, {"text": "The strategies can prevent a possible breakdown, and the probability of possible breakdowns can be calculated using different metrics according to different systems.", "labels": [], "entities": []}, {"text": "For example, a neural network generation system ( can use the posterior probability to decide if the generated utterance is possibly causing a breakdown, thus replacing it with a designed strategy.", "labels": [], "entities": []}, {"text": "In this paper, we implemented the strategies and policies in a keyword retrievalbased non-task-oriented system.", "labels": [], "entities": []}, {"text": "We used the retrieval confidence as the criteria to decide whether a strategy needed to be triggered or not.", "labels": [], "entities": []}, {"text": "Reinforcement learning was introduced to the dialog community two decades ago ( and has mainly been used in task-oriented systems).", "labels": [], "entities": [{"text": "Reinforcement learning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.862690657377243}]}, {"text": "Researchers have proposed to design dialogue systems in the formalism of Markov decision processes (MDPs) () or partially observable Markov decision processes (POMDPs).", "labels": [], "entities": []}, {"text": "Ina stochastic environment, a dialog system's actions are system utterances, and the state is represented by the dialog history.", "labels": [], "entities": []}, {"text": "The goal is to design a dialog system that takes actions to maximize some measure of system reward, such as task completion rate or dialog length.", "labels": [], "entities": []}, {"text": "The difficulty of such modeling lies in the state representation.", "labels": [], "entities": []}, {"text": "Representing the dialog by the entire history is often neither feasible nor conceptually useful, and the so-called belief state approach is not possible, since we do not even know what features are required to represent the belief state.", "labels": [], "entities": []}, {"text": "Previous work () has largely dealt with this issue by imposing prior limitations on the features used to represent the approximate state.", "labels": [], "entities": []}, {"text": "In this paper, instead of focusing on task-oriented systems, we apply reinforcement learning to design a policy to select designed conversation strategies in a non-task-oriented dialog systems.", "labels": [], "entities": []}, {"text": "Unlike task-oriented dialog systems, non-task-oriented systems have no specific goal that guides the interaction.", "labels": [], "entities": []}, {"text": "Consequently, evaluation metrics that are traditionally used for reward design, such as task completion rate, are no longer appropriate.", "labels": [], "entities": [{"text": "reward design", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.8594793379306793}, {"text": "completion rate", "start_pos": 93, "end_pos": 108, "type": "METRIC", "confidence": 0.8566800653934479}]}, {"text": "The state design in reinforcement learning is even more difficult for nontask-oriented systems, as the same conversation would not occur more than once; one slightly different answer would lead to a completely different conversation; moreover there is no clear sense of when such a conversation is \"complete\".", "labels": [], "entities": []}, {"text": "We simplify the state design by introducing expert knowledge, such as not repeating the same strategy in a row, as well as statistics obtained from conversational data analysis.", "labels": [], "entities": []}, {"text": "We implemented and deployed a non-taskoriented dialog system driven by a statistical policy to avoid possible system breakdowns using designed general conversation strategies.", "labels": [], "entities": []}, {"text": "We evaluated the system on the Amazon Mechanical Turk platform with metrics that consider both the local and the global quality of the conversation.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk platform", "start_pos": 31, "end_pos": 62, "type": "DATASET", "confidence": 0.8877659291028976}]}, {"text": "In addition, we also published the system source code and the collected conversations 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "In the learning process of the reinforcement learning, we use a metric which is a combination of three metrics: turn-level appropriateness, conversational depth and information gain.", "labels": [], "entities": []}, {"text": "Conversational depth and information gain measure the quality of the conversation across multiple turns.", "labels": [], "entities": []}, {"text": "Since we use another chatbot as the simulator, making sure the overall conversation quality is accessed is critical.", "labels": [], "entities": []}, {"text": "All three metrics are related to each other but cover different aspects of the conversation.", "labels": [], "entities": []}, {"text": "We used a weighted score of the three metrics for the learning process, which is shown in Equation (2).", "labels": [], "entities": [{"text": "Equation", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9624834060668945}]}, {"text": "The coefficients are chosen based on empirical heuristics.", "labels": [], "entities": []}, {"text": "We built automatic predictors for turn-level appropriateness and conversation depth based on annotated data as well.", "labels": [], "entities": [{"text": "turn-level appropriateness", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.8206692039966583}, {"text": "conversation depth", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7919317185878754}]}], "tableCaptions": [{"text": " Table 2: An example conversation of TickTock in the simulated setting", "labels": [], "entities": []}, {"text": " Table 3: Appropriateness rating distribution when  the recent three utterances are positive.", "labels": [], "entities": [{"text": "Appropriateness rating distribution", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.9561048547426859}]}, {"text": " Table 6: Performance of different policies in the simulated setting", "labels": [], "entities": []}, {"text": " Table 7: Performance of different policies in the real-world setting.", "labels": [], "entities": []}]}