{"title": [{"text": "Efficient construction of metadata-enhanced web corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "Metadata extraction is known to be a problem in general-purpose Web corpora, and so is extensive crawling with little yield.", "labels": [], "entities": [{"text": "Metadata extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9084113240242004}]}, {"text": "The contributions of this paper are threefold: a method to find and download large numbers of WordPress pages; a targeted extraction of content featuring much needed metadata; and an analysis of the documents in the corpus with insights of actual blog uses.", "labels": [], "entities": []}, {"text": "The study focuses on a publishing software (WordPress), which allows for reliable extraction of structural elements such as metadata, posts, and comments.", "labels": [], "entities": []}, {"text": "The download of about 9 million documents in the course of two experiments leads after processing to 2.7 billion tokens with usable metadata.", "labels": [], "entities": []}, {"text": "This comparatively high yield is a step towards more efficiency with respect to machine power and \"Hi-Fi\" web corpora.", "labels": [], "entities": []}, {"text": "The resulting corpus complies with formal requirements on metadata-enhanced corpora and on weblogs considered as a series of dated entries.", "labels": [], "entities": []}, {"text": "However, existing typolo-gies on Web texts have to be revised in the light of this hybrid genre.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Ina previous experiment, the largest platform for WordPress-hosted websites, wordpress.com, blogs under CC license were targeted ().", "labels": [], "entities": []}, {"text": "In the summer of 2015, sitemaps were retrieved for all known home pages, which lead to the integral download of 145,507 different websites fora total number of 6,605,078 documents (390 Gb), leaving 6,095,630 files after processing (36 Gb).", "labels": [], "entities": []}, {"text": "There are 6,024,187 \"valid\" files (with usable date and content) from 141,648 websites, whose text amounts to about 2.11 billion tokens.", "labels": [], "entities": []}, {"text": "The distribution of harvested documents in the course of years is documented in table 6, there are 6,095,206 documents with at least a reliable indication of publication year, i.e. 92.3% of all documents.", "labels": [], "entities": []}, {"text": "Contrarily to dates in the literature, these results are not from reported permalinks dates from feeds, but directly from page metadata; nonetheless, there is also a fair share of implausible dates, comparable to the 3% of the TREC blog corpus).", "labels": [], "entities": [{"text": "TREC blog corpus", "start_pos": 227, "end_pos": 243, "type": "DATASET", "confidence": 0.8859954873720804}]}, {"text": "This indicates that these dates are not an extraction problem but rather a creative license on the side of the authors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of documents among plausi- ble years in the first experiment", "labels": [], "entities": [{"text": "Distribution of documents", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8685628970464071}]}, {"text": " Table 4: Most frequent tags in the first experiment", "labels": [], "entities": []}, {"text": " Table 5: Iterations and yields in the second exper- iment", "labels": [], "entities": []}, {"text": " Table 6: Distribution of documents among plausi- ble years in the second experiment", "labels": [], "entities": [{"text": "Distribution of documents", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8679364522298177}]}]}