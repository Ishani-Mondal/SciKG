{"title": [{"text": "Letter Sequence Labeling for Compound Splitting", "labels": [], "entities": [{"text": "Letter Sequence Labeling", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7598483562469482}, {"text": "Compound Splitting", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7292095124721527}]}], "abstractContent": [{"text": "For languages such as German where compounds occur frequently and are written as single tokens, a wide variety of NLP applications benefits from recognizing and splitting compounds.", "labels": [], "entities": []}, {"text": "As the traditional word frequency-based approach to compound splitting has several drawbacks, this paper introduces a letter sequence labeling approach, which can utilize rich word form features to build discriminative learning models that are optimized for splitting.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7682038843631744}, {"text": "letter sequence labeling", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.6080491642157236}]}, {"text": "Experiments show that the proposed method significantly outperforms state-of-the-art compound splitters.", "labels": [], "entities": [{"text": "compound splitters", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.7583189308643341}]}], "introductionContent": [{"text": "In many languages including German, compounds are written as single word-tokens without word delimiters separating their constituent words.", "labels": [], "entities": []}, {"text": "For example, the German term for 'place name' is Ortsname, which is formed by Ort 'place' and Name 'name' together with the linking element 's' between constituents.", "labels": [], "entities": []}, {"text": "Given the productive nature of compounding, treating each compound as a unique word would dramatically increase the vocabulary size.", "labels": [], "entities": []}, {"text": "Information about the existence of compounds and about their constituent parts is thus helpful to many NLP applications such as machine translation and term extraction.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.820121556520462}, {"text": "term extraction", "start_pos": 152, "end_pos": 167, "type": "TASK", "confidence": 0.7787124812602997}]}, {"text": "Compound splitting is the NLP task that automatically breaks compounds into their constituent words.", "labels": [], "entities": [{"text": "Compound splitting", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8579358458518982}]}, {"text": "As the inputs to compound splitters often include unknown words, which are not necessarily compounds, splitters usually also need to distinguish between compounds and non-compounds.", "labels": [], "entities": []}, {"text": "Many state-of-the-art splitters for German mainly implement variants of the following twostep frequency approach first proposed in: 1.", "labels": [], "entities": []}, {"text": "Matching the input word with known words, generating splitting hypotheses, including the non-splitting hypothesis that predicts the input word to be a non-compound.", "labels": [], "entities": []}, {"text": "2. Choosing the hypothesis with the highest geometric mean of frequencies of constituents as the best splitting.", "labels": [], "entities": []}, {"text": "If the frequency of the input word is higher than the geometric mean of all possible splittings, non-splitting is chosen.", "labels": [], "entities": []}, {"text": "The frequency approach is simple and efficient.", "labels": [], "entities": []}, {"text": "However, frequency criteria are not necessarily optimal for identifying the best splitting decisions.", "labels": [], "entities": []}, {"text": "In practice, this often leads to splitting compounds at wrong positions, erroneously splitting non-compounds, and incorrectly predicting frequent compounds to be non-compounds.", "labels": [], "entities": []}, {"text": "Parallel corpora) and linguistic analysis etc. were used to improve the frequency approach, but the above-mentioned issues remain.", "labels": [], "entities": []}, {"text": "Moreover, frequencies encode no information about word forms, which hinders knowledge transfer between words with similar forms.", "labels": [], "entities": []}, {"text": "In an extreme yet common case, when one or more compound constituents are unknown words, the correct splitting is not even generated in Step 1 of the frequency approach.", "labels": [], "entities": []}, {"text": "To address the above-mentioned problems, this paper proposes a letter sequence labeling (LSL) approach (Section 2) to compound splitting.", "labels": [], "entities": [{"text": "letter sequence labeling (LSL)", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.6602205882469813}, {"text": "compound splitting", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.875824362039566}]}, {"text": "We cast the compound splitting problem as a sequence labeling problem.", "labels": [], "entities": [{"text": "compound splitting problem", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.7799314260482788}]}, {"text": "To predict labels, we train conditional random fields (CRF;), which are directly optimized for splitting.", "labels": [], "entities": []}, {"text": "Our CRF models can leverage rich features of letter ngrams (Section 2.3), such as ung (a German nominalization suffix), which are shared among words and applicable to many unknown compounds and constituents.", "labels": [], "entities": []}, {"text": "Our method is language independent, although this paper focuses on German.", "labels": [], "entities": []}, {"text": "Evaluated with the compound data from GermaNet and Parra Escart\u00edn (2014), experiments in Section 3 show that our approach significantly outperforms previously developed splitters.", "labels": [], "entities": [{"text": "GermaNet", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.8836060762405396}]}, {"text": "The contributions of this paper are two-fold: \u2022 A novel letter sequence labeling approach to compound splitting \u2022 Empirical evaluations of the proposed approach and developed feature on a large compound list 2 Letter Sequence Labeling (LSL) for Compound Splitting", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.8852963745594025}, {"text": "Letter Sequence Labeling (LSL)", "start_pos": 210, "end_pos": 240, "type": "TASK", "confidence": 0.7485645910104116}, {"text": "Compound Splitting", "start_pos": 245, "end_pos": 263, "type": "TASK", "confidence": 0.7447880506515503}]}], "datasetContent": [{"text": "In our experiments, we use evaluation metrics proposed in, which are widely used in the compound splitting literature.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7687849402427673}]}, {"text": "Each compound splitting result falls into one of the following categories: correct split: words that should be split (i.e. compounds) and are correctly split; correct non-split: words that should not be split (i.e. non-compounds) and are not split; wrong non-split: words that should be split but are not split; wrong faulty split: words that should be split and are split, but at wrong position(s); wrong split: words that should not be split but are split.", "labels": [], "entities": []}, {"text": "The remaining 20% is the test set, which is put aside during training and only used for evaluation.", "labels": [], "entities": []}, {"text": "When comparing our method with frequencybased ones, it would be ideal if each method was trained and tested (on disjoint partitions of) the same benchmark data, which provides both goldstandard splitting and frequency information.", "labels": [], "entities": []}, {"text": "Unfortunately, GermaNet provides no frequency information and most large-scale word frequency lists have no gold-standard splits, which makes neither suitable benchmarks.", "labels": [], "entities": []}, {"text": "Another practical difficulty is that many splitters are not publicly available.", "labels": [], "entities": [{"text": "splitters", "start_pos": 42, "end_pos": 51, "type": "TASK", "confidence": 0.9764580726623535}]}, {"text": "We plan to complement the GN data with frequency information extracted from large corpora to construct such benchmark data in the future.", "labels": [], "entities": []}, {"text": "For the present work, we evaluate our model on the test data that other methods have been evaluated on.", "labels": [], "entities": []}, {"text": "For this purpose, we use the PE data, as two state-of-the-art splitters, namely Popovi\u00b4c and , have been evaluated on it.", "labels": [], "entities": [{"text": "PE data", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.7417324185371399}]}, {"text": "We train the best model from the last subsection using modified GN data, which has longer noncompounds up to 15 letters in length and excludes words that also appear in the PE data.", "labels": [], "entities": [{"text": "GN data", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.8756580352783203}, {"text": "PE data", "start_pos": 173, "end_pos": 180, "type": "DATASET", "confidence": 0.772313266992569}]}, {"text": "The model is evaluated on the PE data using the same metrics as described in Section 3.2, except that the evaluation is by token rather than by type, to be compatible with the original PE results.", "labels": [], "entities": [{"text": "PE data", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.8150019943714142}]}, {"text": "shows the results, which are analyzed in the remainder of this section.", "labels": [], "entities": []}, {"text": "Accuracy and precision 1 Parra Escart\u00edn (2014) evaluated Weller and Heid (2012) 'as is', using a model pre-trained on unknown data, which might have overlaps with the test data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9942166805267334}, {"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996205568313599}]}, {"text": "consider both non-compounds and compounds and are influenced by the ratio of the two, which is 8.8:1 for the PE data.", "labels": [], "entities": [{"text": "PE data", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.7103376984596252}]}, {"text": "It means that both metrics are mostly influenced by how well the systems distinguish compounds from non-compounds.", "labels": [], "entities": []}, {"text": "By contrast, recall depends solely on compounds and is thus the best indicator for splitting performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.999140739440918}, {"text": "splitting", "start_pos": 83, "end_pos": 92, "type": "TASK", "confidence": 0.9642254114151001}]}, {"text": "The recall of our model is significantly higher than that of previous methods, which shows that it generalizes well to splitting unknown compounds.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9997045397758484}]}, {"text": "The relatively low precision of our model is mainly caused by the high wrong split count.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9992170333862305}, {"text": "wrong split count", "start_pos": 71, "end_pos": 88, "type": "METRIC", "confidence": 0.8071959416071574}]}, {"text": "We found that almost half of these \"non-compounds\" that our model \"wrongly\" splits are compounds, as the PE annotation skips all adjectival and verbal compounds and also ignores certain nominal compounds.", "labels": [], "entities": []}, {"text": "The remaining of wrong split errors can be reduced by using higher quality training cases of non-compounds, as the current gold-standard non-compounds were chosen by the word length heuristic, which introduced noise in learning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of models with different context  features on GermaNet. Best results in bold face.", "labels": [], "entities": [{"text": "GermaNet", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9286015033721924}]}, {"text": " Table 3: Comparison with the state-of-the-art. Best results are marked in bold face.", "labels": [], "entities": []}]}