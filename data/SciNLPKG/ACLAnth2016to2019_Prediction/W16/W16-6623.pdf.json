{"title": [{"text": "Selecting Domain-Specific Concepts for Question Generation With Lightly-Supervised Methods", "labels": [], "entities": [{"text": "Question Generation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.92534339427948}]}], "abstractContent": [{"text": "In this paper we propose content selection methods for question generation (QG) which exploit domain knowledge.", "labels": [], "entities": [{"text": "question generation (QG)", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.8421206951141358}]}, {"text": "Traditionally, QG systems apply syntactical transformation on individual sentences to generate open domain questions.", "labels": [], "entities": []}, {"text": "We hypothesize that a QG system informed by domain knowledge can ask more important questions.", "labels": [], "entities": []}, {"text": "To this end, we propose two lightly-supervised methods to select salient target concepts for QG based on domain knowledge collected from a corpus.", "labels": [], "entities": []}, {"text": "One method selects important semantic roles with bootstrapping and the other selects important semantic relations with Open Information Extraction (OpenIE).", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of the two proposed methods on heterogeneous corpora in the business domain.", "labels": [], "entities": []}, {"text": "This work exploits domain knowledge in QG task and provides a promising paradigm to generate domain-specific questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic question generation (QG) has been successfully applied in various applications.", "labels": [], "entities": [{"text": "Automatic question generation (QG)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7715493341286978}]}, {"text": "QG was used to generate reading comprehension questions from text, to aid academic writing () and to build conversational characters (.", "labels": [], "entities": []}, {"text": "In this work, we focus on generating a set of question and answer (Q&A) pairs fora given input document.", "labels": [], "entities": []}, {"text": "Possible applications of this task are to automatically generate a Q&A section for company profiles or product descriptions.", "labels": [], "entities": []}, {"text": "It can also help the reader to recapitulate the main ideas of a document in a lively manner.", "labels": [], "entities": []}, {"text": "We can coarsely divide QG into two steps: \"what to ask\" (target concept selection and question type determination), and \"how to ask\" (question realisation).", "labels": [], "entities": [{"text": "target concept selection and question type determination", "start_pos": 57, "end_pos": 113, "type": "TASK", "confidence": 0.6326260907309396}]}, {"text": "It is important to view question generation not merely as realising a question from a declarative sentence.", "labels": [], "entities": [{"text": "question generation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7264632433652878}]}, {"text": "When the input is a document, the sentences (and candidate concepts) are of different importance.", "labels": [], "entities": []}, {"text": "It is therefore critical fora QG system to identify a set of salient concepts as target concepts before it attempts to generate questions.", "labels": [], "entities": []}, {"text": "In this work, we propose two novel target concept selection methods that lead to QG systems which can ask more important questions.", "labels": [], "entities": []}, {"text": "Our approaches are motivated by the conditions fora human reader to ask good questions.", "labels": [], "entities": []}, {"text": "In order to ask good questions, he needs to satisfy three prerequisites: 1) good command of the language, 2) good reasoning and analytical skills and 3) sufficient domain knowledge.", "labels": [], "entities": []}, {"text": "Some may argue prior knowledge is not necessary because we ask about things we do not know.", "labels": [], "entities": []}, {"text": "However, it is no surprise that a professor in computational linguistics may not ask as important and relevant questions in the field of organic chemistry as a second-year chemistry student.", "labels": [], "entities": []}, {"text": "What makes the difference is the domain knowledge.", "labels": [], "entities": []}, {"text": "Correspondingly, we hypothesize that a successful QG system needs to satisfy the following requirements: 1) able to generate questions that are grammatical and understandable by humans, 2) able to analyse the input document (e.g. keyword identification, discourse parsing or summarization), and 3) able to exploit domain knowledge.", "labels": [], "entities": [{"text": "keyword identification", "start_pos": 230, "end_pos": 252, "type": "TASK", "confidence": 0.7121584862470627}, {"text": "discourse parsing", "start_pos": 254, "end_pos": 271, "type": "TASK", "confidence": 0.6909798383712769}, {"text": "summarization", "start_pos": 275, "end_pos": 288, "type": "TASK", "confidence": 0.6964527368545532}]}, {"text": "Previous works mainly focused on addressing the first two requirements.", "labels": [], "entities": []}, {"text": "Researchers tend to prefer systems that ask open domain questions because the dependency on domain knowledge is usually regarded as an disadvantage.", "labels": [], "entities": []}, {"text": "Several NLG applications successfully utilized domain knowledge, such as virtual shopping assistant) and sport event summarization).", "labels": [], "entities": [{"text": "sport event summarization", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.588175505399704}]}, {"text": "However, the domain knowledge that they used are manually constructed by human experts.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this paper is the first work in QG that attempts to utilize domain knowledge obtained in a lightly-supervised manner.", "labels": [], "entities": []}, {"text": "Although we choose QG as the application in this work, the lightly-supervised content selection methods that we propose could also be applied to augment other NLG tasks such as summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 177, "end_pos": 190, "type": "TASK", "confidence": 0.9861327409744263}]}, {"text": "In section 2, we present previous works of QG and how we position this work into the full storyline.", "labels": [], "entities": [{"text": "QG", "start_pos": 43, "end_pos": 45, "type": "DATASET", "confidence": 0.8823162913322449}]}, {"text": "In section 3, we briefly describe the dataset we use.", "labels": [], "entities": []}, {"text": "Section 4 introduces two target concept selection methods based on automatically constructed domain knowledge.", "labels": [], "entities": []}, {"text": "Section 5 describes methods to generate Q&A pairs from target concepts.", "labels": [], "entities": []}, {"text": "In section 6, we present our experimental results.", "labels": [], "entities": []}, {"text": "Lastly, we present conclusions and suggest future directions.", "labels": [], "entities": []}, {"text": "The contributions of this paper are: 1.", "labels": [], "entities": []}, {"text": "Propose to select target concepts for question generation with lightly-supervised approaches.", "labels": [], "entities": [{"text": "question generation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7877416610717773}]}, {"text": "2. Demonstrate that the use of domain knowledge helps to ask more important questions.", "labels": [], "entities": []}, {"text": "3. Quantitatively evaluate the impact of different ways to represent and select target concepts on question generation task.", "labels": [], "entities": [{"text": "question generation task", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.7968660493691763}]}], "datasetContent": [{"text": "We make use of two datasets obtained from the Internet.", "labels": [], "entities": []}, {"text": "One is 200k company profiles from CrunchBase.", "labels": [], "entities": []}, {"text": "Another is 57k common crawl business news articles.", "labels": [], "entities": [{"text": "common crawl business news articles", "start_pos": 15, "end_pos": 50, "type": "DATASET", "confidence": 0.7483492612838745}]}, {"text": "We refer to these two corpora as \"Company Profile Corpus\" and \"News Corpus\".", "labels": [], "entities": [{"text": "Company Profile Corpus", "start_pos": 34, "end_pos": 56, "type": "DATASET", "confidence": 0.6608743170897166}, {"text": "News Corpus", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.9445772171020508}]}, {"text": "Each article in News Corpus is also assigned a subcategory by editors (e.g. credit-debt-loan, financial planning, hedge fund, insurance.).", "labels": [], "entities": [{"text": "News Corpus", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.969548910856247}]}, {"text": "There are altogether 12 subcategories.", "labels": [], "entities": []}, {"text": "We randomly selected 30 company profiles and 30 news articles for manual evaluation.", "labels": [], "entities": []}, {"text": "The rest of the datasets are used for development.", "labels": [], "entities": []}, {"text": "We benchmarked our two systems with, which is often used as a baseline for later QG systems 8 . Heilman's system took an overgeneration approach which relied on a question ranker to rank the Q&A pairs.", "labels": [], "entities": []}, {"text": "We noted that many top questions the system generated are near duplicates of each other 9 . Hence, we manually removed the near duplicate Q&A pairs before the evaluation and kept only the ones with the highest score.", "labels": [], "entities": []}, {"text": "The source code is available at www.ark.cs.cmu.edu/mheilman/questions/.", "labels": [], "entities": []}, {"text": "9 Generated by applying different question templates on the same source sentence.", "labels": [], "entities": []}, {"text": "E.g. \"Q: Is Windows Microsoft's product?", "labels": [], "entities": []}, {"text": "A: Yes.\" and \"Q: Whose product is Windows?", "labels": [], "entities": []}, {"text": "We generated questions with the three systems (Heilman, role-based QG and relation-based QG) on the evaluation set, which consists of 30 company profiles and 30 news articles.", "labels": [], "entities": []}], "tableCaptions": []}