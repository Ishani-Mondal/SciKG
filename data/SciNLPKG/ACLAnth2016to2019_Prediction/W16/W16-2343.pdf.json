{"title": [], "abstractContent": [{"text": "Paraphrase can help match synonyms or match phrases with the same or similar meaning, thus it plays an important role in automatic evaluation of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.704645112156868}]}, {"text": "The traditional approaches extract paraphrase in general domain from bilingual corpus.", "labels": [], "entities": []}, {"text": "Because the WMT16 metrics task consists of three sub-tasks, namely news domain, medical domain, and IT domain, we propose to extract domain-specific paraphrase tables from monolingual corpus to replace the general paraphrase table.", "labels": [], "entities": [{"text": "WMT16 metrics task", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.506377120812734}]}, {"text": "We utilize the M-L approach to filter the large scale general monolingual corpus into a domain specific sub-corpus, and exploit Markov Network model to extract paraphrase tables from the sub-corpus.", "labels": [], "entities": []}, {"text": "The experimental results on WMT15 Metrics task show that METEOR metric using the domain-specific paraphrase tables outperforms that using the paraphrase table in general domain extracted from the bilingual corpus.", "labels": [], "entities": [{"text": "WMT15 Metrics task", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.7677867412567139}, {"text": "METEOR", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9847632050514221}]}], "introductionContent": [{"text": "Machine translation (MT) automatic evaluation metrics, such as BLEU (), NIST), METEOR (), TER (), MAXSIM ( etc., evaluate the quality of the MT system output by calculating the similarity between the translation output and the human reference.", "labels": [], "entities": [{"text": "Machine translation (MT) automatic evaluation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8714947955948966}, {"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9986409544944763}, {"text": "METEOR", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9876583218574524}, {"text": "TER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9953010082244873}, {"text": "MAXSIM", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.8898822665214539}, {"text": "MT", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.9625005125999451}]}, {"text": "Accurately matching words or phrases with the same or similar meaning is critical to the performance of the automatic evaluation metrics (.", "labels": [], "entities": []}, {"text": "Recently, many works enhanced traditional metrics by adding paraphrase match.", "labels": [], "entities": []}, {"text": "For instance, in the latest version of METEOR package), the paraphrase match was added after the standard exact word match, stem match and synonym match.", "labels": [], "entities": [{"text": "METEOR package", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.7571062743663788}]}, {"text": "And the latest version of TER package () relaxes the condition of word match or chunk shift by adding paraphrase match.", "labels": [], "entities": [{"text": "TER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.6981683969497681}]}, {"text": "Note that the paraphrase tables used in latest METE-OR and TER metrics belong to the general domain and they are extracted from bilingual parallel corpus by the Pivot approach ().", "labels": [], "entities": [{"text": "TER", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.842579185962677}]}, {"text": "However, the WMT16 metrics task consists of sub-tasks on specific domains involving several different languages.", "labels": [], "entities": [{"text": "WMT16 metrics task", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.6694539785385132}]}, {"text": "Confronted with the changes, we propose a Monolingual Paraphrase Extraction method based on Domain Adaptation (MPEDA), and use the new domain-specific paraphrase table to replace the traditional paraphrase tables in the latest METEOR package.", "labels": [], "entities": [{"text": "Paraphrase Extraction", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.7285336852073669}]}], "datasetContent": [{"text": "To test the quality of the domain-specific paraphrase extracted from monolingual corpus by the proposed approach, we conducted experiments on WMT15 Metrics task.", "labels": [], "entities": [{"text": "WMT15 Metrics task", "start_pos": 142, "end_pos": 160, "type": "DATASET", "confidence": 0.711116890112559}]}, {"text": "The METEOR-Universal metric) using the paraphrase tables which were extracted from the bilingual parallel corpus was set as the baseline metric.", "labels": [], "entities": [{"text": "METEOR-Universal", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.817648708820343}]}, {"text": "We used the paraphrase tables in general domain extracted by the Markov Network model, and the domainspecific paraphrase tables extracted by our approach substituted for the original paraphrased tables, respectively.", "labels": [], "entities": []}, {"text": "The updated metrics are called as METEOR-Markov and METEOR-MPEDA.", "labels": [], "entities": [{"text": "METEOR-Markov", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.668158233165741}, {"text": "METEOR-MPEDA", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.8067646622657776}]}, {"text": "We compared the METEOR-MPEDA metric with the METEOR-Markov metric and METEOR-Universal metric to demonstrate the quality of the domain-specific paraphrase table extracted by our approach.", "labels": [], "entities": [{"text": "METEOR-MPEDA", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.5555408000946045}, {"text": "METEOR-Markov metric", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.6380398869514465}, {"text": "METEOR-Universal metric", "start_pos": 70, "end_pos": 93, "type": "METRIC", "confidence": 0.5598749667406082}]}, {"text": "Besides, we compared the METEOR-MPEDA with METEOR metric () which only uses the exact word match, stem match and synonym match.", "labels": [], "entities": [{"text": "METEOR-MPEDA", "start_pos": 25, "end_pos": 37, "type": "METRIC", "confidence": 0.5078003406524658}, {"text": "METEOR", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.4944647252559662}]}, {"text": "After dividing the training data into documents, we processed the corpus by the following procedure: tokenize the training data and the references; delete the punctuations; transform the capitalized letters of words into lowercase.", "labels": [], "entities": []}, {"text": "Then, we employed 4-gram language model with Kneser-Ney discounting to train corresponding language models for training data and the references.", "labels": [], "entities": []}, {"text": "The difference of cross entropy of each sentence in the training data language model was calculated.", "labels": [], "entities": []}, {"text": "Then we summed up and normalized the difference of the cross entropy of the documents' sentences.", "labels": [], "entities": []}, {"text": "Thus every document in the training data received a score.", "labels": [], "entities": []}, {"text": "The smaller the value is, the closer the document is to the reference.", "labels": [], "entities": []}, {"text": "Later, we arranged the values in an ascending order, meanwhile, a threshold value was set, and the corpus beyond the threshold was abandoned.", "labels": [], "entities": []}, {"text": "In this way, we obtained a smaller subcorpus with the approximately same domain with the training data.", "labels": [], "entities": []}, {"text": "Finally, we gave different threshold value to the different sub-tasks, in other words, we selected the top n documents after ordering.", "labels": [], "entities": []}, {"text": "We used the Markov network to build a term Markov network model in the sub-corpus, then we calculated the relation among words according to words co-occurrence, next, we extracted the word chunks in the Markov network, and computed the likelihood that two words area paraphrase pair by comparing the two chunks' similarity.", "labels": [], "entities": []}, {"text": "In this work, we extracted ten paraphrase tables for ten sub-tasks in six languages on WMT15.", "labels": [], "entities": [{"text": "WMT15", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.9813200235366821}]}], "tableCaptions": [{"text": " Table 1. The statistics of the corpus", "labels": [], "entities": []}, {"text": " Table 2. The number of documents in training data", "labels": [], "entities": []}, {"text": " Table 3. The system-level correlation of metrics on evaluation translation into English on WMT15 Metrics task", "labels": [], "entities": [{"text": "WMT15 Metrics", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.9135502874851227}]}, {"text": " Table 4. The system-level correlation of metrics on evaluation translation out of English on WMT15 Metrics task", "labels": [], "entities": [{"text": "WMT15 Metrics", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9017618298530579}]}, {"text": " Table 5. The segment-level correlation of metrics on evaluation translation into English on WMT15 Metrics task", "labels": [], "entities": [{"text": "WMT15 Metrics", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.9101082384586334}]}, {"text": " Table 6. The segment-level correlation of metrics on evaluation translation out of English on WMT15 Metrics task", "labels": [], "entities": [{"text": "evaluation translation out of English", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.7990756392478943}, {"text": "WMT15 Metrics", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.9078780114650726}]}]}