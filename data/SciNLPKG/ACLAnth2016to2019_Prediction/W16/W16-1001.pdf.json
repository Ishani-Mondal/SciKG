{"title": [{"text": "\"Making the News\": Identifying Noteworthy Events in News Articles", "labels": [], "entities": [{"text": "Identifying Noteworthy Events in News Articles", "start_pos": 19, "end_pos": 65, "type": "TASK", "confidence": 0.8857845366001129}]}], "abstractContent": [{"text": "Most events described in a news article are background events-only a small number are noteworthy, and a even smaller number serve as the trigger for writing of that article.", "labels": [], "entities": []}, {"text": "Although these events are difficult to identify, they are crucial to NLP tasks such as first story detection, document summarization and event coreference, and to many applications of event analysis that depend on event counting and identifying trends.", "labels": [], "entities": [{"text": "first story detection", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.7691282431284586}, {"text": "document summarization", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.6797893047332764}, {"text": "event coreference", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.7277789860963821}, {"text": "event analysis", "start_pos": 184, "end_pos": 198, "type": "TASK", "confidence": 0.7171662151813507}, {"text": "event counting", "start_pos": 214, "end_pos": 228, "type": "TASK", "confidence": 0.6704536378383636}]}, {"text": "In this work, we introduce the notion of news-peg, a concept borrowed from the political science literature, in an attempt to remedy this problem.", "labels": [], "entities": []}, {"text": "A news-peg is an event which prompted the author to write the article, and it serves as a more fine-grained measure of noteworthiness than a summary.", "labels": [], "entities": []}, {"text": "We describe anew task of news-peg identification and release an annotated dataset for its evaluation.", "labels": [], "entities": [{"text": "news-peg identification", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7451261579990387}]}, {"text": "We formalize an operational definition of a news-peg, on which human anno-tators achieve high inter-annotator agreement (over 80%), and present a rule-based system for this task, which exploits syntactic features derived from established journalistic devices.", "labels": [], "entities": []}], "introductionContent": [{"text": "The narratives in news articles often follow certain established styles, such as inverted pyramid reporting, to emphasize certain parts more than others.", "labels": [], "entities": []}, {"text": "Such narratives nicely illustrate discourse level texture -parts which supply the main points and are crucial to conveying information constitute the foreground, while the parts which assist in providing supporting facts or setting the scene are referred to as background.", "labels": [], "entities": []}, {"text": "When reasoning about events in news articles, such document level texture necessitates the ability to distinguish foreground objects from background.", "labels": [], "entities": [{"text": "reasoning about events in news articles", "start_pos": 5, "end_pos": 44, "type": "TASK", "confidence": 0.8196592430273691}]}, {"text": "Event recognition alone is not sufficient to make such distinctions, and the roles these events play in framing of a given story.", "labels": [], "entities": [{"text": "Event recognition", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7600835263729095}]}, {"text": "Consider the summary shown in -it is easy to infer that the \"release\" event is the reason why the news article was written, and other events are present merely to qualify the entities present.", "labels": [], "entities": []}, {"text": "Distinguishing the \"release\" event from others can help understanding what the document is reporting.", "labels": [], "entities": []}, {"text": "This paper introduces news-peg identification: anew task aimed at finding events which triggered the creation of the news article.", "labels": [], "entities": [{"text": "news-peg identification", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.7015556246042252}]}, {"text": "Our notion of newspeg serves (formally defined in \u00a74) as a measure of noteworthiness, assessing how much was the event responsible in prompting the author to write the article.", "labels": [], "entities": []}, {"text": "Such events are also called dominant news elements in the social science literature.", "labels": [], "entities": []}, {"text": "Note that news-pegs determine a stronger measure of noteworthiness than summaries, as a summary can contain events which are not news-pegs.", "labels": [], "entities": []}, {"text": "We discuss how the ability to identify news-pegs can aid progress in several NLP tasks.", "labels": [], "entities": []}, {"text": "The contributions of our work are as follows: 1 \u2022 We define anew task, news-peg identification, and annotate data for its evaluation.", "labels": [], "entities": [{"text": "news-peg identification", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.7322999835014343}]}, {"text": "\u2022 We experimentally demonstrate the feasibility of the task, by showing a high inter-annotator agreement of 81.3% on a manually annotated evaluation dataset of 100 documents.", "labels": [], "entities": [{"text": "agreement", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.536353588104248}]}, {"text": "\u2022 We also evaluate several baseline approaches which exploit syntax to identify news-pegs and propose a rule-based approach which attains a F1 score of 54.7 points.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9876041412353516}]}], "datasetContent": [{"text": "We use the New York Times Annotated Corpus) for all our experiments.", "labels": [], "entities": [{"text": "New York Times Annotated Corpus)", "start_pos": 11, "end_pos": 43, "type": "DATASET", "confidence": 0.752314547697703}]}, {"text": "The corpus contains around 650k documents annotated with human-generated summaries.", "labels": [], "entities": []}, {"text": "We only work with the \"World News\" section of the corpus from 2003 to 2007.", "labels": [], "entities": [{"text": "World News\" section of the corpus from 2003", "start_pos": 23, "end_pos": 66, "type": "DATASET", "confidence": 0.9381402068667941}]}, {"text": "We randomly sampled 100 articles to be manually annotated and used for evaluation.", "labels": [], "entities": []}, {"text": "We generated the list of acceptable frames by identifying the frame evoked by each event trigger in ACE) and ERE.", "labels": [], "entities": [{"text": "ERE", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9905992150306702}]}, {"text": "We use this list of frames to determine whether a predicateargument structure qualifies as an event.", "labels": [], "entities": []}, {"text": "For identifying the frame evoked by a given predicate, we use the state-of-the-art frame-identifier packaged in SEMAFOR ().", "labels": [], "entities": []}, {"text": "For our event extraction, all the documents were annotated using Illinois-SRL (, a state-ofthe-art SRL system, to identify nominal and verbal predicates.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7833640575408936}, {"text": "Illinois-SRL", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.8499746322631836}]}, {"text": "Note that we do not take into account semantic role assignments, as we believe this does not have any consequence on event extraction.", "labels": [], "entities": [{"text": "semantic role assignments", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6289889613787333}, {"text": "event extraction", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.7841928601264954}]}, {"text": "To extract features, we use the constituency and dependency parser from Stanford CoreNLP) to identify the clause structure of sentences.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.8451530933380127}]}, {"text": "Any clause labeled \"SBAR\" is considered subordinate, and a subordinate clause starting with a Wh-word is considered relative 2 . All remaining clauses are considered main.", "labels": [], "entities": []}, {"text": "To identify appositions, we used the Illinois-Comma-SRL (Arivazhagan et al., 2016) package.", "labels": [], "entities": [{"text": "Illinois-Comma-SRL (Arivazhagan et al., 2016) package", "start_pos": 37, "end_pos": 90, "type": "DATASET", "confidence": 0.8906556699011061}]}], "tableCaptions": [{"text": " Table 2: Results comparing baselines and the rule based  classifier. The classifier outperforms the baselines, but  still achieves moderate F1 scores.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9800088405609131}]}]}