{"title": [{"text": "Private or Corporate? Predicting User Types on Twitter", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a series of experiments on discriminating between private and corporate accounts on Twitter.", "labels": [], "entities": []}, {"text": "We define features based on Twitter metadata, morphosyntactic tags and surface forms, showing that the simple bag-of-words model achieves single best results that can, however, be improved by building a weighted soft ensemble of classifiers based on each feature type.", "labels": [], "entities": []}, {"text": "Investigating the time and language dependence of each feature type delivers quite unex-pecting results showing that features based on metadata are neither time-nor language-insensitive as the way the two user groups use the social network varies heavily through time and space.", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to popularity of Twitter and its proactive content harvesting policy, automatic identification of latent user attributes has become a hot research topic in the past couple of years.", "labels": [], "entities": [{"text": "automatic identification of latent user attributes", "start_pos": 74, "end_pos": 124, "type": "TASK", "confidence": 0.7394527892271677}]}, {"text": "Tasks range from discriminating between different types of users), behaviour (), location), gender), age), occupation (, social class () and personality type.", "labels": [], "entities": []}, {"text": "In this paper we present a series of experiments on discriminating between corporate and private accounts on Twitter.", "labels": [], "entities": []}, {"text": "We investigate language-independent features, features extracted from morphosyntactic annotations and simple bag-of-words features.", "labels": [], "entities": []}, {"text": "We additionally investigate the time and space dependence of specific feature sets by measuring how well specific features handle training on onetime span or closely-related language and testing on another time span or other closely-related language.", "labels": [], "entities": []}, {"text": "rich linguistic features (prototypical words and hashtags, generic and domain-specific LDA, sentiment words) yields consistently better results.", "labels": [], "entities": []}, {"text": "employ a more traditional apparoach to author characterization by relying on a set of term-based features in users' messages and a set of stylistic features (character usage, message length, word length, punctuation usage, punctuation marks, stopword usage, stopwords, smiley usage, smileys, vocabulary richness).", "labels": [], "entities": [{"text": "author characterization", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7064973413944244}]}, {"text": "While our work has noticeable overlap with existing research that was carried out on English, it extends that research, beyond the language shift, by (1) performing a more detailed analysis of a broader list of features, and inspecting (2) the stability of those features in different time periods and (3) their portability to a closely-related language.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset used in this paper consists of 7.5 million Slovene tweets collected from June 2013 to January 2016 with the TweetCaT tool).", "labels": [], "entities": []}, {"text": "Each of the 7778 users in the collection was manually annotated as private or corporate.", "labels": [], "entities": []}, {"text": "We performed our experiments on 5842 users for whom we had at least 100 tweets at our disposal.", "labels": [], "entities": []}, {"text": "Out of these 5842 users, 4382 were annotated as private and 1460 as corporate.", "labels": [], "entities": []}, {"text": "We run three batches of experiments.", "labels": [], "entities": []}, {"text": "In the first batch we experiment with the three types of features presented in the previous section.", "labels": [], "entities": []}, {"text": "Additionally, we build an ensemble classifier which uses the output of the three feature-type classifiers.", "labels": [], "entities": []}, {"text": "In the second batch we investigate the time sensitivity of the presented feature types by training on data from onetime span and testing on a later one.", "labels": [], "entities": []}, {"text": "Finally, in the third batch we investigate the portability of two feature types, namely language-independent features and bag-ofwords features, to a closely-related language.", "labels": [], "entities": []}, {"text": "As our weak baseline we use the most-frequent-class (MFC) baseline.", "labels": [], "entities": []}, {"text": "Our classifier of choice on all feature types are support vector machines (SVM).", "labels": [], "entities": []}, {"text": "In case where the number of features is smaller than the number of instances we use the RBF kernel while in the opposite case we use a linear kernel.", "labels": [], "entities": []}, {"text": "In all the experiments we 5-fold with stratification overall our data, optimising in each iteration the classifier via grid search on the development data and evaluating it on the test data via weighted F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 203, "end_pos": 205, "type": "METRIC", "confidence": 0.943369448184967}]}, {"text": "In case of the RBF kernel we optimise the C (2 n , n \u2208 {\u22125, \u22123, \u22121, ...15}) and \u03b3 (2 n , n \u2208 {\u221215, \u221213, \u221211, ...3}) hyperparameters while for the linear kernel we optimise the C parameter in the same range as with the RBF kernel.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: An overview of the language-independent features, sorted by their p-value,", "labels": [], "entities": []}, {"text": " Table 2: An overview of the language-dependent focused features, sorted by their p-value", "labels": [], "entities": []}, {"text": " Table 3: List of the strongest language-dependent bag-of-word features, for private and for corporate  users", "labels": [], "entities": []}, {"text": " Table 4: Evaluation results via weighted F1 on the per-feature-type classifiers and the ensemble classifier.  Statistical significance of the differences is calculated via approximate randomisation.", "labels": [], "entities": [{"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9915674328804016}]}, {"text": " Table 5: Results on the time and language sensitivity of specific feature types", "labels": [], "entities": []}]}