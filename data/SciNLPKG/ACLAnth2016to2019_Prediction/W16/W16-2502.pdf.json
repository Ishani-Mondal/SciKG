{"title": [{"text": "A critique of word similarity as a method for evaluating distributional semantic models", "labels": [], "entities": [{"text": "word similarity", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.7242802828550339}]}], "abstractContent": [{"text": "This paper aims to rethink the role of the word similarity task in distributional semantics research.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7688824435075124}, {"text": "distributional semantics research", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.8314012487729391}]}, {"text": "We argue while it is a valuable tool, it should be used with care because it provides only an approximate measure of the quality of a distributional model.", "labels": [], "entities": []}, {"text": "Word similarity evaluations assume there exists a single notion of similarity that is independent of a particular application.", "labels": [], "entities": [{"text": "Word similarity evaluations", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7636198500792185}]}, {"text": "Further, the small size and low inter-annotator agreement of existing data sets makes it challenging to find significant differences between models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional models of lexical semantics have recently attracted considerable interest in the NLP community.", "labels": [], "entities": []}, {"text": "With the increase in popularity, the issue of evaluation is becoming more important.", "labels": [], "entities": [{"text": "evaluation", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.9497154951095581}]}, {"text": "While extrinsic (task-based) evaluations are increasingly common, the most frequently used family of evaluation procedures (intrinsic evaluations) attempt to directly measure the \"inherent\" quality of a word representation.", "labels": [], "entities": []}, {"text": "This often takes the form of computing the extent to which a model agrees with human-provided word or phrase similarity scores.", "labels": [], "entities": []}, {"text": "This paper highlights the theoretical and practical issues with the word similarity task, which make it a poor measure of the quality of a distributional model.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7947949767112732}]}, {"text": "We investigate five commonly used word similarity datasets, RG,), WS353 (), MEN () and.", "labels": [], "entities": [{"text": "RG", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.5731800198554993}, {"text": "WS353", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.6214984059333801}, {"text": "MEN", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.6028463840484619}]}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}, {"text": "We argue that the notion of lexical similarity is difficult to define outside of the context of a task and without conflating different concepts such as \"similarity\" or \"relatedness\".", "labels": [], "entities": []}, {"text": "We show inter-annotator agreement at the word similarity task is considerably lower compared to other tasks such as document classification or textual entailment.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7376525600751241}, {"text": "document classification", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.7740280628204346}, {"text": "textual entailment", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.6556558012962341}]}, {"text": "Furthermore, we demonstrate that the quality of a model, as measured by a given word similarity data set, can vary substantially because of the small size of the data set.", "labels": [], "entities": []}, {"text": "Lastly, we introduce a simple sanity check for word similarity data sets that tests whether a data set is able to reliably identify corrupted word vectors.", "labels": [], "entities": []}, {"text": "These findings can be adopted as guidelines for designers of evaluation data sets.", "labels": [], "entities": []}, {"text": "The code for our experiments is available at github.com/mbatchkarov/ repeval2016.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distribution of Spearman \u03c1 between  model predictions and gold standard data set.", "labels": [], "entities": [{"text": "Spearman \u03c1", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9445853233337402}, {"text": "gold standard data set", "start_pos": 68, "end_pos": 90, "type": "DATASET", "confidence": 0.761744037270546}]}]}