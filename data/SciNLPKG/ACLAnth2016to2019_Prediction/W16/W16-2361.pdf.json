{"title": [], "abstractContent": [{"text": "Neural sequence to sequence learning recently became a very promising paradigm in machine translation, achieving competitive results with statistical phrase-based systems.", "labels": [], "entities": [{"text": "Neural sequence to sequence learning", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8048979282379151}, {"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7698477208614349}]}, {"text": "In this system description paper , we attempt to utilize several recently published methods used for neural sequential learning in order to build systems for WMT 2016 shared tasks of Automatic Post-Editing and Multimodal Machine Translation.", "labels": [], "entities": [{"text": "WMT 2016 shared tasks", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.674074798822403}, {"text": "Multimodal Machine Translation", "start_pos": 210, "end_pos": 240, "type": "TASK", "confidence": 0.6552578111489614}]}], "introductionContent": [{"text": "Neural sequence to sequence models are currently used for variety of tasks in Natural Language Processing including machine translation ), text summarization (, natural language generation (, and others.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.793441891670227}, {"text": "text summarization", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.7985306680202484}, {"text": "natural language generation", "start_pos": 161, "end_pos": 188, "type": "TASK", "confidence": 0.6396967868010203}]}, {"text": "This was enabled by the capability of recurrent neural networks to model temporal structure in data, including the long-distance dependencies in case of gated networks).", "labels": [], "entities": []}, {"text": "The deep learning models' ability to learn a dense representation of the input in the form of a real-valued vector recently allowed researchers to combine machine vision and natural language processing into tasks believed to be extremely difficult only few years ago.", "labels": [], "entities": []}, {"text": "The distributed representations of words, sentences and images can be understood as a kind of common datatype for language and images within the models.", "labels": [], "entities": []}, {"text": "This is then used in tasks like automatic image captioning (, visual question answering ( or in attempts to ground lexical semantics in vision ().", "labels": [], "entities": [{"text": "automatic image captioning", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.6054780085881551}, {"text": "visual question answering", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.6269253591696421}]}, {"text": "In this system description paper, we bring a summary of the Recurrent Neural Network (RNN)-based system we have submitted to the automatic post-editing task and to the multimodal translation task.", "labels": [], "entities": [{"text": "multimodal translation task", "start_pos": 168, "end_pos": 195, "type": "TASK", "confidence": 0.7533411383628845}]}, {"text": "Section 2 describes the architecture of the networks we have used.", "labels": [], "entities": []}, {"text": "Section 3 summarizes related work on the task of automatic post-editing of machine translation output and describes our submission to the Workshop of Machine Translation (WMT) competition.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.7814174095789591}, {"text": "Workshop of Machine Translation (WMT) competition", "start_pos": 138, "end_pos": 187, "type": "TASK", "confidence": 0.7829840034246445}]}, {"text": "Ina similar fashion, Section 4 refers to the task of multimodal translation.", "labels": [], "entities": [{"text": "multimodal translation", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.6389832496643066}]}, {"text": "Conclusions and ideas for further work are given in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The input sentence is fed to our system in a form of multiple input sequences without explicitly telling which sentence is the source one and which one: Results of experiments on the APE task on the validation data.", "labels": [], "entities": [{"text": "APE task", "start_pos": 183, "end_pos": 191, "type": "TASK", "confidence": 0.6573499739170074}]}, {"text": "The '+' sign indicates the additional regular-expression rules -the system that has been submitted. is the MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.7347985506057739}]}, {"text": "It is up to the network to discover their best use when producing the (single) target sequence.", "labels": [], "entities": []}, {"text": "The initial experiments showed that the network struggles to learn that one of the source sequences is almost correct (even if it shares the vocabulary and word embeddings with the expected target sequence).", "labels": [], "entities": []}, {"text": "Instead, the network seemed to learn to paraphrase the input.", "labels": [], "entities": []}, {"text": "To make the network focus more on editing of the source sentence instead of preserving the meaning of the sentences, we represented the target sentence as a minimum-length sequence of edit operations needed to turn the machine-translated sentence into the reference post-edit.", "labels": [], "entities": []}, {"text": "We extended the vocabulary by two special tokens keep and delete and then encoded the reference as a sequence of keep, delete and insert operations with the insert operation defined by the placing the word itself.", "labels": [], "entities": []}, {"text": "After applying the generated edit operations on the machine-translated sentences in the test phase, we perform a few rule-based orthographic fixes for punctuation.", "labels": [], "entities": []}, {"text": "The performance of the system is given in Table 1.", "labels": [], "entities": []}, {"text": "The system was able to slightly improve upon the baseline (keeping the translation as it is) in both the HTER and BLEU score.", "labels": [], "entities": [{"text": "HTER", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.7794439196586609}, {"text": "BLEU score", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9589160680770874}]}, {"text": "The system was able to deal very well with the frequent error of keeping a word from the source in the translated sentence.", "labels": [], "entities": []}, {"text": "Although neural sequential models usually learn the basic output structure very quickly, in this case it made a lot of errors in pairing parentheses correctly.", "labels": [], "entities": []}, {"text": "We ascribe this to the edit-operation notation which obfuscated the basic orthographic patterns in the target sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of experiments on the APE task  on the validation data. The '+' sign indicates the  additional regular-expression rules -the system  that has been submitted.", "labels": [], "entities": [{"text": "APE task", "start_pos": 40, "end_pos": 48, "type": "TASK", "confidence": 0.7717397511005402}]}, {"text": " Table 2: Results of experiments with the multimodal translation task on the validation data. At the time  of the submission, the models were not tuned as well as our final models. The first six system are targeted  for the translation task. They were trained against one reference -a German translation of one English  caption. The last four systems are target to the cross-lingual captioning task. They were trained with 5  independent German captions (5 times bigger data).", "labels": [], "entities": [{"text": "multimodal translation task", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6983372569084167}, {"text": "translation task", "start_pos": 224, "end_pos": 240, "type": "TASK", "confidence": 0.9226231276988983}, {"text": "cross-lingual captioning task", "start_pos": 369, "end_pos": 398, "type": "TASK", "confidence": 0.7856225172678629}]}]}