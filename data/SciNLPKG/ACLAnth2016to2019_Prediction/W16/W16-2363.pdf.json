{"title": [], "abstractContent": [{"text": "This paper describes the University of Sheffield's submission for the WMT16 Multimodal Machine Translation shared task, where we participated in Task 1 to develop German-to-English and English-to-German statistical machine translation (SMT) systems in the domain of image descriptions.", "labels": [], "entities": [{"text": "WMT16 Multimodal Machine Translation shared task", "start_pos": 70, "end_pos": 118, "type": "TASK", "confidence": 0.7200398693482081}, {"text": "English-to-German statistical machine translation (SMT)", "start_pos": 185, "end_pos": 240, "type": "TASK", "confidence": 0.7597361377307347}]}, {"text": "Our proposed systems are standard phrase-based SMT systems based on the Moses decoder, trained only on the provided data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8579387664794922}]}, {"text": "We investigate how image features can be used to re-rank the n-best list produced by the SMT model, with the aim of improving performance by grounding the translations on images.", "labels": [], "entities": [{"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9877391457557678}]}, {"text": "Our submissions are able to outperform the strong, text-only baseline system for both directions .", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the University of Sheffield's submission fora new WMT16 Multimodal Machine Translation shared task.", "labels": [], "entities": [{"text": "WMT16 Multimodal Machine Translation shared task", "start_pos": 71, "end_pos": 119, "type": "TASK", "confidence": 0.729689434170723}]}, {"text": "The task is aimed at the generation of image descriptions in a target language, given an image and one or more descriptions in a different (source) language.", "labels": [], "entities": []}, {"text": "We participated in Task 1, which takes a source language description and translates it into the target language, supported by information from images.", "labels": [], "entities": []}, {"text": "We submitted systems for the translation between English and German in both directions.", "labels": [], "entities": [{"text": "translation between English and German", "start_pos": 29, "end_pos": 67, "type": "TASK", "confidence": 0.8722316265106201}]}, {"text": "Multimodal approaches for various applications related to language processing have been gaining wider attention from the research community in recent years.", "labels": [], "entities": [{"text": "language processing", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7329474687576294}]}, {"text": "The main motivation is to investigate whether contextual information from various sources can be helpful in improving system performance.", "labels": [], "entities": []}, {"text": "Multimodal approaches have been explored in various tasks such as image and video description, as well as question answering about images (see Section 4).", "labels": [], "entities": [{"text": "image and video description", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.6146500185132027}, {"text": "question answering about images", "start_pos": 106, "end_pos": 137, "type": "TASK", "confidence": 0.8457359820604324}]}, {"text": "However, not much work has been done to explore multimodality in the context of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7765880823135376}]}, {"text": "Whilst a large number of approaches have been developed to improve translation quality, they concern solely textual information.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.8518022894859314}]}, {"text": "The use of non-textual sources such as images and speech has been largely ignored partially because of the lack of datasets and resources.", "labels": [], "entities": []}, {"text": "This shared task provides an interesting opportunity to investigate the effectiveness of information from images in improving the performance of machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7834047973155975}]}, {"text": "The main objective of our proposed system is to explore how image features can be used to rerank an n-best list of translations from a standard phrase-based Statistical Machine Translation (SMT) system.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 157, "end_pos": 194, "type": "TASK", "confidence": 0.7957406143347422}]}, {"text": "This is in contrast to existing) that uses image features jointly with image descriptions to train a Neural Network-based translation model.", "labels": [], "entities": [{"text": "Neural Network-based translation", "start_pos": 101, "end_pos": 133, "type": "TASK", "confidence": 0.6220323940118154}]}, {"text": "The dataset provided for this shared task contains short segments with simple grammar and repetitive vocabulary.", "labels": [], "entities": []}, {"text": "Therefore, it is expected that a standard phrase-based SMT system can already produced reasonably good quality translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.8545538187026978}]}, {"text": "The intuition behind our approach is that image features may help further improve the translation of image descriptions, for example disambiguating words with multiple senses, when these alternatives are available in the n-best list produced by the SMT model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 249, "end_pos": 252, "type": "TASK", "confidence": 0.981816291809082}]}, {"text": "This approach also has the advantage over joint visual-textual alternatives in that the translation model itself is learnt independently from images, and thus does not require datasetspecific images at training time to generate candidate translations.", "labels": [], "entities": []}, {"text": "In fact, images are only used attest time for n-best list re-ranking, and the visual classifier is pre-trained on a generic image dataset.", "labels": [], "entities": []}, {"text": "We use image features from a Convolutional Neural Network (CNN) along with standard Moses features to re-rank the n-best list.", "labels": [], "entities": []}, {"text": "We also propose an alternative scheme for the German-toEnglish direction, where terms in the English image descriptions are matched against 1,000 WordNet synsets, and the probability of these synsets occurring in the image estimated using CNN predictions on the images.", "labels": [], "entities": []}, {"text": "The aggregated probabilities are then used to re-rank the n-best list, with the intuition that the best translations will contain words representing these entities.", "labels": [], "entities": []}, {"text": "Our submissions that re-rank the n-best translations with image vectors are able to marginally outperform the strong, text-only baseline system for both directions.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the procedure to extract image features.", "labels": [], "entities": []}, {"text": "In Section 3 we explain the experiments along with their results.", "labels": [], "entities": []}, {"text": "We finally give a brief overview of related work in Section 4, before presenting some conclusion and future directions (Section 5).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Datasets size and results of a baseline sys- tem on the development set.", "labels": [], "entities": []}, {"text": " Table 2: Results on the development set after re- ranking.", "labels": [], "entities": []}, {"text": " Table 3: Results on the test set: Baseline Moses vs  Re-ranking approach.", "labels": [], "entities": []}]}