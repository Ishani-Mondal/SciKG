{"title": [{"text": "Domain Adaptation of Polarity Lexicon combining Term Frequency and Bootstrapping", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we study several approaches to adapting a polarity lexicon to a specific domain.", "labels": [], "entities": []}, {"text": "On the one hand, the domain adaptation using Term Frequency (TF) and on the other hand, the domain adaptation using pattern matching with a BootStrapping algorithm (BS).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7123899608850479}, {"text": "Term Frequency (TF)", "start_pos": 45, "end_pos": 64, "type": "METRIC", "confidence": 0.8294983744621277}, {"text": "domain adaptation", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7288618087768555}]}, {"text": "Both methods are corpus based and start with the same polarity lexicon, but the first one requires an annotated collection of documents while the second one only needs a corpus where it looks for linguistic patterns.", "labels": [], "entities": []}, {"text": "The performance of both methods overcomes the baseline system using the general polarity lexicon iSOL.", "labels": [], "entities": []}, {"text": "However, although the TF approach achieves very promising results, the BS strategy does not give as much improvement as we expected.", "labels": [], "entities": []}, {"text": "For this reason, we have combined both methods in order to take advantage of the positive aspects of each one.", "labels": [], "entities": []}, {"text": "With this new approach the results obtained are even better that those with the systems applied individually.", "labels": [], "entities": []}, {"text": "Actually, we have achieved a significant improvement of 11.50% (in terms of accuracy) in the polarity classification of the movie reviews with respect to the results achieved with the general purpose lexicon iSOL.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9992675185203552}]}], "introductionContent": [{"text": "Sentiment Analysis (SA) is a discipline that combines Natural Language Processing (NLP) and data mining techniques to deal with the subjectivity in textual information.", "labels": [], "entities": [{"text": "Sentiment Analysis (SA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8917057037353515}]}, {"text": "Several tasks have been studied but perhaps Polarity Classification is the most well known that focuses on determining the semantic orientation of a document: positive, negative or neutral.", "labels": [], "entities": [{"text": "Polarity Classification", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.7217296808958054}]}, {"text": "Although different approaches have been applied to the field of polarity classification, the mainstream basically consists of two major methodologies.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.9194923937320709}]}, {"text": "On the one hand, the Machine Learning (ML) approach that is based on using a collection of data to train the classifiers ().", "labels": [], "entities": [{"text": "Machine Learning (ML)", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.6713837683200836}]}, {"text": "On the other hand, the approach based on Semantic Orientation (SO) that does not need prior training but takes into account the orientation of words, positive or negative.", "labels": [], "entities": [{"text": "Semantic Orientation (SO)", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.7989881873130799}]}, {"text": "In this paper we focus on semantic orientation in order to tackle one of the open issues related to polarity classification: domain adaptation.", "labels": [], "entities": [{"text": "semantic orientation", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.783699095249176}, {"text": "polarity classification", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.723050132393837}, {"text": "domain adaptation", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.708198681473732}]}, {"text": "Our main goal is to propose a method for automatically adapting a general polarity lexicon to a specific domain.", "labels": [], "entities": []}, {"text": "Specifically, we are going to work with the movie domain because, as several papers demonstrate, this is a very difficult domain to deal with and adapt in SA).", "labels": [], "entities": []}, {"text": "In addition, we will focus on Spanish since we consider that a language other than English is a more challenging task in the NLP area in general, and in SA in particular, due to the scarcity of resources.", "labels": [], "entities": []}, {"text": "Different methods have been proposed for tackling the domain adaptation problem by automatically generating polarity lexicons.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7022787630558014}]}, {"text": "One of the primary studies related to SA is (.", "labels": [], "entities": [{"text": "SA", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.964999794960022}]}, {"text": "They note that the polarity of a particular word can carry opposing sentiments depending on the domain, so the general purpose lexicon should be adapted to the specific domain in order to improve the effectiveness.", "labels": [], "entities": []}, {"text": "Two main approaches to creating polarity lexicons automatically have been studied: dictionary-based and corpus-based.", "labels": [], "entities": []}, {"text": "Dictionarybased approaches use external resources such as thesaurus or dictionaries in order to enrich a set of polar terms by aggregating new subjective words ().", "labels": [], "entities": []}, {"text": "On the other hand, corpusbased approaches also start from a set of polar terms but instead of using dictionaries, which are domain dependant and usually difficult to find in some languages, they try to integrate external knowledge from document collections.", "labels": [], "entities": []}, {"text": "In this paper we investigate two corpus-based methods used to adapt a polarity lexicon to a specific domain.", "labels": [], "entities": []}, {"text": "On the one hand, domain adaptation using Term Frequency (TF) and on the other hand, domain adaptation using pattern matching with a BootStrapping algorithm (BS).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8398979902267456}, {"text": "domain adaptation", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.8619199395179749}]}, {"text": "Both methods are corpus based and start with the same polarity lexicon, but the first one requires an annotated collection of documents while the second one only needs a corpus where it seeks the specific patterns.", "labels": [], "entities": []}, {"text": "The performance obtained with both methods significantly overcomes the baseline system using the general polarity lexicon.", "labels": [], "entities": []}, {"text": "Finally, we propose an approach that combines both domain adaptation methods and the results obtained are even better that those with the systems applied individually.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7081494927406311}]}, {"text": "Most studies usually start with a very small set of polar terms and then apply some methods to extract and append new subjective words to the original list.", "labels": [], "entities": []}, {"text": "However, in this paper we use a very large general lexicon as our starting point.", "labels": [], "entities": []}, {"text": "Specifically, in the two approaches presented in the paper, we have used as seed the Spanish polarity lexicon iSOL.", "labels": [], "entities": [{"text": "Spanish polarity lexicon iSOL", "start_pos": 85, "end_pos": 114, "type": "DATASET", "confidence": 0.590205617249012}]}, {"text": "This lexicon has been successfully applied in several studies, showing a very good performance in Spanish SA ().", "labels": [], "entities": [{"text": "Spanish SA", "start_pos": 98, "end_pos": 108, "type": "TASK", "confidence": 0.6342715620994568}]}, {"text": "In addition, we have carried out our experiments with the Spanish movie corpus MuchoCine (.", "labels": [], "entities": [{"text": "Spanish movie corpus MuchoCine", "start_pos": 58, "end_pos": 88, "type": "DATASET", "confidence": 0.7654140144586563}]}, {"text": "Regarding the domain approaches applied, the first one is based on the Term Frequency (TF) in an annotated corpus.", "labels": [], "entities": [{"text": "Term Frequency (TF)", "start_pos": 71, "end_pos": 90, "type": "METRIC", "confidence": 0.8700217723846435}]}, {"text": "This approach has already been applied in previous studies using different domains).", "labels": [], "entities": []}, {"text": "However, although in all the cases the results are improved, we have noted that some polar terms are wrongly added and sometimes some noise is introduced into the systems.", "labels": [], "entities": []}, {"text": "Thus, in this paper we investigate anew method that not only adds new words but also, if some terms are detected to be highly subjective, they are eliminated from the adapted lexicon.", "labels": [], "entities": []}, {"text": "This is the second approach used in this paper and it is based on detecting patterns in a corpus and applying a bootstrapping algorithm in order to enrich and clean the polarity lexicon.", "labels": [], "entities": []}, {"text": "We will call this approach BootStrapping (BS).", "labels": [], "entities": [{"text": "BootStrapping (BS)", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.4655681326985359}]}, {"text": "One of the best points of this second method is that we do not need any annotated corpus to adapt the system.", "labels": [], "entities": []}, {"text": "We only need a polarity lexicon and a corpus of documents.", "labels": [], "entities": []}, {"text": "From this corpus, we extract patterns using the information in the lexicon and we apply the bootstrapping algorithm in order to append or eliminate polar terms from the original lexicon.", "labels": [], "entities": []}, {"text": "In this paper we only look for patterns including adjectives, because according to several studies this kind of word is the best clue to express subjectivity in documents (.", "labels": [], "entities": []}, {"text": "As we will show in Section 5, the results obtained with the TF approach are very promising although the method requires an annotated corpus.", "labels": [], "entities": []}, {"text": "On the other hand, although the results with the BS strategy also surpass the baseline system with the general opinion lexicon, the improvement is lower than we would hope.", "labels": [], "entities": [{"text": "BS", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.6699956655502319}]}, {"text": "For this reason, we have decided to combine both methods in order to take advantage of them.", "labels": [], "entities": []}, {"text": "Thus, we first apply the TF approach obtaining anew adapted polarity lexicon.", "labels": [], "entities": []}, {"text": "This new list is used as the seed of polar terms for the BS algorithm.", "labels": [], "entities": []}, {"text": "In this way, our method not only appends terms that are adapted to the domain but it also eliminates polar terms that can be considered highly subjective in this specific domain.", "labels": [], "entities": []}, {"text": "The results achieved with this combined method improve on the performance of both approaches when they are applied individually.", "labels": [], "entities": []}, {"text": "The rest of the paper is organised as follows: The following section presents a review of the main methods of domain adaptation, focusing mainly on the corpus-based approaches and commenting on some studies that deal with Spanish.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7364703118801117}]}, {"text": "Section 3 introduces the methodology of adaptation based on term frequency and Section 4 presents domain adaptation using bootstrapping.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7178972214460373}]}, {"text": "Section 5 exhibits the resources employed and shows the results obtained.", "labels": [], "entities": []}, {"text": "In addition, we propose the combination of both methods in order to achieve an improvement in the final system.", "labels": [], "entities": []}, {"text": "Section 6 discusses the different results obtained and analyses the systems proposed showing their advantages and disadvantages.", "labels": [], "entities": []}, {"text": "Finally, in Section 7 conclusions and future work are presented.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested two corpus-based approaches and the combination of them for the domain adaptation of a polarity lexicon in Spanish.", "labels": [], "entities": []}, {"text": "The list of opinion words taken as a starting point was iSOL.", "labels": [], "entities": [{"text": "iSOL", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9222817420959473}]}, {"text": "iSOL is a Spanish polarity lexicon generated from the automatic translation of the Bing Liu Lexicon () and the manual revision of it.", "labels": [], "entities": [{"text": "Bing Liu Lexicon", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.7112237811088562}]}, {"text": "It is composed of 2,509 positive and 5,626 negative words.", "labels": [], "entities": []}, {"text": "For the adaptation of this lexicon and for testing the TF and BS approaches we used the Spanish MuchoCine corpus (MC) (.", "labels": [], "entities": [{"text": "Spanish MuchoCine corpus (MC)", "start_pos": 88, "end_pos": 117, "type": "DATASET", "confidence": 0.8640809059143066}]}, {"text": "This dataset consists of 3,878 movie reviews collected from the MuchoCine website.", "labels": [], "entities": [{"text": "MuchoCine website", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.9139758050441742}]}, {"text": "The reviews are written by web users, therefore the sentences found in the reviews may include spelling mistakes or informal expressions and they may not always be grammatically correct.", "labels": [], "entities": []}, {"text": "The dataset contains about 2 million words and an average of 546 words per review.", "labels": [], "entities": []}, {"text": "The opinions in the corpus are rated on a scale from 1 to 5.", "labels": [], "entities": []}, {"text": "A rank of 1 means that the opinion is very bad and 5 means very good.", "labels": [], "entities": []}, {"text": "Reviews with a rating of 3 can be categorized as \"neutral\", which means that the user considers the movie is neither bad nor good.", "labels": [], "entities": []}, {"text": "In our experiments the neutral reviews were not taken into account, the opinions with ratings of 1 or 2 were considered as negative and those with ratings of 4 or 5 as positive (in total 1,351 positive and 1,274 negative reviews).", "labels": [], "entities": []}, {"text": "The 60% of these reviews (781 positive and 794 negative reviews) were employed for the domain adaptation of iSOL and the remaining 40% (570 positive and 480 negative reviews) were used for testing the resultant lists in the task of polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 232, "end_pos": 255, "type": "TASK", "confidence": 0.8346168100833893}]}, {"text": "In order to apply the method based on frequency (TF), the punctuations and the stopwords of the documents were first removed.", "labels": [], "entities": [{"text": "frequency (TF)", "start_pos": 38, "end_pos": 52, "type": "METRIC", "confidence": 0.7104817107319832}]}, {"text": "After this, the absolute frequency of each word in the positive and negative reviews was determined.", "labels": [], "entities": [{"text": "frequency", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.5320166349411011}]}, {"text": "Subsequently, several experiments were carried out to add to iSOL those words of the corpus that verify Equation 1, considering different ratios in order to fix the best ratio between positive and negative terms to consider polar terms.", "labels": [], "entities": []}, {"text": "In the case of the approach based on bootstrapping (BS), the documents were first tokenized and splitted into sentences and each token was tagged with its pertinent part of speech, using Freeling 3).", "labels": [], "entities": [{"text": "Freeling 3", "start_pos": 187, "end_pos": 197, "type": "DATASET", "confidence": 0.8984602987766266}]}, {"text": "Afterwards, the pairs of adjectives that matched with any of the defined patterns were extracted using regular expressions.", "labels": [], "entities": []}, {"text": "Finally, two experiments were performed.", "labels": [], "entities": []}, {"text": "In the first one, the bootstrapping algorithm was applied using as seed the opinion lexicon iSOL, and in the second the opinion lexicon resultant from applying the TF method with the best ratio (eSOLMovie) was employed as seed.", "labels": [], "entities": []}, {"text": "In order to evaluate the experiments we used the traditional measures employed in text classification: precision (P), recall (R), F1 and Accuracy.", "labels": [], "entities": [{"text": "text classification", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7146190255880356}, {"text": "precision (P)", "start_pos": 103, "end_pos": 116, "type": "METRIC", "confidence": 0.944274514913559}, {"text": "recall (R)", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9602041989564896}, {"text": "F1", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.9987336993217468}, {"text": "Accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.998719334602356}]}, {"text": "On the other hand, to calculate the polarity (p) of a review (r) with each lexicon, we take into account the total number of positive words (#positive) and the total number of negative words (#negative) within the review, according to the following strategy: As the baseline of our experimentation we took the general purpose lexicon iSOL, in order to adapt it to the movie domain with the proposed approaches and with the combination of them.", "labels": [], "entities": []}, {"text": "The result of the polarity classification of the documents following the strategy defined previously and using iSOL is of 62.95%, in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9983329176902771}]}, {"text": "Regarding the Term Frequency methodology, we first carried out different experiments in order to determine the best ratio to use in our final experiment combining both strategies.", "labels": [], "entities": [{"text": "Term Frequency", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.8074932396411896}]}, {"text": "Thus, after testing several ratios we determined that the best one is obtained using the ratio n=4.", "labels": [], "entities": []}, {"text": "Therefore, this lexicon was taken as the seed list for the combined experiment with bootstrapping.", "labels": [], "entities": []}, {"text": "shows the results obtained over the MC corpus using iSOL (domain independent) and eSOLMovie n lexicons (adapted to the movie domain with the ratios n=3,4,5,6).", "labels": [], "entities": [{"text": "MC corpus", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.9070082008838654}]}, {"text": "In relation to the Bootstraping strategy, it found 1,841 \"y\"/\"e\" patterns and 39 \"pero\"/\"aunque\" patterns and it was tested using iSOL and eSOLMovie 4 as seeds.", "labels": [], "entities": []}, {"text": "The experiment with iSOL detected 292 highly subjective adjectives, inserted 659 adjectives, removed 110 adjectives and converged in 5 iterations, achieving 63.71% of accuracy in the polarity classification of the corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9994804263114929}]}, {"text": "On the other hand, the experiment with eSOLMovie4 detected 296 highly subjective adjectives, appended 626 adjectives, deleted 228 adjectives and converged in 4 iterations, achieving 70.19% of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9986763596534729}]}, {"text": "shows the results achieved in the classification at the document level of the MC corpus using iSOL (domain independent) adapted to the movie domain with the BS approach, with the TF method (using the best ratio) and with the combination of both strategies (the bootstrapping algorithm over eSOLMovie4).", "labels": [], "entities": [{"text": "MC corpus", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.7539624571800232}]}, {"text": "shows a summary of the results obtained and presents the total number of positive and negative polar terms inserted and eliminated from the original lexicon after applying each method.", "labels": [], "entities": []}, {"text": "As we can see, the results obtained with the TF method (eSOLMovie n ) are very promising.", "labels": [], "entities": []}, {"text": "It improves the accuracy of the classification with respect to the general purpose lexicon (iSOL) by 10.29%, inserting only 132 positive words and 126 negative words (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9995409250259399}]}, {"text": "However, the restriction of this approach is that we need a corpus previously tagged with the polarity of the documents.", "labels": [], "entities": []}, {"text": "Moreover, this strategy only appends new words to the original lexicon and sometimes the new terms introduce noise (for example, we consider that the words \"f\u00e1cil\" (easy) and \"r\u00e1pido\" (fast) could not be indicators of negative opinion in the movie domain, see).", "labels": [], "entities": []}, {"text": "Therefore, we also decided to conduct experiments with a technique based on bootstrapping that does not require an annotated corpus and that not only appends words but also removes some of them.", "labels": [], "entities": []}, {"text": "The application of the BS approach for the adaptation of iSOL to the movie domain also achieves an improvement in the classification (iSOL + bootstrapping) over the baseline (iSOL), although this improvement is not as great as we expected (it is only about 1.21%, see).", "labels": [], "entities": []}, {"text": "We think that one of the reasons could be that in this approach we have only used patterns for the extraction of adjectives, while the TF method appends a word independently of its PoS.", "labels": [], "entities": []}, {"text": "Furthermore, we think that the fact of not only inserting but also removing adjectives would be promising but if we take a look at some of the words removed), there are adjectives that a priori it seems they should not have been eliminated (for example, the adjectives \"agradable\" (pleasant) and \"espectacular\" (spectacular) of the positive list).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results obtained using the different lexicons eSOLMovie n .", "labels": [], "entities": []}, {"text": " Table 2: Results obtained with the different approaches.", "labels": [], "entities": []}, {"text": " Table 4: Total of positive/negative words inserted into iSOL and removed from iSOL with the different approaches.", "labels": [], "entities": []}]}