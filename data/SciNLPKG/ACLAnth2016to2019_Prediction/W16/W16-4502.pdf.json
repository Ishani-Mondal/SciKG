{"title": [{"text": "Neural Network Language Models for Candidate Scoring in Hybrid Multi-System Machine Translation", "labels": [], "entities": [{"text": "Multi-System Machine Translation", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.6691105167071024}]}], "abstractContent": [{"text": "This paper presents the comparison of how using different neural network based language modelling tools for selecting the best candidate fragments affects the final output translation quality in a hybrid multi-system machine translation setup.", "labels": [], "entities": []}, {"text": "Experiments were conducted by comparing perplexity and BLEU scores on common test cases using the same training data set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9943089485168457}]}, {"text": "A 12-gram statistical language model was selected as a baseline to oppose three neural network based models of different characteristics.", "labels": [], "entities": []}, {"text": "The models were integrated in a hybrid system that depends on the perplexity score of a sentence fragment to produce the best fitting translations.", "labels": [], "entities": []}, {"text": "The results show a correlation between language model perplexity and BLEU scores as well as overall improvements in BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9982158541679382}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9944561123847961}]}], "introductionContent": [{"text": "Multi-system machine translation (MT) is a subset of hybrid MT where multiple MT systems are combined in a single system in order to boost the accuracy and fluency of the translations.", "labels": [], "entities": [{"text": "Multi-system machine translation (MT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7866332679986954}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9985600113868713}]}, {"text": "It is also referred to as multi-engine MT, MT coupling or just MT system combination.", "labels": [], "entities": [{"text": "MT coupling", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.7607785165309906}]}, {"text": "Some recent open-source multisystem MT (MSMT) approaches tend to use statistical language models (LMs) for scoring and comparing candidate translations or translation fragments.", "labels": [], "entities": [{"text": "open-source multisystem MT (MSMT)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.701583057641983}]}, {"text": "It is understandable, because the statistical approaches have been dominant for the past decades.", "labels": [], "entities": []}, {"text": "Whereas lately, neural networks (NNs) have been showing increasingly greater potential in modelling long distance dependencies in data when compared to state of the art statistical models.", "labels": [], "entities": []}, {"text": "Therefore, the aim of this research is to utilise this potential in combining translations.", "labels": [], "entities": []}, {"text": "Since LMs are probability distributions over sequences of words, they area great tool for estimating the relative likelihood of whether some sequence of words belongs to a certain language.", "labels": [], "entities": []}, {"text": "Sentence perplexity -a probability score that can be generated by querying a LM -has been proven to correlate with human judgments close to the BLEU score (), that has become the main metric for scoring MT, and is a good evaluation method for MT without reference translations).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 144, "end_pos": 154, "type": "METRIC", "confidence": 0.9811957776546478}, {"text": "MT", "start_pos": 203, "end_pos": 205, "type": "TASK", "confidence": 0.928024172782898}, {"text": "MT", "start_pos": 243, "end_pos": 245, "type": "TASK", "confidence": 0.9928570985794067}]}, {"text": "It has been also used in other previous attempts of MSMT to score output from different MT engines as mentioned by and.", "labels": [], "entities": [{"text": "MSMT", "start_pos": 52, "end_pos": 56, "type": "TASK", "confidence": 0.9404560327529907}, {"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9275856614112854}]}, {"text": "Most recently, different order LMs have been used in open-source MSMT approaches like ChunkMT (.", "labels": [], "entities": [{"text": "ChunkMT", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9589774012565613}]}, {"text": "This system and the statistical model from) that it uses will be treated as the baseline for further experiments.", "labels": [], "entities": []}, {"text": "This paper presents an enrichment of the existing MSMT tool with the addition of neural language models.", "labels": [], "entities": []}, {"text": "The experiments described use multiple combinations of outputs from online MT sources.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9635634422302246}]}, {"text": "ExThis work is licenced under a Creative Commons Attribution 4.0 International License.", "labels": [], "entities": [{"text": "ExThis work", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9185099601745605}]}, {"text": "License details: http://creativecommons.org/licenses/by/4.0/ periments described in this paper are performed for English-Latvian.", "labels": [], "entities": []}, {"text": "Translating from and to other languages is supported, but it has some limitations as described in the original paper.", "labels": [], "entities": []}, {"text": "The code of the developed system is freely available at GitHub 1 . The structure of this paper is as following: Section 2 summarizes related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the architecture of the baseline system.", "labels": [], "entities": []}, {"text": "Section 4 outlines the LM toolkits that are used in the experiments and section 5 provides the experiment setup and results.", "labels": [], "entities": []}, {"text": "Finally, conclusions and aims for further directions of work are summarized.", "labels": [], "entities": []}], "datasetContent": [{"text": "To justify using different language modelling approaches, different language models were trained with the same and similar (half of the corpus in one case) training data.", "labels": [], "entities": []}, {"text": "Since Char-RNN achieved the best results, several in-depth experiments were conducted using just this tool with varying training dataset sizes (for faster training) and NN layer combinations.", "labels": [], "entities": []}, {"text": "shows how the network evolves in a setup with two 512-neuron layers.", "labels": [], "entities": []}, {"text": "This experiment was conducted on a smaller dataset -only 1/6 th of the corpus -allowing it to run for more epochs without early stopping.", "labels": [], "entities": []}, {"text": "The perplexity on test data gradually decreased, reaching a lowest score of 22.18..", "labels": [], "entities": [{"text": "perplexity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9733946323394775}]}, {"text": "Changes of training loss and perplexity when training a two-layer Char-RNN with 512 neurons on 500 000 sentences.", "labels": [], "entities": []}, {"text": "Another variation for training a LM with Char-RNN is shown in.", "labels": [], "entities": []}, {"text": "Here 1/3 rd of the corpus was used to train a 3-layer RNN with 1,024 neurons per layer.", "labels": [], "entities": []}, {"text": "The lowest achieved perplexity was 21.23 after training one day on a GPU.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9262605309486389}]}, {"text": "The last column of shows differences in BLEU scores when NN LMs were used.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.973486065864563}]}, {"text": "Correlation between LM perplexity and the resulting BLEU score is visible as well as a slight improvement in the overall result.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9851734638214111}]}, {"text": "Again, due to the outstanding scores of Char-RNN models, they were inspected closer to see how BLEU changes along with perplexity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9989080429077148}]}, {"text": "The following charts show how perplexity correlates with BLEU in translation test cases on the general domain and legal domain test datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9978305697441101}, {"text": "legal domain test datasets", "start_pos": 114, "end_pos": 140, "type": "DATASET", "confidence": 0.6434002965688705}]}, {"text": "represents results from evaluating a combination of Google and Bing (BG) online MT translations (denoted with darker blue colours) and a combination of Hugo and Yandex (HY) online MT (brighter blue colours) on the general domain test dataset.", "labels": [], "entities": [{"text": "MT translations", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.8074292540550232}, {"text": "Hugo and Yandex (HY) online MT", "start_pos": 152, "end_pos": 182, "type": "DATASET", "confidence": 0.804531067609787}, {"text": "general domain test dataset", "start_pos": 214, "end_pos": 241, "type": "DATASET", "confidence": 0.7193177044391632}]}, {"text": "The trend lines (dotted) indicate that for this dataset the combination of BG stays mostly stable but the combination of HY gradually improves as the perplexity of the LM gets lower.", "labels": [], "entities": [{"text": "BG", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9899925589561462}, {"text": "HY", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.8177003264427185}]}, {"text": "Whereas shows results of combining the same MT on the legal domain test dataset.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.863743007183075}, {"text": "legal domain test dataset", "start_pos": 54, "end_pos": 79, "type": "DATASET", "confidence": 0.6598903983831406}]}, {"text": "In this case, while perplexity becomes lower at each time step, the linear trend line for BLEU score of the BG hybrid system does not show a tendency towards climbing higher.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9802221655845642}]}, {"text": "As opposed to the BLEU score trend line for HY hybrid system, that showcases improvement along with perplexity.", "labels": [], "entities": [{"text": "BLEU score trend line", "start_pos": 18, "end_pos": 39, "type": "METRIC", "confidence": 0.9698449820280075}]}], "tableCaptions": [{"text": " Table 1 shows differences in perplex- ity evaluations that outline the superiority of NN LMs. It also shows that the statistical model is much  faster to train on a CPU and that NN LMs train more efficiently on GPUs.", "labels": [], "entities": []}]}