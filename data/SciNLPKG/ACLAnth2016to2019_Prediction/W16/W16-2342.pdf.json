{"title": [], "abstractContent": [{"text": "Recently, the capability of character-level evaluation measures for machine translation output has been confirmed by several metrics.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.8364437023798624}]}, {"text": "This work proposes translation edit rate on character level (CharacTER), which calculates the character level edit distance while performing the shift edit on word level.", "labels": [], "entities": [{"text": "translation edit rate", "start_pos": 19, "end_pos": 40, "type": "METRIC", "confidence": 0.8361765146255493}, {"text": "CharacTER)", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9185447990894318}]}, {"text": "The novel metric shows high system-level correlation with human rankings, especially for morphologically rich languages.", "labels": [], "entities": []}, {"text": "It outperforms the strong CHRF by up to 7% correlation on different metric tasks.", "labels": [], "entities": [{"text": "CHRF", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.7956866025924683}]}, {"text": "In addition, we apply the hypothesis sentence length for normalizing the edit distance in CharacTER, which also provides significant improvements compared to using the reference sentence length.", "labels": [], "entities": [{"text": "CharacTER", "start_pos": 90, "end_pos": 99, "type": "DATASET", "confidence": 0.8088526129722595}]}], "introductionContent": [{"text": "The approaches for automatic evaluation of machine translation facilitated the development of statistical machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7414140552282333}, {"text": "statistical machine translation", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.703415165344874}]}, {"text": "They provide objective evaluation criteria for the translation results, and avoid the tedious and expensive manual evaluation.", "labels": [], "entities": [{"text": "translation", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.967503011226654}]}, {"text": "Currently the most commonly applied evaluation measures are the Bilingual Evaluation Understudy (BLEU) () and the Translation Edit Rate (TER)) evaluation indicators.", "labels": [], "entities": [{"text": "Bilingual Evaluation Understudy (BLEU)", "start_pos": 64, "end_pos": 102, "type": "METRIC", "confidence": 0.7041439960400263}, {"text": "Translation Edit Rate (TER))", "start_pos": 114, "end_pos": 142, "type": "METRIC", "confidence": 0.8075904498497645}]}, {"text": "Most of the researchers use BLEU and TER as the primary metrics for evaluating their translation hypotheses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9986743927001953}, {"text": "TER", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9951034784317017}]}, {"text": "The aim of the machine translation evaluation is to properly and objectively reflect the achievements and the functionality of machine translation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.8985612591107687}, {"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7942180335521698}]}, {"text": "Through the evaluation, the developers of machine translation systems can learn the problems of the system and keep improving them.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.751719743013382}]}, {"text": "The evaluation metric not only provides the most reliable basis for machine translation systems, but also can be applied as the optimizing criterion in the parameter tuning step like BLEU.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.8226913511753082}, {"text": "BLEU", "start_pos": 183, "end_pos": 187, "type": "METRIC", "confidence": 0.9780071377754211}]}, {"text": "Thus, a good evaluation metric should demonstrate accuracy, universality and applicability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9994567036628723}]}, {"text": "In order to evaluate the applicability of different evaluation metrics, the correlation with human judgement is calculated.", "labels": [], "entities": []}, {"text": "Currently the most common techniques for calculating the correlation between human and automatic evaluations are the Spearman's rank correlation coefficient) and the Pearson product-moment correlation coefficient.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 117, "end_pos": 156, "type": "METRIC", "confidence": 0.6554138779640197}, {"text": "Pearson product-moment correlation coefficient", "start_pos": 166, "end_pos": 212, "type": "METRIC", "confidence": 0.8333678543567657}]}, {"text": "In the recent past, several groups have reported further evaluation metrics, such as BEER () and CHRF, which actually outperformed the classic BLEU and TER metrics on Spearman and Pearson correlation with human judgement.", "labels": [], "entities": [{"text": "BEER", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9991474151611328}, {"text": "CHRF", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9945047497749329}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9975683093070984}, {"text": "TER", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.8969431519508362}]}, {"text": "In this work, we propose a novel translation edit rate on character level (CharacTER), which achieves a better correlation on the system-level for four different morphologically rich languages compared to BEER and CHRF.", "labels": [], "entities": [{"text": "translation edit", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7952786087989807}, {"text": "CharacTER)", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9546246528625488}, {"text": "BEER", "start_pos": 205, "end_pos": 209, "type": "METRIC", "confidence": 0.9461419582366943}]}, {"text": "In addition, we also found that if we apply the hypothesis sentence length instead of reference sentence length to normalize the edit distance, the correlations of TER and CharacTER are improved by up to 9% on different languages.", "labels": [], "entities": [{"text": "TER", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.9981195330619812}, {"text": "CharacTER", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.7958793640136719}]}], "datasetContent": [{"text": "The evaluation metrics are correlated with human rankings by means of Spearman's rank correlation coefficient for the WMT13 task and Pearson product-moment correlation coefficient for the WMT14 task) and WMT15 task) on the system level.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.8610775470733643}, {"text": "Pearson product-moment correlation coefficient", "start_pos": 133, "end_pos": 179, "type": "METRIC", "confidence": 0.9018377065658569}, {"text": "WMT14", "start_pos": 188, "end_pos": 193, "type": "DATASET", "confidence": 0.8774008750915527}, {"text": "WMT15", "start_pos": 204, "end_pos": 209, "type": "DATASET", "confidence": 0.8861737847328186}]}, {"text": "Through the experiments we aim to investigate the following points: \u2022 What is the most suitable threshold value to identify the phrase matching?", "labels": [], "entities": [{"text": "phrase matching", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.6665040105581284}]}, {"text": "\u2022 What shift cost should we apply?", "labels": [], "entities": []}, {"text": "\u2022 Which normalizer performs better?", "labels": [], "entities": []}, {"text": "\u2022 How does CharacTER perform compared to other metrics?", "labels": [], "entities": [{"text": "CharacTER", "start_pos": 11, "end_pos": 20, "type": "TASK", "confidence": 0.7904307246208191}]}], "tableCaptions": [{"text": " Table 1: Average correlations on WMT13 (Spear- man) and WMT14 (Pearson) tasks for different  variants of CharacTER. en-* indicates the av- erage correlation for translations out of English,  while * -en the translations into English. The best  results in each direction are in bold.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.8229565024375916}, {"text": "WMT14", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.8628008365631104}, {"text": "av- erage correlation", "start_pos": 136, "end_pos": 157, "type": "METRIC", "confidence": 0.6786154359579086}]}, {"text": " Table 2: System-level correlations of automatic evaluation metrics and the official WMT human scores.  The best results in each direction are in bold. We calculated the CharacTER and CHRF3 scores and cited  the other scores from the WMT metric papers (", "labels": [], "entities": [{"text": "WMT human scores", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.7686143517494202}, {"text": "CharacTER", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.8377423882484436}, {"text": "CHRF3", "start_pos": 184, "end_pos": 189, "type": "METRIC", "confidence": 0.595636248588562}, {"text": "WMT metric papers", "start_pos": 234, "end_pos": 251, "type": "DATASET", "confidence": 0.8837453524271647}]}, {"text": " Table 3: The preliminary results of the WMT16 metrics task: Absolute Pearson correlation of out-of- English and to-English system-level metric scores. All results are cited from (Bojar et al., 2016).", "labels": [], "entities": [{"text": "WMT16 metrics task", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.5869762301445007}, {"text": "Pearson correlation", "start_pos": 70, "end_pos": 89, "type": "METRIC", "confidence": 0.78264981508255}]}]}