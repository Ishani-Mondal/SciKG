{"title": [{"text": "Predicting Brexit: Classifying Agreement is Better than Sentiment and Pollsters", "labels": [], "entities": [{"text": "Predicting Brexit", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.5732719451189041}, {"text": "Classifying Agreement", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.8498931229114532}]}], "abstractContent": [{"text": "On June 23rd 2016, UK held the referendum which ratified the exit from the EU.", "labels": [], "entities": [{"text": "UK", "start_pos": 19, "end_pos": 21, "type": "DATASET", "confidence": 0.953229546546936}]}, {"text": "While most of the traditional pollsters failed to forecast the final vote, there were online systems that hit the result with high accuracy using opinion mining techniques and big data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9943293929100037}]}, {"text": "Starting one month before, we collected and monitored millions of posts about the referendum from social media conversations, and exploited Natural Language Processing techniques to predict the referendum outcome.", "labels": [], "entities": []}, {"text": "In this paper we discuss the methods used by traditional pollsters and compare it to the predictions based on different opinion mining techniques.", "labels": [], "entities": []}, {"text": "We find that opinion mining based on agreement/disagreement classification works better than opinion mining based on polarity classification in the forecast of the referendum outcome.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.773139625787735}, {"text": "agreement/disagreement classification", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.6275888085365295}]}], "introductionContent": [{"text": "The outcome of the 2016 EU referendum did not only spell disaster for the UK government and the Remain campaign.", "labels": [], "entities": []}, {"text": "It also amounted to a Press Release disaster for commercial pollsters.", "labels": [], "entities": []}, {"text": "YouGov, Populus, ComRes, ORB, Ipsos-Mori and Survation, all failed to correctly predict the outcome.", "labels": [], "entities": [{"text": "YouGov", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9808419942855835}, {"text": "Populus", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.9459812641143799}, {"text": "ORB", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.6740643978118896}, {"text": "Ipsos-Mori", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.8013743758201599}, {"text": "Survation", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.8204502463340759}]}, {"text": "Of the larger pollsters, only TNS and Opinium correctly called the outcome, although still underestimating the Leave vote.", "labels": [], "entities": [{"text": "TNS", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.8613510131835938}]}, {"text": "This general failure, moreover, follows hard on the heels of similar failures in both the 2010 and 2015 General Elections, and public faith in commercial polling has taken another serious blow.", "labels": [], "entities": [{"text": "General", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.7862469553947449}]}, {"text": "By contrast, predictions using Natural Language Processing (NLP) and Computational Linguistics (CL) techniques, such as opinion mining, proved to be much more reliable.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 120, "end_pos": 134, "type": "TASK", "confidence": 0.8226716816425323}]}, {"text": "In what follows we will refer to opinion mining as the automatic task of assigning a polarity to a topic in context (); to polarity classification and sentiment analysis as the tasks for the extraction of emotive polarity or scores from text; to agreement/disagreement classification as the task of recognizing the opinion of a message towards others in a thread () or pairs of replying posts (; and to stance classification as the task of recognizing the overall opinion of an author from text.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7459194958209991}, {"text": "polarity classification", "start_pos": 123, "end_pos": 146, "type": "TASK", "confidence": 0.7756876051425934}, {"text": "sentiment analysis", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.8083638548851013}, {"text": "agreement/disagreement classification", "start_pos": 246, "end_pos": 283, "type": "TASK", "confidence": 0.6336921006441116}, {"text": "stance classification", "start_pos": 403, "end_pos": 424, "type": "TASK", "confidence": 0.8735450208187103}]}, {"text": "In this paper we discuss the methods used by traditional pollsters and compare them to the predictions based on different opinion mining techniques, in particular polarity classification and agreement/disagreement classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 163, "end_pos": 186, "type": "TASK", "confidence": 0.7076444029808044}, {"text": "agreement/disagreement classification", "start_pos": 191, "end_pos": 228, "type": "TASK", "confidence": 0.575541578233242}]}, {"text": "We describe a system that predicted the outcome of the referendum correctly to within one-tenth of a percentage point.", "labels": [], "entities": []}, {"text": "Unlike many political prediction papers that provide post-hoc analyses), our final prediction was publicly available the day before the referendum (i.e. 22nd of June) on our referendum monitoring website; thus, it is indeed the prediction of the future result.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: in Section 2 we report the techniques used by commercial pollsters to forecast the votes, in Section 3 we report related work on forecasts from social media using opinion mining.", "labels": [], "entities": []}, {"text": "Section 4 describes the methodology we have used for data collection and the system for the prediction of the referendum outcome.", "labels": [], "entities": [{"text": "prediction of the referendum outcome", "start_pos": 92, "end_pos": 128, "type": "TASK", "confidence": 0.7761245965957642}]}, {"text": "In the same section we provide analyses of the representativeness of our data sources and the methods for topic labeling and automatic annotation of opinions.", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.760819137096405}]}, {"text": "Finally, in Section 5 we analyze and compare polling and NLP-based predictions.", "labels": [], "entities": []}, {"text": "We hope that the results and discussions presented in this paper will contribute to pushing NLP further in the exploitation of para-semantic analysis techniques to forecast and understand collective decisions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Overview of the results obtained and methods adopted by traditional pollsters for the referendum.", "labels": [], "entities": []}, {"text": " Table 3: Counts and percentages for Leave and Remain as predicted by sentiment polarity prediction system", "labels": [], "entities": [{"text": "Leave", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9977523684501648}, {"text": "Remain", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.680862307548523}, {"text": "sentiment polarity prediction", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.7846486568450928}]}]}