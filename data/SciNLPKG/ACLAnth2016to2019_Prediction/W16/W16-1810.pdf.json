{"title": [], "abstractContent": [{"text": "Recent works in Natural Language Processing (NLP) using neural networks have focused on learning dense word representations to perform classification tasks.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.7472966114679972}]}, {"text": "When dealing with phrase prediction problems, is is common practice to use special tagging schemes to identify segments boundaries.", "labels": [], "entities": [{"text": "phrase prediction", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.8744692504405975}]}, {"text": "This allows these tasks to be expressed as common word tagging problems.", "labels": [], "entities": [{"text": "common word tagging", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.6776092946529388}]}, {"text": "In this paper, we propose to learn fixed-size representations for arbitrarily sized chunks.", "labels": [], "entities": []}, {"text": "We introduce a model that takes advantage of such representations to perform phrase tagging by directly identifying and classifying phrases.", "labels": [], "entities": [{"text": "phrase tagging", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.8093318939208984}]}, {"text": "We evaluate our approach on the task of multiword expression (MWE) tagging and show that our model outperforms the state-of-the-art model for this task.", "labels": [], "entities": [{"text": "multiword expression (MWE) tagging", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.7163630624612173}]}], "introductionContent": [{"text": "Traditional NLP tasks such as part-of-speech (POS) tagging or semantic role labeling (SRL) consists in tagging each word in a sentence with a tag.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.6876872062683106}, {"text": "semantic role labeling (SRL)", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.7937833070755005}]}, {"text": "Another class of problems such as Named Entity Recognition (NER) or shallow parsing (chunking) consists in identifying and labeling phrases (i.e. groups of words) with predefined tags.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.805707057317098}, {"text": "shallow parsing (chunking", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.7807743847370148}]}, {"text": "Such tasks can be expressed as word classification problems by identifying the phrase boundaries instead of directly identifying the whole phrases.", "labels": [], "entities": [{"text": "word classification", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7423330545425415}]}, {"text": "In practice, this consists in prefixing every tag with an extra-label indicating the position of the word inside a phrase (at the beginning (B), inside (I), at the end (E), single word (S) or not in a phrase).", "labels": [], "entities": []}, {"text": "Different schemes have been used in the literature, * All research was conducted at the Idiap Research Institute, before Ronan Collobert joined Facebook AI Research such as the IOB2, IOE1 and IOE2 schemes) or IOBES scheme () with no clear predominance.", "labels": [], "entities": [{"text": "Idiap Research Institute", "start_pos": 88, "end_pos": 112, "type": "DATASET", "confidence": 0.9784873127937317}, {"text": "Ronan Collobert", "start_pos": 121, "end_pos": 136, "type": "DATASET", "confidence": 0.8811823129653931}]}, {"text": "These tasks have been tackled using various machine learning methods such as Support Vector Machines (SVM) for POS tagging () or chunking (), second order random fields for chunking or a combination of different classifiers for NER (.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.8076092302799225}]}, {"text": "All these approaches use carefully selected handcrafted features.", "labels": [], "entities": []}, {"text": "Recent studies in NLP introduced neural network based systems that can be trained in an end-to-end manner, using minimal prior knowledge.", "labels": [], "entities": []}, {"text": "These models take advantage of continuous representations of words.", "labels": [], "entities": []}, {"text": "(2011) the authors proposed a deep neural network, which learns the word representations (the features) and produces IOBES-prefixed tags discriminatively trained in an end-to-end manner.", "labels": [], "entities": []}, {"text": "This system is trained using a conditional random field () that accounts for the structure of the sentence.", "labels": [], "entities": []}, {"text": "This architecture has been applied to various NLP tasks, such as POS tagging, NER or semantic role labeling and achieves state-of-the-art performance in all of them.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.7865274846553802}, {"text": "semantic role labeling", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.603823333978653}]}, {"text": "In this paper, we propose to learn fixed-size continuous representations of arbitrarily sized chunks by composing word embeddings.", "labels": [], "entities": []}, {"text": "These representations are used to directly classify phrases without using the classical IOB(ES) prefixing step.", "labels": [], "entities": []}, {"text": "The proposed approach is evaluated on the task of multiword expression (MWE) tagging.", "labels": [], "entities": [{"text": "multiword expression (MWE) tagging", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.6942777087291082}]}, {"text": "Using the SPRML 2014 data for French MWE tagging ( , we show that our phrase representations are able to capture enough knowledge to perform on par with the IOBES-based model of applied to MWE tagging.", "labels": [], "entities": [{"text": "SPRML 2014 data", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8718831539154053}, {"text": "French MWE tagging", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7046862045923868}, {"text": "MWE tagging", "start_pos": 189, "end_pos": 200, "type": "TASK", "confidence": 0.8881514370441437}]}, {"text": "Furthermore, we show that our system outperforms the winner of the SPMRL (Syntactic Parsing of Morphologicaly Rich Language) 2013 shared task for MWE tagging) which is currently the best published system.", "labels": [], "entities": [{"text": "SPMRL (Syntactic Parsing of Morphologicaly Rich Language) 2013 shared task", "start_pos": 67, "end_pos": 141, "type": "TASK", "confidence": 0.778539165854454}, {"text": "MWE tagging", "start_pos": 146, "end_pos": 157, "type": "TASK", "confidence": 0.808317631483078}]}], "datasetContent": [{"text": "We evaluate the performance of the proposed network on MWE tagging using the three metrics described in , reporting for each of them the recall, precision and F-score.", "labels": [], "entities": [{"text": "MWE tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9393631517887115}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9997231364250183}, {"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9984095692634583}, {"text": "F-score", "start_pos": 159, "end_pos": 166, "type": "METRIC", "confidence": 0.9976087808609009}]}, {"text": "MWE correspond to the full MWEs, in which a predicted MWE counts as correct if it has the correct span (same group as in the gold data).", "labels": [], "entities": []}, {"text": "MWE+POS is defined in the same fashion, except that the predicted MWE counts as correct if it has both correct span and correct POS tag.", "labels": [], "entities": []}, {"text": "COMP correspond to the non-head components of MWEs: a non-head component of MWE counts as correct if it is attached to the head of the MWE, with the specific label indicating that it is part of an MWE.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of k-sized chunks in the training  corpus", "labels": [], "entities": []}, {"text": " Table 2: Results on the test corpus (4043 MWEs)  in terms of F-measure. WI stands for word initial- ization.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9947677850723267}]}, {"text": " Table 3: Results on the test corpus (4043 MWEs)  in terms of F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.992049515247345}]}]}