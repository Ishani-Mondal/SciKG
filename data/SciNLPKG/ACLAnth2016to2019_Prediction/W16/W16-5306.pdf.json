{"title": [{"text": "The Power of Language Music: Arabic Lemmatization through Patterns", "labels": [], "entities": []}], "abstractContent": [{"text": "The interaction between roots and patterns in Arabic has intrigued lexicographers and morphol-ogists for centuries.", "labels": [], "entities": []}, {"text": "While roots provide the consonantal building blocks, patterns provide the syllabic vocalic moulds.", "labels": [], "entities": []}, {"text": "While roots provide abstract semantic classes, patterns realize these classes in specific instances.", "labels": [], "entities": []}, {"text": "In this way both roots and patterns are indispensable for understanding the derivational, morphological and, to some extent, the cognitive aspects of the Arabic language.", "labels": [], "entities": []}, {"text": "In this paper we perform lemmatization (a high-level lexical processing) without relying on a lookup dictionary.", "labels": [], "entities": []}, {"text": "We use a hybrid approach that consists of a machine learning classifier to predict the lemma pattern fora given stem, and mapping rules to convert stems to their respective lemmas with the vocalization defined by the pattern.", "labels": [], "entities": []}], "introductionContent": [{"text": "Roots and patterns in Arabic are essential for understanding the derivational aspects of the lexicon.", "labels": [], "entities": []}, {"text": "In Arabic, roots and patterns function like metadata.", "labels": [], "entities": []}, {"text": "Lemmas or lexical entries recorded in dictionaries only represent a static lexicon at a fixed point in time, while roots and patterns (as part of the mental lexicon) have stronger dynamic role in the creation of new entries and the prediction of their semantic paradigms.", "labels": [], "entities": []}, {"text": "So, derivation in Arabic is about the construction of a large semantic forests of concepts that are related through a single grand-parent or a super-lemma, that is the root.", "labels": [], "entities": []}, {"text": "The power of roots and patterns has not yet been fully utilized or understood in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 81, "end_pos": 114, "type": "TASK", "confidence": 0.6918251514434814}]}, {"text": "They are traditionally considered as a convenient way for listing words in dictionaries or teaching Arabic for second language learners, but they have a great potential for automatic processing, due to their strong generalizing capacity and their function as an instrument for decomposing word forms.", "labels": [], "entities": [{"text": "listing words in dictionaries", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.85206738114357}]}, {"text": "Roots and patterns are the hidden layers through which Arabic speakers organize, memorize and access the Arabic lexicon.", "labels": [], "entities": []}, {"text": "In many NLP tasks, using surface word forms is found to be inefficient as it significantly adds to sparsity, especially in highly inflected languages; thus, some form of normalization is necessary.", "labels": [], "entities": []}, {"text": "Normalization in general, and lemmatization in particular, are meant to reduce the variability in word forms by collapsing related words.", "labels": [], "entities": []}, {"text": "This has been shown to be beneficial for information retrieval), parsing, summarization), document clustering (), keyphrase extraction (El-Shishtawy and Al-Sammak, 2012), and text indexing and classification.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7920789420604706}, {"text": "parsing", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9780884385108948}, {"text": "summarization", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.9155187606811523}, {"text": "document clustering", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7959144413471222}, {"text": "keyphrase extraction", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.7732565104961395}, {"text": "text indexing", "start_pos": 175, "end_pos": 188, "type": "TASK", "confidence": 0.7880955636501312}]}, {"text": "From a lexical point of view, normalization can be conducted at the level of the root, stem or lemma.", "labels": [], "entities": []}, {"text": "Lemmatization relates surface forms to their canonical base representations (or dictionary lookup form).", "labels": [], "entities": []}, {"text": "It is the inverse of inflection (), as it renders words to a default and uninflected form, or as is the case with Arabic, a least marked form.", "labels": [], "entities": []}, {"text": "A lemma is the common denominator () of a set of forms that share the same semantic, morphological and syntactic composition, where it represents the least marked word form without any inflectional affixes.", "labels": [], "entities": []}, {"text": "In Arabic, a verb lemma is chosen to be the perfective, indicative, 3rd person, masculine, and singular such as $akara 1 \"to thank\".", "labels": [], "entities": []}, {"text": "Whereas a nominal lemma (namely, nouns and adjectives) is in the nominative, singular, masculine (where possible), such as TAlib \"student\".", "labels": [], "entities": [{"text": "TAlib", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.7880663871765137}]}, {"text": "Stemming and lemmatization are quite distinct processes, albeit frequently confused the terms are sometimes used interchangeably ().", "labels": [], "entities": []}, {"text": "Stemming strips off prefixes and suffixes leaving a bare stem with no guarantee that the resulting form is a valid standalone word, while lemmatization renders word forms (inflected forms) in their dictionary citation forms.", "labels": [], "entities": []}, {"text": "To illustrate this with an example, consider the Arabic verb form 'yanotaZiruwn' \"they wait\".", "labels": [], "entities": []}, {"text": "Stemming will remove the present prefix 'ya' and the plural suffix 'uwn' and leave 'notaZir' which is a non-word in Arabic.", "labels": [], "entities": []}, {"text": "By contrast, full lemmatization will reveal that the word has gone through a morphological alteration process and return the canonical 'AinotaZar' \"to wait\" as the base form.", "labels": [], "entities": []}, {"text": "The root, by contrast, is the three (or four) radical based form from which a word is formed, that is nZr for the above example.", "labels": [], "entities": []}, {"text": "assume that the relationship between a root and a lemma is purely diachronic (related to the historical derivation of words and their semantic net).", "labels": [], "entities": []}, {"text": "However, we show that the relationship is not only diachronic, but also synchronic related to inflection, as root radicals remain the pivots for inflectional affixes.", "labels": [], "entities": []}, {"text": "In our approach we treat the lemmatization as a classification problem relying mainly on word patterns.", "labels": [], "entities": []}, {"text": "Unlike previous work, we do not use lexicons or morphological rules or analyzers.", "labels": [], "entities": []}, {"text": "Our methodology is based on the powerful and instrumental component that patterns play in the Arabic morphology system.", "labels": [], "entities": []}, {"text": "For example, verb lemmas are derived from roots selected from 10 morphological patterns and 35 phonological patterns, see Section 3.1.", "labels": [], "entities": []}, {"text": "Additionally, verbs are also inflected for the imperfective, passive voice and imperative through patterns.", "labels": [], "entities": []}, {"text": "Noun and adjective lemmas are similarly derived either from roots or from verbs through patterns.", "labels": [], "entities": []}, {"text": "Nouns are also inflected for the plural (broken plural) selected from a large set of 83 phonological patterns.", "labels": [], "entities": []}, {"text": "This paper shows how the process of derivation is closely tied to a compact list of patterns with a backward and forward movement directions.", "labels": [], "entities": [{"text": "derivation", "start_pos": 36, "end_pos": 46, "type": "TASK", "confidence": 0.9716439247131348}]}, {"text": "For the benefit of the research community we make our list of morpho-phonological patterns publicly available for download 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "It is hard to directly predict lemmas from stems due to the very fine granularity level.", "labels": [], "entities": []}, {"text": "Thus, we generate patterns for all stems and lemmas, which are relatively limited in numbers in comparison to the actual lexical items rendering the search space for the classifier, therefore more manageable.", "labels": [], "entities": []}, {"text": "We have two types of patterns: a) automatic patterns generated by replacing all consonants in a word with placeholders, and b) morpho-phonological patterns which only replaces the consonantal base with placeholders.", "labels": [], "entities": []}, {"text": "For example, the verb inoTalaq will be replaced by .i.o.a.a. in the automatic pattern, while it will be replaced by ino.a.a. in the morpho-phonological patterns which correctly identifies the sequence ino outside of the consonantal base.", "labels": [], "entities": []}, {"text": "The number of unique automatic lemma patterns is 77, the number of automatic stem patterns is 225, and the number of morpho-phonological lemma patterns is 43 for verbs and 209 for nominals.", "labels": [], "entities": []}, {"text": "Then for each stem pattern, we predict the Lemma-pattern (either automatic or morpho-phonological pattern).", "labels": [], "entities": []}, {"text": "The features used in our classifier are the stem, autoStemPattern (the pattern automatically generated from the stem by replacing consonants with placeholders), and affixes (PREF 0 , PREF 1 , . .", "labels": [], "entities": [{"text": "autoStemPattern", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.9669536352157593}]}, {"text": ", PREF n , SUFF 0 , SUFF 1 , . .", "labels": [], "entities": [{"text": "PREF", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.7533624768257141}]}, {"text": ", SUFF m ), where n and mare based on the maximum number of prefixes and suffixes, respectively, in the data.", "labels": [], "entities": [{"text": "SUFF", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9827809929847717}]}, {"text": "Note that the prefixes and suffixes refer to both clitics (coordinating conjunctions, prepositions and particles) and morphological markers (related to number, gender, person, aspect, mood, etc.), as depicted which shows the affixes for verbs.", "labels": [], "entities": []}, {"text": "Then we compare the performance of two machine learning classifiers to predict the morphophonological pattern or the automatic pattern of the lemma for each stem using the features specified above.", "labels": [], "entities": []}, {"text": "The results are discussed in Section 4.", "labels": [], "entities": []}, {"text": "The method we develop is meant as a proof-of-concept that shows the usability of patterns in the subtask of retrieving lemmas.", "labels": [], "entities": []}, {"text": "It takes as input tokenized and POS-tagged texts.", "labels": [], "entities": []}, {"text": "Due to the fact that we are not developing a full scale morphological processor, we cannot compare our results with state-of-the-art applications, such as MADA (, and therefore we use intrinsic evaluation.", "labels": [], "entities": [{"text": "MADA", "start_pos": 155, "end_pos": 159, "type": "DATASET", "confidence": 0.7472417950630188}]}, {"text": "We evaluate our approach on the diacritized and undiacritized version of the ATB.", "labels": [], "entities": [{"text": "ATB", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.6079086661338806}]}, {"text": "For the baseline we consider the stem as the lemma without any further processing.", "labels": [], "entities": []}, {"text": "Our data contains 128,293 nouns, 31,666 verbs, and 35,176 adjectives.", "labels": [], "entities": []}, {"text": "We divide the data into 80% for training, 10% for development, and 10% for testing.", "labels": [], "entities": []}, {"text": "The results in this section are reported against the test set.", "labels": [], "entities": []}, {"text": "Our method consists of two steps.", "labels": [], "entities": []}, {"text": "First we use ML classifier to predict the lemma pattern fora given stem.", "labels": [], "entities": []}, {"text": "For the ML step we notice that the results in general are remarkably better than the baseline.", "labels": [], "entities": [{"text": "ML", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9365870952606201}]}, {"text": "We conduct two classification experiments.", "labels": [], "entities": []}, {"text": "In the first we take the set of morpho-phonological patterns (morphPtrn) as the prediction class, and in the second we use patterns automatically generated from the lemmas by removing cardinal letters (autoPtrn).", "labels": [], "entities": []}, {"text": "For example, an autoPtrn can be generated from the lemma AinotiqAl \"moving\", which can be transformed into an autoPtrn by replacing all consonants with a placeholder \"Ai.o.i.A.\".", "labels": [], "entities": []}, {"text": "This allows us to automatically generate a list of patterns without being constrained by a manually constructed list.", "labels": [], "entities": []}, {"text": "We tested two ML algorithms: Decision Trees (C4.5) and Extra Trees classifier.", "labels": [], "entities": []}, {"text": "We notice that using C4.5 produces significantly better results than Extra Trees.", "labels": [], "entities": []}, {"text": "The second step is related to the reconstruction of the actual lemma from combining the stem and the predicted pattern using mapping rules.", "labels": [], "entities": []}, {"text": "The results of this step show a marginal loss on the output of the prediction step.", "labels": [], "entities": []}, {"text": "The experimental results are shown in , and 7, for verbs, nouns and adjectives respectively, and for both diacritized undiacritized words.", "labels": [], "entities": []}, {"text": "The overall results are largely higher than the baseline with diacritized texts.", "labels": [], "entities": []}, {"text": "For example, the baseline for verbs is 72.35% while our best result is 98.23%, with similar results for both adjectives and nouns.", "labels": [], "entities": []}, {"text": "With undiacritized words, the performance of the process varies with the type of entries.", "labels": [], "entities": []}, {"text": "With verbs and nouns our results are higher than the baseline, with adjectives the prediction scores for patterns remains consistently high (mostly above 95%).", "labels": [], "entities": [{"text": "prediction", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9560701251029968}]}, {"text": "But the mapping from pattern to lemma seems to fall below the baseline.", "labels": [], "entities": []}, {"text": "Our justification is that adjectives do not undergo as many non-concatenative derivation.", "labels": [], "entities": []}, {"text": "Moreover, mapping rules for undiacritized adjectives needs more improvement.", "labels": [], "entities": []}, {"text": "One of the interesting results we found in these experiments is that results with autoPtrn are comparable to (and sometimes even better than) morphPtrn which is an indication that patterns are machine learnable and that we do not need to rely solely on hand-crafted lists of Arabic templates.", "labels": [], "entities": [{"text": "autoPtrn", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9019973874092102}]}], "tableCaptions": [{"text": " Table 2: Frequency of alterations in Arabic words", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8545293807983398}]}]}