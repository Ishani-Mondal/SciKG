{"title": [], "abstractContent": [{"text": "Vector space models have become popular in distributional semantics, despite the challenges they face in capturing various semantic phenomena.", "labels": [], "entities": []}, {"text": "We propose a novel probabilistic framework which draws on both formal semantics and recent advances in machine learning.", "labels": [], "entities": []}, {"text": "In particular, we separate predicates from the entities they refer to, allowing us to perform Bayesian inference based on logical forms.", "labels": [], "entities": []}, {"text": "We describe an implementation of this framework using a combination of Restricted Boltz-mann Machines and feedforward neural networks.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate the feasibility of this approach by training it on a parsed corpus and evaluating it on established similarity datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current approaches to distributional semantics generally involve representing words as points in a high-dimensional vector space.", "labels": [], "entities": []}, {"text": "However, vectors do not provide 'natural' composition operations that have clear analogues with operations informal semantics, which makes it challenging to perform inference, or capture various aspects of meaning studied by semanticists.", "labels": [], "entities": []}, {"text": "This is true whether the vectors are constructed using a count approach (e.g. or an embedding approach (e.g., and indeed showed that there are close links between them.", "labels": [], "entities": []}, {"text": "Even the tensorial approach described by and, which naturally captures argument structure, does not allow an obvious account of context dependence, or logical inference.", "labels": [], "entities": []}, {"text": "In this paper, we build on insights drawn from formal semantics, and seek to learn representations which have a more natural logical structure, and which can be more easily integrated with other sources of information.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are to introduce a novel framework for distributional semantics, and to describe an implementation and training regime in this framework.", "labels": [], "entities": []}, {"text": "We present some initial results to demonstrate that training this model is feasible.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report the first experiments carried out within our framework.", "labels": [], "entities": []}, {"text": "As our first attempt at evaluation, we chose to look at two lexical similarity datasets.", "labels": [], "entities": []}, {"text": "The aim of this evaluation was simply to verify that the model was learning something reasonable.", "labels": [], "entities": []}, {"text": "We did not expect this task to illustrate our model's strengths, since we need richer tasks to exploit its full expressiveness.", "labels": [], "entities": []}, {"text": "Both of our chosen datasets aim to evaluate similarity, rather than thematic relatedness: the first is Results are given in table 2.", "labels": [], "entities": [{"text": "similarity", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.948613703250885}]}, {"text": "We also trained's Word2Vec model on the SVO data described in section 4.1, in order to give a direct comparison of models on the same training data.", "labels": [], "entities": [{"text": "SVO data", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.9460931420326233}]}, {"text": "In particular, we used the continuous bag-of-words model with negative sampling, as implemented i\u0148 Reh\u016f\u0159ek and Sojka (2010)'s gensim package, with off-the-shelf hyperparameter settings.", "labels": [], "entities": []}, {"text": "We also converted these to sparse vectors using's algorithm, again using off-the-shelf hyperparameter settings.", "labels": [], "entities": []}, {"text": "To measure similarity of our semantic functions, we treated each function's parameters as a vector and used cosine similarity, for simplicity.", "labels": [], "entities": []}, {"text": "For comparison, we also include the performance of Word2Vec when trained on raw text.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.9494386911392212}]}, {"text": "For SimLex-999, we give the results reported by, where the 2-word window model was the best performing model that they tested.", "labels": [], "entities": []}, {"text": "For WordSim-353, we trained a model on the full WikiWoods text, after stripping all punctuation and converting to lowercase.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9658578634262085}]}, {"text": "We used the gensim implementation with off-the-shelf settings, except for window size (2 or 10) and dimension (200, as recommended by.", "labels": [], "entities": [{"text": "dimension", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9952919483184814}]}, {"text": "In fact, our re-trained model performed better on SimLex-999 than Hill: Spearman rank correlation of different models with average annotator judgements.", "labels": [], "entities": [{"text": "SimLex-999", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.8706531524658203}]}, {"text": "Note that we would like to have a low score on the final column (which measures relatedness, rather than similarity).", "labels": [], "entities": []}, {"text": "flood / water (related verb and noun) .06 flood / water (related nouns) .43 law / lawyer (related nouns) .44 sadness / joy (near-antonyms) .77 happiness / joy (near-synonyms) .78 aunt / uncle (differ in a single feature) .90 cat / dog (differ in many features) .92: Similarity scores for thematically related words, and various types of co-hyponym.", "labels": [], "entities": []}, {"text": "et al. reported (even when we used less preprocessing or a different edition of Wikipedia), although still worse than our sparse SVO Word2Vec model.", "labels": [], "entities": []}, {"text": "It is interesting to note that training Word2Vec on verbs and their arguments gives noticeably better results on SimLex-999 than training on full sentences, even though far less data is being used: \u223c72m tokens, rather than \u223c1000m.", "labels": [], "entities": []}, {"text": "The better performance suggests that semantic dependencies may provide more informative contexts than simple word windows.", "labels": [], "entities": []}, {"text": "This is inline with previous results, such as Levy and Goldberg (2014a)'s work on using syntactic dependencies.", "labels": [], "entities": []}, {"text": "Nonetheless, this result deserves further investigation.", "labels": [], "entities": []}, {"text": "Of all the models we tested, only our semantic function model failed on the relatedness subset of WordSim-353.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.9491468071937561}]}, {"text": "We take this as a positive result, since it means the model clearly distinguishes relatedness and similarity.", "labels": [], "entities": []}, {"text": "Examples of thematically related predicates and various kinds of co-hyponym are given in table 3, along with our model's similarity scores.", "labels": [], "entities": []}, {"text": "However, it is not clear that it is possible, or even desirable, to represent these varied relationships on a single scale of similarity.", "labels": [], "entities": []}, {"text": "For example, it could be sensible to treat aunt and uncle either as synonyms (they refer to relatives of the same degree of relatedness) or as antonyms (they are \"opposite\" in some sense).", "labels": [], "entities": []}, {"text": "Which view is more appropriate will depend on the application, or on the context.", "labels": [], "entities": []}, {"text": "Nouns and verbs are very strongly distinguished, which we would expect given the structure of our model.", "labels": [], "entities": []}, {"text": "This can be seen in the similarity scores between flood and water, when flood is considered either as a verb or as a noun.", "labels": [], "entities": [{"text": "similarity", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9627010226249695}]}, {"text": "8 SimLex-999 generally assigns low scores to nearantonyms, and to pairs differing in a single feature, which might explain why the performance of our model is not higher on this task.", "labels": [], "entities": []}, {"text": "However, the separation of thematically related predicates from co-hyponyms is a promising result.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Spearman rank correlation of different models with average annotator judgements. Note that we  would like to have a low score on the final column (which measures relatedness, rather than similarity).", "labels": [], "entities": []}]}