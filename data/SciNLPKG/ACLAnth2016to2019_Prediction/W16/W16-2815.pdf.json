{"title": [{"text": "Fill the Gap! Analyzing Implicit Premises between Claims from Online Debates", "labels": [], "entities": [{"text": "Analyzing Implicit Premises between Claims from Online Debates", "start_pos": 14, "end_pos": 76, "type": "TASK", "confidence": 0.8504872098565102}]}], "abstractContent": [{"text": "Identifying the main claims occurring across texts is important for large-scale argumentation mining from social media.", "labels": [], "entities": [{"text": "Identifying the main claims occurring across texts", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8414809448378426}, {"text": "argumentation mining", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.8709952533245087}]}, {"text": "However, the claims that users make are often unclear and build on implicit knowledge , effectively introducing a gap between the claims.", "labels": [], "entities": []}, {"text": "In this work, we study the problem of matching user claims to predefined main claims, using implicit premises to fill the gap.", "labels": [], "entities": []}, {"text": "We build a dataset with implicit premises and analyze how human annota-tors fill the gaps.", "labels": [], "entities": []}, {"text": "We then experiment with computational claim matching models that utilize these premises.", "labels": [], "entities": [{"text": "computational claim matching", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.6549995938936869}]}, {"text": "We show that using manually-compiled premises improves similarity-based claim matching and that premises generalize to unseen user claims.", "labels": [], "entities": [{"text": "similarity-based claim matching", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.6352526644865671}]}], "introductionContent": [], "datasetContent": [{"text": "The claim matching task can be approached in a supervised or unsupervised manner.", "labels": [], "entities": [{"text": "claim matching task", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7782740592956543}]}, {"text": "We focus hereon the latter, based on semantic similarity between the claims and the premises.", "labels": [], "entities": []}, {"text": "We think unsupervised claim matching provides a more straightforward and explicit way of incorporating the implicit premises.", "labels": [], "entities": [{"text": "unsupervised claim matching", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.7471091945966085}]}, {"text": "Furthermore, the unsupervised approach better corresponds to the very idea of argumentation, where claims and premises are compared to each other and combined to derive other claims.", "labels": [], "entities": []}, {"text": "We use the implicit premise dataset from Section 3, consisting of 125 claim pairs for each of the four topics.", "labels": [], "entities": []}, {"text": "We use the gap-filling premise sets from annotator A1, who on average has provided the largest number of implicit premises.", "labels": [], "entities": []}, {"text": "We refer to this dataset as the development set.", "labels": [], "entities": []}, {"text": "In addition, we sample and additional test set consisting of 125 pairs for each topic from the dataset of; for claim pairs from this set we have no implicit premises.", "labels": [], "entities": []}, {"text": "We adopt the distributional semantics approach to computing semantic textual similarity.", "labels": [], "entities": [{"text": "computing semantic textual similarity", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.6405398398637772}]}, {"text": "We rely on distributed representation based on the neural network skip-gram model of.", "labels": [], "entities": []}, {"text": "We represent the texts of the claims and the premises by summing up the distributional vectors of their individual words, as the semantic composition of short phrases via simple vector addition has been shown to work).", "labels": [], "entities": []}, {"text": "We measure claim similarity using cosine distance between two vectors.", "labels": [], "entities": []}, {"text": "Inspired by), we also attempted to model claim matching using textual entailment.", "labels": [], "entities": [{"text": "claim matching", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7326742857694626}]}, {"text": "However, our results, obtained using the Excitement Open Platform, were considerably worse than that of distributional similarity models, hence we do not consider them further in this paper.", "labels": [], "entities": []}, {"text": "First, an unsupervised baseline, which simply computes the similarity between the user claim and main claim vectors without using the implicit premises.", "labels": [], "entities": []}, {"text": "Each user claim is matched to the most similar main claim.", "labels": [], "entities": []}, {"text": "The other is a supervised baseline, which uses a support vector machine (SVM) classifier with an RBF kernel, trained on the user comments, to predict the label corresponding to the main claim.", "labels": [], "entities": []}, {"text": "We train and evaluate the model using a nested 5\u00d73 cross-validation, separately for each topic.", "labels": [], "entities": []}, {"text": "The hyperparameters C and \u03b3 are optimized using grid search.", "labels": [], "entities": []}, {"text": "We use the well-known LibSVM implementation (Chang and Lin, 2011).", "labels": [], "entities": []}, {"text": "Premise sets and combination with claims.", "labels": [], "entities": []}, {"text": "To obtain a single combined representation of a premise set, we simply concatenate the premises together before computing the distributional vector representation.", "labels": [], "entities": []}, {"text": "We do the same when combining the premises with either of the claims.", "labels": [], "entities": []}, {"text": "In what follows, we denote the user claim, the main claim, and the gap-filling premise set with U i , M j , and P ij , respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Gap-filling parameters for the three anno- tators.", "labels": [], "entities": []}, {"text": " Table 5: User claim, the matching main claim, and  the implicit premise(s) filling the gap provided by  two different annotators.", "labels": [], "entities": []}, {"text": " Table 6: Gap-filling parameters for the four topics.", "labels": [], "entities": []}, {"text": " Table 8: Performance of claim matching baselines  and oracle performance of the claim matching mod- els utilizing implicit premises from annotator A1  (macro-averaged F1-score).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9570968151092529}]}, {"text": " Table 9: Performance of claim matching baselines  and the models utilizing the implicit premises on  the test set (macro-averaged F1-score).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9452288150787354}]}, {"text": " Table 10: Performance of the claim matching  model with premise retrieval on the dev. set (upper  part) and test set (lower part); macro-avg. F1-score.", "labels": [], "entities": [{"text": "claim matching", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.739913672208786}, {"text": "F1-score", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9664757251739502}]}]}