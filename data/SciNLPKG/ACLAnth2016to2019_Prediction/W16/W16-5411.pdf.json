{"title": [{"text": "Selective Annotation of Sentence Parts: Identification of Relevant Sub-sentential Units", "labels": [], "entities": []}], "abstractContent": [{"text": "Many NLP tasks involve sentence-level annotation yet the relevant information is not encoded at sentence level but at some relevant parts of the sentence.", "labels": [], "entities": []}, {"text": "Such tasks include but are not limited to: sentiment expression annotation, product feature annotation, and template annotation for Q&A systems.", "labels": [], "entities": [{"text": "sentiment expression annotation", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.8244186838467916}]}, {"text": "However, annotation of the full corpus sentence by sentence is resource intensive.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approach that iteratively extracts frequent parts of sentences for annotating, and compresses the set of sentences after each round of annotation.", "labels": [], "entities": []}, {"text": "Our approach can also be used in preparing training sentences for binary classification (domain-related vs. noise, subjectivity vs. objectivity, etc.), assuming that sentence-type annotation can be predicted by annotation of the most relevant sub-sentences.", "labels": [], "entities": []}, {"text": "Two experiments are performed to test our proposal and evaluated in terms of time saved and agreement of annotation.", "labels": [], "entities": [{"text": "agreement", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9938309192657471}]}], "introductionContent": [{"text": "High quality resources are essential to the performance of NLP systems and in recent information content related NLP tasks, such resources are typically constructed through annotation at sentence level.", "labels": [], "entities": []}, {"text": "For instance, to implement opinion mining systems, sentiment/polarity/emotion expressions or lexicons are often built from a corpus of movie or product reviews; and question templates are extracted from sentence corpora for Q&A systems.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7926653325557709}]}, {"text": "The construction of these two types of resources are quite similar to construction of resources for other information content related NLP tasks, such as information quality tasks (detection of best/most help answers, hyperbole/embellishment, or lying), and speaker attitude/intention tasks (detection of metaphor/metonymy/irony/sarcasm), etc.", "labels": [], "entities": [{"text": "detection of metaphor/metonymy/irony/sarcasm)", "start_pos": 291, "end_pos": 336, "type": "TASK", "confidence": 0.7843406498432159}]}, {"text": "They typically involve extraction of a set of specific expressions (words, word sequences, long-distance collocation etc.) from sentence corpora.", "labels": [], "entities": [{"text": "extraction of a set of specific expressions (words, word sequences, long-distance collocation etc.) from sentence corpora", "start_pos": 23, "end_pos": 144, "type": "Description", "confidence": 0.6083116680383682}]}, {"text": "Our selective annotation approach relies on a sequence mining algorithm to generate frequent patterns in sentences, and annotate patterns (as sub-sentences) instead of full sentences.", "labels": [], "entities": []}, {"text": "Our proposed approach can find most important expressions statistically, and will annotate a pattern only once to avoid repeated annotation.", "labels": [], "entities": []}, {"text": "In the following sections, we will describe our approach, and two experiments are provided to show that our approach is effective in specific tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our paper, we give two experiments for two types of tasks respectively.", "labels": [], "entities": []}, {"text": "They are Extracting Chinese Sentiment Expressions and Annotating Sentences for Binary Classification.", "labels": [], "entities": [{"text": "Binary Classification", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.8398288488388062}]}, {"text": "Because Chinese is our native language, both experiments use Chinese corpora to reduce annotation uncertainty.", "labels": [], "entities": []}, {"text": "We set MinSupRatio=0.05 and MinSup=3.", "labels": [], "entities": []}, {"text": "At first, we extract frequent patterns using MinSupRatio, then gradually decrease the MinSupRatio to find less frequent patterns in next rounds.", "labels": [], "entities": []}, {"text": "When the support equals MinSup and no patterns are mined, we stop our algorithm.", "labels": [], "entities": []}, {"text": "Some statistics of the annotating process are shown in table 3.", "labels": [], "entities": []}, {"text": "Using our approach, we can cover 850\u223c880 sentences by annotating frequent patterns, and the rest 120\u223c150 sentences mainly contain words of low-frequency and still need manual annotation.", "labels": [], "entities": []}, {"text": "Averagely, 40% time are saved for those sentences annotated by our approach.", "labels": [], "entities": []}, {"text": "If there are more sentences, the time saved by our approach is supposed to increase.", "labels": [], "entities": []}, {"text": "During the annotating, the significant difference between W and other two annotators is that W took along time to annotate short patterns which are often single Chinese characters due to wrong word segmentation.", "labels": [], "entities": []}, {"text": "Annotator W tended to annotate more \"Doubt\" patterns because she lacked domain knowledge, which causes more long patterns in the later rounds annotated.", "labels": [], "entities": []}, {"text": "From such observation, it can be seen that our approach has a higher requirement for domain knowledge, which is the cost for high speed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: statistics of annotating 1000 sentences", "labels": [], "entities": []}, {"text": " Table 4: Agreement of annotation. X,S,W denotes annotators X,S,W annotating sentences manually;  X  *  ,S  *  ,W  *  denote annotators X,S,W annotating sentences by our approach", "labels": [], "entities": []}, {"text": " Table 5: Agreement of annotation", "labels": [], "entities": [{"text": "Agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.6870385408401489}]}]}