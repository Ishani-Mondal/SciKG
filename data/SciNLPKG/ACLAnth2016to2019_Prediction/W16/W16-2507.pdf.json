{"title": [{"text": "Intrinsic Evaluations of Word Embeddings: What Can We Do Better?", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents an analysis of existing methods for the intrinsic evaluation of word embeddings.", "labels": [], "entities": []}, {"text": "We show that the main methodological premise of such evaluations is \"interpretability\" of word embed-dings: a \"good\" embedding produces results that make sense in terms of traditional linguistic categories.", "labels": [], "entities": []}, {"text": "This approach is not only of limited practical use, but also fails to do justice to the strengths of dis-tributional meaning representations.", "labels": [], "entities": []}, {"text": "We argue fora shift from abstract ratings of word embedding \"quality\" to exploration of their strengths and weaknesses.", "labels": [], "entities": []}], "introductionContent": [{"text": "The number of word embeddings is growing every year.", "labels": [], "entities": []}, {"text": "A new model is typically evaluated across several tasks, and is considered an improvement if it achieves better accuracy than its predecessors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9974184036254883}]}, {"text": "There are numerous real-use applications that can be used for this purpose, including named entity recognition (), semantic role labeling (, and syntactic parsing.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.6532539228598276}, {"text": "semantic role labeling", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.7064933379491171}, {"text": "syntactic parsing", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.7515782117843628}]}, {"text": "However, different applications rely on different aspects of word embeddings, and good performance in one application does not necessarily imply equally good performance on another.", "labels": [], "entities": []}, {"text": "To avoid laborious evaluation across multiple extrinsic tests a number of intrinsic tasks are used.", "labels": [], "entities": []}, {"text": "Ideally they would predict how a model performs in downstream applications.", "labels": [], "entities": []}, {"text": "However, it has been shown that intrinsic and extrinsic scores do not always correlate (.", "labels": [], "entities": []}, {"text": "This study discusses the methodology behind several existing intrinsic evaluations for word embeddings, showing that their chief premise is \"interpretability\" of a model as a measure of its quality.", "labels": [], "entities": []}, {"text": "This approach has methodological issues, and it also ignores the unique feature of word embeddings -their ability to represent fluidity and fuzziness of meaning that is unattainable by traditional linguistic analysis.", "labels": [], "entities": []}, {"text": "We argue fora shift from absolute ratings of word embeddings towards more exploratory evaluations that would aim not for generic scores, but for identification of strengths and weaknesses of embeddings, thus providing better predictions about their performance in downstream tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "The comparative intrinsic evaluation for word embeddings was introduced by.", "labels": [], "entities": []}, {"text": "Several models are trained on the same corpus, and polled for the nearest neighbors of words from a test set.", "labels": [], "entities": []}, {"text": "For each word, human raters choose the most \"similar\" answer, and the model that gets the most votes is deemed the best.", "labels": [], "entities": []}, {"text": "Subjects asked to choose the most \"similar\" word would presumably prefer synonyms (word 1 in table 1), if any were present (thus the \"best\" model would be the one favoring similarity over relatedness).", "labels": [], "entities": []}, {"text": "They would easily exclude the clearly unrelated words (word 4 for SVD model).", "labels": [], "entities": []}, {"text": "But they would provide less reliable feedback on \"related\" options, where the choice would be between different semantic relations (words 2,3).", "labels": [], "entities": []}, {"text": "Many answers would be subjective, if not random, and likely to reflect frequency, speed of association, and possibly the order of presentation of words -rather than purely semantic factors that we are trying to evaluate.", "labels": [], "entities": []}], "tableCaptions": []}