{"title": [{"text": "A Report on the Automatic Evaluation of Scientific Writing Shared Task", "labels": [], "entities": []}], "abstractContent": [{"text": "The Automated Evaluation of Scientific Writing , or AESW, is the task of identifying sentences in need of correction to ensure their ap-propriateness in a scientific prose.", "labels": [], "entities": [{"text": "AESW", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.8811838030815125}, {"text": "ap-propriateness", "start_pos": 133, "end_pos": 149, "type": "METRIC", "confidence": 0.9908363819122314}]}, {"text": "The data set comes from a professional editing company, VTeX, with two aligned versions of the same text-before and after editing-and covers a variety of textual infelicities that proofreaders have edited.", "labels": [], "entities": [{"text": "VTeX", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9639717936515808}]}, {"text": "While previous shared tasks fo-cused solely on grammatical errors (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014), this time edits cover other types of linguistic misfits as well, including those that almost certainly could be interpreted as style issues and similar \"matters of opinion\".", "labels": [], "entities": []}, {"text": "The latter arise because of different language editing traditions, experience, and the absence of uniform agreement on what \"good\" scientific language should look like.", "labels": [], "entities": [{"text": "language editing", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7570179104804993}]}, {"text": "Initiating this task, we expected the participating teams to help identify the characteristics of \"good\" scientific language, and help create a consensus of which language improvements are acceptable (or necessary).", "labels": [], "entities": []}, {"text": "Six participating teams took on the challenge.", "labels": [], "entities": []}], "introductionContent": [{"text": "The vast number of scientific papers being authored by non-native English speakers creates an immediate demand for effective computer-based writing tools to help writers compose scientific articles.", "labels": [], "entities": []}, {"text": "Several shared tasks have been organized before that in part addressed this challenge, all with English language learners in mind: Helping Our Own, HOO, with two editions in 2011 and 2012; and two Grammatical Error Correction Tasks in).", "labels": [], "entities": []}, {"text": "The four shared tasks focused on grammar error detection and correction, and constituted a major step towards evaluating the feasibility of building novel grammar error correction technologies.", "labels": [], "entities": [{"text": "grammar error detection and correction", "start_pos": 33, "end_pos": 71, "type": "TASK", "confidence": 0.7508716583251953}, {"text": "grammar error correction", "start_pos": 155, "end_pos": 179, "type": "TASK", "confidence": 0.711138904094696}]}, {"text": "An extensive overview of the automated grammatical error detection for language learners was conducted by.", "labels": [], "entities": [{"text": "automated grammatical error detection", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.6494032740592957}]}, {"text": "In subsequent years two English language learner (ELL) corpora were made available for research purposes.", "labels": [], "entities": []}, {"text": "While these achievements are critical for language learners, we also need to develop tools that support genre-specific writing features.", "labels": [], "entities": []}, {"text": "This shared task focused on the genre of scientific writing.", "labels": [], "entities": []}, {"text": "Most scientific publications are written in English by non-native speakers of English.", "labels": [], "entities": []}, {"text": "Submitted articles are often returned to the authors with an encouragement to improve the language or have a native speaker proofread the paper.", "labels": [], "entities": []}, {"text": "lists 10 top reasons why manuscripts are not accepted for publication, with poor writing in the 7th place.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the task and its objectives; Section 3 gives an overview of the data set; Section 4 introduces the participating teams; Section 5 describes the framework used for organizing competitions; Section 6 summarizes the results of the task; Section 7 provides a detailed analysis and discussion of the results; and, finally, Section 8 presents the main conclusions of the Shared Task and our proposed future actions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The main statistics of the AESW data-set (version 1.2).", "labels": [], "entities": [{"text": "AESW data-set", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.9734097421169281}]}, {"text": " Table 4: The number of result submissions for each shared task phase on https://competitions.codalab.org.", "labels": [], "entities": []}, {"text": " Table 5: The summary of AESW 2016 Shared Task participant systems.", "labels": [], "entities": [{"text": "AESW 2016 Shared Task participant", "start_pos": 25, "end_pos": 58, "type": "DATASET", "confidence": 0.8420899271965027}]}, {"text": " Table 6: Binary prediction results.", "labels": [], "entities": [{"text": "Binary prediction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9509638845920563}]}, {"text": " Table 7: Probabilistic estimation results.", "labels": [], "entities": []}, {"text": " Table 8: Probabilistic estimation results, using the corresponding boolean value.", "labels": [], "entities": []}, {"text": " Table 9: Agreement between gold annotations and all systems", "labels": [], "entities": []}]}