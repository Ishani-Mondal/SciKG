{"title": [{"text": "A Comparative Study on Vocabulary Reduction for Phrase Table Smoothing", "labels": [], "entities": [{"text": "Vocabulary Reduction", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.8556055426597595}, {"text": "Phrase Table Smoothing", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.8699640035629272}]}], "abstractContent": [{"text": "This work systematically analyzes the smoothing effect of vocabulary reduction for phrase translation models.", "labels": [], "entities": [{"text": "vocabulary reduction", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.7920532822608948}, {"text": "phrase translation", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8403344750404358}]}, {"text": "We extensively compare various word-level vocabularies to show that the performance of smoothing is not significantly affected by the choice of vocabulary.", "labels": [], "entities": []}, {"text": "This result provides empirical evidence that the standard phrase translation model is extremely sparse.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7853173911571503}]}, {"text": "Our experiments also reveal that vocabulary reduction is more effective for smoothing large-scale phrase tables.", "labels": [], "entities": [{"text": "vocabulary reduction", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.7940745949745178}, {"text": "smoothing large-scale phrase tables", "start_pos": 76, "end_pos": 111, "type": "TASK", "confidence": 0.7935141623020172}]}], "introductionContent": [{"text": "Phrase-based systems for statistical machine translation (SMT) ( have shown state-of-the-art performance over the last decade.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.8182990103960037}]}, {"text": "However, due to the huge size of phrase vocabulary, it is difficult to collect robust statistics for lots of phrase pairs.", "labels": [], "entities": []}, {"text": "The standard phrase translation model thus tends to be sparse.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7674167454242706}]}, {"text": "A fundamental solution to a sparsity problem in natural language processing is to reduce the vocabulary size.", "labels": [], "entities": []}, {"text": "By mapping words onto a smaller label space, the models can be trained to have denser distributions.", "labels": [], "entities": []}, {"text": "Examples of such labels are part-of-speech (POS) tags or lemmas.", "labels": [], "entities": []}, {"text": "In this work, we investigate the vocabulary reduction for phrase translation models with respect to various vocabulary choice.", "labels": [], "entities": [{"text": "vocabulary reduction", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.7100226432085037}, {"text": "phrase translation", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.772240400314331}]}, {"text": "We evaluate two types of smoothing models for phrase translation probability using different kinds of word-level labels.", "labels": [], "entities": [{"text": "phrase translation probability", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.8732777635256449}]}, {"text": "In particular, we use automatically generated word classes () to obtain label vocabularies with arbitrary sizes and structures.", "labels": [], "entities": []}, {"text": "Our experiments reveal that the vocabulary of the smoothing model has no significant effect on the end-to-end translation quality.", "labels": [], "entities": []}, {"text": "For example, a randomized label space also leads to a decent improvement of BLEU or TER scores by the presented smoothing models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9992685914039612}, {"text": "TER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.990016520023346}]}, {"text": "We also test vocabulary reduction in translation scenarios of different scales, showing that the smoothing works better with more parallel corpora.", "labels": [], "entities": []}, {"text": "propose integrating a label vocabulary as a factor into the phrase-based SMT pipeline, which consists of the following three steps: mapping from words to labels, labelto-label translation, and generation of words from labels.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.8955457806587219}, {"text": "labelto-label translation", "start_pos": 162, "end_pos": 187, "type": "TASK", "confidence": 0.7065391540527344}]}, {"text": "verify the effectiveness of word classes as factors.", "labels": [], "entities": []}, {"text": "Assuming probabilistic mappings between words and labels, the factorization implies a combinatorial expansion of the phrase table with regard to different vocabularies.", "labels": [], "entities": []}, {"text": "show a simplified case of the factored translation by adopting hard assignment from words to labels.", "labels": [], "entities": []}, {"text": "In the end, they train the existing translation, language, and reordering models on word classes to build the corresponding smoothing models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Translation results for various initializa- tions of the clustering. 100 classes on both sides.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9222022294998169}]}, {"text": " Table 2: Translation results for different vocabu- lary sizes.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9439185857772827}]}, {"text": " Table 3: Bilingual training data statistics for IWSLT 2012 German\u2192English, WMT 2015  Finnish\u2192English, WMT 2014 English\u2192German, and WMT 2015 English\u2192Czech tasks.", "labels": [], "entities": [{"text": "IWSLT 2012 German\u2192English", "start_pos": 49, "end_pos": 74, "type": "DATASET", "confidence": 0.8257704734802246}, {"text": "WMT 2015  Finnish\u2192English", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.5581028282642364}, {"text": "WMT 2014 English\u2192German", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.6488497138023377}, {"text": "WMT 2015 English\u2192Czech tasks", "start_pos": 132, "end_pos": 160, "type": "DATASET", "confidence": 0.6776540875434875}]}, {"text": " Table 5: Comparison of translation outputs for the smoothing models with different vocabularies. \"op- timized\" denotes 30 iterations of the clustering algorithm, whereas \"non-optimized\" means the initial  (default) clustering.", "labels": [], "entities": []}]}