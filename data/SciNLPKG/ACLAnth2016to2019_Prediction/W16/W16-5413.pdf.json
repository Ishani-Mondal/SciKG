{"title": [{"text": "Automatic Evaluation of Commonsense Knowledge for Refining Japanese ConceptNet", "labels": [], "entities": [{"text": "Refining Japanese ConceptNet", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.8602217038472494}]}], "abstractContent": [{"text": "In this paper we present two methods for automatic commonsense knowledge evaluation for Japanese entries in ConceptNet ontology.", "labels": [], "entities": [{"text": "commonsense knowledge evaluation", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.7438185413678488}]}, {"text": "Our proposed methods utilize text-mining approach, which is inspired by related research for evaluation of generality on natural sentences using commercial search engines and simpler input: one with relation clue words and WordNet synonyms, and one without.", "labels": [], "entities": []}, {"text": "Both methods were tested with a blog corpus.", "labels": [], "entities": []}, {"text": "The system based on our proposed methods reached relatively high precision score for three relations (MadeOf, UsedFor, AtLocation).", "labels": [], "entities": [{"text": "precision score", "start_pos": 65, "end_pos": 80, "type": "METRIC", "confidence": 0.9644328355789185}, {"text": "MadeOf", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.852440595626831}]}, {"text": "We analyze errors and discuss problems of commonsense evaluation, both manual and automatic and propose ideas for further improvements.", "labels": [], "entities": [{"text": "commonsense evaluation", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9023422300815582}]}], "introductionContent": [{"text": "The lack of commonsense knowledge has been one of main problems for creating human level intelligent systems and for improving their tasks as natural language understanding, computer vision, or robot manipulation.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 142, "end_pos": 172, "type": "TASK", "confidence": 0.7516485651334127}, {"text": "robot manipulation", "start_pos": 194, "end_pos": 212, "type": "TASK", "confidence": 0.7190541326999664}]}, {"text": "Researchers have tackled with this deficiency usually taking one of the following three approaches.", "labels": [], "entities": []}, {"text": "One is to hire knowledge specialists to enter the knowledge manually and CyC is the most widely known project of this kind.", "labels": [], "entities": []}, {"text": "Second is to use crowdsourcing.", "labels": [], "entities": []}, {"text": "In Open Mind Common Sense project (OMCS) (), non-specialists input phrases or words manually, which generates knowledge in relatively short time.", "labels": [], "entities": [{"text": "Open Mind Common Sense project (OMCS)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.4812312349677086}]}, {"text": "For making the input process less monotonous, researchers also use Games With A Purpose (GWAPs), for instance N\u00af aja-to nazo nazo 1 (Riddles with Nadya) 2 for acquiring Japanese commonsense knowledge.", "labels": [], "entities": []}, {"text": "Third approach is to use text-mining techniques.", "labels": [], "entities": []}, {"text": "KNEXT (), NELL 3 or WebChild () are famous projects for acquiring commonsense knowledge automatically.", "labels": [], "entities": [{"text": "KNEXT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8622891902923584}]}, {"text": "Last two approaches are immune to quality problems.", "labels": [], "entities": []}, {"text": "For example, knowledge acquired through Nadya interface reached 58% precision (), and NELL system reached 74% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9993268251419067}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9981381893157959}]}, {"text": "This is because public contributors input and source Web texts tend to be noisy.", "labels": [], "entities": []}, {"text": "Therefore, acquired knowledge should be evaluated, but there is no gold standard method for estimating whether acquired knowledge is commonsensical or not.", "labels": [], "entities": []}, {"text": "Usually, manual evaluation by specialists or by crowdsourcing () is used.", "labels": [], "entities": []}, {"text": "However, this is costly and time-consuming, and even specialists have different opinions on concepts' usualness.", "labels": [], "entities": []}, {"text": "Another method is to evaluate automatically acquired knowledge by utilizing it in some tasks.", "labels": [], "entities": []}, {"text": "For example, there is a research using IQ tests () for commonsense knowledge level estimation, but it does not help improving or refining quality of existing or newly acquired concepts.", "labels": [], "entities": [{"text": "commonsense knowledge level estimation", "start_pos": 55, "end_pos": 93, "type": "TASK", "confidence": 0.7945282161235809}]}, {"text": "In this paper, we present automatic evaluation system for commonsense knowledge.", "labels": [], "entities": []}, {"text": "Our approach is to use frequency of phrase occurrences in a Web corpus.", "labels": [], "entities": []}, {"text": "There is a previous research using Internet resources and Japanese WordNet () for evaluating generality of natural sentences from OMCS (.", "labels": [], "entities": [{"text": "OMCS", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.9182195663452148}]}, {"text": "In that research, frequency of occurrence in Yahoo Japan search engine search results snippets are used to determine thresholds for eliminating noise and verb conjugation is used to increase number of hits.", "labels": [], "entities": []}, {"text": "Our approach for evaluating commonsense knowledge is similar but we aim at higher precision without using commercial search engines.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9966866374015808}]}, {"text": "Currently access to commercial engines is limited even fora researchers so we decide to introduce methods that can be used also with relatively smaller, self-made (crawled), corpora.", "labels": [], "entities": []}, {"text": "Our research can also improve crowdsourcing methods, because it can decrease costs or be less time-consuming if distinctly wrong entries are automatically filtered out.", "labels": [], "entities": []}, {"text": "Last but not least, we work on concepts and relations while in previous research only simple word pairs (e.g. \"to throw\" + \"a ball\") were used.", "labels": [], "entities": []}, {"text": "Our contributions presented in this paper can be summarized as follows: \u2022 We evaluate Japanese commonsense knowledge from ConceptNet (Speer and Havasi, 2012) (explained in the next section) by using phrase occurrences in a blog corpus.", "labels": [], "entities": []}, {"text": "\u2022 We apply proposed methods to three relation types to investigate their flexibility.", "labels": [], "entities": []}, {"text": "\u2022 We analyze evaluation errors, discuss problems of our methods and propose their expansion for increasing efficiency of automatic evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "To confirm the efficiency of our proposed system in automatic evaluating commonness of a concept, we performed series of experiments.", "labels": [], "entities": []}, {"text": "From ConceptNet 5.4 we randomly selected 100 assertions for each of the three relations under investigation.", "labels": [], "entities": []}, {"text": "To create the correct data set, 10 annotators (one female student, 8 male students, one male worker, all in their 20's) evaluated 300 assertions presented in Japanese sentences.", "labels": [], "entities": []}, {"text": "We needed to manually create these using a fixed template, because there were many cases where ConceptNet did not contain a natural sentence in Japanese, and the way of expression was not united.", "labels": [], "entities": []}, {"text": "For instance, in case of (C 1 ) banira (vanilla), (R) \"MadeOf\", and (C 2 ) gy\u00af uny\u00af u (milk), we inserted all elements into following template: \"Banira-wa gy\u00af uny\u00af u-kara tsukurareru\" (vanilla is made from milk).", "labels": [], "entities": []}, {"text": "As we treated unarguably common facts starting zero point with growing peculiarity of assertinos, annotators evaluated commonness of such sentences using 10 points scale (from 1 to 10, where 1 is commonsense, and 10 is non-common sense).", "labels": [], "entities": []}, {"text": "We treated the results labelled 1-5 as usual (commonsensical, Figure 2: Equation for calculating f-score.", "labels": [], "entities": [{"text": "commonsensical", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.9689822196960449}, {"text": "Equation", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9556310772895813}, {"text": "f-score", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9042704105377197}]}, {"text": "correct), and 6-10 as unusual (not commonsensical, incorrect).", "labels": [], "entities": []}, {"text": "In and, we show possible combinations of relations between human annotators and system agreement, and f-score calculation equation.", "labels": [], "entities": []}, {"text": "Experiments results showed that our proposed methods achieved high precision for each type of relation (see, and 4).", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9988079071044922}]}, {"text": "These results also proved that the proposed text-mining approach can be used to evaluate relational commonsense knowledge without commercial search engines and thresholds manipulation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation results for \"MadeOf\" relations (\"All Elements\" and \"Concepts Only\" methods).  MadeOf Accuracy Precision Recall F-score  All Elements Method (C 1 , R, C 2 )  0.450  0.780 0.410  0.538  Concepts Only Method (C 1 , C 2 )  0.640  0.792 0.730  0.760", "labels": [], "entities": [{"text": "Accuracy Precision Recall F-score", "start_pos": 106, "end_pos": 139, "type": "METRIC", "confidence": 0.7437953948974609}]}, {"text": " Table 3: Evaluation results for \"UsedFor\" relations (\"All Elements\" and \"Concepts Only\" methods).  UsedFor Accuracy Precision Recall F-score  All Elements Method (C 1 , R, C 2 )  0.530  1.00 0.413  0.584  Concepts Only Method (C 1 , C 2 )  0.650  0.868 0.662  0.735", "labels": [], "entities": [{"text": "Accuracy Precision Recall F-score", "start_pos": 108, "end_pos": 141, "type": "METRIC", "confidence": 0.7513670772314072}]}, {"text": " Table 4: Evaluation results for \"AtLocation\" relations (\"All Elements\" and \"Concepts Only\" methods).  AtLocation Accuracy Precision Recall F-score  All Elements Method (C 1 , R, C 2 )  0.500  0.615 0.285  0.390  Concepts Only Method (C 1 , C 2 )  0.550  0.582 0.696  0.634", "labels": [], "entities": [{"text": "Accuracy Precision Recall F-score", "start_pos": 114, "end_pos": 147, "type": "METRIC", "confidence": 0.7114366590976715}]}, {"text": " Table 6: Evaluation results for \"UsedFor\" relations (\"All Elements\" and \"Concepts Only\" methods)  without doubtful assertions.  UsedFor Accuracy Precision Recall F-score  All Elements Method (C 1 , R, C 2 )  0.479  1.00 0.390  0.561  Concepts Only Method (C 1 , C 2 )  0.667  0.903 0.682  0.778", "labels": [], "entities": [{"text": "Accuracy Precision Recall F-score", "start_pos": 137, "end_pos": 170, "type": "METRIC", "confidence": 0.7400795668363571}]}, {"text": " Table 7: Evaluation results for \"AtLocation\" relations (\"All Elements\" and \"Concepts Only\" methods)  without doubtful assertions.  UsedFor Accuracy Precision Recall F-score  All Elements Method (C 1 , R, C 2 )  0.547  0.765 0.342  0.473  Concepts Only Method (C 1 , C 2 )  0.594  0.630 0.763  0.690", "labels": [], "entities": [{"text": "Accuracy Precision Recall F-score", "start_pos": 140, "end_pos": 173, "type": "METRIC", "confidence": 0.7509915083646774}]}, {"text": " Table 8: Evaluation results for each relations when (C 1 ,R) and (C 2 ,R) were used.  Relation Method Accuracy Precision Recall F-score  MadeOf (C 1 ,R) and (C 2 ,R)  0.620  0.786 0.705  0.743  UsedFor (C 1 ,R) and (C 2 ,R)  0.550  0.872 0.513  0.646  AtLocation (C 2 ,R)  0.590  0.619 0.696  0.650", "labels": [], "entities": [{"text": "Relation", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9190946221351624}, {"text": "Accuracy Precision Recall F-score  MadeOf", "start_pos": 103, "end_pos": 144, "type": "METRIC", "confidence": 0.7232644498348236}]}]}