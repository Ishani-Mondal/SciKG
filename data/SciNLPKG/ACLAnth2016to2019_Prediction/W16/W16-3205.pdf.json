{"title": [{"text": "Natural Language Descriptions of Human Activities Scenes: Corpus Generation and Analysis", "labels": [], "entities": [{"text": "Natural Language Descriptions of Human Activities Scenes", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.7749215917927879}, {"text": "Corpus Generation and Analysis", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.7810039818286896}]}], "abstractContent": [{"text": "There has been continuous growth in the volume and ubiquity of video material.", "labels": [], "entities": []}, {"text": "It has become essential to define video semantics in order to aid the searchabil-ity and retrieval of this data.", "labels": [], "entities": []}, {"text": "Although the method of annotating this data with keywords is relatively well researched, the quality can be improved through describing videos with natural language.", "labels": [], "entities": []}, {"text": "We are exploring approaches to generating natural language descriptions of interrelations between human activities in a video stream.", "labels": [], "entities": []}, {"text": "This paper focuses on creation of a dataset that can be used for development and evaluation.", "labels": [], "entities": []}, {"text": "To this end a corpus of video clips, manually selected from the Hollywood2 dataset, and their natural language descriptions has been generated.", "labels": [], "entities": [{"text": "Hollywood2 dataset", "start_pos": 64, "end_pos": 82, "type": "DATASET", "confidence": 0.9880644381046295}]}, {"text": "Analysis of the hand annotation presents insights into human interests and thoughts.", "labels": [], "entities": []}, {"text": "Such resource can be used to evaluate automatic natural language generation systems for video.", "labels": [], "entities": [{"text": "automatic natural language generation", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.6641986221075058}]}], "introductionContent": [{"text": "Video synopses can be created by converting video summaries using natural language.", "labels": [], "entities": [{"text": "Video synopses", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.5936471968889236}]}, {"text": "They serve to generate a multimedia archive where video analysis, retrieval and summarisation can be developed.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.9841655492782593}]}, {"text": "The majority of previous research, in particular for video description tasks, has relied upon short video clips.", "labels": [], "entities": [{"text": "video description tasks", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.8348353703816732}]}, {"text": "They typically presented one subject performing one action, hence a single sentence was often sufficient to annotate them.", "labels": [], "entities": []}, {"text": "By contrast reality-based video scenarios incorporate various camera shots depicting a range of actions.", "labels": [], "entities": []}, {"text": "We are exploring approaches to generating natural language description for inter-relations of humans and their activities within video streams.", "labels": [], "entities": []}, {"text": "The first step of the study was to create a dataset that could be used for development and evaluation, as we did not find publicly available resource that suitably considered the spatial and temporal relations between individual entities.", "labels": [], "entities": []}, {"text": "Initially, from the Human Actions and Scenes dataset (Hollywood2 dataset 1 ), 120 video segments were selected, 10 for each of the twelve categories.", "labels": [], "entities": [{"text": "Human Actions and Scenes dataset (Hollywood2 dataset 1 )", "start_pos": 20, "end_pos": 76, "type": "DATASET", "confidence": 0.7075960397720337}]}, {"text": "They were relatively long videos ranging from 1 to 3 minutes, selected based on a number of criteria, such as the number of camera shots and the variety of human actions.", "labels": [], "entities": []}, {"text": "For selected video clips, a dataset was then created, comprising hand annotations with natural language descriptions.", "labels": [], "entities": []}, {"text": "We refer to this dataset as NLDHA 2 Corpus.", "labels": [], "entities": [{"text": "NLDHA 2 Corpus", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9184357523918152}]}, {"text": "The contributions of the work presented in this paper include the following two aspects: \u2022 A total of 12 participants manually annotated this dataset in two ways: a brief synopsis (title) consisting of a single phrase or sentence, and a full explanation in everyday language, set out using a number of sentences.", "labels": [], "entities": []}, {"text": "\u2022 An action classification experiment based on hand annotations was performed to demonstrate the application of the corpus with natural language descriptions.", "labels": [], "entities": [{"text": "action classification", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.7255773991346359}]}], "datasetContent": [{"text": "This section uses an action classification task for demonstrating the application of the NLDHA Corpus with natural language descriptions.: Similarity scores within 12 hand annotations using the cosine similarity.", "labels": [], "entities": [{"text": "action classification", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7534942626953125}, {"text": "NLDHA Corpus", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.9644275605678558}]}, {"text": "For each class, scores are calculated in three conditions: (A) raw hand annotations; (B) applying Porter Stemmer and removing stop words, without replacing synonyms; (C) without removing stop words, but applying Porter Stemmer and replacing synonyms.", "labels": [], "entities": []}, {"text": "Textual document features can be expressed through tf-idf scores ().", "labels": [], "entities": []}, {"text": "The importance of a term t within a particular document d can be measured by The term frequency tf (t, d) is given by where the number of occurrences oft ind is presented by N t,d , while the denominator is the size of the document |d|.", "labels": [], "entities": [{"text": "term frequency tf", "start_pos": 81, "end_pos": 98, "type": "METRIC", "confidence": 0.765333334604899}]}, {"text": "Further, the inverse document where N is the total number of documents in the corpus and W (t) is the total number of documents containing the term t.", "labels": [], "entities": []}, {"text": "A term-document matrix is presented by When conducting the experiment, stop words were removed and stemming was applied.", "labels": [], "entities": []}, {"text": "For the action classication task, the most frequent 1000 words were used.", "labels": [], "entities": [{"text": "action classication task", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.8901453812917074}]}, {"text": "We applied the Naive Bayes probabilistic supervised learning algorithm from the Weka machine learning library (.", "labels": [], "entities": [{"text": "Weka machine learning library", "start_pos": 80, "end_pos": 109, "type": "DATASET", "confidence": 0.8048258125782013}]}, {"text": "Ten-fold cross validation was performed and the outcome was measured using precision, recall and F1-measure.: Outcomes for the action classification experiment using the Naive Bayes classifier.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9996042847633362}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9991337656974792}, {"text": "F1-measure.", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9991123080253601}, {"text": "action classification", "start_pos": 127, "end_pos": 148, "type": "TASK", "confidence": 0.7976540923118591}]}, {"text": "presents the outcomes of the monitored classification assessment using tf-idf characteristics.", "labels": [], "entities": []}, {"text": "The F1 scores for certain categories, such as 'AnswerPhone', 'Eat', 'DriveCar' and 'Run', were greater than some others.", "labels": [], "entities": [{"text": "F1", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.999421238899231}, {"text": "Run", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9157263040542603}]}, {"text": "For these categories, description concerning humans and the important objects (e.g., dining table, car, phone) were found inmost of hand annotations thus classification was not too difficult.", "labels": [], "entities": [{"text": "classification", "start_pos": 154, "end_pos": 168, "type": "TASK", "confidence": 0.9632166028022766}]}, {"text": "In general, F1 scores were higher for categories where human's interaction with an object was evident.", "labels": [], "entities": [{"text": "F1", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9997324347496033}]}, {"text": "In comparison some categories, such as 'SitDown', 'SitUp' and 'StandUp', had the substantially lower F1 scores than the rest.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9843791127204895}]}, {"text": "There were two potential reasons why the annotators did not pay sufficient attention to these actions.", "labels": [], "entities": []}, {"text": "Firstly, these actions were performed very quickly in the context of some videos.", "labels": [], "entities": []}, {"text": "For example, when a person sat down or stood up during an eating scene, the annotators would have focused on eating (rather than sitting down or standing up) in their description.", "labels": [], "entities": []}, {"text": "Secondly, these actions were often overlapped with another action by different humans in the video, which the annotators might have found more important for description.", "labels": [], "entities": []}, {"text": "Overall outcome of the classification experiment indicates that the corpus is a reliable tool for assessing natural language description of video streams.", "labels": [], "entities": [{"text": "assessing natural language description of video streams", "start_pos": 98, "end_pos": 153, "type": "TASK", "confidence": 0.7741420950208392}]}], "tableCaptions": [{"text": " Table 1: Similarity scores within 12 hand annota- tions using the cosine similarity. For each class,  scores are calculated in three conditions: (A) raw  hand annotations; (B) applying Porter Stemmer  and removing stop words, without replacing syn- onyms; (C) without removing stop words, but ap- plying Porter Stemmer and replacing synonyms.", "labels": [], "entities": []}, {"text": " Table 2: Outcomes for the action classification ex- periment using the Naive Bayes classifier.", "labels": [], "entities": []}]}