{"title": [{"text": "Filtering and Measuring the Intrinsic Quality of Human Compositionality Judgments", "labels": [], "entities": [{"text": "Human Compositionality Judgments", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.74980096022288}]}], "abstractContent": [{"text": "This paper analyzes datasets with numerical scores that quantify the semantic compositionality of MWEs.", "labels": [], "entities": []}, {"text": "We present the results of our analysis of crowdsourced compositionality judgments for noun compounds in three languages.", "labels": [], "entities": [{"text": "crowdsourced compositionality judgments", "start_pos": 42, "end_pos": 81, "type": "TASK", "confidence": 0.777926524480184}]}, {"text": "Our goals are to look at the characteristics of the annotations in different languages; to examine intrinsic quality measures for such data; and to measure the impact of filters proposed in the literature on these measures.", "labels": [], "entities": []}, {"text": "The cross-lingual results suggest that greater agreement is found for the extremes in the compositionality scale, and that outlier annotation removal is more effective than outlier annotator removal.", "labels": [], "entities": [{"text": "agreement", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9912057518959045}, {"text": "outlier annotation removal", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.6054561038812002}]}], "introductionContent": [{"text": "Noun compounds (NCs) area pervasive class of multiword expressions (MWEs) in many languages.", "labels": [], "entities": []}, {"text": "They are conventionalized noun phrases whose semantics range from idiomatic to fully compositional interpretations.", "labels": [], "entities": []}, {"text": "In idiomatic NCs, the meaning of the whole does not come directly from the meaning of its parts (.", "labels": [], "entities": []}, {"text": "For instance, an ivory tower is not a physical place, but a non-realistic perspective.", "labels": [], "entities": []}, {"text": "Its semantic interpretation has little or nothing to do with a literal tower built out of ivory.", "labels": [], "entities": []}, {"text": "The semantic compositionality of MWEs can be represented as a numerical score.", "labels": [], "entities": []}, {"text": "Its value indicates how much individual words contribute to the meaning of the whole: e.g. olive oil maybe seen as 80% olive and 100% oil, whereas dead end is 5% dead and 90% end.", "labels": [], "entities": []}, {"text": "Low values imply idiomaticity, while high values imply compositionality.", "labels": [], "entities": []}, {"text": "This information can be useful, e.g. to decide how an MWE should be translated).", "labels": [], "entities": []}, {"text": "Many datasets with compositionality judgments have been collected (e.g. and).", "labels": [], "entities": []}, {"text": "asked Mechanical Turkers to annotate 90 English noun-noun compounds on a scale from 0 to 5 with respect to the literality of member words.", "labels": [], "entities": []}, {"text": "This resource has been used to evaluate compositionality prediction systems (.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.9377636611461639}]}, {"text": "A similar resource has been created for German by, who propose two filtering techniques adopted in our experiments.", "labels": [], "entities": []}, {"text": "created a dataset of 1042 compounds in English with binary annotations by 4 experts.", "labels": [], "entities": []}, {"text": "The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction functions.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.9114596247673035}]}, {"text": "In this paper we report a cross-lingual examination of quality measures and filtering strategies for compound compositionality annotations.", "labels": [], "entities": []}, {"text": "Using the dataset by and its extension to English, French and Portuguese by, we examine the filters reported by for German and assess whether they improve overall dataset quality in these three languages.", "labels": [], "entities": []}, {"text": "This analysis aims at studying the distributions and characteristics of the human ratings, examining quality measures for the collected data, and measuring the impact of simple filtering techniques on these quality measures.", "labels": [], "entities": []}, {"text": "In particular, we look at how the scores obtained are distributed across the compositionality scale, whether the scores of the individual components are correlated with those of the compounds, and if there are cases of compounds that are more difficult to annotate than others.", "labels": [], "entities": []}, {"text": "This paper is structured as follows: the three compositionality datasets are presented in \u00a72.", "labels": [], "entities": []}, {"text": "The quality measures and filtering strategies are described in \u00a73 and the results of the analysis in \u00a74.", "labels": [], "entities": []}, {"text": "The paper concludes with discussion of the results and of future work ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this task, we built three datasets, in French (fr), Portuguese (pt) and English (en), containing human-annotated compositionality scores for 2-word NCs.", "labels": [], "entities": []}, {"text": "Annotators were native speakers using an online nontimed questionnaire.", "labels": [], "entities": []}, {"text": "They were shown a NC (e.g. en ivory tower) and three sentences where the compound occurs in a particular sense as context for disambiguation.", "labels": [], "entities": [{"text": "NC", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9723867177963257}]}, {"text": "They then provide three numerical scores in a scale from 0 (idiomatic) to 5 (compositional): the contribution of the headword to the whole (s H ), the contribution of the modifier word to the whole (s M ) and the contribution of both words to the whole (s NC ).", "labels": [], "entities": []}, {"text": "Each entry in the raw dataset can be represented as a tuple, containing: \u2022 annot: identifier of a human annotator \u2022 H: syntactic head of the NC (noun).", "labels": [], "entities": []}, {"text": "\u2022 M: syntactic modifier of the head, can be a noun (en) or an adjective (en pt fr).", "labels": [], "entities": []}, {"text": "\u2022 s NC : integer rating given by the human annotator annot assessing the compositionality of the NC.", "labels": [], "entities": []}, {"text": "\u2022 s H and s M : Same ass NC for the contribution of H and M to the meaning of the whole NC.", "labels": [], "entities": []}, {"text": "\u2022 equiv: A list of at least two paraphrases, synonyms or equivalent formulations.", "labels": [], "entities": []}, {"text": "For instance, for ivory tower, common paraphrases include privilege and utopia.", "labels": [], "entities": []}, {"text": "The datasets contain comparable data collected using different methodologies due to the requirement and availability of native speakers.", "labels": [], "entities": []}, {"text": "For en and fr, we used Amazon Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 23, "end_pos": 51, "type": "DATASET", "confidence": 0.9180160363515218}]}, {"text": "Native en speakers abound on the platform, unlike for the other languages.", "labels": [], "entities": []}, {"text": "For fr, the annotation took considerably longer, and the quality was not as good as en.", "labels": [], "entities": []}, {"text": "For pt, not enough native speakers were found.", "labels": [], "entities": []}, {"text": "Therefore, we developed a stand-alone interface for collecting pt judgments from volunteer annotators.", "labels": [], "entities": [{"text": "collecting pt judgments from volunteer annotators", "start_pos": 52, "end_pos": 101, "type": "TASK", "confidence": 0.7166890750328699}]}, {"text": "The pt and fr datasets contain 180 manually selected noun-adjective NCs each.", "labels": [], "entities": []}, {"text": "The en dataset is the combination of 2 parts: Reddy (Reddy et al., 2011) with the original dataset downloaded from the authors' websites, and en + , with 90 manually selected noun-noun and adjective-noun compounds.", "labels": [], "entities": [{"text": "en dataset", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.6775595992803574}, {"text": "Reddy (Reddy et al., 2011)", "start_pos": 46, "end_pos": 72, "type": "DATASET", "confidence": 0.7697502188384533}]}, {"text": "For each NC, the final scores are calculated as the average of all its annotations.", "labels": [], "entities": []}, {"text": "For instance, if the 5 annotations for the contribution of ivory to ivory tower were [0,1,0,2,0], the final \u00b5 M score would be 3/5.", "labels": [], "entities": [{"text": "\u00b5 M score", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9408653974533081}]}, {"text": "In other words, we obtain 3 scores per compound (for the contribution of H, M and for both) by aggregating individual annotator's scores using the arithmetic mean \u00b5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Intrinsic quality measures for the raw and filtered datasets", "labels": [], "entities": [{"text": "Intrinsic quality", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9577009975910187}]}]}