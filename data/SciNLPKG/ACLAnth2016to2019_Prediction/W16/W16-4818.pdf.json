{"title": [{"text": "UnibucKernel: An Approach for Arabic Dialect Identification based on Multiple String Kernels", "labels": [], "entities": [{"text": "Arabic Dialect Identification", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.7543837428092957}]}], "abstractContent": [{"text": "The most common approach in text mining classification tasks is to rely on features like words, part-of-speech tags, stems, or some other high-level linguistic features.", "labels": [], "entities": [{"text": "text mining classification tasks", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8816253691911697}]}, {"text": "Unlike the common approach, we present a method that uses only character p-grams (also known as n-grams) as features for the Arabic Dialect Identification (ADI) Closed Shared Task of the DSL 2016 Challenge.", "labels": [], "entities": [{"text": "Arabic Dialect Identification (ADI) Closed Shared Task", "start_pos": 125, "end_pos": 179, "type": "TASK", "confidence": 0.6795459787050883}, {"text": "DSL 2016 Challenge", "start_pos": 187, "end_pos": 205, "type": "DATASET", "confidence": 0.807082732518514}]}, {"text": "The proposed approach combines several string kernels using multiple kernel learning.", "labels": [], "entities": []}, {"text": "In the learning stage, we try both Kernel Discriminant Analysis (KDA) and Kernel Ridge Regression (KRR), and we choose KDA as it gives better results in a 10-fold cross-validation carried out on the training set.", "labels": [], "entities": [{"text": "Kernel Discriminant Analysis (KDA)", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.7251382370789846}, {"text": "Kernel Ridge Regression (KRR)", "start_pos": 74, "end_pos": 103, "type": "METRIC", "confidence": 0.7138824413220087}]}, {"text": "Our approach is shallow and simple, but the empirical results obtained in the ADI Shared Task prove that it achieves very good results.", "labels": [], "entities": []}, {"text": "Indeed, we ranked on the second place with an accuracy of 50.91% and a weighted F 1 score of 51.31%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9997565150260925}, {"text": "F 1 score", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9865616162618002}]}, {"text": "We also present improved results in this paper, which we obtained after the competition ended.", "labels": [], "entities": []}, {"text": "Simply by adding more regularization into our model to make it more suitable for test data that comes from a different distribution than training data, we obtain an accuracy of 51.82% and a weighted F 1 score of 52.18%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9995858073234558}, {"text": "F 1 score", "start_pos": 199, "end_pos": 208, "type": "METRIC", "confidence": 0.9871600667635599}]}, {"text": "Furthermore, the proposed approach has an important advantage in that it is language independent and linguistic theory neutral, as it does not require any NLP tools.", "labels": [], "entities": []}], "introductionContent": [{"text": "It seems natural to use words as basic units in text categorization, authorship identification, plagiarism detection or similar text mining tasks.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7206958532333374}, {"text": "authorship identification", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7966135442256927}, {"text": "plagiarism detection", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.8015308380126953}, {"text": "text mining", "start_pos": 128, "end_pos": 139, "type": "TASK", "confidence": 0.7155814319849014}]}, {"text": "Perhaps surprisingly, recent results indicate that methods handling the text at the character level can also be very effective (;.", "labels": [], "entities": []}, {"text": "By avoiding to explicitly consider features of natural language such as words, phrases, or meaning, an approach that works at the character level has an important advantage in that it is language independent and linguistic theory neutral.", "labels": [], "entities": []}, {"text": "In this context, we present a method based on character p-grams that we designed for the Arabic Dialect Identification (ADI) Shared Task of the DSL 2016 Challenge (.", "labels": [], "entities": [{"text": "Arabic Dialect Identification (ADI) Shared Task of the DSL 2016 Challenge", "start_pos": 89, "end_pos": 162, "type": "TASK", "confidence": 0.6419244821255023}]}, {"text": "In this task, the participants had to discriminate between Modern Standard Arabic (MSA) and 4 Arabic dialects, in a 5-way classification setting.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 59, "end_pos": 87, "type": "DATASET", "confidence": 0.8125194013118744}]}, {"text": "A number of 18 teams have submitted their results on the final test set, and our team (UnibucKernel) ranked on the second place with an accuracy of 50.91% and a weighted F 1 score of 51.31%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9995282888412476}, {"text": "F 1 score", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.9799384474754333}]}, {"text": "Our best scoring system is based on combining three different string kernels via multiple kernel learning (MKL) ().", "labels": [], "entities": []}, {"text": "The first kernel that we considered is the p-grams presence bits kernel 1 , which takes into account only the presence of p-grams instead of their frequency.", "labels": [], "entities": []}, {"text": "The second kernel is the (histogram) intersection string kernel 2 , which was first used in a text mining task by , although it is much more popular in computer vision (.", "labels": [], "entities": [{"text": "text mining task", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.8126999735832214}]}, {"text": "The third kernel is derrived from Local Rank Distance 3 , a distance measure that was first introduced in computational biology), but it has also shown its application in NLP (.", "labels": [], "entities": []}, {"text": "Although character p-grams have been employed for ADI in several works (, to the best of our knowledge, none of these string kernels have been previously used for ADI.", "labels": [], "entities": []}, {"text": "Interestingly, these kernels have also been used for native language identification (, obtaining state-of-the-art performance for several languages, including Arabic.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.6569187243779501}]}, {"text": "Two kernel classifiers) were considered for the learning task, namely Kernel Ridge Regression (KRR) and Kernel Discriminant Analysis (KDA).", "labels": [], "entities": [{"text": "Kernel Discriminant Analysis (KDA)", "start_pos": 104, "end_pos": 138, "type": "TASK", "confidence": 0.7125070492426554}]}, {"text": "The KDA classifier is sometimes able to improve accuracy by avoiding the masking problem.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9978958368301392}]}, {"text": "Ina set of preliminary experiments performed on the training set, we found that KDA gives slightly better results than KRR.", "labels": [], "entities": [{"text": "KDA", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.5230011940002441}]}, {"text": "Hence, all our submissions are based on learning with KDA.", "labels": [], "entities": [{"text": "KDA", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.7288575768470764}]}, {"text": "Before submitting our results, we have also tuned our string kernels for the task.", "labels": [], "entities": []}, {"text": "First of all, we tried out p-grams of various lengths, including blended variants of string kernels as well.", "labels": [], "entities": []}, {"text": "The best accuracy was obtained with blended kernels of 3 to 6 p-grams.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9989527463912964}]}, {"text": "Second of all, we have evaluated the individual kernels and various MKL combinations.", "labels": [], "entities": []}, {"text": "The empirical results indicate that combining kernels via MKL can help to improve the accuracy by nearly 1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9996114373207092}]}, {"text": "All these choices played a significant role in obtaining the second place in the final ranking of the ADI Shared Task.", "labels": [], "entities": [{"text": "ADI Shared Task", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.5958700577418009}]}, {"text": "After the challenge, as we learned that the test set comes from a different source, we further improved our models just by adding more regularization.", "labels": [], "entities": []}, {"text": "Interestingly, our approach treats the text documents simply as strings, since it does not involve any linguistic processing of the text, not even tokenization.", "labels": [], "entities": []}, {"text": "Therefore, our method is language independent and linguistic theory neutral.", "labels": [], "entities": []}, {"text": "Furthermore, the proposed approach is simple and effective, as it is just based on shallow features (character p-grams).", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Work related to Arabic dialect identification and to methods based on string kernels is presented in Section 2.", "labels": [], "entities": [{"text": "Arabic dialect identification", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.5783186157544454}]}, {"text": "Section 3 presents the string kernels that we used in our approach.", "labels": [], "entities": []}, {"text": "The learning methods used in the experiments are described in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents details about experiments, including parameter tuning, combining kernels and results of submitted systems.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7130847126245499}]}, {"text": "Finally, we draw our conclusion in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The sample distribution per class for the ADI Shared Task training and test sets.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy rates of different blended string  kernels combined with either KRR or KDA. The re- sults are obtained in a 10-fold cross-validation car- ried out on the training set. The best result is high- lighted in bold.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9837096929550171}]}, {"text": " Table 3: Confusion matrix (on the test set) of  KDA based on the sum of the blended p-grams  presence bits kernel and the blended intersec- tion kernel. The regularization parameter is set  to 0.8, so the F 1 score of this model is 52.18%.", "labels": [], "entities": [{"text": "KDA", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.7020780444145203}, {"text": "F 1 score", "start_pos": 206, "end_pos": 215, "type": "METRIC", "confidence": 0.9886330366134644}]}, {"text": " Table 4: Results for test set C (closed training) of various models based on string kernels. Some models  that have not been submitted for the challenge are also included. For each model, the regularization  parameter used to control the trade-off between overfitting and underfitting is reported as well.", "labels": [], "entities": []}, {"text": " Table 3.  For this model, it takes about 12 minutes to compute the two kernels, train the KDA classifier and predict  the labels on a computer with Intel Core i7 2.3 GHz processor and 8 GB of RAM using a single Core.", "labels": [], "entities": []}]}