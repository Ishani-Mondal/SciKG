{"title": [{"text": "Fast and highly parallelizable phrase table for statistical machine translation", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.7230015695095062}]}], "abstractContent": [{"text": "Speed of access is a very important property for phrase tables in phrase based statistical machine translation as they are queried many times per sentence.", "labels": [], "entities": [{"text": "Speed", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9430533051490784}, {"text": "phrase based statistical machine translation", "start_pos": 66, "end_pos": 110, "type": "TASK", "confidence": 0.577658623456955}]}, {"text": "In this paper we present anew standalone phrase table, optimized for query speed and memory locality.", "labels": [], "entities": []}, {"text": "The phrase table is cache free and can optionally incorporate a reordering table within.", "labels": [], "entities": []}, {"text": "We are able to achieve two times faster decoding by using our phrase table in the Moses decoder in place of the current state-of-the-art phrase table solution without sacrificing translation quality.", "labels": [], "entities": []}, {"text": "Using anew, experimental version of Moses we are able to achieve 10 times faster decoding using our novel phrase table .", "labels": [], "entities": []}], "introductionContent": [{"text": "Phrase tables are the most basic component of a statistical machine translation decoder, containing the parallel phrases necessary to perform phrasebased machine translation.", "labels": [], "entities": [{"text": "statistical machine translation decoder", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.6999828964471817}, {"text": "phrasebased machine translation", "start_pos": 142, "end_pos": 173, "type": "TASK", "confidence": 0.6450806756814321}]}, {"text": "Due to the noisy nature of phrase extraction and the large phrase vocabulary, phrase tables' size can reach hundreds of gigabytes in size.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.8565650880336761}]}, {"text": "describes phrase tables of size of half of terabyte.", "labels": [], "entities": []}, {"text": "A decade ago it was prohibitively expensive fora phrase table of this size to reside in memory, even if hardware supported it: a gigabyte of RAM back in 2006 costed about a 100 USD, compared to 5 USD in 2016.", "labels": [], "entities": []}, {"text": "Because of that fora longtime Machine Translation was considered a big data problem and the engineering efforts were focused on reducing the model size.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.9082935750484467}]}, {"text": "This lead to the creation of several binary phrase table implementations that tackled the memory usage problem: and Junczys-Dowmunt (2012b) developed memory mapped phrase tables which also reduce memory usage using specific datastructures.", "labels": [], "entities": []}, {"text": "The former uses a trie and the latter uses specific for the purpose phrasal rank encoding. and developed suffix array based phrase tables, which work directly with the parallel corpora in order to enable easier addition of new data, avoid long binarization times and keep memory usage low, but traditional precomputed phrase tables offer better performance.", "labels": [], "entities": []}, {"text": "RAM prices have dropped 20 times over the past 10 years and high performance server machines have hundreds of gigabytes of memory.", "labels": [], "entities": [{"text": "RAM", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9909371733665466}]}, {"text": "For those machines it is no longer needed to sacrifice query performance in favour of compression techniques such as the one in Junczys-Dowmunt (2012a).", "labels": [], "entities": []}, {"text": "Furthermore the machines nowadays are highly parallel and locking caches which didn't hurt performance in the past now prevent implementations from scaling.", "labels": [], "entities": []}, {"text": "We have designed anew phrase table called ProbingPT based on linear probing hash) for storage and lock-free querying, in order to deliver the best possible performance in modern use cases where memory is not an issue.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our performance evaluation we used FrenchEnglish model trained on 2 million EUROPARL sentences.", "labels": [], "entities": [{"text": "FrenchEnglish", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.881165087223053}]}, {"text": "We used a KenLM (Heafield, 2011) 2 Anonymous for submission Anonymous for submission language model and cube pruning algorithm) with a pop-limit of 400.", "labels": [], "entities": []}, {"text": "We time the end to end translation of 200,000 sentences from the training set.", "labels": [], "entities": []}, {"text": "All experiments were performed on a machine with two Xeon E5-2680 processors clocked at 2.7 Ghz with total of 16 cores and 16 hyperthreads and 290 GB of RAM.", "labels": [], "entities": []}, {"text": "In all of our figures \"32 cores\" means 16 cores and 16 hyperthreads.", "labels": [], "entities": []}, {"text": "Note that hyperthread do not provide additional computational power but merely permit better resource utilization by allowing more work to be scheduled for the CPU by the OS.", "labels": [], "entities": []}, {"text": "This allows the CPU to already have scheduled work to do while a scheduled process is waiting for IO.", "labels": [], "entities": []}, {"text": "Using hyperthreads will not necessarily increase performance and in cases with high lock contention it can be detrimental for performance.", "labels": [], "entities": [{"text": "lock contention", "start_pos": 84, "end_pos": 99, "type": "METRIC", "confidence": 0.8093689680099487}]}, {"text": "We find it likely that the performance of the ProbingPT system on is hampered by the inclusion of CompactPT based reordering.", "labels": [], "entities": []}, {"text": "Moses doesn't support ProbingPT based reordering and in order to measure the head-to-head performance of the two phrase tables we conducted the same test using two systems that do not use reordering tables and only differ by the phrase table, as shown on.", "labels": [], "entities": []}, {"text": "We can see that ProbingPT consistently outperforms CompactPT by 10-20% at lower thread count but the difference grows as much as 5 times in favour of ProbingPT at the maximum available thread count on the system.", "labels": [], "entities": [{"text": "ProbingPT", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.7739455103874207}, {"text": "ProbingPT", "start_pos": 150, "end_pos": 159, "type": "DATASET", "confidence": 0.9094048738479614}]}, {"text": "If we compare the best performance achieved from both system, ProbingPT is capable of delivering twice the performance of CompactPT.", "labels": [], "entities": [{"text": "ProbingPT", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.8918277621269226}]}, {"text": "It is important to note that ProbingPT's performance always increases with the increase of the thread count, whereas CompactPT's performance doesn't improve past 8 threads.", "labels": [], "entities": []}, {"text": "We can also see that the ProbingPT based system can even take advantage of hyperthreads, which is not possible with any system that uses CompactPT based table ().", "labels": [], "entities": []}, {"text": "On we can observe that removing the reordering table from the CompactPT system has a much smaller effect than removing it from the ProbingPT system.", "labels": [], "entities": [{"text": "ProbingPT system", "start_pos": 131, "end_pos": 147, "type": "DATASET", "confidence": 0.9271525740623474}]}, {"text": "This hints that lexicalized reordering only slows down the decoder because it is implemented in a inefficient manner.", "labels": [], "entities": []}, {"text": "We can conclude that Moses can achieve faster translation times on highly parallel systems by using ProbingPT.", "labels": [], "entities": [{"text": "ProbingPT", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.9138367176055908}]}], "tableCaptions": [{"text": " Table 2: Time (in minutes) it took to translate our test set with Moses with different number of cores  used. The systems differ by the type of phrase table used (ProbingPT or CompactPT) and whether they  use a reordering table (based on CompactPT). The fastest translation time for each system is highligthed.", "labels": [], "entities": []}, {"text": " Table 3: Time (in minutes) it took to translate  our test set with Moses2 with different number of  cores used. Since the only phrase table that is used  is ProbingPT, the systems differ by the reordering  table used. The fastest translation time for each  system is highligthed.", "labels": [], "entities": []}]}