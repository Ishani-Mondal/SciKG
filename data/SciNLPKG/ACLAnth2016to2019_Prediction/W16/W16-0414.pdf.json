{"title": [{"text": "Classification of comment helpfulness to improve knowledge sharing among medical practitioners", "labels": [], "entities": [{"text": "Classification of comment helpfulness", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8574234247207642}]}], "abstractContent": [{"text": "Clinical research article summaries called infoPOEMs (Patient-Oriented Evidence that Matters) are emailed by the Cana-dian Medical Association to family physicians who read them and answer the online Information Assessment Method (IAM) questionnaire which a free form textual opinion fields to comment on the value or content of the infoPOEM.", "labels": [], "entities": [{"text": "Cana-dian Medical Association", "start_pos": 113, "end_pos": 142, "type": "DATASET", "confidence": 0.8748292326927185}]}, {"text": "This article presents results of a relevance evaluation study applied on these comments to automatically determine their helpful-ness and consequently the interest of sharing them among the medical community.", "labels": [], "entities": []}, {"text": "A dataset of 3,470 manually annotated comments provides a gold standard, containing structural, syntactic, and semantic features taken from the Unified Medical Language System and IAM questionnaire.", "labels": [], "entities": [{"text": "IAM questionnaire", "start_pos": 180, "end_pos": 197, "type": "DATASET", "confidence": 0.8782363831996918}]}, {"text": "Applied machine learning algorithms show a global f-measure improvement of 9.1% when compared to a binary occurrence bag-of-word baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of opinion mining has gained importance in the last years with our world being increasingly made of posted information with crowds commenting on such information.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.8352703750133514}]}, {"text": "Such increased volume of crowd comments has led to text analysis research aiming at understanding and clustering the opinions found in those comments (e.g. see recent articles) and to help manage interactions within the online community).", "labels": [], "entities": [{"text": "text analysis", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.8144761025905609}]}, {"text": "An even more recent task is not so much on understanding comments content, but rather on evaluating comments value, impact or helpfulness for the community reading them.", "labels": [], "entities": []}, {"text": "Most research addressing this task, as shown in the Related work section, uses comments on product information on Amazon.", "labels": [], "entities": []}, {"text": "But the idea of evaluating comments helpfulness can be extended to other contexts such as community learning or sharing of knowledge.", "labels": [], "entities": []}, {"text": "In the present research, we look particularly at the community of medical practitioners in the context of reading and commenting on scientific article summaries which are called infoPOEMs R (Patient-Oriented Evidence that Matters).", "labels": [], "entities": []}, {"text": "Within this medical community, comments about an infoPOEM made by one practitioner could be useful to other practitioners regardless of the opinion expressed.", "labels": [], "entities": []}, {"text": "The helpfulness dimension is not necessarily correlated with the opinion dimension with typical values being positive, negative or neutral.", "labels": [], "entities": []}, {"text": "For example, the comment \"good article\" certainly has a different helpfulness value than the comment \"this is a very good article since it shows for the first time that drug X can be useful in disease Y\", even though both comments are positive.", "labels": [], "entities": []}, {"text": "The automatic identification of helpfulness becomes the subject of our research.", "labels": [], "entities": [{"text": "automatic identification of helpfulness", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.6892255917191505}]}, {"text": "Practitioners are not interested in reading all comments, only the valuable or \"helpful\" ones, and an automatic identification of helpfulness would provide a more efficient way for knowledge sharing among them.", "labels": [], "entities": []}], "datasetContent": [{"text": "The gold standard was annotated by three medical students with different experience levels.", "labels": [], "entities": []}, {"text": "They were asked to read anonymous comments submitted by physicians and indicate if they found them valuable for their knowledge or practice.", "labels": [], "entities": []}, {"text": "Each annotator was provided with a list of anonymous comments and their associated infoPOEM for reference.", "labels": [], "entities": []}, {"text": "They could access, if needed, the full text of the infoPOEM if the comment was not clear to them.", "labels": [], "entities": []}, {"text": "A preliminary annotation phase was done with 300 randomly selected comments to be annotated by the three annotators (100 each).", "labels": [], "entities": []}, {"text": "This phase provided a better understanding of the problem to validate the annotation schema used for the main annotation task.", "labels": [], "entities": []}, {"text": "The classification schema included three choices to annotate the helpfulness of a comment: \"valuable\", \"non-valuable\" or \"I don't know\".", "labels": [], "entities": []}, {"text": "The annotators were asked to consider each comment independently and not let the reading of previous comments influence their choice.", "labels": [], "entities": []}, {"text": "The main annotation task was based on two batches of comments.", "labels": [], "entities": []}, {"text": "A first one, relatively small, contained 250 comments and was given to all three reviewers and allowed us to calculate an interannotator agreement.", "labels": [], "entities": []}, {"text": "A larger set of comments was split in three parts to have each comment annotated by a single reviewer.", "labels": [], "entities": []}, {"text": "This provided a total of 3,470 comments associated with 327 randomly picked infoPOEMs.", "labels": [], "entities": []}, {"text": "Of these comments, 1,586 (45.6%) were deemed valuable and 1,884 (54.3%) non-valuable.", "labels": [], "entities": []}, {"text": "A dozen comments were tagged \"I don't know\" and removed from the dataset.", "labels": [], "entities": []}, {"text": "The 300 comments from the preliminary annotation step joined with the 250 comments for the inter-annotator agreement were used as the development dataset (550 unique comments) to define, develop, test and refine features presented in the next section but were not used in the dataset for the final evaluation.", "labels": [], "entities": []}, {"text": "The other set of 3,470 comments was used as the evaluation dataset for performance assessment.", "labels": [], "entities": []}, {"text": "The size of the manually annotated dataset compares advantageously to the 1000 annotated comments of and the 267 of.", "labels": [], "entities": []}, {"text": "Using the first 250 comments annotated by the three annota-tors, an inter-annotator agreement of 0.4846 was computed using the Fleiss' Kappa method for multiple annotators with all three classes (valuable / non-valuable / i don't know).", "labels": [], "entities": []}, {"text": "The interannotator agreement was recalculated using only the 247 comments with only the two main classes (valuable/non-valuable) which provided a score of 0.5004.", "labels": [], "entities": []}, {"text": "The remaining data shows a stronger agreement on valuable comments than on nonvaluable ones.", "labels": [], "entities": []}, {"text": "The level of agreement calculated on this dataset is considered moderate according to when compared to pure chance agreement and is of the same order as in.", "labels": [], "entities": []}, {"text": "Using each annotator as the gold standard versus others, the f-measures were 0.806 between annotators 1 and 2, 0.783 between 1 and 3 and 0.792 between 2 and 3.", "labels": [], "entities": [{"text": "f-measures", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9618286490440369}]}, {"text": "The reason behind the average ratings for interannotator agreement score can be explained by one or many of the following points: coding instructions were interpreted differently by each annotator, coding decision is based on factors which are not present in textual data (like relevant prior knowledge, expertise domain or interest, personal taste or bias and so on), decision factors were present in the text but not correctly understood by the readers, etc.", "labels": [], "entities": []}, {"text": "While it is difficult to provide a clear and proven diagnosis of the reason behind these scores, lower scores usually increase the difficulty to develop prediction systems.", "labels": [], "entities": []}, {"text": "As such, the average agreement provides a contextualisation of potential performance for this task; a near-perfect classification of comments is not the goal as it would overfit the three annotator's classification.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Weighted f-measure results for algo- rithms on each dataset.", "labels": [], "entities": []}]}