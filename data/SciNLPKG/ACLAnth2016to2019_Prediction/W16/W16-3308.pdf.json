{"title": [], "abstractContent": [{"text": "Because PCFGs are, as their name suggests , context-free, they cannot encode many dependencies that occur in natural language, such as the dependencies between determiners and nouns, allowing them to overgenerate phrases like those cat.", "labels": [], "entities": []}, {"text": "One formalism that is able to capture many dependencies that PCFGs cannot is that of probabilistic tree-substitution grammars (PTSGs).", "labels": [], "entities": []}, {"text": "Because PTSGs allow larger subtrees to be used as grammar rules, they can better model natural language but are also more difficult to induce from a corpus.", "labels": [], "entities": []}, {"text": "In this paper, I will show how PTSGs can be used to represent dependencies between determiners and nouns and present a novel method for inducing a PTSG from a parsed corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "PCFGs are ill equipped to handle the case of simple noun phrases consisting of a determiner followed by a noun, as the grammaticality of these phrases is dependent on what types of nouns the determiner in question can precede.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8843759298324585}]}, {"text": "For example, while the can precede any noun, determiners like a and another can only occur with singular count nouns, while those can only precede plural nouns, and determiners like more can precede either plural nouns or mass nouns but not singular count nouns.", "labels": [], "entities": []}, {"text": "Thus, the noun phrases a book, more coffee, and those cards are grammatical, whereas a cards, more book, and those coffee are not.", "labels": [], "entities": []}, {"text": "Representing these sorts of noun phrases with a PCFG is difficult.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.8801434636116028}]}, {"text": "Consider the following toy corpus: Thus, such a grammar would generate the blatantly ungrammatical noun phrase those cat.", "labels": [], "entities": []}, {"text": "Furthermore, even to assign it a low probability, we would need a low probability for at least one of the rules that generated it, which would reduce the probability of at least one of the trees in (1).", "labels": [], "entities": []}, {"text": "While we could perhaps mitigate this problem by removing the nodes labeled N and splitting the DT label into a set of labels corresponding to specific types of DTs, such a solution would overlook some generalizations (including, say, that the could precede any noun).", "labels": [], "entities": []}, {"text": "Instead, we look fora formalism that represents these dependencies more naturally.", "labels": [], "entities": []}, {"text": "One such formalism is tree-substitution grammars (TSGs).", "labels": [], "entities": []}, {"text": "Whereas CFG rules can be seen as one-level subtrees of the parse trees they generate, TSG rules can be any subtrees of these parse trees.", "labels": [], "entities": []}, {"text": "A leaf node of a TSG rule whose label is not a lexical item is known as a substitution node, and parse trees can be built by replacing a substitution node with a rule whose root has the same label as the substitution node.", "labels": [], "entities": []}, {"text": "Because TSGs allow for larger rules than CFGs, they can handle dependencies that CFGs do not.", "labels": [], "entities": []}, {"text": "Consider, once again, the toy corpus presented in (1).", "labels": [], "entities": []}, {"text": "While it is impossible to represent it using a CFG that does not overgenerate ungrammatical noun phrases, we could represent it with a TSG containing the following rules: and (3b) can be combined to get (1a), and (3c) and (3d) can be combined to get (1b).", "labels": [], "entities": []}, {"text": "It is no longer possible to generate (2), as the rule that produces those has a substitution node labeled NNS and thus can only accept plural nouns.", "labels": [], "entities": []}, {"text": "(Similarly, the rule that produces a requires a singular count noun).", "labels": [], "entities": []}, {"text": "Thus, TSGs allow us to accurately capture the dependencies between noun types and determiners.", "labels": [], "entities": []}, {"text": "While PTSGs present a more accurate model of natural language than PCFGs, they are also harder to induce from a corpus.", "labels": [], "entities": []}, {"text": "Given a parse tree fora sentence, one can determine what CFG rules must have generated it simply by looking at each nonterminal node and its children.", "labels": [], "entities": []}, {"text": "Then, given an entire treebank of parse trees, one can simply extract all the necessary CFG rules to produce that treebank and then get a PCFG by estimating the probability of each rule using one of a variety of techniques, the simplest of which involve simply counting the number of times each rule is used in producing the context.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.7366968393325806}]}, {"text": "However, given a parse tree generated from a PTSG, it is less clear which rules formed it, as it is unknown which of the tree's internal nodes were substitution nodes in its derivation and which ones were already internal nodes in elementary trees.", "labels": [], "entities": []}, {"text": "Furthermore, while some of the subtrees of the completed parse trees, such as the rules presented in (3), contain linguistically relevant information, others, such as many CFG rules or rules that simply memorize each full tree in the corpus, are not specific enough or overly specific and, as such, should have low probability or not be present in a PTSG at all.", "labels": [], "entities": []}, {"text": "In this paper, I will present a novel approach for inducing a PTSG from a parsed corpus, focusing specifically on PTSGs that model noun phrases such as the one in (3).", "labels": [], "entities": []}, {"text": "This approach focuses on determining which nodes in the training set are substitution nodes.", "labels": [], "entities": []}, {"text": "It does so by repeatedly sampling grammars from the training set.", "labels": [], "entities": []}, {"text": "It then parses the data set with these grammars and uses the results of the parses to update the probability of trees' internal nodes being substitution nodes.", "labels": [], "entities": []}, {"text": "This approach is simpler to understand and implement than other node-based approaches and does not require complex prior distributions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Log probabilities of training and test sets  on different grammars", "labels": [], "entities": []}, {"text": " Table 2: Probability distributions of noun types  cooccurring with common determiners in the  training set", "labels": [], "entities": []}, {"text": " Table 3: Probability distributions of noun types  cooccurring with common determiners in noun  phrases generated by the PTSG", "labels": [], "entities": [{"text": "PTSG", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.7431824803352356}]}, {"text": " Table 4: K-L divergences of noun phrases gener- ated by the node-based PTSG and the PCFG com- pared to the empirical distribution", "labels": [], "entities": [{"text": "PTSG", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.5851925611495972}, {"text": "PCFG", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.9043014049530029}]}]}