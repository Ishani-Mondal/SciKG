{"title": [{"text": "The FBK Participation in the WMT 2016 Automatic Post-editing Shared Task", "labels": [], "entities": [{"text": "FBK", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8353177905082703}, {"text": "WMT 2016 Automatic Post-editing Shared Task", "start_pos": 29, "end_pos": 72, "type": "TASK", "confidence": 0.7200516859690348}]}], "abstractContent": [{"text": "In this paper, we present a novel approach to combine the two variants of phrase-based APE (monolingual and context-aware) by a factored machine translation model that is able to leverage benefits from both.", "labels": [], "entities": []}, {"text": "Our factored APE models include part-of-speech-tag and class-based neural language models (LM) along with statistical word-based LM to improve the fluency of the post-edits.", "labels": [], "entities": []}, {"text": "These models are built upon a data augmentation technique which helps to mitigate the problem of over-correction in phrase-based APE systems.", "labels": [], "entities": []}, {"text": "Our primary APE system further incorporates a quality estimation (QE) model, which aims to select the best translation between the MT output and the automatic post-edit.", "labels": [], "entities": []}, {"text": "According to the shared task results, our primary and con-trastive (which does not include the QE module) submissions have similar performance and achieved significant improvement of 3.31% TER and 4.25% BLEU (relative) over the baseline MT system on the English-German evaluation set.", "labels": [], "entities": [{"text": "TER", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9992632269859314}, {"text": "BLEU", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.9994595646858215}, {"text": "MT", "start_pos": 237, "end_pos": 239, "type": "TASK", "confidence": 0.8687635660171509}, {"text": "English-German evaluation set", "start_pos": 254, "end_pos": 283, "type": "DATASET", "confidence": 0.6471963624159495}]}], "introductionContent": [{"text": "Translation from and to multiple languages is a growing need of this era.", "labels": [], "entities": []}, {"text": "Especially in a multilingual continent like Europe this poses a challenge to the language service providers (LSPs) that need to quickly deliver high quality translations.", "labels": [], "entities": []}, {"text": "To cope with the increasing demand, the LSPs have shifted human translation from a completely manual process to a semi-automated one, with the help of computer-assisted translation (CAT) tools.", "labels": [], "entities": [{"text": "human translation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7110985517501831}, {"text": "computer-assisted translation (CAT)", "start_pos": 151, "end_pos": 186, "type": "TASK", "confidence": 0.7928890943527221}]}, {"text": "CAT tools are indeed becoming a standard and ubiquitous tool for LSPs, which have to daily face the trade-off between quality and productivity, under the pressure of a growing demand.", "labels": [], "entities": [{"text": "CAT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9334580898284912}]}, {"text": "Machines, however, are not yet perfect: machine translation (MT), in particular, is often prone to systematic errors that human post-editing (PE) has to fix before publication.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.8459089756011963}]}, {"text": "This process of translation results in the generation of parallel data consisting of MT output on one side and its corrected version on the other side.", "labels": [], "entities": [{"text": "translation", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.9678979516029358}]}, {"text": "This data can be leveraged to develop Automatic Post-Editing (APE) systems capable not only to spot recurring MT errors, but also to correct them (in abroad sense, ranging from fixing typos to adapting terminology to a specific domain or even modeling the personal style of an individual translator).", "labels": [], "entities": [{"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.8654056191444397}]}, {"text": "These capabilities become crucial especially when the MT system used to produce the translation suggestions is a \"blackbox\" whose inner workings are not accessible and cannot be tuned or re-trained (a frequent condition for small LSPs).", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9461531639099121}]}, {"text": "A recent study on APE by over six language pairs have reported consistent improvement (7.3% to 14.7% TER reduction) in the quality of machine translated text across all language pairs.", "labels": [], "entities": [{"text": "APE", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.7536354660987854}, {"text": "TER reduction", "start_pos": 101, "end_pos": 114, "type": "METRIC", "confidence": 0.9932506084442139}]}, {"text": "They performed the experiments using the state-of-the-art statistical phrase-based machine translation technique with two variants, which are discussed briefly in Section 2.", "labels": [], "entities": [{"text": "statistical phrase-based machine translation", "start_pos": 58, "end_pos": 102, "type": "TASK", "confidence": 0.5635510608553886}]}, {"text": "Based on the observed complementarity between the two variants and the room for mutual improvement, in Section 3 we present a factored APE model capable to leverage the two methods.", "labels": [], "entities": []}, {"text": "In Section 4 we describe how to create different representations of the data in order to train each of the variants (monolingual, context-aware) and the factored models.", "labels": [], "entities": []}, {"text": "Different configurations of our experiments and their corresponding results are discussed in Section 5.", "labels": [], "entities": []}, {"text": "The results of our submissions in the shared task are reported in Section 6, followed by concluding remarks in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "As defined in the shared task, the training data (English-German) consist of 12K triplets of source (src), MT output (mt), and human post-edits (pe).", "labels": [], "entities": []}, {"text": "We split the development data (consisting of 1K triplets) released in this shared task into 400 and 600 triplets (selected randomly) to tune and evaluate our APE systems.", "labels": [], "entities": []}, {"text": "We use the pe from the training data to build a 5-gram word-based statistical language model using the KENLM toolkit mt pairs of the training data by using MGIZA++ (.", "labels": [], "entities": []}, {"text": "To develop the APE systems we use the phrase-based statistical machine translation toolkit MOSES (  with alignment heuristic set to \"grow-diag-final-and\", and reordering heuristic to \"msd-bidirectional-fe\".", "labels": [], "entities": [{"text": "phrase-based statistical machine translation toolkit MOSES", "start_pos": 38, "end_pos": 96, "type": "TASK", "confidence": 0.543644537528356}]}, {"text": "For building the word alignment models we use MGIZA++ (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7356693744659424}, {"text": "MGIZA++", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.7105172574520111}]}, {"text": "For tuning the feature weights we use MERT optimizing TER ().", "labels": [], "entities": [{"text": "MERT", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.8910104632377625}, {"text": "TER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.5808795094490051}]}, {"text": "We run case-sensitive evaluation with TER, which is based on edit distance, and BLEU), which is based on modified n-gram precision.", "labels": [], "entities": [{"text": "TER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9979454874992371}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9992768168449402}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.8969855308532715}]}, {"text": "In addition to the standard evaluation metrics, we also measure precision of our APE system using sentence level TER score as defined in  Baseline: For internal evaluation we consider the MT system as one of the baselines (an APE system outputting the input sentence), and the two variants of phrase-based APE as described in Section 2.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9991183876991272}, {"text": "sentence level TER score", "start_pos": 98, "end_pos": 122, "type": "METRIC", "confidence": 0.6573662385344505}]}, {"text": "The monolingual variant is labeled as APE-1 and the context-aware as APE-2.", "labels": [], "entities": [{"text": "APE-1", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.775287389755249}]}, {"text": "The baseline results reported in show that the naive monolingual APE system already outperforms the MT baseline by 1.5 BLEU score.", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.8079097270965576}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9992656111717224}]}, {"text": "However, the low precision of the APE systems indicate that they are prone to over-correction and modifies word-: Performance of the APE baselines.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9976765513420105}, {"text": "APE baselines", "start_pos": 133, "end_pos": 146, "type": "DATASET", "confidence": 0.8772435188293457}]}, {"text": "Addressing over-correction: In order to avoid the problem of over-correction (making unnecessary corrections), the APE system should learn to preserve the chunks of the input which are already correct.", "labels": [], "entities": []}, {"text": "To this aim, we augmented the parallel corpus with the post-editions (12K) available in the training data.", "labels": [], "entities": []}, {"text": "So now our training corpus consist of 12K mt-pe or mt#src-pe pairs (to learn post-editing rules) and an additional 12K pe-pe or pe#src-pe pairs (to preserve correct input chunks).", "labels": [], "entities": []}, {"text": "Replicating the baseline APE systems with the augmented data showed significant improvements with all the evaluation metrics as reported in.", "labels": [], "entities": []}, {"text": "For this reason, we use the augmented parallel data in all the further experiments.", "labels": [], "entities": []}, {"text": "Among the two variants we noticed that the APE-2 gets maximum benefit with an absolute precision improvement of 16.40% (from 54.01% to 70.41%).", "labels": [], "entities": [{"text": "APE-2", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.6626462340354919}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9640125632286072}]}, {"text": "we study the effect on the performance of the APE system of using an additional 8-gram statistical as well as a neural POS-tag and a class-based LMs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Example of parallel corpus with factored representation.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the Factored APE-2 for various LMs (statistical word-based LM is present in  all the experiments by default).", "labels": [], "entities": []}, {"text": " Table 4: Performance of the APE baselines.", "labels": [], "entities": [{"text": "APE baselines", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.7849669456481934}]}, {"text": " Table  5. For this reason, we use the augmented parallel  data in all the further experiments. Among the two  variants we noticed that the APE-2 gets maximum  benefit with an absolute precision improvement of  16.40% (from 54.01% to 70.41%).", "labels": [], "entities": [{"text": "APE-2", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.480136901140213}, {"text": "precision", "start_pos": 185, "end_pos": 194, "type": "METRIC", "confidence": 0.9366620779037476}]}, {"text": " Table 5: Performance of the APE system with data  augmentation technique.", "labels": [], "entities": [{"text": "APE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8334363698959351}]}, {"text": " Table 6. Although the TER is almost  the same for different tuning strategies, but slight  improvement is observed with MIRA in terms of  BLEU score.", "labels": [], "entities": [{"text": "TER", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9991495609283447}, {"text": "MIRA", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9983426332473755}, {"text": "BLEU score", "start_pos": 139, "end_pos": 149, "type": "METRIC", "confidence": 0.9804543852806091}]}, {"text": " Table 6: Performance of the combined factored  model for various tuning configurations.", "labels": [], "entities": []}, {"text": " Table 7. Using QE with  threshold of 5 performs slightly better than the one  without QE, so our primary submission is the fac- tored model with QE, whereas, the contrastive one  is without QE.", "labels": [], "entities": []}, {"text": " Table 7: Performance of the APE system with  quality estimation for various thresholds.", "labels": [], "entities": [{"text": "APE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.6700871586799622}]}, {"text": " Table 8: Results of the shared task for our submis- sions", "labels": [], "entities": []}]}