{"title": [{"text": "VSoLSCSum: Building a Vietnamese Sentence-Comment Dataset for Social Context Summarization", "labels": [], "entities": [{"text": "VSoLSCSum", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8838680982589722}, {"text": "Vietnamese Sentence-Comment Dataset", "start_pos": 22, "end_pos": 57, "type": "DATASET", "confidence": 0.7167055209477743}, {"text": "Social Context Summarization", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.7399631639321645}]}], "abstractContent": [{"text": "This paper presents VSoLSCSum, a Vietnamese linked sentence-comment dataset, which was manually created to treat the lack of standard corpora for social context summarization in Viet-namese.", "labels": [], "entities": [{"text": "social context summarization", "start_pos": 146, "end_pos": 174, "type": "TASK", "confidence": 0.6409803231557211}]}, {"text": "The dataset was collected through the keywords of 141 Web documents in 12 special events, which were mentioned on Vietnamese Web pages.", "labels": [], "entities": [{"text": "Vietnamese Web pages", "start_pos": 114, "end_pos": 134, "type": "DATASET", "confidence": 0.9433603882789612}]}, {"text": "Social users were asked to involve in creating standard references and the label of each sentence or comment.", "labels": [], "entities": []}, {"text": "The inter-agreement calculated by Cohen's Kappa among raters after validating is 0.685.", "labels": [], "entities": [{"text": "inter-agreement", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9610980749130249}]}, {"text": "To illustrate the potential use of our dataset, a learning to rank method was trained by using a set of local and cross features.", "labels": [], "entities": []}, {"text": "Experimental results indicate that the summary model trained on our dataset outperforms state-of-the-art baselines in both ROUGE-1 and ROUGE-2 in social context summarization.", "labels": [], "entities": [{"text": "social context summarization", "start_pos": 146, "end_pos": 174, "type": "TASK", "confidence": 0.5420890748500824}]}], "introductionContent": [{"text": "In the context of social media, users can freely discuss the content of an event mentioned in a Web document in the form of comments.", "labels": [], "entities": []}, {"text": "For example, after reading an event, e.g. CASA rescue airplane explosion from Dan Tri 1 , readers can write their comments on the interface of Dan Tri.", "labels": [], "entities": [{"text": "CASA rescue airplane explosion from Dan Tri 1", "start_pos": 42, "end_pos": 87, "type": "DATASET", "confidence": 0.8659656122326851}, {"text": "Dan Tri", "start_pos": 143, "end_pos": 150, "type": "DATASET", "confidence": 0.9250962138175964}]}, {"text": "These comments, one form of social information, have two critical characteristics: (i) reflecting the content and sharing the topic of a Web document, and (ii) revealing the opinions of readers with respect to that event.", "labels": [], "entities": []}, {"text": "This observation inspires a novel summarization task, which utilizes the social information of a Web document to support sentences for generating summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9877650141716003}]}, {"text": "Automatic summarization was first studied by.", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5874726176261902}]}, {"text": "Until now, extractive summarization methods usually focus on plain-text documents and select salient sentences by using statistical or linguistic information in the form of binary classification ().", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.8192292749881744}]}, {"text": "These methods, however, only consider internal information of a Web document, e.g. sentences while ignoring its social information.", "labels": [], "entities": []}, {"text": "Social context summarization is a task which selects both important sentences and representative comments from readers of a Web document.", "labels": [], "entities": [{"text": "Social context summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6750202476978302}]}, {"text": "It has been studied by using different kind of social information such as hyperlinks, click-through data (), comments, opinionated text (, or tweets (.) proposed a dual wing factor graph model for incorporating tweets into the summarization and used Support Vector Machines (SVM) and Conditional Random Fields (CRF) () as preliminary steps in calculating the weight of edges for building the graph.", "labels": [], "entities": []}, {"text": "() used a learning to rank (L2R) approach with 35 features trained by RankBoost for news highlight extraction.", "labels": [], "entities": [{"text": "RankBoost", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.9619636535644531}, {"text": "news highlight extraction", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.6444866061210632}]}, {"text": "() extended the work of  semantic features for summarizing Web documents and their comments.", "labels": [], "entities": [{"text": "summarizing Web documents and their comments", "start_pos": 47, "end_pos": 91, "type": "TASK", "confidence": 0.9089097579320272}]}, {"text": "In contrast, ( proposed a cross-collection topic-aspect modeling (cc-TAM) as a preliminary step to generate a bipartite graph, which was used by a co-ranking method to select sentences and tweets for multi-document summarization.", "labels": [], "entities": []}, {"text": "( proposed a variation of LexRank, which used auxiliary tweets for building a heterogeneous graph random walk (HGRW) to summarize single documents.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9437623620033264}, {"text": "summarize single documents", "start_pos": 120, "end_pos": 146, "type": "TASK", "confidence": 0.8438732425371805}]}, {"text": "proposed SoRTESum, a ranking method using a set of recognizing textual entailment features () for single-document summarization.", "labels": [], "entities": []}, {"text": "However, these methods were applied for English.", "labels": [], "entities": []}, {"text": "To the best our knowledge, no existing method studies social context summarization for Vietnamese due to the lack of a standard corpora.", "labels": [], "entities": [{"text": "social context summarization", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.5743678212165833}]}, {"text": "The objective of this study is to create a standard corpus for social context summarization in Vietnamese.", "labels": [], "entities": [{"text": "social context summarization", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.6313753128051758}]}, {"text": "This paper makes the following contributions: \u2022 We create and release a Vietnamese dataset 2 which can be used to evaluate summary methods in social context and traditional summarization.", "labels": [], "entities": [{"text": "Vietnamese dataset", "start_pos": 72, "end_pos": 90, "type": "DATASET", "confidence": 0.9270233511924744}]}, {"text": "The dataset includes 141 Web documents with their comments in 12 special events.", "labels": [], "entities": []}, {"text": "The gold-standard references are selected by social users.", "labels": [], "entities": []}, {"text": "\u2022 We investigate social context summarization by state-of-the-art summary approaches.", "labels": [], "entities": [{"text": "social context summarization", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.5873988668123881}]}, {"text": "This investigation helps to point out the best summarization method in this task.", "labels": [], "entities": [{"text": "summarization", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9752223491668701}]}, {"text": "Our demo system can be also accessed 3 . In the following sections, we first introduce the creation of our dataset with detail observation.", "labels": [], "entities": []}, {"text": "Next, we show the formulation of summarization in the form of a learning to rank task.", "labels": [], "entities": [{"text": "summarization", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9829419851303101}]}, {"text": "After training a summary model, we compare our results with various summary methods, along with discussion and analysis.", "labels": [], "entities": []}, {"text": "We finish by drawing important conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Comments with less than five tokens were eliminated since they are fairly short for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.9840251803398132}]}, {"text": "5-fold cross validation with m = 6 (less than 30% average sentences, see) was used.", "labels": [], "entities": []}, {"text": "Support Vector Machines (SVM) 10 (Cortes and Vapnik, 1995) was selected for the classification because it has shown as a competitive method for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 144, "end_pos": 157, "type": "TASK", "confidence": 0.9840294718742371}]}, {"text": "Uni-gram and bi-gram taken from KenLM 11 trained from Vietnamese data 12 , were used as language models for learning to rank (L2R).", "labels": [], "entities": [{"text": "KenLM 11 trained from Vietnamese data 12", "start_pos": 32, "end_pos": 72, "type": "DATASET", "confidence": 0.8620970164026532}]}, {"text": "Gold-standard references were used for the evaluation of summary methods.", "labels": [], "entities": []}, {"text": "Evaluation metric is F-1 of ROUGE-N 16 (N=1, 2) (Lin and Hovy, 2003).", "labels": [], "entities": [{"text": "F-1", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.998337984085083}, {"text": "ROUGE-N", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9813887476921082}]}, {"text": "shows the results of summary methods on our dataset.", "labels": [], "entities": []}, {"text": "The results indicate that: (i) our dataset benefits social context summarization in Vietnamese and (ii) social information accelerates the performance of summary methods, e.g. RTE-One Wing vs. RTE Inter Wing and Dual Wing.", "labels": [], "entities": [{"text": "social context summarization", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.6182522376378378}]}, {"text": "Ranking SVM with local and cross features is the best in.", "labels": [], "entities": []}, {"text": "This is because, firstly, SVMRank inherits powerful properties of SVM.", "labels": [], "entities": [{"text": "SVMRank", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.7217037081718445}]}, {"text": "For example, it can create correct margins for classification based on the help of margin maximization.", "labels": [], "entities": []}, {"text": "In training, these properties help SVMRank to avoid an overfitting problem, which often appears in other methods, e.g. AdaBoost or RankBoost.", "labels": [], "entities": [{"text": "RankBoost", "start_pos": 131, "end_pos": 140, "type": "DATASET", "confidence": 0.9009636640548706}]}, {"text": "The results of L2R using RankBoost in support this statement.", "labels": [], "entities": [{"text": "RankBoost", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.9471395015716553}]}, {"text": "Secondly, SVMRank integrates social information leading to significant improvements compared to SVM which only uses local features, e.g. sentence length, sentence position.", "labels": [], "entities": []}, {"text": "This also shows the efficiency of local and cross features proposed in).", "labels": [], "entities": []}, {"text": "Finally, formulating the summarization in the form of learning to rank maybe more appropriate than sentence classification, i.e. SVM.: Summary performance on our dataset; * is supervised method; bold is the best value; italic is the second best; SentenceLead was not used in summarizing comments.", "labels": [], "entities": [{"text": "summarization", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9659806489944458}, {"text": "sentence classification", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.7141940146684647}, {"text": "summarizing comments", "start_pos": 275, "end_pos": 295, "type": "TASK", "confidence": 0.917986124753952}]}, {"text": "Methods with S use social information.", "labels": [], "entities": []}, {"text": "Results in also indicate that HGRW is a competitive method, which achieves a second best result compared to Ranking SVM.", "labels": [], "entities": [{"text": "HGRW", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.49261415004730225}]}, {"text": "This is because HGRW exploits the support of social information for the summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.9838244915008545}]}, {"text": "It also notes that HGRW is an unsupervised method.", "labels": [], "entities": []}, {"text": "SVM obtains competitive results even social information was not integrated.", "labels": [], "entities": [{"text": "SVM", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8349236249923706}]}, {"text": "This shows the efficiency of features for summarization in.", "labels": [], "entities": [{"text": "summarization", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9805712103843689}]}, {"text": "SoRTESum with the support from social information obtains significant improvements as opposed to a strong method Sentence Lead, which simulates the summarization by picking up some first sentences.", "labels": [], "entities": [{"text": "summarization", "start_pos": 148, "end_pos": 161, "type": "TASK", "confidence": 0.9694971442222595}]}, {"text": "Interestingly, cc-TAM achieves the lowest result even though this method is competitive in English ().", "labels": [], "entities": []}, {"text": "The reason is that cc-TAM was developed for multi-document summarization but our dataset was created for single-document summarization.", "labels": [], "entities": [{"text": "cc-TAM", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.90708327293396}, {"text": "multi-document summarization", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.508841022849083}]}], "tableCaptions": [{"text": " Table 3: Statistical observation; s: sentences, c: comments.", "labels": [], "entities": [{"text": "Statistical observation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7990602552890778}]}, {"text": " Table 5: The statistics of six datasets", "labels": [], "entities": []}, {"text": " Table 6: Summary performance on our dataset; * is supervised method; bold is the best value; italic  is the second best; SentenceLead was not used in summarizing comments. Methods with S use social  information.", "labels": [], "entities": [{"text": "summarizing comments", "start_pos": 151, "end_pos": 171, "type": "TASK", "confidence": 0.912998616695404}]}, {"text": " Table 7: The performance of L2R methods.", "labels": [], "entities": []}, {"text": " Table 6. The trend of extracted comments in", "labels": [], "entities": []}]}