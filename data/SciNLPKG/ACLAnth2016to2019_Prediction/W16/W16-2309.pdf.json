{"title": [], "abstractContent": [{"text": "We describe the neural machine translation system of New York University (NYU) and University of Mon-treal (MILA) for the translation tasks of WMT'16.", "labels": [], "entities": []}, {"text": "The main goal of NYU-MILA submission to WMT'16 is to evaluate anew character-level decoding approach in neural machine translation on various language pairs.", "labels": [], "entities": [{"text": "NYU-MILA submission to WMT'16", "start_pos": 17, "end_pos": 46, "type": "DATASET", "confidence": 0.8480324894189835}, {"text": "neural machine translation", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.7203593452771505}]}, {"text": "The proposed neural machine translation system is an attention-based encoder-decoder with a subword-level en-coder and a character-level decoder.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.760288655757904}]}, {"text": "The decoder of the neural machine translation system does not require explicit segmen-tation, when characters are used as tokens.", "labels": [], "entities": []}, {"text": "The character-level decoding approach provides benefits especially when translating a source language into other morphologically rich languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word-level modelling with explicit segmentation has been a standard approach in statistical machine translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.6364196141560873}]}, {"text": "This is mainly due to the issue of data sparsity, caused by the, exponential growth of the state space as the length of sequences grows larger.", "labels": [], "entities": []}, {"text": "This becomes much more severe when a sequence is represented with characters.", "labels": [], "entities": []}, {"text": "In addition to the data sparsity issue, in linguistics, words or their segmented-out lexemes are usually considered as basic units of meaning, which makes words to be more suitable when solving natural language processing tasks.", "labels": [], "entities": []}, {"text": "There are however two pressing issues here.", "labels": [], "entities": []}, {"text": "The first issue is the absence of a perfect segmentation algorithm for any single language.", "labels": [], "entities": []}, {"text": "A perfect This system description paper summarizes and details the experimental procedure described in segmentation algorithm should be able to segment given unsegmented sentence into a sequence of lexemes and morphemes.", "labels": [], "entities": []}, {"text": "The other issue, which is specific to neural network approaches, is that neural machine translation systems suffers from increased complexity due to the large vocabulary size (), which does not happen with character-level modelling.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.7056196530659994}]}, {"text": "Most issues of word-level modelling can be addressed to certain extent by switching into finer tokens, e.g., characters.", "labels": [], "entities": [{"text": "word-level modelling", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7607469856739044}]}, {"text": "In fact, to neural networks, each and every token in the vocabulary is treated as an independent entity, and the semantics of tokens are simply learned to maximize the objective function (.", "labels": [], "entities": []}, {"text": "This property allows a lot of freedom to the neural machine translation system in the choice of tokens.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.742094099521637}]}, {"text": "The NYU-MILA neural machine translation system is built on the idea of directly generating characters, instead of words, that can possibly unlink a machine translation system from the need of explicit segmentation as a preprocessing step, which is often suboptimal in solving translation tasks.", "labels": [], "entities": [{"text": "NYU-MILA neural machine translation", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.8663597255945206}, {"text": "machine translation", "start_pos": 148, "end_pos": 167, "type": "TASK", "confidence": 0.7215295135974884}]}, {"text": "We focus on representing the target sentence as a sequence of characters, and the source sentence as a sequence of subwords ().", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the details of the experimental settings for our system.", "labels": [], "entities": []}, {"text": "Corpora and Preprocessing We use all available training parallel corpora for four language pairs from WMT'16: En-Cs, En-De, En-Ru and En-Fi.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9796852469444275}, {"text": "WMT'16", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.92559814453125}]}, {"text": "They consist of 63.5M, 4.5M, 2.3M and 2M sentence pairs, respectively.", "labels": [], "entities": []}, {"text": "We do not use any monolingual corpus.", "labels": [], "entities": []}, {"text": "We only use the sentence pairs, when the source side is up to 50 subword symbols long and the target side is up to 500 characters.", "labels": [], "entities": []}, {"text": "For all the pairs other than En-Fi, we use newstest-2013 as a development set, and for EnFi, we use newsdev-2015 as a development set.", "labels": [], "entities": [{"text": "En-Fi", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9529626369476318}, {"text": "EnFi", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.962784469127655}]}, {"text": "All of the source corpora were preprocessed using BPE (, and for the target corpora, no additional preprocessing step is required.", "labels": [], "entities": [{"text": "BPE", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.7683926820755005}]}, {"text": "For the target vocabulary, we use 300 characters and two additional tokens reserved for EOS and UNK.", "labels": [], "entities": [{"text": "EOS", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.9166052341461182}, {"text": "UNK", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.849246084690094}]}, {"text": "For the source vocabulary we constrain the size of BPE symbols up to 30, 000.", "labels": [], "entities": [{"text": "BPE", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.5640320181846619}]}, {"text": "The results of the NYU-MILA system is presented in.", "labels": [], "entities": [{"text": "NYU-MILA", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.9175715446472168}]}, {"text": "The character-level decoding works well on most of the languages that are tested, achieving comparable BLEU-c scores to other approaches using words or subwords (BPE) as tokens.", "labels": [], "entities": [{"text": "BLEU-c", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9993752837181091}]}, {"text": "Note that our system does not incorporate extra monolingual training corpus, and does not include any kind of postprocessing e.g., reranking.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Empirical results of the NYU-MILA systems on WMT'16 test sets. All of our submitted  systems are constrained. For ranking by BLEU-c scores, when there are multiple submissions from a  single system, we count it as one system. Some of the systems that showed in BLEU-c case do not show  in the human evaluation, hence the total number of systems does not match. We present the ranking in  both constrained setting (cons.) and unconstrained setting (uncons.) on the table.", "labels": [], "entities": [{"text": "NYU-MILA", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.9362025260925293}, {"text": "WMT'16 test sets", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.9790083964665731}, {"text": "BLEU-c", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9972821474075317}, {"text": "BLEU-c", "start_pos": 271, "end_pos": 277, "type": "METRIC", "confidence": 0.9928870797157288}]}]}