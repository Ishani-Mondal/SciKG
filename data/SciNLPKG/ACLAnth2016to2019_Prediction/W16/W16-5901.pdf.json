{"title": [{"text": "Inside-Outside and Forward-Backward Algorithms Are Just Backprop (Tutorial Paper)", "labels": [], "entities": []}], "abstractContent": [{"text": "A probabilistic or weighted grammar implies a posterior probability distribution over possible parses of a given input sentence.", "labels": [], "entities": []}, {"text": "One often needs to extract information from this distribution , by computing the expected counts (in the unknown parse) of various grammar rules, constituents, transitions, or states.", "labels": [], "entities": []}, {"text": "This requires an algorithm such as inside-outside or forward-backward that is tailored to the grammar formalism.", "labels": [], "entities": []}, {"text": "Conveniently, each such algorithm can be obtained by automatically differentiating an \"inside\" algorithm that merely computes the log-probability of the evidence (the sentence).", "labels": [], "entities": []}, {"text": "This mechanical procedure produces correct and efficient code.", "labels": [], "entities": []}, {"text": "As for any other instance of back-propagation, it can be carried out manually or by software.", "labels": [], "entities": []}, {"text": "This pedagogical paper carefully spells out the construction and relates it to traditional and non-traditional views of these algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "The inside-outside algorithm) is a core method in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6413241624832153}]}, {"text": "Given a sentence, it computes the expected count of each possible grammatical substructure at each position in the sentence.", "labels": [], "entities": []}, {"text": "Such expected counts are commonly used (1) to train grammar weights from data, (2) to select low-risk parses, and (3) as soft features that characterize sentence positions for other NLP tasks.", "labels": [], "entities": []}, {"text": "The algorithm can be derived directly but is generally perceived as tricky.", "labels": [], "entities": []}, {"text": "This paper explains how it can be obtained simply and automatically by backpropagation-more precisely, by differentiating the inside algorithm.", "labels": [], "entities": []}, {"text": "In the same way, the forwardbackward algorithm can begotten by differentiating the backward algorithm.", "labels": [], "entities": []}, {"text": "Back-propagation is now widely known in the natural language processing and machine learning communities, thanks to the recent surge of interest in neural networks.", "labels": [], "entities": []}, {"text": "Thus, it now seems useful to call attention to its role in some of NLP's core algorithms for structured prediction.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.8065260052680969}]}], "datasetContent": [], "tableCaptions": []}