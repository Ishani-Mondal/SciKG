{"title": [{"text": "Don't Mention the Shoe! A Learning to Rank Approach to Content Selection for Image Description Generation", "labels": [], "entities": [{"text": "Image Description", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7953804135322571}]}], "abstractContent": [{"text": "We tackle the sub-task of content selection as part of the broader challenge of automatically generating image descriptions.", "labels": [], "entities": [{"text": "content selection", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.725965142250061}]}, {"text": "More specifically, we explore how decisions can be made to select what object instances should be mentioned in an image description, given an image and labelled bounding boxes.", "labels": [], "entities": []}, {"text": "We propose casting the content selection problem as a learning to rank problem, where object instances that are most likely to be mentioned by humans when describing an image are ranked higher than those that are less likely to be mentioned.", "labels": [], "entities": [{"text": "content selection", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7078361213207245}]}, {"text": "Several features are explored: those derived from bounding box localisations, from concept labels, and from image regions.", "labels": [], "entities": []}, {"text": "Object instances are then selected based on the ranked list, where we investigate several methods for choosing a stopping criterion as the 'cut-off' point for objects in the ranked list.", "labels": [], "entities": []}, {"text": "Our best-performing method achieves state-of-the-art performance on the ImageCLEF2015 sentence generation challenge.", "labels": [], "entities": [{"text": "ImageCLEF2015 sentence generation challenge", "start_pos": 72, "end_pos": 115, "type": "TASK", "confidence": 0.8123408704996109}]}], "introductionContent": [{"text": "In recent years, there has been significant interest in developing systems capable of generating literal, sentential descriptions of images (a boy playing with a frisbee in the park).", "labels": [], "entities": []}, {"text": "The task poses an interesting and difficult challenge for natural language generation, and is important for improved text and image retrieval.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6465514202912649}, {"text": "text and image retrieval", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.6310084387660027}]}, {"text": "The image description task could potentially advance research and provide insights into multimodal natural language generation, e.g. building language models of how humans naturally describe the visual world.", "labels": [], "entities": [{"text": "image description", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7460220754146576}, {"text": "multimodal natural language generation", "start_pos": 88, "end_pos": 126, "type": "TASK", "confidence": 0.6240187436342239}]}, {"text": "A standard paradigm for approaching this task is to first detect instances of pre-defined concepts in the image to be described, and then to reason about the detected concepts to generate image descriptions.", "labels": [], "entities": []}, {"text": "Thus, such approaches may involve various components of a standard Natural Language Generation pipeline), such as document planning (including content determination), microplanning (lexicalisation/referring expression generation) and realisation.", "labels": [], "entities": [{"text": "document planning", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.7282403856515884}, {"text": "content determination)", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.7539586027463278}, {"text": "referring expression generation", "start_pos": 197, "end_pos": 228, "type": "TASK", "confidence": 0.6283249060312907}]}, {"text": "In this paper, we concentrate on a specific subproblem in such an image description generation pipeline.", "labels": [], "entities": [{"text": "image description generation pipeline", "start_pos": 66, "end_pos": 103, "type": "TASK", "confidence": 0.7870277166366577}]}, {"text": "More specifically, we explore the content selection problem proposed by . In this setting, object instances are assumed to have already been localised in an image.", "labels": [], "entities": [{"text": "content selection", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7185791283845901}]}, {"text": "Thus, given gold standard labelled bounding boxes of object instances in an image, the task is to select the appropriate bounding box instances to be mentioned in the eventual image description that is to be generated (see for an example).", "labels": [], "entities": []}, {"text": "To our knowledge, there has been minimal work specifically tackling the content selection problem.", "labels": [], "entities": [{"text": "content selection", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7748510241508484}]}, {"text": "However, the task is important to image description generation as not all entities depicted in an image will be mentioned by humans.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.8283379276593527}]}, {"text": "For example, a fork lying on a table probably will not be mentioned in a picture of a family having dinner in the kitchen.", "labels": [], "entities": []}, {"text": "Determining which entity will be described thus poses an interesting research question, and may provide insights into how humans decide what is important enough to be described in an image description.", "labels": [], "entities": []}, {"text": "Thus, the main objective of this paper is to propose methods for learning to predict the object entities depicted in an image that will be mentioned in a human-authored description of the image.", "labels": [], "entities": []}, {"text": "Our main contribution is to develop a ranking-based content selection system that exploits stronger tex-193 arm woman wall head Figure 1: Given labelled bounding boxes as input, we tackle the content selection task, i.e. deciding which bounding box instances should be selected to be mentioned in the corresponding image description.", "labels": [], "entities": []}, {"text": "This is an important task as humans do not mention everything that is depicted in an image.", "labels": [], "entities": []}, {"text": "We propose casting the content selection problem as a ranking task, that is to order the bounding box instances by how likely they are to be mentioned in a human-authored image description.", "labels": [], "entities": [{"text": "content selection problem", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7911030451456705}]}, {"text": "tual and image features from data for the content selection problem, than those used in the baselines proposed in . We propose casting the content selection problem as a learning to rank problem.", "labels": [], "entities": [{"text": "content selection problem", "start_pos": 139, "end_pos": 164, "type": "TASK", "confidence": 0.747981478770574}]}, {"text": "More specifically, given a set of labelled bounding boxes in an image, bounding boxes instances are ranked by how likely they are to be mentioned in a corresponding human description.", "labels": [], "entities": []}, {"text": "However, as we are interested in both precision and recall, we do not require all labelled bounding boxes to be ranked; for example object instances that are unlikely to be mentioned in the description need not be ranked.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9989262223243713}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9981254935264587}]}, {"text": "Thus, we also propose various 'stopping criterion' to automatically select only relevant instances based on the rankings.", "labels": [], "entities": []}, {"text": "Our hypothesis is that humans inherently prioritise important entities to be selected based on background knowledge and other cues, and we will thus be able to exploit this to tackle the content selection problem.", "labels": [], "entities": [{"text": "content selection", "start_pos": 187, "end_pos": 204, "type": "TASK", "confidence": 0.7513920962810516}]}], "datasetContent": [{"text": "Following the convention of the ImageCLEF2015 Sentence Generation challenge, we evaluate content selection using the fine-grained evaluation metric proposed in  and . More specifically, we measure the F -score (including P recision and Recall) when comparing the object instances selected by our system to the object instances mentioned in the gold standard human-authored image descriptions.", "labels": [], "entities": [{"text": "ImageCLEF2015 Sentence Generation challenge", "start_pos": 32, "end_pos": 75, "type": "TASK", "confidence": 0.6836681067943573}, {"text": "F -score", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9939175049463908}, {"text": "P recision", "start_pos": 221, "end_pos": 231, "type": "METRIC", "confidence": 0.8550785183906555}, {"text": "Recall)", "start_pos": 236, "end_pos": 243, "type": "METRIC", "confidence": 0.9294652044773102}]}, {"text": "The human upper-bound is estimated by evaluating one description against the other descriptions of the image and repeating the process for all descriptions.", "labels": [], "entities": []}, {"text": "We compare our results to the winning participants of past ImageCLEF challenges.: Results of combining all features: Mean P recision, Recall and F -score (with standard deviations) for different algorithms and stopping criteria, compared to the winning ImageCLEF participants, the best reported results of) and a human upper-bound.", "labels": [], "entities": [{"text": "Mean P recision", "start_pos": 117, "end_pos": 132, "type": "METRIC", "confidence": 0.9336742361386617}, {"text": "Recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9949630498886108}, {"text": "F -score", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9918896555900574}]}, {"text": "using a binary SVM classifier with bounding box localisation and visual features.", "labels": [], "entities": []}, {"text": "We also compare our performance to the best reported results in, namely by combining bigram and bounding box size priors with a stopping criterion of k = 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of combining all features: Mean  P recision, Recall and F -score (with standard de- viations) for different algorithms and stopping cri- teria, compared to the winning ImageCLEF par- ticipants", "labels": [], "entities": [{"text": "Mean  P recision", "start_pos": 45, "end_pos": 61, "type": "METRIC", "confidence": 0.9108831485112509}, {"text": "Recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.996250331401825}, {"text": "F -score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9945675333340963}]}, {"text": " Table 2: Results of combining features derived  from bounding box localisation and concept labels  (excluding image region features). In contrast to  Table 1, excluding image region features improves  the performance of both cascent and svmrank.", "labels": [], "entities": []}, {"text": " Table 3: Mean P recision, Recall and F -score for  features derived from bounding box localisation.  Both cascent and svmrank return the same scores  (shown). rforest is unable to handle single dimen- sional vectors. The results for k=3 and k=4 are  comparable to", "labels": [], "entities": [{"text": "Mean P recision", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.796181877454122}, {"text": "Recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9984562397003174}, {"text": "F -score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.994901200135549}]}, {"text": " Table 4: Mean P recision, Recall and F -score for  features derived from concept labels (one-hot in- dicator vectors and text embeddings).", "labels": [], "entities": [{"text": "Mean P recision", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.8549447059631348}, {"text": "Recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9980523586273193}, {"text": "F -score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9937810301780701}]}, {"text": " Table 5: Mean P recision, Recall and F -score for  features derived from image region features (im- age embeddings).", "labels": [], "entities": [{"text": "Mean P recision", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.8749838670094808}, {"text": "Recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9980608820915222}, {"text": "F -score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9946867624918619}]}]}