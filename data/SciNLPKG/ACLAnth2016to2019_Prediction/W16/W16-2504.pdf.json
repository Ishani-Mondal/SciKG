{"title": [{"text": "Evaluating Word Embeddings Using a Representative Suite of Practical Tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings are increasingly used in natural language understanding tasks requiring sophisticated semantic information.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.6790706316630045}]}, {"text": "However, the quality of new embedding methods is usually evaluated based on simple word similarity benchmarks.", "labels": [], "entities": []}, {"text": "We propose evaluating word embeddings in vivo by evaluating them on a suite of popular downstream tasks.", "labels": [], "entities": []}, {"text": "To ensure the ease of use of the evaluation, we take care to find a good point in the tradeoff space between (1) creating a thorough evaluation -i.e., we evaluate on a diverse set of tasks; and (2) ensuring an easy and fast evaluation-by using simple models with few tuned hyperparameters.", "labels": [], "entities": []}, {"text": "This allows us to release this evaluation as a standardized script and online evaluation, available at http://veceval.com/.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many modern NLP systems, especially those employing deep learning techniques, benefit from word representations in the form of lowdimensional word embeddings.", "labels": [], "entities": []}, {"text": "This has led to a burgeoning body of work focusing on improving these representations.", "labels": [], "entities": []}, {"text": "These word representations are used either as features fora conventional statistical classifier, or in a deep learning setup, where they are tuned to a particular task through backpropagation.", "labels": [], "entities": []}, {"text": "However, the quality of these unsupervised embeddings is often asserted on the basis of restricted lexical semantics tasks, such as scoring word similarity or linear relationships for analogies.", "labels": [], "entities": []}, {"text": "These intrinsic evaluations are carried outwith little attention paid to how performance correlates with downstream tasks.", "labels": [], "entities": []}, {"text": "We propose instead evaluating word embeddings using a standardized suite of characteristic downstream tasks.", "labels": [], "entities": []}, {"text": "This has two advantages, which constitute the main contributions of the paper.", "labels": [], "entities": []}, {"text": "First, an improvement in performance on these representative tasks is more likely to generalize to real-world applications of the embedding, as compared to improvements in performance on current word similarity benchmarks.", "labels": [], "entities": []}, {"text": "Therefore, this evaluation offers a better metric to hill-climb on than current lexical semantics tasks.", "labels": [], "entities": []}, {"text": "Second, this evaluation allows for higher fidelity qualitative assessment on the strengths and weaknesses of an embedding method.", "labels": [], "entities": []}, {"text": "For instance, certain embeddings may excel at syntactic tasks, or on sequence modeling tasks, whereas others may capture the semantics of a word better, or work better for classification tasks.", "labels": [], "entities": [{"text": "sequence modeling tasks", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.7662568092346191}]}, {"text": "We believe this evaluation can facilitate consolidating and formalizing such insights, currently latent in the collective consciousness of the NLP community.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our goal is to distill performance on extrinsic tasks into a short but comprehensive \"report\" that indicates the strengths and weaknesses of a particular set of embeddings on a variety of downstream tasks.", "labels": [], "entities": []}, {"text": "For each set of embeddings tested, we report results based on the metric most appropriate for the task -F 1 score for NER, and accuracy for the rest of the tasks.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9836408495903015}, {"text": "NER", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.5757272243499756}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.999367892742157}]}, {"text": "We use SVD as a baseline embedding method.", "labels": [], "entities": []}, {"text": "Using the hyperwords software of, we apply SVD to a PMI-transformed cooccurrence matrix derived from the same pre- Figure 1: An example of the type of result report created by our evaluation.", "labels": [], "entities": []}, {"text": "The first chart shows the relative error reduction of the embedding method compared to the SVD baseline, disallowing backpropagation into the vectors.", "labels": [], "entities": []}, {"text": "This measures the extent to which the original vectors capture linguistic phenomena.", "labels": [], "entities": []}, {"text": "Values above 0 perform better than SVD on the task; the magnitude of the improvement are on comparable scales between tasks.", "labels": [], "entities": []}, {"text": "The second chart is identical to the first chart, but allowing backpropagation into the vectors.", "labels": [], "entities": []}, {"text": "This measures how good the vectors are as an initialization for neural network methods.", "labels": [], "entities": []}, {"text": "The first table shows the raw accuracy numbers for each task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9897991418838501}]}, {"text": "The second table shows the vectors' result on the WordSim and Analogy tasks.", "labels": [], "entities": [{"text": "WordSim", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.938745379447937}]}, {"text": "scribed corpus, resulting in a set of SVD vectors.", "labels": [], "entities": []}, {"text": "We present a baseline for each task, which is the F 1 score or accuracy attained by the SVD vectors.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9841386477152506}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9694724678993225}]}, {"text": "Due to the diversity of the tasks, it is difficult to compare the raw values or differences over the baseline.", "labels": [], "entities": []}, {"text": "These measures, especially when aggregated, tend to implicitly reward large improvements over low-baseline tasks more than small improvements over high-baseline tasks.", "labels": [], "entities": []}, {"text": "To illustrate, whereas a 1% improvement on POS tagging should be considered significant, the same 1% improvement on a task with a 80% baseline is less impressive.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.8208440244197845}]}, {"text": "As such, the primary metric we report is not accuracy or F 1 , but rather the relative error reduction as compared to the SVD baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9997438788414001}, {"text": "F 1", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9949258863925934}, {"text": "error reduction", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.8801745772361755}]}, {"text": "This allows us to calculate a meaningful aggregate, averaging relative error reduction over tasks.", "labels": [], "entities": []}, {"text": "For backwards compatibility with prior work, we also report correlations on WordSim-353, as well as precision at 1 for the analogy task presented in.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.9774967432022095}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9995261430740356}]}, {"text": "The figure shows an example report generated by our evaluation, using arbitrary but realistic values.", "labels": [], "entities": []}, {"text": "It can be seen that the relative error reduction depicted in the charts enables a clearer representation of the relative performance on different tasks, as compared to the raw values provided in the table.", "labels": [], "entities": []}], "tableCaptions": []}