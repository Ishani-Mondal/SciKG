{"title": [{"text": "Real Multi-Sense or Pseudo Multi-Sense: An Approach to Improve Word Representation", "labels": [], "entities": [{"text": "Improve Word Representation", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.69176384806633}]}], "abstractContent": [{"text": "Previous researches have shown that learning multiple representations for polysemous words can improve the performance of word embeddings on many tasks.", "labels": [], "entities": []}, {"text": "However, this leads to another problem.", "labels": [], "entities": []}, {"text": "Several vectors of a word may actually point to the same meaning, namely pseudo multi-sense.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the concept of pseudo multi-sense, and then propose an algorithm to detect such cases.", "labels": [], "entities": []}, {"text": "With the consideration of the detected pseudo multi-sense cases, we try to refine the existing word embeddings to eliminate the influence of pseudo multi-sense.", "labels": [], "entities": []}, {"text": "Moreover, we apply our algorithm on previous released multi-sense word embeddings and tested it on artificial word similarity tasks and the analogy task.", "labels": [], "entities": []}, {"text": "The result of the experiments shows that diminishing pseudo multi-sense can improve the quality of word representations.", "labels": [], "entities": []}, {"text": "Thus, our method is actually an efficient way to reduce linguistic complexity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing meanings of words by embedding them into a high dimensional vector space, so called word embedding, is a useful technique in natural language processing.", "labels": [], "entities": [{"text": "Representing meanings of words", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8996163159608841}, {"text": "word embedding", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.6922909319400787}, {"text": "natural language processing", "start_pos": 138, "end_pos": 165, "type": "TASK", "confidence": 0.6435799598693848}]}, {"text": "An intuitive idea is to encode one word into a single vector, which contains the semantic information of the word in corpus (.", "labels": [], "entities": []}, {"text": "There is a consensus that natural languages always include lots of polysemous words.", "labels": [], "entities": []}, {"text": "For example, when the word star appears together with words like planet, satellite, it may roughly denote a kind of celestial body; when star appears with words like movie, song, drama, it may stand fora famous person.", "labels": [], "entities": []}, {"text": "For most cases, we human beings can easily point out which sense a word belongs to based on its context.", "labels": [], "entities": []}, {"text": "Considering the polysemous words, some previous approaches have learned multiple embeddings fora word, discriminating different senses by their context, related syntax and topics.", "labels": [], "entities": []}, {"text": "The authors also provided methods to disambiguate among the multiple representations.", "labels": [], "entities": []}, {"text": "have demonstrated that multi-sense word embeddings could be helpful to improve the performance on many NLP and NLU tasks.", "labels": [], "entities": []}, {"text": "However, this leads to another problem.", "labels": [], "entities": []}, {"text": "It's much more difficult for computer than human beings to detect whether two appearances of a same word stand for the same sense.", "labels": [], "entities": []}, {"text": "Moreover, the contexts maybe totally different even if these appearances belong to the same meaning based on human judgement.", "labels": [], "entities": []}, {"text": "Previous multi-sense word embedding approaches often tend to embed a word in such situation into more than one vector by mistake (actually, they have the same meaning and should be embedded into only one vector).", "labels": [], "entities": []}, {"text": "Consider three different representations of word bear learnt by the method introduced by, which are shown by their nearest neighbors in the vector space MSSG-50d.", "labels": [], "entities": [{"text": "MSSG-50d", "start_pos": 153, "end_pos": 161, "type": "DATASET", "confidence": 0.9519228935241699}]}, {"text": "\u2022 emerald, bears, three-toed, snake, periwinkle, ruffed, hoopoe, distinctive, unmistakable \u2022 bird, wolf, arrow, pelican, emerald, canyon, diamond, buck, deer \u2022 pride, lady, hide, king, gift, crane, afflict, promise, reap, protect The words clearly related to the domain animals are bolded.", "labels": [], "entities": []}, {"text": "We could infer that the first two representations have the same meaning that points to the animal bear, and the third representation has different meaning.", "labels": [], "entities": []}, {"text": "We call such different learnt representations of a word with the same meaning (e.g. the first two representations of word bear shown above) pseudo multi-sense, where we judge whether senses are pseudo multi-sense by comparing their domains.", "labels": [], "entities": []}, {"text": "Given the word embeddings, which have multiple vectors for each polysemous word, we introduce an algorithm based on domains and semantic relations to detect pseudo multi-sense, since word representations which stand for the same meaning would have the same hypernym and belong to the same domain.", "labels": [], "entities": []}, {"text": "Then we try to eliminate the effect of pseudo multi-sense by training a global transition matrix which projects the original word vectors into anew vector space based on the detected pseudo multi-sense pairs, minimizing the distance between pseudo multi-sense pairs in the vector space while keeping the spatial relation of other pairs.", "labels": [], "entities": []}, {"text": "We propose the algorithm in Section 3 and evaluate it in Section 4.", "labels": [], "entities": []}, {"text": "Obviously, detecting and diminishing pseudo multi-sense would make word sense representations, which can be processed by computer, closer to human thinking.", "labels": [], "entities": []}, {"text": "We also suggest this approach can improve the performance on real world NLU tasks by evaluating the algorithm on the analogy test dataset introduced by, and also on WordSim-353 () and SCWS () dataset which include human judgements on similarity between pairs of words.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 165, "end_pos": 176, "type": "DATASET", "confidence": 0.9603714346885681}]}], "datasetContent": [{"text": "We evaluate our pseudo multi-sense detecting and eliminating method both qualitatively and quantitatively.", "labels": [], "entities": [{"text": "multi-sense detecting and eliminating", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.7133804336190224}]}, {"text": "We apply our method to the released word embeddings by and, which were both trained on the same Wikipedia corpus, and display the performance of our method based on the nearest neighbor task, word similarity tasks and the analogy task.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 192, "end_pos": 207, "type": "TASK", "confidence": 0.7195161879062653}]}], "tableCaptions": [{"text": " Table 2: Experimental result on WordSim-353 dataset (Spearman \u03c1 \u00d7 100). We apply both random  choosing and mean vector to compute the representative vector for each group of pseudo multi-sense.  Our method gains a slight improvement on all models except MSSG-300d.", "labels": [], "entities": [{"text": "WordSim-353 dataset", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.991764098405838}, {"text": "MSSG-300d", "start_pos": 255, "end_pos": 264, "type": "DATASET", "confidence": 0.9793274998664856}]}, {"text": " Table 3: Experimental result on SCWS dataset (Spearman \u03c1 \u00d7 100). It shows that the elimination  of pseudo multi-sense can significantly improves the performance of word embeddings with the metric  localSim, while the performances of projected vectors on the metric avgSim and avgSimC are about the  same as those of original vectors. In other words, the elimination of pseudo multi-sense improves the  ability of representing a real sense of each sense vector locally.", "labels": [], "entities": [{"text": "SCWS dataset", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.9046427309513092}]}, {"text": " Table 4: Sample quadruple instances in analogy testing dataset. The relations are divided into 5 semantic  types and 9 syntactic types.", "labels": [], "entities": []}, {"text": " Table 5: Test result for analogy task. We also apply both random choosing and mean vector to get the  representative vector for each pseudo multi-sense group. It shows that our improved vectors perform  better on this task.", "labels": [], "entities": [{"text": "analogy task", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.8914951384067535}]}]}