{"title": [{"text": "From Distributions to Labels: A Lexical Proficiency Analysis using Learner Corpora", "labels": [], "entities": [{"text": "Lexical Proficiency Analysis", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.8019891182581583}]}], "abstractContent": [{"text": "This paper presents work on how we can link word lists derived from learner corpora to target proficiency levels for lexical complexity analysis.", "labels": [], "entities": [{"text": "lexical complexity analysis", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.6772442857424418}]}, {"text": "The word lists present frequency distributions over different proficiency levels.", "labels": [], "entities": []}, {"text": "We present a mapping approach which takes these distributions and maps each word to a single proficiency level.", "labels": [], "entities": []}, {"text": "We are also investigating how we can evaluate the mapping from distribution to proficiency level.", "labels": [], "entities": []}, {"text": "We show that the distributional profile of words from the essays, informed with the essays' levels, consistently overlaps with our frequency-based method, in the sense that words holding the same level of proficiency as predicted by our mapping tend to cluster together in a semantic space.", "labels": [], "entities": []}, {"text": "In the absence of a gold standard, this information can be useful to see how often a word is associated with the same level in two different models.", "labels": [], "entities": []}, {"text": "Also, in this case we have a similarity measure that can show which words are more central to a given level and which words are more peripheral.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this work we look at how information from second language learner essay corpora can be used for the evaluation of unseen learner essays.", "labels": [], "entities": [{"text": "evaluation of unseen learner essays", "start_pos": 103, "end_pos": 138, "type": "TASK", "confidence": 0.7695402264595032}]}, {"text": "Using a corpus of learner essays which have been graded by well-trained human assessors using the Common European Framework of Reference (CEFR) (Council of), we extract a list of word distributions over CEFR levels.", "labels": [], "entities": [{"text": "Common European Framework of Reference (CEFR) (Council of)", "start_pos": 98, "end_pos": 156, "type": "DATASET", "confidence": 0.7661251773436865}]}, {"text": "For the analysis of unseen essays, we want to map each word to a so-called target CEFR level using this word list.", "labels": [], "entities": []}, {"text": "The aim of this project is two-fold: first, we want to create a list of words linked to target proficiency levels.", "labels": [], "entities": []}, {"text": "Second, we want to apply this list for lexical complexity analysis of unseen learner essays.", "labels": [], "entities": []}, {"text": "Most vocabulary lists used for second language learner evaluation, such as estimation of vocabulary size, are often derived from native speaker (L1) materials and thus might be ill suited to the needs of second language (L2) learners.", "labels": [], "entities": [{"text": "second language learner evaluation", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.6142958104610443}, {"text": "estimation of vocabulary size", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.8289930671453476}]}, {"text": "It is hypothesized that second language learners need to focus on aspects of a language which are not present in native speaker materials).", "labels": [], "entities": []}, {"text": "However, such word lists are important for example in essay classification or lexical complexity analysis).", "labels": [], "entities": [{"text": "essay classification", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.8301471769809723}, {"text": "lexical complexity analysis", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.6849851409594218}]}, {"text": "We thus base our approach on a learner corpus.", "labels": [], "entities": []}, {"text": "From this corpus, we extract a list of words with their frequency distributions across proficiency levels.", "labels": [], "entities": []}, {"text": "We then link each word to one single proficiency level.", "labels": [], "entities": []}, {"text": "In contrast to traditional frequency based proficiency estimations, our approach includes information about learners.", "labels": [], "entities": [{"text": "frequency based proficiency estimations", "start_pos": 27, "end_pos": 66, "type": "TASK", "confidence": 0.6606326624751091}]}, {"text": "We look at \"diversity\" of a word, i.e. by how many different learners the word has been used at each level.", "labels": [], "entities": []}, {"text": "We hypothesize that including diversity scores in the calculation of distributionto-label mapping yields more reliable and plausible mappings.", "labels": [], "entities": [{"text": "distributionto-label mapping", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.7364576160907745}]}, {"text": "The question that remains concerns evaluation.", "labels": [], "entities": [{"text": "evaluation", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.9585595726966858}]}, {"text": "How can we measure the \"accuracy\" of our mapping in the absence of a gold standard?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9987620115280151}]}, {"text": "We address this problem by, on one hand, taking into account expert knowledge from teachers in order to refine the algorithms and, on the other hand, using a second separate approach to see to what extent both methods overlap.", "labels": [], "entities": []}, {"text": "The method we have chosen for evaluation is a semantic space approach.", "labels": [], "entities": []}, {"text": "One of the advantages of the semantic space approach is that it gives us graded results; we can see to what extent words are similar to each other, possibly identifying core vocabulary and peripheral vocabulary at the different proficiency stages.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first reason we used a semantic space to model L2 essays vocabulary is to see whether, using a different approach, we might obtain results consistent with the frequency-based learner-augmented lists we described in the first part of the paper.", "labels": [], "entities": []}, {"text": "As we explained, we don't expect simple distributional models to work very well on this task, but we tried to monitor the performance of a so-called \"indexed method\" to try to make words characteristic of specific proficiency level closer between them and to the level label itself in the semantic space.", "labels": [], "entities": []}, {"text": "If a semantic space model trained as described above reproduces the predictions of our frequency-based lists (for example clustering together words that are in the same proficiency level in the lists) we could be a little more confident that our labeling is sensible.", "labels": [], "entities": []}, {"text": "To test this we randomly selected 100 words from our frequency lists, equally distributed among the 5 proficiency classes A1-C1.", "labels": [], "entities": []}, {"text": "On these 100 words we ran two tests: one based on the word-label cosine similarity, and one based on the word-word cosine similarity.", "labels": [], "entities": []}, {"text": "The first test selects, in the semantic space, the nearest proficiency label to a given word.", "labels": [], "entities": []}, {"text": "For example given the word eftersom 'because', we select the label holding the nearest cosine similarity with it, for example \"A2\"; if eftersom is mapped to the level A2 according to our mapping algorithm, we have an agreement among our models.", "labels": [], "entities": []}, {"text": "We can then count how many \"nearest labels\" coincided with the frequency-based prediction and determine to what extent the two approaches are consistent in modeling the data.", "labels": [], "entities": []}, {"text": "The second test consists in simply retrieving, for every word, its n-nearest neighbours in the semantic space.", "labels": [], "entities": []}, {"text": "We can then determine whether these neighbours belong to the same proficiency level of the given word in the frequency list.", "labels": [], "entities": []}, {"text": "For example, we can retrieve the nearest neighbours of the word tisdag 'Tuesday' and find them to be l\u00f6rdag 'Saturday' and tr\u00f6tt 'tired'.", "labels": [], "entities": []}, {"text": "If these two words are of the same proficiency level as tisdag in our lists, we can suppose a certain consistency between the two approaches.", "labels": [], "entities": []}, {"text": "shows the results for the different tests and different models.", "labels": [], "entities": []}, {"text": "We tested two indexed models, with window size 1 and 60 respectively, and a nonindexed model with window size 10.", "labels": [], "entities": []}, {"text": "The numbers indicate how many items were assigned the same proficiency levels in both the semantic space model and the frequency-based mapping, with the upper limit being 100.", "labels": [], "entities": []}, {"text": "We are indicating counts, but as the upper limit is 100, the numbers can also be understood as percentages.", "labels": [], "entities": []}, {"text": "For the word-word similarity test, we look at the first, second and third most similar words according to the cosine similarity and check whether their proficiency label is the same as the one assigned by the frequency-based mapping.", "labels": [], "entities": []}, {"text": "The figures in parentheses indicate the number of close mismatches (off-by-one errors).", "labels": [], "entities": [{"text": "number of close mismatches (off-by-one errors", "start_pos": 40, "end_pos": 85, "type": "METRIC", "confidence": 0.6875449887343815}]}, {"text": "Apparently, an \"indexed\" semantic space with a large window shows the highest agreement with our model.", "labels": [], "entities": [{"text": "agreement", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9578568935394287}]}, {"text": "Considering that we are predicting labels word-label test 38 24 28 (34) over five proficiency levels, accuracies of 51% and 67% are encouraging numbers.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9959983825683594}]}, {"text": "What is maybe even more interesting is the number of close mismatches.", "labels": [], "entities": []}, {"text": "These cases are interesting because they could show that the models are setting different boundaries, but tendentially agree on the general progression of the vocabulary.", "labels": [], "entities": []}, {"text": "If the number of close mismatches is high, it means that we have many cases where A1 words (in our frequency list) are \"labeled\" as, or cluster with, A2 words in the semantic space: it is easy to see that similar cases are qualitatively very different from cases where an A1 word clusters with C1 vocabulary.", "labels": [], "entities": []}, {"text": "The large presence of similar cases in our results brings us the next reason that induced us to use semantic spaces: they can give nuanced results.", "labels": [], "entities": []}, {"text": "If we use a distributional space to label a lemma, we'll have not only the most probable level of such lemma, but also its distance to the next and previous level.", "labels": [], "entities": []}, {"text": "For example, both our frequency list and our best performing semantic space label resa 'to travel' as an A2 word.", "labels": [], "entities": []}, {"text": "From the semantic space, we can also see that it is much closer to B1 than to A1 -we can suppose that it is a rather \"advanced\" word that tends to lie between A2 and B1.", "labels": [], "entities": [{"text": "A1", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9567224979400635}]}, {"text": "In the same way, fredag 'Friday', labeled as A2 by the frequency lists, clusters in our space both with A2 and (less closely) A1 lemmas, showing that it is likely to be a term on the \"easy\" spectrum of the A2 vocabulary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of items per CEFR level", "labels": [], "entities": [{"text": "CEFR", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9356284737586975}]}, {"text": " Table 2: Extracted data: Example", "labels": [], "entities": [{"text": "Example", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.4614727795124054}]}]}