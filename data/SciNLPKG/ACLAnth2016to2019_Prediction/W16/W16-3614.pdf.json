{"title": [{"text": "Language Portability for Dialogue Systems: Translating a Question-Answering System from English into Tamil", "labels": [], "entities": []}], "abstractContent": [{"text": "A training and test set fora dialogue system in the form of linked questions and responses is translated from English into Tamil.", "labels": [], "entities": []}, {"text": "Accuracy of identifying an appropriate response in Tamil is 79%, compared to the English accuracy of 89%, suggesting that translation can be useful to startup a dialogue system.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.97245854139328}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.8178579807281494}]}, {"text": "Machine translation of Tamil inputs into English also results in 79% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9989762306213379}]}, {"text": "However, machine translation of the English training data into Tamil results in a drop inaccuracy to 54% when tested on manually authored Tamil, indicating that there is still a large gap before machine translated dialogue systems can interact with human users.", "labels": [], "entities": []}], "introductionContent": [{"text": "Much of the effort in creating a dialogue system is devoted to the collection of training data, to allow the system to process, understand, and react to input coming from real users.", "labels": [], "entities": []}, {"text": "If a comparable system is available fora different language, it would be helpful to use some of the existing foreign language resources in order to cut down the development time and cost -an issue known as language portability.", "labels": [], "entities": [{"text": "language portability", "start_pos": 206, "end_pos": 226, "type": "TASK", "confidence": 0.6997962296009064}]}, {"text": "Recent efforts have shown machine translation to bean effective tool for porting dialogue system resources from French into Italian (; this system used concept-based language understanding, and the findings were that machine translation of the target language inputs yielded better results than using translation to train an understanding component directly for the target language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7570918798446655}]}, {"text": "Here we report similar findings on more challenging data, by exploring a dialogue system with a less structured understanding component, using off-the-shelf rather than domainadapted machine translation, and with languages that are not as closely related.", "labels": [], "entities": []}, {"text": "Question-answering characters are designed to sustain a conversation driven primarily by the user asking questions.", "labels": [], "entities": []}, {"text": "developed algorithms for training such characters using linked questions and responses in the form of unstructured natural language text.", "labels": [], "entities": []}, {"text": "Given a novel user question, the character finds an appropriate response from a list of available responses, and when a direct answer is not available, the character selects an \"off-topic\" response according to a set policy, ensuring that the conversation remains coherent even with a finite number of responses.", "labels": [], "entities": []}, {"text": "The response selection algorithms are languageindependent, also allowing the questions and responses to be in separate languages.", "labels": [], "entities": [{"text": "response selection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7563014030456543}]}, {"text": "These algorithms have been incorporated into a tool ( ) which has been used to create characters fora variety of applications (e.g..", "labels": [], "entities": []}, {"text": "To date, most characters created using these principles understood and spoke only English; one fairly limited character spoke Pashto, a language of Afghanistan (.", "labels": [], "entities": []}, {"text": "To test language portability we chose Tamil, a Dravidian language spoken primarily in southern India.", "labels": [], "entities": []}, {"text": "Tamil has close to 70 million speakers worldwide (, is the official language of Tamil Nadu and Puducherry in India, and an official language in Sri Lanka and Singapore.", "labels": [], "entities": []}, {"text": "There is active development of language processing tools in Tamil such as stemmers (, POS taggers (, constituent and dependency parsers (; Ramasamy and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2011), sentence generators, etc.; commercial systems are also available, such as Google Translate 1 between Tamil and English.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.6447884887456894}]}, {"text": "Information-providing spoken dialogue systems have been developed for Tamil), but we are not aware of any conversational dialogue systems.", "labels": [], "entities": []}, {"text": "The main questions we want to answer in this paper are: (Q1) How good is a dialogue system created using translation between English and Tamil?", "labels": [], "entities": []}, {"text": "(Q2) Is there a difference between manual and machine translation in this regard?", "labels": [], "entities": [{"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7198189944028854}]}, {"text": "(Q3) Can machine translation be used for interaction with users, that is with manually translated test data?", "labels": [], "entities": [{"text": "machine translation", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7993929982185364}]}, {"text": "To answer these questions, we translated linked questions and responses from an English questionanswering system into Tamil both mechanically and manually, and tested the response selection algorithms on the English and both versions of the Tamil data.", "labels": [], "entities": []}, {"text": "We found that translation caused a drop in performance of about 10% on either manually or mechanically translated text, answering a tentative fair to Q1 and no to Q2.", "labels": [], "entities": [{"text": "translation", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9809366464614868}]}, {"text": "The answer to Q3 is mixed: a similar performance drop of about 10% was found with machine translation on the target language inputs (that is, translating test questions from Tamil into English); a much more severe drop in performance was observed when using machine translation to create a system in the target language (that is, translating the training data from English into Tamil, and testing on manually authored Tamil).", "labels": [], "entities": []}, {"text": "The remainder of the paper describes the experiment and results, and concludes with directions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use accuracy as our success measure: the top ranked response to a test question is considered correct if it is identified as a correct response in the linked test data (there are up to 4 correct responses per question).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.999097466468811}]}, {"text": "This measure does not take into account non-understanding, that is the classifier's determination that the best response is not good enough (), since this capability was not implemented; however, since all of our test questions are known to have at least one appropriate response, any non-understanding of a question would necessarily count against accuracy anyway.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 349, "end_pos": 357, "type": "METRIC", "confidence": 0.9983834028244019}]}], "tableCaptions": [{"text": " Table 1: Response accuracy on 28 test questions", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8927176594734192}]}, {"text": " Table 2: Accuracy with question and response in  different languages (G = Google, M = manual)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.997232973575592}]}]}