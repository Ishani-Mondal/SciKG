{"title": [{"text": "Training an adaptive dialogue policy for interactive learning of visually grounded word meanings", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a multi-modal dialogue system for interactive learning of perceptually grounded word meanings from a human tutor.", "labels": [], "entities": [{"text": "interactive learning of perceptually grounded word meanings", "start_pos": 45, "end_pos": 104, "type": "TASK", "confidence": 0.6350504245076861}]}, {"text": "The system integrates an incremen-tal, semantic parsing/generation framework Dynamic Syntax and Type Theory with Records (DS-TTR)-with a set of visual classifiers that are learned throughout the interaction and which ground the meaning representations that it produces.", "labels": [], "entities": [{"text": "semantic parsing/generation framework Dynamic Syntax and Type Theory", "start_pos": 39, "end_pos": 107, "type": "TASK", "confidence": 0.6806294620037079}]}, {"text": "We use this system in interaction with a simulated human tutor to study the effects of different dialogue policies and capabilities on accuracy of learned meanings , learning rates, and efforts/costs to the tutor.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9932214021682739}]}, {"text": "We show that the overall performance of the learning agent is affected by (1) who takes initiative in the dialogues; (2) the ability to express/use their confidence level about visual attributes; and (3) the ability to process elliptical and incre-mentally constructed dialogue turns.", "labels": [], "entities": []}, {"text": "Ultimately , we train an adaptive dialogue policy which optimises the trade-off between classifier accuracy and tutoring costs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9624291062355042}]}], "introductionContent": [{"text": "Identifying, classifying, and talking about objects or events in the surrounding environment are key capabilities for intelligent, goal-driven systems that interact with other agents and the external world (e.g. robots, smart spaces, and other automated systems).", "labels": [], "entities": [{"text": "Identifying, classifying, and talking about objects or events in the surrounding environment", "start_pos": 0, "end_pos": 92, "type": "TASK", "confidence": 0.5945382373673576}]}, {"text": "To this end, there has recently been a surge of interest and significant progress made on a variety of related tasks, including generation of Natural Language (NL) descriptions of images, or identifying images based on NL descriptions; Bruni et.", "labels": [], "entities": [{"text": "generation of Natural Language (NL) descriptions of images", "start_pos": 128, "end_pos": 186, "type": "TASK", "confidence": 0.7638427972793579}]}, {"text": "Our goal is to build interactive systems that can learn grounded word meanings relating to their perceptions of real-world objects -this is different from previous work such as e.g., that learn groundings from descriptions without any interaction, and more recent work using Deep Learning methods (e.g. ().", "labels": [], "entities": []}, {"text": "Most of these systems rely on training data of high quantity with no possibility of online error correction.", "labels": [], "entities": []}, {"text": "Furthermore, they are unsuitable for robots and multimodal systems that need to continuously, and incrementally learn from the environment, and may encounter objects they haven't seen in training data.", "labels": [], "entities": []}, {"text": "These limitations are likely to be alleviated if systems can learn concepts, as and when needed, from situated dialogue with humans.", "labels": [], "entities": []}, {"text": "Interaction with a human tutor also enables systems to take initiative and seek the particular information they need or lack by e.g. asking questions with the highest information gain (see e.g. (), and).", "labels": [], "entities": []}, {"text": "For example, a robot could ask questions to learn the colour of a \"square\" or to request to be presented with more \"red\" things to improve its performance on the concept (see e.g.).", "labels": [], "entities": []}, {"text": "Furthermore, such systems could allow for meaning negotiation in the form of clarification interactions with the tutor.", "labels": [], "entities": [{"text": "meaning negotiation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8053838312625885}]}, {"text": "This setting means that the system must be trainable from little data, compositional, adaptive, and able to handle natural human dialogue with all its glorious context-sensitivity and messiness -for instance so that it can learn visual concepts suitable for specific tasks/domains, or even those specific to a particular user.", "labels": [], "entities": []}, {"text": "Interactive systems that learn continuously, and over the long run from humans need to do so incrementally, quickly, and with minimal effort/cost to human tutors.", "labels": [], "entities": []}, {"text": "In this paper, we first outline an implemented dialogue system that integrates an incremental, semantic grammar framework, especially suited to dialogue processing -Dynamic Syntax and Type Theory with Records (DS-TTR 1 () with visual classifiers which are learned during the interaction, and which provide perceptual grounding for the basic semantic atoms in the semantic representations (Record Types in TTR) produced by the parser (see, and section 3).", "labels": [], "entities": [{"text": "dialogue processing", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7289535999298096}]}, {"text": "We then use this system in interaction with a simulated human tutor, to test hypotheses about how the accuracy of learned meanings, learning rates, and the overall cost/effort for the human tutor are affected by different dialogue policies and capabilities: (1) who takes initiative in the dialogues; (2) the agent's ability to utilise their level of uncertainty about an object's attributes; and (3) their ability to process elliptical as well as incrementally constructed dialogue turns.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9857465028762817}]}, {"text": "The results show that differences along these dimensions have significant impact both on the accuracy of the grounded word meanings that are learned, and the processing effort required by the tutors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9986984729766846}]}, {"text": "In section 4.3 we train an adaptive dialogue strategy that finds a better trade-off between classifier accuracy and tutor cost.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9669867157936096}]}], "datasetContent": [{"text": "As noted in the introduction, interactive systems that learn continuously, and over the long run from humans need to do so incrementally; as quickly as possible; and with as little effort/cost to the human tutor as possible.", "labels": [], "entities": []}, {"text": "In addition, when learning takes place through dialogue, the dialogue needs to be as human-like/natural as possible.", "labels": [], "entities": []}, {"text": "In general, there are several different dialogue capabilities and policies that a concept-learning agent might adopt, and these will lead to different outcomes for the accuracy of the learned concepts/meanings, learning rates, and cost to the tutor -with trade-offs between these.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9738880395889282}]}, {"text": "Our goal in this paper is therefore an experimental study of the effect of different dialogue policies and capabilities on the overall performance of the learning agent, which, as we describe below is a measure capturing the trade-off between accuracy of learned meanings and the cost of tutoring.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 243, "end_pos": 251, "type": "METRIC", "confidence": 0.9903139472007751}]}, {"text": "We use the dialogue system outlined above to carryout our main experiment with a 2 \u00d7 2 \u00d7 2 factorial design, i.e. with three factors each with two levels.", "labels": [], "entities": []}, {"text": "Together, these factors determine the learner's dialogue behaviour: (1) Initiative (Learner/Tutor): determines who takes initiative in the dialogues.", "labels": [], "entities": []}, {"text": "When the tutor takes initiative, s/he is the one that drives the conversation forward, by asking questions to the learner (e.g. \"What colour is this?\" or \"So this is a ....\"", "labels": [], "entities": []}, {"text": ") or making a statement about the attributes of the object.", "labels": [], "entities": []}, {"text": "On the other hand, when the learner has initiative, it makes statements, asks questions, initiates topics etc.", "labels": [], "entities": []}, {"text": "(2) Uncertainty (+UC/-UC): determines whether the learner takes into account, in its dialogue behaviour, its own subjective confidence about the attributes of the presented object.", "labels": [], "entities": []}, {"text": "The confidence is the probability assigned by any of its attribute classifiers of the object being a positive instance of an attribute (e.g. 'red') -see below for how a confidence threshold is used here.", "labels": [], "entities": []}, {"text": "In +UC, the agent will not ask a question if it is confident about the answer, and it will hedge the answer to a tutor question if it is not confident, e.g. \"T: What is this?", "labels": [], "entities": []}, {"text": "L: errm, maybe a square?\".", "labels": [], "entities": []}, {"text": "In -UC, the agent always takes itself to know the attributes of the given object (as given by its currently trained classifiers), and behaves according to that assumption.", "labels": [], "entities": []}, {"text": "(3) Context-Dependency (+CD/-CD): determines whether the learner can process (produce/parse) context-dependent expressions such as short answers and incrementally constructed turns, e.g. \"T: What is this?", "labels": [], "entities": []}, {"text": "L: a square\", or \"T:  To test how the different dialogue capabilities and strategies affect the learning process, we consider both the cost to the tutor and the accuracy of the learned meanings, i.e. the classifiers that ground our colour and shape concepts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9981343150138855}]}, {"text": "Cost The cost measure reflects the effort needed by a human tutor in interacting with the system.", "labels": [], "entities": []}, {"text": "point out that a comprehensive teachable system should learn as autonomously as possible, rather than involving the human tutor too frequently.", "labels": [], "entities": []}, {"text": "We associate a higher cost with correction of statements than that of polar questions.", "labels": [], "entities": []}, {"text": "This is to penalise the learning agent when it confidently makes a false statement -thereby incorporating an aspect of trust in the metric (humans will not trust systems which confidently make false statements).", "labels": [], "entities": []}, {"text": "And finally, parsing (C parse ) as well as production (C production ) costs for tutor are taken into account: each single word costs 0.5 when parsed by the tutor, and 1 if generated (production costs twice as much as parsing).", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9548205137252808}]}, {"text": "These exact values are based on intuition but are kept constant across the experimental conditions and therefore do not confound the results reported below.", "labels": [], "entities": []}, {"text": "Learning Performance As mentioned above, an efficient learner dialogue policy should consider both classification accuracy and tutor effort (Cost).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9297380447387695}, {"text": "tutor effort (Cost)", "start_pos": 127, "end_pos": 146, "type": "METRIC", "confidence": 0.9234349727630615}]}, {"text": "We thus define an integrated measure -the Overall Performance Ratio (R per f ) -that we use to compare the learner's overall performance across the different conditions: i.e. the increase inaccuracy per unit of the cost, or equivalently the gradient of the curve in.", "labels": [], "entities": [{"text": "Overall Performance Ratio (R per f )", "start_pos": 42, "end_pos": 78, "type": "METRIC", "confidence": 0.8412024192512035}]}, {"text": "We seek dialogue strategies that maximise this.", "labels": [], "entities": []}, {"text": "Dataset The dataset used here is comprised of 600 images of single, simple handmade objects with a white background (see) . There are nine attributes considered in this dataset: 6 colours (black, blue, green, orange, purple and red) and 3 shapes (circle, square and triangle), with a relative balance on the number of instances per attribute.", "labels": [], "entities": []}, {"text": "In each round, the system is trained using 500 training instances, with the rest set aside for test-3 All data from this paper will be made freely available. ing.", "labels": [], "entities": []}, {"text": "For each training instance, the system interacts (only through dialogue) with the simulated tutor.", "labels": [], "entities": []}, {"text": "Each dialogue about an object ends either when both the shape and the colour of the object are discussed and agreed upon, or when the learner requests to be presented with the next image (this happens only in the Learner initiative conditions).", "labels": [], "entities": []}, {"text": "We define a Learning Step as comprised of 10 such dialogues.", "labels": [], "entities": []}, {"text": "At the end of each learning step, the system is tested using the test set (100 test instances).", "labels": [], "entities": []}, {"text": "This process is repeated 20 times, i.e. for 20 rounds/folds, each time with a different, random 500-100 split, thus resulting in 20 data-points for cost and accuracy after every learning step.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9967431426048279}]}, {"text": "The values reported below, including those on the plots in and 6c, correspond to averages across the 20 folds.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tutoring Cost Table  C in f C ack C crt C parsing C production  1  0.25  1  0.5  1", "labels": [], "entities": [{"text": "Tutoring", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.973952054977417}, {"text": "ack C crt C parsing C production  1  0.25", "start_pos": 40, "end_pos": 81, "type": "METRIC", "confidence": 0.6603670517603556}]}]}