{"title": [], "abstractContent": [{"text": "We describe the statistical machine translation system developed at the National Research Council of Canada (NRC) for the Russian-English news translation task of the First Conference on Machine Translation (WMT 2016).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.6127565403779348}, {"text": "Russian-English news translation task of the First Conference on Machine Translation (WMT 2016)", "start_pos": 122, "end_pos": 217, "type": "TASK", "confidence": 0.8386492590109508}]}, {"text": "Our submission is a phrase-based SMT system that tackles the morphological complexity of Russian through comprehensive use of lemmatiza-tion.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8426999449729919}]}, {"text": "The core of our lemmatization strategy is to use different views of Russian for different SMT components: word alignment and bilingual neural network language models use lemmas, while sparse features and reordering models use fully inflected forms.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9937291145324707}, {"text": "word alignment", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7849890291690826}]}, {"text": "Some components, such as the phrase table, use both views of the source.", "labels": [], "entities": []}, {"text": "Russian words that remain out-of-vocabulary (OOV) after lemmatization are transliterated into English using a statistical model trained on examples mined from the parallel training corpus.", "labels": [], "entities": []}, {"text": "The NRC Russian-English MT system achieved the highest uncased BLEU and the lowest TER scores among the eight participants in WMT 2016.", "labels": [], "entities": [{"text": "NRC Russian-English MT", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6215876638889313}, {"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9926359057426453}, {"text": "TER", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9992641806602478}, {"text": "WMT 2016", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.7537810206413269}]}], "introductionContent": [{"text": "We present NRC's submission to the RussianEnglish news translation task of WMT 2016.", "labels": [], "entities": [{"text": "NRC", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.9165827035903931}, {"text": "RussianEnglish news translation task of WMT 2016", "start_pos": 35, "end_pos": 83, "type": "TASK", "confidence": 0.7157547048160008}]}, {"text": "Russian-English is a challenging language pair for statistical machine translation because Russian is a highly inflectional and free word order language.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.7312873899936676}]}, {"text": "Case information is encoded by modifying the Russian words, which makes the number of word types present in the Russian side of a RussianEnglish parallel corpus much higher than in the English side, introducing a data sparsity problem.", "labels": [], "entities": []}, {"text": "Lemmatization is one of the possible solutions for handling data sparsity when translating highly inflectional languages.", "labels": [], "entities": []}, {"text": "However, Russian is a free word order language, meaning that case information conveyed through inflection plays an important role in disambiguating the meaning of a sentence.", "labels": [], "entities": []}, {"text": "The MT system would be unable to recover this case information if we were to blindly lemmatize all the Russian words to their root form.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.906193196773529}]}, {"text": "Instead, we rely most heavily on lemmatization only when the missing inflections are unlikely to cause ambiguity.", "labels": [], "entities": []}, {"text": "For example, in automatic word alignment, the missing case information should not confuse the system as competing inflections are unlikely to appear in the same sentence.", "labels": [], "entities": [{"text": "automatic word alignment", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.6239931384722391}]}, {"text": "Therefore, we build automatic word alignments with lemmatized Russian, but then restore the Russian lemmas to their inflected forms before estimating our other model parameters.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7268621325492859}]}, {"text": "The end result is a system with higher-quality word alignments, but which can still use case information to drive its translation and reordering models.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7076555341482162}]}, {"text": "Similarly, our bilingual language models have large source context windows that allow them to resolve ambiguities introduced by lemmatization, so we build these based on lemmatized versions of the source by default.", "labels": [], "entities": []}, {"text": "These include neural network joint models (NNJMs) and lexical translation models (NNLTMs)).", "labels": [], "entities": []}, {"text": "We have found that blind lemmatization of phrase tables is actually quite harmful to translation, but Russian morphology still causes a significant increase in the number of OOVs.", "labels": [], "entities": [{"text": "translation", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.9762665033340454}]}, {"text": "Therefore, we built a fallback Russian lemma phrase table for the OOVs in the Russian input, implemented as an interpolated phrase table.", "labels": [], "entities": []}, {"text": "For any remaining Russian OOVs, we use a semi-supervised transliteration system to translate the word orthographically.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out a large number of development experiments throughout the design of this system, using the data conditions described in Section 2.1, with the WMT 2014 test set as our tuning set (dev), and the WMT 2015 test set as our test set.", "labels": [], "entities": [{"text": "WMT 2014 test set", "start_pos": 156, "end_pos": 173, "type": "DATASET", "confidence": 0.9573770463466644}, {"text": "WMT 2015 test set", "start_pos": 207, "end_pos": 224, "type": "DATASET", "confidence": 0.9731865376234055}]}, {"text": "We monitored uncased BLEU on a systemtokenized version of the test set, reporting the average and the best of 5 random tuning replications.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9921107888221741}]}, {"text": "shows an example of how the different components improve the translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.9593645930290222}]}, {"text": "The word baseline reflects a system with standard phrase-based features, reordering models, sparse features, monolingual language models and an uninterpolated phrase table.", "labels": [], "entities": []}, {"text": "We then replace the word alignment for all components with lemma alignment to form the lemma baseline.", "labels": [], "entities": []}, {"text": "We then add the neural components, the fallback lemma table and the transliteration component.", "labels": [], "entities": []}, {"text": "The rescoring step is only done on the best model as the final step before recasing and detokenizing.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.958649218082428}]}, {"text": "Given such a strong lemma baseline, the biggest impact comes from the addition of the first NNJM.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.9667112827301025}]}, {"text": "The next largest jump comes from the fallback Russian lemma phrase table, which also improved our OOV rate considerably.", "labels": [], "entities": [{"text": "Russian lemma phrase table", "start_pos": 46, "end_pos": 72, "type": "DATASET", "confidence": 0.714867927134037}, {"text": "OOV rate", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9816422760486603}]}, {"text": "We were pleasantly surprised to seethe transliteration component helping to the extent that it does.", "labels": [], "entities": []}, {"text": "These sorts of point-wise vocabulary improvements do not always have a visible impact on BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9672712683677673}]}, {"text": "We are optimistic that its impact will be even more pronounced in the human evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Selected results from our development  experiments.", "labels": [], "entities": []}]}