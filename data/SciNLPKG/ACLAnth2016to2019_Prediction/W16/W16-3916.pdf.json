{"title": [{"text": "Exploring Word Embeddings for Unsupervised Textual User-Generated Content Normalization", "labels": [], "entities": [{"text": "Textual User-Generated Content Normalization", "start_pos": 43, "end_pos": 87, "type": "TASK", "confidence": 0.6589305698871613}]}], "abstractContent": [{"text": "Text normalization techniques based on rules, lexicons or supervised training requiring large corpora are not scalable nor domain interchangeable, and this makes them unsuitable for normalizing user-generated content (UGC).", "labels": [], "entities": [{"text": "Text normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6875256150960922}, {"text": "normalizing user-generated content (UGC)", "start_pos": 182, "end_pos": 222, "type": "TASK", "confidence": 0.6665308872858683}]}, {"text": "Current tools available for Brazilian Portuguese make use of such techniques.", "labels": [], "entities": []}, {"text": "In this work we propose a technique based on distributed representation of words (or word embeddings).", "labels": [], "entities": []}, {"text": "It generates continuous numeric vectors of high-dimensionality to represent words.", "labels": [], "entities": []}, {"text": "The vectors explicitly encode many linguistic regularities and patterns, as well as syntactic and semantic word relationships.", "labels": [], "entities": []}, {"text": "Words that share semantic similarity are represented by similar vectors.", "labels": [], "entities": []}, {"text": "Based on these features, we present a totally unsupervised, expandable and language and domain independent method for learning normalization lexicons from word embeddings.", "labels": [], "entities": [{"text": "learning normalization lexicons from word embeddings", "start_pos": 118, "end_pos": 170, "type": "TASK", "confidence": 0.7858602503935496}]}, {"text": "Our approach obtains high correction rate of orthographic errors and internet slang in product reviews, outperforming the current available tools for Brazilian Portuguese.", "labels": [], "entities": [{"text": "correction rate", "start_pos": 26, "end_pos": 41, "type": "METRIC", "confidence": 0.9780960381031036}]}], "introductionContent": [{"text": "The huge amount of data currently available on the Web allows computer-based knowledge discovery to thrive.", "labels": [], "entities": [{"text": "knowledge discovery", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7365136742591858}]}, {"text": "The growth in recent years of user-generated content (UGC) -especially the one created by ordinary people -brings forth anew niche of promising practical applications (.", "labels": [], "entities": []}, {"text": "Textual UGC, such as product reviews, blogs, and social network posts, often serves as abase for natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "This type of content maybe used for business intelligence, targeted marketing, prediction of political election results, analysis of sociolinguistic phenomena, among many other possibilities.", "labels": [], "entities": [{"text": "prediction of political election results", "start_pos": 79, "end_pos": 119, "type": "TASK", "confidence": 0.8840214371681213}]}, {"text": "Despite its wide range of application, UGC is hard for NLP to handle.", "labels": [], "entities": []}, {"text": "Most of the natural language processing tools and techniques are developed from and for texts of standard language.", "labels": [], "entities": []}, {"text": "From basic components of a NLP-based system, such as taggers, to complex tools aiming to tackle more significant problems, there is a reliance on well structured textual information in order to achieve a proper behavior.", "labels": [], "entities": []}, {"text": "However, user generated content does not necessarily follow the structured form of standard language.", "labels": [], "entities": []}, {"text": "This type of text is often full of idiosyncrasies, which represent noise for NLP purposes.", "labels": [], "entities": []}, {"text": "Beyond the context of NLP, textual UGC may also represent an obstacle for end-users.", "labels": [], "entities": []}, {"text": "Specially on social networks, where a domain-specific and context reliant language is used, users not familiar to its particularities may have difficulties to fully grasp the expressed content.", "labels": [], "entities": []}, {"text": "Considering the aforementioned problems, it is relevant to identify noises in UGC for purposes which include a simple identification of the noise, its typification, eventually its substitution for another word or expression.", "labels": [], "entities": []}, {"text": "These actions aim to both enhance NLP tools performance and facilitate end-user text comprehension, besides to provide an overview of the linguistics practices on the web.", "labels": [], "entities": []}, {"text": "The process of identifying noise and suggesting possible substitutions is known as text normalization.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.788445383310318}]}, {"text": "Non-standard words (NSWs) are often regarded as noise, but the precise definition of what constitutes them depends on the application domain.", "labels": [], "entities": []}, {"text": "Some examples that might be seen as deviations from standard language and that should be normalized include spelling errors, abbreviations, mixed case words, acronyms, internet slang, hashtags, and emoticons.", "labels": [], "entities": []}, {"text": "In general, NSWs are words which properties and meaning cannot be derived directly from a lexicon ().", "labels": [], "entities": []}, {"text": "The term \"lexicon\" in this context does not necessarily mean the list of words that are formally recognized in a language, but rather a set of words that are considered treatable by the specific application.", "labels": [], "entities": []}, {"text": "Therefore, it is not possible to clearly state what is noise and what should be normalized.", "labels": [], "entities": []}, {"text": "In contrast to standardized language, UGC is often informal, with less adherence to conventions regarding punctuation, spelling, and style (.", "labels": [], "entities": []}, {"text": "Therefore, it is considerably noisy, containing ad-hoc abbreviations, phonetic substitutions, customized abbreviations, and slang language.", "labels": [], "entities": []}, {"text": "Considering the specificity of such content, traditional techniques -such as lexiconbased substitution -are not capable to properly handle with many types of noise.", "labels": [], "entities": []}, {"text": "Thus, in order to achieve satisfactory results, it is necessary to deeply analyze UGC and develop specific methods aimed at normalizing it.", "labels": [], "entities": []}, {"text": "Conventional string similarity measures (such as edit distance) are not, by themselves, capable of accurately correcting many errors found in UGC.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 49, "end_pos": 62, "type": "METRIC", "confidence": 0.6776802688837051}]}, {"text": "Abbreviations and shorthands found in informal texts, specially on social networks, may contain a large number of edits often resulting in low string similarity -discouraging the use of traditional techniques.", "labels": [], "entities": []}, {"text": "More recently, the interest in techniques based on distributed representations of words (also called word embeddings) has increased.", "labels": [], "entities": []}, {"text": "These representations are able to generate continuous numeric vectors of high-dimensionality to represent words.", "labels": [], "entities": []}, {"text": "The vectors explicitly encode many linguistic regularities and patterns, as well as syntactic and semantic word relationships (.", "labels": [], "entities": []}, {"text": "Words that share semantic similarity are represented by similar vectors.", "labels": [], "entities": []}, {"text": "For example, the result of a vector calculation vec(\"King\") -vec(\"Man\") + vec(\"Woman\") is closer to vec(\"Queen\") than to any other word vector.", "labels": [], "entities": []}, {"text": "Another strong characteristic of distributed representations is their capability of capturing the notion of contextual similarity -which is essential for textual normalization.", "labels": [], "entities": [{"text": "textual normalization", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.7419408559799194}]}, {"text": "The models used for learning these vectors, such as Skip-grams, are totally unsupervised and can be implemented efficiently.", "labels": [], "entities": []}, {"text": "In this work, we exploit this set of advantages, combined with lexical similarity measures, in order to capture contextual similarity and learn normalization lexicons based on word embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the proposed method and compare it to already existing tools for Brazilian Portuguese, two annotated samples of product reviews written by users were used.", "labels": [], "entities": []}, {"text": "Each sample contains 60 reviews, with every error manually annotated by a specialist.", "labels": [], "entities": []}, {"text": "The annotation considers the six categories of noise proposed by), but our technique can only be applied to the correction of orthographic errors and internet slang.", "labels": [], "entities": []}, {"text": "First, we conducted experiments in order to determine the best word embedding model.", "labels": [], "entities": []}, {"text": "We trained both Skip-gram and Continuous bag-of-words (Cbow) models, implemented in Gensim, to learn the embeddings.", "labels": [], "entities": []}, {"text": "We used a context window of size 5, i.e 2 words before and 2 after the center, and used hierarchical sampling for reducing the vocabulary size during training, considering only words with a minimum count of 10 occurrences.", "labels": [], "entities": []}, {"text": "We generated embeddings with 100, 300 and 500 dimensions.", "labels": [], "entities": []}, {"text": "For each dimension size, we trained the embeddings on two different sets of data: the first one, referred as Noisy, is exactly as described in Section 3, and the second one, referred as Hybrid, includes an additional dataset containing 38 million sentences from Wikipedia.", "labels": [], "entities": []}, {"text": "shows the recall measure obtained for the correction of orthographic errors (O) (without the LM probabilities) and internet slang (I) (with the best setup) using each embedding model trained.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9992824196815491}, {"text": "orthographic errors (O)", "start_pos": 56, "end_pos": 79, "type": "METRIC", "confidence": 0.7071913421154022}]}, {"text": "The Skip-gram model with 500 dimensions showed the best results.", "labels": [], "entities": []}, {"text": "Therefore, further experiments were conducted using these embeddings.", "labels": [], "entities": []}, {"text": "In order to better evaluate the framework, we performed experiments using three different learned lexicons models.", "labels": [], "entities": []}, {"text": "The only difference between them is the word embedding model employed.", "labels": [], "entities": []}, {"text": "The first one, referred as Noisy, uses embeddings trained on data preprocessed exactly as described in Section 3.", "labels": [], "entities": []}, {"text": "The second model, referred as Clean, uses embedding trained on data with an additional preprocessing step: every symbol that is not either a letter or a number is removed.", "labels": [], "entities": []}, {"text": "Both are Skip-gram models with 500 dimensions.", "labels": [], "entities": []}, {"text": "The third model is an Ensemble containing the previous ones.", "labels": [], "entities": []}, {"text": "Its output is defined as max(output(Clean), output(N oisy)).", "labels": [], "entities": []}, {"text": "We have found empirically that the Noisy model is better for internet slang correction, the Clean model is better for orthographic errors and the Ensemble joins the best of both.", "labels": [], "entities": [{"text": "internet slang correction", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.558175394932429}]}, {"text": "In we compare the three lex-icon models with UGCNormal, the tool proposed by.", "labels": [], "entities": []}, {"text": "The tables follow the format X/Y = Z, being X the total of corrections performed by each model, Y the total of annotated errors in the sample and Z the obtained recall measure.", "labels": [], "entities": [{"text": "recall measure", "start_pos": 161, "end_pos": 175, "type": "METRIC", "confidence": 0.9777524173259735}]}, {"text": "The results show that the ensemble model with expansion step and context representation (LM) outperforms every other model, including UGCNormal.", "labels": [], "entities": [{"text": "UGCNormal", "start_pos": 134, "end_pos": 143, "type": "DATASET", "confidence": 0.8743618130683899}]}, {"text": "For RWEs, the combination of expansion step and language model outperforms UGCNormal by a large margin.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for error correction obtained from each word embedding model", "labels": [], "entities": []}]}