{"title": [{"text": "How Do Cultural Differences Impact the Quality of Sarcasm Annotation?: A Case Study of Indian Annotators and American Text", "labels": [], "entities": [{"text": "Sarcasm Annotation", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8322866261005402}]}], "abstractContent": [{"text": "Sarcasm annotation extends beyond linguistic expertise, and often involves cultural context.", "labels": [], "entities": [{"text": "Sarcasm annotation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.933772623538971}]}, {"text": "This paper presents our first-of-its-kind study that deals with impact of cultural differences on the quality of sarcasm annotation.", "labels": [], "entities": [{"text": "sarcasm annotation", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8095073401927948}]}, {"text": "For this study, we consider the case of American text and Indian annotators.", "labels": [], "entities": []}, {"text": "For two sarcasm-labeled datasets of American tweets and discussion forum posts that have been annotated by American annotators, we obtain annotations from Indian annotators.", "labels": [], "entities": []}, {"text": "Our Indian an-notators agree with each other more than their American counterparts, and face difficulties in case of unfamiliar situations and named entities.", "labels": [], "entities": []}, {"text": "However, these difficulties in sarcasm annotation result in statistically insignificant degradation in sarcasm classification.", "labels": [], "entities": [{"text": "sarcasm annotation", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9026366174221039}, {"text": "sarcasm classification", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.8510391414165497}]}, {"text": "We also show that these disagreements between annotators can be predicted using textual properties.", "labels": [], "entities": []}, {"text": "Although the current study is limited to two annotators and one culture pair, our paper opens up a novel direction in evaluation of the quality of sarcasm annotation, and the impact of this quality on sarcasm classification.", "labels": [], "entities": [{"text": "sarcasm classification", "start_pos": 201, "end_pos": 223, "type": "TASK", "confidence": 0.8895363211631775}]}, {"text": "This study forms a steppingstone towards systematic evaluation of quality of these datasets annotated by non-native annotators, and can be extended to other culture combinations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sarcasm is a linguistic expression where literal sentiment of a text is different from the implied sentiment, with the intention of ridicule ().", "labels": [], "entities": []}, {"text": "Several data-driven approaches have been reported for computational detection of sarcasm (.", "labels": [], "entities": [{"text": "computational detection of sarcasm", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.8148507177829742}]}, {"text": "As is typical of supervised approaches, they rely on datasets labeled with sarcasm.", "labels": [], "entities": []}, {"text": "We refer to the process of creating such sarcasm-labeled datasets as sarcasm annotation.", "labels": [], "entities": []}, {"text": "Linguistic studies concerning cross-cultural dependencies of sarcasm have been reported.", "labels": [], "entities": [{"text": "cross-cultural dependencies of sarcasm", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.6898551434278488}]}, {"text": "However, these studies do not look at the notion of cross-cultural sarcasm annotation of text.", "labels": [], "entities": [{"text": "cross-cultural sarcasm annotation of text", "start_pos": 52, "end_pos": 93, "type": "TASK", "confidence": 0.7259341418743134}]}, {"text": "This paper reports the first set of findings from our ongoing line of research: evaluation of quality of sarcasm annotation when obtained from annotators of non-native cultures.", "labels": [], "entities": [{"text": "sarcasm annotation", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.8114073574542999}]}, {"text": "We consider the case of annotators of Indian origin annotating datasets (consisting of discussion forums/tweets from the US) that were earlier annotated by American annotators.", "labels": [], "entities": []}, {"text": "It maybe argued that since crowd-sourcing is prevalent now, a large pool of annotators makes up for cultural differences among few annotators.", "labels": [], "entities": []}, {"text": "However, a fundamental study like ours that performs a micro-analysis of culture combinations is likely to be useful fora variety of reasons such as judging the quality of new datasets, or deciding among annotators.", "labels": [], "entities": []}, {"text": "Balancing the linguistic and computational perspectives, we present our findings in two ways: (a) degradation in quality of sarcasm annotation by nonnative annotators, and (b) impact of this quality on sarcasm classification.", "labels": [], "entities": [{"text": "sarcasm classification", "start_pos": 202, "end_pos": 224, "type": "TASK", "confidence": 0.870296061038971}]}, {"text": "The motivation behind our study is described in Section 2, while our annotation experiments are in Section 3.", "labels": [], "entities": []}, {"text": "We present our analysis in terms of four questions: (a) Are there peculiar difficulties that non-native annotators face during sarcasm annotation?", "labels": [], "entities": [{"text": "sarcasm annotation", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.8324706554412842}]}, {"text": "(Section 4.1), (b) How do these difficulties impact the quality of sarcasm annotation?", "labels": [], "entities": [{"text": "sarcasm annotation", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8068506121635437}]}, {"text": "(Section 4.2), (c) How do cultural differences affect sarcasm classification that uses such annotation?", "labels": [], "entities": [{"text": "sarcasm classification", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.8663192987442017}]}, {"text": "(Section 4.3), and (c) Can these difficulties be predicted using features of text?", "labels": [], "entities": []}, {"text": "All labeled datasets are available on request for future work.", "labels": [], "entities": []}, {"text": "For every textual unit, they contain multiple annotations, by native (as given in past works), and non-native annotators.", "labels": [], "entities": []}, {"text": "the host fora meal maybe perceived as polite in some cultures, but sarcastic in some others.", "labels": [], "entities": []}, {"text": "Due to popularity of crowd-sourcing, cultural background of annotators may not be known at all.", "labels": [], "entities": []}, {"text": "Keeping these constraints in mind, a study of non-native annotation, and its effect on the corresponding NLP task assumes importance.", "labels": [], "entities": []}, {"text": "Our work is the first-of-itskind study related to sarcasm annotation.", "labels": [], "entities": [{"text": "sarcasm annotation", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9272453188896179}]}, {"text": "Similar studies have been reported for related tasks.", "labels": [], "entities": []}, {"text": "deal with result of cultural differences on annotation of images with emotions.", "labels": [], "entities": []}, {"text": "Das and Bandyopadhyay (2011) describe multi-cultural observations during creation of an emotion lexicon.", "labels": [], "entities": []}, {"text": "For example, they state that the word 'blue' maybe correlated to sadness in some cultures but to evil in others.", "labels": [], "entities": []}, {"text": "Similar studies to understand annotator biases have been performed for subjectivity annotation) and machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8216407597064972}]}, {"text": "show how some annotators may have individual biases towards a certain subjective label, and devise a method to obtain bias-corrected tags.", "labels": [], "entities": []}, {"text": "consider annotator biases for the task of assigning quality scores to machine translation output.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.7268158892790476}]}], "datasetContent": [{"text": "In this section, we describe our annotation experiments in terms of datasets, annotators and experiment details.", "labels": [], "entities": []}, {"text": "We use two sarcasm-labeled datasets that have been reported in past work.", "labels": [], "entities": []}, {"text": "The first dataset is Tweet-A. This dataset, introduced by, consists of 2278 manually labeled tweets, out of which 506 are sarcastic.", "labels": [], "entities": []}, {"text": "We call these annotations American1.", "labels": [], "entities": [{"text": "American1", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.9795892834663391}]}, {"text": "An example of a sarcastic tweet in this dataset is 'Back to the oral surgeon #yay'.", "labels": [], "entities": []}, {"text": "The second dataset is Discussion-A: This dataset, introduced by, consists of 5854 discussion forum posts, out of which 742 are sarcastic.", "labels": [], "entities": []}, {"text": "This dataset was created using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.9595532218615214}]}, {"text": "IP addresses of Turk workers were limited to USA during the experiment 1 . We call these annotations American2.", "labels": [], "entities": [{"text": "USA", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8838675022125244}, {"text": "American2", "start_pos": 101, "end_pos": 110, "type": "DATASET", "confidence": 0.9532954096794128}]}, {"text": "An example post here is: 'A master baiter like you should present your thesis to betaken seriously.", "labels": [], "entities": [{"text": "betaken", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.904421865940094}]}, {"text": "You haven't and you aren't.'.", "labels": [], "entities": []}, {"text": "The annotation experiment is conducted as follows.", "labels": [], "entities": []}, {"text": "Our annotators read a unit of text, and determine whether it is sarcastic or not.", "labels": [], "entities": []}, {"text": "The experiment is conducted in sessions of 50 textual units, and the annotators can pause anywhere through a session.", "labels": [], "entities": []}, {"text": "This results in datasets where each textual unit has three annotations as follows: (A) Tweet-A annotated by American1, Indian1, Indian2, (B) Discussion-A annotated by American2, Indian1, Indian2.", "labels": [], "entities": []}, {"text": "The American annotations are from past work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of sentences that the Indian annotators found difficult to annotate; 'twitter handle' are twitter  handles suppressed for anonymity", "labels": [], "entities": []}, {"text": " Table 2: Inter-annotator agreement statistics for Tweet- A; Avg. American1 is as reported in the original paper", "labels": [], "entities": [{"text": "Avg", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9270983934402466}, {"text": "American1", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.5114262700080872}]}, {"text": " Table 4. For Discussion-A, Kappa coefficient be- tween the two Indian annotators is 0.700, while that be- tween Indian1/2 and American annotators is 0.569 and  0.288 respectively. Average values for American anno- tators are not available in the original paper, and hence  not mentioned. This shows that inter-annotator agree- ment between our annotators is higher than their indi- vidual agreement with the American annotators. Kappa  values are lower in case of tweets than discussion fo- rum posts.  Agreement (%) indicates the percent-", "labels": [], "entities": [{"text": "Agreement", "start_pos": 504, "end_pos": 513, "type": "METRIC", "confidence": 0.9946691393852234}]}, {"text": " Table 4: Inter-annotator agreement statistics for  Discussion-A", "labels": [], "entities": []}, {"text": " Table 3: Impact of non-native annotation on sarcasm classification; Values for Indian-American are averaged over  Indian annotators", "labels": [], "entities": [{"text": "sarcasm classification", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.9777452051639557}]}]}