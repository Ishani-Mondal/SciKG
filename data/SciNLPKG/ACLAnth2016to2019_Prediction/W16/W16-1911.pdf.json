{"title": [{"text": "Modelling the informativeness and timing of non-verbal cues in parent-child interaction", "labels": [], "entities": []}], "abstractContent": [{"text": "How do infants learn the meanings of their first words?", "labels": [], "entities": [{"text": "learn the meanings of their first words", "start_pos": 15, "end_pos": 54, "type": "TASK", "confidence": 0.7408736263002668}]}, {"text": "This study investigates the in-formativeness and temporal dynamics of non-verbal cues that signal the speaker's referent in a model of early word-referent mapping.", "labels": [], "entities": []}, {"text": "To measure the information provided by such cues, a supervised clas-sifier is trained on information extracted from a multimodally annotated corpus of 18 videos of parent-child interaction with three children aged 7 to 33 months.", "labels": [], "entities": []}, {"text": "Contradicting previous research, we find that gaze is the single most informative cue, and we show that this finding can be attributed to our fine-grained temporal annotation.", "labels": [], "entities": []}, {"text": "We also find that offsetting the timing of the non-verbal cues reduces accuracy , especially if the offset is negative.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9990749359130859}]}, {"text": "This is inline with previous research, and suggests that synchrony between verbal and non-verbal cues is important if they are to be perceived as causally related.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "This section describes how we used our model in three experiments to try to measure the informativeness and timing of non-verbal cues.", "labels": [], "entities": []}, {"text": "First, we were interested in obtaining measures of the informativeness of the non-verbal cues from both the parent and child as seen from a thirdperson observer (in effect, looking at their joint interaction), as well as from the agents as seen separately.", "labels": [], "entities": []}, {"text": "To this end, we trained classifiers on cues including gaze and hand manipulation for the input from each agent as well as from both of them.", "labels": [], "entities": []}, {"text": "For this experiment, we used the two target objects as referents.", "labels": [], "entities": []}, {"text": "We did not include the child, because the objective here was to use external information sources as seen from the parent and child, and we did not include any other objects for lack of data.", "labels": [], "entities": []}, {"text": "The half-life of the short-term memory given by the most frequently referred one, target object 1 (Siffu), which was used in 58% of the cases.", "labels": [], "entities": []}, {"text": "An uninformed model could thus achieve an accuracy of 58% by always predicting Siffu.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9995198249816895}, {"text": "Siffu", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.8777090907096863}]}, {"text": "shows the accuracy of the model's predictions given different cue combinations and information sources (agents).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993185997009277}]}, {"text": "Overall, the differences in predictive accuracy between the various cue combinations are fairly small, but we can note some things.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9635074734687805}]}, {"text": "First, gaze turns out to be more informative than hand manipulation of objects.", "labels": [], "entities": []}, {"text": "Secondly, a comparison of the P and C columns shows that roughly the same amount of information is provided by both agents, indicating a high degree of convergence in their interaction.", "labels": [], "entities": []}, {"text": "For comparison, we also include at the end of table 5 the corresponding accuracies obtained using the paradigm of, that is, discarding our fine-grained temporal information and using only utterance-level binary features.", "labels": [], "entities": []}, {"text": "The result is a sharp decline in prediction accuracy.", "labels": [], "entities": [{"text": "prediction", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.9597745537757874}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9610506892204285}]}, {"text": "It is noteworthy that gaze comes out as less informative than hand manipulation under these circumstances, which is consistent with the results reported by Frank et al.", "labels": [], "entities": []}, {"text": "The relative importance of cues thus seems to depend strongly on the resolution of the temporal information available to the model.", "labels": [], "entities": []}, {"text": "Finally, we can see that the prediction accuracy is higher when the information sources are combined, as we would expect.", "labels": [], "entities": [{"text": "prediction", "start_pos": 29, "end_pos": 39, "type": "TASK", "confidence": 0.8510559797286987}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9732421040534973}]}, {"text": "The P + C column shows that the prediction accuracy of a third person view classifier (trained on both parent and child input) is consistently higher than the accuracy of the classifiers trained on input from P and C, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.8717809319496155}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9885056018829346}]}, {"text": "In the second experiment, we were interested in determining if there were differences in informativess of non-verbal cues that depended on the object referred to.", "labels": [], "entities": []}, {"text": "This question may bear upon problems related to givenness and accessibility in the domain.", "labels": [], "entities": []}, {"text": "In each dyad, the child is a secondperson referent, and the target objects are thirdperson referents.", "labels": [], "entities": []}, {"text": "For example, according to, second-person referents are consistently highly accessible, whereas third-person referents are highly accessible only when they constitute the discourse topic.", "labels": [], "entities": []}, {"text": "Our model thus permits us to investigate whether there are differences in the informativity of non-verbal cues with respect to second-and third-person referents.", "labels": [], "entities": []}, {"text": "Since the number of references to the child was exceeded only by the target objects, we therefore included this as a third object.", "labels": [], "entities": []}, {"text": "For this experiment, we thus trained classifiers on cues including gaze and hand manipulation for the input from both agents combined.", "labels": [], "entities": []}, {"text": "shows that predicting the child is much more difficult than the external (target) objects.", "labels": [], "entities": [{"text": "predicting", "start_pos": 11, "end_pos": 21, "type": "TASK", "confidence": 0.9700794816017151}]}, {"text": "Using gaze and action information from both participants, we achieve F -scores of 71.6% and 80.0% for the two toys, but only 18.6% for the child.", "labels": [], "entities": [{"text": "F -scores", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9943113128344218}]}, {"text": "given a short-term memory of 1, 3, and 10 seconds, respectively.", "labels": [], "entities": [{"text": "memory", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9015788435935974}]}, {"text": "Time = 0 coincides with the start of the mentions by the parent.", "labels": [], "entities": [{"text": "Time", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9761402010917664}]}, {"text": "Our final experiment concerned the timing of nonverbal cues.", "labels": [], "entities": [{"text": "timing of nonverbal cues", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.8518445044755936}]}, {"text": "Previous research has highlighted the time-synchronicity of non-verbal cues with verbal utterances).", "labels": [], "entities": []}, {"text": "Furthermore, there has been work in the HSP paradigm on determining the effects to referential transparency by displacing these cues (.", "labels": [], "entities": [{"text": "referential transparency", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.7703857421875}]}, {"text": "Using our fine-grained representation of time, we wanted to investigate the effects in our model to see if would arrive at similar effects as Trueswell et al.", "labels": [], "entities": []}, {"text": "Our hypothesis was that non-verbal cues are synchronised with speech, and that displacing the verbal mention from its actual temporal position in the input would lead to a drop in classifier performance.", "labels": [], "entities": []}, {"text": "We tested this by training a classifier on input where the timing of the predictions relative to the onset of speech had been moved by whole seconds up/down to \u00b14 seconds.", "labels": [], "entities": [{"text": "timing", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9581172466278076}]}, {"text": "This is comparable to displacing the speech relative to the nonverbal event with the same amount of time.", "labels": [], "entities": []}, {"text": "We also explored how short-term memory decay influenced classification accuracy by comparing three classifiers with a memory half-life of 1, 3 and 10 seconds, respectively.", "labels": [], "entities": [{"text": "classification", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.9645107984542847}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.8786736130714417}]}, {"text": "The effects of the timing displacement on accuracy appear in.", "labels": [], "entities": [{"text": "timing", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9829211831092834}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9982566237449646}]}, {"text": "The 0 second verbal mention offset is the baseline, with an accuracy of about 86% for the 1 second memory model, and around 88% for the 3 and 10 second memory models.", "labels": [], "entities": [{"text": "0 second verbal mention offset", "start_pos": 4, "end_pos": 34, "type": "METRIC", "confidence": 0.5585701942443848}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.99945467710495}]}, {"text": "Accuracy dropped when verbal mention offset was displaced.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9946371912956238}]}, {"text": "Moving the verbal mention offset ahead in time by as little as two seconds resulted inaccuracy scores of 82% for the 1 second model, and 84% for the 3 and 10 second memory models.", "labels": [], "entities": []}, {"text": "Delaying the verbal mention by 2 seconds had a less detrimental effect, in particular for the 10 second model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Values of Cohen's Kappa (required over- lap 0.6)", "labels": [], "entities": [{"text": "Cohen's Kappa", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.690280944108963}]}, {"text": " Table 5: Results of experiment 1. Accuracy (in  percent) of model prediction given type of cue.  Columns show from which agents information is  incorporated into the model (P = parent, C = child,  P + C = both). The upper half shows results from  our model as described, the lower half uses the  same data but only utterance-level binary features,  thus emulating the model of", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9949599504470825}]}, {"text": " Table 6: Results of experiment 2. Accuracy (in  percent) of model prediction per referent.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9993054866790771}]}]}