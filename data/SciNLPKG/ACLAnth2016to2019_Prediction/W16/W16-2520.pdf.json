{"title": [{"text": "Correlation-based Intrinsic Evaluation of Word Vector Representations", "labels": [], "entities": [{"text": "Correlation-based Intrinsic Evaluation of Word Vector Representations", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.6988989157336098}]}], "abstractContent": [{"text": "We introduce QVEC-CCA-an intrinsic evaluation metric for word vector representations based on correlations of learned vectors with features extracted from linguistic resources.", "labels": [], "entities": []}, {"text": "We show that QVEC-CCA scores are an effective proxy fora range of extrinsic semantic and syntactic tasks.", "labels": [], "entities": []}, {"text": "We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Being linguistically opaque, vector-space representations of words-word embeddings-have limited practical value as standalone items.", "labels": [], "entities": []}, {"text": "They are effective, however, in representing meaningthrough individual dimensions and combinations of thereof-when used as features in downstream applications (, inter alia).", "labels": [], "entities": []}, {"text": "Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself.", "labels": [], "entities": []}, {"text": "The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for.", "labels": [], "entities": []}, {"text": "This paper advocates a novel approach to constructing such a proxy.", "labels": [], "entities": []}, {"text": "What are the desired properties of an intrinsic evaluation measure of word embeddings?", "labels": [], "entities": []}, {"text": "First, retraining models that use word embeddings as features is often expensive.", "labels": [], "entities": []}, {"text": "A computationally efficient intrinsic evaluation that correlates with extrinsic scores is useful for faster prototyping.", "labels": [], "entities": []}, {"text": "Second, an intrinsic evaluation that enables interpretation and analysis of properties encoded by vector dimensions is an auxiliary mechanism for analyzing how these properties affect the target downstream task.", "labels": [], "entities": []}, {"text": "It thus facilitates refinement of word vector models and, consequently, improvement of the target task.", "labels": [], "entities": []}, {"text": "Finally, an intrinsic evaluation that approximates a range of related downstream tasks (e.g., semantic text-classification tasks) allows to assess generality (or specificity) of a word vector model, without actually implementing all the tasks.", "labels": [], "entities": []}, {"text": "proposed an evaluation measure-QVEC-that was shown to correlate well with downstream semantic tasks.", "labels": [], "entities": []}, {"text": "Additionally, it helps shed new light on how vector spaces encode meaning thus facilitating the interpretation of word vectors.", "labels": [], "entities": [{"text": "interpretation of word vectors", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.7815866619348526}]}, {"text": "The crux of the method is to correlate distributional word vectors with linguistic word vectors constructed from rich linguistic resources, annotated by domain experts.", "labels": [], "entities": []}, {"text": "QVEC can easily be adjusted to specific downstream tasks (e.g., part-of-speech tagging) by selecting task-specific linguistic resources (e.g., part-of-speech annotations).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7315799295902252}]}, {"text": "However, QVEC suffers from two weaknesses.", "labels": [], "entities": [{"text": "QVEC", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.7826366424560547}]}, {"text": "First, it is not invariant to linear transformations of the embeddings' basis, whereas the bases in word embeddings are generally arbitrary ().", "labels": [], "entities": []}, {"text": "Second, it produces an unnormalized score: the more dimensions in the embedding matrix the higher the score.", "labels": [], "entities": []}, {"text": "This precludes comparison of models of different dimensionality.", "labels": [], "entities": []}, {"text": "In this paper, we introduce QVEC-CCA, which simultaneously addresses both problems, while preserving major strengths of QVEC.", "labels": [], "entities": [{"text": "QVEC", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.8645564317703247}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Linguistic dimension word vector matrix  with semantic vectors, constructed using SemCor.", "labels": [], "entities": []}, {"text": " Table 2: Linguistic dimension word vector matrix  with syntactic vectors, constructed using PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.8060591816902161}]}, {"text": " Table 3: Pearson's correlations between word  similarity/QVEC/QVEC-CCA scores and the down- stream text classification tasks.", "labels": [], "entities": [{"text": "down- stream text classification", "start_pos": 87, "end_pos": 119, "type": "TASK", "confidence": 0.6369601726531983}]}, {"text": " Table 4: Pearson's correlations between word  similarity/QVEC/QVEC-CCA scores and the down- stream syntactic tasks.", "labels": [], "entities": [{"text": "word  similarity/QVEC/QVEC-CCA scores", "start_pos": 41, "end_pos": 78, "type": "METRIC", "confidence": 0.5317012284483228}]}]}