{"title": [{"text": "The Social Mood of News: Self-reported Annotations to Design Automatic Mood Detection Systems", "labels": [], "entities": [{"text": "Automatic Mood Detection", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.7369114557902018}]}], "abstractContent": [{"text": "In this paper, we address the issue of automatic prediction of readers' mood from newspaper articles and comments.", "labels": [], "entities": [{"text": "automatic prediction of readers' mood from newspaper articles and comments", "start_pos": 39, "end_pos": 113, "type": "TASK", "confidence": 0.8148281633853912}]}, {"text": "As online newspapers are becoming more and more similar to social media platforms, users can provide affective feedback, such as mood and emotion.", "labels": [], "entities": []}, {"text": "We have exploited the self-reported annotation of mood categories obtained from the metadata of the Italian online newspaper corriere.it to design and evaluate a system for predicting five different mood categories from news articles and comments: indignation, disappointment, worry, satisfaction, and amusement.", "labels": [], "entities": [{"text": "Italian online newspaper corriere.it", "start_pos": 100, "end_pos": 136, "type": "DATASET", "confidence": 0.7258490100502968}, {"text": "amusement", "start_pos": 302, "end_pos": 311, "type": "METRIC", "confidence": 0.9512435793876648}]}, {"text": "The outcome of our experiments shows that overall, bag-of-word-ngrams perform better compared to all other feature sets; however, stylometric features perform better for the mood score prediction of articles.", "labels": [], "entities": []}, {"text": "Our study shows that self-reported annotations can be used to design automatic mood prediction systems.", "labels": [], "entities": [{"text": "automatic mood prediction", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.6523676216602325}]}], "introductionContent": [], "datasetContent": [{"text": "In this section, we report experiments on mood score prediction and mood classification.", "labels": [], "entities": [{"text": "mood score prediction", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.5578218698501587}, {"text": "mood classification", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7201724052429199}]}, {"text": "The development set is used for the preliminary experiments and final models are trained by joining training and development sets.", "labels": [], "entities": []}, {"text": "For the mood score prediction experiments, we utilized Random Forests as a learning algorithm.", "labels": [], "entities": [{"text": "mood score prediction", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.6542747716108958}]}, {"text": "It is a decision tree based algorithm where instances and features are randomly sampled to generate several trees (forest).", "labels": [], "entities": []}, {"text": "Then the score of the forest is computed by averaging the scores from the trees.", "labels": [], "entities": []}, {"text": "For this experiment, the number of trees is set to 100.", "labels": [], "entities": []}, {"text": "We did not optimize the number of trees for the task and plan to address this in the future.", "labels": [], "entities": []}, {"text": "We measure the performance of the mood score prediction system as Root Mean Square Error (RMSE).", "labels": [], "entities": [{"text": "mood score prediction", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.5191026131312052}, {"text": "Root Mean Square Error (RMSE)", "start_pos": 66, "end_pos": 95, "type": "METRIC", "confidence": 0.9033249276024955}]}, {"text": "The performances of models are compared to the baseline that is produced by randomly generating the scores using Gaussian distribution with respect to the prior mean and standard deviation, as presented in.", "labels": [], "entities": []}, {"text": "In, we present the performances of different feature sets.", "labels": [], "entities": []}, {"text": "The best results for the mood of: Performance of the different feature sets on the test set as RMSE (lower is better).", "labels": [], "entities": [{"text": "RMSE", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9965349435806274}]}, {"text": "Baseline performances are produced by randomly selecting from the Gaussian distribution with respect to prior mean and standard deviation.", "labels": [], "entities": []}, {"text": "Base: Baseline, W-ng: word ngram, C-ng: character ngram.", "labels": [], "entities": []}, {"text": "Amusement (Amu), Disappointment (Dis), Indignation (Indig), Satisfaction (Sat), Worry (Wor).", "labels": [], "entities": [{"text": "Amusement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7625304460525513}, {"text": "Indignation (Indig)", "start_pos": 39, "end_pos": 58, "type": "METRIC", "confidence": 0.918464794754982}, {"text": "Worry", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.9751020669937134}]}, {"text": "the articles are obtained using stylometric features, and the second best results are obtained using wordngrams.", "labels": [], "entities": []}, {"text": "For the comments, on the other hand, the best results are obtained with the word-and characterngrams.", "labels": [], "entities": []}, {"text": "Moreover, for comments, all the feature sets produce close results.", "labels": [], "entities": []}, {"text": "The reason for this might be the noisy nature of comment content, and part-of-speech tags, stylometric and LIWC features might not be able to capture significant information.", "labels": [], "entities": []}, {"text": "Yet another reason might be high variation in comment length, thus high feature sparseness.", "labels": [], "entities": []}, {"text": "In terms of the performance and the number of features, we speculate that stylometric features might be useful for cross-language/domain experiments.", "labels": [], "entities": []}, {"text": "For the classification task, we first transformed the mood scores into binary classes such as positive and negative.", "labels": [], "entities": []}, {"text": "This is done by first computing an overall mood class label score by subtracting the sum of \"Disappointment\", \"Worry\" and \"Indignation\" scores from the sum of \"Amusement\" and\"Satisfaction\" scores (see Equation 1).", "labels": [], "entities": []}, {"text": "Then, the score is mapped into either of the two classes -positive and negative -with respect to Equation 2.", "labels": [], "entities": [{"text": "Equation", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9907633662223816}]}, {"text": "The instances with the overall score of zero are ignored.", "labels": [], "entities": []}, {"text": "As a result, 63% of articles are assigned to a negative category and 37% to positive.", "labels": [], "entities": []}, {"text": "The distribution of comments into negative and positive categories, on the other hand, is more balanced: 53% (negative) vs 47% (positive).", "labels": [], "entities": []}, {"text": "class label score = (amusement+satisf action)\u2212(disappointment+worry +indignation) (1) For the task of classification, we train a Support Vector Machines (SVM)) model with a linear kernel.", "labels": [], "entities": [{"text": "amusement+satisf action)\u2212(disappointment+worry +indignation", "start_pos": 21, "end_pos": 80, "type": "METRIC", "confidence": 0.7111370414495468}, {"text": "classification", "start_pos": 102, "end_pos": 116, "type": "TASK", "confidence": 0.9611780047416687}]}, {"text": "The performance is measured in terms of macro-averaged precision, recall, F1-measure, and accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.963285505771637}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.999704897403717}, {"text": "F1-measure", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9993762373924255}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9991236329078674}]}, {"text": "Baseline results are computed by randomly generating the class labels, such as positive or negative, based on the prior class distribution of the training set (i.e. chance baseline) as shown in.", "labels": [], "entities": []}, {"text": "In, we present the classification results for the articles and comments.", "labels": [], "entities": []}, {"text": "For the articles, we obtain the best results using word-ngrams and the second best result using character-ngrams.", "labels": [], "entities": []}, {"text": "For the comments, on the other hand, we observe similar results with both word and character ngrams, however, character-ngram model is slightly better.", "labels": [], "entities": []}, {"text": "The performances of POS, LIWC, and stylometric feature sets are lower.", "labels": [], "entities": []}, {"text": "Compared to the chance baseline, the results are statistically significant with McNemar's test and p < 0.05.", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 80, "end_pos": 94, "type": "METRIC", "confidence": 0.5367483695348104}]}], "tableCaptions": [{"text": " Table 2: Performance of the different feature sets on the test set as RMSE (lower is better). Baseline  performances are produced by randomly selecting from the Gaussian distribution with respect to prior  mean and standard deviation. Base: Baseline, W-ng: word ngram, C-ng: character ngram. Amusement  (Amu), Disappointment (Dis), Indignation (Indig), Satisfaction (Sat), Worry (Wor).", "labels": [], "entities": [{"text": "RMSE", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.8680346012115479}, {"text": "Indignation (Indig)", "start_pos": 333, "end_pos": 352, "type": "METRIC", "confidence": 0.8837752938270569}, {"text": "Worry (Wor)", "start_pos": 374, "end_pos": 385, "type": "METRIC", "confidence": 0.9202709048986435}]}]}