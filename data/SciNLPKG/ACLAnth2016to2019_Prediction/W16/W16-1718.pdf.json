{"title": [{"text": "Generating Disambiguating Paraphrases for Structurally Ambiguous Sentences", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a method that, for the first time in abroad coverage setting, uses natural language generation to automatically construct disambiguating paraphrases for structurally ambiguous sentences.", "labels": [], "entities": []}, {"text": "By simply asking naive annotators to clarify which paraphrase is closer in meaning to the original sentence, the resulting paraphrases can potentially enable meaning judgments for parser training and domain adaptation to be crowd-sourced on a massive scale.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 200, "end_pos": 217, "type": "TASK", "confidence": 0.7189008742570877}]}, {"text": "To validate the method, we demonstrate that meaning judgments crowd-sourced in this way via Amazon Mechanical Turk have reasonably high accuracy-e.g. 80%, given a strong majority choice between two paraphrases-with accuracy increasing as the level of agreement among annotators increases.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 92, "end_pos": 114, "type": "DATASET", "confidence": 0.9123717347780863}, {"text": "accuracy-e.g.", "start_pos": 136, "end_pos": 149, "type": "METRIC", "confidence": 0.9961202144622803}, {"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.9974187612533569}]}, {"text": "We also show that even with just the limited validation data gathered to date, the crowd-sourced judgments make it possible to retrain a parser to achieve significantly higher accuracy in a novel domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9952982068061829}]}, {"text": "We conclude with lessons learned for gathering such judgments on a much larger scale.", "labels": [], "entities": []}], "introductionContent": [{"text": "While early dialogue systems such as SHRDLU were capable of asking questions to clarify the meaning of structurally ambiguous sentences, to our knowledge the task of generating questions to clarify structural ambiguities has not been investigated on abroad scale.", "labels": [], "entities": []}, {"text": "Given the development in recent years of statistical parsers and realizers using a reversible grammar or a common set of dependencies, one might expect that in principle it should be possible today to generate paraphrases to help clarify the meaning of structurally ambiguous sentences simply by chaining the parser and realizer end-to-end.", "labels": [], "entities": []}, {"text": "However, realization ranking models are typically trained to prefer corpus sentences over possible variants, and thus statistical realizers chained with statistical parsers are apt to just reproduce the input sentence, which is of no help for disambiguation.", "labels": [], "entities": []}, {"text": "Moreover, while it is easy enough to require the realizer to output a distinct sentence, for most realizers there is no guarantee that the realization will in fact unambiguously express one or the other possible meaning.", "labels": [], "entities": []}, {"text": "In early work in natural language generation, investigated algorithms for avoiding ambiguity in surface realization.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.6516403059164683}]}, {"text": "More recently, developed a method for using statistical parsers together with a realization ranking model to balance the competing concerns of fluency and ambiguity avoidance, given that sentences of even moderate length are rarely unambiguous according to abroad coverage grammar.", "labels": [], "entities": [{"text": "ambiguity avoidance", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.6881856620311737}]}, {"text": "In this paper, we present and validate a related method that aims to ensure that the difference in dependencies between two competing parses is unambiguously expressed in the realization corresponding to each parse (albeit at the expense of fluency), so that the realizations can serve as disambiguating paraphrases for the input sentence.", "labels": [], "entities": []}, {"text": "To the extent that the method is successful, it then becomes possible to clarify the meaning of structurally ambiguous sentences simply by asking naive annotators which paraphrase is closer in meaning to the original sentence.", "labels": [], "entities": []}, {"text": "As is well known, the performance of most NLP tools such as statistical parsers has remained much higher for the domains and genres for which large-scale annotated training corpora are available.", "labels": [], "entities": []}, {"text": "Domain adaptation techniques are not always successful (, and while self-training can yield substantial error reductions), large gaps in performance persist.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.807610958814621}]}, {"text": "Consequently, to achieve high performance, there remains a need to collect new annotated data in the target domain and genre.", "labels": [], "entities": []}, {"text": "Moreover, experience with ImageNet () in vision research suggests that breakthroughs in NLP performance might likewise be enabled by collecting annotated data across domains and genres on a massive scale.", "labels": [], "entities": []}, {"text": "As a first step towards that end, we present a validation experiment which demonstrates that our method enables meaning judgments to be crowdsourced on Amazon's Mechanical Turk (AMT) with reasonably high accuracy, achieving 80% agreement with our own gold standard judgments when there is a strong majority choice between two paraphrases.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (AMT)", "start_pos": 152, "end_pos": 182, "type": "DATASET", "confidence": 0.8882647241864886}, {"text": "accuracy", "start_pos": 204, "end_pos": 212, "type": "METRIC", "confidence": 0.9849085807800293}]}, {"text": "Moreover, accuracy remains satisfactorily high for the subset of sentences where the top parse is incorrect.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993895292282104}]}, {"text": "We also present a preliminary experiment which shows that even with just the limited validation data gathered to date, the \"silver standard\" crowd-sourced judgments make it possible to retrain a parser to achieve significantly higher accuracy in a novel domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 234, "end_pos": 242, "type": "METRIC", "confidence": 0.9930116534233093}]}, {"text": "Ina previous study on obtaining crowd-sourced syntactic annotations, presented results indicating that with some training, annotators on AMT could accurately select prepositional phrase (PP) attachment sites, with accuracy also increasing with the level of agreement among annotators. and Zeldes (2016) also found that it was possible to obtain fairly high quality class-sourced annotations where students only received a modest amount of training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.998380184173584}]}, {"text": "Our work is quite different in that we aim to gather meaning judgments with no training whatsoever, simply by asking questions in natural language.", "labels": [], "entities": []}, {"text": "Our work also differs from Jha et al.'s in that it is not limited to PP-attachment ambiguities.", "labels": [], "entities": []}, {"text": "Since the Jha et al. study used a different corpus, our results are not directly comparable, though we note that our method also achieves satisfactory accuracy on PP-attachment cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.998832643032074}]}, {"text": "Finally, we note that our paper shows that the crowd-sourced data can enable parser improvements, while their study does not include parser retraining results.", "labels": [], "entities": [{"text": "parser retraining", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.844250500202179}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review the parsing and realization ranking models that serve as a starting point for the paper.", "labels": [], "entities": []}, {"text": "In Section 3, we present our method for generating disambiguating paraphrases.", "labels": [], "entities": []}, {"text": "In Section 4, we present our experiment validating the accuracy of naive annotator choices on AMT.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.998855710029602}]}, {"text": "In Section 5, we present an analysis of errors and a regression analysis investigating the factors affecting annotator decisions.", "labels": [], "entities": []}, {"text": "In Section 6, we present our preliminary parser retraining experiment.", "labels": [], "entities": [{"text": "parser retraining", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.8597166538238525}]}, {"text": "In Section 7, after briefly comparing our results with Jha et al.'s, we discuss the implications of the analyses for future data collection and parser adaptation experiments.", "labels": [], "entities": [{"text": "data collection", "start_pos": 124, "end_pos": 139, "type": "TASK", "confidence": 0.7677501142024994}, {"text": "parser adaptation", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.9071936905384064}]}, {"text": "Finally, in Section 8, we conclude with a summary of the lessons learned.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Coverage and Accuracy", "labels": [], "entities": [{"text": "Coverage", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9371868968009949}, {"text": "Accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9941431879997253}]}, {"text": " Table 2: Accuracy of AMT workers' judgments", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9965593218803406}, {"text": "AMT workers' judgments", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.8943814833958944}]}, {"text": " Table 3: Accuracy of 'next' parses (accuracies sig- nificantly higher than chance in bold)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991284012794495}]}, {"text": " Table 4: Coefficients of regression analysis of  AMT workers' choice (significance codes are *:  p \u2264 0.05; **: p \u2264 0.01; ***: p \u2264 0.001)", "labels": [], "entities": [{"text": "AMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9258235692977905}]}]}