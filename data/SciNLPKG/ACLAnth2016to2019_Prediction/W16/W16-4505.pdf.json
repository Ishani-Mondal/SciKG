{"title": [{"text": "Modifications of Machine Translation Evaluation Metrics by Using Word Embeddings", "labels": [], "entities": [{"text": "Machine Translation Evaluation Metrics", "start_pos": 17, "end_pos": 55, "type": "TASK", "confidence": 0.8582101911306381}]}], "abstractContent": [{"text": "Traditional machine translation evaluation metrics such as BLEU and WER have been widely used, but these metrics have poor correlations with human judgements because they badly represent word similarity and impose strict identity matching.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.8021748661994934}, {"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9970625042915344}, {"text": "WER", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.8826888799667358}]}, {"text": "In this paper, we propose some modifications to the traditional measures based on word embeddings for these two metrics.", "labels": [], "entities": []}, {"text": "The evaluation results show that our modifications significantly improve their correlation with human judgements.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the challenges for Machine Translation (MT) research is how to evaluate the quality of translations automatically and correctly.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8572753071784973}]}, {"text": "Earlier word-based metrics such as BLEU (), WER and TER) have been widely used in machine translation, but these metrics have poor correlations with human judgements, especially at the sentence level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9987288117408752}, {"text": "WER", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9743460416793823}, {"text": "TER", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9551457762718201}, {"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7121672630310059}]}, {"text": "One reason is that they just allow strict string matchings between hypothesis and references.", "labels": [], "entities": []}, {"text": "For example, the semantically related words \"learn\" and \"study\" and words that differ only by morphological markers, such as \"study\" and \"studies\" are considered different words although they have a similar meaning.", "labels": [], "entities": []}, {"text": "The traditional solution for improving their performance is to use more references.", "labels": [], "entities": []}, {"text": "However, multiple references are rare and expensive.", "labels": [], "entities": []}, {"text": "Moreover, these n-gram-based evaluations have been shown to be biased in favour of statistical methods, largely because they do not allow grammatically-costrained lexical freedom.", "labels": [], "entities": []}, {"text": "In recent years, many proposals have been put forth and new metrics have appeared and shown their good performance.", "labels": [], "entities": []}, {"text": "However, improving the performance of existing metrics does not require developing a whole new metric.", "labels": [], "entities": []}, {"text": "Proposals that modify existing metrics and show competitive results have also been proposed.", "labels": [], "entities": []}, {"text": "One of the common solutions to improve traditional metrics consists in changing strict string matching to fuzzy matching at the surface level.", "labels": [], "entities": []}, {"text": "For example,) -a variant of standard BLEU, also called \"Letter-edit-BLEU\" or \"Levenshtein-BLEU\" -takes into account letteredit distance -Levenshtein distance including the spaces between the words -between hypothesis and references instead of strict n-gram matchings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9927142262458801}, {"text": "letteredit distance -Levenshtein distance", "start_pos": 116, "end_pos": 157, "type": "METRIC", "confidence": 0.6514583945274353}]}, {"text": "More recently, have proposed a character-level TER (CharacTER) which calculates the character-level edit distance, while still performing the shift edits at the word level.", "labels": [], "entities": [{"text": "TER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.7841702103614807}, {"text": "CharacTER", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.7391296625137329}]}, {"text": "The evaluation results show that this kind of modifications have a good effect on string-level similar words, but that they don't work well on words that are semantically similar, but are orthografically different strings.", "labels": [], "entities": []}, {"text": "To capture semantic similarity, one established way is to apply additional linguistic knowledge, such as synonym dictionaries.", "labels": [], "entities": []}, {"text": "For example, TER-Plus () use WordNet to compute synonym matches in addition to the four original operations (Insertion, Deletion, Substitution and Shift).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.9518507122993469}]}, {"text": "Although such linguistic resources are helpful, they are often lacking in coverage and affect computation speed and ease of use.", "labels": [], "entities": [{"text": "coverage", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9730735421180725}]}, {"text": "Current research on word embeddings () maps each word to a low-dimensional vector.", "labels": [], "entities": []}, {"text": "The vectors of the words that are semantically similar have been shown to be close to each other in vector space.", "labels": [], "entities": []}, {"text": "The similarity between words then can be captured by calculating the geometric distance between their vectors.", "labels": [], "entities": []}, {"text": "On this basis, extend word-level representation to sentence and document level, which allows them to compute the similarity between two sequence of words.", "labels": [], "entities": []}, {"text": "Recently, this kind of vector representation has been widely integrated in MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.9840405881404877}]}, {"text": "use Latent Semantic Indexing to project sentences as bag-of-words into a low-dimensional continuous space to measure the adequacy on an hypothesis.", "labels": [], "entities": []}, {"text": "A monolingual continuous space has been used to capture the similarity between hypothesis and reference and a cross-language continuous space has been used to calculate the similarity between source sentence and hypothesis.", "labels": [], "entities": []}, {"text": "With the same idea, proposed a Bayesian Ridge Regressor which use document-level embeddings as features and METEOR score as target to predict the adequacy of hypothesis.", "labels": [], "entities": [{"text": "METEOR score", "start_pos": 108, "end_pos": 120, "type": "METRIC", "confidence": 0.9762052297592163}]}, {"text": "The study of uses vector representation more directly.", "labels": [], "entities": []}, {"text": "In their study, each sentence has been transformed into a vector (they tried 3 kinds of vector representation: one-hot, word embedding and recursive auto-encoder representations).", "labels": [], "entities": []}, {"text": "The evaluation score is calculated by the distance between the hypothesis vector and the reference vector, with a length penalty.", "labels": [], "entities": [{"text": "length", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9787185192108154}]}, {"text": "More recently, combine word embeddings and DBnary, a multilingual lexical resource, to enrich ME-TEOR.", "labels": [], "entities": []}, {"text": "In this paper, we also incorporate word embeddings in our similarity score to improve machine translation evaluation metrics.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.8859535853068033}]}, {"text": "We propose measures that, while being largely compatible with previous proposals (BLEU and WER), include semantic word similarity and improve on the state of the art.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9966717958450317}, {"text": "WER", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9200141429901123}, {"text": "semantic word similarity", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.6039891044298807}]}, {"text": "Differently from with the above-mentioned works, our approach simply uses monolingual word embeddings, and still has competitive performance at both sentence and system level.", "labels": [], "entities": []}, {"text": "Because these measures are modifications of BLEU and WER (we call them BLEU modif and WER modif ), they also support systematic comparisons of results: if BLEU modif or WER modif is better correlated with human judgments because word embeddings allow it to better captures lexical semantic similarity, then the improvement in performance must be due to the fact that the system translation exhibits lexical semantic variation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9955574870109558}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9898791313171387}, {"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9575530290603638}]}, {"text": "These modified measures then allow us to compare different architectures according to their amount of lexical variation.", "labels": [], "entities": []}, {"text": "Compared to the standard BLEU and WER versions, which have been argued to penalize rule-based systems more, these modified measures do not penalize systems based on their architecture.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9963604807853699}, {"text": "WER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.7986947894096375}]}, {"text": "This gives us the possibility to evaluate fairly both the rule-based and the statistical components of a hybrid system.", "labels": [], "entities": []}, {"text": "In this paper, we will first descible our method in next section.", "labels": [], "entities": []}, {"text": "Our experimental results in section 3 show that even a simple modification could significantly improve the performances over traditional metrics.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out some experiments to study our modified metrics.", "labels": [], "entities": []}, {"text": "The experiments are based on the English-to-French, English-to-German and French-to-English, German-to-English data provided for the metrics task of the Workshops on Statistical Machine Translation (WMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 166, "end_pos": 203, "type": "TASK", "confidence": 0.7794496069351832}]}, {"text": "This kind of data consists of human judgements for the outputs of different MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9788156747817993}]}, {"text": "The principle of the experiments is to tune and evaluate our modified metrics by measuring the correlation between our scores and the human judgement scores at the segment-level and at the system-level.", "labels": [], "entities": []}, {"text": "The segment-level correlation is calculated by the Kendall's rank correlation coefficient and the system-level correlation is calculated by Pearson's correlation coefficient.", "labels": [], "entities": [{"text": "Kendall's rank correlation coefficient", "start_pos": 51, "end_pos": 89, "type": "METRIC", "confidence": 0.6851721465587616}, {"text": "Pearson's correlation coefficient", "start_pos": 140, "end_pos": 173, "type": "METRIC", "confidence": 0.8743682354688644}]}, {"text": "We use the dataset of WMT-14 4 for the tuning task and WMT-15 5 for the evaluation task.", "labels": [], "entities": [{"text": "WMT-14", "start_pos": 22, "end_pos": 28, "type": "DATASET", "confidence": 0.8872033357620239}, {"text": "WMT-15", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.8637659549713135}]}, {"text": "Our word embedding models are trained on a multilingual corpus called \"News Crawl\" shared by WMT-16 . This corpus contains a large amount of news articles from 2007 to 2015 in different languages.", "labels": [], "entities": [{"text": "WMT-16", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.7262742519378662}]}, {"text": "The size of our training data is 2.917 billion words for English, 0.877 billion words for French and 1.752 billion words for German.", "labels": [], "entities": []}, {"text": "For each language, we trained two embedding models with the two different algorithms Skip-Gram (Vector Size = 500, Window Size = 10) and CBOW (Vector Size = 500, Window Size = 5)  We evaluated our modified metrics on the dataset of WMT-15 with the best parameters found in the tuning phase.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9312058687210083}, {"text": "WMT-15", "start_pos": 232, "end_pos": 238, "type": "DATASET", "confidence": 0.9476632475852966}]}, {"text": "For a better understanding of the general performance of our measures, we compared our modified metrics with standard BLEU, sentence-level smoothed BLEU, TER, NIST and WER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9976346492767334}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.8327986001968384}, {"text": "TER", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.9944867491722107}, {"text": "NIST", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.6573561429977417}, {"text": "WER", "start_pos": 168, "end_pos": 171, "type": "METRIC", "confidence": 0.9903243780136108}]}, {"text": "The results reported in show that, compared with their original versions, both the modified BLEU or the modified WER show an improvement on the correlation with human judgements, both at the segmentlevel and at the system-level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.997714638710022}, {"text": "WER", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9880052208900452}]}, {"text": "Their performance is much better than TER and NIST, especially on the English-to-French,German data.", "labels": [], "entities": [{"text": "TER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.7660192847251892}, {"text": "NIST", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.8997957706451416}]}, {"text": "If we observe the ranking of metrics, we find that: System-level and segment-level correlation with the human judgement on the WMT-15 dataset.", "labels": [], "entities": [{"text": "WMT-15 dataset", "start_pos": 127, "end_pos": 141, "type": "DATASET", "confidence": 0.975898027420044}]}, {"text": "To-En includes French and German to English.", "labels": [], "entities": [{"text": "To-En", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8399453163146973}]}, {"text": "From-En includes English to French and German.", "labels": [], "entities": []}, {"text": "The correlation scores are the averages of the languages mentioned.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9509655833244324}]}, {"text": "after our modifications, the ranks of BLEU and WER are increased by at least four or five ranks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9990710020065308}, {"text": "WER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.989231526851654}]}, {"text": "For English-to-French,German system-level, the modified WER becomes the top metric among eighteenth participants.", "labels": [], "entities": [{"text": "WER", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9894468784332275}]}, {"text": "The results show that a measure that simply augments matching by a similarity notion has better performance than strict string matching, and that current word embeddings techniques capture this notion of similarity.", "labels": [], "entities": []}, {"text": "A qualitative analysis of results also shows that the captured notion of similarity corresponds to ranking of sentence alternatives by native speakers.", "labels": [], "entities": []}, {"text": "For example, looking at some randomly chosen individual sentences, we find some interesting examples: The source sentence \"History is a great teacher\" is translated as \"Die Geschichte ist ein gro\u00dfartiger Lehrmeister\" in German.", "labels": [], "entities": []}, {"text": "The following hypotheses are the output translations of three MT systems from WMT-15 translation task.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9792532920837402}, {"text": "WMT-15 translation task", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.7818834185600281}]}], "tableCaptions": [{"text": " Table 1: Tuning results: The results for modified BLEU shown in this table are the results of different  embedding algorithms with the best threshold \ud97b\udf59. To-En includes French and German to English. From- En includes English to French and German. The correlation scores are the averages of the languages  mentioned.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9956661462783813}]}, {"text": " Table 2: System-level and segment-level correlation with the human judgement on the WMT-15 dataset.  To-En includes French and German to English. From-En includes English to French and German. The  correlation scores are the averages of the languages mentioned.", "labels": [], "entities": [{"text": "WMT-15 dataset", "start_pos": 85, "end_pos": 99, "type": "DATASET", "confidence": 0.9768671095371246}]}, {"text": " Table 3: Single translation evaluation scores.", "labels": [], "entities": [{"text": "Single translation evaluation", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.8166082501411438}]}, {"text": " Table 4: English-to-German system-level evaluation scores of \"PROM-RULE and \"Online-A\" (Systems  from WMT-15 Translation Task)", "labels": [], "entities": [{"text": "WMT-15 Translation Task", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.7050745487213135}]}]}