{"title": [{"text": "Computer-assisted stylistic revision with incomplete and noisy feedback A pilot study", "labels": [], "entities": [{"text": "Computer-assisted stylistic revision", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.587569902340571}]}], "abstractContent": [{"text": "We investigate how users of intelligent writing assistance tools deal with correct, incorrect, and incomplete feedback.", "labels": [], "entities": []}, {"text": "To this end, we conduct an empirical user study around an L1 text revision task for German.", "labels": [], "entities": [{"text": "text revision task", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7277054389317831}]}, {"text": "Our participants should revise stylistic issues in two given texts using a novel web-based writing environment that highlights potential issues and provides corresponding feedback messages.", "labels": [], "entities": []}, {"text": "In comparison to a control group, we find that precision plays a more important role than recall, which confirms previous findings for other languages, issue types, user groups, and experimental setups.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9991061091423035}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9987953901290894}]}], "introductionContent": [], "datasetContent": [{"text": "The most widely accepted evaluation methodology in this area is an intrinsic setup to compare a system's output with annotated reference data.", "labels": [], "entities": []}, {"text": "For automatically identifying language-related issues and generating corresponding corrections, the Helping Our Own shared tasks) constituted a community around this type of system evaluation, which has successfully continued at the CoNLL conferences ( and very recently at the BEA workshop.", "labels": [], "entities": [{"text": "BEA workshop", "start_pos": 278, "end_pos": 290, "type": "DATASET", "confidence": 0.8698195219039917}]}, {"text": "These initiatives are completed by numerous independent evaluation studies, such as the ones by to name just two examples.", "labels": [], "entities": []}, {"text": "Major challenges to this evaluation methodology are: achieving a meaningful comparison of multiple systems, properly interpreting the performance metrics, and ensuring the reliability of the reference data.", "labels": [], "entities": []}, {"text": "discuss the comparability of grammatical error detection systems and give recommendations for best practices.", "labels": [], "entities": [{"text": "grammatical error detection", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.6829470495382944}]}, {"text": "pose the highly important question of what is considered high-quality error detection with regard to human performance.", "labels": [], "entities": [{"text": "error detection", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.685251995921135}]}, {"text": "Obviously, the quality of the reference data directly affects the evaluation scores.", "labels": [], "entities": []}, {"text": "Systems are penalized for detecting an actual error that remained unseen by the human annotators or suggesting a valid correction not covered by the gold standard.", "labels": [], "entities": []}, {"text": "Inter-rater agreement measures) provide a useful tool to assess the reliability, but as) note, metrics such as the kappa coefficient do \"not take into account the fact that there is often more than one valid way to correct a sentence\".", "labels": [], "entities": [{"text": "reliability", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.9641464352607727}, {"text": "kappa coefficient", "start_pos": 115, "end_pos": 132, "type": "METRIC", "confidence": 0.8647764027118683}]}, {"text": "We believe that data-driven evaluation of intelligent writing assistance systems is vital, but given these issues, we suggest that they should be complemented by user-driven evaluation studies.", "labels": [], "entities": []}, {"text": "The user-driven evaluation of different types of language feedback has been a major research topic in writing and language learning research, before most automatic writing assistance systems evolved.,, and are early works in this direction discussing feedback by teachers and peers, based on different educational resources, and using different media.", "labels": [], "entities": []}, {"text": "More recently, the effects of giving automatically generated feedback became an important research question.", "labels": [], "entities": []}, {"text": "report a large-scale study of the Criterion system (.", "labels": [], "entities": [{"text": "Criterion system", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.934616208076477}]}, {"text": "He automatically scores essays before and after providing automated feedback and notes an overall improvement of the writing quality when providing feedback.", "labels": [], "entities": []}, {"text": "The study does, however, not vary the type of feedback in anyway.", "labels": [], "entities": []}, {"text": "distinguish feedback at the text, sentence, and word levels and evaluate different granularities with a questionnaire.", "labels": [], "entities": []}, {"text": "study different ways of formulating feedback messages for spelling errors and find solution suggestions yielding improved results.", "labels": [], "entities": [{"text": "formulating feedback messages for spelling errors", "start_pos": 24, "end_pos": 73, "type": "TASK", "confidence": 0.8488303224245707}]}, {"text": "Ina similar line of research, compare immediate and delayed feedback and find that students more likely responded to correct feedback.", "labels": [], "entities": []}, {"text": "vary the extent of feedback messages about English preposition errors using a crowdsourcing setup.", "labels": [], "entities": []}, {"text": "Regardless of the extent of the feedback messages, they find a learning effect in detecting errors over multiple writing sessions.", "labels": [], "entities": []}, {"text": "But only participants who received correct and detailed feedback were able to fix more errors.", "labels": [], "entities": []}, {"text": "They, however, note limitations of their study setup due to the unclear distribution of preposition errors and language proficiency of the crowdsourcing population.", "labels": [], "entities": []}, {"text": "None of these works systematically varies correct, incorrect, and incomplete feedback.", "labels": [], "entities": []}, {"text": "The work by is most closely related to ours.", "labels": [], "entities": []}, {"text": "They ask 26 language learners to write a number of essays and revise them under four experimental conditions: without any technological assistance, with recall-oriented automatic feedback, with precision-oriented automatic feedback, and with human feedback.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 153, "end_pos": 168, "type": "METRIC", "confidence": 0.966386616230011}, {"text": "precision-oriented", "start_pos": 194, "end_pos": 212, "type": "METRIC", "confidence": 0.9668728113174438}]}, {"text": "They focus on two types of grammatical errors and find the precisionoriented feedback to maximize the learning effect of the participants.", "labels": [], "entities": []}, {"text": "Their work differs from the present paper in multiple ways: First, we consider a revision task of an unknown text instead of a self-written essay, which allows us to control for the number and distribution of errors overall participants.", "labels": [], "entities": []}, {"text": "With this setup, we get in a position to compare the users' revisions systematically.", "labels": [], "entities": []}, {"text": "Second, we consider German native speakers rather than English learners.", "labels": [], "entities": []}, {"text": "Since most previous work is focused on English learners, we believe that addressing native speakers and other languages is an important research gap.", "labels": [], "entities": []}, {"text": "Third, we consider stylistic rather than grammatical issues, which has not been extensively discussed before.", "labels": [], "entities": []}, {"text": "Fourth, we are interested in the usefulness of intelligent writing assistance systems for improving the text quality rather than the learning effect of the users.", "labels": [], "entities": []}, {"text": "Still, we are eager to compare our findings with the previous work and discuss this in section 7.", "labels": [], "entities": []}, {"text": "To test our hypotheses, we conduct an empirical user study, in which we ask our participants to enhance the quality of two given texts.", "labels": [], "entities": []}, {"text": "We employ a 2 \u00d7 2 mixed factorial design.", "labels": [], "entities": []}, {"text": "That is, we divide the participants into an experimental and a control group (between-subject variable) and provide them with texts of two different text types (within-subject variable).", "labels": [], "entities": []}, {"text": "While the control group does not receive any assistance, the experimental group receives correct, incorrect, and incomplete feedback about stylistic issues in the texts.", "labels": [], "entities": []}, {"text": "Below, we first introduce the textual data and the types of issues we consider, before we describe the participants and the overall setup of the study.", "labels": [], "entities": []}], "tableCaptions": []}