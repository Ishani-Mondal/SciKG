{"title": [{"text": "Collecting Reliable Human Judgements on Machine-Generated Language: The Case of the QG-STEC Data *", "labels": [], "entities": [{"text": "Collecting Reliable Human Judgements on Machine-Generated Language", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.8610763464655194}, {"text": "QG-STEC Data", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.9435273706912994}]}], "abstractContent": [{"text": "Question generation (QG) is the problem of automatically generating questions from inputs such as declarative sentences.", "labels": [], "entities": [{"text": "Question generation (QG)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8620982348918915}]}, {"text": "The Shared Evaluation Task Challenge (QG-STEC) Task B that took place in 2010 evaluated several state-of-the-art QG systems.", "labels": [], "entities": [{"text": "Shared Evaluation Task Challenge (QG-STEC) Task", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.6703851148486137}]}, {"text": "However, analysis of the evaluation results was affected by low inter-rater reliability.", "labels": [], "entities": []}, {"text": "We adapted Non-aka & Takeuchi's knowledge creation cycle to the task of improving the evaluation annotation guidelines with a preliminary test showing clearly improved inter-rater reliability.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since 2008, researchers from Discourse Analysis, Dialogue Modelling, Formal Semantics, Intelligent Tutoring Systems, NLG, NLU and Psycholinguistics have met at a series of QG workshops).", "labels": [], "entities": [{"text": "Discourse Analysis", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7733943164348602}, {"text": "NLG", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.8909468054771423}]}, {"text": "These workshops bring together different researchers working on QG activities and collectively are of great value to the QG community.", "labels": [], "entities": []}, {"text": "One such activity was the Shared Task Evaluation Challenge Task B that took place in 2010 ().", "labels": [], "entities": [{"text": "Shared Task Evaluation Challenge Task B", "start_pos": 26, "end_pos": 65, "type": "TASK", "confidence": 0.8407589395840963}]}, {"text": "The challenge was to generate specific questions from single sentences.", "labels": [], "entities": []}, {"text": "These questions were evaluated independently by human judges.", "labels": [], "entities": []}, {"text": "The average scores of the annotations were used to rank participating QG-STEC systems on these criteria.", "labels": [], "entities": []}, {"text": "Of * We would like to thank Alistair Willis and Brian Pl\u00fcss for helpful feedback on the work reported in this paper.", "labels": [], "entities": []}, {"text": "\u2020 keith.godwin@open.ac.uk \u2021 paul.piwek@open.ac.uk particular interest were the criteria relating to relevance of the generated questions and their grammaticality and fluency.", "labels": [], "entities": []}, {"text": "Ideally, when a system generates a question from a sentence, the question should be about the information in that sentence (i.e., be relevant) and it should be fluent and grammatical.", "labels": [], "entities": []}, {"text": "Our assumption is that ordinary speakers of English are reasonably in agreement with each other when they make such judgements.", "labels": [], "entities": []}, {"text": "However, in practice, we found low inter-rater reliability (IRR) for the task results.", "labels": [], "entities": [{"text": "inter-rater reliability (IRR)", "start_pos": 35, "end_pos": 64, "type": "METRIC", "confidence": 0.9120763778686524}]}, {"text": "We established this using Krippendorff's \u03b1, see.", "labels": [], "entities": []}, {"text": "For four evaluation criteria, \u03b1 was well below 0.4, with only one criterion achieving an \u03b1 of 0.409.", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9678782820701599}]}, {"text": "This does not meet Krippendorff's requirement of an \u03b1 of at least 0.8, if one wants to draw any conclusions from the results.", "labels": [], "entities": []}, {"text": "Nor does it meet the requirement that tentative conclusions are only permitted for 0.67 < \u03b1 < 0.8.", "labels": [], "entities": []}, {"text": "It is common practice when evaluating statistical NLP to create an annotation manual.", "labels": [], "entities": []}, {"text": "The manual must systematise the annotation process, making it as unambiguous as possible.", "labels": [], "entities": []}, {"text": "It should contain a scheme and a set of guidelines.", "labels": [], "entities": []}, {"text": "The scheme represents the theoretical backbone of the evaluation process.", "labels": [], "entities": []}, {"text": "The guidelines that supplement the scheme provide additional information, often with examples, making clear the scheme usage).", "labels": [], "entities": []}, {"text": "In the original evaluation, the guidelines were minimal.", "labels": [], "entities": []}, {"text": "As the QG-STEC IRR reliability scores show, it seems that judges interpret an annotation scheme for these criteria very differently, when they use the scheme independently, with minimal guidelines.", "labels": [], "entities": [{"text": "QG-STEC IRR reliability scores", "start_pos": 7, "end_pos": 37, "type": "DATASET", "confidence": 0.622492715716362}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Syntactic correctness and fluency. The syntactic", "labels": [], "entities": [{"text": "Syntactic correctness", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.788998931646347}]}, {"text": " Table 3: Ambiguity. The question should make sense when", "labels": [], "entities": []}, {"text": " Table 6: Krippendorff's alpha IRR measure for original and", "labels": [], "entities": [{"text": "IRR measure", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.6897779107093811}]}]}