{"title": [{"text": "How Many Languages Can a Language Model Model? (invited talk)", "labels": [], "entities": []}], "abstractContent": [{"text": "One of the purposes of the VarDial workshop series is to encourage research into NLP methods that treat human languages as a continuum, by designing models that exploit the similarities between languages and variants.", "labels": [], "entities": []}, {"text": "In my work, I amusing a continuous vector representation of languages that allows modeling and exploring the language continuum in a very direct way.", "labels": [], "entities": []}, {"text": "The basic tool for this is a character-based recurrent neural network language model conditioned on language vectors whose values are learned during training.", "labels": [], "entities": []}, {"text": "By feeding the model Bible translations in a thousand languages, not only does the learned vector space capture language similarity, but by interpolating between the learned vectors it is possible to generate text in unattested intermediate forms between the training languages.", "labels": [], "entities": []}, {"text": "Biography Robert\u00a8OstlingRobert\u00a8 Robert\u00a8Ostling is working on ways to use parallel corpora in computational linguistics, including machine translation, cross-language learning and language typology.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.7825733423233032}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}