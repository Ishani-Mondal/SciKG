{"title": [{"text": "Discrimination between Similar Languages, Varieties and Dialects using CNN-and LSTM-based Deep Neural Networks", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we describe a system (CGLI) for discriminating similar languages, varieties and dialects using convolutional neural networks (CNNs) and long short-term memory (LSTM) neu-ral networks.", "labels": [], "entities": []}, {"text": "We have participated in the Arabic dialect identification sub-task of DSL 2016 shared task for distinguishing different Arabic language texts under closed submission track.", "labels": [], "entities": [{"text": "Arabic dialect identification", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.6876832246780396}]}, {"text": "Our proposed approach is language independent and works for discriminating any given set of languages, varieties and dialects.", "labels": [], "entities": []}, {"text": "We have obtained 43.29% weighted-F1 accuracy in this sub-task using CNN approach using default network parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9778004884719849}]}], "introductionContent": [{"text": "Discriminating between similar languages, language varieties is a well-known research problem in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 97, "end_pos": 130, "type": "TASK", "confidence": 0.8044014573097229}]}, {"text": "In this paper we describe about Arabic dialect identification.", "labels": [], "entities": [{"text": "Arabic dialect identification", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.7024386922518412}]}, {"text": "Arabic dialect classification is a challenging problem for Arabic language processing, and useful in several NLP applications such as machine translation, natural language generation and information retrieval and speaker identification).", "labels": [], "entities": [{"text": "Arabic dialect classification", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6041152576605479}, {"text": "Arabic language processing", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.6565157771110535}, {"text": "machine translation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7922911643981934}, {"text": "natural language generation", "start_pos": 155, "end_pos": 182, "type": "TASK", "confidence": 0.7189218997955322}, {"text": "information retrieval", "start_pos": 187, "end_pos": 208, "type": "TASK", "confidence": 0.7382492125034332}, {"text": "speaker identification", "start_pos": 213, "end_pos": 235, "type": "TASK", "confidence": 0.7777719795703888}]}, {"text": "Modern Standard Arabic (MSA) language is the standardized and literary variety of Arabic that is standardized, regulated, and taught in schools, used in written communication and formal speeches.", "labels": [], "entities": []}, {"text": "The regional dialects, used primarily for day-to-day activities present mostly in spoken communication when compared to the MSA.", "labels": [], "entities": []}, {"text": "The Arabic has more dialectal varieties, in which Egyptian, Gulf, Iraqi, Levantine, and Maghrebi are spoken in different regions of the Arabic population ().", "labels": [], "entities": []}, {"text": "Most of the linguistic resources developed and widely used in Arabic NLP are based on MSA.", "labels": [], "entities": []}, {"text": "Though the language identification task is relatively considered to be solved problem in official texts, there will be further level of problems with the noisy text which can be introduced when compiling languages texts from the heterogeneous sources.", "labels": [], "entities": [{"text": "language identification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.7135357707738876}]}, {"text": "The identification of varieties from the same language differs from the language identification task in terms of difficulty due to the lexical, syntactic and semantic variations of the words in the language.", "labels": [], "entities": [{"text": "language identification task", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.7958080172538757}]}, {"text": "In addition, since all Arabic varieties use the same character set, and much of the vocabulary is shared among different varieties, it is not straightforward to discriminate dialects from each other).", "labels": [], "entities": []}, {"text": "Several other researchers attempted the language varsities and dialects identification problems.", "labels": [], "entities": [{"text": "dialects identification", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.7447165250778198}]}, {"text": "Zampieri and Gebre (2012) investigated varieties of Portuguese using different word and character n-gram features.", "labels": [], "entities": []}, {"text": "proposed multi-dialect Arabic classification using various word and character level features.", "labels": [], "entities": [{"text": "multi-dialect Arabic classification", "start_pos": 9, "end_pos": 44, "type": "TASK", "confidence": 0.6905327836672465}]}, {"text": "In order to improve the language, variety and dialect identification further,, and have been organizing the Discriminating between Similar Languages (DSL) shared task.", "labels": [], "entities": [{"text": "variety and dialect identification", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.6397659406065941}, {"text": "Discriminating between Similar Languages (DSL) shared task", "start_pos": 108, "end_pos": 166, "type": "TASK", "confidence": 0.6768124997615814}]}, {"text": "The aim of the task is to encourage researchers to propose and submit systems using state of the art approaches to discriminate several groups of similar languages and varieties.", "labels": [], "entities": []}, {"text": "achieved 95.7% accuracy which is best among all the submissions in 2014 shared task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9946504235267639}]}, {"text": "In their system, authors employed two-step classification approach to predict first the language group of the text and subsequently the language using SVM classifier with word and character level n-gram features. and  achieved 95.65% and 95.54% state of the art accuracies under open and closed tracks respectively in 2015 DSL shared task.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 262, "end_pos": 272, "type": "METRIC", "confidence": 0.9732751846313477}, {"text": "DSL shared task", "start_pos": 323, "end_pos": 338, "type": "DATASET", "confidence": 0.8206940491994222}]}, {"text": "presents a comprehensive evaluation of state of-the-art language identification systems trained to recognize similar languages and language varieties using the results of the first two DSL shared tasks.", "labels": [], "entities": [{"text": "language identification", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.7815245985984802}]}, {"text": "Their experimental results suggest that humans also find it difficult discriminating between similar languages and language varieties.", "labels": [], "entities": []}, {"text": "This year, DSL 2016 shared task proposed two subtasks: first sub-task is about discriminating between similar languages and national language varieties.", "labels": [], "entities": [{"text": "DSL 2016 shared task", "start_pos": 11, "end_pos": 31, "type": "DATASET", "confidence": 0.819269448518753}]}, {"text": "Second sub-task is about Arabic dialect identification which is introduced first time in DSL 2016 shared task.", "labels": [], "entities": [{"text": "Arabic dialect identification", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.6415351033210754}, {"text": "DSL 2016 shared task", "start_pos": 89, "end_pos": 109, "type": "DATASET", "confidence": 0.9169237464666367}]}, {"text": "We have participated in the sub-task2 of dialect identification on Egyptian, Gulf, Levantine, and North-African, and Modern Standard Arabic (MSA) Arabic dialects.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6932344734668732}, {"text": "Modern Standard Arabic (MSA) Arabic dialects", "start_pos": 117, "end_pos": 161, "type": "DATASET", "confidence": 0.6741012223064899}]}, {"text": "We describe about dataset used for dialect classification in section 4.", "labels": [], "entities": [{"text": "dialect classification", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7578370571136475}]}, {"text": "In classifying Arabic dialects,,,, and  employed supervised and semsupervised learning methods with and without ensembles and meta classifiers with various levels of word, character and morphological features.", "labels": [], "entities": []}, {"text": "Most of these approaches are sensitive to the topic bias in the language and use expensive set of features and limited to short texts.", "labels": [], "entities": []}, {"text": "Moreover, generating these features can be a tedious and complex process.", "labels": [], "entities": []}, {"text": "In this paper, we propose deep learning based supervised techniques for Arabic dialect identification without the need for expensive feature engineering.", "labels": [], "entities": [{"text": "Arabic dialect identification", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.6207097470760345}]}, {"text": "Inspired by the advances in sentence classification and sequence classification) using distributional word representations, we use convolutional neural networks (CNN) and long short-term memory (LSTM)-based deep neural network approaches for Arabic dialect identification.", "labels": [], "entities": [{"text": "sentence classification and sequence classification", "start_pos": 28, "end_pos": 79, "type": "TASK", "confidence": 0.711622828245163}, {"text": "Arabic dialect identification", "start_pos": 242, "end_pos": 271, "type": "TASK", "confidence": 0.6419375836849213}]}, {"text": "The rest of the paper is organized as follows: in section 2, we describe related work on Arabic dialect classification.", "labels": [], "entities": [{"text": "Arabic dialect classification", "start_pos": 89, "end_pos": 118, "type": "TASK", "confidence": 0.6459400355815887}]}, {"text": "In section 3, we introduce two deep learning based supervised classification techniques and describe about the proposed methodology.", "labels": [], "entities": [{"text": "deep learning based supervised classification", "start_pos": 31, "end_pos": 76, "type": "TASK", "confidence": 0.6907448053359986}]}, {"text": "We give a brief overview about the dataset used in the shared task in section 4, and also we present experimental results on dialect classification.", "labels": [], "entities": [{"text": "dialect classification", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.7842703759670258}]}, {"text": "In section 5, we discuss about results and analyse various types of errors in dialect classification and conclude the paper.", "labels": [], "entities": [{"text": "dialect classification", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7746809720993042}]}, {"text": "Additional analysis and comparison with the other submitted systems are available in the 2016 shared task overview (", "labels": [], "entities": []}], "datasetContent": [{"text": "We modeled dialect classification as a sentence classification task.", "labels": [], "entities": [{"text": "dialect classification", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.7455349415540695}, {"text": "sentence classification task", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.8005548516909281}]}, {"text": "We tokenized the corpus with white space tokenizer.", "labels": [], "entities": []}, {"text": "We performed multi-class 5-way classification on the given arabic data set containing 5 language dialects.", "labels": [], "entities": []}, {"text": "We We used 80% of the training set for training and 20% of the data for validation set and performed 5-fold cross validation in CNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.9351295828819275}]}, {"text": "In LSTM, we used 80% of the given training set for building the model and rest 20% of the data is used as development set.", "labels": [], "entities": []}, {"text": "We updated input embedding vectors during the training.", "labels": [], "entities": []}, {"text": "In the CNN approach, we used a stochastic gradient descent-based optimization method for minimizing the cross entropy loss during the training with the Rectified Linear Unit (ReLU) non-linear activation function.", "labels": [], "entities": []}, {"text": "We used default window filter sizes set at.", "labels": [], "entities": []}, {"text": "In the case of LSTM, model was trained using an adaptive learning rate optimizer-adadelta) over shuffled mini-batches with the sigmoid activation function at input, output and forget gates and tanh non-linear activation function at cell state.", "labels": [], "entities": []}, {"text": "Post competition we performed experiments without and with average pooling using LSTM networks and reported the results as shown in tables 5 and 6.", "labels": [], "entities": []}, {"text": "We used hyper-parameters such as drop-out for avoiding over-fitting), and batch size and learning rates on 20% of the cross-validation/development set.", "labels": [], "entities": []}, {"text": "We varied batch sizes, drop-out rate, embedding sizes, and learning rate on development set.", "labels": [], "entities": []}, {"text": "We obtained the best CNN performance with learning rate decay 0.95, batch size 50, drop-out 0.5, and embedding size 300 and ran 20 epochs on cross validated dataset.", "labels": [], "entities": [{"text": "learning rate decay", "start_pos": 42, "end_pos": 61, "type": "METRIC", "confidence": 0.9014582435290018}]}, {"text": "For LSTM, we got the best results on development set with learning rate 0.001, drop-out 0.5, and embedding size 300, batch-size of 32 and at 12 epochs.", "labels": [], "entities": [{"text": "learning rate 0.001", "start_pos": 58, "end_pos": 77, "type": "METRIC", "confidence": 0.942990799744924}]}, {"text": "We used same settings similar to the development set but varied drop-out rate over [0.5,0.6,0.7] and obtained best results on test set using drop-out 0.7.", "labels": [], "entities": []}, {"text": "We obtained best results on test set with drop-out 0.5 using average pooling.", "labels": [], "entities": []}, {"text": "We used the gensim (ehek and Sojka, 2010) word2vec program to compile embeddings from the given training corpus.", "labels": [], "entities": []}, {"text": "We compiled 300-dimensional embedding vectors for the words that appear at least 3 times in the Arabic dialect corpus, and for rest of the vocabulary, embedding vectors are assigned uniform distribution in the range of [\u22120.25, 0.25].", "labels": [], "entities": []}, {"text": "We used these pre-compiled embeddings in LSTM and reported run2 results in the test set.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.779194712638855}]}, {"text": "In this section we describe about DSL 2016 shared task data sets and the experimental results.", "labels": [], "entities": [{"text": "DSL 2016 shared task data sets", "start_pos": 34, "end_pos": 64, "type": "DATASET", "confidence": 0.7745094299316406}]}], "tableCaptions": [{"text": " Table 1: The distribution of training and test data sets", "labels": [], "entities": []}, {"text": " Table 2: Results for test set C for all runs (closed training).", "labels": [], "entities": []}, {"text": " Table 3: LSTM experimental results (run1) on development set without embeddings after 12 epochs of  training.", "labels": [], "entities": []}, {"text": " Table 4: CNN Average 5-fold cross-validation results (run3) without embeddings after 20 epochs", "labels": [], "entities": [{"text": "CNN", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9316233992576599}]}, {"text": " Table 5: LSTM experimental results on test set  without pooling", "labels": [], "entities": []}]}