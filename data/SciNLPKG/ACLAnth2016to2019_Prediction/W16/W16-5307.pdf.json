{"title": [{"text": "Word Sense Disambiguation using a Bidirectional LSTM", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6954801479975382}]}], "abstractContent": [{"text": "In this paper we present a clean, yet effective, model for word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.7894621094067892}]}, {"text": "Our approach leverage a bidirectional long short-term memory network which is shared between all words.", "labels": [], "entities": []}, {"text": "This enables the model to share statistical strength and to scale well with vocabulary size.", "labels": [], "entities": []}, {"text": "The model is trained end-to-end, directly from the raw text to sense labels, and makes effective use of word order.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two standard datasets, using identical hyperparameter settings, which are in turn tuned on a third set of held out data.", "labels": [], "entities": []}, {"text": "We employ no external resources (e.g. knowledge graphs, part-of-speech tagging, etc), language specific features, or hand crafted rules, but still achieve statistically equivalent results to the best state-of-the-art systems, that employ no such limitations.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.735817164182663}]}], "introductionContent": [{"text": "Words are in general ambiguous and can have several related or unrelated meanings depending on context.", "labels": [], "entities": []}, {"text": "For instance, the word rock can refer to both a stone and a music genre, but in the sentence \"Without the guitar, there would be no rock music\" the sense of rock is no longer ambiguous.", "labels": [], "entities": []}, {"text": "The task of assigning a word token in a text, e.g. rock, to a well defined word sense in a lexicon is called word sense disambiguation (WSD).", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 109, "end_pos": 140, "type": "TASK", "confidence": 0.7536478042602539}]}, {"text": "From the rock example above it is easy to see that the context surrounding the word is what disambiguates the sense.", "labels": [], "entities": []}, {"text": "However, it may not be so obvious that this is a difficult task.", "labels": [], "entities": []}, {"text": "To see this, consider instead the phrase \"Solid rock\" where changing the order of words completely changes the meaning, or \"Hard rock crushes heavy metal\" where individual words seem to indicate stone but together they actually define the word token as music.", "labels": [], "entities": []}, {"text": "With this in mind, our thesis is that to do WSD well we need to go beyond bag of words and into the territory of sequence modeling.", "labels": [], "entities": [{"text": "WSD", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9831621646881104}, {"text": "sequence modeling", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.8127841949462891}]}, {"text": "Improved WSD would be beneficial to many natural language processing (NLP) problems, e.g. machine translation), information Retrieval, information Extraction, and sense aware word representations (.", "labels": [], "entities": [{"text": "WSD", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9210001826286316}, {"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7791121006011963}, {"text": "information Retrieval", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.7992440462112427}, {"text": "information Extraction", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.748733788728714}, {"text": "sense aware word representations", "start_pos": 163, "end_pos": 195, "type": "TASK", "confidence": 0.6235516369342804}]}, {"text": "However, though much progress has been made in the area, many current WSD systems suffer from one or two of the following deficits.", "labels": [], "entities": [{"text": "WSD", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9512896537780762}]}, {"text": "(1) Disregarding the order of words in the context which can lead to problems as described above.", "labels": [], "entities": []}, {"text": "(2) Relying on complicated and potentially language specific hand crafted features and resources, which is a big problem particularly for resource poor languages.", "labels": [], "entities": []}, {"text": "We aim to mitigate these problems by (1) modeling the sequence of words surrounding the target word, and (2) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation, i.e. word embeddings.", "labels": [], "entities": []}, {"text": "Using word embeddings has previously been shown to improve WSD (.", "labels": [], "entities": [{"text": "WSD", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.38102614879608154}]}, {"text": "However, these works did not consider the order of words or their operational effect on each other.", "labels": [], "entities": []}, {"text": "\u2022 A purely learned approach to WSD that achieves results on par with state-of-the-art resource heavy systems, employing e.g. knowledge graphs, parsers, part-of-speech tagging, etc.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9853095412254333}, {"text": "part-of-speech tagging", "start_pos": 152, "end_pos": 174, "type": "TASK", "confidence": 0.7363122403621674}]}, {"text": "\u2022 Parameter sharing between different word types to make more efficient use of labeled data and make full vocabulary scaling plausible without the number of parameters exploding.", "labels": [], "entities": [{"text": "Parameter sharing", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.7992786765098572}]}, {"text": "\u2022 Empirical evidence that highlights the importance of word order for WSD.", "labels": [], "entities": [{"text": "word order", "start_pos": 55, "end_pos": 65, "type": "TASK", "confidence": 0.7143847644329071}, {"text": "WSD", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9785609245300293}]}, {"text": "\u2022 A WSD system that, by using no explicit window, is allowed to combine local and global information when deducing the sense.", "labels": [], "entities": []}], "datasetContent": [{"text": "The hyperparameter settings used during the experiments, presented in, were tuned on a separate validation set with data picked from the SE2 training set.", "labels": [], "entities": [{"text": "SE2 training set", "start_pos": 137, "end_pos": 153, "type": "DATASET", "confidence": 0.9128734866778055}]}, {"text": "The source code, implemented using TensorFlow (, has been released as open source 1 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for Senseval 2 and 3 on the English lexical sample task.", "labels": [], "entities": [{"text": "Senseval", "start_pos": 22, "end_pos": 30, "type": "TASK", "confidence": 0.9138650894165039}]}]}