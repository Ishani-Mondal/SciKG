{"title": [{"text": "Research on attention memory networks as a model for learning natural language inference", "labels": [], "entities": [{"text": "learning natural language inference", "start_pos": 53, "end_pos": 88, "type": "TASK", "confidence": 0.5943208560347557}]}], "abstractContent": [{"text": "Natural Language Inference (NLI) is a fundamentally important task in natural language processing that has many applications.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.780877540508906}]}, {"text": "It is concerned with classifying the logical relation between two sentences.", "labels": [], "entities": [{"text": "classifying the logical relation between two sentences", "start_pos": 21, "end_pos": 75, "type": "TASK", "confidence": 0.8224197796412877}]}, {"text": "In this paper, we propose attention memory networks (AMNs) to recognize entailment and contradiction between two sentences.", "labels": [], "entities": []}, {"text": "In our model, an attention memory neural network (AMNN) has a variable sized encoding memory and supports semantic compositionality.", "labels": [], "entities": []}, {"text": "AMNN captures sentence level semantics and reasons relation between the sentence pairs; then we use a S-parsemax layer over the output of the generated matching vectors (sentences) for classification.", "labels": [], "entities": []}, {"text": "Our experiments on the Stanford Natural Language Inference (SNLI) Corpus show that our model outperforms the state of the art, achieving an accuracy of 87.4% on the test data .", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) Corpus", "start_pos": 23, "end_pos": 72, "type": "DATASET", "confidence": 0.6011724770069122}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9993382096290588}]}], "introductionContent": [{"text": "Natural Language Inference (NLI) refers to the problem of determining entailment and contradiction relationships between two sentences.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8037828554709753}]}, {"text": "The challenge in Natural Language Inference, also known as Recognizing Textual Entailment (RTE), is to correctly decide whether a sentence (called a hypothesis) entails or contradicts or is neutral in respect to another sentence (referred to as a premise).", "labels": [], "entities": [{"text": "Natural Language Inference", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.7180135448773702}, {"text": "Recognizing Textual Entailment (RTE)", "start_pos": 59, "end_pos": 95, "type": "TASK", "confidence": 0.7704262336095175}]}, {"text": "Provided with a premise sentence, the task is to judge whether the hypothesis can be inferred (Entailment) or the hypothesis cannot be true (Contradiction) or the truth is unknown (Neutral).", "labels": [], "entities": []}, {"text": "Few examples are illustrated in NLI is the core of natural language understanding and has wide applications in NLP, e.g., automatic text summarization (; and question answering ().", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.6510050992170969}, {"text": "automatic text summarization", "start_pos": 122, "end_pos": 150, "type": "TASK", "confidence": 0.6333838204542795}, {"text": "question answering", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.8982934653759003}]}, {"text": "Moreover, NLI is also related to other tasks of sentence pair modeling, including relation recognition of discourse units (, paraphrase detection (), etc.", "labels": [], "entities": [{"text": "sentence pair modeling", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7273264129956564}, {"text": "relation recognition of discourse units", "start_pos": 82, "end_pos": 121, "type": "TASK", "confidence": 0.8779913306236267}, {"text": "paraphrase detection", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.8152278661727905}]}, {"text": "Bowman released the Stanford Natural Language Inference (SNLI) corpus for the purpose of encouraging more learning centered approaches to NLI.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 20, "end_pos": 69, "type": "DATASET", "confidence": 0.6004884392023087}]}, {"text": "Published SNLI corpus makes it possible to use deep learning methods to solve NLI problems.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.8086136281490326}]}, {"text": "So far proposed work based on neural networks for text similarity tasks including NLI have been published in recent years (;;.", "labels": [], "entities": [{"text": "text similarity tasks", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.8010878761609396}]}, {"text": "The core of these models is to build deep sentence encoding models, for example, with convolutional networks ( or long short-term memory networks) with the goal of deeper semantic encoders.", "labels": [], "entities": [{"text": "sentence encoding", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7255254983901978}]}, {"text": "Recurrent neural networks (RNNs) equipped with internal short memories, such as long short-term memories (LSTMs) have achieved a notable success in sentence encoding.", "labels": [], "entities": [{"text": "sentence encoding", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.8355624377727509}]}, {"text": "LSTMs are powerful because it learns to control its short term memories.", "labels": [], "entities": []}, {"text": "However, the short term memories in LSTMs area part of the training parameters.", "labels": [], "entities": []}, {"text": "This imposes some practical difficulties in training and modeling long sequences with LSTMs.", "labels": [], "entities": []}, {"text": "In this paper, we proposed a deep learning framework for natural language inference, which mainly consists of two layers.", "labels": [], "entities": [{"text": "natural language inference", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.6526749829451243}]}, {"text": "As we can see from the fig-", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the performance of our model, we conducted our experiments on Stanford Natural Language Inference (SNLI) corpus.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 74, "end_pos": 123, "type": "DATASET", "confidence": 0.8645564541220665}]}, {"text": "The dataset, which consists of 549,367/9,842/9,824 premise-hypothesis pairs for train/dev/test sets and target label indicating their relation.", "labels": [], "entities": []}, {"text": "Each pair consists of a premise and a hypothesis, manually labeled with one the labels EN-TAILMENT, CONTRADICTION, or NEUTRAL.", "labels": [], "entities": [{"text": "EN-TAILMENT", "start_pos": 87, "end_pos": 98, "type": "METRIC", "confidence": 0.9539108276367188}, {"text": "CONTRADICTION", "start_pos": 100, "end_pos": 113, "type": "METRIC", "confidence": 0.8364600539207458}]}, {"text": "We used the provided training, development, and test splits.", "labels": [], "entities": []}], "tableCaptions": []}