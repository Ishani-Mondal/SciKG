{"title": [{"text": "Climbing Mount BLEU: The Strange World of Reachable High-BLEU Translations", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9942818880081177}, {"text": "Translations", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.5941108465194702}]}], "abstractContent": [{"text": "We present a method for finding oracle BLEU translations in phrase-based statistical machine translation using exact document-level scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9628574848175049}, {"text": "phrase-based statistical machine translation", "start_pos": 60, "end_pos": 104, "type": "TASK", "confidence": 0.5678103789687157}]}, {"text": "Experiments are presented where the BLEU score of a candidate translation is directly optimised in order to examine the properties of reachable translations with very high BLEU scores.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9768137335777283}, {"text": "BLEU", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9974799752235413}]}, {"text": "This is achieved by running the document-level decoder Docent in BLEU-decoding mode, where proposed changes to the translation of a document are only accepted if they increase BLEU.", "labels": [], "entities": [{"text": "BLEU-decoding", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.992771565914154}, {"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9985558390617371}]}, {"text": "The results confirm that the reference translation cannot inmost cases be reached by the decoder, which is limited by the set of phrases in the phrase table, and demonstrate that high-BLEU translations are often of poor quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents a method for finding oracle translations in phrase-based (PB) statistical machine translation (SMT) using exact document-level BLEU scores.", "labels": [], "entities": [{"text": "finding oracle translations in phrase-based (PB) statistical machine translation (SMT)", "start_pos": 33, "end_pos": 119, "type": "TASK", "confidence": 0.6535647468907493}, {"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9310328960418701}]}, {"text": "The method, which we call BLEU decoding, is implemented in the document-level machine translation decoder Docent.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9798254370689392}, {"text": "document-level machine translation decoder Docent", "start_pos": 63, "end_pos": 112, "type": "TASK", "confidence": 0.586583536863327}]}, {"text": "BLEU decoding is a stochastic hill climbing algorithm: changes are proposed by the decoder to an initial translation and only accepted if they increase BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9034242033958435}, {"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9972063899040222}]}, {"text": "Analysing the translations obtained in this way we corroborate previous research on the problem of reference reachability: perfect BLEU scores, corresponding to the decoder finding the reference translation exactly, are rarely possible; meanwhile we add to the extensive literature on problems and biases with the BLEU metric itself, showing for the first time clear examples of sentences from documents with high BLEU scores with obvious poor translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9945160746574402}, {"text": "BLEU", "start_pos": 314, "end_pos": 318, "type": "METRIC", "confidence": 0.9457224011421204}, {"text": "BLEU", "start_pos": 414, "end_pos": 418, "type": "METRIC", "confidence": 0.9965128302574158}]}, {"text": "The paper is structured in the following manner: Section 2 describes the BLEU metric, Section 3 presents the Docent decoder and BLEU decoding, Section 4 details experiments carried outwith BLEU decoding and presents their results, while Section 5 comprises a discussion.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9664190411567688}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9828754663467407}]}], "datasetContent": [{"text": "A German-English Moses translation model was trained on just over 1.5 million sentences from Europarl v7.", "labels": [], "entities": [{"text": "German-English Moses translation", "start_pos": 2, "end_pos": 34, "type": "TASK", "confidence": 0.5715917547543844}, {"text": "Europarl v7", "start_pos": 93, "end_pos": 104, "type": "DATASET", "confidence": 0.9570631682872772}]}, {"text": "The test data was a set of 3052 sentences from the newstest2013 data, divided into 52 separate documents.", "labels": [], "entities": [{"text": "newstest2013 data", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.9682927429676056}]}, {"text": "Two types of experiments were carried out, firstly with the candidate translation initialised by running Moses (with a 5-gram language model trained with KenLM on 2.2 million Europarl sentences and feature weights tuned using MERT on a development set of 2525 sentences from the newstest2009 data), and secondly by random initialisation (i.e. random segmentation and random phrase translation).", "labels": [], "entities": [{"text": "candidate translation initialised", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.7578830222288767}, {"text": "KenLM", "start_pos": 154, "end_pos": 159, "type": "DATASET", "confidence": 0.7392454743385315}, {"text": "Europarl", "start_pos": 175, "end_pos": 183, "type": "DATASET", "confidence": 0.9274951815605164}, {"text": "MERT", "start_pos": 226, "end_pos": 230, "type": "METRIC", "confidence": 0.9130936861038208}, {"text": "newstest2009 data", "start_pos": 279, "end_pos": 296, "type": "DATASET", "confidence": 0.9579901397228241}, {"text": "phrase translation", "start_pos": 374, "end_pos": 392, "type": "TASK", "confidence": 0.7297065556049347}]}, {"text": "Docent was then run in BLEU-decoding mode: only changes to the translation that increased BLEU were accepted.", "labels": [], "entities": [{"text": "BLEU-decoding", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9865911602973938}, {"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9986771941184998}]}, {"text": "Model and BLEU scores were monitored at exponentially increasing intervals, after iterations 2 8 , 2 9 , ..., 2 25 . The motivation for this sampling is that many more proposed changes to the translation are accepted in the beginning: as decoding progresses and the translation improves, there are simply more iterations between each interesting event.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983273148536682}]}, {"text": "coding ranged from 7.2 to 33.1, with a mean of 19.3; after subsequently running Docent in BLEU-decoding mode, the mean had increased to 50.4, with a range from 25.9 to 72.5.", "labels": [], "entities": [{"text": "BLEU-decoding", "start_pos": 90, "end_pos": 103, "type": "METRIC", "confidence": 0.8369863629341125}]}, {"text": "A substantial and consistent increase in BLEU, as expected, is thus observed.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.99960857629776}]}], "tableCaptions": []}