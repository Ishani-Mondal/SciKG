{"title": [{"text": "Linguistic Input Features Improve Neural Machine Translation", "labels": [], "entities": [{"text": "Improve Neural Machine Translation", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.7149388715624809}]}], "abstractContent": [{"text": "Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8405451973279318}]}, {"text": "In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.8969342708587646}]}, {"text": "We generalize the embedding layer of the encoder in the at-tentional encoder-decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature.", "labels": [], "entities": []}, {"text": "We add morphological features, part-of-speech tags, and syntactic dependency labels as input features to English\u2194German and English\u2192Romanian neural machine translation systems.", "labels": [], "entities": [{"text": "English\u2192Romanian neural machine translation", "start_pos": 124, "end_pos": 167, "type": "TASK", "confidence": 0.597353329261144}]}, {"text": "In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: per-plexity, BLEU and CHRF3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9993019104003906}, {"text": "CHRF3", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.7330524921417236}]}, {"text": "An open-source implementation of our neural MT system is available 1 , as are sample files and configurations 2 .", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.8416234254837036}]}], "introductionContent": [{"text": "Neural machine translation has recently achieved impressive results (, while learning from raw, sentencealigned parallel text and using little in the way of external linguistic information.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7976186275482178}]}, {"text": "However, we hypothesize that various levels of linguistic annotation can be valuable for neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.63254314661026}]}, {"text": "Lemmatisation can reduce data sparse-ness, and allow inflectional variants of the same word to explicitly share a representation in the model.", "labels": [], "entities": []}, {"text": "Other types of annotation, such as partsof-speech (POS) or syntactic dependency labels, can help in disambiguation.", "labels": [], "entities": []}, {"text": "In this paper we investigate whether linguistic information is beneficial to neural translation models, or whether their strong learning capability makes explicit linguistic features redundant.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8088653087615967}]}, {"text": "Let us motivate the use of linguistic features using examples of actual translation errors by neural MT systems.", "labels": [], "entities": []}, {"text": "In translation out of English, one problem is that the same surface word form maybe shared between several word types, due to homonymy or word formation processes such as conversion.", "labels": [], "entities": [{"text": "word formation", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.704937294125557}]}, {"text": "For instance, close can be a verb, adjective, or noun, and these different meanings often have distinct translations into other languages.", "labels": [], "entities": []}, {"text": "Consider the following English\u2192German example: 1.", "labels": [], "entities": []}, {"text": "We thought a win like this might be close.", "labels": [], "entities": []}, {"text": "2. Wir dachten, dass ein solcher Sieg nah sein k\u00f6nnte.", "labels": [], "entities": []}, {"text": "3. *Wir dachten, ein Sieg wie dieser k\u00f6nnte schlie\u00dfen.", "labels": [], "entities": []}, {"text": "For the English source sentence in Example 1 (our translation in Example 2), a neural MT system (our baseline system from Section 4) mistranslates close as a verb, and produces the German verb schlie\u00dfen (Example 3), even though close is an adjective in this sentence, which has the German translation nah.", "labels": [], "entities": []}, {"text": "Intuitively, partof-speech annotation of the English input could disambiguate between verb, noun, and adjective meanings of close.", "labels": [], "entities": []}, {"text": "As a second example, consider the following German\u2192English example: 4.", "labels": [], "entities": []}, {"text": "Gef\u00e4hrlich ist die Route aber dennoch . dangerous is the route but still . 5. However the route is dangerous . 6. *Dangerous is the route , however . German main clauses have a verb-second (V2) word order, whereas English word order is generally SVO.", "labels": [], "entities": []}, {"text": "The German sentence (Example 4; English reference in Example 5) topicalizes the predicate gef\u00e4hrlich 'dangerous', putting the subject die Route 'the route' after the verb.", "labels": [], "entities": []}, {"text": "Our baseline system (Example 6) retains the original word order, which is highly unusual in English, especially for prose in the news domain.", "labels": [], "entities": []}, {"text": "A syntactic annotation of the source sentence could support the attentional encoder-decoder in learning which words in the German source to attend (and translate) first.", "labels": [], "entities": []}, {"text": "We will investigate the usefulness of linguistic features for the language pair German\u2194English, considering the following linguistic features: \u2022 lemmas \u2022 subword tags (see Section 3.2) \u2022 morphological features", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our systems on the WMT16 shared translation task English\u2194German.", "labels": [], "entities": [{"text": "WMT16 shared translation task English\u2194German", "start_pos": 31, "end_pos": 75, "type": "TASK", "confidence": 0.7753310203552246}]}, {"text": "The parallel  To enable open-vocabulary translation, we encode words via joint BPE), learning 89 500 merge operations on the concatenation of the source and target side of the parallel training data.", "labels": [], "entities": [{"text": "open-vocabulary translation", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.700380489230156}, {"text": "BPE", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.7650396823883057}]}, {"text": "We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024.", "labels": [], "entities": []}, {"text": "We clip the gradient norm to 1.0 ().", "labels": [], "entities": []}, {"text": "We train the models with Adadelta, reshuffling the training corpus between epochs.", "labels": [], "entities": []}, {"text": "We validate the model every 10 000 minibatches via BLEU and perplexity on a validation set (newstest2013).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9990503191947937}]}, {"text": "For neural MT, perplexity is a useful measure of how well the model can predict a reference translation given the source sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.7065824270248413}]}, {"text": "Perplexity is thus a good indicator of whether input features provide any benefit to the models, and we report the best validation set perplexity of each experiment.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9555567502975464}]}, {"text": "To evaluate whether the features also increase translation performance, we report casesensitive BLEU scores with mteval-13b.perl on two test sets, newstest2015 and newstest2016.", "labels": [], "entities": [{"text": "translation", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.96412593126297}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9873749017715454}]}, {"text": "We also report CHRF3), a character ngram F 3 score which was found to correlate well with human judgments, especially for translations out of English.", "labels": [], "entities": [{"text": "CHRF3", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9197078943252563}, {"text": "character ngram F 3 score", "start_pos": 25, "end_pos": 50, "type": "METRIC", "confidence": 0.7083351492881775}]}, {"text": "The two metrics may occasionally disagree, partly because they are highly sensitive to the length of the output.", "labels": [], "entities": []}, {"text": "BLEU is precision-based, whereas CHRF3 considers both precision and recall, with a bias for recall.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.976865291595459}, {"text": "precision-based", "start_pos": 8, "end_pos": 23, "type": "METRIC", "confidence": 0.9984520673751831}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9989699125289917}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.998218834400177}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9972611665725708}]}, {"text": "For BLEU, we also report whether differences between systems are statistically significant   according to a bootstrap resampling significance test).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9903908967971802}]}, {"text": "We train models for about a week, and report results for an ensemble of the 4 last saved models (with models saved every 12 hours).", "labels": [], "entities": []}, {"text": "The ensemble serves to smooth the variance between single models.", "labels": [], "entities": []}, {"text": "Decoding is performed with beam search with abeam size of 12.", "labels": [], "entities": [{"text": "Decoding", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.779991626739502}]}, {"text": "To ensure that performance improvements are not simply due to an increase in the number of model parameters, we keep the total size of the embedding layer fixed to 500.", "labels": [], "entities": []}, {"text": "lists the embedding size we use for linguistic featuresthe embedding layer size of the word-level feature varies, and is set to bring the total embedding layer size to 500.", "labels": [], "entities": []}, {"text": "If we include the lemma feature, we roughly split the embedding vector one-to-two between the lemma feature and the word feature.", "labels": [], "entities": []}, {"text": "The table also shows the network vocabulary size; for all features except lemmas, we can represent all feature values in the network vocabulary -in the case of words, this is due to BPE segmentation.", "labels": [], "entities": [{"text": "BPE segmentation", "start_pos": 182, "end_pos": 198, "type": "TASK", "confidence": 0.6784186810255051}]}, {"text": "For lemmas, we choose the same vocabulary size as for words, replacing rare lemmas with a special UNK symbol.", "labels": [], "entities": []}, {"text": "(2016b) report large gains from using monolingual in-domain training data, automatically back-translated into the source language to produce a synthetic parallel training corpus.", "labels": [], "entities": []}, {"text": "We use the synthetic corpora produced in these experiments 7 (3.6-4.2 million sentence pairs), and we trained systems which include this data to compare against the state of the art.", "labels": [], "entities": []}, {"text": "We note that our experiments with this data entail a syntactic annotation of automatically translated data, which maybe a source of noise.", "labels": [], "entities": []}, {"text": "For the systems with synthetic data, we double the training time to two weeks.", "labels": [], "entities": []}, {"text": "We also evaluate linguistic features for the lower-resourced translation direction English\u2192Romanian, with 0.6 million sentence pairs of parallel training data, and 2.2 million sentence pairs of synthetic parallel data.", "labels": [], "entities": [{"text": "translation direction English\u2192Romanian", "start_pos": 61, "end_pos": 99, "type": "TASK", "confidence": 0.8498748302459717}]}, {"text": "We use the same linguistic features as for English\u2192German.", "labels": [], "entities": []}, {"text": "We follow in the configuration, and use dropout for the English\u2192Romanian systems.", "labels": [], "entities": []}, {"text": "We dropout full words (both on the source and target side) with a probability of 0.1.", "labels": [], "entities": []}, {"text": "For all other layers, the dropout probability is set to 0.2.", "labels": [], "entities": []}, {"text": "shows our main results for German\u2192English, and English\u2192German.", "labels": [], "entities": []}, {"text": "The baseline system is a neural MT system with only one input feature, the (sub)words themselves.", "labels": [], "entities": []}, {"text": "For both translation directions, linguistic features improve the best perplexity on the development data (47.3 \u2192 46.2, and 54.9 \u2192 52.9, respectively).", "labels": [], "entities": []}, {"text": "For German\u2192English, the linguistic features lead to an increase of 1.5 BLEU (31.4\u219232.9) and 0.5 CHRF3 (58.0 \u2192 58.5), on the newstest2016 test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9993172883987427}, {"text": "CHRF3", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9919722080230713}, {"text": "newstest2016 test set", "start_pos": 124, "end_pos": 145, "type": "DATASET", "confidence": 0.9806259671847025}]}, {"text": "For English\u2192German, we observe improvements of 0.6 BLEU (27.8 \u2192 28.4) and 1.2 CHRF3 (56.0 \u2192 57.2).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9991177916526794}, {"text": "CHRF3", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9701765179634094}]}], "tableCaptions": [{"text": " Table 2: German\u2194English translation results: best perplexity on dev (newstest2013), and BLEU and  CHRF3 on test15 (newstest2015) and test16 (newstest2016). BLEU scores that are significantly different  (p < 0.05) from respective baseline are marked with (*).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9994137287139893}, {"text": "CHRF3", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.862559974193573}, {"text": "BLEU", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9950215816497803}]}, {"text": " Table 3: Contrastive experiments with individual linguistic features: best perplexity on dev (new- stest2013), and BLEU and CHRF3 on test15 (newstest2015) and test16 (newstest2016). BLEU scores  that are significantly different (p < 0.05) from respective baseline are marked with (*).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9991051554679871}, {"text": "BLEU", "start_pos": 183, "end_pos": 187, "type": "METRIC", "confidence": 0.9922217726707458}]}, {"text": " Table 4: German\u2194English translation results with additional, synthetic training data: best perplexity on  dev (newstest2013), and BLEU and CHRF3 on test15 (newstest2015) and test16 (newstest2016). BLEU  scores that are significantly different (p < 0.05) from respective baseline are marked with (*).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9992461204528809}, {"text": "CHRF3", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.8972151279449463}, {"text": "BLEU", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.9957584738731384}]}, {"text": " Table 5: English\u2192Romanian translation results:  best perplexity on newsdev2016, and BLEU and  CHRF3 on newstest2016. BLEU scores that are  significantly different (p < 0.05) from respective  baseline are marked with (*).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9994106292724609}, {"text": "CHRF3", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9502001404762268}, {"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9977909326553345}]}]}