{"title": [{"text": "Towards Using Conversations with Spoken Dialogue Systems in the Automated Assessment of Non-Native Speakers of English", "labels": [], "entities": []}], "abstractContent": [{"text": "Existing speaking tests only require non-native speakers to engage in dialogue when the assessment is done by humans.", "labels": [], "entities": []}, {"text": "This paper examines the viability of using off-the-shelf systems for spoken dialogue and for speech grading to automate the holistic scoring of the conversational speech of non-native speakers of English.", "labels": [], "entities": []}], "introductionContent": [{"text": "Speaking tests for assessing non-native speakers of English (NNSE) often include tasks involving interactive dialogue between a human examiner and a candidate.", "labels": [], "entities": [{"text": "assessing non-native speakers of English (NNSE)", "start_pos": 19, "end_pos": 66, "type": "TASK", "confidence": 0.7021471634507179}]}, {"text": "An IELTS 1 example is shown in.", "labels": [], "entities": [{"text": "IELTS 1", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.5905712693929672}]}, {"text": "In contrast, most automated spoken assessment systems target only the non-interactive portions of existing speaking tests, e.g., the task of responding to a stimulus in TOEFL 2 ( or BULATS 3.", "labels": [], "entities": [{"text": "BULATS", "start_pos": 182, "end_pos": 188, "type": "METRIC", "confidence": 0.8982469439506531}]}, {"text": "This gap between the current state of manual and automated testing provides an opportunity for spoken dialogue systems (SDS) research.", "labels": [], "entities": [{"text": "spoken dialogue systems (SDS)", "start_pos": 95, "end_pos": 124, "type": "TASK", "confidence": 0.7006115118662516}]}, {"text": "First, as illustrated by, human-human testing dialogues share some features with existing computer-human dialogues, e.g., examiners use standardized topic-based scripts and utterance phrasing.", "labels": [], "entities": []}, {"text": "Second, automatic assessment of spontaneous (but non-conversational) speech is an active research area, which work in SDS-based assessment International English Language Testing System.", "labels": [], "entities": [{"text": "International English Language Testing System", "start_pos": 139, "end_pos": 184, "type": "DATASET", "confidence": 0.6391281425952912}]}, {"text": "Test of English as a Foreign Language.", "labels": [], "entities": []}, {"text": "E: Do you work or are you a student C: I'm a student in university er E: And what subject are you studying should be able to build on.", "labels": [], "entities": []}, {"text": "Third, there is increasing interest in building automated systems not to replace human examiners during testing, but to help candidates prepare for human testing.", "labels": [], "entities": []}, {"text": "Similarly to systems for writing (, automation could provide unlimited self-assessment and practice opportunities.", "labels": [], "entities": [{"text": "writing (", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.9728214740753174}]}, {"text": "There is already some educationally-oriented SDS work in computer assisted language learning (  and physics tutoring) to potentially build upon.", "labels": [], "entities": [{"text": "SDS", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9199444055557251}]}, {"text": "On the other hand, differences between speaking assessment and traditional SDS applications can also pose research challenges.", "labels": [], "entities": [{"text": "speaking assessment", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8324810862541199}, {"text": "SDS", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9660504460334778}]}, {"text": "First, currently available SDS corpora do not focus on including speech from non-native speakers, and when such speech exists it is not scored for English skill.", "labels": [], "entities": []}, {"text": "Even if one could get an assessment company to release a scored corpus of human-human dialogues, there would likely be a mismatch with the computer-human dialogues that are our target for automatic assessment.", "labels": [], "entities": []}, {"text": "Second, there is alack of optimal technical infrastructure.", "labels": [], "entities": []}, {"text": "Existing SDS components such as speech recognizers will likely need modification to handle non-native speech.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.696983739733696}]}, {"text": "Existing automated graders will likely need modification to process spontaneous speech produced during dialogue, rather than after a prompt such as a request to describe a visual ( ).", "labels": [], "entities": []}, {"text": "We make a first step at examining these issues, by using three off-the-shelf SDS to collect dialogues which are then assessed by a human expert and an existing spontaneous speech grader.", "labels": [], "entities": []}, {"text": "Our focus is on the following research questions: RQ1: Will different corpus creation methods 5 influence the English skill level of the SDS users we are able to recruit for data collection purposes?", "labels": [], "entities": [{"text": "RQ1", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.44407030940055847}]}, {"text": "RQ2: Can an expert human grader assess speakers conversing with an SDS?", "labels": [], "entities": [{"text": "RQ2", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6774033308029175}]}, {"text": "RQ3: Can an automated grader for spontaneous (but prompted) speech assess SDS speech?", "labels": [], "entities": [{"text": "RQ3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7234691977500916}, {"text": "SDS speech", "start_pos": 74, "end_pos": 84, "type": "TASK", "confidence": 0.8562299311161041}]}, {"text": "Our preliminary results suggest that while SDSbased speech assessment shows promise, much work remains to be done.", "labels": [], "entities": [{"text": "SDSbased speech assessment", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.8846742709477743}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Human CEFR dialogue assessments, average # of user turns per dialogue, and average number  of recognized words per turn, across corpora. L = Laptop, R=Restaurant, B=Bus, C=Combined.", "labels": [], "entities": [{"text": "Combined", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.923676073551178}]}]}