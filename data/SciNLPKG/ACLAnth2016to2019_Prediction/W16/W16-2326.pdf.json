{"title": [], "abstractContent": [{"text": "This paper summarises the contributions of the teams at the University of Helsinki, Uppsala University and the University of Turku to the news translation tasks for translating from and to Finnish.", "labels": [], "entities": [{"text": "news translation", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.7226632088422775}, {"text": "translating from and to Finnish", "start_pos": 165, "end_pos": 196, "type": "TASK", "confidence": 0.8375593543052673}]}, {"text": "Our models address the problem of treating morphology and data coverage in various ways.", "labels": [], "entities": []}, {"text": "We introduce anew efficient tool for word alignment and discuss factori-sations, gappy language models and re-inflection techniques for generating proper Finnish output.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.8078708350658417}]}, {"text": "The results demonstrate once again that training data is the most effective way to increase translation performance .", "labels": [], "entities": [{"text": "translation", "start_pos": 92, "end_pos": 103, "type": "TASK", "confidence": 0.9724304676055908}]}], "introductionContent": [{"text": "In this paper we revisit phrase-based models with and without factors to translate from and into a morphologically-rich language, Finnish.", "labels": [], "entities": []}, {"text": "We discuss the impact of training data, the use of factored models and ideas of re-inflection as postprocessing.", "labels": [], "entities": []}, {"text": "We also introduce the framework of gappy language models within document-level machine translation (without much success in the given task).", "labels": [], "entities": [{"text": "document-level machine translation", "start_pos": 64, "end_pos": 98, "type": "TASK", "confidence": 0.6115901271502177}]}, {"text": "Our efforts prove the importance of training data once again and demonstrate the use of noisy and out-of-domain data sets as well as the possibility of integrating synthetic training data based on back-translation in phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 230, "end_pos": 233, "type": "TASK", "confidence": 0.79168701171875}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Lower-cased BLEU scores for standard- phrase based SMT on development test data (new- stest 2015). The first three and the second-to- last rows represent constrained settings whereas  the other rows refer to systems with additional re- sources. Efmaral is used in all cases except for the  two models at the top. The last two systems in- clude back-translated news data. Running time is  given for some aligners in terms of walltime (real)  and CPU time (user+sys).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9966927766799927}, {"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.7791101336479187}]}, {"text": " Table 3: Lower-cased BLEU scores for factored  SMT models for Finnish-to-English on develop- ment test data (newstest 2015).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9975839853286743}, {"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.96513432264328}, {"text": "develop- ment test data (newstest 2015)", "start_pos": 85, "end_pos": 124, "type": "DATASET", "confidence": 0.6513543791241116}]}, {"text": " Table 4: Official results for the WMT 2016 news test set. The systems including the back-translated  news data were submitted after the deadline and will not be listed as official submissions. The system in  italics are marked for manual evaluation at WMT.", "labels": [], "entities": [{"text": "WMT 2016 news test set", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.9133817553520203}, {"text": "WMT", "start_pos": 253, "end_pos": 256, "type": "DATASET", "confidence": 0.9803434610366821}]}]}