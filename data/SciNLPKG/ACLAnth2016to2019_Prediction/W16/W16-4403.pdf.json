{"title": [{"text": "A Hierarchical Neural Network for Information Extraction of Product Attribute and Condition Sentences", "labels": [], "entities": [{"text": "Information Extraction of Product Attribute and Condition Sentences", "start_pos": 34, "end_pos": 101, "type": "TASK", "confidence": 0.8615348562598228}]}], "abstractContent": [{"text": "This paper describes a hierarchical neural network we propose for sentence classification to extract product information from product documents.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7283990830183029}]}, {"text": "The network classifies each sentence in a document into attribute and condition classes on the basis of word sequences and sentence sequences in the document.", "labels": [], "entities": []}, {"text": "Experimental results showed the method using the proposed network significantly outperformed baseline methods by taking semantic representation of word and sentence sequential data into account.", "labels": [], "entities": []}, {"text": "We also evaluated the network with two different product domains (insurance and tourism domains) and found that it was effective for both the domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the increase in the number of product documents in electronic form, it is becoming increasingly important to build technologies to extract information from these documents.", "labels": [], "entities": []}, {"text": "In particular, it is useful to extract information about product attributes (such as \"Insurance Premiums\") and their values (such as \"$ 0.50 per day\") from web product documents for many applications such as commodity comparison, product recommendation and question answering systems about products.", "labels": [], "entities": [{"text": "commodity comparison", "start_pos": 208, "end_pos": 228, "type": "TASK", "confidence": 0.6900803297758102}, {"text": "question answering", "start_pos": 257, "end_pos": 275, "type": "TASK", "confidence": 0.6778851747512817}]}, {"text": "For instance, to provide a question answering system that compares particular attributes of products, we need to extract the values of common attributes from each product document.", "labels": [], "entities": [{"text": "question answering", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7223452776670456}]}, {"text": "In this study, we tackled the following two problems for extracting information from product documents on the Web.", "labels": [], "entities": [{"text": "extracting information from product documents", "start_pos": 57, "end_pos": 102, "type": "TASK", "confidence": 0.8114682316780091}]}, {"text": "The first and main problem is to classify each sentence into attribute classes and the second one is to distinguish whether or not each sentence includes condition information, which is helpful in subdividing the attribute.", "labels": [], "entities": []}, {"text": "shows an example insurance product document and an example of classification results of attribute and condition.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the quality of information extraction by judging whether the extracted values matched the annotated labels, excluding the \"NIL\" label which means that the sentence cannot be represented as pre-defined attributes.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7365771383047104}]}, {"text": "We defined the correct values as those for which the annotated labels of the sentence include the extracted values of attributes except the \"NIL\" label.", "labels": [], "entities": []}, {"text": "We used a micro-averaged F 1 score as the evaluation metric for the seven insurance domain documents by applying leave-one-out cross-validation and for the 44 tourism domain documents by applying 10 fold cross-validation.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9841499924659729}]}, {"text": "A comparison of our method with other methods follows.", "labels": [], "entities": []}, {"text": "\u2022 Baseline MaxEnt: This is a method using a maximum entropy model that selects the |V | most frequent words from the training dataset and uses the count of each word as features.", "labels": [], "entities": []}, {"text": "We consider this to be the baseline method for classifying a sentence into value and condition classes using simply words as input features.", "labels": [], "entities": []}, {"text": "\u2022 (proposed) HN: This is a method using the hierarchical network described in Section 3.", "labels": [], "entities": []}, {"text": "The network captures semantic representations of word-and sentence-sequential data and classifies each sentence in a document into attributes and condition classes.", "labels": [], "entities": []}, {"text": "\u2022 HN-word: This is a method using a network that has the same architecture as the proposed network but has no output layer or sentence encoder.", "labels": [], "entities": []}, {"text": "The network takes only the word-sequential information as input features to classify a sentence into value and condition classes.", "labels": [], "entities": []}, {"text": "We used this method to evaluate the effects of using sentence-sequential information to classify a sentence.", "labels": [], "entities": []}, {"text": "\u2022 HN-sent: This is a method using a network that has the same architecture as the proposed network but has no word encoder or word attention layer.", "labels": [], "entities": []}, {"text": "The network ignores the word-sequential information and uses the count of each word as features in classifying a sentence into value and condition classes.", "labels": [], "entities": []}, {"text": "We used this method to evaluate the effects of using word-sequential information and an attention mechanism to classify a sentence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Micro-averaged F 1 scores for test datasets. Asterisks mean there is a significant difference  between the F 1 score obtained for the method indicated and the F 1 score obtained for the baseline  method. Daggers mean there is a significant difference between the F 1 score obtained for the method  and the next largest F 1 score obtained for another method. (*, \u2020: p < .05 , **: p < .01)", "labels": [], "entities": [{"text": "F 1 score obtained", "start_pos": 117, "end_pos": 135, "type": "METRIC", "confidence": 0.9217000752687454}, {"text": "F 1 score", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.884615937868754}, {"text": "F 1 score", "start_pos": 273, "end_pos": 282, "type": "METRIC", "confidence": 0.8781970938046774}]}]}