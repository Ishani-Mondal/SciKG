{"title": [{"text": "On the Evaluation of Dialogue Systems with Next Utterance Classification", "labels": [], "entities": []}], "abstractContent": [{"text": "An open challenge in constructing dialogue systems is developing methods for automatically learning dialogue strategies from large amounts of unlabelled data.", "labels": [], "entities": []}, {"text": "Recent work has proposed Next-Utterance-Classification (NUC) as a sur-rogate task for building dialogue systems from text data.", "labels": [], "entities": []}, {"text": "In this paper we investigate the performance of humans on this task to validate the relevance of NUC as a method of evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Significant efforts have been made in recent years to develop computational methods for learning dialogue strategies offline from large amounts of text data.", "labels": [], "entities": []}, {"text": "One of the challenges of this line of work is to develop methods to automatically evaluate, either directly or indirectly, models that are trained in this manner (), without requiring human labels or human user experiments, which are time consuming and expensive.", "labels": [], "entities": []}, {"text": "The use of automatic tasks and metrics is one key issue in scaling the development of dialogue systems from small domainspecific systems, which require significant engineering, to general conversational agents.", "labels": [], "entities": []}, {"text": "In this paper, we consider tasks and evaluation measures for what we call 'unsupervised' dialogue systems, such as chatbots.", "labels": [], "entities": []}, {"text": "These are in contrast to 'supervised' dialogue systems, which we define as those that explicitly incorporate some supervised signal such as task completion or user satisfaction.", "labels": [], "entities": []}, {"text": "Unsupervised systems can be roughly separated into response generation systems that attempt to produce a likely response given a conversational context, and retrieval-based systems that attempt to select a response from a (possibly large) list of utterances in a corpus.", "labels": [], "entities": []}, {"text": "While there has been significant work on building end-to-end response generation systems (, it has recently been shown that many of the automatic evaluation metrics used for such systems correlate poorly or not at all with human judgement of the generated responses (.", "labels": [], "entities": []}, {"text": "Retrieval-based systems are of interest because they admit a natural evaluation metric, namely the recall and precision measures.", "labels": [], "entities": [{"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9991976618766785}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9876461625099182}]}, {"text": "First introduced for evaluating user simulations by, such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems ().", "labels": [], "entities": []}, {"text": "These models are trained on the task of selecting the correct response from a candidate list, which we call NextUtterance-Classification (NUC, detailed in Section 3), and are evaluated using the metric of recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 205, "end_pos": 211, "type": "METRIC", "confidence": 0.9938536882400513}]}, {"text": "NUC is useful for several reasons: 1) the performance (i.e. loss or error) is easy to com-pute automatically, 2) it is simple to adjust the difficulty of the task, 3) the task is interpretable and amenable to comparison with human performance, 4) it is an easier task compared to generative dialogue modeling, which is difficult for endto-end systems ( , and 5) models trained with NUC can be converted to dialogue systems by retrieving from the full corpus (.", "labels": [], "entities": [{"text": "generative dialogue modeling", "start_pos": 280, "end_pos": 308, "type": "TASK", "confidence": 0.9672826925913492}]}, {"text": "In this case, NUC additionally allows for making hard constraints on the allowable outputs of the system (to prevent offensive responses), and guarantees that the responses are fluent (because they were generated by humans).", "labels": [], "entities": [{"text": "NUC", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.7918291687965393}]}, {"text": "Thus, NUC can bethought of both as an intermediate task that can be used to evaluate the ability of systems to understand natural language conversations, similar to the bAbI tasks for language understanding , and as a useful framework for building chatbots.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 184, "end_pos": 206, "type": "TASK", "confidence": 0.7226631194353104}]}, {"text": "With the huge size of current dialogue datasets that contain millions of utterances () and the increasing amount of natural language data, it is conceivable that retrieval-based systems will be able to have engaging conversations with humans.", "labels": [], "entities": []}, {"text": "However, despite the current work with NUC, there has been no verification of whether machine and human performance differ on this task.", "labels": [], "entities": [{"text": "NUC", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.969257116317749}]}, {"text": "This cannot be assumed; it is possible that no significant gap exists between the two, as is the case with many current automatic response generation metrics (.", "labels": [], "entities": []}, {"text": "Further, it is important to benchmark human performance on new tasks such as NUC to determine when research has outgrown their use.", "labels": [], "entities": [{"text": "NUC", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.6991218328475952}]}, {"text": "In this paper, we consider to what extent NUC is achievable by humans, whether human performance varies according to expertise, and whether there is room for machine performance to improve (or has reached human performance already) and we should move to more complex conversational tasks.", "labels": [], "entities": []}, {"text": "We performed a user study on three different datasets: the SubTle Corpus of movie dialogues, the Twitter Corpus (, and the Ubuntu Dialogue Corpus ().", "labels": [], "entities": [{"text": "SubTle Corpus of movie dialogues", "start_pos": 59, "end_pos": 91, "type": "DATASET", "confidence": 0.913356339931488}, {"text": "Twitter Corpus", "start_pos": 97, "end_pos": 111, "type": "DATASET", "confidence": 0.8635419607162476}, {"text": "Ubuntu Dialogue Corpus", "start_pos": 123, "end_pos": 145, "type": "DATASET", "confidence": 0.858002781867981}]}, {"text": "Since conversations in the Ubuntu Dialogue Corpus are highly technical, we recruit 'expert' humans who are adept with the Ubuntu terminology, whom we compare with a state-of-the-art machine learning agent on all datasets.", "labels": [], "entities": [{"text": "Ubuntu Dialogue Corpus", "start_pos": 27, "end_pos": 49, "type": "DATASET", "confidence": 0.8837065498034159}]}, {"text": "We find that there is indeed a significant separation between machine and expert hu- man performance, suggesting that NUC is a useful intermediate task for measuring progress.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data on the 145 AMT participants.", "labels": [], "entities": [{"text": "AMT participants", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.7865273058414459}]}, {"text": " Table 2: Average results on each corpus. 'Number of Users' indicates the number of respondents for each  category. 'AMT experts' and 'AMT non-experts' are combined for the Movie and Twitter corpora. 95%  confidence intervals are calculated using the normal approximation, which assumes subjects answer each  question independently of other examples and subjects. Starred (*) results indicate a poor approximation  of the confidence interval due to high scores with small sample size, according to the rule of thumb by", "labels": [], "entities": []}]}