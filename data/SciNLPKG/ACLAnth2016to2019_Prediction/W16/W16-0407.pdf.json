{"title": [{"text": "Fashioning Data -A Social Media Perspective on Fast Fashion Brands", "labels": [], "entities": [{"text": "Fashioning Data -A Social Media Perspective on Fast Fashion Brands", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.6780064133080569}]}], "abstractContent": [{"text": "In this paper, we study the performance of N-gram language models on classification tasks such as sentiment analysis and spam detection and evaluate the effect of prior probability estimates on the results.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.9509737193584442}, {"text": "spam detection", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.9217998385429382}]}, {"text": "Our data is in the form of public online posts pertaining to fast fashion brands, from different social media channels (Twitter and Facebook).", "labels": [], "entities": []}, {"text": "We propose a novel ensemble model based on the combination of different N-grams in order to deal with the heteroskedastic nature of data collected from these social media channels.", "labels": [], "entities": []}, {"text": "This has been further extended to increase the efficacy of the classification results.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, the rise of social media channels like Twitter, Facebook and Instagram have opened new avenues for people to express their opinions and generate their own content.", "labels": [], "entities": []}, {"text": "Companies such as Simplymeasured (www.simplymeasured.com) and Gnip (www.gnip.com) aggregate data from different networks to help brands form a more complete understanding of how well they engage with users and perform online.", "labels": [], "entities": []}, {"text": "Companies such as Metamind, Alchemy and Semantria provide online APIs for sentiment analysis and associated tasks.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.9478921294212341}]}, {"text": "Sentiment analysis is a growing area of Natural Language Processing with research ranging from document level classification (Pang and Lee) to learning the polarity of words and phrases (Esuli and Sebastiani).", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9553628861904144}, {"text": "document level classification", "start_pos": 95, "end_pos": 124, "type": "TASK", "confidence": 0.6399024029572805}]}, {"text": "Given the character limitations on tweets, classifying the sentiment of Twitter messages is most similar to sentence-level sentiment analysis.", "labels": [], "entities": [{"text": "classifying the sentiment of Twitter messages", "start_pos": 43, "end_pos": 88, "type": "TASK", "confidence": 0.8312571545441946}, {"text": "sentence-level sentiment analysis", "start_pos": 108, "end_pos": 141, "type": "TASK", "confidence": 0.7367368439833323}]}, {"text": "Some researchers have explored the use of part-of-speech features but results remain mixed.", "labels": [], "entities": []}, {"text": "Researchers in and have reported different ways of automatically collecting training data by relying on emoticons for defining the sentiment labels in their training data.", "labels": [], "entities": []}, {"text": "However as per our observations the presence of an emoticon does not necessarily divulge its sentiment and hence we have taken the approach of manually labeling the training and test data.", "labels": [], "entities": []}, {"text": "Da Silva et al. have introduced an approach of using classifier ensembles to determine the sentiment of tweets.", "labels": [], "entities": []}, {"text": "However they only consider a binary classification of tweets (i.e. positive and negative) and use a heterogeneous ensemble of classifiers like Multinomial Na\u00efve Bayes, SVM, Logistic Regression and Random Forests.", "labels": [], "entities": []}, {"text": "Agarwal et al propose a method of sentiment analysis using a tree kernel and a set of hand crafted POS features.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9480824172496796}]}, {"text": "Twitter hashtags have been extensively used in to train a classifier using the Adaboost algorithm.", "labels": [], "entities": []}, {"text": "In recent years several competitions like SemEval 2014 have included sentiment analysis of tweets as a major task.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.830193042755127}, {"text": "sentiment analysis of tweets", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.9195089340209961}]}, {"text": "This has led to several state-of-the-art performances like where the authors have used a Deep Learning approach using a combination of sentiment specific semantic word embeddings and hand crafted features.", "labels": [], "entities": []}, {"text": "The authors in have enhanced Twitter sentiment classification using contextual information like geolocation, timezones etc.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.6672330300013224}]}, {"text": "While a great deal of recent research has focused on sentiment analysis of Twitter data and spam detection (Wang et al) less attention has been devoted to extending these classification tasks to public Facebook posts.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.937422901391983}, {"text": "spam detection", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.833204984664917}]}, {"text": "Furthermore, while domains such as politics (; Yang et al.) and sports (Hong and Skiena) have received strong coverage, the genre of commercial fashion brands has not been mined as frequently for predictive and classification tasks The absence of literature on cross channel sentiment analysis with a special focus on the implications of prior distributions on the classification results has motivated us to undertake the following study.", "labels": [], "entities": [{"text": "cross channel sentiment analysis", "start_pos": 261, "end_pos": 293, "type": "TASK", "confidence": 0.7081622779369354}]}, {"text": "The niche segment of fast fashion brands was chosen because it remains largely unexplored.", "labels": [], "entities": []}, {"text": "An attempt has been made to provide a clear comprehensive view of the performance metrics across different channels (Twitter and Facebook in our case) while noting the difference in trends among them.", "labels": [], "entities": []}, {"text": "The major contributions of the present work are as follows: We have collected and manually annotated a dataset ** of posts from major social media channels (Twitter and Facebook).", "labels": [], "entities": []}, {"text": "Our dataset was based on posts about fast fashion brands, thereby extending the application of sentiment analysis and spam detection to this infrequently-explored genre.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.9497700333595276}, {"text": "spam detection", "start_pos": 118, "end_pos": 132, "type": "TASK", "confidence": 0.8607670664787292}]}, {"text": "The inclusion of more than one social channel provides across sectional view of the social media spectrum.", "labels": [], "entities": []}, {"text": "It was also helpful in gauging the performance of the same algorithm on channels with disparate content (different in terms of syntactic and semantic structure).", "labels": [], "entities": []}, {"text": "We have extensively analyzed the effect of priors on the associated tasks and compared different N-gram models on many statistical performance metrics like Accuracy, Precision, Recall, Specificity and F1-score.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9992642998695374}, {"text": "Precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9709464311599731}, {"text": "Recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9374759793281555}, {"text": "F1-score", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9969897270202637}]}, {"text": "Use of N-grams as features obviates the need for tedious feature engineering which often entails a classification task.", "labels": [], "entities": []}, {"text": "The proposed generative ensemble model provides an easily implementable and lightweight framework which can be extended to any classification problem.", "labels": [], "entities": [{"text": "generative ensemble", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.9093619585037231}]}, {"text": "This is because it does not make any implicit/explicit assumption about the nature or distribution of the data.", "labels": [], "entities": []}, {"text": "Thus, we have developed a roadmap for cross channel text analysis and classification, thereby providing a unified and holistic view of any topic or subject (fashion brands in our case).", "labels": [], "entities": [{"text": "cross channel text analysis", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.6928490698337555}]}, {"text": "The key brands identified were fast fashion brands (Zara, Forever 21, H&M etc.) which target young customers in their late teens and early twenties and have a high turnover rate as part of their business strategy.", "labels": [], "entities": []}, {"text": "We were particularly interested in studying this demographic since their customers frequently take to social media to express their satisfaction or dissatisfaction with products purchased.", "labels": [], "entities": []}, {"text": "Due to the high turnover rate and short shelf-life of most of their products, opinions about their newest items are created every few months.", "labels": [], "entities": []}, {"text": "This made data collection easier and more attuned to public opinion.", "labels": [], "entities": [{"text": "data collection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.69146828353405}]}, {"text": "We divide the paper as follows: In Section 2 we discuss the collection and distribution of data in details, we also include the steps taken for preprocessing the data, in Section 3 the algorithms used for spam detection and sentiment analysis have been detailed along with intuitive explanation of why they work, in Section 4 we present our experiments and observations which includes a detailed analysis of the proposed algorithm for each channel along with across channel view of different performance metrics, in Section 5 we conclude the paper.", "labels": [], "entities": [{"text": "spam detection", "start_pos": 205, "end_pos": 219, "type": "TASK", "confidence": 0.9157544374465942}, {"text": "sentiment analysis", "start_pos": 224, "end_pos": 242, "type": "TASK", "confidence": 0.8565445840358734}]}], "datasetContent": [{"text": "N-gram models have been trained for different values of N (N = 1 to N = 7).", "labels": [], "entities": []}, {"text": "During the classification phase using the Na\u00efve Bayes classifier, we have experimented under two conditions 1.", "labels": [], "entities": [{"text": "classification", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9764910340309143}, {"text": "Na\u00efve Bayes classifier", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.8711556196212769}]}, {"text": "Calculating the class label probabilities while discounting the prior probabilities learned during the training phase (i.e. assuming equal prior class distribution).", "labels": [], "entities": []}, {"text": "2. Taking the prior probabilities into account while calculating the class label probabilities.", "labels": [], "entities": []}, {"text": "In order to get a clear insight into the effects of priors on different classification tasks across different channels, we have studied each scenario independently.", "labels": [], "entities": []}, {"text": "First we present the results of each channel separately on the twin classification tasks then we provide across channel view of the effect of priors.", "labels": [], "entities": []}, {"text": "illustrates the performance (in terms of accuracy) of different N-gram models for spam detection on Facebook, as can be seen from the figure the effect of priors is more profound on the unigrams and bigrams in comparison to the higher order N-grams.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9990350008010864}, {"text": "spam detection", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.8717837333679199}]}, {"text": "Another notable feature is how the performance almost plateaus after N=4 for both cases (i.e. taking the prior into account and discounting the prior).", "labels": [], "entities": []}, {"text": "Thus, priors play an important role in the prediction of classes especially if the data is highly biased (as in our case).", "labels": [], "entities": [{"text": "prediction of classes", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.8803694446881613}]}, {"text": "depict the performance of spam detection on Facebook they are exactly are in the same vein as the results for sentiment analysis.", "labels": [], "entities": [{"text": "spam detection", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8366626501083374}, {"text": "sentiment analysis", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.9296181201934814}]}], "tableCaptions": [{"text": " Table 5. Performance Metrics for Sentiment Analysis of Facebook  Posts (Without Priors)", "labels": [], "entities": [{"text": "Sentiment Analysis of Facebook  Posts", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.9228732943534851}]}, {"text": " Table 6. Performance Metrics for Sentiment Analysis of Facebook  Posts (With Priors)", "labels": [], "entities": [{"text": "Sentiment Analysis of Facebook  Posts", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.9186154007911682}]}, {"text": " Table 8. Performance Metrics for Spam Detection of Facebook Posts  (With Priors)", "labels": [], "entities": [{"text": "Spam Detection of Facebook Posts", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.9263558626174927}]}, {"text": " Table 9: Performance Metrics for Sentiment Analysis on Facebook  (Proposed Method i.e. Algorithm 2)", "labels": [], "entities": [{"text": "Sentiment Analysis on Facebook", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.867341473698616}]}, {"text": " Table 10: Performance Metrics for Spam Detection on Facebook  (Proposed Method i.e. Algorithm 2)", "labels": [], "entities": [{"text": "Spam Detection on Facebook", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.8612451553344727}]}, {"text": " Table 11: Performance Metrics for Sentiment Analysis of Tweets  (Without Priors)", "labels": [], "entities": [{"text": "Sentiment Analysis of Tweets", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.9422128349542618}]}, {"text": " Table 12: Performance Metrics for Sentiment Analysis of Tweets  (With Priors)", "labels": [], "entities": [{"text": "Sentiment Analysis of Tweets", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.9435496479272842}]}, {"text": " Table 13: Performance Metrics for Spam Detection of Twitter Posts  (Without Priors)", "labels": [], "entities": [{"text": "Spam Detection of Twitter Posts", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.9364607214927674}]}, {"text": " Table 14: Performance Metrics for Spam Detection of Twitter Posts  (With Priors)", "labels": [], "entities": [{"text": "Spam Detection of Twitter Posts", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.9354959726333618}]}, {"text": " Table 15: Performance Metrics for Sentiment Analysis of Tweets  (Proposed Method i.e. Algorithm 2)", "labels": [], "entities": [{"text": "Sentiment Analysis of Tweets", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.9155023992061615}]}, {"text": " Table 16: Performance Metrics for Spam Detection of Tweets  (Proposed Method i.e. Algorithm 2)", "labels": [], "entities": [{"text": "Spam Detection of Tweets", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.9287074953317642}]}]}