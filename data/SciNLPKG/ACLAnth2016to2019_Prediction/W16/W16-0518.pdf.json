{"title": [{"text": "Feature-Rich Error Detection in Scientific Writing Using Logistic Regression", "labels": [], "entities": [{"text": "Feature-Rich Error Detection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5863946874936422}]}], "abstractContent": [{"text": "The goal of the Automatic Evaluation of Scientific Writing (AESW) Shared Task 2016 is to identify sentences in scientific articles which need editing to improve their correct-ness and readability or to make them better fit within the genre at hand.", "labels": [], "entities": [{"text": "Automatic Evaluation of Scientific Writing (AESW) Shared Task", "start_pos": 16, "end_pos": 77, "type": "TASK", "confidence": 0.6750844538211822}]}, {"text": "We encode many different types of errors occurring in the dataset by linguistic features.", "labels": [], "entities": []}, {"text": "We use logistic regression to assign a probability indicating whether a sentence needs to be edited.", "labels": [], "entities": []}, {"text": "We participate in both tracks at AESW 2016: binary prediction and probabilistic estimation.", "labels": [], "entities": [{"text": "AESW 2016", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.8849489688873291}, {"text": "binary prediction", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8210639655590057}]}, {"text": "In the former track, our model (HITS) gets the fifth place and in the latter one, it ranks first according to the evaluation metric.", "labels": [], "entities": [{"text": "HITS", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.7729843854904175}]}], "introductionContent": [{"text": "The AESW 2016 Shared Task is about predicting if a given sentence in a scientific article needs language editing.", "labels": [], "entities": [{"text": "AESW 2016 Shared Task", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.8552386164665222}]}, {"text": "It can therefore be pictured as a binary classification task.", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7315448323885599}]}, {"text": "Two types of prediction are evaluated: binary prediction (false or true) and probabilistic estimation (between 0 and 1).", "labels": [], "entities": []}, {"text": "These types of prediction form the two tracks of the shared task, both of which we participate in.", "labels": [], "entities": []}, {"text": "We solve both problems by applying a logistic regression model.", "labels": [], "entities": []}, {"text": "We design a variety of features based on a thorough analysis of the training data.", "labels": [], "entities": []}, {"text": "We choose the set of features that yields the highest performance on training and development sets.", "labels": [], "entities": []}, {"text": "Accounting for the imbalance of numbers of wrong and correct sentences in the training data during feature selection we obtain a model for the probabilistic task that outranks our competitors' systems.", "labels": [], "entities": []}, {"text": "However, a detailed analysis of the results shows that the model takes advantage of the evaluation metric and that our less informed system produces results that are, although not yielding atop evaluation score, more meaningful.", "labels": [], "entities": []}, {"text": "In the course of a profound analysis of the training data we encounter both linguistic errors, which likely occur in diverse genres, and such errors that are intrinsic to scientific writing and thus rank among the major challenges of this task.", "labels": [], "entities": []}, {"text": "As pointed out on the AESW 2016 webpage 1 , correcting problems concerning diction and style is a matter of opinion.", "labels": [], "entities": [{"text": "AESW 2016 webpage 1", "start_pos": 22, "end_pos": 41, "type": "DATASET", "confidence": 0.9691665023565292}]}, {"text": "It depends on factors that are not necessarily deducible from linguistic properties.", "labels": [], "entities": []}, {"text": "Common abbreviations are an example.", "labels": [], "entities": []}, {"text": "There are cases where they are accepted by an editor, and there are cases where they are corrected.", "labels": [], "entities": []}, {"text": "That is, sometimes e.g. is left as is and sometimes it is changed to for instance or for example without any obvious reason.", "labels": [], "entities": []}, {"text": "There are even words that are corrected in opposite directions.", "labels": [], "entities": []}, {"text": "For example, the first letter of the name prefix van has been corrected to be uppercase in some sentences and also has been corrected to be lowercase in other sentences.", "labels": [], "entities": []}, {"text": "Especially abbreviations that are not common within one particular domain, but are used in isolated documents are problematic.", "labels": [], "entities": []}, {"text": "This is due to limitations of the dataset, which provides only paragraphs, but not documents as contexts for sentences.", "labels": [], "entities": []}, {"text": "For example, we may assume that R-G has been introduced as a technical term at some point in a document.", "labels": [], "entities": []}, {"text": "But since we do not know which paragraphs belong to this document, we cannot be sure that this is the case.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of the types of errors we encountered.", "labels": [], "entities": []}, {"text": "In Section 3 we introduce our system design, detail on how we derive features from our data analysis, what kinds of language models we apply, give a short outline on logistic regression and describe the implementation of our system.", "labels": [], "entities": []}, {"text": "In Section 4 we describe our training steps, followed by reporting results in Section 5, a discussion of lessons learned in Section 6 and related work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation score works well fora system whose only purpose is the identification of erroneous sentences, so for the binary classification task the F1-score is perfectly suitable.", "labels": [], "entities": [{"text": "identification of erroneous sentences", "start_pos": 70, "end_pos": 107, "type": "TASK", "confidence": 0.8180173188447952}, {"text": "binary classification", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.6725026816129684}, {"text": "F1-score", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9986716508865356}]}, {"text": "However, it maybe worth considering whether the information that a sentence is fine could be valuable, too.", "labels": [], "entities": []}, {"text": "That might be the case whenever sentences must be further processed.", "labels": [], "entities": []}, {"text": "In that case the accuracy metric might be the better choice, because it takes all correct classifications into account, whereas the F1-score does not reward instances correctly classified as false.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9993477463722229}, {"text": "F1-score", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9988564252853394}]}, {"text": "As for the probabilistic task, our results show that the evaluation score is not strict enough, and that it is prone to misjudge the expressiveness of the results.", "labels": [], "entities": []}, {"text": "In fact, correctly assigning 1.0 to only one faulty sentence and 0.5 to all other sentences yields a score of 0.8571.", "labels": [], "entities": []}, {"text": "The result is not as extreme if precision and recall are computed based on the mean absolute error, which results in 0.6667.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9994142055511475}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9994179010391235}, {"text": "mean absolute error", "start_pos": 79, "end_pos": 98, "type": "METRIC", "confidence": 0.6980223457018534}]}, {"text": "This, still, clearly overestimates the quality of the results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: Results on test data", "labels": [], "entities": []}]}