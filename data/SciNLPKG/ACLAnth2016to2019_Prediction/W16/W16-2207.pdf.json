{"title": [{"text": "Neural Network-based Word Alignment through Score Aggregation", "labels": [], "entities": [{"text": "Neural Network-based Word Alignment", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6525839790701866}]}], "abstractContent": [{"text": "We present a simple neural network for word alignment that builds source and target word window representations to compute alignment scores for sentence pairs.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7856516540050507}]}, {"text": "To enable unsupervised training, we use an aggregation operation that summarizes the alignment scores fora given target word.", "labels": [], "entities": []}, {"text": "A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present.", "labels": [], "entities": []}, {"text": "Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on English-Czech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment .", "labels": [], "entities": [{"text": "alignment", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.5641108751296997}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.917404294013977}, {"text": "AER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9975675344467163}, {"text": "AER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9965774416923523}, {"text": "AER", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.9967162609100342}]}], "introductionContent": [{"text": "Word alignment is the task of finding the correspondence between source and target words in a pair of sentences that are translations of each other.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7149732112884521}]}, {"text": "Generative models for this task) still form the basis for many machine translation systems (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7745482623577118}]}, {"text": "Recent neural approaches include who introduce a feed-forward networkbased model trained on alignments that were generated by a traditional generative model.", "labels": [], "entities": []}, {"text": "This treats potentially erroneous alignments as supervision.", "labels": [], "entities": []}, {"text": "sidesteps this issue by negative sampling to train a recurrent-neural network on unlabeled data.", "labels": [], "entities": []}, {"text": "They optimize a global loss that requires an expensive beam search to approximate the sum overall alignments.", "labels": [], "entities": []}, {"text": "In this paper we introduce a word alignment model that is simpler in structure and which relies on a more tractable training procedure.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.8037135601043701}]}, {"text": "Our model is a neural network that extracts context information from source and target sentences and then computes simple dot products to estimate alignment links.", "labels": [], "entities": []}, {"text": "Our objective function is wordfactored and does not require the expensive computation associated with global loss functions.", "labels": [], "entities": []}, {"text": "The model can be easily trained on unlabeled data via a novel but simple aggregation operation which has been successfully applied in the computer vision literature (.", "labels": [], "entities": []}, {"text": "The aggregation combines the scores of all source words fora particular target word and promotes source words which are likely to be aligned with a given target word according to the knowledge the model has learned so far.", "labels": [], "entities": []}, {"text": "At test time, the aggregation operation is removed and source words are aligned to target words by choosing the highest scoring candidates ( \u00a72, \u00a73).", "labels": [], "entities": []}, {"text": "We evaluate several forms for our aggregation operation such as computing the sum, max and LogSumExp over alignment scores.", "labels": [], "entities": [{"text": "max", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.8580034375190735}]}, {"text": "Results on English-French, English-Romanian, and CzechEnglish alignment show that our model significantly outperforms Fast Align, a popular loglinear reparameterization of IBM Model 2 (Dyer et al., 2013; \u00a74).", "labels": [], "entities": [{"text": "CzechEnglish alignment", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7587519288063049}]}], "datasetContent": [{"text": "We use the English-French Hansards corpus as distributed by the NAACL 2003 shared task and a set of 515 sentences for testing).", "labels": [], "entities": [{"text": "Hansards corpus", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.8362056910991669}, {"text": "NAACL 2003 shared task", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.9451621025800705}]}, {"text": "Our models are evaluated in terms of precision, recall, F-measure and Alignment Error Rate (AER).", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9997512698173523}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9997556805610657}, {"text": "F-measure", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9978265166282654}, {"text": "Alignment Error Rate (AER)", "start_pos": 70, "end_pos": 96, "type": "METRIC", "confidence": 0.9672141174475352}]}, {"text": "We train models in each language direction and then symmetrize the resulting alignments using either the intersection or the grow-diag-final-and heuristic).", "labels": [], "entities": []}, {"text": "We validated the choice of symmetrization heuristic on each language pair and chose the best one for each model considering the two aforementioned types as well as grow-diag-final and growdiag.", "labels": [], "entities": []}, {"text": "Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit ().", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.6115143199761709}]}, {"text": "For English-French, we train on the news commentary corpus v10, for English-Czech we used news commentary corpus v11, and for Romanian-English we used the Europarl corpus v8.", "labels": [], "entities": [{"text": "Europarl corpus v8", "start_pos": 155, "end_pos": 173, "type": "DATASET", "confidence": 0.9704630970954895}]}, {"text": "We tuned our models on the WMT2015 test set for EnglishCzech as well as for Romanian-English; for English-French we tuned on the WMT2014 test set.", "labels": [], "entities": [{"text": "WMT2015 test set", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.9880957206090292}, {"text": "EnglishCzech", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9398720264434814}, {"text": "WMT2014 test set", "start_pos": 129, "end_pos": 145, "type": "DATASET", "confidence": 0.9918448527654012}]}, {"text": "Final results are reported on the WMT2016 test set for English-Czech as well as RomanianEnglish, and for English-French we report results on the WMT2015 test set (as there is no track for this language-pair in 2016).", "labels": [], "entities": [{"text": "WMT2016 test set", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9834770162900289}, {"text": "WMT2015 test set", "start_pos": 145, "end_pos": 161, "type": "DATASET", "confidence": 0.9870678981145223}]}, {"text": "We compare our model to Fast Align, a popular log-linear reparameterization of IBM Model 2 (Dyer et al., 2013).", "labels": [], "entities": []}, {"text": "Our alignments achieve slightly better results for Romanian-English as well as English-Czech while performing on par with Fast Align on English-French translation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Alignment error rates for different aggre- gation operations in each language direction and  with grow-diag-final-and symmetrization.", "labels": [], "entities": []}, {"text": " Table 2: Alignment error rates using different input features in each language direction and with grow- diag-final-and symmetrization.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9431205987930298}]}, {"text": " Table 3: English-French results on the test set in  terms of precision (P), recall (R), F-score (F1) and  AER; ensemble denotes a combination of four sys- tems and we use the intersection (inter) and grow- diag-final-and symmetrization (gdfa) heuristics.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9412662237882614}, {"text": "recall (R)", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9592382907867432}, {"text": "F-score (F1)", "start_pos": 89, "end_pos": 101, "type": "METRIC", "confidence": 0.9309050440788269}, {"text": "AER", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9978908896446228}]}, {"text": " Table 4: Romanian-English results (cf.", "labels": [], "entities": []}, {"text": " Table 5: Czech-English results (cf.", "labels": [], "entities": []}]}