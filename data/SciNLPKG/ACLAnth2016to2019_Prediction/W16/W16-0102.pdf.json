{"title": [{"text": "Crowdsourcing for (almost) Real-time Question Answering", "labels": [], "entities": [{"text": "Real-time Question Answering", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.6318447887897491}]}], "abstractContent": [{"text": "Modern search engines have made dramatic progress in the answering of many user's questions about facts, such as those that might be retrieved or directly inferred from a knowledge base.", "labels": [], "entities": [{"text": "answering of many user's questions about facts, such as those that might be retrieved or directly inferred from a knowledge base", "start_pos": 57, "end_pos": 185, "type": "Description", "confidence": 0.7482729411643484}]}, {"text": "However, many other questions that real users ask are more complex, such as asking for opinions or advice fora particular situation, and are still largely beyond the competence of the computer systems.", "labels": [], "entities": []}, {"text": "As conversational agents become more popular, QA systems are increasingly expected to handle such complex questions, and to do so in (nearly) real-time, as the searcher is unlikely to wait longer than a minute or two for an answer.", "labels": [], "entities": []}, {"text": "One way to overcome some of the challenges in complex question answering is crowdsourc-ing.", "labels": [], "entities": [{"text": "question answering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.752057820558548}]}, {"text": "We explore two ways crowdsourcing can assist a question answering system that operates in (near) real time: by providing answer validation, which could be used to filter or re-rank the candidate answers, and by creating the answer candidates directly.", "labels": [], "entities": [{"text": "question answering", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7576439678668976}]}, {"text": "Specifically , we focus on understanding the effects of time restrictions in the near real-time QA setting.", "labels": [], "entities": []}, {"text": "Our experiments show that even within a one minute time limit, crowd workers can produce reliable ratings for up to three answer candidates, and generate answers that are better than an average automated system from the LiveQA 2015 shared task.", "labels": [], "entities": [{"text": "LiveQA 2015 shared task", "start_pos": 220, "end_pos": 243, "type": "DATASET", "confidence": 0.9196744561195374}]}, {"text": "Our findings can be useful for developing hybrid human-computer systems for automatic question answering and conversational agents.", "labels": [], "entities": [{"text": "question answering", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.8031544387340546}]}], "introductionContent": [{"text": "It has long been a dream to communicate with a computer as one might with another human being using natural language speech and text.", "labels": [], "entities": []}, {"text": "Nowadays, we are coming closer to this dream, as natural language interfaces become increasingly popular.", "labels": [], "entities": []}, {"text": "Our phones are already reasonably good at recognizing speech, and personal assistants, such as Apple Siri, Google Now, Microsoft Cortana, Amazon Alexa, etc., help us with everyday tasks and answer some of our questions.", "labels": [], "entities": []}, {"text": "Chat bots are arguably considered \"the next big thing\", and a number of startups developing this kind of technology has emerged in Silicon Valley and around the world . Question answering is one of the major components of such personal assistants.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.8495047092437744}]}, {"text": "Existing techniques already allow users to get direct answers to their factoid questions.", "labels": [], "entities": []}, {"text": "However, there is still a large number of more complex questions, such as advice or accepted general opinions, for which users have to dig into the \"10 blue links\" and extractor synthesize answers from information buried within the retrieved documents.", "labels": [], "entities": []}, {"text": "To cater to these informational needs, community question answering (CQA) sites emerged, such as Yahoo!", "labels": [], "entities": [{"text": "community question answering (CQA)", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.8130228916803995}]}, {"text": "These sites provide a popular way to connect information seekers with answerers.", "labels": [], "entities": []}, {"text": "Unfortunately, it can take minutes or hours, and sometimes days, for the community to respond, and some questions are left unanswered altogether.", "labels": [], "entities": []}, {"text": "To facilitate research on this challenge, TREC LiveQA shared task 2 was started in 2015, where automated systems attempt to answer real users' questions within a 1 minute period.", "labels": [], "entities": [{"text": "TREC LiveQA shared task 2", "start_pos": 42, "end_pos": 67, "type": "DATASET", "confidence": 0.7823632478713989}]}, {"text": "This task was successful, with the winning system able to automatically return a reasonable answer to more than half of the submitted questions, as assessed for TREC by the trained judges from NIST.", "labels": [], "entities": [{"text": "TREC", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9805328845977783}, {"text": "NIST", "start_pos": 193, "end_pos": 197, "type": "DATASET", "confidence": 0.9771296381950378}]}, {"text": "Nevertheless, many questions were unable to be answered well by any of the participating systems.", "labels": [], "entities": []}, {"text": "In this work we explore two ways common crowdsourcing can be used to help an automated system answer complex user questions in near realtime scenario, e.g., within a minute.", "labels": [], "entities": []}, {"text": "More specifically, we study if crowd workers can quickly and reliably judge the quality of the proposed answer candidates, and if it is possible to obtain reasonable written answers from the crowd within a limited amount of time.", "labels": [], "entities": []}, {"text": "Our research questions can be stated as: 1.", "labels": [], "entities": []}, {"text": "Can crowdsourcing be used to judge the quality of answers to non-factoid questions under a time limit?", "labels": [], "entities": []}, {"text": "Is it possible to use crowdsourcing to collect answers to real user questions under a time limit?", "labels": [], "entities": []}, {"text": "How does the quality of crowdsourced answers to non-factoid questions compare to original CQA answers, and to automatic answers from TREC LiveQA systems?", "labels": [], "entities": [{"text": "TREC LiveQA", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.8656379282474518}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of different types of answers for Yahoo! Answers questions", "labels": [], "entities": [{"text": "Yahoo! Answers questions", "start_pos": 55, "end_pos": 79, "type": "DATASET", "confidence": 0.8217793852090836}]}]}