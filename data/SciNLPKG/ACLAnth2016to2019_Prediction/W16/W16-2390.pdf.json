{"title": [], "abstractContent": [{"text": "This paper outlines the UU-SVM system for Task 1 of the WMT16 Shared Task in Quality Estimation.", "labels": [], "entities": [{"text": "WMT16 Shared Task in Quality Estimation", "start_pos": 56, "end_pos": 95, "type": "TASK", "confidence": 0.6347491145133972}]}, {"text": "Our system uses Support Vector Machine Regression to investigate the impact of a series of features aiming to convey translation quality.", "labels": [], "entities": []}, {"text": "We propose novel features measuring reordering and noun translation errors.", "labels": [], "entities": [{"text": "noun translation errors", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.756004532178243}]}, {"text": "We show that we can outperform the baseline when we combine it with a subset of our new features .", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we describe Uppsala University's submission to the WMT16 shared task in Quality Estimation (QE).", "labels": [], "entities": [{"text": "Uppsala University", "start_pos": 27, "end_pos": 45, "type": "DATASET", "confidence": 0.9376339912414551}, {"text": "WMT16 shared task in Quality Estimation (QE)", "start_pos": 66, "end_pos": 110, "type": "TASK", "confidence": 0.7322847644488016}]}, {"text": "Machine Translation Quality Estimation is the task of assessing the quality of a machine translated unit at runtime, without using reference translations.", "labels": [], "entities": [{"text": "Machine Translation Quality Estimation", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8239348381757736}]}, {"text": "The different units considered for the 2016 shared task in quality estimation are words, phrases and sentences.", "labels": [], "entities": []}, {"text": "We participated in task 1, which focuses on sentence-level QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.624213457107544}]}, {"text": "Most modern approaches set the task as a regression problem -attempting to accurately predict a continuous quality label through representing translations with feature vectors.", "labels": [], "entities": []}, {"text": "The performance of such approaches rely on determining and extracting features that correlate strongly with the proposed quality label and the impact of a wide variety of features.", "labels": [], "entities": []}, {"text": "Different types of systems, including system-dependent (glass-box) or system-independent (black-box), linguistically or statistically motivated features, have been explored ().", "labels": [], "entities": []}, {"text": "The quality label proposed for the sentence-level task is Human-targeted Translation Edit Rate (HTER)), which sets the focus on predicting the post-editing effort needed to correct the translation.", "labels": [], "entities": [{"text": "Human-targeted Translation Edit Rate (HTER))", "start_pos": 58, "end_pos": 102, "type": "METRIC", "confidence": 0.766428257737841}]}, {"text": "As no information from the MT system used to translate the data was provided, only black-box features can be considered.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.7003364562988281}]}, {"text": "Furthermore, since the dataset only consists of one translation direction, English-German, language-specific features can be exploited.", "labels": [], "entities": []}, {"text": "Our submission proposes novel features attempting to capture some common noun translation errors from English to German as well as measuring the amount of reordering done by the SMT system.", "labels": [], "entities": [{"text": "common noun translation errors from English to German", "start_pos": 66, "end_pos": 119, "type": "TASK", "confidence": 0.7988474778831005}, {"text": "SMT", "start_pos": 178, "end_pos": 181, "type": "TASK", "confidence": 0.9816102981567383}]}, {"text": "These features are combined with more generic linguistically motivated black-box features that improved the prediction accuracy.", "labels": [], "entities": [{"text": "prediction", "start_pos": 108, "end_pos": 118, "type": "TASK", "confidence": 0.9381045699119568}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9368798136711121}]}], "datasetContent": [{"text": "The dataset for task 1 spans a total of 15,000 English-German translations from the IT domain.", "labels": [], "entities": []}, {"text": "Each entry consists of a source segment, its machine translation, a post-edition of the translation and an edit distance score (HTER) derived from the post-edited version.", "labels": [], "entities": [{"text": "edit distance score (HTER)", "start_pos": 107, "end_pos": 133, "type": "METRIC", "confidence": 0.924351821343104}]}, {"text": "The dataset was split into 12,000 segments as training data, 1,000 for development and 2,000 for testing.", "labels": [], "entities": []}, {"text": "The translations were produced by a single in-house MT system from which no system-dependent information was made available for the sentence-level task.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9592503905296326}]}, {"text": "These translations were post-edited by professional translators and the HTER was computed using TER(default settings: tokenised, case insensitive, exact matching only, but with scores capped to 100).", "labels": [], "entities": [{"text": "HTER", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.8811255693435669}, {"text": "TER", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9987764954566956}]}, {"text": "In addition to the dataset, we were provided with a set of resources consisting of a language model (LM), an ngram-counts list of raw ngram occurrences as well as a lexical translation table.", "labels": [], "entities": []}, {"text": "Initial experiments consisted of concatenating features with the baseline set, in order to sort out the features that had a positive impact on performance and disregard the ones that had a negative impact.", "labels": [], "entities": []}, {"text": "As per the QuEst++ framework, performance was measured in terms of Mean Average Error (MAE) and Root Mean Square Error (RMSE) which are defined in Eqs.", "labels": [], "entities": [{"text": "Mean Average Error (MAE)", "start_pos": 67, "end_pos": 91, "type": "METRIC", "confidence": 0.9802716871102651}, {"text": "Root Mean Square Error (RMSE)", "start_pos": 96, "end_pos": 125, "type": "METRIC", "confidence": 0.8754723412649972}]}, {"text": "2 and 3, where xi , ...", "labels": [], "entities": []}, {"text": ", x n are the values predicted by the SVM model and y i , ...", "labels": [], "entities": []}, {"text": ", y n are the values provided by the organisers.", "labels": [], "entities": []}, {"text": "Positive Impact A majority of the proposed features proved to have a negative impact on the performance metrics through our experiments, leaving only 5/16 features with a positive impact:: Performance in terms of MAE and RMSE for the combined features resulting in the submitted system \u2022 Noun group ratio \u2022 Kendall Tau distance \u2022 Source PCFG log probability \u2022 Target PCFG log probability \u2022 Ratio of percentage of verbs We present their individual performance when added to the baseline features in and when added in combination in.", "labels": [], "entities": [{"text": "MAE", "start_pos": 213, "end_pos": 216, "type": "METRIC", "confidence": 0.9964193105697632}, {"text": "RMSE", "start_pos": 221, "end_pos": 225, "type": "METRIC", "confidence": 0.9691181182861328}, {"text": "Target PCFG log probability", "start_pos": 360, "end_pos": 387, "type": "METRIC", "confidence": 0.5931330919265747}]}, {"text": "All these features have an individual positive impact on MAE, whereas only noun group ratio and Tau perform well on RMSE.", "labels": [], "entities": [{"text": "MAE", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.654956042766571}, {"text": "RMSE", "start_pos": 116, "end_pos": 120, "type": "TASK", "confidence": 0.6359614133834839}]}, {"text": "Furthermore, the noun group ratio and Kendall Tau Distance showed promising results both individually and in combination with our other new features.", "labels": [], "entities": [{"text": "Kendall Tau Distance", "start_pos": 38, "end_pos": 58, "type": "METRIC", "confidence": 0.663915604352951}]}, {"text": "The verb ratio feature, however, increased RMSE individually but was included in our final system despite this due to its contribution to MAE when combined, as MAE carries a heavier weight in evaluation.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9725527763366699}, {"text": "MAE", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.7309397459030151}]}, {"text": "Due to time constraints, we did not investigate the relationship between the RMSE and MAE further.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.6224551796913147}, {"text": "MAE", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.47519123554229736}]}, {"text": "The performance of the novel features in the noun translation errors and reordering measure groups in: Performance in terms of MAE and RMSE for the indiviual features describing noun translation errors and reordering reordering based on word alignments, we notice that only Tau give a positive impact.", "labels": [], "entities": [{"text": "noun translation errors", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.7947491804758707}, {"text": "MAE", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.988072395324707}, {"text": "RMSE", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9825819730758667}]}, {"text": "Our feature for genitive constructions did not give good results.", "labels": [], "entities": []}, {"text": "The surprisingly small amount of positive features maybe a result of a disagreement between the proposed features and the data.", "labels": [], "entities": []}, {"text": "The features mainly rely on linguistic analyses while the data, being exclusively from the IT-Domain, is inherently irregular.", "labels": [], "entities": []}, {"text": "POS-and syntactic phrase-features appears to be particularly unreliable which maybe due to the nature of the domain, where series of constituents of uncommon character are frequent, e.g: Choose File > Save As , and choose Photoshop DCS 1.0 or Photoshop DCS 2.0 from the Format menu .", "labels": [], "entities": [{"text": "Photoshop DCS 1.0", "start_pos": 222, "end_pos": 239, "type": "DATASET", "confidence": 0.8703922629356384}, {"text": "Photoshop DCS 2.0", "start_pos": 243, "end_pos": 260, "type": "DATASET", "confidence": 0.876464307308197}]}], "tableCaptions": [{"text": " Table 1: Performance in terms of MAE and RMSE  for the indiviual features in the Positive Impact set", "labels": [], "entities": [{"text": "MAE", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9948575496673584}, {"text": "RMSE", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9838889837265015}]}, {"text": " Table 2: Performance in terms of MAE and RMSE  for the combined features resulting in the submit- ted system", "labels": [], "entities": [{"text": "MAE", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9935030341148376}, {"text": "RMSE", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9735205173492432}]}, {"text": " Table 3: Performance in terms of MAE and RMSE  for the indiviual features describing noun transla- tion errors and reordering", "labels": [], "entities": [{"text": "MAE", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9767603278160095}, {"text": "RMSE", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.94752037525177}]}]}