{"title": [{"text": "A Two-stage Approach for Extending Event Detection to New Types via Neural Networks", "labels": [], "entities": [{"text": "Extending Event Detection", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.9175791541735331}]}], "abstractContent": [{"text": "We study the event detection problem in the new type extension setting.", "labels": [], "entities": [{"text": "event detection", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.885446697473526}]}, {"text": "In particular , our task involves identifying the event instances of a target type that is only specified by a small set of seed instances in text.", "labels": [], "entities": []}, {"text": "We want to exploit the large amount of training data available for the other event types to improve the performance of this task.", "labels": [], "entities": []}, {"text": "We compare the convolutional neu-ral network model and the feature-based method in this type extension setting to investigate their effectiveness.", "labels": [], "entities": []}, {"text": "In addition, we propose a two-stage training algorithm for neural networks that effectively transfers knowledge from the other event types to the target type.", "labels": [], "entities": []}, {"text": "The experimental results show that the proposed algorithm outper-forms strong baselines for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Event detection (ED) is an important task of information extraction that seeks to locate instances of events with some types in text.", "labels": [], "entities": [{"text": "Event detection (ED)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8390724062919617}, {"text": "information extraction", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7317901253700256}]}, {"text": "Each event mention is associated with a phrase, the event trigger 1 , which evokes that event.", "labels": [], "entities": []}, {"text": "Our task, more precisely stated, involves identifying event triggers of some types of interest.", "labels": [], "entities": []}, {"text": "For instance, in the sentence \"A cameramen was shot in Texas today\", an ED system should be able to recognize the word \"shot\" as a trigger for the event \"Attack\".", "labels": [], "entities": []}, {"text": "ED is a crucial component in the overall task of event extraction, which also involves event argument discovery.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.7982677221298218}, {"text": "event argument discovery", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.7184727390607198}]}, {"text": "There have been two major approaches to ED in the literature.", "labels": [], "entities": [{"text": "ED", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9934194684028625}]}, {"text": "The first approach extensively leverages linguistic analysis and knowledge resources to capture the discrete structures for ED, focusing on the combination of various properties 1 most often a single verb or nominalization such as lexicon, syntax, and gazetteers.", "labels": [], "entities": []}, {"text": "This is called the feature-based approach that has dominated the ED research in the last decade).", "labels": [], "entities": []}, {"text": "The second approach, on the other hand, is proposed very recently and uses convolutional neural networks (CNN) to exploit the continuous representations of words.", "labels": [], "entities": []}, {"text": "These continuous representations have been shown to effectively capture the underlying structures of a sentence, thereby significantly improving the performance for ED.", "labels": [], "entities": [{"text": "ED", "start_pos": 165, "end_pos": 167, "type": "TASK", "confidence": 0.9141695499420166}]}, {"text": "The previous research has mainly focused on building an ED system in a supervised setting.", "labels": [], "entities": []}, {"text": "The performance of such systems strongly depends on a sufficient amount of labeled instances for each event type in the training data.", "labels": [], "entities": []}, {"text": "Unfortunately, this setting does not reflect the real world situation very well.", "labels": [], "entities": []}, {"text": "In practice, we often have a large amount of training data for some old event types but are interested in extracting instances of anew event type.", "labels": [], "entities": []}, {"text": "The new event type is only specified by a small set of seed instances provided by clients (the event type extension setting).", "labels": [], "entities": []}, {"text": "How can we effectively leverage the training data of old event types to facilitate the extraction of the new event type?", "labels": [], "entities": []}, {"text": "Inspired by the work on transfer learning and domain adaptation, in this paper, we systematically evaluate the representative methods (i.e, the feature based model and the CNN model) for ED to gain an insight into which kind of method performs better in the new extension setting.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8963456153869629}, {"text": "domain adaptation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7403253614902496}]}, {"text": "In addition, we propose a two-stage algorithm to train a CNN model that effectively learns and transfers the knowledge from the old event types for the extraction of the target type.", "labels": [], "entities": []}, {"text": "The experimental results show that this two-stage algorithm significantly outperforms the traditional methods in the type extension setting for ED and demonstrates the benefit of CNN in transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 186, "end_pos": 203, "type": "TASK", "confidence": 0.9129176139831543}]}, {"text": "To our knowledge, this is the first work on the type extension setting as well as on transferring knowledge with neural networks for ED of natural language processing.", "labels": [], "entities": [{"text": "type extension setting", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.9114150603612264}]}], "datasetContent": [{"text": "Following the previous work (, we consider the ED task of the 2005 Automatic Context Extraction (ACE) evaluation that annotates 8 event types and 33 event subtypes 2 . As the numbers of event mentions (triggers) for each subtype in ACE are small, in this work, we focus on the extraction of the event types: \"Life\", \"Movement\", \"Transaction\", \"Business\", \"Conflict\", \"Contact\", \"Personell\", and \"Justice\".", "labels": [], "entities": [{"text": "Automatic Context Extraction (ACE)", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.7821990897258123}]}, {"text": "We remove the event triggers of types \"Transaction\" and \"Business\" due to their small numbers of occurrences, resulting in the dataset with six remaining event types (denoted from 1 to 6).", "labels": [], "entities": []}, {"text": "In the experiments, we use the same data split in with 40 newswire documents as a test set, 30 other documents as a development set and the 529 remaining documents as a training set.", "labels": [], "entities": []}, {"text": "Note that the training documents correspond to our original dataset D above.", "labels": [], "entities": []}, {"text": "Let P i be the positive instance set of the type i in D (i = 1 to 6).", "labels": [], "entities": []}, {"text": "We take each event type i as the target type T and treat the other 5 types as the auxiliary types, constituting 6 sets of experiments.", "labels": [], "entities": []}, {"text": "In each set of experiments fora target type i (T ), we randomly select S positive instances of T for the seed set D T (S = |D T |) and treat the remaining target instances P i \\ D T as negative.", "labels": [], "entities": []}, {"text": "Note that this essentially introduces false negatives into the training data and makes the task more challenging.", "labels": [], "entities": []}, {"text": "In order to deal with false negatives, we remove all the sentences that do not contain any events in the original dataset D.", "labels": [], "entities": []}, {"text": "In this way, we remove a large number of true negatives along with a fraction of the false negatives, leading to the reduced dataset D . We do the experiments on D with: We note that (Jiang, 2009) uses a different setting in training where she removes all the remaining target instances P i \\ D T directly.", "labels": [], "entities": []}, {"text": "In our opinion, this is unrealistic as it assumes the label of the instances in P i \\ D T while we are only provided with the label of the seed set D T in practice.", "labels": [], "entities": []}, {"text": "Finally, similar to (Jiang, 2009), we remove the positive instances of the auxiliary event types from the test set to concentrate on the classification accuracy for the target type.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.901519775390625}]}, {"text": "We also remove all the positive instances of the target type in the development set to make it more realistic.", "labels": [], "entities": []}, {"text": "This section compares the four baseline models in Section 4.1 with the proposed two-stage model CNN-2-STAGE.", "labels": [], "entities": [{"text": "CNN-2-STAGE", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.9069088101387024}]}, {"text": "For completeness, we also evaluate the transfer learning model in, adapted to the event type extension task (called JIANG).", "labels": [], "entities": []}, {"text": "For JIANG, we apply the automatic feature separation method as the general syntactic patterns and type constraints for relation in Jiang are not applicable to our ED task.", "labels": [], "entities": [{"text": "JIANG", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.5486606359481812}, {"text": "feature separation", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7084243148565292}]}, {"text": "For each described model, we perform six sets of experiments in Section 6.2, where the number of seed instances |D T | is varied from 0 to 150.", "labels": [], "entities": []}, {"text": "We then report the average F-scores of the six experiment sets for each value of S. shows the curves.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9958101511001587}]}, {"text": "Assuming the same kind of model (i.e, either FET or CNN), we see that UNION is better than TARGET when |D T | is small, confirming our hypothesis in Section 4.2.", "labels": [], "entities": [{"text": "FET", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8255420923233032}, {"text": "UNION", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.9365692138671875}, {"text": "TARGET", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9817591905593872}]}, {"text": "This demonstrates the benefit of UNION and the training data DA of the auxiliary types when there are not enough training  Witnesses said the soldiers responded by firing teargas and rubber bullets, which led to ten demonstrators being injured.", "labels": [], "entities": [{"text": "UNION", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.5320870280265808}, {"text": "DA", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9752307534217834}]}, {"text": "John Hinckley attempted to assassinate Ronald Reagan.", "labels": [], "entities": []}, {"text": "Justice Since May, Russia has jailed over 20 suspected terrorists without atrial.", "labels": [], "entities": []}, {"text": "A judicial source said today, Friday, that five Croatians were arrested last Tuesday during an operation . .", "labels": [], "entities": []}, {"text": ".: Examples for the trigger words with the latent semantic.", "labels": [], "entities": []}, {"text": "The trigger words are underlined.", "labels": [], "entities": []}, {"text": "instances for T . However, when we are provided with more seed instances for the target type (i.e, |D T | becomes larger), TARGET turns out to be significantly better than UNION.", "labels": [], "entities": [{"text": "TARGET", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9959586262702942}, {"text": "UNION", "start_pos": 172, "end_pos": 177, "type": "METRIC", "confidence": 0.8584818243980408}]}, {"text": "We also observe that CNN outperforms FET in the TARGET mechanism.", "labels": [], "entities": [{"text": "CNN", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.7644789218902588}, {"text": "FET", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9946516156196594}]}, {"text": "This is consistent with the previous studies for ED).", "labels": [], "entities": [{"text": "ED", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.532013475894928}]}, {"text": "However, in the UNION mechanism, CNN is less effective than FET, suggesting that UNION is not a good mechanism to transfer knowledge in CNN.", "labels": [], "entities": [{"text": "FET", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9336801171302795}]}, {"text": "We do not see much performance improvement of JIANG over FET-UNION.", "labels": [], "entities": [{"text": "JIANG", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9723467826843262}, {"text": "FET-UNION", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9174124002456665}]}, {"text": "This can be explained by the lack of explicit linguistic guidance (i.e, the syntactic patterns and type constraints) for the general features in the event extension task that are crucial to the success of the model in Jiang (2009).", "labels": [], "entities": [{"text": "event extension task", "start_pos": 149, "end_pos": 169, "type": "TASK", "confidence": 0.7705941597620646}]}, {"text": "Finally and most importantly, we see that the two-stage model CNN-2-STAGE outperforms all the compared models regardless of |D T |.", "labels": [], "entities": []}, {"text": "This is significant when |D T | is greater than 50.", "labels": [], "entities": [{"text": "D T |", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9715773661931356}]}, {"text": "These results suggest the effectiveness of the two-stage training algorithm on transferring knowledge from the auxiliary types to the target type for CNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 150, "end_pos": 153, "type": "TASK", "confidence": 0.9164505004882812}]}], "tableCaptions": []}