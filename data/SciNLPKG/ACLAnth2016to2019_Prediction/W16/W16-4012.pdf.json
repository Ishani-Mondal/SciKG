{"title": [{"text": "Challenges and Solutions for Latin Named Entity Recognition", "labels": [], "entities": [{"text": "Latin Named Entity Recognition", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.730827659368515}]}], "abstractContent": [{"text": "Although spanning thousands of years and genres as diverse as liturgy, historiography, lyric and other forms of prose and poetry, the body of Latin texts is still relatively sparse compared to English.", "labels": [], "entities": []}, {"text": "Data sparsity in Latin presents a number of challenges for traditional Named Entity Recognition techniques.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.7561026215553284}]}, {"text": "Solving such challenges and enabling reliable Named Entity Recognition in Latin texts can facilitate many downstream applications, from machine translation to digital historiography, enabling Classicists, historians, and archaeologists for instance, to track the relationships of historical persons, places, and groups on a large scale.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.6951672236124674}, {"text": "machine translation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.734821692109108}, {"text": "track the relationships of historical persons, places, and groups", "start_pos": 253, "end_pos": 318, "type": "TASK", "confidence": 0.708826867016879}]}, {"text": "This paper presents the first annotated corpus for evaluating Named Entity Recognition in Latin, as well as a fully supervised model that achieves over 90% F-score on a held-out test set, significantly outperforming a competitive baseline.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.7332276304562887}, {"text": "F-score", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.9989727735519409}]}, {"text": "We also present a novel active learning strategy that predicts how many and which sentences need to be annotated for named entities in order to attain a specified degree of accuracy when recognizing named entities automatically in a given text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9936155080795288}]}, {"text": "This maximizes the productivity of annotators while simultaneously controlling quality.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We set out to determine if (a) our sentence selection algorithm is efficient and (b) we can reliably predict tagging accuracy based on how many sentences we have already selected and annotated.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7221052348613739}, {"text": "tagging", "start_pos": 109, "end_pos": 116, "type": "TASK", "confidence": 0.9354133009910583}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.8661152124404907}]}, {"text": "Such are the practical concerns of e.g. Classicists studying the portrayal of GRP's in the liturgical Index Thomisticus.", "labels": [], "entities": [{"text": "GRP's in the liturgical Index Thomisticus", "start_pos": 78, "end_pos": 119, "type": "DATASET", "confidence": 0.8809837102890015}]}, {"text": "Accuracy on a held-out set does not concern them, only how many and which sentences must be annotated within Index Thomisticus to ensure that the remainder can be tagged with sufficient accuracy to meet their projects' needs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9658229351043701}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9766842126846313}]}, {"text": "Thus, we return to our 3 domain-disjoint folds over which we tested the fully supervised model, pretending that the test sets represent never-before-seen documents of varying tagging difficulties.", "labels": [], "entities": []}, {"text": "For each fold, we run our sentence selection algorithm on the test set, incrementally updating the training set with selected sentences and testing on those remaining.", "labels": [], "entities": []}, {"text": "depicts the results from running the tagger on each fold until the learning curve levels off.", "labels": [], "entities": []}, {"text": "The accuracy on BG levels off at the top of the probability space once all capitalized UNK's have been seen.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999725878238678}, {"text": "BG", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.8523554801940918}]}, {"text": "AA leveling off in the low 90's is an effect of the inherent challenge of tagging a text in which the same NE is frequently used to refer to different classes, merely reflecting the lower inter-annotator agreement in this test set.", "labels": [], "entities": [{"text": "AA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8714465498924255}]}, {"text": "The flaw with the sentence selection algorithm is that Remaining UNK's never) are homonymous with non-NE's, representing a third priority to be incorporated in the updated algorithm.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.79804527759552}]}, {"text": "However, for now, we can ignore the effect of homonymy by only considering results in Ep from 0 to 0.5 additional normalized sentences, the other texts being free of such effects.", "labels": [], "entities": []}], "tableCaptions": []}