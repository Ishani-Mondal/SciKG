{"title": [{"text": "Attention-Based Convolutional Neural Network for Machine Comprehension", "labels": [], "entities": []}], "abstractContent": [{"text": "Understanding open-domain text is one of the primary challenges in NLP.", "labels": [], "entities": []}, {"text": "Machine comprehension benchmarks evaluate a system's ability to understand text based on the text content only.", "labels": [], "entities": []}, {"text": "In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.8198925256729126}]}, {"text": "Prior work is mainly based on feature engineering approaches.", "labels": [], "entities": []}, {"text": "We come up with a neu-ral network framework, named hierarchical attention-based convolutional neural network (HABCNN), to address this task without any manually designed features.", "labels": [], "entities": []}, {"text": "Specifically, we explore HABCNN for this task by two routes, one is through traditional joint mod-eling of document, question and answer, one is through textual entailment.", "labels": [], "entities": []}, {"text": "HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question.", "labels": [], "entities": [{"text": "HABCNN", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.792790412902832}]}, {"text": "Experiments show that HABCNN outperforms prior deep learning approaches by a big margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine comprehension is an open-domain question-answering problem which contains factoid questions, but the answers can be derived by extraction or induction of key clues.", "labels": [], "entities": []}, {"text": "shows one example in MCTest (.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.9435447454452515}]}, {"text": "Each example consists of one document, four associated questions; each question is followed by four answer candidates of which only one is correct.", "labels": [], "entities": []}, {"text": "Questions in MCTest have two categories; \"one\" questions can be answered based on a single sentence from document where \"multiple\" questions require several sentences.", "labels": [], "entities": []}, {"text": "To correctly answer the first question in the example, the two blue sentences are required; for the second question instead, we only need the red sentence.", "labels": [], "entities": []}, {"text": "The following observations hold for the whole MCTest.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.9153795838356018}]}, {"text": "(i) Most of the sentences in the document are irrelevant fora given question.", "labels": [], "entities": []}, {"text": "It hints that we need to pay attention to just some key regions.", "labels": [], "entities": []}, {"text": "(ii) Answer candidates vary in length and abstraction level and usually do not appear in the document.", "labels": [], "entities": []}, {"text": "For example, candidate B for the second question is \"outside\", which is one word and does not exist in the document, while the answer candidates for the first question are longer texts with some auxiliary words like \"Because\" in the text.", "labels": [], "entities": []}, {"text": "This requires our system to handle flexible texts via extraction as well as abstraction.", "labels": [], "entities": []}, {"text": "(iii) Some questions require multiple sentences to infer the answer, and those vital sentences mostly appear close to each other (we call them snippet).", "labels": [], "entities": []}, {"text": "Hence, our system should be able to make a choice or compromise between potential single-sentence clue and snippet clue.", "labels": [], "entities": []}, {"text": "Prior work is mostly based on feature engineering.", "labels": [], "entities": []}, {"text": "We take the lead in presenting a deep neural network without linguistic feature engineering.", "labels": [], "entities": []}, {"text": "Concretely, we propose HABCNN, a hierarchical attention-based convolutional neural network, to address this task in two roadmaps.", "labels": [], "entities": []}, {"text": "In the first one, we project the document in two different ways, one based on question-attention, one based on answerattention and then compare the two projected document representations to determine whether the answer matches the question.", "labels": [], "entities": []}, {"text": "In the second one, every question-answer pair is reformatted into a statement, then the whole task reduces to textual entailment.", "labels": [], "entities": []}, {"text": "In both roadmaps, convolutional neural network (CNN) is explored to model all types of text.", "labels": [], "entities": []}, {"text": "As human beings usually do for such a QA task, our model is expected to be able to detect the key snippets, key sentences, and key words or phrases in the document.", "labels": [], "entities": [{"text": "QA task", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.8876786828041077}]}, {"text": "In order to detect those informative parts required by questions, we explore an attention mechanism to model the document so that in its representation the required information is emphasized.", "labels": [], "entities": []}, {"text": "In practice, instead of imitating human beings in QA task top-down, our system models the document bottom-up, through accumulating the most relevant information from word level to snippet level.", "labels": [], "entities": []}, {"text": "Our approach is novel in three aspects.", "labels": [], "entities": []}, {"text": "(i) A document is modeled by a hierarchical CNN for different granularity, from word to sentence level, then from sentence to snippet level.", "labels": [], "entities": []}, {"text": "(ii) In the example in, apparently not all sentences are required given a question, and usually different snippets are required by different questions.", "labels": [], "entities": []}, {"text": "Hence, the same document should have different representations based on what the question is.", "labels": [], "entities": []}, {"text": "To this end, attention is incorporated into the hierarchical CNN to guide the learning of dynamic document representations which closely match the information requirements by questions.", "labels": [], "entities": []}, {"text": "(iii) Document representations at sentence and snippet levels both are informative for the question.", "labels": [], "entities": []}, {"text": "Therefore a highway network is developed to combine them, enabling our system to make a flexible tradeoff.", "labels": [], "entities": []}, {"text": "Overall, we make three contributions.", "labels": [], "entities": []}, {"text": "(i) We present a hierarchical attention-based CNN system \"HABCNN\".", "labels": [], "entities": [{"text": "HABCNN", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.6380587220191956}]}, {"text": "It is, to our knowledge, the first deep learning (DL) based system for this MCTest task.", "labels": [], "entities": [{"text": "MCTest task", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.8471200168132782}]}], "datasetContent": [{"text": "MCTest 1 has two subsets.", "labels": [], "entities": [{"text": "MCTest 1", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9132955372333527}]}, {"text": "MCTest-160 contains 160 items (70 train, 30 dev, 60 test), each consisting of a document, four questions followed by one correct anwer and three incorrect answers and MCTest-500 500 items (300 train, 50 dev, 150 test).", "labels": [], "entities": [{"text": "MCTest-160", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9466362595558167}, {"text": "MCTest-500", "start_pos": 167, "end_pos": 177, "type": "DATASET", "confidence": 0.8229219317436218}]}], "tableCaptions": [{"text": " Table 2: Experimental results for one-sentence (one), multiple-sentence (mul) and all cases.", "labels": [], "entities": []}]}