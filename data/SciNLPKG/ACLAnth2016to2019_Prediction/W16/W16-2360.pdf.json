{"title": [], "abstractContent": [{"text": "We present a novel neural machine translation (NMT) architecture associating visual and textual features for translation tasks with multiple modalities.", "labels": [], "entities": []}, {"text": "Transformed global and regional visual features are concatenated with text to form attend-able sequences which are dissipated over parallel long short-term memory (LSTM) threads to assist the encoder generating a representation for attention-based decoding.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed NMT outperform the text-only baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "In fields of machine translation, neural network attracts lots of research attention recently that the encoder-decoder framework is widely used.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7717804312705994}]}, {"text": "Nevertheless, the main drawback of this neural machine translation (NMT) framework is that the decoder only depends on the last state of the encoder, which may deteriorate the performance when the sentence is long.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.8548514048258463}]}, {"text": "To overcome this problem, attention based encoder-decoder framework as shown in is proposed.", "labels": [], "entities": []}, {"text": "With the attention model, in each time step the decoder depends on both the previous LSTM hidden state and the context vector, which is the weighted sum of the hidden states in the encoder.", "labels": [], "entities": []}, {"text": "With attention, the decoder can \"refresh\" it's memory to focus on source words that may help to translate the correct words rather than only seeing the last state of the sentences where the words in the sentence and the ordering of words are missing.", "labels": [], "entities": []}, {"text": "Most of the machine translation task only focus textual sentences of the source language and target language; however, in the real world, the sentences may contain information of what people see.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7824423611164093}]}, {"text": "Beyond the bilingual translation, in WMT 16' multimodal translation task, we would like to translate the image captions in English into German.", "labels": [], "entities": [{"text": "bilingual translation", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6422462165355682}, {"text": "WMT 16", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.6858040690422058}, {"text": "multimodal translation", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.64438197016716}]}, {"text": "With the additional information from images, we would further resolve the problem of ambiguity in languages.", "labels": [], "entities": []}, {"text": "For example, the word \"bank\" may refer to the financial institution or the land of the river's edge.", "labels": [], "entities": []}, {"text": "It would be confusing if we only look at the language itself.", "labels": [], "entities": []}, {"text": "In this task, the image may help to disambiguate the meaning if it shows that there is a river and thus the \"bank\" means \"river bank\".", "labels": [], "entities": []}, {"text": "In this paper, we explore approaches to integrating multimodal information (text and image) into the attention-based encoder-decoder architecture.", "labels": [], "entities": []}, {"text": "We transform and make the visual features as one of the steps in the encoder as text, and then make it possible to attend to both the text and the image while decoding.", "labels": [], "entities": []}, {"text": "The image features we used are (visual) semantic features extracted from the entire images (global) as well as the regional bounding boxes proposed by the region-based convolutional neural networks (R-CNN) ().", "labels": [], "entities": []}, {"text": "In the following section, we first describe the related works, and then we introduce the proposed multimodal attention-based NMT in Section 3, followed by re-scoring of the translation candidates in Section 4.", "labels": [], "entities": []}, {"text": "Finally we demonstrate the experiments in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the official WMT 2016 multimodal translation task dataset, there are 29,000 parallel sentences from English to German for training, 1014 for validation and 1000 for testing.", "labels": [], "entities": [{"text": "WMT 2016 multimodal translation task dataset", "start_pos": 16, "end_pos": 60, "type": "DATASET", "confidence": 0.757688045501709}]}, {"text": "Each sentence describes an image from Flickr30k dataset ().", "labels": [], "entities": [{"text": "Flickr30k dataset", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.9846009016036987}]}, {"text": "We preprocessed all the descriptions into lowercase with tokenization and German compound word splitting.", "labels": [], "entities": [{"text": "German compound word splitting", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.5112325474619865}]}, {"text": "Global visual features (fc7) are extracted with VGG-19 ().", "labels": [], "entities": [{"text": "VGG-19", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.9566415548324585}]}, {"text": "For regional visual features, the region proposal network in RCNN () first recognizes bounding boxes of objects in an image and then we computed 4096-dimensional fc7 features from these regions with VGG-19.", "labels": [], "entities": [{"text": "VGG-19", "start_pos": 199, "end_pos": 205, "type": "DATASET", "confidence": 0.979431688785553}]}, {"text": "The RPN of RCNN is pre-trained on ImageNet dataset 2 and then fine-tuned on MSCOCO dataset 3 with 80 ob-) with similar settings: (a) we uniformly initialized all parameters between -0.1 and 0.1, (b) we trained the LSTM for 20 epochs using simple SGD, (c) the learning rate was initialized as 1.0, multiplied by 0.7 after 12 epochs, (d) dropout rate was 0.8.", "labels": [], "entities": [{"text": "ImageNet dataset", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9297046661376953}, {"text": "MSCOCO dataset", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.938263475894928}, {"text": "dropout rate", "start_pos": 336, "end_pos": 348, "type": "METRIC", "confidence": 0.9442866444587708}]}, {"text": "Note that the same dropout mask and NMT parameters are shared by all LSTM threads in model 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of re-scoring using monolin- gual LSTM, Bi-lingual auto-encoder, and dictio- nary based on multimodal NMT results.", "labels": [], "entities": []}]}