{"title": [{"text": "Evaluating Inter-Annotator Agreement on Historical Spelling Normalization", "labels": [], "entities": [{"text": "Historical Spelling Normalization", "start_pos": 40, "end_pos": 73, "type": "TASK", "confidence": 0.8026420076688131}]}], "abstractContent": [{"text": "This paper deals with means of evaluating inter-annotator agreement fora nor-malization task.", "labels": [], "entities": []}, {"text": "This task differs from common annotation tasks in two important aspects: (i) the class of labels (the normalized wordforms) is open, and (ii) annotations can match to different degrees.", "labels": [], "entities": []}, {"text": "We propose anew method to measure inter-annotator agreement for the normal-ization task.", "labels": [], "entities": []}, {"text": "It integrates common chance-corrected agreement measures, such as Fleiss's \u03ba or Krippendorff's \u03b1.", "labels": [], "entities": []}, {"text": "The novelty of our proposed method lies in the way the annotated word forms are treated.", "labels": [], "entities": []}, {"text": "First, they are evaluated character-wise; second, certain characters are mapped to more general categories.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, and in particular in the context of digital humanities, historical language data has been gaining increasing significance.", "labels": [], "entities": []}, {"text": "The focus is on providing easy access to the information contained in the data.", "labels": [], "entities": []}, {"text": "To this end, historical texts are digitized and processed by OCR or even transcribed manually.", "labels": [], "entities": []}, {"text": "Due to the absence of standards, historical data often exhibits large variance, especially with regard to spelling.", "labels": [], "entities": []}, {"text": "Hence, further processing either has to rely on fuzzy-matching strategies, or on standardization of the data.", "labels": [], "entities": []}, {"text": "In the Anselm project), we opted for the second way.", "labels": [], "entities": [{"text": "Anselm project", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.7688294649124146}]}, {"text": "We provide normalized wordforms for the full corpus that have been manually annotated according to guidelines specifically created for this task.", "labels": [], "entities": []}, {"text": "These normalizations can be useful for search queries, further downstream applications such as POS tagging, or as training data for automatic normalization methods.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.7320250272750854}]}, {"text": "This paper deals with means of quantitative evaluation of these normalization guidelines.", "labels": [], "entities": []}, {"text": "We would like to quantify the degree of consistency that can be achieved with annotations according to the guidelines, i.e., the inter-annotator agreement (IAA).", "labels": [], "entities": [{"text": "inter-annotator agreement (IAA)", "start_pos": 129, "end_pos": 160, "type": "METRIC", "confidence": 0.6783746600151062}]}, {"text": "While a range of measures has been proposed for measuring agreement (e.g., seethe survey by), our task differs from common annotation tasks, such as part-of-speech tagging or semantic role labeling, in two important aspects: (i) the class of labels (the normalized wordforms) is open, and label distribution is sparse; and (ii) annotations are biased to be similar to the surface form of the token they belong to, and can match to different degrees.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.735673189163208}, {"text": "semantic role labeling", "start_pos": 175, "end_pos": 197, "type": "TASK", "confidence": 0.640812357266744}]}, {"text": "For example, we would like to score almost identical annotations like n\u00e4hme -nehme 'take' (for the historical form neme) higher than annotations that are rather dissimilar, like dr\u00fcckte 'pressed' -trocknete 'dried' (for trvckente).", "labels": [], "entities": []}, {"text": "We investigate why conventional IAA measures are not suitable to the normalization task, and propose anew method that integrates common chance-corrected agreement measures, such as Fleiss's \u03ba or.", "labels": [], "entities": [{"text": "IAA", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8819338083267212}, {"text": "normalization task", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.9263888001441956}]}, {"text": "The novelty of our proposed method lies in the way the annotated wordforms are treated.", "labels": [], "entities": []}, {"text": "First, we reframe normalization as a character-based task; and second, we model the inherent properties of normalization by mapping certain characters to more general categories.", "labels": [], "entities": []}, {"text": "We first present the annotation guidelines (Sec. 2) and the dataset that our evaluation is based on (Sec. 3).", "labels": [], "entities": []}, {"text": "4 discusses the problems that arise from applying common agreement measures to the normalization task.", "labels": [], "entities": [{"text": "normalization task", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.9267579019069672}]}, {"text": "5 introduces our new method, followed by an evaluation in Sec.", "labels": [], "entities": []}, {"text": "6, comparing and assessing the results of different ways of measuring agreement.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first compare agreement scores of the naive word-based evaluation with those obtained using the character-based representation of the task.", "labels": [], "entities": []}, {"text": "For both scenarios, we calculate average percentage agreement (agr % ) and Krippendorff's \u03b1 using the N LD distance function defined in Sec.", "labels": [], "entities": [{"text": "average percentage agreement", "start_pos": 33, "end_pos": 61, "type": "METRIC", "confidence": 0.8815410137176514}, {"text": "agr", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.7697480916976929}]}, {"text": "4. We find that values for \u03c0 and \u03ba, either naively averaged overall annotator pairs or using the generalization of \u03c0 * , almost always differ only after the fifth or sixth decimal place; we therefore restrict ourselves to reporting \u03c0 * . We evaluate separately on all tokens (ALL), tokens whereat least one annotator made a modification to the historical token (MEDIUM), and tokens where all five annotators made a modification (STRICT).", "labels": [], "entities": [{"text": "STRICT", "start_pos": 429, "end_pos": 435, "type": "METRIC", "confidence": 0.9898389577865601}]}, {"text": "shows the agreement scores for this evaluation.", "labels": [], "entities": [{"text": "agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.980549156665802}]}, {"text": "The average word-based agreement overall tokens is 92.62%, and \u03c0 * values for the word-based task are always similar to the percentage agreement.", "labels": [], "entities": []}, {"text": "Values for \u03b1 N LD are naturally higher, since it also considers partial agreement within the normalizations.", "labels": [], "entities": []}, {"text": "For the character-based task, percentage agreement is always much higher, but \u03c0 * values are now noticeably lower compared to the percentage values.", "labels": [], "entities": [{"text": "percentage agreement", "start_pos": 30, "end_pos": 50, "type": "METRIC", "confidence": 0.6922013461589813}]}, {"text": "This is a consequence of the character-based reframing of the task being much more sensitive to agreement on the actual modifications (cf. Sec. 5.2).", "labels": [], "entities": []}, {"text": "Comparing the different evaluation sets, percentage agreement on the STRICT set is noticeably higher than on the MEDIUM set.", "labels": [], "entities": [{"text": "agreement", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.7086144089698792}, {"text": "STRICT", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.8749613761901855}, {"text": "MEDIUM set", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.7654851973056793}]}, {"text": "This is particularly remarkable since the MEDIUM set only has 185 tokens more.", "labels": [], "entities": [{"text": "MEDIUM set", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.7968543171882629}]}, {"text": "Therefore, cases where annotators disagree whether a change to the historical wordform is even needed appear to be particularly problematic.", "labels": [], "entities": []}, {"text": "On the other hand, if all annotators agree that a change needs to be made, they seem to reliably produce similar normalizations.", "labels": [], "entities": []}, {"text": "Our evaluation dataset consists of passages from four different texts that exhibit different spelling characteristics (cf. Sec. 3).", "labels": [], "entities": []}, {"text": "Since it is conceivable that this affects the difficulty of the normalization task, we also choose to evaluate on each text excerpt separately.", "labels": [], "entities": [{"text": "normalization task", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9026521146297455}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Generally, there are only minor differences between the texts: for the word-based evaluation, N4 consistently shows the highest agreement, while ST2 usually has the lowest values (except for \u03b1 N LD , where M1 ranks worse).", "labels": [], "entities": [{"text": "agreement", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9895907044410706}]}, {"text": "The same is true for agr % on the character-based task.", "labels": [], "entities": []}, {"text": "However, the agreement coefficients for the character-based task show very different trends: here, M1 gets the highest scores, while the values for HK1 are lowest by a noticeably margin.", "labels": [], "entities": [{"text": "M1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9385334253311157}, {"text": "HK1", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.72763991355896}]}, {"text": "This evaluation shows that our character-based evaluation is also useful for providing a different: Inter-annotator agreement on type of modernization; ALL = all tokens, MEDIUM = at least one annotator chose a modernization category (INFL/SEM/EXT), STRICT = all annotators chose a modernization category.", "labels": [], "entities": [{"text": "ALL", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.9539064168930054}, {"text": "MEDIUM", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9868481755256653}, {"text": "STRICT", "start_pos": 249, "end_pos": 255, "type": "METRIC", "confidence": 0.9385023713111877}]}, {"text": "perspective on the annotated data than word-based agreement.", "labels": [], "entities": []}, {"text": "Due to the nature of the modernization layer, a character-based evaluation of the wordforms is problematic, since modernized forms usually do not need to bear any resemblance to the historical token.", "labels": [], "entities": []}, {"text": "An exception are modernized forms that have been assigned due to inflectional changes (INFL), which we would assume to be similar to the respective historical and normalized forms.", "labels": [], "entities": [{"text": "INFL)", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9182066321372986}]}, {"text": "To test this assumption, we evaluate character-: Inter-annotator agreement on modernization, using character-based evaluation, separately for tokens where all annotators agree on the type of modernization.", "labels": [], "entities": []}, {"text": "based agreement on the modernization layer for tokens where all annotators agree on a modernization type).", "labels": [], "entities": []}, {"text": "For ORIG and NORM, we assume the modernized wordform to be identical to the normalization.", "labels": [], "entities": [{"text": "ORIG", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.7145829200744629}, {"text": "NORM", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8179382681846619}]}, {"text": "The results confirm our expectations: \u03c0 * on INFL is 0.9559, while it drops considerably for SEM and EXT; however, the significance of these results might be limited due to the low sample size for these cases.", "labels": [], "entities": [{"text": "\u03c0", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.9809572696685791}, {"text": "INFL", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.5018054246902466}]}, {"text": "Another notable result is the extremely high agreement (\u03c0 * = 0.9870) for tokens where all annotators agree on type NORM.", "labels": [], "entities": [{"text": "agreement", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9919310212135315}, {"text": "NORM", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.735248863697052}]}, {"text": "This tells us that most of the disagreements from the normalization evaluation (cf.) stem from cases whereat least one annotator decided that a modernization was necessary; these tokens therefore appear to be more difficult to agree on not only on the modernization layer, but already on the normalization layer.", "labels": [], "entities": [{"text": "normalization evaluation", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.8909119069576263}]}, {"text": "While it is plausible that extinct wordforms, as well as words with different meaning or inflection than in modern language, are inherently more difficult to annotate, the intention of the guidelines was to move this difficulty to the modernization layer, while having unambiguous rules for the annotation of the normalization layer.", "labels": [], "entities": []}, {"text": "These results show that while we achieve a good reliability overall, the guidelines were notable to remove this difficulty completely for these cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The texts of the four annotated fragments, with information about their provenance and frequen- cies (%) of normalization and modernization types.", "labels": [], "entities": [{"text": "frequen- cies", "start_pos": 97, "end_pos": 110, "type": "METRIC", "confidence": 0.9503249526023865}]}, {"text": " Table 5: Inter-annotator agreement on normalization across five annotators; ALL = all tokens, MEDIUM =  at least one annotator made a change to the original token, STRICT = all annotators made a change to the  original token.", "labels": [], "entities": [{"text": "ALL", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9760622382164001}, {"text": "MEDIUM", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.993547260761261}, {"text": "STRICT", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9755688309669495}]}, {"text": " Table 6: Inter-annotator agreement on normalization, separately for each text; highest score for each  measure shown in bold, lowest score shown in italics.", "labels": [], "entities": [{"text": "normalization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9579727053642273}]}, {"text": " Table 7: Inter-annotator agreement on type of  modernization; ALL = all tokens, MEDIUM = at  least one annotator chose a modernization cate- gory (INFL/SEM/EXT), STRICT = all annotators  chose a modernization category.", "labels": [], "entities": [{"text": "ALL", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9665614366531372}, {"text": "MEDIUM", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9934660792350769}, {"text": "STRICT", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.9159749746322632}]}, {"text": " Table 8: Confusion matrix of annotator judgments  between modernization types, averaged across all  annotator pairs", "labels": [], "entities": []}, {"text": " Table 9: Inter-annotator agreement on moderniza- tion, using character-based evaluation, separately  for tokens where all annotators agree on the type  of modernization.", "labels": [], "entities": []}]}