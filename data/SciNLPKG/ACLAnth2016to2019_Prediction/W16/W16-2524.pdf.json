{"title": [{"text": "Probing for semantic evidence of composition by means of simple classification tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a diagnostic method for probing specific information captured in vector representations of sentence meaning, via simple classification tasks with strategically constructed sentence sets.", "labels": [], "entities": [{"text": "probing specific information captured in vector representations of sentence meaning", "start_pos": 35, "end_pos": 118, "type": "TASK", "confidence": 0.7981775939464569}]}, {"text": "We identify some key types of semantic information that we might expect to be captured in sentence composition, and illustrate example classification tasks for targeting this information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence-level meaning representations, when formed from word-level representations, require a process of composition.", "labels": [], "entities": [{"text": "Sentence-level meaning representations", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7970079183578491}]}, {"text": "Central to evaluation of sentence-level vector representations, then, is evaluating how effectively a model has executed this composition process.", "labels": [], "entities": []}, {"text": "In assessing composition, we must first answer the question of what it means to do composition well.", "labels": [], "entities": []}, {"text": "On one hand, we might define effective composition as production of sentence representations that allow for high performance on a task of interest (.", "labels": [], "entities": []}, {"text": "A limitation of such an approach is that it is likely to produce overfitting to the characteristics of the particular task.", "labels": [], "entities": []}, {"text": "Alternatively, we might define effective composition as generation of a meaning representation that makes available all of the information that we would expect to be extractable from the meaning of the input sentence.", "labels": [], "entities": []}, {"text": "For instance, in a representation of the sentence \"The dog didn't bark, but chased the cat\", we would expect to be able to extract the information that there is an event of chasing, that a dog is doing the chasing and a cat is being chased, and that there is no barking event (though there is a semantic relation between dog and bark, albeit modified by negation, which we likely want to be able to extract as well).", "labels": [], "entities": []}, {"text": "A model able to produce meaning representations that allow for extraction of these kinds of key semantic characteristics-semantic roles, event information, operator scope, etc-should be much more generalizable across applications, rather than targeting any single application at the cost of others.", "labels": [], "entities": []}, {"text": "With this in mind, we propose here a linguistically-motivated but computationally straightforward diagnostic method, intended to provide a targeted means of assessing the specific semantic information that is being captured in sentence representations.", "labels": [], "entities": []}, {"text": "We propose to accomplish this by constructing sentence datasets controlled and annotated as precisely as possible for their linguistic characteristics, and directly testing for extractability of semantic information by testing classification accuracy in tasks defined by the corresponding linguistic characteristics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 242, "end_pos": 250, "type": "METRIC", "confidence": 0.94964998960495}]}, {"text": "We present the results of preliminary experiments as proof-of-concept.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal in constructing the sentence dataset is to capture a wide variety of syntactic structures and configurations, so as to reflect as accurately as possible the diversity of sentences that systems will need to handle in naturally-occurring text-while maintaining access to detailed labeling of as many relevant linguistic components of our data as possible.", "labels": [], "entities": []}, {"text": "Ideally, we want a dataset with enough variation and annotation to allow us to draw data for all of our desired classification tasks from this single dataset.", "labels": [], "entities": []}, {"text": "For our illustrations here, we restrict our structural variation to that available from active/passive alternations, use of relative clauses at various syntactic locations, and use of negation at various syntactic locations.", "labels": [], "entities": []}, {"text": "This allows us to demonstrate decent structural variety without distracting from illustration of the semantic characteristics of interest.", "labels": [], "entities": []}, {"text": "Many more components can be added to increase complexity and variation, and to make sentences better reflect natural text.", "labels": [], "entities": []}, {"text": "More detailed discussion of considerations for construction of the actual dataset is given in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 94, "end_pos": 103, "type": "DATASET", "confidence": 0.9011954963207245}]}, {"text": "As proof-of-concept, we have conducted some preliminary experiments to test that this method yields results patterning in the expected direction on tasks for which we have clear predictions about whether a type of information could be captured.", "labels": [], "entities": []}, {"text": "We compared three sentence embedding methods: 1) Averaging GloVe vectors (), 2) Paraphrastic word averaging embeddings (Paragram) trained with a compositional objective (. and 3) Skip-Thought embeddings (ST) ().", "labels": [], "entities": [{"text": "Averaging GloVe", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7966113984584808}, {"text": "Paraphrastic word averaging embeddings", "start_pos": 80, "end_pos": 118, "type": "TASK", "confidence": 0.6925932392477989}]}, {"text": "1 For each task, we used a logistic regression classifier with train/test sizes of 1000/500.", "labels": [], "entities": []}, {"text": "The classification accuracies are summarized in.", "labels": [], "entities": []}, {"text": "We used three classification tasks for preliminary testing.", "labels": [], "entities": []}, {"text": "First, before testing actual indicators of composition, as a sanity check we tested whether classifiers could be trained to recognize the simple presence of a given lexical item, specifically school.", "labels": [], "entities": []}, {"text": "As expected, we see that all three models are able to perform this task with 100% accuracy, suggesting that this information is wellcaptured and easily accessible.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9992592930793762}]}, {"text": "As an extension of this sanity check, we also trained classifiers to We used the pretrained models provided by the authors.", "labels": [], "entities": []}, {"text": "GloVe and Paragram embeddings are of size 300 while SkipThought embeddings are of size 2400.", "labels": [], "entities": []}, {"text": "We tuned each classifier with 5-fold cross validation.", "labels": [], "entities": []}, {"text": "recognize sentences containing a token in the category of \"human\".", "labels": [], "entities": []}, {"text": "To test for generalization across the category, we ensured that no human nouns appearing in the test set were included in training sentences.", "labels": [], "entities": []}, {"text": "All three models reach a high classification performance on this task, though Paragram lags behind slightly.", "labels": [], "entities": []}, {"text": "Finally, we did a preliminary experiment pertaining to an actual indicator of composition: semantic role.", "labels": [], "entities": []}, {"text": "We constructed a simple dataset with structural variation stemming only from active/passive alternation, and tested whether models could differentiate sentences with school appearing in an agent role from sentences with school appearing as a patient.", "labels": [], "entities": []}, {"text": "All training and test sentences contained the lexical item \"school\", with both active and passive sentences selected randomly from the full dataset for inclusion in training and test sets.", "labels": [], "entities": []}, {"text": "Note that with sentences of this level of simplicity, models can plausibly use fairly simple order heuristics to solve the classification task, so a model that retains order information (in this case, only ST) should have a good chance of performing well.", "labels": [], "entities": []}, {"text": "Indeed, we see that ST reaches a high level of performance, while the two averaging-based models never exceed chancelevel performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Sentence labeling for two classification tasks: \"contains professor as AGENT of recommend\"  (column 2), and \"sentence meaning involves professor performing act of recommending\" (column 3).", "labels": [], "entities": [{"text": "Sentence labeling", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9502456188201904}, {"text": "AGENT", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9932152032852173}]}, {"text": " Table 3: Percentage correct on has-school, has- human, and has-school-as-agent tasks.", "labels": [], "entities": []}]}