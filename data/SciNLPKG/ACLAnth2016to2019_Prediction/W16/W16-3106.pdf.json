{"title": [], "abstractContent": [{"text": "This paper describes a question-answering system that returns relevant documents and snippets (with particular emphasis on snippets) from a large medical document collection.", "labels": [], "entities": []}, {"text": "The system is implemented as part of our participation to Phase A of Task 4b in the 2016 BioASQ Challenge.", "labels": [], "entities": []}, {"text": "The proposed system retrieves candidate answer sentences using a cluster-based language model.", "labels": [], "entities": []}, {"text": "Then, it re-ranks the retrieved top-n sentences using five independent similarity models based on shallow semantic analysis.", "labels": [], "entities": []}, {"text": "The experimental results show that the proposed system is the first to find snippets in batches 2 (MAP 0.0604), 3 (MAP 0.0728), 4 (MAP 0.1182), and 5 (MAP 0.1187).", "labels": [], "entities": []}], "introductionContent": [{"text": "BioASQ 2016 is the fourth annual BioASQ challenge as an established international competition for large-scale biomedical semantic indexing and question-answering, since).", "labels": [], "entities": [{"text": "BioASQ 2016", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8790986835956573}, {"text": "biomedical semantic indexing", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.6687139173348745}]}, {"text": "The challenge consists of two tasks: Task 4a on large-scale online biomedical semantic indexing and Task 4b on biomedical semantic question-answering.", "labels": [], "entities": [{"text": "large-scale online biomedical semantic indexing", "start_pos": 48, "end_pos": 95, "type": "TASK", "confidence": 0.6191961944103241}]}, {"text": "Task 4b is further divided into two phases: Phase A and Phase B. In Phase A, participating systems are required to return a maximum of 10 relevant concepts, documents, snippets, and triples during five batches.", "labels": [], "entities": []}, {"text": "Participation in Phase A can be partial, which means that it is acceptable to participate in only some of the batches and to return only relevant documents without snippets, triples, and concepts.", "labels": [], "entities": []}, {"text": "This paper * Corresponding author.", "labels": [], "entities": []}, {"text": "describes a questionanswering system of Kangwon National University and Sogang University submitted for Phase A of Task 4b in BioASQ 2016.", "labels": [], "entities": [{"text": "Kangwon National University", "start_pos": 40, "end_pos": 67, "type": "DATASET", "confidence": 0.9764270385106405}, {"text": "Sogang University", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.8770483434200287}]}, {"text": "The proposed system is focused on returning relevant documents and snippets (with particular emphasis on snippets).", "labels": [], "entities": []}, {"text": "2 Question-answering system based on sentence retrieval and re-ranking techniques KSAnswer consists of two submodules: A retrieval model for finding candidate answer sentences from a large medical collection and a reranking model for determining the final answer among the retrieved candidate answer sentences.", "labels": [], "entities": [{"text": "sentence retrieval", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7081138342618942}, {"text": "KSAnswer", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.7726730704307556}]}], "datasetContent": [{"text": "We indexed the full data set of PubMed journals using Lucene 4.0.0.", "labels": [], "entities": [{"text": "data set of PubMed journals", "start_pos": 20, "end_pos": 47, "type": "DATASET", "confidence": 0.6732540607452393}, {"text": "Lucene 4.0.0", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.8922590017318726}]}, {"text": "The number of document units was 12,208,342 and the number of sentence trigram units was 99,911,516.", "labels": [], "entities": []}, {"text": "The language model parameters (\u00b5 values) for the document and sentence trigram units were set to 500 and 100, respectively.", "labels": [], "entities": []}, {"text": "The weighting parameter \u221d in Eq. was 0.8.", "labels": [], "entities": [{"text": "Eq.", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.8852803111076355}]}, {"text": "Then, the weighting parameters \u221d, \u03b2, and \u03b3 i in Eq.", "labels": [], "entities": []}, {"text": "(3) were 0.5, 0.9, and 0.3, respectively.", "labels": [], "entities": []}, {"text": "In Phase A of Task 4b, our best submission was the first to find snippets in batches 2, 3, 4, and 5.", "labels": [], "entities": []}, {"text": "In batch 1, we indexed the limit set of PubMed and achieved the second place in finding snippets.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.9355663657188416}]}, {"text": "Table 1 shows the best performances of KSAnswer.", "labels": [], "entities": [{"text": "KSAnswer", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.6203778982162476}]}, {"text": "The parenthesized values are informal performances that are calculated using gold standard answers for each batch.", "labels": [], "entities": []}, {"text": "In an additional experiment, we found that the degree of the sub-model importance in the reranking model is as follows: Sim SN T (Q,S) >> Sim EAT (Q,S) > Sim F OCU S (Q,S) \u2248 Sim ME (Q,S) \u2248 Sim EM B (Q,S)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results of submitted runs", "labels": [], "entities": []}]}