{"title": [{"text": "Assessing the Corpus Size vs. Similarity Trade-off for Word Embeddings in Clinical NLP", "labels": [], "entities": []}], "abstractContent": [{"text": "The proliferation of deep learning methods in natural language processing (NLP) and the large amounts of data they often require stands in stark contrast to the relatively data-poor clinical NLP domain.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.7240959952274958}]}, {"text": "In particular, large text corpora are necessary to build high-quality word embeddings, yet often large corpora that are suitably representative of the target clinical data are unavailable.", "labels": [], "entities": []}, {"text": "This forces a choice between building embeddings from small clinical corpora and less representative , larger corpora.", "labels": [], "entities": []}, {"text": "This paper explores this trade-off, as well as intermediate compromise solutions.", "labels": [], "entities": []}, {"text": "Two standard clinical NLP tasks (the i2b2 2010 concept and assertion tasks) are evaluated with commonly used deep learning models (recurrent neural networks and convolutional neural networks) using a set of six corpora ranging from the target i2b2 data to large open-domain datasets.", "labels": [], "entities": []}, {"text": "While combinations of corpora are generally found to work best, the single-best corpus is generally task-dependent.", "labels": [], "entities": []}], "introductionContent": [{"text": "The use of vector representations in natural language processing (NLP) has a solid foundation).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.7719925542672476}]}, {"text": "These enable dense representations that often encode semantic properties and are particularly useful for machine learning tasks as an alternative to extremely sparse, \"one-hot\" vocabulary-length vector representations.", "labels": [], "entities": []}, {"text": "Many ways of building these vectors exist, including random indexing), clustering), regression (), and neural ( methods.", "labels": [], "entities": []}, {"text": "This paper focuses on the last such type of vector representation, often referred to as embeddings, and exemplified by the popular method word2vec (.", "labels": [], "entities": []}, {"text": "Embeddings are particularly useful in neural network architectures, which due to their heavy use of matrix multiplication typically favor low-dimensional, dense representation.", "labels": [], "entities": []}, {"text": "In particular, neural network models that utilize multiple layers of operations to find abstractions in the data (collectively referred to as deep learning models) area natural fit for these dense semantic representations.", "labels": [], "entities": []}, {"text": "In what is typically a semi-supervised process, word embeddings are generated from a large, representative sample of data.", "labels": [], "entities": []}, {"text": "Then, a smaller manually annotated sample is used to train the deep learning models.", "labels": [], "entities": []}, {"text": "However, this results in a common problem for clinical NLP: large representative corpora (at least comparable to those used in much open-domain NLP research) are not often available for building these embeddings.", "labels": [], "entities": []}, {"text": "This is due to the significant restrictions on the use of electronic health record (EHR) data, especially narrative notes, for research purposes.", "labels": [], "entities": []}, {"text": "Clinical NLP researchers and practitioners are often then left with a trade-off: using a small-but-representative corpus versus a large-but-unrepresentative corpus.", "labels": [], "entities": []}, {"text": "The former may not be large enough to properly capture the necessary semantics, while the latter might not be representative enough to capture the semantics of some of the most important words in the corpus.", "labels": [], "entities": []}, {"text": "For instance, a large open-domain corpus might associate the abbreviation ms with millisecond (or Mississippi) rather than multiple sclerosis (or mitral stenosis).", "labels": [], "entities": [{"text": "Mississippi", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.9750362038612366}]}, {"text": "In theory, one could simply experiment with multiple corpora to see what works best fora given task.", "labels": [], "entities": []}, {"text": "But in practice this maybe overly burdensome, especially in the context of deep learning models that have many, many other important parameters and architectural choices to consider, in addition to their long training times.", "labels": [], "entities": []}, {"text": "What would be useful, then, is some intuitive notion or rule-of-thumb on what corpora to use for building word embeddings for clinical NLP.", "labels": [], "entities": []}, {"text": "From a practical point-of-view, one can see two ideal scenarios: 1.", "labels": [], "entities": []}, {"text": "A small target corpus (several hundred or a few thousand documents) that is highly representative of the annotated notes in the clinical NLP task (possibly including the annotated notes themselves).", "labels": [], "entities": []}, {"text": "2. A large corpus (millions of documents) that is completely general-purpose (likely not containing clinical note text at all).", "labels": [], "entities": []}, {"text": "If the first scenario were to result in optimal system performance, this would be quite easy for the clinical NLP practitioner: for each NLP task, generate a set of embeddings specific to the corpus.", "labels": [], "entities": []}, {"text": "The second scenario is even easier: simply use an \"off-the-shelf\" set of word embeddings.", "labels": [], "entities": []}, {"text": "However, there are many possible compromise solutions between these two extremes.", "labels": [], "entities": []}, {"text": "For example, a medium-size corpus of clinical notes from a different corpus, or a large corpus of scientific articles, or even a combination of two or more of these.", "labels": [], "entities": []}, {"text": "The goal of this paper is to explore this size vs. similarity trade-off, specifically for clinical NLP purposes.", "labels": [], "entities": [{"text": "similarity", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.8916536569595337}]}, {"text": "A handful of corpora ranging from a small target corpus to a large generalpurpose corpus are used to build embeddings.", "labels": [], "entities": []}, {"text": "Experiments using two common deep learning models in combination with two standard clinical NLP datasets are used to evaluate this trade-off.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes related work with word embeddings, including its use in clinical NLP.", "labels": [], "entities": []}, {"text": "Section 3 describes the tasks used to evaluate the embeddings.", "labels": [], "entities": []}, {"text": "Section 4 describes the datasets used to generate the embeddings.", "labels": [], "entities": []}, {"text": "Section 5 describes the experimental setup, including the parameters for generating the word embeddings as well as the parameters for the deep learning models.", "labels": [], "entities": []}, {"text": "Section 6 shows the results of the experiments.", "labels": [], "entities": []}, {"text": "Section 7 discusses the implications, with some practical considerations.", "labels": [], "entities": []}], "datasetContent": [{"text": "Both word embeddings and deep learning models have very many possible parameters that can impact downstream tasks.", "labels": [], "entities": []}, {"text": "The following experimental description is by no means likely to be optimal for the tasks, but was made based on a combination of default parameters, conventional wisdom, and practical necessity.", "labels": [], "entities": []}, {"text": "In some cases experiments were conducted to test parameter impact on the downstream tasks (mostly with the more crucial deep learning model parameters).", "labels": [], "entities": []}, {"text": "See Section 7.1 fora discussion of the limitations of these experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Frequencies of concept types in the i2b2 2010 corpus.", "labels": [], "entities": [{"text": "i2b2 2010 corpus", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.8501483201980591}]}, {"text": " Table 4: Frequencies of assertion types in the i2b2 2010 corpus.", "labels": [], "entities": [{"text": "i2b2 2010 corpus", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.7975903352101644}]}, {"text": " Table 5: Basic corpus statistics, including the proportion of three important clinical terms (diabetes,  myocardial, tumor) to illustrate how representative each corpus is of clinical text. Note that this excludes  common clinical abbreviations (e.g., dm or dm2 for diabetes). \"N/A\" indicates the word was not in the  top 100k terms and thus not included in the embeddings.", "labels": [], "entities": [{"text": "N/A\"", "start_pos": 279, "end_pos": 283, "type": "METRIC", "confidence": 0.6842948347330093}]}, {"text": " Table 6: Results for RNN-based concept recognition on the i2b2 2010 corpus, measured with precision  (P), recall (R), and F 1 -measure.", "labels": [], "entities": [{"text": "RNN-based concept recognition", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.879278838634491}, {"text": "i2b2 2010 corpus", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.8202101190884908}, {"text": "precision  (P)", "start_pos": 91, "end_pos": 105, "type": "METRIC", "confidence": 0.9421398490667343}, {"text": "recall (R)", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.9593453854322433}, {"text": "F 1 -measure", "start_pos": 123, "end_pos": 135, "type": "METRIC", "confidence": 0.979571521282196}]}, {"text": " Table 7: Results for CNN-based assertion classification on the i2b2 2010 corpus, measured with accu- racy, along with the F 1 -measure for present (P), absent (A), hypothetical (H), possible (B), conditional  (C), and associated with someone else (O).", "labels": [], "entities": [{"text": "CNN-based assertion classification", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.6454784472783407}, {"text": "i2b2 2010 corpus", "start_pos": 64, "end_pos": 80, "type": "DATASET", "confidence": 0.8530218203862509}, {"text": "accu- racy", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9075800577799479}, {"text": "F 1 -measure", "start_pos": 123, "end_pos": 135, "type": "METRIC", "confidence": 0.9704058170318604}]}, {"text": " Table 7. Unlike concepts, the single best corpus is the target i2b2 data. All other corpora  performed close, with the worst performance being WebMD with a 1.1 point drop in accuracy. Only  slight gains are seen by adding in other corpora, the best being all corpora except Gigaword for a 0.3  point improvement, but no substantial losses are seen either.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9975323677062988}]}]}