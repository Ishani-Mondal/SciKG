{"title": [{"text": "Learning to recognise named entities in tweets by exploiting weakly labelled data", "labels": [], "entities": []}], "abstractContent": [{"text": "Named entity recognition (NER) in social media (e.g., Twitter) is a challenging task due to the noisy nature of text.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7958108385403951}]}, {"text": "As part of our participation in the W-NUT 2016 Named Entity Recognition Shared Task, we proposed an unsupervised learning approach using deep neural networks and leverage a knowledge base (i.e., DBpedia) to bootstrap sparse entity types with weakly labelled data.", "labels": [], "entities": [{"text": "W-NUT 2016 Named Entity Recognition Shared Task", "start_pos": 36, "end_pos": 83, "type": "TASK", "confidence": 0.7267434000968933}]}, {"text": "To further boost the performance, we employed a more sophisticated tagging scheme and applied dropout as a regularisation technique in order to reduce overfitting.", "labels": [], "entities": []}, {"text": "Even without hand-crafting linguistic features nor leveraging any of the W-NUT-provided gazetteers, we obtained robust performance with our approach, which ranked third amongst all shared task participants according to the official evaluation on a gold standard named entity-annotated corpus of 3,856 tweets.", "labels": [], "entities": [{"text": "W-NUT-provided gazetteers", "start_pos": 73, "end_pos": 98, "type": "DATASET", "confidence": 0.8422117233276367}]}], "introductionContent": [{"text": "Named entity recognition (NER) is one of the most fundamental natural language processing (NLP) tasks that is central to understanding unstructured, textual data.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8249424695968628}]}, {"text": "Popular approaches () mainly rely on hand-crafted features and gazetteers which require knowledge about the domain.", "labels": [], "entities": []}, {"text": "Recently, there has been a surge in terms of interest in applying deep learning techniques to NLP tasks.", "labels": [], "entities": []}, {"text": "These methods, together with substantial amount of annotated data, can learn features automatically and have been reported to outperform traditional methods on certain NLP tasks (.", "labels": [], "entities": []}, {"text": "However, in spite of the perceived success of deep learning methods particularly in NER in newswire, performance on social media content, particularly that from Twitter, has been lagging behind (.", "labels": [], "entities": []}, {"text": "There are two state-of-the-art deep learning methods for newswire NER, namely, those proposed by and.", "labels": [], "entities": []}, {"text": "Considering that the former requires hand-crafted features (e.g., word matches against gazetteers) and yet obtains only a small boost in performance over the latter which does not rely on any such features, we took the approach of and applied it on microblog posts from the Twitter platform, i.e., tweets.", "labels": [], "entities": []}, {"text": "Based on the results of our initial experiments, we observed suboptimal performance relative to that on newswire.", "labels": [], "entities": []}, {"text": "Informed by the error analysis that we carried out, we employed a distant supervision method exploiting an external lexical resource, i.e.,, in order to generate weakly labelled data for low-frequency named entity types.", "labels": [], "entities": []}, {"text": "We also investigated the effect of using different token-level tagging schemes and confirmed that improved performance can be obtained by applying the finer-grained BIOES scheme rather than the more popularly used BIO convention.", "labels": [], "entities": [{"text": "token-level tagging", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8216211199760437}, {"text": "BIOES", "start_pos": 165, "end_pos": 170, "type": "METRIC", "confidence": 0.8544296026229858}]}, {"text": "Furthermore, we explored the use of generic placeholders to address out-of-embedding-vocabulary (OOEV) words, although this approach did not lead to better performance.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Entity type distribution in training and development sets", "labels": [], "entities": []}, {"text": " Table 2: Proposed mapping between W-NUT entity types and DBpedia categories", "labels": [], "entities": []}, {"text": " Table 3: Description of word embeddings pre-trained using GloVe", "labels": [], "entities": [{"text": "GloVe", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.5987684726715088}]}, {"text": " Table 4: Percentage distribution of IV and OOEV words in the pre-trained embeddings", "labels": [], "entities": [{"text": "OOEV", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.8480450510978699}]}, {"text": " Table 5: OOEV word categories and their distribution", "labels": [], "entities": [{"text": "OOEV word categories", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.5287906527519226}]}, {"text": " Table 6: Percentage distribution of named entities with IV and OOEV words", "labels": [], "entities": [{"text": "OOEV", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9697450399398804}]}, {"text": " Table 7: Comparison of performance when using different tagging schemes", "labels": [], "entities": []}, {"text": " Table 8: Comparison of performance based on different pre-trained word embeddings", "labels": [], "entities": []}, {"text": " Table 9: Performance of different embedding strategies for representing OOEV words", "labels": [], "entities": []}, {"text": " Table 10: Effect on the F-scores after adding weakly labelled data to sparse entity types", "labels": [], "entities": [{"text": "F-scores", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9898764491081238}]}, {"text": " Table 11: Performance of our NER system, akora, compared to other top-ranking W-NUT 2016 systems", "labels": [], "entities": [{"text": "NER", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.7099881172180176}]}, {"text": " Table 12: Named entity recognition performance on the official evaluation data set", "labels": [], "entities": [{"text": "Named entity recognition", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.7580435872077942}, {"text": "official evaluation data set", "start_pos": 55, "end_pos": 83, "type": "DATASET", "confidence": 0.7334582060575485}]}]}