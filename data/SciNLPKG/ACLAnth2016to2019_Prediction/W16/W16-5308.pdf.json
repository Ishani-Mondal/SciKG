{"title": [{"text": "Towards a resource based on users' knowledge to overcome the Tip-of-the-Tongue problem", "labels": [], "entities": []}], "abstractContent": [{"text": "Language production is largely a matter of words which, in the case of access problems, can be searched for in an external resource (lexicon, thesaurus).", "labels": [], "entities": []}, {"text": "In this kind of dialogue the user provides the momentarily available knowledge concerning the target and the system responds with the best guess(es) it can make given this input.", "labels": [], "entities": []}, {"text": "As tip-of-the-tongue (ToT)-studies have shown, people always have some knowledge concerning the target (meaning fragments, number of syllables, ...) even if its complete form is eluding them.", "labels": [], "entities": []}, {"text": "We will show here how to tap on this knowledge to build a resource likely to help authors (speakers/writers) to overcome the ToT-problem.", "labels": [], "entities": []}, {"text": "Yet, before doing so we need a better understanding of the various kinds of knowledge people have when looking fora word.", "labels": [], "entities": []}, {"text": "To this end, we asked crowdworkers to provide some cues to describe a given target and to specify then how each one of them relates to the target, in the hope that this could help others to find the elusive word.", "labels": [], "entities": []}, {"text": "Next, we checked how well a given search strategy worked when being applied to differently built lexical networks.", "labels": [], "entities": []}, {"text": "The results showed quite dramatic differences, which is not really surprising.", "labels": [], "entities": []}, {"text": "After all, different networks are built for different purposes; hence each one of them is more or less suited fora given task.", "labels": [], "entities": []}, {"text": "What was more surprising though is the fact that the relational information given by the users did not allow us to find the elusive word in WordNet better than without it.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 140, "end_pos": 147, "type": "DATASET", "confidence": 0.9596169590950012}]}, {"text": "1 The problem : word access in language production Communication is largely based on words which encode various sorts of information, conceptual (lex-ical semantics, encyclopedic knowledge), linguistic (word forms, part of speech), ...", "labels": [], "entities": []}, {"text": "If ever we lack any of this information we may reach fora dictionary, a thesaurus or an encyclopedia in the hope to find what we are looking for.", "labels": [], "entities": []}, {"text": "Information access works generally quite well for readers, but much less for authors.", "labels": [], "entities": []}, {"text": "Obviously, readers and writers have different needs, and while both provide words as input , they clearly pursue different goals.", "labels": [], "entities": []}, {"text": "Readers start from word forms in the hope to get meanings, while authors go the opposite direction: starting from meanings (or meaning fragments), broader topical categories (thesaurus) or specific target-related words (associations, co-occurrences) they hope to find the elusive word (target).", "labels": [], "entities": []}, {"text": "We will be concerned herewith this latter kind of search.", "labels": [], "entities": []}, {"text": "There are two major access modes, one being automatic, and the other deliberate.", "labels": [], "entities": []}, {"text": "The former relies solely on our brain (on-line processing when speaking or writing) whereas the latter uses an additional, external resource (paper or electronic dictionary).", "labels": [], "entities": []}, {"text": "In general we resort to this second strategy only if spontaneous access fails.", "labels": [], "entities": []}, {"text": "Alas, most dictionaries are not very well suited for this purpose (see Section 3).", "labels": [], "entities": []}, {"text": "Yet, even if we had such a dictionary, we are still faced with the problems of input and size.", "labels": [], "entities": []}, {"text": "What information shall the user give to allow the resource to guess the elusive word?", "labels": [], "entities": []}, {"text": "Since dictionaries are generally quite large, arises the question of how to reduce the entire set of words (scope of the lexicon) to one, the target.", "labels": [], "entities": []}, {"text": "This leads to the next question: how to reduce quickly the initial space to a subspace which is neither too big nor too small, that is, how to ensure that the output contains only a reasonable set of candidates (not too big), yet still potentially relevant information?", "labels": [], "entities": []}, {"text": "Inconsiderate filtering This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": [{"text": "Inconsiderate filtering", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7403073012828827}]}], "introductionContent": [], "datasetContent": [{"text": "In this section, we describe the experimental set-up to answer the following research questions: (a) When being in the ToT-state what cues do people provide to help the system find the target?", "labels": [], "entities": []}, {"text": "(b) How good are existing lexical resources for retrieving the targets by using these cues?", "labels": [], "entities": []}, {"text": "(c) How big is the added value of knowing the relationship between the cue (source word) and the target?", "labels": [], "entities": []}, {"text": "Put differently, does it enhance retrieval precision and speed?", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9854313135147095}, {"text": "speed", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.981474757194519}]}, {"text": "Since it is not trivial to put people in the ToT state, we have reformulated the problem in the following way: we ask people to describe a given target to other people who may not know the word (e.g. language learners), by providing three cues.", "labels": [], "entities": []}, {"text": "Crowdworkers were asked to provide single-word cues rather than descriptions or definitions.", "labels": [], "entities": []}, {"text": "Note that the idea was not the creation of a resource, but rather the creation of a set of data to see how well they would behave with respect to our three resources (section, 6.1).", "labels": [], "entities": []}, {"text": "Also, in order to get a clearer picture concerning our third question, i.e. the added value of the relation between cue and target, we asked subjects to also specify the relationship between the target and each one of the given three cues.", "labels": [], "entities": []}, {"text": "Relations were defined indirectly, i.e. via examples.", "labels": [], "entities": []}, {"text": "They comprise synonyms, hypernyms/hyponyms, meronyms/holonyms, typical properties, typical roles (verb-subject, verb-object) and free associations.", "labels": [], "entities": []}, {"text": "Data acquisition was done via the Crowdflower crowdsourcing platform.", "labels": [], "entities": []}, {"text": "11 In order to check whether crowdworkers had given the right answer and understood the target, we presented the latter together with three definitions.", "labels": [], "entities": []}, {"text": "For our experiment we used only trials that the crowdworkers had fully understood, that is, for which they had picked the correct definition.", "labels": [], "entities": []}, {"text": "After data collection, we excluded data from crowdworkers that deliberately had ignored our instructions.", "labels": [], "entities": [{"text": "data collection", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.7711820602416992}]}, {"text": "For the targets and definitions we used the 208 common nouns listed in (, who examined the ToT state from a psychological angle.", "labels": [], "entities": []}, {"text": "Full data, instructions and judgments are available online.", "labels": [], "entities": []}, {"text": "Data collection yielded a total of 1186 cue triplets, provided by 65 participants, who worked on 3 to 132 targets.", "labels": [], "entities": []}, {"text": "After manual correction of typos and lemmatization, cue triplets were filtered by eliminating words outside of the vocabulary of the respective resource used in the experiments.", "labels": [], "entities": []}, {"text": "Inspection of the data revealed that crowdworkers generally chose the cues quite well, but many of them had a hard time to assign the appropriate relation, which is not all that surprising, as this requires quite a bit of metalinguistic knowledge.", "labels": [], "entities": []}, {"text": "It is also possible that some participants had chosen the relation without taking the needed care since we did not perform any quality checks during the task.", "labels": [], "entities": []}, {"text": "We probably need a different kind of experiment to validate this or measure the extent to which linguistically innocent users can accurately classify semantic relations.", "labels": [], "entities": []}, {"text": "below shows the distribution of relations expressed in the first 200 cue triplets (target range 'a-c', i.e. abacus -calisthetics, in alphabetical order) containing also some manually assigned relations.", "labels": [], "entities": []}, {"text": "The results show the importance of taxonomic relations, a fact well exploited by WN.", "labels": [], "entities": [{"text": "taxonomic relations", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.9462172985076904}, {"text": "WN", "start_pos": 81, "end_pos": 83, "type": "DATASET", "confidence": 0.8067465424537659}]}, {"text": "Representing nearly 46% of the relations, they confirm the intuition that paradigmatic associations are an important means to access the desired word.", "labels": [], "entities": []}, {"text": "However, the next largest class are syntagmatic, i.e. untyped, associations (37%).", "labels": [], "entities": []}, {"text": "Note that about 17% of the cues come from a different word class than the targets.: Distribution of relations between target and cue, as well as typical part of speech (POS) for the cue (N: Noun, V: Verb, A: Adjective), manually assigned by the authors.", "labels": [], "entities": []}, {"text": "Our methodology is very similar to the one of Thorat and Choudhari: we query the lexical network with cues and retrieve then a ranked list of potential ToT targets.", "labels": [], "entities": []}, {"text": "With more appropriate cues and better lexical resources, our targets will probably get a boost, appearing higher in the list.", "labels": [], "entities": []}, {"text": "Our vocabulary of WN comprises 139,784 terms, including multiwords, which can be mutually reached through the query procedure described above was used in the first experiment.", "labels": [], "entities": []}, {"text": "The intersection of the vocabulary of the three networks consists of 34,365 terms, all of them being single words, just as the ones used in the second experiment.", "labels": [], "entities": []}, {"text": "Here below are the criteria used in our evaluation: \u2022 Minimum rank per cue (MinRank): if all cues were processed strictly in parallel, when would the target appear for the first time?", "labels": [], "entities": [{"text": "Minimum rank per cue (MinRank)", "start_pos": 54, "end_pos": 84, "type": "METRIC", "confidence": 0.8619298934936523}]}, {"text": "\u2022 Target rank in sum of ranks (+Rank): if the retrieval time depends on the average rank per cue, we sum the ranks of the three cues and sort the list of terms in ascending order, reporting the position of the target.", "labels": [], "entities": []}, {"text": "Note that this score is strongly influenced by negative outlier cues.", "labels": [], "entities": []}, {"text": "\u2022 Target rank in multiplication of ranks (*Rank): To model a multiplicative instead of an additive combination, we multiply the target ranks per cue, sort the list of terms by this score in ascending order, and report then the position of the target.", "labels": [], "entities": []}, {"text": "This score is less sensitive to negative outliers.", "labels": [], "entities": []}, {"text": "\u2022 Average Precision@100 (P@100) measures the fraction of trials containing the target among the first 100 hits, for each of the above.", "labels": [], "entities": [{"text": "Average Precision@100 (P@100)", "start_pos": 2, "end_pos": 31, "type": "METRIC", "confidence": 0.9133293496237861}]}, {"text": "While 100 is an arbitrary number, it seems a reasonable wordlist size to allow for the quick retrieval of a target.", "labels": [], "entities": []}, {"text": "Note that the minimum rank is not necessarily lower than the other two scores.", "labels": [], "entities": []}, {"text": "It is possible, and it even happens in our data, that a target gets a low rank because all three cues rank it consistently low, while the targets preferred by single cues are ranked much less favorably than others.", "labels": [], "entities": []}, {"text": "For example, the target \"agnostic\" was retrieved from WN (untyped) by its three cues \"believer, god, atheist\" with ranks 170, 890, respectively 25.", "labels": [], "entities": [{"text": "WN", "start_pos": 54, "end_pos": 56, "type": "DATASET", "confidence": 0.9356154203414917}]}, {"text": "Minimum Rank is thus 25, but ranking via sum of ranks lists the target at position 14, while the multiplicative combination results in rank 15.", "labels": [], "entities": [{"text": "Minimum Rank", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8700776994228363}]}, {"text": "In the next section, we will qualitatively assess the differences in rankings from our different semantic networks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of relations between target and cue, as well as typical part of speech (POS) for  the cue (N: Noun, V: Verb, A: Adjective), manually assigned by the authors.", "labels": [], "entities": []}, {"text": " Table 2: Scores for target retrieval in WordNet by using or ignoring relational information  for 200 cue triples on a vocabulary of 139,784 terms  Both settings perform much better than the random baseline, which returns the vocabulary in ran- dom order irrespective of the dictionary's structure. The random baseline was obtained by running", "labels": [], "entities": [{"text": "target retrieval", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.7137864828109741}, {"text": "WordNet", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9463640451431274}]}]}