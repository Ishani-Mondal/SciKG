{"title": [{"text": "EmpiriST 2015: A Shared Task on the Automatic Linguistic Annotation of Computer-Mediated Communication and Web Corpora", "labels": [], "entities": [{"text": "EmpiriST 2015", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9645161926746368}, {"text": "Automatic Linguistic Annotation of Computer-Mediated Communication and Web Corpora", "start_pos": 36, "end_pos": 118, "type": "TASK", "confidence": 0.7506642010476854}]}], "abstractContent": [{"text": "This paper describes the goals, design and results of a shared task on the automatic linguistic annotation of German language data from genres of computer-mediated communication (CMC), social media interactions and Web corpora.", "labels": [], "entities": [{"text": "automatic linguistic annotation of German language data from genres of computer-mediated communication (CMC)", "start_pos": 75, "end_pos": 183, "type": "TASK", "confidence": 0.8053506135940551}]}, {"text": "The two sub-tasks of tokenization and part-of-speech tagging were performed on two data sets: (i) a genuine CMC data set with samples from several CMC genres, and (ii) a Web corpora data set of CC-licensed Web pages which represents the type of data found in large corpora crawled from the Web.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.9675107002258301}, {"text": "part-of-speech tagging", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7442806661128998}, {"text": "CMC data set", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.8499536712964376}, {"text": "Web corpora data set of CC-licensed Web pages", "start_pos": 170, "end_pos": 215, "type": "DATASET", "confidence": 0.8193794302642345}]}, {"text": "The teams participating in the shared task achieved a substantial improvement over current off-the-shelf tools for Ger-man.", "labels": [], "entities": []}, {"text": "The best tokenizer reached an F 1-score of 99.57% (vs. 98.95% off-the-shelf baseline), while the best tagger reached an accuracy of 90.44% (vs. 84.86% baseline).", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9921320080757141}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9993875026702881}]}, {"text": "The gold standard (more than 20,000 tokens of training and test data) is freely available online together with detailed annotation guidelines.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Evaluation of the submissions to EmpiriST 2015 was carried out by the task organizers.", "labels": [], "entities": [{"text": "EmpiriST 2015", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9419472813606262}]}, {"text": "Following, results for the tokenization task were evaluated based on the unweighted harmonic average (F 1 ) between precision (pr) and recall (rc) of the token boundaries in the participants' submissions.", "labels": [], "entities": [{"text": "tokenization task", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.9268759489059448}, {"text": "unweighted harmonic average (F 1 )", "start_pos": 73, "end_pos": 107, "type": "METRIC", "confidence": 0.8442049622535706}, {"text": "precision (pr)", "start_pos": 116, "end_pos": 130, "type": "METRIC", "confidence": 0.9312437623739243}, {"text": "recall (rc)", "start_pos": 135, "end_pos": 146, "type": "METRIC", "confidence": 0.9682721197605133}]}, {"text": "Formally, let B retrieved be the set of token boundaries predicted by the tokenization procedure to be evaluated and B relevant those present in the gold standard; then: For technical reasons, the trivial token boundary at the beginning of each text file is included in the evaluation, but not the boundary at its end.", "labels": [], "entities": []}, {"text": "Following Giesbrecht and Evert (2009), the PoS tagging task was evaluated in terms of the accuracy (acc) of the PoS tag assignments in the participants' submissions.", "labels": [], "entities": [{"text": "PoS tagging task", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.9155666828155518}, {"text": "accuracy (acc)", "start_pos": 90, "end_pos": 104, "type": "METRIC", "confidence": 0.9328700304031372}]}, {"text": "Formally, let n correct be the number of tokens whose tags agree with the gold standard, and n total the total number of tokens in the data set; then: In order to support participants in development and self-evaluation of their submissions, both evaluation metrics were implemented as Perl scripts by the organizers and published together with the training and test data sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sizes of the training and test data sets,  specified in number of tokens (above) and number  of text samples (below).", "labels": [], "entities": []}, {"text": " Table 3: Agreement between annotators and gold  standard for PoS tagging of the CMC data subset  (training and test sets). Values are accuracy (acc)  percentages.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.8648737967014313}, {"text": "CMC data subset", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.9365375638008118}, {"text": "accuracy (acc)  percentages", "start_pos": 135, "end_pos": 162, "type": "METRIC", "confidence": 0.9196959972381592}]}, {"text": " Table 4: Agreement between annotators and gold  standard for tokenization of the Web corpora test  data. Values are F 1 scores given as percentages.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.9692612290382385}, {"text": "Web corpora test  data", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.5999384075403214}, {"text": "F 1 scores", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.9644176165262858}]}, {"text": " Table 5: Agreement between annotators and gold  standard for PoS tagging of the Web corpora test  data. Values are accuracy (acc) percentages.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.8377010226249695}, {"text": "Web corpora test  data", "start_pos": 81, "end_pos": 103, "type": "DATASET", "confidence": 0.694694273173809}, {"text": "accuracy (acc) percentages", "start_pos": 116, "end_pos": 142, "type": "METRIC", "confidence": 0.9113029837608337}]}, {"text": " Table 7: Results of the tokenization subtask including non-competitive submissions (marked with  *  ) and  baseline systems (marked with  \u2020 ). The last column gives the official EmpiriST 2015 \"podium\" ranking.  pr, rc, and F 1 are given as percentages for better readability.", "labels": [], "entities": [{"text": "tokenization subtask", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.909712553024292}, {"text": "EmpiriST 2015 \"podium\" ranking", "start_pos": 179, "end_pos": 209, "type": "DATASET", "confidence": 0.9232990344365438}, {"text": "F 1", "start_pos": 224, "end_pos": 227, "type": "METRIC", "confidence": 0.9913808405399323}]}, {"text": " Table 8: Results of the PoS tagging subtask including non-competitive or late submissions (marked  with  *  ) and baseline systems (marked with  \u2020 ). If applicable, a subscript indicates the best run of the  respective system (based on overall accuracy), which is listed in the table. The last column gives the  official EmpiriST 2015 \"podium\" ranking. acc is given as a percentage for better readability.", "labels": [], "entities": [{"text": "PoS tagging subtask", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8126600384712219}, {"text": "accuracy", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.997907280921936}, {"text": "EmpiriST 2015 \"podium\" ranking", "start_pos": 322, "end_pos": 352, "type": "DATASET", "confidence": 0.9405481815338135}, {"text": "acc", "start_pos": 354, "end_pos": 357, "type": "METRIC", "confidence": 0.9838488101959229}]}]}