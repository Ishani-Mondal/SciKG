{"title": [{"text": "The GW/LT3 VarDial 2016 Shared Task System for Dialects and Similar Languages Detection", "labels": [], "entities": [{"text": "GW/LT3 VarDial 2016 Shared Task", "start_pos": 4, "end_pos": 35, "type": "DATASET", "confidence": 0.9010719742093768}, {"text": "Dialects and Similar Languages Detection", "start_pos": 47, "end_pos": 87, "type": "TASK", "confidence": 0.5236533582210541}]}], "abstractContent": [{"text": "This paper describes the GW/LT3 contribution to the 2016 VarDial shared task on the identification of similar languages (task 1) and Arabic dialects (task 2).", "labels": [], "entities": [{"text": "GW/LT3", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.7946237126986185}]}, {"text": "For both tasks, we experimented with Logistic Regression and Neural Network classifiers in isolation.", "labels": [], "entities": []}, {"text": "Additionally, we implemented a cascaded classifier that consists of coarse and fine-grained classifiers (task 1) and a classifier ensemble with majority voting for task 2.", "labels": [], "entities": []}, {"text": "The submitted systems obtained state-of-the-art performance and ranked first for the evaluation on social media data (test sets B1 and B2 for task 1), with a maximum weighted F1 score of 91.94%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9852052927017212}]}], "introductionContent": [{"text": "The 2016 DSL shared task objective was to correctly identify the different variations of similar languages (.", "labels": [], "entities": []}, {"text": "DSL2016 covered two main subtasks: \u2022 Task 1: discriminating between similar languages from the same language family and between national language varieties.", "labels": [], "entities": [{"text": "DSL2016", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9375803470611572}]}, {"text": "Covered languages and varieties are: I Bosnian (bs), Croatian (hr) and Serbian (sr) from the South Slavic language family II Malay (my) and Indonesian (id) from the Austronesian language family III Portuguese from Brazil (pt-BR) and Portugal (pt-PT) IV Spanish from Argentina (es-AR), Mexico (es-MX) and Spain (es-ES) V French from France (fr-FR) and Canada (fr-CA)", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset ( ) provided contains Automatic Speech Recognition (ASR) transcripts in Buckwalter encoding 1 and is divided into: \u2022 Training data: unlike Task 1, the training data is unbalanced and contains 1578 EGY, 1672 GLF, 1758 LAV, 999 MSA, and 1612 NOR instances (total of 7619) \u2022 External datasets For the open submission, we used dialect dictionaries to make in-vocabulary frequency count features (as explained in 3).", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR) transcripts", "start_pos": 34, "end_pos": 80, "type": "TASK", "confidence": 0.7706207207271031}]}, {"text": "For MSA, we used the Arabic Gigaword vocabulary, whereas for other dialects we built dictionaries based on data collected from Twitter.", "labels": [], "entities": []}, {"text": "We are aware that using social media data invariably introduces noise, both in terms of misspelled vocabulary entries and with relation to incorrect geographical information.", "labels": [], "entities": []}, {"text": "However, as argued by, such information still provides acceptable dialectal corpora.", "labels": [], "entities": []}, {"text": "We filtered the collected tweets based on the countries of interest that map to the targeted dialects of the shared task (e.g. Syria \u2192 LAV ).", "labels": [], "entities": [{"text": "Syria \u2192 LAV", "start_pos": 127, "end_pos": 138, "type": "DATASET", "confidence": 0.7721653381983439}]}, {"text": "Before creating the dictionaries, we apply normalization (hamza normalization, emoji and URL removal, . .", "labels": [], "entities": [{"text": "URL removal", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.6370152682065964}]}, {"text": "). The resulting dictionary sizes were 76,721 for GLF, 22,003 for EGY, 10,000 for LAV, 286,559 for MSA and 6,343 for NOR.", "labels": [], "entities": [{"text": "EGY", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.611522912979126}, {"text": "NOR", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.8537943363189697}]}], "tableCaptions": [{"text": " Table 1: Task 1 results. System ranks are indicated in superscript.", "labels": [], "entities": []}, {"text": " Table 2: Task 1 per-variant F1-score", "labels": [], "entities": [{"text": "F1-score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9503275752067566}]}, {"text": " Table 3: Task 2 results. System ranks are indicated in superscript.", "labels": [], "entities": []}]}