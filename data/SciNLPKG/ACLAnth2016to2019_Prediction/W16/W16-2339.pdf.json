{"title": [], "abstractContent": [{"text": "The vast majority of Machine Translation (MT) evaluation approaches are based on the idea that the closer the MT output is to a human reference translation, the higher its quality.", "labels": [], "entities": [{"text": "Machine Translation (MT) evaluation", "start_pos": 21, "end_pos": 56, "type": "TASK", "confidence": 0.8781189620494843}]}, {"text": "While translation quality has two important aspects, adequacy and fluency, the existing reference-based metrics are largely focused on the former.", "labels": [], "entities": [{"text": "translation", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.9563350081443787}]}, {"text": "In this work we combine our metric UPF-Cobalt, originally presented at the WMT15 Metrics Task, with a number of features intended to capture translation fluency.", "labels": [], "entities": [{"text": "WMT15 Metrics Task", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.8132757743199667}, {"text": "translation fluency", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.9041053950786591}]}, {"text": "Experiments show that the integration of fluency-oriented features significantly improves the results, rivalling the best-performing evaluation metrics on the WMT15 data.", "labels": [], "entities": [{"text": "WMT15 data", "start_pos": 159, "end_pos": 169, "type": "DATASET", "confidence": 0.977960467338562}]}], "introductionContent": [{"text": "Automatic evaluation plays an instrumental role in the development of Machine Translation (MT) systems.", "labels": [], "entities": [{"text": "Automatic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7199515104293823}, {"text": "Machine Translation (MT)", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.853802216053009}]}, {"text": "It is aimed at providing fast, inexpensive, and objective numerical measurements of translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.9670207500457764}]}, {"text": "As a cost-effective alternative to manual evaluation, the main concern of automatic evaluation metrics is to accurately approximate human judgments.", "labels": [], "entities": []}, {"text": "The vast majority of evaluation metrics are based on the idea that the closer the MT output is to a human reference translation, the higher its quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9774521589279175}]}, {"text": "The evaluation task, therefore, is typically approached by measuring some kind of similarity between the MT (also called candidate translation) and a reference translation.", "labels": [], "entities": []}, {"text": "The most widely used evaluation metrics, such as BLEU (), follow a simple strategy of counting the number of matching words or word sequences in the candidate and reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9977633953094482}]}, {"text": "Despite its wide use and practical utility, automatic evaluation based on a straightforward candidate-reference comparison has long been criticized for its low correlation with human judgments at sentence-level).", "labels": [], "entities": []}, {"text": "The core aspects of translation quality are fidelity to the source text (or adequacy, in MT parlance) and acceptability (also termed fluency) regarding the target language norms and conventions.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8673573434352875}, {"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9367938041687012}]}, {"text": "Depending on the purpose and intended use of the MT, manual evaluation can be performed in a number of different ways.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9836651682853699}]}, {"text": "However, in any setting both adequacy and fluency shape human perception of the overall translation quality.", "labels": [], "entities": []}, {"text": "By contrast, automatic reference-based metrics are largely focused on MT adequacy, as they do not evaluate the appropriateness of the translation in the context of the target language.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9838348627090454}]}, {"text": "Translation fluency is thus assessed only indirectly, through the comparison with the reference.", "labels": [], "entities": [{"text": "Translation fluency", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.949262410402298}]}, {"text": "However, the difference from a particular human translation does not imply that the MT output is disfluent (.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9573017358779907}]}, {"text": "We propose to explicitly model translation fluency in reference-based MT evaluation.", "labels": [], "entities": [{"text": "translation fluency", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9620214402675629}, {"text": "MT evaluation", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9652368724346161}]}, {"text": "To this end, we develop a number of features representing translation fluency and integrate them with our reference-based metric UPF-Cobalt, which was originally presented at WMT15 (.", "labels": [], "entities": [{"text": "translation fluency", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.928459644317627}, {"text": "UPF-Cobalt", "start_pos": 129, "end_pos": 139, "type": "DATASET", "confidence": 0.6681318879127502}, {"text": "WMT15", "start_pos": 175, "end_pos": 180, "type": "DATASET", "confidence": 0.9638341069221497}]}, {"text": "Along with the features based on the target Language Model (LM) probability of the MT output, which have been widely used in the related fields of speech recognition) and quality estimation (, we design a more detailed representation of MT fluency that takes into account the number of disfluent segments observed in the candidate translation.", "labels": [], "entities": [{"text": "MT output", "start_pos": 83, "end_pos": 92, "type": "TASK", "confidence": 0.8602150678634644}, {"text": "speech recognition", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.7564143240451813}, {"text": "MT fluency", "start_pos": 237, "end_pos": 247, "type": "TASK", "confidence": 0.9000897407531738}]}, {"text": "We test our approach with the data avail-able from WMT15 Metrics Task and obtain very promising results, which rival the best-performing system submissions.", "labels": [], "entities": [{"text": "WMT15 Metrics Task", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.9115952253341675}]}, {"text": "We have also submitted the metric to the WMT16 Metrics Task.", "labels": [], "entities": [{"text": "WMT16 Metrics Task", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.6864627003669739}]}], "datasetContent": [{"text": "For our experiments, we use the data available from the WMT14 and WMT15 Metrics Tasks for into-English translation directions.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.9619903564453125}, {"text": "WMT15 Metrics Tasks", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.8430151343345642}]}, {"text": "The datasets consist of source texts, human reference translations and the outputs from the participating MT systems for different language pairs.", "labels": [], "entities": []}, {"text": "During manual evaluation, for each source sentence the annotators are presented with its human translation and the outputs of a random sample of five MT systems, and asked to rank the MT outputs from best to worst (ties are allowed).", "labels": [], "entities": []}, {"text": "Pairwise system comparisons are then obtained from this compact annotation.", "labels": [], "entities": []}, {"text": "Details on the WMT data for each language pair are given in In our work we focus on sentence-level metrics' performance, which is assessed by converting metrics' scores to ranks and comparing them to the human judgements with Kendall rank correlation coefficient (\u03c4 ).", "labels": [], "entities": [{"text": "WMT", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8693487644195557}, {"text": "Kendall rank correlation coefficient (\u03c4 )", "start_pos": 226, "end_pos": 267, "type": "METRIC", "confidence": 0.9077572056225368}]}, {"text": "We use the WMT14 official Kendall's Tau implementation.", "labels": [], "entities": [{"text": "WMT14 official Kendall's Tau implementation", "start_pos": 11, "end_pos": 54, "type": "DATASET", "confidence": 0.9722791612148285}]}, {"text": "Following the standard practice at WMT and to make our work comparable to the official metrics submitted to the task, we exclude ties inhuman judgments both for training and for testing our system.", "labels": [], "entities": [{"text": "WMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.6361310482025146}]}, {"text": "Our model is a simple linear interpolation of the features presented in the previous sections.", "labels": [], "entities": []}, {"text": "For tuning the weights, we use the learn-to-rank approach (), which has been successfully applied in similar settings in previous work.", "labels": [], "entities": []}, {"text": "We use a standard implementation of Logistic Regression algorithm from the Python toolkit scikit-learn 5 . The model is trained on WMT14 dataset and tested on WMT15 dataset.", "labels": [], "entities": [{"text": "Logistic Regression", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8076896071434021}, {"text": "WMT14 dataset", "start_pos": 131, "end_pos": 144, "type": "DATASET", "confidence": 0.9895498156547546}, {"text": "WMT15 dataset", "start_pos": 159, "end_pos": 172, "type": "DATASET", "confidence": 0.9929699599742889}]}, {"text": "For the extraction of word-level backoff behaviour values and sentence-level fluency features, we use Quest++ 6 , an open source tool for quality estimation ( . We employ the LM used to build the baseline system for WMT15 Quality Estimation Task (.", "labels": [], "entities": [{"text": "WMT15 Quality Estimation Task", "start_pos": 216, "end_pos": 245, "type": "TASK", "confidence": 0.6809157058596611}]}, {"text": "This LM provided was trained on data from the WMT12 translation task (a combination of news and Europarl data) and thus matches the domain of the dataset we use in our experiments.", "labels": [], "entities": [{"text": "WMT12 translation task", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7893266081809998}, {"text": "Europarl data", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.9590679109096527}]}, {"text": "PoS tagging was performed with TreeTagger (Schmid, 1999).", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6454611867666245}, {"text": "TreeTagger (Schmid, 1999)", "start_pos": 31, "end_pos": 56, "type": "DATASET", "confidence": 0.9245655834674835}]}, {"text": "summarizes the results of our experiments.", "labels": [], "entities": []}, {"text": "Group I presents the results achieved by UPFCobalt and its decomposed version described in Section 4.1.", "labels": [], "entities": [{"text": "UPFCobalt", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.9328992962837219}]}, {"text": "Contrary to our expectations, the performance is slightly degraded when using the metrics' components (UPF-Cobaltcomp).", "labels": [], "entities": [{"text": "UPF-Cobaltcomp", "start_pos": 103, "end_pos": 117, "type": "METRIC", "confidence": 0.4979085624217987}]}, {"text": "Our intuition is that this happens due to the sparseness of the features based on the counts of different types of lexical matches.", "labels": [], "entities": []}, {"text": "Group II reports the performance of the fluency features presented in Section 4.2.", "labels": [], "entities": []}, {"text": "First of all, we note that these features on their own (FeaturesF)  The results demonstrate that fluency features provide useful information regarding the overall translation quality, which is not fully captured by the standard candidate-reference comparison.", "labels": [], "entities": []}, {"text": "These features are discriminative when the relationship to the reference does not provide enough information to distinguish between the quality of two alternative candidate translations.", "labels": [], "entities": []}, {"text": "For example, it may well be the case that both MT outputs are very different from human reference, but one constitutes a valid alternative translation, while the other is totally unacceptable.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9707175493240356}]}, {"text": "Finally, Groups III and VI contain the results of the best-performing evaluation systems from the WMT15 Metrics Task, as well as the baseline BLEU metric () and a strong competitor,), which we reproduce here for the sake of comparison.", "labels": [], "entities": [{"text": "WMT15 Metrics Task", "start_pos": 98, "end_pos": 116, "type": "DATASET", "confidence": 0.7649349570274353}, {"text": "BLEU metric", "start_pos": 142, "end_pos": 153, "type": "METRIC", "confidence": 0.9754404127597809}]}, {"text": "DPMFComb ( and RATA-TOUILLE () use a learnt combination of the scores from different evaluation metrics, while BEER Treepel combines word matching, word order and syntax-level features.", "labels": [], "entities": [{"text": "DPMFComb", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.901494562625885}, {"text": "BEER Treepel", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.8956300914287567}, {"text": "word matching", "start_pos": 133, "end_pos": 146, "type": "TASK", "confidence": 0.7584235370159149}]}, {"text": "We note that the number and complexity of the metrics used in the above approaches is quite high.", "labels": [], "entities": []}, {"text": "For instance, DPMFComb is based on 72 separate evaluation systems, including the resource-heavy linguistic metrics from the Asiya Toolkit (Gim\u00e9nez and M` arquez, 2010a).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of pairwise comparisons (Rank),  translation systems (Sys) and source sentences  (Src) per language pair for the WMT14 and  WMT15 datasets", "labels": [], "entities": [{"text": "WMT14 and  WMT15 datasets", "start_pos": 130, "end_pos": 155, "type": "DATASET", "confidence": 0.7767357528209686}]}, {"text": " Table 2: Sentence-level evaluation results for WMT15 dataset in terms of Kendall rank correlation coef- ficient (\u03c4 )", "labels": [], "entities": [{"text": "Sentence-level", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9545000195503235}, {"text": "WMT15 dataset", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9275432825088501}, {"text": "Kendall rank correlation coef- ficient (\u03c4", "start_pos": 74, "end_pos": 115, "type": "METRIC", "confidence": 0.8716837242245674}]}]}