{"title": [{"text": "Using Wikipedia and Semantic Resources to Find Answer Types and Appropriate Answer Candidate Sets in Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7408787906169891}]}], "abstractContent": [{"text": "This paper proposes anew idea that uses Wikipedia categories as answer types and defines candidate sets inside Wikipedia.", "labels": [], "entities": []}, {"text": "The focus of a given question is searched in the hierarchy of Wikipedia main pages.", "labels": [], "entities": []}, {"text": "Our searching strategy combines head-noun matching and synonym matching provided in semantic resources.", "labels": [], "entities": [{"text": "head-noun matching", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.764883428812027}, {"text": "synonym matching", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7949426472187042}]}, {"text": "The set of answer candidates is determined by the entry hierarchy in Wikipedia and the hyponymy hierarchy in WordNet.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 69, "end_pos": 78, "type": "DATASET", "confidence": 0.937872052192688}, {"text": "WordNet", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9712972640991211}]}, {"text": "The experimental results show that the approach can find candidate sets in a smaller size but achieve better performance especially for ARTIFACT and ORGANIZATION types, where the performance is better than state-of-the-art Chinese factoid QA systems.", "labels": [], "entities": [{"text": "ORGANIZATION", "start_pos": 149, "end_pos": 161, "type": "METRIC", "confidence": 0.9383000731468201}]}], "introductionContent": [], "datasetContent": [{"text": "Our main interest in this study is to detect a precise answer type and determine its answer candidate set when NER has its limitation, especially for the classes of artifacts and organizations.", "labels": [], "entities": [{"text": "NER", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.80772465467453}]}, {"text": "Unfortunately there are not many QA benchmarks providing answer type information, nor providing evaluation results according to individual answer types.", "labels": [], "entities": []}, {"text": "Hence we chose NTCIR QA datasets even if the number of questions were not large enough.", "labels": [], "entities": [{"text": "NTCIR QA datasets", "start_pos": 15, "end_pos": 32, "type": "DATASET", "confidence": 0.8652758598327637}]}, {"text": "Two benchmarks on Chinese QA have been developed in NTCIR (.", "labels": [], "entities": [{"text": "Chinese QA", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.8346043229103088}, {"text": "NTCIR", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.9623643755912781}]}, {"text": "NTCIR-5 CLQA1 constructed 200 questions and NTCIR-6 CLQA2 tracks constructed 150 questions classified in nine coarse-grained answer types.", "labels": [], "entities": [{"text": "NTCIR-5 CLQA1", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8883105516433716}, {"text": "NTCIR-6", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.8989817500114441}]}, {"text": "We only focused on 4 types including PER-SON, LOCATION, and especially ARTIFACT and ORGANIZATION, because they were harder to be answered correctly in the previous evaluation.", "labels": [], "entities": [{"text": "PER-SON", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9939079284667969}, {"text": "LOCATION", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9736508727073669}, {"text": "ARTIFACT", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9618117809295654}, {"text": "ORGANIZATION", "start_pos": 84, "end_pos": 96, "type": "METRIC", "confidence": 0.9906993508338928}]}, {"text": "Top 1000 relevant documents for each question were retrieved by atypical tf.idf VSM IR module from the official NTCIR CLQA corpus.", "labels": [], "entities": [{"text": "VSM IR module", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.7332514921824137}, {"text": "NTCIR CLQA corpus", "start_pos": 112, "end_pos": 129, "type": "DATASET", "confidence": 0.9405128757158915}]}, {"text": "Answer candidates were searched inside these relevant documents and ranked by several scoring functions in our previous QA system ( which included frequencies of candidates and keywords, and their distances in a document.", "labels": [], "entities": []}, {"text": "The usefulness of answer type determination methods is measured in terms of the size of the answer candidate set and its coverage of correct answers.", "labels": [], "entities": [{"text": "answer type determination", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.8826004068056742}]}, {"text": "The performance of a QA system is evaluated by MRR (mean reciprocal rank, the average of the inverse of the highest rank where a correct answer is proposed) and Top-1 accuracy (the percentage of questions whose top-1 answers are correct).", "labels": [], "entities": [{"text": "MRR", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9985347986221313}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9066274166107178}]}, {"text": "Among these 221 questions, the correct answers of 196 questions are Wikipedia entry titles.", "labels": [], "entities": []}, {"text": "But for only 177 of them, the correct answers appear in their top 1000 relevant documents, so the upper bound performance of the baseline QA system is 0.792.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Number of Questions in Four Answer Types", "labels": [], "entities": []}, {"text": " Table 3. Number of Questions Having Correct Answer Candidates with Different Methods", "labels": [], "entities": []}, {"text": " Table 6. Performance of Answering All Questions", "labels": [], "entities": [{"text": "Performance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9249160885810852}, {"text": "Answering All Questions", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8312737544377645}]}, {"text": " Table 7. Comparison to the Best Teams in CLQA Tasks", "labels": [], "entities": []}]}