{"title": [{"text": "Moving away from semantic overfitting in disambiguation datasets", "labels": [], "entities": []}], "abstractContent": [{"text": "Entities and events in the world have no frequency , but our communication about them and the expressions we use to refer to them do have a strong frequency profile.", "labels": [], "entities": []}, {"text": "Language expressions and their meanings follow a Zip-fian distribution, featuring a small amount of very frequent observations and a very long tail of low frequent observations.", "labels": [], "entities": []}, {"text": "Since our NLP datasets sample texts but do not sample the world, they are no exception to Zipf's law.", "labels": [], "entities": [{"text": "NLP datasets sample texts", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.8079494684934616}]}, {"text": "This causes alack of representativeness in our NLP tasks, leading to models that can capture the head phenomena in language, but fail when dealing with the long tail.", "labels": [], "entities": []}, {"text": "We therefore propose a referential challenge for semantic NLP that reflects a higher degree of ambiguity and variance and captures a large range of small real-world phenomena.", "labels": [], "entities": [{"text": "semantic NLP", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.7910129129886627}]}, {"text": "To perform well, systems would have to show deep understanding on the linguistic tail.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic processing addresses the relation between natural language and a representation of a world, to which language makes reference.", "labels": [], "entities": [{"text": "Semantic processing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.805647999048233}]}, {"text": "A challenging property of this relation is the context-bound complex interaction between lexical expressions and world meanings.", "labels": [], "entities": []}, {"text": "1 Like many natural phenomena, the distribution of expressions and their meanings follows a power law such as Zipf's law , with a few very frequent observations and a very long tail of low frequent observations.", "labels": [], "entities": []}, {"text": "2 Still, the world itself has no frequency.", "labels": [], "entities": []}, {"text": "All entities and events in the world appear to us with a frequency of 1.", "labels": [], "entities": []}, {"text": "Nevertheless, we dominantly talk about only a few instances in the world and refer to them with a small set of expressions, which can only be explained by the contextual constraints within a language community, a topic, a location, and a period of time.", "labels": [], "entities": []}, {"text": "Without taking these into account, it is impossible to fully determine meaning.", "labels": [], "entities": []}, {"text": "Given that instances in the world do not have frequencies, language and our writing about the world is heavily skewed, selective, and biased with respect to that world.", "labels": [], "entities": []}, {"text": "A name such as Ronaldo can have an infinite amount of references and in any world (real or imaginary) each Ronaldo is equally present.", "labels": [], "entities": []}, {"text": "Our datasets, however, usually make reference to only one Ronaldo.", "labels": [], "entities": [{"text": "Ronaldo", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9356446266174316}]}, {"text": "The problem, as we see it, is that our NLP datasets sample texts but do not sample the world.", "labels": [], "entities": [{"text": "NLP datasets sample texts", "start_pos": 39, "end_pos": 64, "type": "DATASET", "confidence": 0.8136458396911621}]}, {"text": "This causes lack of representativeness in our NLP tasks, that has big consequences for language models: they tend to capture the head phenomena in text without considering the context constraints and thus fail when dealing with less dominant world phenomena.", "labels": [], "entities": []}, {"text": "As a result, there is little awareness of the full complexity of the task in relation to the contextual realities, given language as a system of expressions and the possible interpretations within contexts of time, location, community, and topic.", "labels": [], "entities": []}, {"text": "People, however, have no problem to handle local real-world situations that are referenced to in text.", "labels": [], "entities": []}, {"text": "We believe it is time to create a task that encour-ages systems to model the full complexity of disambiguation by enriched context awareness.", "labels": [], "entities": []}, {"text": "We hence propose a semantic referential challenge, eventbased Question Answering (QA), that reflects a high degree of ambiguity and variance and captures a wide range of small real-world phenomena.", "labels": [], "entities": [{"text": "eventbased Question Answering (QA)", "start_pos": 51, "end_pos": 85, "type": "TASK", "confidence": 0.7655771921078364}]}, {"text": "This task requires a deeper semantic understanding of the linguistic tail of several disambiguation challenges.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}