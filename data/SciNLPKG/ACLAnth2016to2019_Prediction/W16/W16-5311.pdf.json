{"title": [{"text": "CogALex-V Shared Task: GHHH -Detecting Semantic Relations via Word Embeddings", "labels": [], "entities": [{"text": "GHHH", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.7662906050682068}]}], "abstractContent": [{"text": "This paper describes our system submission to the CogALex-2016 Shared Task on Corpus-Based Identification of Semantic Relations.", "labels": [], "entities": [{"text": "Corpus-Based Identification of Semantic Relations", "start_pos": 78, "end_pos": 127, "type": "TASK", "confidence": 0.6923207402229309}]}, {"text": "Our system won first place for Task-1 and second place for Task-2.", "labels": [], "entities": []}, {"text": "The evaluation results of our system on the test set is 88.1% (79.0% for TRUE only) f-measure for Task-1 on detecting semantic similarity, and 76.0% (42.3% when excluding RANDOM) for Task-2 on identifying finer-grained semantic relations.", "labels": [], "entities": [{"text": "f-measure", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9001309275627136}, {"text": "detecting semantic similarity", "start_pos": 108, "end_pos": 137, "type": "TASK", "confidence": 0.7848214705785116}]}, {"text": "In our experiments, we try word analogy, linear regression, and multi-task Convolutional Neural Networks (CNNs) with word embeddings from publicly available word vectors.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.7924803793430328}]}, {"text": "We found that linear regression performs better in the binary classification (Task-1), while CNNs have better performance in the multi-class semantic classification (Task-2).", "labels": [], "entities": []}, {"text": "We assume that word analogy is more suited for deterministic answers rather than handling the ambiguity of one-to-many and many-to-many relationships.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.718148872256279}]}, {"text": "We also show that classifier performance could benefit from balancing the distribution of labels in the training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Finding semantic relatedness between words is of crucial importance for natural language processing as it is essential for tasks like query expansion in information retrieval.", "labels": [], "entities": [{"text": "Finding semantic relatedness between words", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7526772916316986}, {"text": "natural language processing", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.6618382235368093}, {"text": "query expansion", "start_pos": 134, "end_pos": 149, "type": "TASK", "confidence": 0.7546334266662598}]}, {"text": "So far, systems have relied mainly on manually constructed semantic hierarchies, such as ontologies and knowledge graphs.", "labels": [], "entities": []}, {"text": "With the recent interest in neural networks and word embeddings, there are attempts to find semantic relations automatically from texts in an arithmetic fashion by measuring the distance between words in the vector space, assuming that words that are similar to each other will tend to have similar contextual embeddings.", "labels": [], "entities": []}, {"text": "This paper describes our system for the CogALex-V Shared Task on Corpus-Based Identification of Semantic Relations.", "labels": [], "entities": [{"text": "CogALex-V Shared Task on Corpus-Based Identification of Semantic Relations", "start_pos": 40, "end_pos": 114, "type": "TASK", "confidence": 0.6872404217720032}]}, {"text": "We evaluated three methods for semantic classification based on word embeddings: word analogy, linear regression, and multi-task CNNs.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.772549033164978}, {"text": "word analogy", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.7461229264736176}]}, {"text": "In all these methods, we use publicly available pre-trained English word vectors.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we outline the experiments and report the results for the three approaches we tested: word analogy, linear regression and multi-task CNN.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 102, "end_pos": 114, "type": "TASK", "confidence": 0.811479240655899}, {"text": "multi-task CNN", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.6012016981840134}]}, {"text": "The results reported in this section are on the training set for all labels including \"FALSE\" for Task-1 and \"RANDOM\" for Task-2.", "labels": [], "entities": [{"text": "FALSE", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9948084950447083}, {"text": "RANDOM", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.959018886089325}]}, {"text": "Results on the test set of our selected systems are reported in Section 5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training data for Task-1 and Task-2.", "labels": [], "entities": []}, {"text": " Table 2: Word Analogy results", "labels": [], "entities": [{"text": "Word Analogy", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.703917846083641}]}, {"text": " Table 3: F1 Score (%) comparison of ML clas-", "labels": [], "entities": [{"text": "F1 Score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.970236212015152}, {"text": "ML clas", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.681267648935318}]}, {"text": " Table 4: Comparison of word vectors (G=Google News,", "labels": [], "entities": [{"text": "Google News", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.9121359586715698}]}, {"text": " Table 5: Results for different limits of unrelated pairs.", "labels": [], "entities": []}, {"text": " Table 6: Final F1 Scores (%) on the test set.", "labels": [], "entities": [{"text": "Final F1 Scores", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.8743525942166647}]}, {"text": " Table 7: Detailed results for Task-2 labels.", "labels": [], "entities": []}, {"text": " Table 8: Confusion Matrix for Task-2.", "labels": [], "entities": []}]}