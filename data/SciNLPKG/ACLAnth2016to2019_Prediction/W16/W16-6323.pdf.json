{"title": [{"text": "Keynote Lecture-2 Neural Models of Text Normalization for Speech Applications", "labels": [], "entities": [{"text": "Neural Models of Text Normalization", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.5727082967758179}]}], "abstractContent": [{"text": "In this talk, I will present our recent research on applying attention-based RNN's to the problem of text normalization for speech applications, in particular text-to-speech.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7371218502521515}]}, {"text": "In this task, we want to transform an input text, e.g. \"A baby giraffe is 6ft tall and weighs 150lb\", into a sequence of words that represents how that text would be read, e.g. \"a baby giraffe is six feet tall and weighs one hundred fifty pounds\".", "labels": [], "entities": []}, {"text": "The state of the art for the complete text normalization problem to date still uses the 20-year-old technology of rule-based weighted finite-state transducers.", "labels": [], "entities": [{"text": "complete text normalization problem", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.7490176931023598}]}, {"text": "We show that RNNs provide an attractive data-driven approach, but that there are issues with it producing the occasional unacceptable error, such as reading \"\u00a3\" as \"euro\".", "labels": [], "entities": []}, {"text": "We propose some possible solutions to those errors, and discuss future directions for this research.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}