{"title": [{"text": "Embedding Senses for Efficient Graph-based Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.5706548690795898}]}], "abstractContent": [{"text": "We propose a simple graph-based method for word sense disambiguation (WSD) where sense and context embeddings are constructed by applying the Skip-gram method to random walks over the sense graph.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.8278254369894663}]}, {"text": "We used this method to build a WSD system for Swedish using the SALDO lexicon, and evaluated it on six different annotated test sets.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9056458473205566}, {"text": "SALDO lexicon", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9209140539169312}]}, {"text": "In all cases, our system was several orders of magnitude faster than a state-of-the-art PageRank-based system, while outperforming a random base-line soundly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense disambiguation (WSD) is a difficult task for automatic systems.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8211153447628021}]}, {"text": "The most accurate WSD systems build on supervised learning models trained on annotated corpora, but because of the difficulty of the sense annotation task, the luxury of supervised training is available fora few languages only.", "labels": [], "entities": [{"text": "WSD", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9729875326156616}]}, {"text": "An approach that circumvents the lack of annotated corpora is to take advantage of the information available in lexical knowledge bases (LKBs) like WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 148, "end_pos": 155, "type": "DATASET", "confidence": 0.9662444591522217}]}, {"text": "This kind of resource encodes word sense lexicons as graphs connecting lexically and semantically related concepts.", "labels": [], "entities": []}, {"text": "Several methods are available that use LKBs for WSD).", "labels": [], "entities": [{"text": "WSD", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8919532299041748}]}, {"text": "These approaches usually apply a relatively complex analysis of the underlying graph based on the context of a target word to disambiguate it; e.g., use the Personalized PageRank algorithm to perform walks on the graph.", "labels": [], "entities": []}, {"text": "However, these methods are computationally very costly, which makes them practically useless for large corpora.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a more time-efficient approach to graph-based WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.8968084454536438}]}, {"text": "We represent the concepts in the LKB by training vector space models on synthetic datasets created using random walks on the LKB's graph.", "labels": [], "entities": []}, {"text": "These synthetic datasets are built on the assumption that a random walk starting at a given node in the graph will be composed of interrelated concepts, effectively building a context for it.", "labels": [], "entities": []}, {"text": "Training a vector space model on a collection of such data generated for each node in an LKB's graph would result in related concepts being represented near each other in the vector space, according to the distributional hypothesis.", "labels": [], "entities": []}, {"text": "We then use these representations to perform contextbased disambiguation taking advantage of the geometric notions of similarity typical of vector space models.", "labels": [], "entities": []}, {"text": "Using simple mechanisms for disambiguation and random walks allows our method to be orders of magnitude faster while keeping its accuracy well above the random-sense baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9993410706520081}]}], "datasetContent": [{"text": "We built a WSD system for Swedish by applying the random walk-based training described above to the SALDO lexicon ().", "labels": [], "entities": [{"text": "SALDO lexicon", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.8182845115661621}]}, {"text": "In the experiments, we then evaluated this system on six different annotated corpora, in which the ambiguous words have been manually disambiguated according to SALDO, and compared it to random and firstsense baselines and UKB, a state-of-the-art graph-based WSD system.", "labels": [], "entities": [{"text": "UKB", "start_pos": 223, "end_pos": 226, "type": "DATASET", "confidence": 0.9440137147903442}]}, {"text": "For development and evaluation, we used six different collections of sense-annotated examples.", "labels": [], "entities": []}, {"text": "The first two, the SALDO examples (SALDO-ex) and Swedish FrameNet examples (SweFN-ex) consist of sentences selected by lexicographers to exemplify the senses).", "labels": [], "entities": []}, {"text": "The former is dominated by the most frequent verbs, while the latter has a more even distribution.", "labels": [], "entities": []}, {"text": "In our experiments, these two collections were used as a development set to tune the system's parameters.", "labels": [], "entities": []}, {"text": "The additional four collections are taken from an ongoing annotation project (; each collection corresponds to a domain: blogs, novels, Wikipedia, and Europarl (.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 151, "end_pos": 159, "type": "DATASET", "confidence": 0.9716618061065674}]}, {"text": "Unlike the two collections mentioned above, in which the instances have been selected by lexicographers to be prototypical and to have a good coverage of the sense variation, the instances in these four collections are sampled uniformly from running text.", "labels": [], "entities": []}, {"text": "A model is trained on synthetic datasets compiled from random walks on SALDO.", "labels": [], "entities": [{"text": "SALDO", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.8316597938537598}]}, {"text": "These walks are parameterized by their stop probability p stop , which effectively controls the length of the random walk and has two effects: it impacts the size of training data (a lower p stop will generate longer walks on average, and vice versa); and it controls the level of relatedness between the target sense and the words included in the context-a longer walk will wander away from the initial sense, including increasingly unrelated concepts, while a shorter one will keep its concepts closely related.", "labels": [], "entities": [{"text": "stop probability p stop", "start_pos": 39, "end_pos": 62, "type": "METRIC", "confidence": 0.7657889127731323}]}, {"text": "We tuned the model by training several versions with different p stop and evaluated their performance on the development datasets.", "labels": [], "entities": []}, {"text": "As the best-performing parameterization, we chose p stop = 0.25, which generates random walks with an average length of 3.75 nodes and achieves an accuracy of 51.6% on the development datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9992786049842834}]}, {"text": "In all cases, the vector space's dimensionality for senses and contexts is 200, and 10 iterations of the training algorithm are used.", "labels": [], "entities": []}, {"text": "Using this parameterization, we trained models on two different RW datasets: on one, random walks were performed on an unweighted version of SALDO (i.e., all edges are equally probable from any given node); on the other, the graph was weighted favoring the selection of a node's unique PD, with probability 0.5, over inverse (incoming) PD connections, which were uniformly distributed over the remaining probability mass.", "labels": [], "entities": [{"text": "RW datasets", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.800676703453064}]}, {"text": "The disambiguation mechanism explained in Section 2 is applied to sentences containing one ambiguous word using the sense and context representations that result from training the models: A score is calculated for each of the senses of an ambiguous target word in a context window of size 10 (to each side of the target word) and the highest scoring sense is selected to disambiguate the entry.", "labels": [], "entities": []}, {"text": "The accuracy of the method is then obtained by comparing these selections with the annotations of the test datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995980858802795}]}, {"text": "The results of evaluating this models on each component of the test dataset are shown in.", "labels": [], "entities": []}, {"text": "The performance of the UKB model 2 by Agirre and Soroa (2009) on our datasets is also shown in this table, along with first-sense (S1) and random-sense baselines (Rand).", "labels": [], "entities": [{"text": "UKB model 2", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.9586159388224283}, {"text": "random-sense baselines (Rand)", "start_pos": 139, "end_pos": 168, "type": "METRIC", "confidence": 0.7798443675041199}]}, {"text": "These figures show that the firstsense approach is still a hard baseline.", "labels": [], "entities": []}, {"text": "Amongst our two models (RW), the one trained on a weighted graph (w) performs consistently better; both of them outperform by a wide margin the random-sense baseline.", "labels": [], "entities": []}, {"text": "The accuracy on the development sets is generally lower, especially in the case of the firstsense baseline, underlying their difference in nature with respect to the test sets (see Section 3.2  Regarding execution times, the tested models take a few hours to train and, once trained, are able to disambiguate over 8 000 instances per second, significantly surpassing the UKB model's times, which disambiguates approximately 8 instances per second.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.99951171875}, {"text": "UKB model", "start_pos": 371, "end_pos": 380, "type": "DATASET", "confidence": 0.9772462248802185}]}, {"text": "This is related to the fact that the complexity of our disambiguation mechanism is linear on the context vectors (see Equation 1), while the UKB model's is dependent on the graph size.", "labels": [], "entities": [{"text": "UKB", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.9628337025642395}]}], "tableCaptions": [{"text": " Table 1: Evaluation corpus statistics.", "labels": [], "entities": []}, {"text": " Table 2: WSD accuracies on the development and test sets.", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9081984162330627}]}]}