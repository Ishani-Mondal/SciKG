{"title": [], "abstractContent": [{"text": "Pinyin is the most widely used romaniza-tion scheme for Mandarin Chinese.", "labels": [], "entities": []}, {"text": "We consider the task of language identification in Pinyin-English codeswitched texts, a task that is significant because of its application to codeswitched text input.", "labels": [], "entities": [{"text": "language identification", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.7088502049446106}]}, {"text": "We create a codeswitched corpus by extracting and automatically labeling existing Mandarin-English codeswitched corpora.", "labels": [], "entities": []}, {"text": "On language identification , we find that SVM produces the best result when using word-level segmentation, achieving 99.3% F1 on a Weibo dataset, while a linear-chain CRF produces the best result at the letter level, achieving 98.2% F1.", "labels": [], "entities": [{"text": "language identification", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7213946878910065}, {"text": "word-level segmentation", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.6858819127082825}, {"text": "F1", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9894827604293823}, {"text": "Weibo dataset", "start_pos": 131, "end_pos": 144, "type": "DATASET", "confidence": 0.9550584852695465}, {"text": "F1", "start_pos": 233, "end_pos": 235, "type": "METRIC", "confidence": 0.997043788433075}]}, {"text": "We then pass the output of our models to a system that converts Pinyin back to Chinese characters to simulate codeswitched text input.", "labels": [], "entities": []}, {"text": "Our method achieves the same level of performance as an oracle system that has perfect knowledge of token-level language identity.", "labels": [], "entities": []}, {"text": "This result demonstrates that Pinyin identification is not the bottleneck towards developing a Chinese-English codeswitched Input Method Editor, and future work should focus on the Pinyin-to-Chinese character conversion step.", "labels": [], "entities": [{"text": "Pinyin identification", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.9131753444671631}, {"text": "Pinyin-to-Chinese character conversion", "start_pos": 181, "end_pos": 219, "type": "TASK", "confidence": 0.6893024643262228}]}], "introductionContent": [{"text": "As more people are connected to the Internet around the world, an increasing number of multilingual texts can be found, especially in informal, online platforms such as Twitter and Weibo 1 (.", "labels": [], "entities": [{"text": "Weibo 1", "start_pos": 181, "end_pos": 188, "type": "DATASET", "confidence": 0.9158808887004852}]}, {"text": "In this paper, we focus on short Mandarin-English mixed texts, in particular those that involve intra-sentential codeswitching, in which the two languages are interleaved within a single utterance or sentence.", "labels": [], "entities": []}, {"text": "Example 1 shows one such case, including the original codeswitched text (CS), and its Mandarin (MAN) and English (EN) translations: (1) CS: \u8fd9\u4e2athermal exchanger\u7684thermal conductivity\u592a\u4f4e.", "labels": [], "entities": []}, {"text": "EN: The thermal conductivity of this thermal exchanger is too low.", "labels": [], "entities": [{"text": "EN", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.7731447815895081}]}, {"text": "A natural first step in processing codeswitched text is to identify which parts of the text are expressed in which language, as having an accurate codeswitched language identification system seems to be a crucial building block for further processing such as POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 259, "end_pos": 270, "type": "TASK", "confidence": 0.7183185815811157}]}, {"text": "Recently, organized the first shared task towards this goal.", "labels": [], "entities": []}, {"text": "The task is to identify the languages in codeswitched social media data in several language pairs, including Mandarin-English (MAN-EN).", "labels": [], "entities": []}, {"text": "Since Chinese characters are assigned a different Unicode encoding range than Latin-script languages like English, identifying MAN-EN codeswitched data is relatively straightforward.", "labels": [], "entities": []}, {"text": "In fact, the baseline system in the shared task, which simply stores the vocabularies of the two languages seen during training, already achieves 90% F1 on identifying Mandarin segments.", "labels": [], "entities": [{"text": "F1", "start_pos": 150, "end_pos": 152, "type": "METRIC", "confidence": 0.9997134804725647}]}, {"text": "Most of the remaining errors are due to misclassifying English segments and named entities, which constitute a separate class in the shared task.", "labels": [], "entities": []}, {"text": "We focus in this paper on performing language identification between Pinyin and English, where Pinyin is the most widely used romanization schemes for Mandarin.", "labels": [], "entities": [{"text": "language identification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.682053953409195}]}, {"text": "It is the official standard in the People's Republic of China and in Singapore.", "labels": [], "entities": []}, {"text": "It is also the most widely used method for Mandarin speaking users to input Chinese characters using Latin-script keyboards.", "labels": [], "entities": []}, {"text": "Example 2 shows the same codeswitched sentence, in which the Chinese characters have been converted to Pinyin: (2) Zhege Thermal Exchanger de Thermal Conductivity taidi.", "labels": [], "entities": []}, {"text": "Distinguishing Pinyin from English or other languages written with the Roman alphabet is an important problem with strong practical motivations.", "labels": [], "entities": [{"text": "Distinguishing Pinyin", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8865795135498047}]}, {"text": "Learners of both English and Chinese could benefit from a system that allows them to input codeswitched text).", "labels": [], "entities": []}, {"text": "More generally, accurate Pinyin-English codeswitched language identification could allow users to input Mandarin-English codeswitched text more easily.", "labels": [], "entities": [{"text": "Pinyin-English codeswitched language identification", "start_pos": 25, "end_pos": 76, "type": "TASK", "confidence": 0.6367664933204651}]}, {"text": "A Chinese Input Method Editor (IME) system that detects Pinyin and converts it into the appropriate Chinese characters would save users from having to repeatedly toggle between the two languages when typing on a standard Latin-script keyboard.", "labels": [], "entities": []}, {"text": "Since Pinyin is written with the same character set as English 2 , character encoding is no longer a reliable indicator of language.", "labels": [], "entities": [{"text": "character encoding", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7273630201816559}]}, {"text": "For example, she, long, and bang are Pinyin syllables that are also English words.", "labels": [], "entities": []}, {"text": "Tisane is a English word, and is also a concatenation of three valid Pinyin syllables: ti, sa, and ne.", "labels": [], "entities": []}, {"text": "Thus, contextual information will be needed to resolve the identity of the language.", "labels": [], "entities": []}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}, {"text": "First, we construct two datasets of Pinyin-English codeswitched data by converting the Chinese characters in Mandarin-English codeswitched data sets to Pinyin, and propose anew task to distinguish Pinyin from English in this codeswitched text.", "labels": [], "entities": []}, {"text": "Then, we compare several approaches to solving this task.", "labels": [], "entities": []}, {"text": "We consider the level of performance when training the model at the level of words versus individual letters in order to see whether having word boundaries would affect performance.", "labels": [], "entities": []}, {"text": "Two standard classification methods, SVMs and linear-chain CRFs are compared for both settings.", "labels": [], "entities": []}, {"text": "We find that SVM produces better results on the word-level setting, achieving 99.3% F1 on a Weibo dataset.", "labels": [], "entities": [{"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9997515082359314}, {"text": "Weibo dataset", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.9745748341083527}]}, {"text": "CRF produces better results on the letter-level setting, achieving 98.2% F1 on the same dataset.", "labels": [], "entities": [{"text": "CRF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6370689868927002}, {"text": "F1", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9997739195823669}]}, {"text": "Lastly, we pass the output of our models to a system that converts Pinyin back to Chinese characters as an extrinsic evaluation.", "labels": [], "entities": []}, {"text": "The result shows that word-level models produce better conversion performance.", "labels": [], "entities": []}, {"text": "Our automatic conversion method achieves the same level of performance as an oracle system with perfect knowledge of token-level language identity.", "labels": [], "entities": []}, {"text": "This result demonstrates that Pinyin identification is not the bottleneck towards developing a Chinese-English codeswitched IME, and that future work should focus on the Pinyin-to-Chinese character conversion step.", "labels": [], "entities": [{"text": "Pinyin identification", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.884229451417923}, {"text": "Pinyin-to-Chinese character conversion", "start_pos": 170, "end_pos": 208, "type": "TASK", "confidence": 0.6640396217505137}]}], "datasetContent": [{"text": "We tested our models on the two codeswitching corpora that we created.", "labels": [], "entities": []}, {"text": "We split each corpus into training (80%) and testing (20%) subsets.", "labels": [], "entities": []}, {"text": "We also created a held-out development set by randomly sampling 100 entries from EMNLP corpus, and used it to select the feature sets described in Section 4.1.", "labels": [], "entities": [{"text": "EMNLP corpus", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.974929004907608}]}, {"text": "We kept the same set of features for the WEIBO corpus, without performing any additional tuning.", "labels": [], "entities": [{"text": "WEIBO corpus", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.9217731058597565}]}, {"text": "We trained the CRF model using CRFsuite) and the SVM model using Scikit-learn).", "labels": [], "entities": []}, {"text": "The models were tested using commonly defined evaluation measures -Precision, Recall and F1 (Powers, 2011) at the word level for WBMs and at the letter level for LBMs.", "labels": [], "entities": [{"text": "Precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.998538613319397}, {"text": "Recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9910686016082764}, {"text": "F1", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9986336827278137}]}, {"text": "Next, we experimented with converting the PinyinEnglish codeswitched inputs back to the original Mandarin-English sentences in an end-to-end, extrinsic evaluation.", "labels": [], "entities": []}, {"text": "Pinyin is the most widely used method for Mandarin users to input Chinese characters using Latin-script keyboards.", "labels": [], "entities": []}, {"text": "Improvements in the language identification step transfer over to the next step of Chinese characters generation.", "labels": [], "entities": [{"text": "language identification", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.7349256575107574}, {"text": "Chinese characters generation", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.5843638181686401}]}, {"text": "Converting Pinyin to Chinese characters is not an easy task, as there are many possible Chinese characters for each Pinyin syllable.", "labels": [], "entities": []}, {"text": "Modern Pinyin Input Methods use statistical methods to rank the possible character candidates in descending probability and predict the top-ranked candidate as the output).", "labels": [], "entities": []}, {"text": "Task Given a Pinyin-English codeswitched input and the corresponding labels produced by our codeswitched language identification models, produce Mandarin-English codeswitched output by converting the parts labelled as Pinyin to Chinese characters.", "labels": [], "entities": []}, {"text": "Method We use a representative approach that models the conversion from Pinyin to Chinese characters as a Hidden Markov model (HMM), in which Chinese characters are the hidden states and Pinyin syllables are the observations.", "labels": [], "entities": []}, {"text": "The model is trained from SogouT corpus (, and the Viterbi algorithm is used to generate the final output.", "labels": [], "entities": [{"text": "SogouT corpus", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9632284343242645}]}, {"text": "We used a Python implementation of this model to convert pinyin segments to Chinese characters while leaving others and non-pinyin segments unchanged.", "labels": [], "entities": []}, {"text": "We use the Pinyin-English codeswitched input, paired with language identification labels from Baseline, SVM-WBM, or CRF-LBM to generate Mandarin-English codeswitched output.", "labels": [], "entities": []}, {"text": "We then evaluated these outputs against the gold standard by measuring precision, recall, and F1 on the Chinese characters.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9997921586036682}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9996254444122314}, {"text": "F1", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9998219609260559}]}, {"text": "We also compare against an oracle topline, which has perfect knowledge of the segmentation of the input into Pinyin vs non-Pinyin.", "labels": [], "entities": []}, {"text": "For the CRF-LBM, we used the Smith-Waterman algorithm to align the output produced by the CRF-LBM method with the gold-standard words.", "labels": [], "entities": [{"text": "CRF-LBM", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.921623945236206}]}], "tableCaptions": [{"text": " Table 1: Frequency count of labels on EMNLP corpus", "labels": [], "entities": [{"text": "Frequency count", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9098847210407257}, {"text": "EMNLP", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9166982173919678}]}, {"text": " Table 2: Frequency count of labels on WEIBO corpus", "labels": [], "entities": [{"text": "Frequency count", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.93550243973732}, {"text": "WEIBO", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8624205589294434}]}, {"text": " Table 3: The performance of the models in terms of precision (P), recall (R), and F1, for each of the three classes. The avg/total", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.9465228319168091}, {"text": "recall (R)", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9631823599338531}, {"text": "F1", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.999804675579071}, {"text": "avg", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9862408638000488}]}]}