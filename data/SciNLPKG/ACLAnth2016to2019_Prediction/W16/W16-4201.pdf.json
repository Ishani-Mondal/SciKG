{"title": [{"text": "The impact of simple feature engineering in multilingual medical NER", "labels": [], "entities": [{"text": "NER", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.5592688918113708}]}], "abstractContent": [{"text": "The goal of this paper is to examine the impact of simple feature engineering mechanisms before applying more sophisticated techniques to the task of medical NER.", "labels": [], "entities": [{"text": "medical NER", "start_pos": 150, "end_pos": 161, "type": "TASK", "confidence": 0.5075279772281647}]}, {"text": "Sometimes papers using scientifically sound techniques present raw baselines that could be improved adding simple and cheap features.", "labels": [], "entities": []}, {"text": "This work focuses on entity recognition for the clinical domain for three languages: English, Swedish and Spanish.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8173488676548004}]}, {"text": "The task is tackled using simple features, starting from the window size, capitalization, prefixes, and moving to POS and semantic tags.", "labels": [], "entities": []}, {"text": "This work demonstrates that a simple initial step of feature engineering can improve the baseline results significantly.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7530182898044586}]}, {"text": "Hence, the contributions of this paper are: first, a shortlist of guidelines well supported with experimental results on three languages and, second, a detailed description of the relevance of these features for medical NER.", "labels": [], "entities": [{"text": "medical NER", "start_pos": 212, "end_pos": 223, "type": "TASK", "confidence": 0.48570914566516876}]}], "introductionContent": [{"text": "Named Entity Recognition (NER), such as the recognition of person names, organizations, locations or medical entities, has become a crucial task in any Natural Language Processing (NLP) application, as a first step to other types of processing as, for example, Relation Extraction (.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7863524208466212}, {"text": "recognition of person names, organizations, locations or medical entities", "start_pos": 44, "end_pos": 117, "type": "TASK", "confidence": 0.6694501313296232}, {"text": "Relation Extraction", "start_pos": 261, "end_pos": 280, "type": "TASK", "confidence": 0.8930114805698395}]}, {"text": "Several tools have been developed for this task, such as CRF++ (, SVM () or Perceptron.", "labels": [], "entities": []}, {"text": "Using these tools, and training them with a set of annotated data, many people can obtain a NER system easily and apply it to the respective domain.", "labels": [], "entities": []}, {"text": "In this paper the experiments will be performed with clinical texts, on the recognition of Medical entities such as disorder or drug brand names.", "labels": [], "entities": [{"text": "recognition of Medical entities such as disorder or drug brand names", "start_pos": 76, "end_pos": 144, "type": "TASK", "confidence": 0.6543849815021862}]}, {"text": "The basic NER models make use of a sequence of (word form, features, tag) elements for training.", "labels": [], "entities": []}, {"text": "For inference, the system will give the tag sequence with the highest score given anew text.", "labels": [], "entities": []}, {"text": "Each model is defined by a set of features, taken from the surroundings of each word to be tagged, usually by means of a sequential tagging approach.", "labels": [], "entities": []}, {"text": "Many techniques have been developed in order to improve the NER results, such as the incorporation of additional information, in the form of lemmatization, POS tagging, dictionaries and ontologies, or the inclusion of knowledge acquired by unsupervised techniques like Brown clusters (, word2vec neural models (Agerri and or deep neural network architectures (dos) that yielded significant improvements.", "labels": [], "entities": [{"text": "NER", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9705538749694824}, {"text": "POS tagging", "start_pos": 156, "end_pos": 167, "type": "TASK", "confidence": 0.7608330249786377}, {"text": "Agerri", "start_pos": 311, "end_pos": 317, "type": "DATASET", "confidence": 0.9480816125869751}]}, {"text": "However, this availability of tools and techniques has led to using only a limited set of predefined or standard models that were successful fora prototypical NER task, without any kind of time-consuming adjusting ().", "labels": [], "entities": [{"text": "NER task", "start_pos": 159, "end_pos": 167, "type": "TASK", "confidence": 0.9049737453460693}]}, {"text": "Moreover, as most published papers center on novel techniques, sometimes less effort is devoted to data analysis or to filtering and tuning the models.", "labels": [], "entities": []}, {"text": "Researchers rarely give the full details of feature engineering and they often present their best configurations, or otherwise they only study the impact of one or two specific types of feature.", "labels": [], "entities": []}, {"text": "However, the benefits of sophisticated techniques would be better highlighted taking a stronger baseline as departure.", "labels": [], "entities": []}, {"text": "In this sense, this paper maybe useful to researchers that are new to the field of medical NER, showing the impact of simple feature engineering on medical texts in three languages.", "labels": [], "entities": [{"text": "medical NER", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.531307190656662}]}, {"text": "As an example, looking at the systems presented at the Semeval 2014 Shared Task 7 on English Medical texts (, we see that most of the system descriptions do not give a precise 1 overview of the contribution of the simplest feature types ( and they give at most a list of the used features, but without a detailed account of each's performance.", "labels": [], "entities": [{"text": "Semeval 2014 Shared Task 7 on English Medical texts", "start_pos": 55, "end_pos": 106, "type": "DATASET", "confidence": 0.6133671237362756}]}, {"text": "For example, while describe word shape features, they do not describe the window of words used, while use a window of three words ().", "labels": [], "entities": []}, {"text": "There exist several available systems for English, as cTAKES (), which was used by some of the participants at Semeval 2014 or cLiner (.", "labels": [], "entities": []}, {"text": "However, for other types of languages, there is a scarcity of resources and information about the usefulness of the available features.", "labels": [], "entities": []}, {"text": "We will experiment the effect of using simple features on medical NER, giving a measure of the improvements that can be achieved without resorting to more sophisticated types of information.", "labels": [], "entities": [{"text": "NER", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.6976398229598999}]}, {"text": "Although most of these techniques have been previously applied in many works (), we think that their effectiveness has not always been clearly evaluated, and they are briefly described as a preprocessing step before applying other, more complex, techniques.", "labels": [], "entities": []}, {"text": "The main contribution of this paper will be a thorough examination of simple features for the recognition of entities in the medical domain.", "labels": [], "entities": []}, {"text": "To give a better account of the generalization across different languages, we will perform our experiments on English, Spanish and Swedish, hoping that these results will be useful for many researchers and will help them to follow the principle of doing the easy things first, before resorting to more complex models.", "labels": [], "entities": [{"text": "generalization", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.9729612469673157}]}], "datasetContent": [{"text": "We will perform a set of experiments using different types of features, starting from the most basic type of information, the word form itself and its derivatives, and continuing with basic language processing tools as lemmatization, POS tagging and medical dictionaries and ontologies: Phase 1: using only word forms (plus lower-casing); Phase 2: using prefixes and suffixes of different length.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 234, "end_pos": 245, "type": "TASK", "confidence": 0.7798311710357666}]}, {"text": "For example, the four letter suffix -itis indicates an inflamatory disease, as in meningitis or bronchitis; Phase 3: using different patterns of capitalization of word forms (word starts with a letter, all letters are capitalized, or different types of numbers); Phase 4: using lemmas; Phase 5: using POS tags.", "labels": [], "entities": []}, {"text": "Phase 6: using Snomed-CT tags.", "labels": [], "entities": []}, {"text": "The entire corpus was provided after anonymization, signing confidentiality agreements and passing the corresponding ethical committees.", "labels": [], "entities": []}, {"text": "From this set of raw clinical text, a subset of 121 texts was randomly selected for manual annotation (3,362 instances of diseases and 1,406 drugs).", "labels": [], "entities": []}, {"text": "\u2022 Swedish (SW) The Swedish clinical text 2 origins from patient records from over 500 different clinical units at Karolinska University Hospital.", "labels": [], "entities": []}, {"text": "The texts were collected during 2009-2010 and are stored in HEALTH BANK (.", "labels": [], "entities": [{"text": "HEALTH", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.7387979626655579}, {"text": "BANK", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.787356436252594}]}, {"text": "For this study, a supervised corpora was created, annotated with medical entities (4,000 entities corresponding to body parts, disorders and findings).", "labels": [], "entities": []}, {"text": "Regarding the English corpus, we only had access to the train and development sets, because the test set was not public.", "labels": [], "entities": [{"text": "English corpus", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.7326080352067947}]}, {"text": "This is not a problem, because from our experiments on the Semeval Shared Task datasets, the results on the test set increased by about 2 percent points (), as using the train and development sets for training compensates the effect of evaluating on the unseen test set.", "labels": [], "entities": [{"text": "Semeval Shared Task datasets", "start_pos": 59, "end_pos": 87, "type": "DATASET", "confidence": 0.5439122095704079}]}, {"text": "For that reason, we will use the train set for training and will evaluate on the development set.", "labels": [], "entities": []}, {"text": "For the experiments, we will use our own implementation of the averaged structured perceptron), a state of the art tagging model that relies on Viterbi decoding of training examples combined with simple additive updates.", "labels": [], "entities": []}, {"text": "The algorithm is competitive to maximumentropy taggers or CRFs).", "labels": [], "entities": []}, {"text": "The experiments were tested following a greedy approach, taking at each phase the best model in the previous phase as a baseline.", "labels": [], "entities": []}, {"text": "This approach can be debatable, as it could happen that the knowledge used in phase x+1 could not be useful when applied with the best model in phase x, but perhaps it produced improvements at phases earlier than x.", "labels": [], "entities": []}, {"text": "We have also experimented the effect of applying each set of features independently, but our aim is to get an account of the benefits obtained by applying a simple yet coherent approach (from the simplest to more elaborated experiments), and we leave out of the scope of this work the development of more time-consuming tests, such as grid search.", "labels": [], "entities": [{"text": "grid search", "start_pos": 335, "end_pos": 346, "type": "TASK", "confidence": 0.9214687049388885}]}, {"text": "shows the results (F-measure) with different values of the window size (WS).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9948333501815796}, {"text": "window size (WS)", "start_pos": 59, "end_pos": 75, "type": "METRIC", "confidence": 0.6460313320159912}]}, {"text": "There is no use on trying a single WS for all the languages as it has different impacts on different languages.", "labels": [], "entities": []}, {"text": "Note that lowercasing improved the results considerably for all three languages, specially for Spanish.", "labels": [], "entities": []}, {"text": "We hypothesize that this can be due to the informal writing used in the Spanish medical reports, characterized by big differences in writing style and non-consistent use of casing (either lowercase, uppercase or mixed).", "labels": [], "entities": []}, {"text": "The use of prefix/suffixes in Phase 2 (see) helps significantly for all the languages with respect to the best results from Phase 1 (above 5 absolute points in all cases).", "labels": [], "entities": []}, {"text": "Lower casing does not seem useful for English and Swedish (0.5 improvement for English over the best result without lower casing, and no improvement for Swedish), but it gives an increase of 2 points on Spanish.", "labels": [], "entities": []}, {"text": "presents the effect of adding features to represent capitalization patterns (words formed only by capital letters and words that start with a capital letter) and number types 3 . The improvements are modest for Swedish and slightly better for English (adding 0.8 points) and Spanish (almost one point).", "labels": [], "entities": []}, {"text": "Using lemmatization, shows that we get an improvement on English (+0.3) and a decrease for Spanish and Swedish.", "labels": [], "entities": []}, {"text": "This seems surprising, as in principle lemmatization could be useful to normalize terms (e.g. singular/plural and feminine/masculine in Spanish).", "labels": [], "entities": []}, {"text": "Note that, as we are performing a greedy approach, the number of features used grows from one phase to the next one, and this is the main reason why we limited the number of feature templates, because the gains are decreasing for each phase.", "labels": [], "entities": []}, {"text": "It should be clear that our experiments do not conclude that lemmatization is not useful but, rather, they show that it is not useful after applying other features.", "labels": [], "entities": []}, {"text": "shows the results using POS features, helpful for English and Spanish, but not for Swedish.", "labels": [], "entities": []}, {"text": "We hypothesize that it could be due to the poorer quality of the Swedish POS tagger (.", "labels": [], "entities": [{"text": "Swedish POS tagger", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.8102887670199076}]}, {"text": "Finally, presents the results using specialized medical dictionaries, giving the best results for English and Swedish, but no improvement for Spanish.", "labels": [], "entities": []}, {"text": "This aspect deserves further work, because the Spanish Snomed has similar coverage to the English version regarding concepts (around 300,000), but less terms (660,000 compared to 480,000).", "labels": [], "entities": [{"text": "Spanish Snomed", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9093796610832214}]}], "tableCaptions": [{"text": " Table 1: Results changing the window size and  capitalization of words (wf(i, j) = unigram fea- tures of words in a window from i to j).", "labels": [], "entities": []}, {"text": " Table 2: Results adding prefixes and suffixes  of word forms, using the best model of phase 1  as baseline. (pN 1 N 2 ...N k = prefix of size N 1 ,  N 2 , ... N k for the current word).", "labels": [], "entities": []}, {"text": " Table 3: Results adding capitalization and  numbers to the best model of phase 2.", "labels": [], "entities": []}, {"text": " Table 4: Results adding features based on lem- mas (on the best model of phase 3) (lem(i, j) =  unigram features of lemmas in a window from  i to j).", "labels": [], "entities": []}, {"text": " Table 5: Results adding features based on POS  tags on the best model of the previous phase.", "labels": [], "entities": []}, {"text": " Table 6: Results adding features based on  Snomed tags.", "labels": [], "entities": [{"text": "Snomed tags", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.8277562260627747}]}]}