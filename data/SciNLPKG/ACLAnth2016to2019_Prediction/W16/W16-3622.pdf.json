{"title": [{"text": "A Context-aware Natural Language Generator for Dialogue Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contex-tually appropriate responses.", "labels": [], "entities": []}, {"text": "The generator is based on recurrent neural networks and the sequence-to-sequence approach.", "labels": [], "entities": []}, {"text": "It is fully trainable from data which include preceding context along with responses to be generated.", "labels": [], "entities": []}, {"text": "We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pair-wise preference test.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina conversation, speakers are influenced by previous utterances of their counterparts and tend to adapt (align, entrain) their way of speaking to each other, reusing lexical items as well as syntactic structure).", "labels": [], "entities": []}, {"text": "Entrainment occurs naturally and subconsciously, facilitates successful conversations, and forms a natural source of variation in dialogues.", "labels": [], "entities": []}, {"text": "In spoken dialogue systems (SDS), users were reported to entrain to system prompts (.", "labels": [], "entities": []}, {"text": "The function of natural language generation (NLG) components in task-oriented SDS typically is to produce a natural language sentence from a dialogue act (DA) () representing an action, such as inform or request, along with one or more attributes (slots) and their values (see).", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.8324114382266998}]}, {"text": "NLG is an important component of SDS which has a great impact on the perceived naturalness of the system; its quality can also influence the overall task success  NLG systems in SDS only take the input DA into account and have noway of adapting to the user's way of speaking.", "labels": [], "entities": [{"text": "SDS", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.968366801738739}]}, {"text": "To avoid repetition and add variation into the outputs, they typically alternate between a handful of preset variants) or use overgeneration and random sampling from a k-best list of outputs ().", "labels": [], "entities": [{"text": "repetition", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9348092675209045}]}, {"text": "There have been several attempts at introducing entrainment into NLG in SDS, but they are limited to rule-based systems (see Section 4).", "labels": [], "entities": [{"text": "SDS", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.7596297860145569}]}, {"text": "We present a novel, fully trainable contextaware NLG system for SDS that is able to entrain to the user and provides naturally variable outputs because generation is conditioned not only on the input DA, but also on the preceding user utterance (see).", "labels": [], "entities": [{"text": "SDS", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9514164328575134}]}, {"text": "Our system is an extension of's generator based on sequence-to-sequence (seq2seq) models with attention (.", "labels": [], "entities": []}, {"text": "It is, to our knowledge, the first fully trainable entrainment-enabled NLG system for SDS.", "labels": [], "entities": [{"text": "SDS", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9758275747299194}]}, {"text": "We also present our first results on the dataset of, which includes the preceding user utterance along with each data instance (i.e., pair of input meaning representation and output sentence), and we show that our context-aware system outperforms the baseline in both automatic metrics and a human pairwise preference test.", "labels": [], "entities": []}, {"text": "In the following, we first present the architecture of our generator (see Section 2), then give an account of our experiments in Section 3.", "labels": [], "entities": []}, {"text": "We include a brief survey of related work in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 contains concluding remarks and plans for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment on the publicly available dataset of for NLG in the pub-lic transport information domain, which includes preceding context along with each pair of input DA and target natural language sentence.", "labels": [], "entities": []}, {"text": "It contains over 5,500 utterances, i.e., three paraphrases for each of the over 1,800 combinations of input DA and context user utterance.", "labels": [], "entities": []}, {"text": "The data concern bus and subway connections on Manhattan, and comprise four DA types (iconfirm, inform, inform no match, request).", "labels": [], "entities": []}, {"text": "They are delexicalized for generation to avoid sparsity, i.e., stop names, vehicles, times, etc., are replaced by placeholders ().", "labels": [], "entities": []}, {"text": "We applied a 3:1:1 split of the set into training, development, and test data.", "labels": [], "entities": []}, {"text": "We use the three paraphrases as separate instances in training data, but they serve as three references fora single generated output in validation and evaluation.", "labels": [], "entities": []}, {"text": "We test the three context-aware setups described in Section 2.2 and their combinations, and we compare them against the baseline noncontext-aware seq2seq generator.", "labels": [], "entities": []}, {"text": "Same as, we train the seq2seq models by minimizing cross-entropy on the training set using the Adam optimizer (, and we measure BLEU on the development set after each pass over the training data, selecting the best-performing parameters.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9987941980361938}]}, {"text": "The content classification reranker is trained in a similar fashion, measuring misclassification on both training and development set after each pass.", "labels": [], "entities": [{"text": "content classification reranker", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7592038412888845}]}, {"text": "We use 5 difcontext_nlg_dataset), which contains several small fixes.", "labels": [], "entities": []}, {"text": "Based on our preliminary experiments on development data, we use embedding size 50, LSTM cell size 128, learning rate 0.0005, and batch size 20.", "labels": [], "entities": [{"text": "learning rate 0.0005", "start_pos": 104, "end_pos": 124, "type": "METRIC", "confidence": 0.925332248210907}]}, {"text": "Training is run for at least 50 and up to 1000 passes, with early stopping if the top 10 validation BLEU scores do not change for 100 passes.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 60, "end_pos": 74, "type": "METRIC", "confidence": 0.8668556213378906}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9464900493621826}]}, {"text": "We use the same settings except for the number of passes over the training data, which is at least 20 and 100 at most.", "labels": [], "entities": []}, {"text": "For validation, development set is given 10 times more importance than the training set.", "labels": [], "entities": [{"text": "validation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9752389788627625}]}, {"text": "ferent random initializations of the networks and average the results.", "labels": [], "entities": []}, {"text": "Decoding is run with abeam size of 20 and the penalty weight for content classification reranker set to 100.", "labels": [], "entities": [{"text": "Decoding", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7022435665130615}]}, {"text": "We set the n-gram match reranker weight based on experiments on development data.", "labels": [], "entities": []}, {"text": "lists our results on the test data in terms of the BLEU and NIST metrics ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.996712327003479}, {"text": "NIST metrics", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.8452534377574921}]}, {"text": "We can see that while the n-gram match reranker brings a BLEU score improvement, using context prepending or separate encoder results in scores lower than the baseline.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9687969088554382}]}, {"text": "However, using the n-gram match reranker together with context prepending or separate encoder brings significant improvements of about 2.8 BLEU points in both cases, better than using the n-gram match reranker alone.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9992839694023132}]}, {"text": "We believe that adding the context information into the decoder does increase the chances of contextually appropriate outputs appearing on the decoder kbest lists, but it also introduces a lot more uncertainty and therefore, the appropriate outputs may not end on top of the list based on decoder scores alone.", "labels": [], "entities": []}, {"text": "The n-gram match reranker is then able to promote the relevant outputs to the top of the k-best list.", "labels": [], "entities": []}, {"text": "However, if the generator itself does not have access to context information, the n-gram match reranker has a smaller effect as contextually appropriate outputs may not appear on the k-best lists at all.", "labels": [], "entities": []}, {"text": "A closer look at the generated outputs confirms that entrainment is present in sentences generated by the context-aware setups (see).", "labels": [], "entities": []}, {"text": "In addition to BLEU and NIST scores, we measured the slot error rate ERR (), i.e., the proportion of missing or superfluous slot placeholders in the delexicalized generated outputs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9960152506828308}, {"text": "NIST", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.7454188466072083}, {"text": "slot error rate ERR", "start_pos": 53, "end_pos": 72, "type": "METRIC", "confidence": 0.8347456157207489}]}, {"text": "For all our setups, ERR stayed around 3%.", "labels": [], "entities": [{"text": "ERR", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9987576007843018}]}, {"text": "We evaluated the best-performing setting based on BLEU/NIST scores, i.e., prepending context with n-gram match reranker, in a blind pairwise preference test with untrained judges recruited on the CrowdFlower crowdsourcing platform.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9954800605773926}, {"text": "NIST", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.7910086512565613}]}, {"text": "The judges were given the context and the system output for the baseline and the context-aware system, and they were asked to pick the variant that sounds more natural.", "labels": [], "entities": []}, {"text": "We used a random sample of 1,000 pairs of different system outputs overall 5 random initializations of the networks, and collected 3 judgments for each of them.", "labels": [], "entities": []}, {"text": "The judges preferred the context-aware system output in 52.5% cases, significantly more than the baseline.", "labels": [], "entities": []}, {"text": "We examined the judgments in more detail and found three probable causes for the rather small difference between the setups.", "labels": [], "entities": []}, {"text": "First, both setups' outputs fit the context relatively well in many cases and the judges tend to prefer the overall more frequent variant (e.g., for the context \"starting from Park Place\", the output \"Where do you want to go?\" is preferred over \"Where are you going to?\").", "labels": [], "entities": []}, {"text": "Second, the context-aware setup often selects a shorter response that fits the context well (e.g., \"Is there an option at 10:00 am?\" is confirmed simply with \"At 10:00 am.\"), but the judges seem to prefer the more eloquent variant.", "labels": [], "entities": []}, {"text": "And third, both setups occasionally produce non-fluent outputs, which introduces a certain amount of noise.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU and NIST scores of different gen- erator setups on the test data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985779523849487}, {"text": "NIST", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.6743451356887817}]}]}