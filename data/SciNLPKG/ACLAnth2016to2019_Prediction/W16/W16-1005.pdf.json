{"title": [{"text": "Event Nugget and Event Coreference Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we describe the event nugget annotation created in support of the pilot Event Nugget Detection evaluation in 2014 and in support of the Event Nugget Detection and Co-reference open evaluation in 2015, which was one of the Knowledge Base Population tracks within the NIST Text Analysis Conference.", "labels": [], "entities": [{"text": "Event Nugget Detection evaluation", "start_pos": 87, "end_pos": 120, "type": "TASK", "confidence": 0.6470363289117813}, {"text": "Event Nugget Detection and Co-reference open evaluation in 2015", "start_pos": 151, "end_pos": 214, "type": "TASK", "confidence": 0.742621617184745}, {"text": "NIST Text Analysis Conference", "start_pos": 281, "end_pos": 310, "type": "DATASET", "confidence": 0.7586878389120102}]}, {"text": "We present the data volume annotated for both training and evaluation data for the 2015 evaluation as well as changes to annotation in 2015 as compared to that of 2014.", "labels": [], "entities": []}, {"text": "We also analyze the annotation for the 2015 evaluation as an example to show the annotation challenges and consistency, and identify the event types and subtypes that are most difficult for human an-notators.", "labels": [], "entities": []}, {"text": "Finally, we discuss annotation issues that we need to take into consideration in the future.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Text Analysis Conference (TAC) is a series of workshops organized by the National Institute of Standards and Technology (NIST), aiming to encourage research in natural language processing (NLP).", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC)", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.858679880698522}, {"text": "natural language processing (NLP)", "start_pos": 164, "end_pos": 197, "type": "TASK", "confidence": 0.7428665161132812}]}, {"text": "The Knowledge Base Population (KBP) tracks of TAC encourage the development of systems that can match entities mentioned in natural texts with those appearing in a knowledge base and extract information about entities from a document collection and add it to anew or existing knowledge base.", "labels": [], "entities": []}, {"text": "Starting in 2014, TAC KBP added a track for event evaluation.", "labels": [], "entities": [{"text": "TAC KBP", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.8624436855316162}, {"text": "event evaluation", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7339073419570923}]}, {"text": "The goal of the TAC KBP Event track is to extract information about events such that the information would be suitable as input to a knowledge base.", "labels": [], "entities": [{"text": "TAC KBP Event track", "start_pos": 16, "end_pos": 35, "type": "DATASET", "confidence": 0.6477176398038864}]}, {"text": "Event Nugget (EN) evaluation, as one of the evaluation tasks in the TAC KBP event track, aims to evaluate system performance on EN detection and EN coreference ).", "labels": [], "entities": [{"text": "Event Nugget (EN) evaluation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6034923096497854}, {"text": "TAC KBP event track", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.8848489224910736}, {"text": "EN detection", "start_pos": 128, "end_pos": 140, "type": "TASK", "confidence": 0.7240178883075714}]}, {"text": "An event nugget, as defined by the task, includes a text extent that instantiates an event, a classification of event type and subtype, and an indication of the realis of the event.", "labels": [], "entities": []}, {"text": "This is different from nuggets) and the Summary Content Units (SCU) described in () where nuggets and SCU are units of meaning (usually in the form of a sentential clause) which not only includes the main verb/word that instantiates an event, but also all arguments.", "labels": [], "entities": []}, {"text": "In this paper, we describe the event nugget annotation to support the pilot EN Detection evaluation in 2014 and in support of the EN Detection and Coreference TAC KBP open evaluation in 2015.", "labels": [], "entities": [{"text": "EN Detection evaluation", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.5628882745901743}, {"text": "EN Detection and Coreference TAC KBP open evaluation", "start_pos": 130, "end_pos": 182, "type": "TASK", "confidence": 0.557093907147646}]}, {"text": "We discuss changes to the annotation in 2015 as compared to that of 2014 and present the data volume annotated for both training and evaluation data for the 2015 evaluation.", "labels": [], "entities": []}, {"text": "We analyze the annotation for the 2015 evaluation data to show the annotation and consistency challenges, identify the event types and subtypes that are most difficult for human annotators, and finally discuss annotation issues that we need to take into consideration in the future.", "labels": [], "entities": [{"text": "consistency", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.9440305233001709}]}], "datasetContent": [{"text": "The EN evaluation was introduced in 2014 and run as a pilot evaluation, which sought to serve two purposes.", "labels": [], "entities": []}, {"text": "One was to measure event detection by performers' systems in the Deep Exploration and Filtering of Text (DEFT) program of the Defense Advanced Research Projects Agency.", "labels": [], "entities": [{"text": "event detection", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7220370024442673}]}, {"text": "The program aims to address remaining capability gaps in state-of-the-art natural language processing technologies related to inference, causal relationships and anomaly detection.", "labels": [], "entities": [{"text": "anomaly detection", "start_pos": 162, "end_pos": 179, "type": "TASK", "confidence": 0.7686055898666382}]}, {"text": "The other purpose of the pilot, however, was to test run the evaluation framework before opening it up to the full TAC KBP community.", "labels": [], "entities": [{"text": "TAC KBP community", "start_pos": 115, "end_pos": 132, "type": "DATASET", "confidence": 0.84345809618632}]}, {"text": "The Pilot EN evaluation had one evaluation task: \uf0b7 EN Detection: Participating systems must identify all relevant Event Mention instances in English texts, categorize each mention's Event types/subtype and identify its realis value (ACTUAL, GENERIC, or OTHER).", "labels": [], "entities": [{"text": "\uf0b7 EN Detection", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.553316573301951}, {"text": "ACTUAL", "start_pos": 233, "end_pos": 239, "type": "METRIC", "confidence": 0.8745095729827881}, {"text": "OTHER", "start_pos": 253, "end_pos": 258, "type": "METRIC", "confidence": 0.8726212978363037}]}, {"text": "As the pilot was considered a success, EN was added to the roster of full-fledged evaluation tracks for TAC KBP 2015, with some modifications to incorporate lessons learned from the pilot and to better align with the other TAC KBP 2015 event-related evaluations -Event Argument Linking (EAL) -and also the Entities, Relations and Events (ERE) data provided to KBP participants for training purposes.", "labels": [], "entities": [{"text": "EN", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.7623077034950256}, {"text": "TAC KBP 2015", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.7583017150561014}, {"text": "TAC KBP 2015 event-related evaluations -Event Argument Linking (EAL)", "start_pos": 223, "end_pos": 291, "type": "TASK", "confidence": 0.6442501445611318}]}, {"text": "The EN evaluation in TAC KBP 2015 included two new tasks in addition to EN Detection: \uf0b7 EN Detection and Coreference: Participating systems must identify not only the event nugget, but also full event coreference links.", "labels": [], "entities": [{"text": "TAC KBP 2015", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.7814310193061829}]}, {"text": "Full Event Coreference is identified when two or more event nuggets refer to the same event.", "labels": [], "entities": [{"text": "Full Event Coreference", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6501316726207733}]}, {"text": "\uf0b7 EN Coreference: Participating systems must identify full event coreference links, given the annotated event nuggets in the text.", "labels": [], "entities": [{"text": "\uf0b7 EN Coreference", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.7672012050946554}]}, {"text": "ERE was developed as an annotation task that would be supportive of multiple research directions and evaluations in the DEFT program, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly.", "labels": [], "entities": [{"text": "DEFT program", "start_pos": 120, "end_pos": 132, "type": "DATASET", "confidence": 0.874035507440567}]}, {"text": "As compared to EN annotation in 2014, Rich ERE Event annotation and 2015 EN annotation include increased taggability in several areas: slightly expanded event ontology, additional attributes for contact and transaction events, and double tagging of event mentions for multiple types/subtypes and for certain types of coordination, in addition to event coreference.General Instructions  In this section, we present the training and evaluation data annotated to support the EN evaluation in 2015.", "labels": [], "entities": []}, {"text": "Source data for the 2015 EN evaluation was a subset of the documents selected for EAL evaluation, which had been manually selected to ensure coverage of all event types and subtypes for that evaluation.", "labels": [], "entities": [{"text": "EAL evaluation", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.8368480205535889}]}, {"text": "Tokenization of the source documents was also provided.", "labels": [], "entities": []}, {"text": "Unlike the 2014 data, in which annotation was performed on pre-tokenized text, in 2015 tokenization was performed as a post-annotation procedure, using tool kits provided by evaluation coordinators.", "labels": [], "entities": []}, {"text": "The EN annotation team consisted of nine annotators, six of whom were also adjudicators, and care was taken to ensure that annotators did not adjudicate their own files.", "labels": [], "entities": []}, {"text": "Following adjudication of all documents, a corpus-wide quality control pass was also performed.", "labels": [], "entities": []}, {"text": "In this pass, annotators manually scanned a list of all event triggers to review event type and subtype values and all event hoppers to make sure that event mentions in the same hopper have same type and subtype value.", "labels": [], "entities": []}, {"text": "All identified outliers were then manually reviewed in context, and corrected if needed.", "labels": [], "entities": []}, {"text": "The evaluation data set consists of 202 documents with a total word count of 88,652.", "labels": [], "entities": [{"text": "evaluation data set", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7502683599789938}]}, {"text": "The gold standard annotation has a total of 6,438 event nuggets and 4,125 event hoppers in total.", "labels": [], "entities": []}, {"text": "shows the profile of the evaluation dataset.", "labels": [], "entities": []}, {"text": "Appendix 1 shows the distribution of each typesubtype in the evaluation data.", "labels": [], "entities": []}, {"text": "Conflict.Attack has the highest representation (591 event nuggets) while Business.EndOrg has the lowest count (6).", "labels": [], "entities": []}, {"text": "With the two newly added contact subtypes (Contact and Broadcast), there are altogether 1,491 contact event nuggets (23%), with 1,101 Contact.Contact and Contact.Broadcast combined (17%).", "labels": [], "entities": []}, {"text": "Each event nugget is labeled with one of the three realis attributes: actual, generic and other.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Event Nugget counts by realis attributes in NW  and DF genres.", "labels": [], "entities": []}]}