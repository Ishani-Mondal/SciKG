{"title": [{"text": "DeepNNNER: Applying BLSTM-CNNs and Extended Lexicons to Named Entity Recognition in Tweets", "labels": [], "entities": [{"text": "DeepNNNER", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8889445662498474}, {"text": "Named Entity Recognition in Tweets", "start_pos": 56, "end_pos": 90, "type": "TASK", "confidence": 0.7591958403587341}]}], "abstractContent": [{"text": "In this paper, we describe the DeepNNNER entry to The 2nd Workshop on Noisy User-generated Text (WNUT) Shared Task #2: Named Entity Recognition in Twitter.", "labels": [], "entities": [{"text": "2nd Workshop on Noisy User-generated Text (WNUT) Shared Task #2: Named Entity Recognition", "start_pos": 54, "end_pos": 143, "type": "TASK", "confidence": 0.5431123849223641}]}, {"text": "Our shared task submission adopts the bidirectional LSTM-CNN model of Chiu and Nichols (2016), as it has been shown to perform well on both newswire and Web texts.", "labels": [], "entities": []}, {"text": "It uses word embeddings trained on large-scale Web text collections together with text normalization to cope with the diversity in Web texts, and lexicons for target named entity classes constructed from publicly-available sources.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7046271711587906}]}, {"text": "Extended evaluation comparing the effectiveness of various word embeddings, text normalization, and lexicon settings shows that our system achieves a maximum F1-score of 47.24, performance surpassing that of the shared task's second-ranked system.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7459934651851654}, {"text": "F1-score", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9991706609725952}]}], "introductionContent": [{"text": "Named entity recognition (NER) is an important part of natural language processing.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8176498115062714}, {"text": "natural language processing", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6552340785662333}]}, {"text": "It is a challenging task that requires robust recognition to detect common entities over a large variety of expressions and vocabularies.", "labels": [], "entities": []}, {"text": "These problems are intensified when targeting Web texts because of challenges such as differences in spelling and punctuation conventions, neologisms, and Web markup (.", "labels": [], "entities": []}, {"text": "Traditional approaches to NER on newswire texts has been dominated by machine learning methods that rely heavily on manual feature engineering and external knowledge sources.", "labels": [], "entities": [{"text": "NER", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9930744171142578}]}, {"text": "Recently, neural network models -especially those that use recursive models -have shown that state of the art performance can be achieved with little feature engineering.", "labels": [], "entities": []}, {"text": "However, despite their popularity for NER on newswire texts, neural networks have not been widely adopted for NER on Web texts, with the exception of the feed-forward neural network (FFNN) model of.", "labels": [], "entities": [{"text": "NER on newswire texts", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.8063526898622513}, {"text": "NER", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9533302187919617}]}, {"text": "In this paper, we present the DeepNNNER entry to the WNUT 2016 Shared Task #2: Named Entity Recognition in Twitter.", "labels": [], "entities": [{"text": "WNUT 2016 Shared Task #2: Named Entity Recognition", "start_pos": 53, "end_pos": 103, "type": "TASK", "confidence": 0.763939517736435}]}, {"text": "Our shared task submission is based on the model of, a hybrid model of bidirectional long short-term memory (BLSTM) networks and convolutional neural networks (CNN) that automatically learns both character-and word-level features, and which holds the current state-of-the-art on both newswire texts) and diverse corpora including Web texts (OntoNotes 5.0).", "labels": [], "entities": []}, {"text": "In contrast to CRFs, FFNNs, and other windowed models, the BLSTM gives our model effectively infinite context on both sides of a word during sequential labeling.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.5674620270729065}]}, {"text": "The character-level CNN allows our model to learn relevant features from the orthography of words, which is important in task where unseen words are commonplace.", "labels": [], "entities": []}, {"text": "Finally, it also encodes partial lexicon matches in neural networks, allowing it to make effective use of lexical knowledge.", "labels": [], "entities": []}, {"text": "Our primary contribution is adapting the model of to Twitter data by developing a text normalization method to effectively apply word embeddings to large vocabulary Web texts and automatically constructing lexicons for the shared task's target NE classes from publicly-available sources.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7109587341547012}]}, {"text": "The rest of our paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the adaptations made to's model.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the evaluation methodology.", "labels": [], "entities": []}, {"text": "In Section 4, we discuss the results and present an error analysis.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 52, "end_pos": 66, "type": "METRIC", "confidence": 0.93851637840271}]}, {"text": "In Section 5, we summarize related research.", "labels": [], "entities": []}, {"text": "Finally, in Section 6, we give concluding remarks.", "labels": [], "entities": []}, {"text": "Figure 1: Our proposed system architecture for NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.717738926410675}]}, {"text": "Feature embeddings are constructed following Section 2.1.", "labels": [], "entities": []}, {"text": "The output from both the forward and the backward LSTM are fed through a linear and a log-softmax layer before being added together (shown as \"Output layers\") to produce the tag scores.", "labels": [], "entities": []}], "datasetContent": [{"text": "The WNUT 2016 dataset consists of user-generated tweets tagged with 10 types of named entities: company, facility, geo-loc, movie, musicartist, other, person, product, sportsteam, and tvshow.", "labels": [], "entities": [{"text": "WNUT 2016 dataset consists of user-generated tweets tagged with 10 types of named entities: company, facility, geo-loc, movie, musicartist, other, person, product, sportsteam", "start_pos": 4, "end_pos": 178, "type": "Description", "confidence": 0.7702110959216952}]}, {"text": "shows the train, dev and test set data splits.", "labels": [], "entities": []}, {"text": "Compared to the well-researched or the OntoNotes 5.0 dataset (, the WNUT dataset contains a lot of spelling irregularities and special symbols.", "labels": [], "entities": [{"text": "OntoNotes 5.0 dataset", "start_pos": 39, "end_pos": 60, "type": "DATASET", "confidence": 0.8965834180514017}, {"text": "WNUT dataset", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9767400026321411}]}, {"text": "For example, Christmas written as xmas, Guys written as Gaiiissss, emoticons such as \":-)\", \":(\", \"<3\"     and soon are commonplace.", "labels": [], "entities": []}, {"text": "Such examples illustrate the diversity of the dataset's vocabulary, motivating us to perform text normalization as described in Section 2.2.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7396423369646072}]}, {"text": "Some inconsistencies were found between Dev2 and the other data.", "labels": [], "entities": [{"text": "Dev2", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.912176251411438}]}, {"text": "The most obvious one is where singers previously tagged as person in Train were tagged as musicartist in Dev2.", "labels": [], "entities": []}, {"text": "This is easily verifiable by comparing tags for the entity Justin Bieber in those datasets.", "labels": [], "entities": []}, {"text": "These tag inconsistencies make it difficult to learn a robust model for those classes, so we manually retagged all person entities, keeping the most precise tag (i.e. tagging all singers as musicartist).", "labels": [], "entities": []}, {"text": "We did so by searching for every person entity with Google and used the surrounding context to determine the most precise tag, replacing a total of 82 person entities out of 664.", "labels": [], "entities": []}, {"text": "Other local inconsistencies were not corrected as not enough evidence was found.", "labels": [], "entities": []}, {"text": "In Section 4.3.2 we explore inconsistencies in common tagging errors.", "labels": [], "entities": []}, {"text": "For each experiment, we report the average for precision and recall, and the average and standard deviation for f1-score for 10 successful trials.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9996398687362671}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9992973804473877}, {"text": "f1-score", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9243601560592651}]}, {"text": "Minor inconsistencies in reported f1-scores and precision and recall result from those scores being averaged independently.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9994858503341675}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9993170499801636}]}, {"text": "Statistical significance is calculated using the Wilcoxon rank sum test, due to its robustness against small sample sizes with unknown distributions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: A comparison of word embedding type and token coverage with and without text normalization.", "labels": [], "entities": []}, {"text": " Table 4: The WNUT 2016 dataset.", "labels": [], "entities": [{"text": "WNUT 2016 dataset", "start_pos": 14, "end_pos": 31, "type": "DATASET", "confidence": 0.8826768398284912}]}, {"text": " Table 5: Dev1 preliminary evaluation.", "labels": [], "entities": []}, {"text": " Table 6: F1-scores for our submitted, fixed, and  best 10 tag models. Rank* is the retroactive rank.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9989954829216003}]}, {"text": " Table 7: F1-scores with different word embed- dings evaluated on test set with final settings.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.998921275138855}]}, {"text": " Table 8: Per-category comparison of our fixed shared task submission settings with and without lexicons.", "labels": [], "entities": []}]}