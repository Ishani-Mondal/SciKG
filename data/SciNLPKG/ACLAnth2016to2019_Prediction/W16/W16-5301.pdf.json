{"title": [{"text": "Vectors or Graphs? On Differences of Representations for Distributional Semantic Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional Semantic Models (DSMs) have recently received increased attention, together with the rise of neural architectures for scalable training of dense vector embeddings.", "labels": [], "entities": [{"text": "Distributional Semantic Models (DSMs)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7728420446316401}]}, {"text": "While some of the literature even includes terms like 'vectors' and 'dimensionality' in the definition of DSMs, there are some good reasons why we should consider alternative formulations of distributional models.", "labels": [], "entities": []}, {"text": "As an instance, I present a scalable graph-based solution to distributional semantics.", "labels": [], "entities": []}, {"text": "The model belongs to the family of 'count-based' DSMs, keeps its representation sparse and explicit, and thus fully interpretable.", "labels": [], "entities": []}, {"text": "I will highlight some important differences between sparse graph-based and dense vector approaches to DSMs: while dense vector-based models are computationally easier to handle and provide a nice uniform representation that can be compared and combined in many ways, they lack interpretability, provenance and robustness.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 102, "end_pos": 106, "type": "TASK", "confidence": 0.9668316841125488}]}, {"text": "On the other hand, graph-based sparse models have a more straightforward interpretation, handle sense distinctions more naturally and can straightforwardly be linked to knowledge bases, while lacking the ability to compare arbitrary lexical units and a compositionality operation.", "labels": [], "entities": []}, {"text": "Since both representations have their merits, I opt for exploring their combination in the outlook.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rooted in Structural Linguistics (, Distributional Semantic Models (DSMs, see e.g. () characterize the meaning of lexical units by the contexts they appear in, cf..", "labels": [], "entities": []}, {"text": "Using the duality of form and contexts, forms can be compared along their contexts, giving rise to the field of Statistical Semantics.", "labels": [], "entities": [{"text": "Statistical Semantics", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.881216287612915}]}, {"text": "A data-driven, unsupervised approach to representing word meaning is attractive as there is no need for laborious creation of lexical resources.", "labels": [], "entities": [{"text": "representing word meaning", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.8418808182080587}]}, {"text": "Further, these approaches naturally adapt to the domain or even language at hand.", "labels": [], "entities": []}, {"text": "Desirable, in general, is a model that provides a firm basis fora wider range of (semantic) tasks, as opposed to specialised solutions on a per-task basis.", "labels": [], "entities": []}, {"text": "While most approaches to distributional semantics rely on dense vector representations, the reasons for this seem rather technical than well-justified.", "labels": [], "entities": []}, {"text": "To de-bias the discussion, I propose a competitive graph-based formulation.", "labels": [], "entities": []}, {"text": "Since all representations have advantages and disadvantages, I will discuss some ways of how to fruitfully combine graphs and vectors in the future.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}