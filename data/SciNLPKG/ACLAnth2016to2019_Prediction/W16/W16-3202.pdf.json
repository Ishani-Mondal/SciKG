{"title": [{"text": "Combining Lexical and Spatial Knowledge to Predict Spatial Relations between Objects in Images", "labels": [], "entities": []}], "abstractContent": [{"text": "Explicit representations of images are useful for linguistic applications related to images.", "labels": [], "entities": []}, {"text": "We design a representation based on first-order models that capture the objects present in an image as well as their spatial relations.", "labels": [], "entities": []}, {"text": "We take a supervised learning approach to the spatial relation classification problem and study the effects of spatial and lexical information on prediction performance.", "labels": [], "entities": [{"text": "spatial relation classification", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.6808244287967682}]}, {"text": "We find that lexical information is required to accurately predict spatial relations when combined with location information, achieving an F-score of 0.80, compared to a most-frequent-class baseline of 0.62.", "labels": [], "entities": [{"text": "F-score", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.999599039554596}]}], "introductionContent": [{"text": "In the light of growing amount of digital image data, methods for automatically linking data to language area great asset.", "labels": [], "entities": []}, {"text": "Due to recent advances in the distinct areas of language technology and computer vision, research combining the two fields has become increasingly popular, including automatical generation of captions) and translation of text into visual scenes.", "labels": [], "entities": [{"text": "automatical generation of captions)", "start_pos": 166, "end_pos": 201, "type": "TASK", "confidence": 0.8235267281532288}, {"text": "translation of text into visual scenes", "start_pos": 206, "end_pos": 244, "type": "TASK", "confidence": 0.8532577355702718}]}, {"text": "One task which has not yet been extensively researched is the automatic derivation of rich abstract representations from images (.", "labels": [], "entities": []}, {"text": "A formal representation of an image goes beyond naming the objects that are present; it can also account for some of the structure of the visual scene by including spatial relations between objects.", "labels": [], "entities": []}, {"text": "This information could enhance the interface between language and vision.", "labels": [], "entities": []}, {"text": "Imagine, for instance, searching for images that show a \"man riding a bicycle\": it is necessary, but not sufficient, for pictures to contain both a man and a bicycle.", "labels": [], "entities": []}, {"text": "In order to satisfy the query, the man also has to be somehow connected to the bicycle, with his feet on the pedals and his hands on the steering bar.", "labels": [], "entities": []}, {"text": "We argue that representations of images which take into account spatial relations can enable more sophisticated interactions between language and vision that go beyond basic object co-occurrence.", "labels": [], "entities": []}, {"text": "The aim of this paper is to use an extension of first-order models to represent images of real situations.", "labels": [], "entities": []}, {"text": "In order to obtain such models, we need (a) high-quality, broad-coverage object localisation and identification and methods to (b) accurately determine object characteristics and to (c) detect spatial relationships between objects.", "labels": [], "entities": []}, {"text": "As broad-coverage object detection systems are not yet available, we carryout steps (a) and (b) manually.", "labels": [], "entities": [{"text": "broad-coverage object detection", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6321122546990713}]}, {"text": "Hence, in this paper, we focus on step (c): the detection of spatial relations.", "labels": [], "entities": []}, {"text": "This is difficult because there is avast number of ways in which a given relation can be realised in a visual scene.", "labels": [], "entities": []}, {"text": "The questions that we want to answer are whether first-order models of classical logic are appropriate to represent images, and what features are suitable for detecting spatial relationships between objects in images.", "labels": [], "entities": [{"text": "detecting spatial relationships between objects in images", "start_pos": 159, "end_pos": 216, "type": "TASK", "confidence": 0.806899334703173}]}, {"text": "In particular, we want to investigate what the impact of lexical knowledge is on determining spatial relations, independent of the quality of object recognition.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.7373572587966919}]}, {"text": "This paper is organised as follows.", "labels": [], "entities": []}, {"text": "We will first give more background about spatial relations (Section 2) and related work on combining vision with language technology (Section 3).", "labels": [], "entities": []}, {"text": "Then we will introduce our data set in Section 4, comprising a hundred images with a total of 583 located objects for which spatial relations need to be determined.", "labels": [], "entities": []}, {"text": "In Section 5 we outline our classification method in detail and present and discuss our results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distribution of class labels in training and  testing data.", "labels": [], "entities": [{"text": "Distribution of class labels", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8224388659000397}]}, {"text": " Table 3: Summary of results on training data  (overall F-scores).", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.6761800050735474}, {"text": "F-scores", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9919186234474182}]}, {"text": " Table 4: Summary of results on unseen test data  (overall F-scores).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9869508147239685}]}, {"text": " Table 5: Confusion matrix for subtask A, using  feature groups 1, 2, 3 and 5.", "labels": [], "entities": []}, {"text": " Table 6: Confusion matrix for subtask B, using  feature groups 1, 2, 3 and 9.", "labels": [], "entities": []}]}