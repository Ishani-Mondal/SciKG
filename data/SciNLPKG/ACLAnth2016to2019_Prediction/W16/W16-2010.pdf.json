{"title": [{"text": "MED: The LMU System for the SIGMORPHON 2016 Shared Task on Morphological Reinflection", "labels": [], "entities": [{"text": "Morphological Reinflection", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.6926749348640442}]}], "abstractContent": [{"text": "This paper presents MED, the main system of the LMU team for the SIGMOR-PHON 2016 Shared Task on Morphological Reinflection as well as an extended analysis of how different design choices contribute to the final performance.", "labels": [], "entities": [{"text": "MED", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9407920241355896}, {"text": "SIGMOR-PHON 2016 Shared Task on Morphological Reinflection", "start_pos": 65, "end_pos": 123, "type": "TASK", "confidence": 0.7140292695590428}]}, {"text": "We model the task of morphological reinflec-tion using neural encoder-decoder models together with an encoding of the input as a single sequence of the morphological tags of the source and target form as well as the sequence of letters of the source form.", "labels": [], "entities": []}, {"text": "The Shared Task consists of three sub-tasks, three different tracks and covers 10 different languages to encourage the use of language-independent approaches.", "labels": [], "entities": []}, {"text": "MED was the system with the overall best performance , demonstrating our method generalizes well for the low-resource setting of the SIGMORPHON 2016 Shared Task.", "labels": [], "entities": [{"text": "MED", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.737428605556488}, {"text": "SIGMORPHON 2016 Shared Task", "start_pos": 133, "end_pos": 160, "type": "TASK", "confidence": 0.5637228488922119}]}], "introductionContent": [{"text": "In many areas of natural language processing (NLP) it is important that systems are able to correctly analyze and generate different morphological forms, including previously unseen forms.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.8315187692642212}]}, {"text": "Two examples are machine translation and question answering, where errors in the understanding of morphological forms can seriously harm performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8374683856964111}, {"text": "question answering", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9297592639923096}]}, {"text": "Accordingly, learning morphological inflection patterns from labeled data is an important challenge.", "labels": [], "entities": []}, {"text": "The task of morphological inflection (MI) consists of generating an inflected form fora given lemma and target tag.", "labels": [], "entities": [{"text": "morphological inflection (MI)", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.803840720653534}]}, {"text": "Several approaches have been developed for this, including machine learning models and models that exploit the paradigm structure of language (.", "labels": [], "entities": []}, {"text": "A more complex problem is morphological reinflection (MRI).", "labels": [], "entities": [{"text": "morphological reinflection (MRI)", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.7823827505111695}]}, {"text": "For this, an inflected form has to be found given another inflected form, a target tag and optionally a source tag.", "labels": [], "entities": []}, {"text": "We use the same approach to both MI and MRI: the character-based and language independent sequence-to-sequence attention model called MED, which stands for Morphological EncoderDecoder.", "labels": [], "entities": [{"text": "MI", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9745461940765381}]}, {"text": "To solve the MRI task, we train one single model on all available source-to-target mappings for each language contained in the training set.", "labels": [], "entities": [{"text": "MRI", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9900525212287903}]}, {"text": "This enables the encoder-decoder to learn good parameters for relatively small amounts of training data per target tag already, because most MRI patterns occur in many source-target tag pairs.", "labels": [], "entities": []}, {"text": "In our model design, what is learned for one pair can be transferred to others.", "labels": [], "entities": []}, {"text": "The most important point for this is the representation we use for MRI.", "labels": [], "entities": [{"text": "MRI", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9122112393379211}]}, {"text": "We encode the input as a single sequence of (i) the morphological tags of the source form, (ii) the morphological tags of the target form and (iii) the sequence of letters of the source form.", "labels": [], "entities": []}, {"text": "The output is the sequence of letters of the target form.", "labels": [], "entities": []}, {"text": "We train a single generic encoder-decoder per language on this representation that can handle all tag pairs, thus making it possible to make efficient use of the available training data.", "labels": [], "entities": []}, {"text": "The SIGMORPHON 2016 Shared Task on Morphological Reinflection covers both, MI and MRI, for 10 languages as well as different settings and MED outperforms all other systems on all subtasks.", "labels": [], "entities": []}, {"text": "The given languages, tracks and tasks will be explained briefly now.", "labels": [], "entities": []}, {"text": "For further details on the Shared Task please refer to.", "labels": [], "entities": [{"text": "Shared Task", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.8067137002944946}]}, {"text": "In total, the Shared Task covers 10 languages: Arabic, Finnish, Georgian, German, Hungarian, Maltese, Navajo, Russian, Spanish and Turkish.", "labels": [], "entities": []}, {"text": "The training and development datasets for Hungarian and Maltese were only released at evaluation time.", "labels": [], "entities": []}, {"text": "The Shared Task consists of 3 separate tasks with increasing difficulty: task 1 is supposed to be the easiest and task 3 the hardest.", "labels": [], "entities": []}, {"text": "The first task consists of mapping a given lemma and target tag to a target form.", "labels": [], "entities": []}, {"text": "Task 2 requires the mapping of a given source form, source tag and target tag to a target form.", "labels": [], "entities": []}, {"text": "Finally, task 3 consists of finding a target form fora given source form and source tag only.", "labels": [], "entities": []}, {"text": "The Shared Task is split into 3 tracks that differ in the information available.", "labels": [], "entities": []}, {"text": "The first track is the standard track and requires the solution for each task to use only the training and development data of the current and all lower-numbered tasks, e.g., to use only the data for tasks 1 and 2 for task 2.", "labels": [], "entities": []}, {"text": "The restricted track limits the available training and development data to the data belonging to the current task, i.e., data from lower tasks cannot be used, making it impossible to reduce task 2 to task 1 or task 3 to task 2.", "labels": [], "entities": []}, {"text": "Track 3 is the bonus track.", "labels": [], "entities": []}, {"text": "In this track, all available data per language can be used, including unlabeled corpora which are provided by the task organizers.", "labels": [], "entities": []}, {"text": "However, those vary a lot in length, depending on the language.", "labels": [], "entities": []}, {"text": "Therefore, we do not make use of them.", "labels": [], "entities": []}, {"text": "In total, there are 90 combinations of languages, tasks and tracks to solve.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: In Section 2, our model for the SIGMOR-PHON 2016 Shared Task is presented.", "labels": [], "entities": [{"text": "SIGMOR-PHON 2016 Shared Task", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.6689877212047577}]}, {"text": "Next, our method to preprocess and thus extend the training data is explained in detail.", "labels": [], "entities": []}, {"text": "In Section 4 the final results on the test data of the Shared Task are presented and discussed.", "labels": [], "entities": []}, {"text": "Afterwards, we analyze the contribution of different settings and components to the overall performance of our system in detail.", "labels": [], "entities": []}, {"text": "Finally, in Section 6, we give information about prior work on topics related to our system.", "labels": [], "entities": []}, {"text": "This paper is mainly concerned with the implementation and analysis of the system we submitted to the Shared Task.", "labels": [], "entities": []}, {"text": "In (, we instead focus on the novel aspects of our new method MED and compare its performance to prior work on other MRI benchmarks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of training samples for task 2 without (given) and with the training data enhancer (restricted and standard  track) together with the factor by which the size of the training set increased. Note that the samples for task 2 in the standard  track are the same as the samples for task 1 in the bonus track.", "labels": [], "entities": []}, {"text": " Table 2: Exact-match accuracy per language for the standard  track of the SIGMORPHON 2016 Shared Task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9267109632492065}, {"text": "SIGMORPHON 2016 Shared Task", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.5872537344694138}]}, {"text": " Table 3: Exact-match accuracy per language for the restricted  track of the SIGMORPHON 2016 Shared Task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9373114109039307}, {"text": "SIGMORPHON 2016 Shared Task", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.6185672730207443}]}, {"text": " Table 5: Performance of MED for different numbers of hid- den units in the encoder and decoder.", "labels": [], "entities": [{"text": "MED", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.7408763766288757}]}, {"text": " Table 6: Performance of MED for different embedding di- mensions in the encoder and decoder.", "labels": [], "entities": []}, {"text": " Table 8: Exact match accuracy for the standard representa- tion (MED) as well as the representation with one embedding  per tag combination (MED-tag-comb) per language. The last  column shows the number of samples that contain tag combi- nations that appear in dev but not in train, either for the source  or the target form.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9153705835342407}]}, {"text": " Table 9: Performance of MED when training on samples with  tags in always the same order (MED) and samples where the  tags are permuted inside each combination (MED-perm).", "labels": [], "entities": [{"text": "MED", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8810061812400818}]}]}