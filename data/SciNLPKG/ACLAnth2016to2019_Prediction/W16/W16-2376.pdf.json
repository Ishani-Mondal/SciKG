{"title": [{"text": "Word Clustering Approach to Bilingual Document Alignment (WMT 2016 Shared Task)", "labels": [], "entities": [{"text": "Bilingual Document Alignment", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.8251737554868063}, {"text": "WMT 2016 Shared Task", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.8628421723842621}]}], "abstractContent": [{"text": "Our participation in Bilingual Document Alignment shared task at WMT16 focuses on building a language-independent, scal-able system for aligning documents based on content as opposed to using webpage meta information.", "labels": [], "entities": [{"text": "Bilingual Document Alignment shared task", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.9147943377494812}, {"text": "WMT16", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.7918274402618408}]}, {"text": "The resulting system is capable of producing scored n-best lists of candidate pages and can therefore be adapted to tasks where either precision or recall is maximized.", "labels": [], "entities": [{"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9992383718490601}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.995739221572876}]}, {"text": "We conduct a series of experiments that show the effectiveness of the system without any specific tuning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Training statistical machine translation systems involves using two kinds of textual data: monoand bilingual.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 9, "end_pos": 40, "type": "TASK", "confidence": 0.6312639713287354}]}, {"text": "While mining monolingual data is rather straightforward, determining pairs of parallel documents is a rather complicated task fora variety of reasons.", "labels": [], "entities": []}, {"text": "First of all, the largest source of text documents -the World Wide Web -has most of its parallel data in an unstructured form, meaning that it is often impossible to determine parallel pairs using meta info only.", "labels": [], "entities": []}, {"text": "While a set of documents within a particular webdomain maybe structured, the structure itself varies between domains and is therefore hard to exploit.", "labels": [], "entities": []}, {"text": "This lack of stucture in the Web forces a mining system to compare every source language document to every target language document from the corpus, thus leading to quadratic complexity and making such straightforward algorithms not applicable to mining parallel data from large web corpora containing billions of documents.", "labels": [], "entities": []}, {"text": "Existing parallel data mining approaches deal with these problems in different ways.", "labels": [], "entities": [{"text": "data mining", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.7551981210708618}]}, {"text": "Methods focused on meta info such as document URL, publication dates or document structure, may work well on small structured corpora but suffer from sparsity and unreliability of meta info in the Web.", "labels": [], "entities": []}, {"text": "One of the advantages of such methods is a lesser computational complexity -simple URL matching, for example, can be performed in linear time and doesn't even require to store HTML bodies as it only operates on URLs.", "labels": [], "entities": [{"text": "URL matching", "start_pos": 83, "end_pos": 95, "type": "TASK", "confidence": 0.7734513282775879}]}, {"text": "Another approach is to analyze document contents only, making zero assumptions about the document structure or meta info.", "labels": [], "entities": []}, {"text": "This approach is more versatile but at the same time more resoursedemanding and tends to suffer from bad scalability.", "labels": [], "entities": []}, {"text": "Applying it to big Web corpora requires implementation of special techniques that reduce the quadratic complexity of a naive algorithm to something manageable, preferrably making the number of document comparisons linear.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Reference document positions and n-best  recall on the train and test data sets.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9164878726005554}, {"text": "train and test data sets", "start_pos": 65, "end_pos": 89, "type": "DATASET", "confidence": 0.8103979349136352}]}, {"text": " Table 2: Quality on the training set using different  metrics.", "labels": [], "entities": []}]}