{"title": [{"text": "Translation of Patent Sentences with a Large Vocabulary of Technical Terms Using Neural Machine Translation", "labels": [], "entities": [{"text": "Translation of Patent Sentences", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8874872028827667}, {"text": "Neural Machine Translation", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.7479528586069742}]}], "abstractContent": [{"text": "Neural machine translation (NMT), anew approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.770622784892718}, {"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7628761231899261}, {"text": "statistical machine translation (SMT)", "start_pos": 157, "end_pos": 194, "type": "TASK", "confidence": 0.7813562353452047}]}, {"text": "Despite its recent success, NMT cannot handle a larger vocabulary because training complexity and decoding complexity proportionally increase with the number of target words.", "labels": [], "entities": []}, {"text": "This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently.", "labels": [], "entities": []}, {"text": "In NMTs, words that are out of vocabulary are represented by a single unknown token.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method that enables NMT to translate patent sentences comprising a large vocabulary of technical terms.", "labels": [], "entities": [{"text": "translate patent sentences", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.861097494761149}]}, {"text": "We train an NMT system on bilingual data wherein technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms.", "labels": [], "entities": []}, {"text": "Further, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 151, "end_pos": 154, "type": "TASK", "confidence": 0.8583017587661743}]}, {"text": "We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT score and that of the NMT rescoring of the translated sentences with technical term tokens.", "labels": [], "entities": [{"text": "SMT translations", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.8959546685218811}, {"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9360155463218689}, {"text": "NMT", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.8290591835975647}]}, {"text": "Our experiments on Japanese-Chinese patent sentences show that the proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over traditional SMT systems and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9976398944854736}, {"text": "SMT", "start_pos": 185, "end_pos": 188, "type": "TASK", "confidence": 0.977767288684845}, {"text": "BLEU", "start_pos": 237, "end_pos": 241, "type": "METRIC", "confidence": 0.9954290390014648}]}], "introductionContent": [{"text": "Neural machine translation (NMT), anew approach to solving machine translation, has achieved promising results).", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7736158122618993}, {"text": "solving machine translation", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6687756379445394}]}, {"text": "An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation.", "labels": [], "entities": []}, {"text": "The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus.", "labels": [], "entities": []}, {"text": "Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies.", "labels": [], "entities": []}, {"text": "This is because the training complexity and decoding complexity proportionally increase with the number of target words.", "labels": [], "entities": []}, {"text": "Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in.", "labels": [], "entities": []}, {"text": "The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms.", "labels": [], "entities": []}, {"text": "There have been a number of related studies that address the vocabulary limitation of NMT systems.", "labels": [], "entities": []}, {"text": "provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system.", "labels": [], "entities": []}, {"text": "proposed annotating the occurrences of a target unknown word token with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy.", "labels": [], "entities": []}, {"text": "proposed to replace out-of-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data.", "labels": [], "entities": []}, {"text": "introduced an effective approach based on encoding rare and unknown words as sequences of subword units.", "labels": [], "entities": []}, {"text": "provided a character-level: Example of translation errors when translating patent sentences with technical terms using NMT and word-level hybrid NMT model to achieve an open vocabulary, and proposed a NMT system based on character-based embeddings.", "labels": [], "entities": []}, {"text": "However, these previous approaches have limitations when translating patent sentences.", "labels": [], "entities": [{"text": "translating patent sentences", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.8964775800704956}]}, {"text": "This is because their methods only focus on addressing the problem of unknown words even though the words are parts of technical terms.", "labels": [], "entities": []}, {"text": "It is obvious that a technical term should be considered as one word that comprises components that always have different meanings and translations when they are used alone.", "labels": [], "entities": []}, {"text": "An example is shown in Figure1, wherein Japanese word \" \"(bridge) should be translated to Chinese word \" \" when included in technical term \"bridge interface\"; however, it is always translated as \" \".", "labels": [], "entities": []}, {"text": "In this paper, we propose a method that enables NMT to translate patent sentences with a large vocabulary of technical terms.", "labels": [], "entities": []}, {"text": "We use an NMT model similar to that used by, which uses a deep long short-term memories (LSTM)) to encode the input sentence and a separate deep LSTM to output the translation.", "labels": [], "entities": []}, {"text": "We train the NMT model on a bilingual corpus in which the technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms.", "labels": [], "entities": []}, {"text": "Similar to, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 154, "end_pos": 191, "type": "TASK", "confidence": 0.7668406963348389}]}, {"text": "We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT and NMT scores of the translated sentences that have been rescored with the technical term tokens.", "labels": [], "entities": [{"text": "SMT translations", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.908324658870697}, {"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.8378674387931824}]}, {"text": "Our experiments on Japanese-Chinese patent sentences show that our proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over a traditional SMT system and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9975556135177612}, {"text": "SMT", "start_pos": 187, "end_pos": 190, "type": "TASK", "confidence": 0.9729712605476379}, {"text": "BLEU", "start_pos": 238, "end_pos": 242, "type": "METRIC", "confidence": 0.9951321482658386}]}], "datasetContent": [{"text": "We calculated automatic evaluation scores for the translation results using two popular metrics: BLEU () and RIBES ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9988456964492798}, {"text": "RIBES", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.992138683795929}]}, {"text": "As shown in, we report the evaluation scores, on the basis of the translations by Moses (, as the baseline SMT and the scores based on translations produced by the equivalent NMT system without our proposed approach as the baseline NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.8924834132194519}]}, {"text": "As shown in, the two versions of the proposed NMT systems clearly improve the translation quality when compared with the baselines.", "labels": [], "entities": []}, {"text": "When compared with the baseline SMT, the performance gain of the proposed system is approximately 3.1 BLEU points if translations are produced by the proposed NMT system of Section 4.3 or 2.3 RIBES points if translations are produced by the proposed NMT system of Section 4.2.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9823935031890869}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9994325041770935}, {"text": "NMT system of Section 4.3", "start_pos": 159, "end_pos": 184, "type": "DATASET", "confidence": 0.8139010071754456}, {"text": "RIBES", "start_pos": 192, "end_pos": 197, "type": "METRIC", "confidence": 0.9905224442481995}, {"text": "NMT system of Section 4.2", "start_pos": 250, "end_pos": 275, "type": "DATASET", "confidence": 0.8588495373725891}]}, {"text": "When compared with the result of decoding with the baseline NMT, the proposed NMT system of Section 4.2 achieved performance gains of 0.8 RIBES points.", "labels": [], "entities": [{"text": "NMT", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.8943158984184265}, {"text": "RIBES", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9865856170654297}]}, {"text": "When compared with the result of reranking with the baseline NMT, the proposed NMT system of Section 4.3 can still achieve performance gains of 0.6 BLEU points.", "labels": [], "entities": [{"text": "NMT", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.9322209358215332}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9991822838783264}]}, {"text": "Moreover, when the output translations produced by NMT decoding and SMT technical term translation described in Section 4.2 with the output translations produced by decoding with the baseline NMT, the number of unknown tokens included in output translations reduced from 191 to 92.", "labels": [], "entities": [{"text": "SMT technical term translation", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.8823675662279129}]}, {"text": "About 90% of remaining unknown tokens correspond to numbers, English words, abbreviations, and symbols.", "labels": [], "entities": []}, {"text": "In this study, we also conducted two types of human evaluation according to the work of: pairwise evaluation and JPO adequacy evaluation.", "labels": [], "entities": []}, {"text": "During the procedure of pairwise eval- uation, we compare each of translations produced by the baseline SMT with that produced by the two versions of the proposed NMT systems, and judge which translation is better, or whether they are with comparable quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9608224034309387}]}, {"text": "The score of pairwise evaluation is defined by the following formula, where Wis the number of better translations compared to the baseline SMT, L the number of worse translations compared to the baseline SMT, and T the number of translations having their quality comparable to those produced by the baseline SMT: The score of pairwise evaluation ranges from \u2212100 to 100.", "labels": [], "entities": []}, {"text": "In the JPO adequacy evaluation, Chinese translations are evaluated according to the quality evaluation criterion for translated patent documents proposed by the Japanese Patent Office (JPO).", "labels": [], "entities": []}, {"text": "The JPO adequacy criterion judges whether or not the technical factors and their relationships included in Japanese patent sentences are correctly translated into Chinese, and score Chinese translations on the basis of the percentage of correctly translated information, where the score of 5 means all of those information are translated correctly, while that of 1 means most of those information are not translated correctly.", "labels": [], "entities": []}, {"text": "The score of the JPO adequacy evaluation is defined as the average over the whole test sentences.", "labels": [], "entities": []}, {"text": "Unlike the study conducted, we randomly selected 200 sentence pairs from the test set for human evaluation, and both human evaluations were conducted using only one judgement.", "labels": [], "entities": []}, {"text": "shows the results of the human evaluation for the baseline SMT, the baseline NMT, and the proposed NMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9060788154602051}]}, {"text": "We observed that the proposed system achieved the best performance for both pairwise evaluation and JPO adequacy evaluation when we replaced technical term tokens with SMT technical term translations after decoding the source sentence with technical term tokens.", "labels": [], "entities": [{"text": "JPO adequacy evaluation", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.6848324338595072}, {"text": "SMT technical term translations", "start_pos": 168, "end_pos": 199, "type": "TASK", "confidence": 0.9021032154560089}]}, {"text": "Throughout 7, we show an identical source Japanese sentence and each of its translations produced by the two versions of the proposed NMT systems, compared with translations produced by the three baselines, respectively.", "labels": [], "entities": [{"text": "NMT systems", "start_pos": 134, "end_pos": 145, "type": "DATASET", "confidence": 0.9030537605285645}]}, {"text": "shows an example of correct translation produced by the proposed system in comparison to that produced by the baseline SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9517292380332947}]}, {"text": "In this example, our model correctly translates the Japanese sentence into Chinese, whereas the translation by the baseline SMT is a translation error with several erroneous syntactic structures.", "labels": [], "entities": []}, {"text": "As shown in, the second example highlights that the proposed NMT system of Section 4.2 can correctly translate the Japanese technical term \" \"(laminated wafer) to the Chinese technical term \" \".", "labels": [], "entities": [{"text": "NMT system of Section 4.2", "start_pos": 61, "end_pos": 86, "type": "DATASET", "confidence": 0.7932485461235046}]}, {"text": "The translation by the baseline NMT is a translation error because of not only the erroneously translated unknown token but also the Chinese word \" \", which is not appropriate as a component of a Chinese technical term.", "labels": [], "entities": [{"text": "NMT", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.9328420162200928}]}, {"text": "Another example is shown in, where we compare the translation of a reranking SMT 1,000-best translation produced by the proposed NMT system with that produced by reranking with the baseline NMT.", "labels": [], "entities": [{"text": "SMT 1,000-best translation", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.7956247925758362}]}, {"text": "It is interesting to observe that compared with the baseline NMT, we obtain a better translation when we rerank the 1,000-best SMT translations using the proposed NMT system, in which technical term tokens represent technical terms.", "labels": [], "entities": [{"text": "NMT", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.9066017866134644}, {"text": "SMT translations", "start_pos": 127, "end_pos": 143, "type": "TASK", "confidence": 0.9157219231128693}]}, {"text": "It is mainly because the correct Chinese translation \" \"(wafter) of Japanese word \" \" is out of the 40K NMT vocabulary (Chinese), causing reranking with the baseline NMT to produce the translation with an erroneous construction of \"noun phrase of noun phrase of noun phrase\".", "labels": [], "entities": [{"text": "NMT vocabulary", "start_pos": 104, "end_pos": 118, "type": "DATASET", "confidence": 0.854211300611496}, {"text": "NMT", "start_pos": 166, "end_pos": 169, "type": "DATASET", "confidence": 0.9578976035118103}]}, {"text": "As shown in, the proposed NMT system of Section 4.3 produced the translation with a correct construction, mainly because Chinese word \" \"(wafter) is apart of Chinese technical term \" \"(laminated wafter) and is replaced with a technical term token and then rescored by the NMT model (with technical term tokens \"T T 1 \", \"T T 2 \", . . .).", "labels": [], "entities": [{"text": "NMT system of Section 4.3", "start_pos": 26, "end_pos": 51, "type": "DATASET", "confidence": 0.8480520486831665}]}], "tableCaptions": [{"text": " Table 1: Automatic evaluation results", "labels": [], "entities": []}, {"text": " Table 2: Human evaluation results (the score of pairwise evaluation ranges from \u2212100 to 100 and the  score of JPO adequacy evaluation ranges from 1 to 5)", "labels": [], "entities": []}]}