{"title": [{"text": "Towards Deep Learning in Hindi NER: An approach to tackle the Labelled Data Scarcity", "labels": [], "entities": [{"text": "Deep Learning in Hindi NER", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.5732586860656739}]}], "abstractContent": [{"text": "In this paper we describe an end to end Neural Model for Named Entity Recognition (NER) which is based on Bi-Directional RNN-LSTM.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.8081798950831095}]}, {"text": "Almost all NER systems for Hindi use Language Specific features and handcrafted rules with gazetteers.", "labels": [], "entities": []}, {"text": "Our model is language independent and uses no domain specific features or any handcrafted rules.", "labels": [], "entities": []}, {"text": "Our models rely on semantic information in the form of word vectors which are learnt by an un-supervised learning algorithm on an unan-notated corpus.", "labels": [], "entities": []}, {"text": "Our model attained state of the art performance in both English and Hindi without the use of any morphological analysis or without using gazetteers of any sort.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity recognition (NER) is a very important task in Natural Language Processing.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.820936769247055}]}, {"text": "In the NER task, the objective is to find and cluster named entities in text into any desired categories such as person names (PER), organizations (ORG), locations (LOC), time expressions, etc.", "labels": [], "entities": [{"text": "NER task", "start_pos": 7, "end_pos": 15, "type": "TASK", "confidence": 0.9112818837165833}]}, {"text": "NER is an important precursor to tasks like Machine Translation, Question Answering , Topic Modelling and Information Extraction among others.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7339004278182983}, {"text": "Machine Translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8604888916015625}, {"text": "Question Answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.8667226731777191}, {"text": "Topic Modelling", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.8814521729946136}, {"text": "Information Extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7483532428741455}]}, {"text": "Various methods have been used in the past for NER including Hidden Markov models, Conditional Random fields, Feature engineering approaches using Support Vector Machines, Max Entropy classifiers for finally classifying outputs and more recently neural network based approaches.", "labels": [], "entities": [{"text": "NER", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9801733493804932}]}, {"text": "Development of an NER system for Indian languages is a comparatively difficult task.", "labels": [], "entities": [{"text": "NER", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9583032131195068}]}, {"text": "Hindi and * * indicates these authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "many other Indian languages provide some inherent difficulties in many NLP related tasks.", "labels": [], "entities": [{"text": "NLP related tasks", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.901896059513092}]}, {"text": "The structure of the languages contain many complexities like free-word ordering (which affect n-gram based approaches significantly), no capitalization information and its inflectional nature (affecting hand-engineered approaches significantly).", "labels": [], "entities": []}, {"text": "Also, in Indian languages there are many word constructions that can be classified as Named Entities (Derivational/Inflectional constructions) etc and these constraints on these constructions vary from language to language hence carefully crafted rules need to be made for each language which is a very time consuming and expensive task.", "labels": [], "entities": []}, {"text": "Another major problem in Indian languages is the fact that we have scarce availability of annotated data for indian languages.", "labels": [], "entities": []}, {"text": "The task is hard for rule-based NLP tools, and the scarcity of labelled data renders many of the statistical approaches like Deep Learning unusable.", "labels": [], "entities": []}, {"text": "This complexity in the task is a significant challenge to solve.", "labels": [], "entities": []}, {"text": "Can we develop tools which can generalize to other languages(unlike rule based approaches) but still can perform well on this task?", "labels": [], "entities": []}, {"text": "On the other hand, RNNs and its variants have consistently performed better than other approaches on English NER and many other sequence labelling tasks.", "labels": [], "entities": [{"text": "sequence labelling tasks", "start_pos": 128, "end_pos": 152, "type": "TASK", "confidence": 0.7295316557089487}]}, {"text": "We believe RNN would be a very effective method compared to fixedwindow approaches as the memory cell takes much larger parts of the sentence into context thus solving the problem of sentences being freely ordered to a large extent.", "labels": [], "entities": []}, {"text": "We propose a method to be able to model the NER task using RNN based approaches using the unsupervised data available and achieve good improvements in accuracies over many other models without any hand-engineered features or any rule-based approach.", "labels": [], "entities": [{"text": "NER task", "start_pos": 44, "end_pos": 52, "type": "TASK", "confidence": 0.9298160374164581}, {"text": "accuracies", "start_pos": 151, "end_pos": 161, "type": "METRIC", "confidence": 0.964423418045044}]}, {"text": "We would learn word-vectors that capture a large number of precise semantic and syntactic word relationships from a large unlabelled corpus and use them 154 to initialize RNNs thus allowing us to leverage the capabilities of RNNs on the currently available data.", "labels": [], "entities": []}, {"text": "We believe to the best of our knowledge, that this is the first approach capable of using RNN for NER in Hindi data.", "labels": [], "entities": [{"text": "NER in Hindi data", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.8543761521577835}]}, {"text": "We believe learning based approaches like these could generalize to other Indian languages without having to handcraft features or develop dependence on other NLP related tools.", "labels": [], "entities": []}, {"text": "Our model uses no language specific features or gazetteers or dictionaries.", "labels": [], "entities": []}, {"text": "We use a small amount of supervised training data along with some unannotated corpus for training word embeddings yet we achieve accuracies on par with the state of the art results on the CoNLL 2003 dataset for English and achieve 77.48% accuracy on ICON 2013 NLP tools corpus for Hindi language.", "labels": [], "entities": [{"text": "CoNLL 2003 dataset", "start_pos": 188, "end_pos": 206, "type": "DATASET", "confidence": 0.9320630431175232}, {"text": "accuracy", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.9992766976356506}, {"text": "ICON 2013 NLP tools corpus", "start_pos": 250, "end_pos": 276, "type": "DATASET", "confidence": 0.9377404570579528}]}, {"text": "Our paper is mainly divided into the following sections: \u2022 In Section 1 we begin with an introduction to the task of NER and briefly describe our approach.", "labels": [], "entities": [{"text": "NER", "start_pos": 117, "end_pos": 120, "type": "TASK", "confidence": 0.9496957063674927}]}, {"text": "\u2022 In Section 2, we mention the issues with hindi NER and provide an overview of the past approaches to NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9531306028366089}]}, {"text": "\u2022 In Section 3, we descibe our proposed RNN based approach to the task of NER and the creation of word embeddings for NER which are at the core of our model.", "labels": [], "entities": [{"text": "NER", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.8966960906982422}]}, {"text": "\u2022 In Section 4 We explain our experimental setup, describe the dataset for both Hindi and English and give results and observations of testing on both the datasets.", "labels": [], "entities": []}, {"text": "\u2022 In Section 5 We give our conclusions from the experiments and also describe methods to extend our approach to other languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform extensive experimentation to validate our methodology.", "labels": [], "entities": []}, {"text": "We have described the datasets we use and the experimental setup in detail in this section.", "labels": [], "entities": []}, {"text": "We then present our results and provide a set of observations made for those results.", "labels": [], "entities": []}, {"text": "We test the effectiveness of our approach on ICON 2013 NLP tools contest dataset for Hindi language, along with cross-validating our methodology on the well-established CoNLL 2003 English named entity recognition dataset () .  We used the ICON 2013 NLP tools contest dataset to evaluate our models on Hindi.", "labels": [], "entities": [{"text": "ICON 2013 NLP tools contest dataset", "start_pos": 45, "end_pos": 80, "type": "DATASET", "confidence": 0.9372506539026896}, {"text": "CoNLL 2003 English named entity recognition dataset", "start_pos": 169, "end_pos": 220, "type": "DATASET", "confidence": 0.8532390253884452}, {"text": "ICON 2013 NLP tools contest dataset", "start_pos": 239, "end_pos": 274, "type": "DATASET", "confidence": 0.9328923126061758}]}, {"text": "The dataset contains words annotated with part-of-speech (POS) tags and corresponding named entity labels in Shakti Standard Form (SSF) format () . The dataset primarily contains 11 entity types: Organization (ORG), Person (PER), Location (LOC), Entertainment, Facilities, Artifact, Living things, Locomotives, Plants, Materials and Diseases.", "labels": [], "entities": []}, {"text": "Rest of the corpus was tagged as non-entities (O).", "labels": [], "entities": []}, {"text": "The dataset was randomly divided into three splits: Train, Development and Test in the ratios 70%, 17% and 13%.", "labels": [], "entities": [{"text": "Train", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.8769468665122986}, {"text": "Test", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9859492182731628}]}, {"text": "The training set consists of 3,199 sentences comprising 56,801 tokens, development set contains 707 sentences comprising 12,882 tokens and test set contains 571 sentences comprising of 10,396 tokens.", "labels": [], "entities": []}, {"text": "We 157 use the F1-measure to evaluate our performance against other approaches.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.998390793800354}]}, {"text": "We perform extensive experiments on the CoNLL 2003 dataset for Named Entity Recognition.", "labels": [], "entities": [{"text": "CoNLL 2003 dataset", "start_pos": 40, "end_pos": 58, "type": "DATASET", "confidence": 0.9477539658546448}, {"text": "Named Entity Recognition", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.7872568170229594}]}, {"text": "The dataset is primarily a collection of Reuters newswire articles annotated for NER with four entity types: Person (PER), Location(LOC), Organization(ORG), Miscellaneous (MISC) along with nonentity elements tagged as (O).", "labels": [], "entities": []}, {"text": "The data is provided with a training set contains 15,000 sentences consisting of approximately 203,000 tokens, along with a development set containing 3466 sentences consisting of around 51,000 tokens and a test set containing 3684 sentences comprising of approximately 46,435 tokens.", "labels": [], "entities": []}, {"text": "We use the standard evaluation scripts provided along with the dataset for assessing the performance of our methodology.", "labels": [], "entities": []}, {"text": "The scripts use the F1-score to evaluate the performance of models.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9989812970161438}]}, {"text": "We use this architecture for the network because of the constraint on the dataset size caused by scarcity of labelled data.", "labels": [], "entities": []}, {"text": "We used a NVIDIA 970 GTX GPU and a 4.00 GHz Intel i7-4790 processor with 64GB RAM to train our models.", "labels": [], "entities": []}, {"text": "As the datasets in this domain expand, we would like to scale up our approach to bigger architectures.", "labels": [], "entities": []}, {"text": "The results obtained on ICON 2013 NLP Tools dataset are summarized in.", "labels": [], "entities": [{"text": "ICON 2013 NLP Tools dataset", "start_pos": 24, "end_pos": 51, "type": "DATASET", "confidence": 0.9321705818176269}]}, {"text": "We crossvalidated our approach with English language using the CoNLL 2003 dataset.", "labels": [], "entities": [{"text": "CoNLL 2003 dataset", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.9826672077178955}]}, {"text": "The results are summarized in, We are able to achieve stateof-the-art accuracies without using additional information like Gazetteers, Chunks along with not using any hand-crafted features which are considered essential for NER task as chunking provides us data about the phrases and Gazetteers provide a list of words which have high likelihood of being a named entity.", "labels": [], "entities": [{"text": "Chunks", "start_pos": 135, "end_pos": 141, "type": "DATASET", "confidence": 0.8863476514816284}, {"text": "NER", "start_pos": 224, "end_pos": 227, "type": "TASK", "confidence": 0.948008120059967}]}], "tableCaptions": [{"text": " Table 1: Results on the CoNLL 2003 dataset.  We achieve 90.32% accuracy without using any  Gazetter information (Gaz.)", "labels": [], "entities": [{"text": "CoNLL 2003 dataset", "start_pos": 25, "end_pos": 43, "type": "DATASET", "confidence": 0.9648232062657675}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.997636079788208}, {"text": "Gaz.", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.7054766416549683}]}, {"text": " Table 2: Results on the ICON NLP Tools 2013  dataset. We achieve 77.48% accuracy without us- ing any Gazetter information (Gaz.) or Chunking  Information (Chu.).", "labels": [], "entities": [{"text": "ICON NLP Tools 2013  dataset", "start_pos": 25, "end_pos": 53, "type": "DATASET", "confidence": 0.9241639137268066}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9990912675857544}]}, {"text": " Table 3: Entity wise Precision, Recall and F1  scores on the ICON NLP Tools 2013 Hindi dataset  (Test Set) for glove 300 size Embeddings and Bi- LSTM 1 layer deep model.", "labels": [], "entities": [{"text": "Entity", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9680066108703613}, {"text": "Precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9826552867889404}, {"text": "Recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9985665678977966}, {"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9993712306022644}, {"text": "ICON NLP Tools 2013 Hindi dataset  (Test Set", "start_pos": 62, "end_pos": 106, "type": "DATASET", "confidence": 0.9397768378257751}]}]}