{"title": [{"text": "Chinese Grammatical Error Diagnosis with Long Short-Term Memory Networks", "labels": [], "entities": [{"text": "Chinese Grammatical Error Diagnosis", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7397303432226181}]}], "abstractContent": [{"text": "Grammatical error diagnosis is an important task in natural language processing.", "labels": [], "entities": [{"text": "Grammatical error diagnosis", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8538650274276733}, {"text": "natural language processing", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6554324726263682}]}, {"text": "This paper introduces our Chinese Grammatical Error Diagnosis (CGED) system in the NLP-TEA-3 shared task for CGED.", "labels": [], "entities": [{"text": "Chinese Grammatical Error Diagnosis (CGED)", "start_pos": 26, "end_pos": 68, "type": "TASK", "confidence": 0.7182816565036774}]}, {"text": "The CGED system can diagnose four types of grammatical errors which are redundant words (R), missing words (M), bad word selection (S) and disordered words (W).", "labels": [], "entities": []}, {"text": "We treat the CGED task as a sequence labeling task and describe three models, including a CRF-based model, an LSTM-based model and an ensemble model using stacking.", "labels": [], "entities": []}, {"text": "We also show in details how we build and train the models.", "labels": [], "entities": []}, {"text": "Evaluation includes three levels, which are detection level, identification level and position level.", "labels": [], "entities": []}, {"text": "On the CGED-HSK dataset of NLP-TEA-3 shared task, our system presents the best F1-scores in all the three levels and also the best recall in the last two levels.", "labels": [], "entities": [{"text": "CGED-HSK dataset", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.9755272567272186}, {"text": "F1-scores", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9984130859375}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9993189573287964}]}], "introductionContent": [{"text": "Chinese has been considered as one of the most difficult languages in the world.", "labels": [], "entities": []}, {"text": "Unlike English, Chinese has no verb tenses and pluralities, and there usually exist various ways to express the same meaning in Chinese.", "labels": [], "entities": []}, {"text": "Consequently, it is common for non-native speakers of Chinese to make grammatical errors of various types in their writings.", "labels": [], "entities": []}, {"text": "The goal of Chinese Grammatical Error Diagnosis (CGED) is to build a system that can automatically diagnose errors in Chinese sentences.", "labels": [], "entities": [{"text": "Chinese Grammatical Error Diagnosis (CGED)", "start_pos": 12, "end_pos": 54, "type": "TASK", "confidence": 0.8241187334060669}]}, {"text": "Evaluation is carried out in three levels, based on the detection of error occurrences in a sentence, as well as their types and positions.", "labels": [], "entities": []}, {"text": "In this work, we formalize the CGED task as a sequence labeling problem, which assigns each Chinese character in a target sentence with a tag indicating both the error type (R, M, S, W) and position.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.6674937903881073}]}, {"text": "Therefore, the CGED task can be readily solved with atypical conditional random fields (CRF) model (.", "labels": [], "entities": []}, {"text": "However, the main challenge for CGED is that the detection of errors usually requires long-term dependencies.", "labels": [], "entities": [{"text": "CGED", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.7210411429405212}]}, {"text": "For example, in, the grammatical error at \"\u8868\u793a(represent)\" may not be detected until the last word \"\u635f\u5bb3(damage)\" shows up.", "labels": [], "entities": []}, {"text": "Traditional models with features extracted from a limited context window may not be able to handle these situations.", "labels": [], "entities": []}, {"text": "Neural network-based models have been extensively used in natural language processing (NLP) during recent years, due to their strong capability of automatical feature learning.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.808963268995285}]}, {"text": "In particular, the long shortterm memory (LSTM)) based recurrent neural networks (RNN) have been proved to be highly effective in various applications that involves sequence modeling, such as language modeling, named entity recognition () and parsing (, etc.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 192, "end_pos": 209, "type": "TASK", "confidence": 0.7756945788860321}, {"text": "named entity recognition", "start_pos": 211, "end_pos": 235, "type": "TASK", "confidence": 0.6249268551667532}, {"text": "parsing", "start_pos": 243, "end_pos": 250, "type": "TASK", "confidence": 0.9684768915176392}]}, {"text": "Therefore, in this paper, we propose to use LSTM-based RNNs to solve the CGED problem.", "labels": [], "entities": []}, {"text": "In order to leverage both the merits of CRF models and LSTM models, we further present an ensemble model using.", "labels": [], "entities": []}, {"text": "Evaluations on the NLP-TEA-3 shared task for CGED show that our models achieve the best F1-scores in all levels and the best recall in two levels.", "labels": [], "entities": [{"text": "CGED", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.8322491645812988}, {"text": "F1-scores", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9983605742454529}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9992488026618958}]}, {"text": "The rest of the paper is organized as follows: Section 2 gives the definition of the CEGD task.", "labels": [], "entities": []}, {"text": "Section 3 describes how LSTM network is used to predict errors and what other works we have done.", "labels": [], "entities": []}, {"text": "Section 4 shows the evaluation results.", "labels": [], "entities": []}, {"text": "Section 5 gives some related works.", "labels": [], "entities": []}, {"text": "Section 6 gives conclusion and future work of this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first conduct experiments with the CRF-based and the LSTM-based model.", "labels": [], "entities": []}, {"text": "After that, we examine the effect of Stacking by taking the output of the CRF model as features of the LSTM model.", "labels": [], "entities": [{"text": "Stacking", "start_pos": 37, "end_pos": 45, "type": "TASK", "confidence": 0.9381461143493652}]}, {"text": "We use the validation dataset to select the best hyper-parameters in both the CRF-based model and the LSTM-based model.", "labels": [], "entities": []}, {"text": "As we can see, the LSTM-based model (LSTM (U+B)) has better Recall and F1-score than the CRF-based model, but lower in precision.", "labels": [], "entities": [{"text": "Recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9993969202041626}, {"text": "F1-score", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9920307397842407}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9987025260925293}]}, {"text": "Besides, the bigram embeddings has a very significant impact on the LSTM-based model.: Results on Validation Dataset.", "labels": [], "entities": []}, {"text": "'U' in the bracket after LSTM refers to using unigram of characters and 'B' refers to using bigram of characters.", "labels": [], "entities": []}, {"text": "When testing on the final evaluation dataset, we merged our training dataset and validation dataset, and retrain our models.", "labels": [], "entities": []}, {"text": "shows the results of our three submissions.", "labels": [], "entities": []}, {"text": "The three models we submitted includes the LSTM-based model (HIT-Run1), Stacking model (HITRun2) and LSTM-based model with some post-process (HIT-Run3).", "labels": [], "entities": []}, {"text": "The post-process mainly includes changing the 'I-X' errors without a 'B-X' error before it into a single 'B-X' error.", "labels": [], "entities": []}, {"text": "This increases the recall rate on three levels but slightly decreases the precision.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9851388931274414}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9992857575416565}]}], "tableCaptions": [{"text": " Table 8: Results on Validation Dataset. 'U' in the bracket after LSTM refers to using unigram of charac- ters and 'B' refers to using bigram of characters.", "labels": [], "entities": []}, {"text": " Table 9: Results on Evaluation Dataset.", "labels": [], "entities": [{"text": "Evaluation Dataset", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.7281743288040161}]}]}