{"title": [{"text": "Open-domain Factoid Question Answering via Knowledge Graph Search", "labels": [], "entities": [{"text": "Factoid Question Answering", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.7305801113446554}]}], "abstractContent": [{"text": "We introduce a highly scalable approach for open-domain question answering with no dependence on any data set for surface form to logical form mapping or any linguistic analytic tool such as POS tagger or named entity recognizer.", "labels": [], "entities": [{"text": "question answering", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.6976542174816132}, {"text": "POS tagger", "start_pos": 191, "end_pos": 201, "type": "TASK", "confidence": 0.6916368752717972}, {"text": "named entity recognizer", "start_pos": 205, "end_pos": 228, "type": "TASK", "confidence": 0.5844190021355947}]}, {"text": "We define our approach under the Constrained Conditional Models framework which lets us scale up to a full knowledge graph with no limitation on the size.", "labels": [], "entities": []}, {"text": "On a standard benchmark, we obtained near 4 percent improvement over the state-of-the-art in open-domain question answering task.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.779417097568512}]}], "introductionContent": [{"text": "We consider the task of simple open-domain question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7102624326944351}]}, {"text": "The answer to a simple question can be obtained only by knowing one entity and one property (.", "labels": [], "entities": []}, {"text": "A property is an attribute which is asked about a specific thing, place or people in a question.", "labels": [], "entities": []}, {"text": "The thing, place or people in the question are samples of an entity.", "labels": [], "entities": []}, {"text": "The answer to such a question is again an entity or a set of entities.", "labels": [], "entities": []}, {"text": "For instance, in the question \"What is the timezone in Dublin?\", Dublin is an entity and timezone is a property.", "labels": [], "entities": [{"text": "Dublin", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.8810269236564636}, {"text": "Dublin", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.9268163442611694}]}, {"text": "Freebase, the knowledge graph which we used in our experiments, contains about 58 million such entities and more than 14 thousands such properties.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9677394032478333}]}, {"text": "Hence, entities which we obtain from the knowledge graph are ambiguous in majority of cases.", "labels": [], "entities": []}, {"text": "We extract metadata available in the knowledge graph and integrate them into our system using Constrained Conditional Model framework (CCM)) to disambiguate entities.", "labels": [], "entities": []}, {"text": "In WebQuestions (), a data set of 5810 questions which are compiled using the Google suggest API, 86% of the questions are answerable by knowing only one entity (.", "labels": [], "entities": []}, {"text": "It suggests that a large number of the questions which ordinary people ask on the Internet are simple questions and it emphasizes on the importance of simple question answering systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7437582612037659}]}, {"text": "Besides, the best result on this task is 63.9% () which shows open-domain simple QA is still an unresolved task in NLP.", "labels": [], "entities": []}, {"text": "Despite the title, simple QA is not a simple task at all.", "labels": [], "entities": [{"text": "QA", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.702890157699585}]}, {"text": "Flexible and unbound number of entities and their properties in open-domain questions is an intimidating challenge for entity recognition.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7334690392017365}]}, {"text": "However, knowledge graphs by providing a structural knowledge base on entities can help a lot.", "labels": [], "entities": []}, {"text": "We use a knowledge graph to recognize entities attest time.", "labels": [], "entities": []}, {"text": "Defining a model for entity disambiguation on a single question instead of a whole data set lets us to scale the system up to a large knowledge graph irrespective to its size.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7151314169168472}]}, {"text": "We elaborate on entity recognition in Sections 6.2 and 6.3.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8813310861587524}]}, {"text": "The contributions of this paper area highly scalable QA system and a high performance entity recognition model using knowledge graph search.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7159556150436401}]}], "datasetContent": [{"text": "The input for training step in our approach is training and validation sets with their knowledge graph assertions.", "labels": [], "entities": []}, {"text": "Using Word2Vec toolkit), we replaced the tokens in the data sets with their vector representations to use them as \u03c6(q) in our model.", "labels": [], "entities": [{"text": "Word2Vec toolkit", "start_pos": 6, "end_pos": 22, "type": "DATASET", "confidence": 0.9340474903583527}]}, {"text": "We pruned questions with more than 20 tokens in length and fixate shorter ones by adding extra <.> token.", "labels": [], "entities": []}, {"text": "We already did some simple pre-processing jobs on the input data including non-alphanumeric character removal and spaceseparation tokens.", "labels": [], "entities": [{"text": "character removal", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7586102485656738}]}, {"text": "Using these features and the model described above, we trained a classifier using logistic regression, neural network and CNN technique.", "labels": [], "entities": []}, {"text": "We used the trained classifier attest time for detecting 100-best properties for each test question.", "labels": [], "entities": []}, {"text": "For multilayer trained on property Acc.", "labels": [], "entities": []}, {"text": "neural network, we used two hidden layer each with 1024 neurons and for CNN we used the same number of neurons for convolution and softmax layers.", "labels": [], "entities": []}, {"text": "We tried to enforce regularization on weight vectors, however as already tested in) it had no effect on final results.", "labels": [], "entities": [{"text": "regularization", "start_pos": 20, "end_pos": 34, "type": "METRIC", "confidence": 0.7785999178886414}]}, {"text": "We also included anew channel in our CNN using POS of tokens which improved the final model but not significantly.", "labels": [], "entities": []}, {"text": "The trained classifier, test questions and Freebase knowledge graph) are inputs attest time.", "labels": [], "entities": [{"text": "Freebase knowledge graph", "start_pos": 43, "end_pos": 67, "type": "DATASET", "confidence": 0.8709731698036194}]}, {"text": "For testing our system, we used SimpleQuestions data set () which contains 108442 factoid questions.", "labels": [], "entities": [{"text": "SimpleQuestions data set", "start_pos": 32, "end_pos": 56, "type": "DATASET", "confidence": 0.8947110176086426}]}, {"text": "We divided the data set into 70%, 10%, and 20% portions for train, validation and test sets respectively and we did this for three times randomly as a mean of cross validation.", "labels": [], "entities": []}, {"text": "However, to make our result comparable to the results of SimpleQuestion authors we reported our results on the official separation test data.", "labels": [], "entities": [{"text": "official separation test data", "start_pos": 111, "end_pos": 140, "type": "DATASET", "confidence": 0.7442745044827461}]}, {"text": "For entity recognition (including detection and disambiguation), we defined two sets of constraints which are described above.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8323767781257629}]}, {"text": "Constraints in the first category (i.e., type constraints) helped with entity detection and constraints in the second category (i.e.,lexical similarity) helped with entity disambiguation.", "labels": [], "entities": [{"text": "entity detection", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.7909907400608063}, {"text": "entity disambiguation", "start_pos": 165, "end_pos": 186, "type": "TASK", "confidence": 0.7920869886875153}]}, {"text": "We used Gurobi solver for searching through the space of possible entities.", "labels": [], "entities": [{"text": "Gurobi solver", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.6621251404285431}]}, {"text": "The space of search for each question is around 100 to 500 thousands entities.", "labels": [], "entities": []}, {"text": "With this space, Gurobi solver was able to detect entity for 20 questions per second.", "labels": [], "entities": [{"text": "Gurobi solver", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.5833085179328918}]}, {"text": "We used path-level accuracy for evaluating the system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9459367990493774}]}, {"text": "In path-level accuracy, a prediction is considered correct if the predicted entity and the property both are correct.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9220480918884277}]}, {"text": "This is the same evaluation metric which is used by the data set authors.", "labels": [], "entities": []}, {"text": "We obtained our best validation accuracy using greedy approach for entity recognition and 128 dimensional embeddings for property detection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9683360457420349}, {"text": "entity recognition", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8162743747234344}, {"text": "property detection", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7366663366556168}]}, {"text": "Using the same configuration, we reported the accuracy of our system on test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9996503591537476}]}], "tableCaptions": [{"text": " Table 1: Experimental results on test set of SimpleQuestions (SQ) data set. LR stands for logistic regression model,  NN for neural network, CNN-1 for convolutional neural network with one channel and CNN-2 for CNN two channels  model.", "labels": [], "entities": [{"text": "SimpleQuestions (SQ) data set", "start_pos": 46, "end_pos": 75, "type": "DATASET", "confidence": 0.6688088575998942}]}]}