{"title": [], "abstractContent": [{"text": "Semi-supervised clustering is an attractive alternative for traditional (unsupervised) clustering in targeted applications.", "labels": [], "entities": [{"text": "Semi-supervised clustering", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6062428653240204}]}, {"text": "By using the information of a small annotated dataset, semi-supervised clustering can produce clusters that are customized to the application domain.", "labels": [], "entities": []}, {"text": "In this paper, we present a semi-supervised clustering technique based on a multi-objective evolutionary algorithm (NSGA-II-clus).", "labels": [], "entities": []}, {"text": "We apply this technique to the task of clustering medical publications for Evidence Based Medicine (EBM) and observe an improvement of the results against unsupervised and other semi-supervised clustering techniques.", "labels": [], "entities": [{"text": "clustering medical publications for Evidence Based Medicine (EBM)", "start_pos": 39, "end_pos": 104, "type": "TASK", "confidence": 0.7593716353178024}]}], "introductionContent": [{"text": "Clustering is an unsupervised machine learning method that attempts to find groups (clusters) in a collection of documents ().", "labels": [], "entities": []}, {"text": "Clustering is useful for applications where the goal is to find structure in a collection of documents, and can be applied in a wide range of tasks, such as finding groups among patients with breast cancer, or identifying groups of shoppers with similar browsing and purchase histories.", "labels": [], "entities": []}, {"text": "A common problem with clustering, however, is that the structure that is found might not reflect the desired structure that is relevant fora particular application.", "labels": [], "entities": []}, {"text": "For example, one might wish to cluster words in the hope of learning their parts-of-speech, but instead the clusters group words according to their meanings.", "labels": [], "entities": []}, {"text": "In supervised learning, we have labeled information, but the annotation can be costly to produce.", "labels": [], "entities": []}, {"text": "So a trade-off is needed, and a semi-supervised framework provides this trade-off.", "labels": [], "entities": []}, {"text": "In semi-supervised clustering (, part of the documents to cluster are annotated with information about how they cluster, and the task consists of clustering the entire set of documents.", "labels": [], "entities": []}, {"text": "By incorporating the information of the known clusters of apart of the documents, the final clusters have a better chance to match the desired clusters of the application domain.", "labels": [], "entities": []}, {"text": "In this paper we focus on clustering the documents that are relevant to a clinical query for the practice of Evidence Based Medicine (EBM).", "labels": [], "entities": [{"text": "Evidence Based Medicine (EBM)", "start_pos": 109, "end_pos": 138, "type": "TASK", "confidence": 0.7208872636159261}]}, {"text": "Here, each cluster is expected to group the documents that describe a particular aspect of the answer to the clinical question.", "labels": [], "entities": []}, {"text": "Let us take an example of the disease Asperger's syndrome.", "labels": [], "entities": [{"text": "Asperger's syndrome", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.5149333079655966}]}, {"text": "There are five policies for the treatment of this disease, namely 'special education', 'behavior modification', 'speech', 'physical and occupational therapy and medication', and 'social skill therapies and medications'.", "labels": [], "entities": [{"text": "behavior modification", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.7445690929889679}]}, {"text": "Now the documents which are assigned to each of these possible treatment policies represent a cluster.", "labels": [], "entities": []}, {"text": "shows an example of such clustering.", "labels": [], "entities": []}, {"text": "Moreover, shows that some documents maybe associated with multiple treatments, and therefore the clustering task is non-overlapping.", "labels": [], "entities": []}, {"text": "Most of the clustering techniques in existing literature focus on optimizing only one validity index (), which measures the goodness of an obtained par-,, titioning.", "labels": [], "entities": []}, {"text": "However, in order to determine a proper partitioning, optimizing a single cluster validity index is not always sufficient, especially in the situation when we deal with text documents having clusters of different shapes and sizes.", "labels": [], "entities": []}, {"text": "The concept of multi-objective optimization (MOO) can be brought into consideration where we need to optimize several objective functions at the same time.", "labels": [], "entities": [{"text": "multi-objective optimization (MOO)", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.8761767864227294}]}, {"text": "The advantage of MOO is that we can generate clusters by optimizing several cluster validity indices.", "labels": [], "entities": [{"text": "MOO", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8740139007568359}]}, {"text": "Inspired by this, proposed a MOO-based approach for clustering medical documents for EBM by using the search capability of a simulated annealing based approach, AMOSA (Archived MultiObjective Simulated Annealing based technique) (.", "labels": [], "entities": [{"text": "clustering medical documents", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.8686981598536173}]}, {"text": "However, it has been shown that for some benchmark datasets, AMOSA performs slowly compared to a popular genetic algorithm based MOO technique, NSGA-II (Non-dominated Sorting Genetic Algorithm-II) (.", "labels": [], "entities": [{"text": "AMOSA", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.48051804304122925}]}, {"text": "Therefore, an alternative MOO-based approach is needed in order to verify whether we can improve the run-time complexity of AMOSA.", "labels": [], "entities": [{"text": "MOO-based", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.8251383304595947}, {"text": "AMOSA", "start_pos": 124, "end_pos": 129, "type": "DATASET", "confidence": 0.7842467427253723}]}, {"text": "Moreover, have used some labeled information to select a single solution from the final set of trade-off solutions.", "labels": [], "entities": []}, {"text": "In general semi-supervised methods perform well compared to unsupervised clustering techniques.", "labels": [], "entities": []}, {"text": "In our present work we propose to develop a semi-supervised clustering technique and apply that for EBM.", "labels": [], "entities": [{"text": "EBM", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.8021901249885559}]}, {"text": "The proposed approach uses only 10% labeled information which is easy to obtain.", "labels": [], "entities": []}, {"text": "The proposed technique is novel in away that it uses the labeled information during the internal steps of the proposed clustering process.", "labels": [], "entities": []}, {"text": "More specifically we can say that the internal steps of NSGA-II based clustering are modified to take care of this labeled information.", "labels": [], "entities": [{"text": "NSGA-II based clustering", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.5671139558156332}]}, {"text": "The labeled information was used by to select a single solution from the final Pareto optimal front after the execution of AMOSA based clustering technique.", "labels": [], "entities": []}, {"text": "Moreover, as mentioned by, the complexity of AMOSA is higher than that of NSGA-II.", "labels": [], "entities": [{"text": "complexity", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9758080244064331}]}, {"text": "Thus, the use of NSGA-II as the underlying optimization technique makes the system less complex and time consuming.", "labels": [], "entities": []}, {"text": "In this paper, we propose the use of NSGA-II () for semi-supervised clustering of documents.", "labels": [], "entities": [{"text": "semi-supervised clustering of documents", "start_pos": 52, "end_pos": 91, "type": "TASK", "confidence": 0.7072165459394455}]}, {"text": "We propose two different versions of the NSGA-II based semi-supervised clustering technique.", "labels": [], "entities": []}, {"text": "In the first approach the available supervised information in the form of must-link and cannot-link constraints can be used during the selection phase of clustering.", "labels": [], "entities": []}, {"text": "These constraints are taken into account while calculating crowding distance which is further used to assign ranks to different solutions of the combined population.", "labels": [], "entities": []}, {"text": "Thus, the available supervised information is used in each generation of the proposed technique.", "labels": [], "entities": []}, {"text": "In the second approach, we use a semi-supervised approach to select a single solution from the set of final solutions produced by the MOO-based approach.", "labels": [], "entities": []}, {"text": "In this case, supervised information is used only at the final stage rather than during the clustering phase.", "labels": [], "entities": []}, {"text": "In recent years, several semi-supervised clustering techniques (;) have been proposed in the literature which are applicable for general data sets.", "labels": [], "entities": []}, {"text": "In this paper we also extend those techniques to solve the problem of EBM.", "labels": [], "entities": [{"text": "EBM", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9246304631233215}]}, {"text": "Some of the promising methods include the ones based on K-means with a distance metric () and Kmeans with a probabilistic framework ().", "labels": [], "entities": []}, {"text": "We, thereafter, present a thorough comparative analysis with our proposed methods and other existing semi-supervised clustering techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the dataset made available by Moll\u00e1 and Santiago-Martinez (2011), from which we randomly extract 276 clinical questions.", "labels": [], "entities": []}, {"text": "Each question is associated with an average of 5.89 documents, and can be seen as an independent clustering task.", "labels": [], "entities": []}, {"text": "The proposed NSGA-II-clus (internal and external) and AMOSAclus () clustering techniques are therefore applied on each question individually.", "labels": [], "entities": [{"text": "NSGA-II-clus", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.743647575378418}, {"text": "AMOSAclus", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.7227208018302917}]}, {"text": "The average entropy value of all the questions is then calculated.", "labels": [], "entities": []}, {"text": "For both internal and external NSGA-II clus algorithm, we first select 10% of the must-link and cannot-link constraints.", "labels": [], "entities": [{"text": "NSGA-II clus", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.8206062316894531}]}, {"text": "For Internal-NSGA-II-clus, this supervised information is used in providing ranking of all the solutions during selection phase of each generation.", "labels": [], "entities": []}, {"text": "In case of External-NSGA-II-clus this available supervised information is used in assigning a score to each of the solutions on the final Pareto front.", "labels": [], "entities": [{"text": "Pareto front", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.8143681585788727}]}, {"text": "Based on the highest score we select a single solution and compute the entropy values accordingly.", "labels": [], "entities": []}, {"text": "The parameters for the proposed NSGA-II-clus (internal and external) semi-supervised approach are as follows: population size = 20, number of generations = 20, mutation probability = 0.2 and crossover probability = 0.6.", "labels": [], "entities": [{"text": "mutation probability", "start_pos": 160, "end_pos": 180, "type": "METRIC", "confidence": 0.8898868560791016}, {"text": "crossover probability", "start_pos": 191, "end_pos": 212, "type": "METRIC", "confidence": 0.9628771245479584}]}, {"text": "These values were determined after performing a thorough sensitivity study.", "labels": [], "entities": []}, {"text": "The parameters of AMOSA-clus are kept similar to those reported by.The proposed NSGA-II-clus (internal and external) and AMOSA-clus approaches along with two semi-supervised approaches, namely K-Means with Distance Metric () and K-Means with probabilistic framework () are applied on the same datasets.", "labels": [], "entities": [{"text": "AMOSA-clus", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.6840786933898926}, {"text": "NSGA-II-clus", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.864912748336792}]}, {"text": "The K-Means+ Distance Metric () algorithm in its simplest sense is a variation of Kmeans.", "labels": [], "entities": []}, {"text": "In the usual K-means, Euclidean or cosine distance is used as a measure of distance or separation between any two points in the space.", "labels": [], "entities": []}, {"text": "Suppose an user wants certain points to be regarded as similar, according to some distance metric.", "labels": [], "entities": []}, {"text": "Our task is to learn a distance metric automatically over a set of points which takes into account this relationship.", "labels": [], "entities": []}, {"text": "In this algorithm, however instead of Euclidean or cosine distance, the concept of Distance Metric is used for our benefit.", "labels": [], "entities": []}, {"text": "In the case of a probabilistic framework, a set of data points is randomly partitioned into a specific number of clusters which serve as the unsupervised partitioning initially.", "labels": [], "entities": []}, {"text": "Here also supervision is provided in terms of two constraints i.e., must-link and cannot-link (.", "labels": [], "entities": []}, {"text": "A modified version of Expectation-Maximization algorithm is used hereto obtain the final partitioning which also obeys the available supervised information.", "labels": [], "entities": [{"text": "Expectation-Maximization", "start_pos": 22, "end_pos": 46, "type": "METRIC", "confidence": 0.8790656328201294}]}, {"text": "Two versions of the proposed NSGA-II-clus (internal and external) and AMOSA-clus algorithms are executed with the following distance measures: i) (version 1) Euclidean distance as the similarity measure for the assignment of documents to different clusters and also for the computation of objective functions; and ii) (version 2) with cosine similarity as the similarity measure for the assignment of documents to different clusters and also for the computation of objective functions.", "labels": [], "entities": [{"text": "NSGA-II-clus", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.8981682658195496}, {"text": "Euclidean distance", "start_pos": 158, "end_pos": 176, "type": "METRIC", "confidence": 0.848298579454422}]}, {"text": "The average entropy values attained by these techniques are reported in.", "labels": [], "entities": []}, {"text": "For the best-case computation we select those solutions from the final Pareto front obtained by internal-NSGA-II-clus and AMOSA-clus which possess the minimum entropy values.", "labels": [], "entities": [{"text": "Pareto front", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.8703862726688385}, {"text": "AMOSA-clus", "start_pos": 122, "end_pos": 132, "type": "DATASET", "confidence": 0.5739442110061646}]}, {"text": "In the case of external-NSGA-II-clus, the best solution is selected using the steps as discussed in Section 4.3.", "labels": [], "entities": []}, {"text": "The corresponding entropy values for those solutions are calculated.", "labels": [], "entities": []}, {"text": "For the average case (unsupervised) computation we select all the solutions on the final Pareto Optimal front and calculate entropy for each of the solutions.", "labels": [], "entities": []}, {"text": "Then we take the average entropy of all the solutions and report those values both for NSGA-II-clus (internal and external) and AMOSA-clus in. shows that, using 10% supervised information, the probabilistic framework approach outperforms the distance metric learning approach in case of Euclidean distance measure.", "labels": [], "entities": [{"text": "NSGA-II-clus", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.7532392144203186}, {"text": "AMOSA-clus", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.6259818077087402}]}, {"text": "Among all the algorithms, semi-supervised internal-NSGA-II-clus yields the highest performance in the best case as well as in the average case.", "labels": [], "entities": []}, {"text": "This is also better than the AMOSA-based clustering algorithm, which was used for EBM by.", "labels": [], "entities": [{"text": "AMOSA-based clustering", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.5589345395565033}]}, {"text": "In order to show that our proposed NSGA-II-clus (internal and external) is also able to predict the correct number of clusters from different questions automatically, we have reported the error rate as below: Here target i denotes the actual number of clusters fora particular question and predicted i denotes the predicted number of clusters by the proposed NSGA-II-clus (internal and external) technique fora particular question.", "labels": [], "entities": []}, {"text": "Here as mentioned earlier in Section 2, for each question, we have varied the number of clusters in the range 2 to \u221a n where n is the number of documents per question.", "labels": [], "entities": []}, {"text": "The average number of clusters identified by the proposed Internal-NSGA-II-clus optimizing XB-index and I-index as the objective functions for each question are 2.13 and 2.27, respectively, with cosine and Euclidean distance measurements.", "labels": [], "entities": []}, {"text": "The average number of clusters identified by the proposed external-NSGA-II-clus optimizing XB-index and I-index as the objective functions for each question are 2.45 and 2.32, respectively, with cosine and Euclidean distance measurements.", "labels": [], "entities": [{"text": "XB-index", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.8222246170043945}, {"text": "I-index", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9112723469734192}]}, {"text": "The average number of clusters in the actual annotated set is 2.38.", "labels": [], "entities": []}, {"text": "Moreover we have also computed the error rates of different automatic clustering techniques.", "labels": [], "entities": [{"text": "error rates", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9608030021190643}]}, {"text": "For AMOSA-clus the error rates are 1.90 with cosine similarity and 1.91 with Euclidean distance.", "labels": [], "entities": [{"text": "error", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9872184991836548}]}, {"text": "For internal-NSGA-II-clus the error rates are 1.33 with cosine similarity and 1.49 with Euclidean distance.", "labels": [], "entities": [{"text": "error", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9894653558731079}]}, {"text": "For External-NSGA-II-clus the error rates are 1.74 with cosine similarity and 1.69 with Euclidean distance.", "labels": [], "entities": [{"text": "error", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9891045093536377}]}, {"text": "() it has already been proved that AMOSA-clus provides the minimal error rate compared to different existing techniques and heuristics.", "labels": [], "entities": [{"text": "AMOSA-clus", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.5189268589019775}, {"text": "error rate", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9404334425926208}]}, {"text": "But the proposed approach provides minimal error rate compared to AMOSA-clus.", "labels": [], "entities": [{"text": "error rate", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.970517098903656}, {"text": "AMOSA-clus", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.7189574837684631}]}, {"text": "This again proves the efficacy of the proposed semi-supervised approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Cluster entropies obtained by different approaches. Here KM DM and KM P rob denote, respec- tively, the K-means with distance-metric-based approach and K-means with probabilistic approach", "labels": [], "entities": []}]}