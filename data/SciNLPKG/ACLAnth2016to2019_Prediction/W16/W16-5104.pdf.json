{"title": [{"text": "Named Entity Recognition in Swedish Health Records with Character-Based Deep Bidirectional LSTMs", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7278920561075211}, {"text": "Swedish Health Records", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.8395631909370422}]}], "abstractContent": [{"text": "We propose an approach for named entity recognition in medical data, using a character-based deep bidirectional recurrent neural network.", "labels": [], "entities": [{"text": "named entity recognition in medical data", "start_pos": 27, "end_pos": 67, "type": "TASK", "confidence": 0.7497399648030599}]}, {"text": "Such models can learn features and patterns based on the character sequence, and are not limited to a fixed vocabulary.", "labels": [], "entities": []}, {"text": "This makes them very well suited for the NER task in the medical domain.", "labels": [], "entities": [{"text": "NER task", "start_pos": 41, "end_pos": 49, "type": "TASK", "confidence": 0.9376059472560883}]}, {"text": "Our experimental evaluation shows promising results, with a 60% improvement in F 1 score over the baseline, and our system generalizes well between different datasets.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9932299256324768}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) is the task of finding mentions of named entities in a text.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7870775461196899}, {"text": "finding mentions of named entities in a text", "start_pos": 46, "end_pos": 90, "type": "TASK", "confidence": 0.4915153980255127}]}, {"text": "In nonmedical NER, entity classes are typically people, organizations, and locations.", "labels": [], "entities": [{"text": "NER", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.7075022459030151}]}, {"text": "It is one of the fundamental Natural Language Processing (NLP) tasks and has been studied extensively.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP) tasks", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.7261116674968174}]}, {"text": "In this paper, we approach the problem of finding medical entities such as (1) disorders and findings, (2) pharmaceutical drugs, and (3) body structure.", "labels": [], "entities": []}, {"text": "Our proposed method uses deep bidirectional character-based recurrent neural networks (RNNs), trained in an end-to-end fashion to perform both boundary detection and classification at the same time.", "labels": [], "entities": [{"text": "boundary detection", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.7211814373731613}]}, {"text": "There area number of properties that make this problem especially challenging in biomedical text ().", "labels": [], "entities": []}, {"text": "Firstly, names composed of multiple words are frequently used to describe an entity, highlighting the requirement of good boundary detection on an NER system.", "labels": [], "entities": []}, {"text": "Secondly, one noun can be part of a mention of several entities at the same time E.g: \"91 and 84 kDa proteins\" consists of two entity names: \"91 kDa proteins\" and \"84 kDa proteins\".", "labels": [], "entities": []}, {"text": "Thirdly, it is common to write the same biomedical entity in different ways, e.g: \"N-acetylcysteine\", \"N-acetyl-cysteine\", \"NAcetylCysteine\".", "labels": [], "entities": []}, {"text": "Lastly, ambiguous mentions are common, including abbreviations that refer to different things in different contexts.", "labels": [], "entities": []}, {"text": "(The examples above are from).", "labels": [], "entities": []}, {"text": "Our proposed method has a number of benefits over previous work: Firstly, the model can simultaneously recognize and classify entity mentions.", "labels": [], "entities": []}, {"text": "Secondly, using an end-to-end neural network approach eliminates the need for feature engineering.", "labels": [], "entities": []}, {"text": "All features needed are learned by the model during training.", "labels": [], "entities": []}, {"text": "Thirdly, because our model works on the raw character sequence, it does not suffer from out-ofvocabulary terms, it can learn that different character patterns represent the same thing, and it can learn the typical character-based features often used in traditional machine learning based solutions to NER.", "labels": [], "entities": []}, {"text": "We evaluate the model on Swedish health records in the Stockholm EPR corpus and obtain promising results.", "labels": [], "entities": [{"text": "Swedish health records in the Stockholm EPR corpus", "start_pos": 25, "end_pos": 75, "type": "DATASET", "confidence": 0.7611061222851276}]}, {"text": "We also note that the method generalizes well between different datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section explains the set-up of the empirical study of our model.", "labels": [], "entities": []}, {"text": "We use an approach similar to to obtain the data needed for training and evaluation.", "labels": [], "entities": []}, {"text": "The datasets that we prepared for training, validating and testing our model are available for download at https://github.com/olofmogren/biomedical-ner-data-swedish/.", "labels": [], "entities": []}, {"text": "The L\u00e4kartidningen corpus was originally presented by, and contains articles from the Swedish journal for medical professionals.", "labels": [], "entities": [{"text": "L\u00e4kartidningen corpus", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.7459428012371063}]}, {"text": "This was annotated for NER as apart of this work.", "labels": [], "entities": [{"text": "NER", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.7877212166786194}]}, {"text": "All occurrences of seed-terms were extracted (see Section 4.2), along with a context window of 60 characters (approximately ten words).", "labels": [], "entities": []}, {"text": "The window is positioned so that the entity mention is located randomly within the sequence.", "labels": [], "entities": []}, {"text": "In addition, negative training examples were extracted in order to prevent the model from learning that classified entities always occur in every sequence.", "labels": [], "entities": []}, {"text": "All the characters in these negative training examples had the same \"non-entity\" label.", "labels": [], "entities": []}, {"text": "Neural models typically benefit from large amounts of training data.", "labels": [], "entities": []}, {"text": "To increase the amount of training data, each occurrence of seed-terms were extracted three more times, where the window was shifted by a random number of steps.", "labels": [], "entities": []}, {"text": "The resulting data is a total of 775,000 of sequences with 60 characters each.", "labels": [], "entities": []}, {"text": "10% of the data is negative data, where every character has the \"non-entity\" label.", "labels": [], "entities": []}, {"text": "Another dataset was built from medical articles on the Swedish Wikipedia.", "labels": [], "entities": [{"text": "Swedish Wikipedia", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.8959173560142517}]}, {"text": "Firstly, an initial list of medical domain articles were chosen manually and fetched.", "labels": [], "entities": []}, {"text": "Secondly, articles were fetched that were linked from the initial articles.", "labels": [], "entities": []}, {"text": "Finally, the seed-terms list (see Section 4.2) was used to create the labels and extract training examples of 60 character sequences, in the same way as was done with L\u00e4kartidningen.", "labels": [], "entities": []}, {"text": "1177 V\u00e5rdguiden iis a website provided by the Swedish public healthcare authorities, containing information, counselling, and other health-care services.", "labels": [], "entities": [{"text": "1177 V\u00e5rdguiden iis a website", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.8616999745368957}]}, {"text": "The corpus consists of 15 annotated documents downloaded during May 2016.", "labels": [], "entities": []}, {"text": "This dataset was manually annotated with the seed-terms list as support (see Section 4.2).", "labels": [], "entities": []}, {"text": "The resulting dataset has 2740 annotations, out of which 1574 are disorder and finding, 546 are pharmaceutical drug, and 620 are body structure.", "labels": [], "entities": [{"text": "disorder", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.990498960018158}]}, {"text": "The Stockholm Electronic Patient Record (EPR) Clinical Entity Corpus () is a dataset with health records of over two million patients at Karolinska University Hospital in Stockholm encompassing the years 2006-2014.", "labels": [], "entities": [{"text": "Stockholm Electronic Patient Record (EPR) Clinical Entity Corpus", "start_pos": 4, "end_pos": 68, "type": "DATASET", "confidence": 0.7873690247535705}]}, {"text": "It consists of 7946 documents containing real-world anonymized health records with annotations in 4 categories: disorder, finding, drug and body structure.", "labels": [], "entities": []}, {"text": "Since we have a category where \"disorder\" and \"finding\" are bundled together they were considered the same.", "labels": [], "entities": []}, {"text": "L\u00e4kartidningen, Swedish Wikipedia, and 1177 V\u00e5rdguiden are all datasets with rather high quality text, most of it even professionally edited.", "labels": [], "entities": [{"text": "Swedish Wikipedia", "start_pos": 16, "end_pos": 33, "type": "DATASET", "confidence": 0.8554245829582214}, {"text": "1177", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.8318402171134949}, {"text": "V\u00e5rdguiden", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.4818034768104553}]}, {"text": "This is in stark contrast to the text in Stockholm EPR where misspellings are common, there are reduntant parts in many records, and writing style is highly diverse ().", "labels": [], "entities": [{"text": "Stockholm EPR", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.8988063931465149}]}, {"text": "A number of settings for hyperparameters were explored during development.", "labels": [], "entities": []}, {"text": "In the variations listed below, one hyperparameter at the time is varied and evaluated, and if we saw an improvement, the change of setting was retained.", "labels": [], "entities": []}, {"text": "A more thorough hyperparameter investigatin is left for future work.", "labels": [], "entities": []}, {"text": "For the three first experiments, dropout was used on the activations from the embedding layer, as well as on the activations on the LSTM outputs.", "labels": [], "entities": []}, {"text": "(See Section 4.1 for details).", "labels": [], "entities": []}, {"text": "Evaluation of the proposed model was performed on two different datasets: Stockholm EPR corpus (, with anonymized health record data in Swedish, and 1177 V\u00e5rdguiden.", "labels": [], "entities": [{"text": "Stockholm EPR corpus", "start_pos": 74, "end_pos": 94, "type": "DATASET", "confidence": 0.9320795734723409}, {"text": "V\u00e5rdguiden", "start_pos": 154, "end_pos": 164, "type": "DATASET", "confidence": 0.704147458076477}]}, {"text": "We report F 1 scores for total named entity recognition, as well as only entity classification (given correct boundary detection, we report scores of the entity classification performed by the system).", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9836603005727133}, {"text": "named entity recognition", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.6899477243423462}, {"text": "entity classification", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.7200862914323807}]}, {"text": "In the BOW baseline the entities are determined beforehand while the Char-BiLSTM model recognizes and classifies them as it traverses the document.", "labels": [], "entities": [{"text": "BOW baseline", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.7762651145458221}]}], "tableCaptions": [{"text": " Table 1: Dictionary baseline performance on the Stockholm EPR corpus. Although total precision is  reasonably good (0.67), the precision (0.12) is not.", "labels": [], "entities": [{"text": "Stockholm EPR corpus", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.9513242840766907}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.8529060482978821}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9986408352851868}]}, {"text": " Table 2: Results for Char-BiLSTM on Stockholm EPR corpus. The model obtains a total precision that  matches the dictionary baseline (0.67), and a recall that is much higher than the baseline (0.24).", "labels": [], "entities": [{"text": "Stockholm EPR corpus", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.9581473469734192}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9955605268478394}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9991852641105652}]}, {"text": " Table 3: Entity classification results of Char-BiLSTM on Stockholm EPR corpus. Given the entity  boundaries, we can see that the classification of entities work very well, obtaining a total F 1 score of  0.75.", "labels": [], "entities": [{"text": "Entity classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7847957611083984}, {"text": "Stockholm EPR corpus", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.9530834754308065}, {"text": "F 1 score", "start_pos": 191, "end_pos": 200, "type": "METRIC", "confidence": 0.9890227317810059}]}, {"text": " Table 4: Comparison of the results between each model on 1177 V\u00e5rdguiden.", "labels": [], "entities": [{"text": "1177 V\u00e5rdguiden", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.7130242884159088}]}]}