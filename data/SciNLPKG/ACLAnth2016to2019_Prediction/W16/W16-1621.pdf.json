{"title": [{"text": "Pair Distance Distribution: a Model of Semantic Representation", "labels": [], "entities": [{"text": "Semantic Representation", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7101706862449646}]}], "abstractContent": [{"text": "We introduce PDD (Pair Distance Distribution), a novel corpus-based model of semantic representation.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.7324394285678864}]}, {"text": "Most corpus-based models are VSMs (Vector Space Models), which while being successful, suffer from both practical and theoretical shortcomings.", "labels": [], "entities": []}, {"text": "VSM models produce very large, sparse matrices, and dimensionality reduction is usually performed, leading to high computational complexity, and obscuring the meaning of the dimensions.", "labels": [], "entities": []}, {"text": "Similarity in VSMs is constrained to be both symmetric and transitive, contrary to evidence from human subject tests.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 14, "end_pos": 18, "type": "TASK", "confidence": 0.7940933108329773}]}, {"text": "PDD is feature-based, created automatically from corpora without producing large, sparse matrices.", "labels": [], "entities": []}, {"text": "The dimensions along which words are compared are meaningful, enabling better understanding of the model and providing an explanation as to how any two words are similar.", "labels": [], "entities": []}, {"text": "Similarity is neither symmetric nor transitive.", "labels": [], "entities": []}, {"text": "The model achieved accuracy of 97.6% on a published semantic similarity test.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9997245669364929}]}], "introductionContent": [{"text": "Semantic representation models are described by as belonging to one of three families, semantic networks, feature-based models and semantic spaces.", "labels": [], "entities": []}, {"text": "Briefly, semantic networks represent words as nodes in a graph and the semantic relations between them as edges, and similarity between words is represented by the path length between them.", "labels": [], "entities": []}, {"text": "Edges may represent a variety of different relations.", "labels": [], "entities": []}, {"text": "Feature-based models assign a list of discrete features to each word, and similarity of words is obtained from the commonalities and differences of their feature sets.", "labels": [], "entities": [{"text": "similarity", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9686608910560608}]}, {"text": "As indicated by, semantic networks and feature-based models are often manually created by modelers, so that an effort is required to produce them, and the results are subjective.", "labels": [], "entities": []}, {"text": "Semantic spaces, also named DSMs (distributional semantic models) by, rely on the distributional hypothesis, that words that occur in the same contexts tend to have similar meanings.", "labels": [], "entities": []}, {"text": "At their most basic form, word co-occurrences in various contexts are used to form feature vectors of words.", "labels": [], "entities": []}, {"text": "DSMs are divided by into unstructured DSMs, where word co-occurrences are counted without regard to the relation between the words, and structured DSMs, where triples of two words and particular syntactic or lexico-syntactic relations between them are counted.", "labels": [], "entities": []}, {"text": "Various feature weighting schemes are employed, and a VSM (vector space model) is usually formed using the feature vectors (topic modeling () is a notable exception).", "labels": [], "entities": []}, {"text": "Similarities between words are measured by distances between vectors in this multi-dimensional space, usually following a dimensionality reduction.", "labels": [], "entities": []}, {"text": "VSMs have been successful in a number of tasks, such as word similarity and word-relation similarity tests.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7942171096801758}, {"text": "word-relation similarity tests", "start_pos": 76, "end_pos": 106, "type": "TASK", "confidence": 0.7666837771733602}]}, {"text": "However, VSMs have several shortcomings.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.8569737076759338}]}, {"text": "Placing all words in a multi-dimensional space, with greater distance between any two words signifying lower similarity between them, implies: \u2022 All words have some similarity with one another.", "labels": [], "entities": []}, {"text": "\u2022 For any word, all other words can be ordered by their similarity to the given word.", "labels": [], "entities": []}, {"text": "\u2022 All pairs of words can be ordered by their similarity.", "labels": [], "entities": []}, {"text": "\u2022 Transitivity -if any two words are both very similar to a third word, they cannot be very dissimilar.", "labels": [], "entities": []}, {"text": "\u2022 All instances of a word, whether the word is ambiguous, polysemous, or attains different meanings in different contexts, are mapped to the same position in space.", "labels": [], "entities": []}, {"text": "It is our view that for similarity to exist between two concepts (represented in our case by words), they must have something in common, such as a common dimension along which they have (possibly different) values.", "labels": [], "entities": []}, {"text": "With nothing in common, two concepts bear no similarity to one another, which is not the same as having little similarity.", "labels": [], "entities": []}, {"text": "As with relatives, some are close relatives of a person, others are more distant relations of his, and yet others are not related to him at all.", "labels": [], "entities": []}, {"text": "Furthermore, similarity is ordinal, with numerical values, when given by human subjects, serving as an aid in ranking similarity, as is done with feelings of pain, or happiness.", "labels": [], "entities": []}, {"text": "Let's illustrate some limitations of VSMs with examples.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.8618873953819275}]}, {"text": "Example 1: is 'bank' more similar to 'embankment' or to 'stock exchange'?", "labels": [], "entities": []}, {"text": "'bank' is ambiguous, so a possible solution would be to map these two different senses independently to different positions in multi-dimensional space, assuming one could automatically disambiguate them.", "labels": [], "entities": []}, {"text": "Example 2: is 'break' more similar to 'interrupt', 'separate', 'breach', 'burst', or 'violate'?", "labels": [], "entities": []}, {"text": "'break' is polysemous, with WordNet 1 listing 59 senses, which are in various degrees related to one another, just for the verb.", "labels": [], "entities": [{"text": "WordNet 1", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.9334732890129089}]}, {"text": "In this case, it does not seem right to map each sense independently, as they share some meaning.", "labels": [], "entities": []}, {"text": "Example 3: is 'queen' more similar to 'king' or to 'woman'?", "labels": [], "entities": []}, {"text": "The court advisers may have one opinion, and the queen's physician another.", "labels": [], "entities": []}, {"text": "Similarly for 'man' vs. 'woman' and 'boy', or 'cat' vs. 'stuffed cat' and 'dog'.", "labels": [], "entities": []}, {"text": "When VSMs are formed from a corpus, context is given by the corpus for all instances of all words as a package deal, and vectors of words are based on that context.", "labels": [], "entities": []}, {"text": "Example 4: How similar is 'cat' to 'submarine'?", "labels": [], "entities": []}, {"text": "If we find nothing in common, there is no similarity, and the question doesn't seem to make sense.", "labels": [], "entities": []}, {"text": "As with partially ordered sets, some pairs of words are related to one another, while others are not.", "labels": [], "entities": []}, {"text": "1 http://wordnet.princeton.edu Example 5: Is 'flat' more similar to For some questions of this kind we may have a firm opinion, for others we may not be so sure, and some questions don't really make sense.", "labels": [], "entities": []}, {"text": "However, a VSM will have definite answers to all questions in the above examples, regardless of sense or context.", "labels": [], "entities": [{"text": "VSM", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.7950648665428162}]}, {"text": "Moreover, the symmetry and transitivity of similarity imposed by VSMs contradict human similarity judgments.", "labels": [], "entities": []}, {"text": "These constraints of VSMs are due to the symmetry and triangle inequality conditions that must be satisfied by any distance function.", "labels": [], "entities": []}, {"text": "In addition, show that geometric models impose an upper bound on the number of points that can share the same nearest neighbor, and that particularly for conceptual data (such as categorical ratings or associations of words), values for these exceed those possible in geometric models.", "labels": [], "entities": []}, {"text": "It has been suggested by that \"similarity maybe better characterized as a feature-matching process based on the weighting of common and distinctive features than as a metric-distance function\".", "labels": [], "entities": []}, {"text": "The model we propose makes use of word co-occurrence in a corpus to build a feature-based model of semantic representation.", "labels": [], "entities": []}, {"text": "We use sentence limits as our context window, and measure the distance (counted in the number of intervening words) between pairs of words that co-occur in sentences.", "labels": [], "entities": []}, {"text": "It is found that fora word, its mean pmf (probability mass function) of distance with its pair-words (hence termed PDD -pair distance distribution) characterizes it across corpora, and that semantically similar words have a similar mean PDD.", "labels": [], "entities": [{"text": "mean pmf (probability mass function)", "start_pos": 32, "end_pos": 68, "type": "METRIC", "confidence": 0.7708348291260856}]}, {"text": "Given a word, its features in our model are its pair-words (those that co-occur with it within sentences), together with the frequency of pair occurrence and its PDD.", "labels": [], "entities": [{"text": "PDD", "start_pos": 162, "end_pos": 165, "type": "METRIC", "confidence": 0.852221667766571}]}, {"text": "Thus we take into account word order, which is disregarded by 'bag-of-words' models.", "labels": [], "entities": []}, {"text": "As no sparse matrices are created, no dimensionality reduction is required.", "labels": [], "entities": []}, {"text": "This makes our model scalable both in computation and in storage, but more importantly, the 'dimensions' along which we compare words are the feature words, which are clear and meaningful.", "labels": [], "entities": []}, {"text": "This stands in contrast to the dimensions obtained following a dimensionality reduction, the meanings of which often aren't clear.", "labels": [], "entities": []}, {"text": "As words are not mapped into highdimensional spaces, and consequently similarities are not measured with distances, the shortcomings of VSMs are avoided.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows: Section 2 gives details of the semantic representation model.", "labels": [], "entities": []}, {"text": "In section 3, an algorithm for evaluating similarity, based on our model, is presented.", "labels": [], "entities": [{"text": "similarity", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9099715948104858}]}, {"text": "In section 4, experiments and their results are presented.", "labels": [], "entities": []}, {"text": "Section 5 discusses the scalability of our method, and section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}