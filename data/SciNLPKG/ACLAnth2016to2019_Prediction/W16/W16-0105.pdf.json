{"title": [{"text": "Neural Enquirer: Learning to Query Tables in Natural Language", "labels": [], "entities": [{"text": "Neural Enquirer", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6806011646986008}]}], "abstractContent": [{"text": "We propose NEURAL ENQUIRER-a neu-ral network architecture for answering natural language (NL) questions given a knowledge base (KB) table.", "labels": [], "entities": [{"text": "NEURAL ENQUIRER-a neu-ral network", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.635433129966259}, {"text": "answering natural language (NL) questions", "start_pos": 62, "end_pos": 103, "type": "TASK", "confidence": 0.7030112019606999}]}, {"text": "Unlike previous work on end-to-end training of semantic parsers, NEU-RAL ENQUIRER is fully \"neuralized\": it gives distributed representations of queries and KB tables, and executes queries through a series of differentiable operations.", "labels": [], "entities": [{"text": "NEU-RAL ENQUIRER", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.8310605883598328}]}, {"text": "The model can be trained with gradient descent using both end-to-end and step-by-step supervision.", "labels": [], "entities": []}, {"text": "During training the representations of queries and the KB table are jointly optimized with the query execution logic.", "labels": [], "entities": []}, {"text": "Our experiments show that the model can learn to execute complex NL queries on KB tables with rich structures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language dialogue and question answering often involve querying a knowledge base).", "labels": [], "entities": [{"text": "Natural language dialogue", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6879125833511353}, {"text": "question answering", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8556155562400818}]}, {"text": "The traditional approach involves two steps: First, a given query\u02dcQquery\u02dc query\u02dcQ is semantically parsed into an \"executable\" representation, which is often expressed in certain logical form\u02dcZform\u02dc form\u02dcZ (e.g., SQL-like queries).", "labels": [], "entities": []}, {"text": "Second, the representation is executed against a KB from which an answer is obtained.", "labels": [], "entities": []}, {"text": "For queries that involve complex semantics and logic (e.g., \"Which city hosted the longest Olympic Games before the Games in Beijing?\"), semantic parsing and query execution become extremely complex.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.7177928686141968}, {"text": "query execution", "start_pos": 158, "end_pos": 173, "type": "TASK", "confidence": 0.798196405172348}]}, {"text": "For example, carefully hand-crafted features and rules are needed to correctly parse a complex query into its logical form (see example in the lower-left corner of.", "labels": [], "entities": []}, {"text": "To partially overcome this complexity, recent works ( attempt to \"backpropagate\" query execution results to revise the semantic representation of a query.", "labels": [], "entities": []}, {"text": "This approach, however, is greatly hindered by the fact that traditional semantic parsing mostly involves rule-based features and symbolic manipulation, and is subject to intractable search space incurred by the great flexibility of natural language.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.7659159898757935}]}, {"text": "In this paper we propose NEURAL ENQUIRER -a neural network system that learns to understand NL queries and execute them on a KB table from examples of queries and answers.", "labels": [], "entities": [{"text": "NEURAL ENQUIRER", "start_pos": 25, "end_pos": 40, "type": "METRIC", "confidence": 0.6633720993995667}]}, {"text": "Unlike similar efforts along this line of research, NEURAL ENQUIRER is a fully neuralized, end-to-end differentiable network that jointly models semantic parsing and query execution.", "labels": [], "entities": [{"text": "NEURAL ENQUIRER", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.40702351927757263}, {"text": "semantic parsing", "start_pos": 145, "end_pos": 161, "type": "TASK", "confidence": 0.6829936653375626}]}, {"text": "It encodes queries and KB tables into distributed representations, and executes compositional queries against the KB through a series of differentiable operations.", "labels": [], "entities": []}, {"text": "The model is trained using queryanswer pairs, where the distributed representations of queries and the KB are optimized together with the query execution logic in an end-to-end fashion.", "labels": [], "entities": []}, {"text": "We demonstrate using a synthetic QA task that NEURAL ENQUIRER is capable of learning to execute complex compositional NL questions.", "labels": [], "entities": [{"text": "NEURAL ENQUIRER", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.4381272792816162}]}], "datasetContent": [{"text": "In this section we evaluate NEURAL ENQUIRER on synthetic QA tasks with NL queries of varying compositional depths.", "labels": [], "entities": [{"text": "NEURAL ENQUIRER", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.4986315965652466}]}, {"text": "In this section we discuss the results under endto-end (N2N) training setting.", "labels": [], "entities": []}, {"text": "On MIXED-25K, the relatively low performance of SEMPRE indicates that our QA task, although synthetic, is highly nontrivial.", "labels": [], "entities": [{"text": "MIXED-25K", "start_pos": 3, "end_pos": 12, "type": "DATASET", "confidence": 0.8759474754333496}, {"text": "SEMPRE", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.6802047491073608}]}, {"text": "Surprisingly, our model outperforms SEMPRE on all types of queries, with a marginal gain on simple queries (SELECT WHERE, SU-PERLATIVE), and significant improvement on complex queries (WHERE SUPERLATIVE, NEST).", "labels": [], "entities": [{"text": "SELECT", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9819195866584778}]}, {"text": "On MIXED-100K, our model achieves a decent overall accuracy of 90.6%.", "labels": [], "entities": [{"text": "MIXED-100K", "start_pos": 3, "end_pos": 13, "type": "DATASET", "confidence": 0.8166800141334534}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9996351003646851}]}, {"text": "These results show that in our QA task, NEURAL ENQUIRER is very effective in answering compositional NL queries, especially those with complex semantic structures compared with the state-of-the-art system.", "labels": [], "entities": [{"text": "NEURAL ENQUIRER", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.6447741091251373}, {"text": "answering compositional NL queries", "start_pos": 77, "end_pos": 111, "type": "TASK", "confidence": 0.9227375984191895}]}, {"text": "To further understand why our model is capable of answering compositional queries, we study the attention weights\u02dcwweights\u02dc weights\u02dcw(\u00b7) of Readers (Eq.", "labels": [], "entities": [{"text": "answering compositional queries", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.8959529797236124}]}, {"text": "1) for executors in intermediate layers, and the answer probability (Eq.", "labels": [], "entities": []}, {"text": "2) the last executor outputs for each entry in the table.", "labels": [], "entities": []}, {"text": "Those statistics are obtained on MIXED-100K.", "labels": [], "entities": [{"text": "MIXED-100K", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9682103991508484}]}, {"text": "We sample two queries (Q 1 and Q 2 ) in the testing set that our model answers correctly and visualize their corresponding values in.", "labels": [], "entities": []}, {"text": "To Q 1 : How long was the game with the most medals that had fewer than 3,000 participants?", "labels": [], "entities": []}, {"text": "better understand the query execution process, we also give the logical forms (Z 1 and Z 2 ) of the two queries.", "labels": [], "entities": []}, {"text": "Note that the logical forms are just for reference purpose and unknown by the model.", "labels": [], "entities": []}, {"text": "We find that each executor actually learns its execution logic from just the correct answers in N2N training, which is in accordance with our assumption.", "labels": [], "entities": []}, {"text": "The model executes Q 1 in three steps, with each of the last three executors performs a specific type of operation.", "labels": [], "entities": []}, {"text": "For each row, Executor-3 takes the value of the # participants field as input, while Executor-4 attends to the # medals field.", "labels": [], "entities": []}, {"text": "Finally, Executor-5 outputs a high probability for the # duration field in the 3-rd row.", "labels": [], "entities": [{"text": "duration field", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.9367314577102661}]}, {"text": "The attention weights for Executor-1 and Executor-2 appear to be meaningless because Q 1 requires only three steps of execution, and the model learns to defer the meaningful execution to the last three executors.", "labels": [], "entities": []}, {"text": "Compared with the logical form Z 1 of Q 1 , we can deduce that Executor-3 \"executes\" the where clause in Z 1 to find row sets R satisfying the condition, and Executor-4 performs the first part of argmax to find the row r \u2208 R with the maximum value of # medals, while Executor-5 outputs the value of # duration in row r.", "labels": [], "entities": [{"text": "argmax", "start_pos": 196, "end_pos": 202, "type": "METRIC", "confidence": 0.9791103005409241}, {"text": "duration", "start_pos": 301, "end_pos": 309, "type": "METRIC", "confidence": 0.7833607792854309}]}, {"text": "Compared with Q 1 , Q 2 is more complicated.", "labels": [], "entities": []}, {"text": "According to Z 2 , Q 2 involves an additional nest subquery to be solved by two extra executors, and requires a total of five steps of execution.", "labels": [], "entities": []}, {"text": "The last three executors function similarly as in answering Q 1 , yet the execution logic for the first two executors (devoted to solving the sub-query) is a bit obscure, since their attention weights are scattered instead of being perfectly centered on the ideal fields as highlighted in red dashed rectangles.", "labels": [], "entities": [{"text": "answering Q 1", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.8349588314692179}]}, {"text": "We posit that this is because during the end-to-end training, the supervision signal propagated from the top layer has decayed along the long path down to the first two executors, which causes vanishing gradients.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example queries for each query type, with annotated SQL-like logical form templates", "labels": [], "entities": []}, {"text": " Table 2: Accuracies on MIXED datasets", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9974332451820374}, {"text": "MIXED datasets", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.844337671995163}]}]}