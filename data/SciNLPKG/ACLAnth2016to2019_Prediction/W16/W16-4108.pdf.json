{"title": [{"text": "Syntactic and Lexical Complexity in Italian Noncanonical Structures", "labels": [], "entities": [{"text": "Syntactic and Lexical Complexity", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.841872900724411}]}], "abstractContent": [{"text": "In this paper we will be dealing with different levels of complexity in the processing of Italian, a Romance language inheriting many properties from Latin which make it an almost free word order language 1.", "labels": [], "entities": []}, {"text": "The paper is concerned with syntactic complexity as measurable on the basis of the cognitive parser that incrementally builds up a syntactic representation to be used by the semantic component.", "labels": [], "entities": []}, {"text": "The theory behind will be LFG and parsing preferences will be used to justify one choice both from a principled and a processing point of view.", "labels": [], "entities": []}, {"text": "LFG is a transformationless theory in which there is no deep structure separate from surface syntactic structure.", "labels": [], "entities": [{"text": "LFG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7967097759246826}]}, {"text": "This is partially in accordance with constructional theories in which noncanonical structures containing non-argument functions FOCUS/TOPIC are treated as multifunctional constituents.", "labels": [], "entities": [{"text": "TOPIC", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.3296518325805664}]}, {"text": "Complexity is computed on a processing basis following suggestions made by Blache and demonstrated by Kluender and Chesi.", "labels": [], "entities": []}, {"text": "1 Introduction In this paper we will be addressing what the CFP of the workshop has defined as \"whether, and to what extent, linguistic phenomena hampering human processing correlate with difficulties in the automatic processing of language\".", "labels": [], "entities": [{"text": "CFP", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9657312035560608}]}, {"text": "This will be done by presenting work done in the past on the topic of noncanonical and difficult to parser syntactic structures that may create ambiguity at phonological level, and how to solve it.", "labels": [], "entities": []}, {"text": "In that case, the goal was creating an automatic system for text-to-speech, i.e. a TTS system.", "labels": [], "entities": []}, {"text": "This will be completed by showing results of an experiment done with statistical data-driven dependency parsers-and a rule-based one-analysing a highly noncanonical text type-children stories or fables.", "labels": [], "entities": []}, {"text": "I will show the evaluation done both on the Italian text and the corresponding English translation.", "labels": [], "entities": []}, {"text": "Current approaches to deep syntactic-semantic natural language modeling are strongly statistically based and have achieved near 90% accuracy in some cases.", "labels": [], "entities": [{"text": "deep syntactic-semantic natural language modeling", "start_pos": 22, "end_pos": 71, "type": "TASK", "confidence": 0.6301119446754455}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9993879795074463}]}, {"text": "The problem with statistical modeling is that they are strictly bound to training material.", "labels": [], "entities": [{"text": "statistical modeling", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.8698619902133942}]}, {"text": "Achieving generality requires mixing diverse domains in the training data.", "labels": [], "entities": []}, {"text": "In such cases, accuracy varies a lot depending on the language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9989218711853027}]}, {"text": "In particular, more canonical languages achieve 85% accuracy on average-this includes English in non-projective structures and 90% accuracy on projective ones 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9986479878425598}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9977908134460449}]}, {"text": "Less canonical languages, like Italian for instance, are below that threshold and average 82/83% accuracy 3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.7444216012954712}]}, {"text": "The question is that syntactic processing is just one step towards natural language understanding, and the hope to cover sentence level semantics in the near future is not very close.", "labels": [], "entities": [{"text": "syntactic processing", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7376579940319061}, {"text": "natural language understanding", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.6942750016848246}]}, {"text": "In addition, if we consider Italian, the results are strongly flawed by the fact that dependency parsers are not equipped for lexically unexpressed categories as the Null Subject, which in Italian constitute some 60% of all Subject cases.", "labels": [], "entities": []}, {"text": "Languages containing Null Subjects also include Chinese where the recognition rate for this language of such Subject positions by current state-of-art statistical parsers averages 50% 1 Parameters usually referred to when defining \"free\" word order languages like Latin include: lack of articles, Null Subjects, lack of expletives, freely omitting the complementizer, intensive case marking, etc.", "labels": [], "entities": [{"text": "intensive case marking", "start_pos": 368, "end_pos": 390, "type": "TASK", "confidence": 0.5853653252124786}]}, {"text": "to quote the most important ones.", "labels": [], "entities": []}, {"text": "Italian only has some of them and the resulting constructions fora simple declarative clause may include all possible permutations at constituent level, but not at word level as Latin for instance would do.", "labels": [], "entities": []}, {"text": "2 Experiments with different domains for test and training are reported in Surdenau et al.", "labels": [], "entities": []}, {"text": "2008 wherein particular the conplete task with WSJ(Marcus et al.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8483805060386658}]}, {"text": "1993) averages 86% F measure, when Brown(Francis & Kucera 1967) is used it drops to 76%.", "labels": [], "entities": [{"text": "F measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9161297976970673}, {"text": "Brown(Francis & Kucera 1967)", "start_pos": 35, "end_pos": 63, "type": "DATASET", "confidence": 0.7148642539978027}]}, {"text": "Average performance for 5 different domains when training and test domain diverge show a significant drop whenever we move from WSJ 89% to biomedical domain GENIA 66.6%, to dialogue domain of SwitchBoard 69% and to Brown 80%.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we will be addressing what the CFP of the workshop has defined as \"whether, and to what extent, linguistic phenomena hampering human processing correlate with difficulties in the automatic processing of language\".", "labels": [], "entities": [{"text": "CFP", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9625979661941528}]}, {"text": "This will be done by presenting work done in the past on the topic of noncanonical and difficult to parser syntactic structures that may create ambiguity at phonological level, and how to solve it.", "labels": [], "entities": []}, {"text": "In that case, the goal was creating an automatic system for text-to-speech, i.e. a TTS system.", "labels": [], "entities": []}, {"text": "This will be completed by showing results of an experiment done with statistical data-driven dependency parsers -and a rule-based one -analysing a highly noncanonical text typechildren stories or fables.", "labels": [], "entities": []}, {"text": "I will show the evaluation done both on the Italian text and the corresponding English translation.", "labels": [], "entities": []}, {"text": "Current approaches to deep syntactic-semantic natural language modeling are strongly statistically based and have achieved near 90% accuracy in some cases.", "labels": [], "entities": [{"text": "deep syntactic-semantic natural language modeling", "start_pos": 22, "end_pos": 71, "type": "TASK", "confidence": 0.6301119446754455}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9993879795074463}]}, {"text": "The problem with statistical modeling is that they are strictly bound to training material.", "labels": [], "entities": [{"text": "statistical modeling", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.8698619902133942}]}, {"text": "Achieving generality requires mixing diverse domains in the training data.", "labels": [], "entities": []}, {"text": "In such cases, accuracy varies a lot depending on the language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9989218711853027}]}, {"text": "In particular, more canonical languages achieve 85% accuracy on average -this includes English in non-projective structures and 90% accuracy on projective ones 2 . Less canonical languages, like Italian for instance, are below that threshold and average 82/83% accuracy . The question is that syntactic processing is just one step towards natural language understanding, and the hope to cover sentence level semantics in the near future is not very close.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9974754452705383}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9957200884819031}, {"text": "accuracy", "start_pos": 261, "end_pos": 269, "type": "METRIC", "confidence": 0.9918010830879211}, {"text": "natural language understanding", "start_pos": 339, "end_pos": 369, "type": "TASK", "confidence": 0.6860629121462504}]}, {"text": "In addition, if we consider Italian, the results are strongly flawed by the fact that dependency parsers are not equipped for lexically unexpressed categories as the Null Subject, which in Italian constitute some 60% of all Subject cases.", "labels": [], "entities": []}, {"text": "Languages containing Null Subjects also include Chinese where the recognition rate for this language of such Subject positions by current state-of-art statistical parsers averages 50% Parameters usually referred to when defining \"free\" word order languages like Latin include: lack of articles, Null Subjects, lack of expletives, freely omitting the complementizer, intensive case marking, etc.", "labels": [], "entities": [{"text": "intensive case marking", "start_pos": 366, "end_pos": 388, "type": "TASK", "confidence": 0.5910630524158478}]}, {"text": "to quote the most important ones.", "labels": [], "entities": []}, {"text": "Italian only has some of them and the resulting constructions fora simple declarative clause may include all possible permutations at constituent level, but not at word level as Latin for instance would do.", "labels": [], "entities": []}, {"text": "Experiments with different domains for test and training are reported in wherein particular the conplete task with WSJ() averages 86% F measure, when Brown) is used it drops to 76%.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.8198884725570679}, {"text": "F measure", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9850043058395386}]}, {"text": "Average performance for 5 different domains when training and test domain diverge show a significant drop whenever we move from WSJ 89% to biomedical domain GENIA 66.6%, to dialogue domain of SwitchBoard 69% and to Brown 80%.", "labels": [], "entities": []}, {"text": "This value has been reported in a mail thread by Giuseppe Attardi experimenting with Universal Dependencies treebanks and the newly released SyntaNet, \"the world's most accurate parser\", as it is publicized on the accuracy.", "labels": [], "entities": [{"text": "Universal Dependencies treebanks", "start_pos": 85, "end_pos": 117, "type": "DATASET", "confidence": 0.8849088748296102}, {"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9987541437149048}]}, {"text": "Semantic processing is thus highly flawed by the grammatically incomplete structures produced by data-driven dependency parsers.", "labels": [], "entities": []}, {"text": "An important factor that determines levels of complexity in linguistic data are presence of noncanonical structures which in some languages and some domains are almost negligible, as for instance English in the corpus constituted by WSJ news -see below.", "labels": [], "entities": [{"text": "WSJ news", "start_pos": 233, "end_pos": 241, "type": "DATASET", "confidence": 0.8759605884552002}]}, {"text": "But when we move to the BROWN corpus the presence of such structures is important and determines a decrease inaccuracy and a drop in performance that can range from 6 to 8 points.", "labels": [], "entities": [{"text": "BROWN corpus", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9052153825759888}]}, {"text": "Italian on the contrary is very rich on such non-canonical structures including discontinuities of all sorts, but as before some genre or domain has more than others.", "labels": [], "entities": []}, {"text": "The paper is organized into two main sections, the following one, section 2 devoted to performance related cases of complexity where we take the stance to use LFG theory and Parsing Strategies to explain ambiguity in the data.", "labels": [], "entities": []}, {"text": "We will then present results of a study carried out on non-canonical Italian sentences as they have been treated by most well-known parsers of Italian.", "labels": [], "entities": []}, {"text": "In this section we will then show results from an experiment with current best statistical data-driven dependency parsers of Italian when presented with a highly non-canonical text.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this final section we will present results of the analysis of a highly non-canonical text, the fable of \"the 3 little pigs\"(see Appendix 1).", "labels": [], "entities": []}, {"text": "We have been using what are regarded best parsers of Italian today, as they have been evaluated in EVALITA; see www.evalita.it) evaluation campaigns.", "labels": [], "entities": [{"text": "EVALITA", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.6635908484458923}]}, {"text": "Most of them are accessible on the web 7 . Two of the parsers are fully statistical . TextPro and DeSR -while the others are hybrid rule-based/statistical parsers -TULE and VISL.", "labels": [], "entities": [{"text": "TextPro", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9219352602958679}]}, {"text": "We checked the output of the four parsers and marked both labeled and dependency errors.", "labels": [], "entities": []}, {"text": "Results are shown in the table below.", "labels": [], "entities": []}, {"text": "In. we report errors made by each parser, divided up into two classes, Labels and Dependencies.", "labels": [], "entities": []}, {"text": "We then indicate number of words, and words with punctuation that we use to make statistics on Error Rate of each parser.", "labels": [], "entities": [{"text": "Error Rate", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.982066422700882}]}, {"text": "As can be easily gathered, Mean values average well over 20% error rate with a resulting accuracy of less than 80%.", "labels": [], "entities": [{"text": "Mean", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9891653656959534}, {"text": "error rate", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9851131439208984}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9993201494216919}]}, {"text": "Another important measure we report is number of fully errorless sentences: their means is below 5 over 25, that is only one fifth of the sentences are able to provide a correct mapping to semantics.", "labels": [], "entities": []}, {"text": "The parsers share their fully correct sentences on the total number of five which are in fact the ones containing only canonical structures.", "labels": [], "entities": []}, {"text": "All remaining sentences have one or more noncanonical structure, thus showing that statistical data-driven parsers are unfit to cope with narrative Italian text.", "labels": [], "entities": []}, {"text": "This is partly due to their models which are unable to generalize to noncanonical structure.", "labels": [], "entities": []}, {"text": "In addition, their algorithms are unable to use global measures of grammaticality to improve their local choices, and in many cases cannot correct the choice of labeling as OBJect an inverted or simply dislocated NP, when the SUBJect is eventually missing, thus leaving the structure without a SUBJect.", "labels": [], "entities": []}, {"text": "We then used the same text in its English translation to check what Stanford Parser was able to do.", "labels": [], "entities": []}, {"text": "Translating Italian fables into English erases most cases of noncanonical structures so that the result is a much simpler (canonical) text to parse.", "labels": [], "entities": []}, {"text": "As can be seen, now the number of fully correct sentence is over half the whole set 14/27 and error rate has decreased dramatically to 10% yielding a 90% accuracy.", "labels": [], "entities": [{"text": "error rate", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9906234443187714}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.997784435749054}]}, {"text": "In the Appendix, we report the Italian text, where we marked with underscores all noncanonical structures, and with italics all sequences of ambiguous attachment which may cause errors.", "labels": [], "entities": [{"text": "Italian text", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.8564809262752533}]}, {"text": "In addition, the story has 37 Null Subjects which have been correctly found only by TULE parser -but not bound to an antecedent or syntactic controller 8 . Then there are 9 long distance dependencies, relative and wh-clauses, all correctly bound again by TULE.", "labels": [], "entities": [{"text": "TULE", "start_pos": 255, "end_pos": 259, "type": "DATASET", "confidence": 0.8921000957489014}]}, {"text": "And 9 additional so-called open adjuncts, participials and adjectival phrases which have only been marked as verb dependent, but which also need argument dependency, since they all require agreement to be checked -this being a shortcoming of dependency structure representation.", "labels": [], "entities": []}, {"text": "None of these structures have been marked, and in fact they constitute one of the elements causing main errors, also for Stanford parser.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Non-projective/noncanonical structures in VIT divided up by functional types.", "labels": [], "entities": [{"text": "VIT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8665764331817627}]}, {"text": " Table 2: NonCanonicalStructures and Unexpressed Subjects in VIT and PT.", "labels": [], "entities": [{"text": "VIT and PT", "start_pos": 61, "end_pos": 71, "type": "TASK", "confidence": 0.6502028505007426}]}, {"text": " Table 3: Experiment with four parsers of Italian + Stanford LP", "labels": [], "entities": []}]}