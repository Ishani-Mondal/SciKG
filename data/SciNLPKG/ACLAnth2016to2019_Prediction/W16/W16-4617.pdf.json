{"title": [{"text": "Character-based Decoding in Tree-to-Sequence Attention-based Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.625326802333196}]}], "abstractContent": [{"text": "This paper reports our systems (UT-AKY) submitted in the 3rd Workshop of Asian Translation 2016 (WAT'16) and their results in the English-to-Japanese translation task.", "labels": [], "entities": [{"text": "Asian Translation 2016 (WAT'16)", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.7282886157433192}, {"text": "English-to-Japanese translation task", "start_pos": 130, "end_pos": 166, "type": "TASK", "confidence": 0.7616700530052185}]}, {"text": "Our model is based on the tree-to-sequence Attention-based NMT (ANMT) model proposed by Eriguchi et al.", "labels": [], "entities": []}, {"text": "We submitted two ANMT systems: one with a word-based decoder and the other with a character-based decoder.", "labels": [], "entities": [{"text": "ANMT", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.815069317817688}]}, {"text": "Experimenting on the English-to-Japanese translation task, we have confirmed that the character-based decoder can cover almost the full vocabulary in the target language and generate translations much faster than the word-based model.", "labels": [], "entities": [{"text": "English-to-Japanese translation task", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.7562088171641032}]}], "introductionContent": [{"text": "End-to-end Neural Machine Translation (NMT) models have recently achieved state-of-the-art results in several translation tasks.", "labels": [], "entities": [{"text": "End-to-end Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7529710701533726}, {"text": "translation tasks", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.904055655002594}]}, {"text": "Those NMT models are based on the idea of sequence-to-sequence learning, where both of the source and the target sentences are considered as a sequence of symbols (e.g. words or characters) and they are directly converted via a vector space.", "labels": [], "entities": []}, {"text": "The sequence of symbols on the source side is input into a vector space, and the sequence of symbols on the target side is output from the vector space.", "labels": [], "entities": []}, {"text": "In the end-to-end NMT models, the above architectures are embodied by a single neural network.", "labels": [], "entities": []}, {"text": "The optimal unit for NMT is an important research question discussed in the community.", "labels": [], "entities": [{"text": "NMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9176095128059387}]}, {"text": "Early NMT models employ a word as a unit of the sequence ().", "labels": [], "entities": []}, {"text": "have used a Byte-Pair Encoding (BPE) method to create a sub-word level vocabulary according to the frequencies of sub-word appearance in the corpus.", "labels": [], "entities": [{"text": "Byte-Pair Encoding (BPE)", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.6751194179058075}]}, {"text": "They successfully replaced a large word vocabulary in German and Russian with a much smaller sub-word vocabulary.", "labels": [], "entities": []}, {"text": "They have also shown that their sub-word-based NMT model gives better translations than the word-based NMT models.", "labels": [], "entities": []}, {"text": "The smallest unit of a sequence of text data is a character.", "labels": [], "entities": []}, {"text": "The character-based approach has attracted much attention in the field of NMT, because it enables an NMT model to handle all of the tokens in the corpus.", "labels": [], "entities": []}, {"text": "A hybrid model of the word-based and the character-based model has also been proposed by.", "labels": [], "entities": []}, {"text": "These studies reported the success and effectiveness in translating the out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "In this paper, we apply character-based decoding to a tree-based NMT model ().", "labels": [], "entities": []}, {"text": "The existing character-based models focus only on the sequence-based NMT models.", "labels": [], "entities": []}, {"text": "The objective of this paper is to analyze the results of the character-based decoding in the tree-based NMT model.", "labels": [], "entities": []}, {"text": "We also enrich the tree-based encoder with syntactic features.", "labels": [], "entities": []}, {"text": "shows an overview of our system.", "labels": [], "entities": []}, {"text": "We conducted the English-to-Japanese translation task on the WAT'16 dataset.", "labels": [], "entities": [{"text": "English-to-Japanese translation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.6140386760234833}, {"text": "WAT'16 dataset", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.9728622138500214}]}, {"text": "The results of our characterbased decoder model show that its translation accuracy is lower than that of the word-based decoder model by 1.34 BLEU scores, but the character-based decoder model needed much less time to generate a sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9609261751174927}, {"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9989677667617798}]}], "datasetContent": [{"text": "We conducted experiments for our system using the 3rd Workshop of Asian Translation 2016 (WAT'16) English-to-Japanese translation task (  half of train-2.txt.", "labels": [], "entities": [{"text": "Asian Translation 2016 (WAT'16) English-to-Japanese translation", "start_pos": 66, "end_pos": 129, "type": "TASK", "confidence": 0.8130435608327389}]}, {"text": "We removed the sentences whose lengths are greater than 50 words.", "labels": [], "entities": []}, {"text": "In the tree-based encoder, binary trees of the source sentences were obtained by Enju (, which is a probabilistic HPSG parser.", "labels": [], "entities": []}, {"text": "We used phrase category labels as the syntactic features in the proposed treebased encoder.", "labels": [], "entities": []}, {"text": "There are 19 types of phrase category labels given by Enju.", "labels": [], "entities": []}, {"text": "In the word-based decoder model, we employed KyTea (Neubig et al., 2011) as the segmentation tool for the Japanese sentences.", "labels": [], "entities": []}, {"text": "We performed the preprocessing steps of the data as recommended in WAT'16. 2 and show the details of the final dataset and the vocabulary sizes in our experiments.", "labels": [], "entities": [{"text": "WAT'16. 2", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.7450728416442871}]}, {"text": "Each vocabulary includes the words and the characters whose frequencies exceed five or two in the training data, respectively.", "labels": [], "entities": []}, {"text": "The out-of-vocabulary words are mapped into a special token i.e. \"UNK\".", "labels": [], "entities": [{"text": "UNK", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.5196189284324646}]}, {"text": "As a result, the vocabulary size of the characters in Japanese is about 22 times smaller than that of the words.", "labels": [], "entities": []}, {"text": "NMT models are often trained on a limited vocabulary, because the high computational cost of the softmax layer for target word generation is usually the bottleneck when training an NMT model.", "labels": [], "entities": []}, {"text": "In the word-based models, we use the BlackOut sampling method () to approximately compute the softmax layer.", "labels": [], "entities": []}, {"text": "The parameter setting of BlackOut follows.", "labels": [], "entities": []}, {"text": "In the characterbased models, we use the original softmax in the softmax layer.", "labels": [], "entities": []}, {"text": "All of the models are trained on CPUs.", "labels": [], "entities": []}, {"text": "We employed multi-threading programming to update the parameters in a mini-batch in parallel.", "labels": [], "entities": []}, {"text": "The training times of the single word-based model and the single character-based model were about 11 days and 7 days, respectively.", "labels": [], "entities": []}, {"text": "We set the dimension size of the hidden states to 512 in both of the LSTMs and the Tree LSTMs.", "labels": [], "entities": []}, {"text": "The dimension size of embedding vectors is set to 512 for the words and to 256 for the characters.", "labels": [], "entities": []}, {"text": "In our proposed tree-based encoder, we use 64 and 128 for the dimension size of the phrase label embedding.", "labels": [], "entities": []}, {"text": "The model parameters are uniformly initialized in [\u22120.1, 0.1], except that the forget biases are filled with 1.0 as recommended in.", "labels": [], "entities": []}, {"text": "Biases, softmax weights and BlackOut weights are filled with 0.", "labels": [], "entities": []}, {"text": "We shuffle the training data randomly per each epoch.", "labels": [], "entities": []}, {"text": "All of the parameters are updated by the plain SGD algorithm with a mini-batch size of 128.", "labels": [], "entities": []}, {"text": "The learning rate of SGD is set to 1.0, and we halve it when the perplexity of development data becomes worse.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9598927795886993}, {"text": "SGD", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.957548975944519}]}, {"text": "The value of gradient norm clipping () is set to 3.0.", "labels": [], "entities": []}, {"text": "We use abeam search in order to obtain a proper translation sentence with the size of 20 and 5 in the word-based decoder and the character-based decoder, respectively.", "labels": [], "entities": []}, {"text": "The maximum length of a generated sentence is set to 100 in the word-based decoder and to 300 in the character-based decoder.", "labels": [], "entities": []}, {"text": "reported that an RNN-based decoder generates a shorter sentence when using the original beam search.", "labels": [], "entities": []}, {"text": "We used the beam search method proposed in in order to output longer translations.", "labels": [], "entities": [{"text": "beam search", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.8317405879497528}]}, {"text": "We evaluated the models by the BLEU score () and the RIBES score () employed as the official evaluation metrics in WAT'16.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9658536314964294}, {"text": "RIBES score", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9620448648929596}, {"text": "WAT'16", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.581861138343811}]}, {"text": "shows the experimental results of the character-based models, the word-based models and the baseline SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9790076017379761}]}, {"text": "BP denotes the brevity penalty in the BLEU score.", "labels": [], "entities": [{"text": "BP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9917435050010681}, {"text": "brevity penalty", "start_pos": 15, "end_pos": 30, "type": "METRIC", "confidence": 0.9618819355964661}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.984525740146637}]}, {"text": "First, we can see small improvements in the RIBES score of the single tree-to-sequence ANMT models with the characterbased decoder using syntactic features, compared to our proposed baseline system.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.955274224281311}]}, {"text": "System 1 is one of our submitted systems.", "labels": [], "entities": []}, {"text": "The translations are output by the ensemble of the three models, and we used a simple: The results of our systems and the baseline systems.", "labels": [], "entities": []}, {"text": "beam search to confirm how much it effects the BLEU scores in the character-based models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9986763596534729}]}, {"text": "We showed the results of our proposed character-based decoder models by using the beam search method proposed in.", "labels": [], "entities": []}, {"text": "We collects the statistics of the relation between the source sentence length (L s ) and the target sentence length (L t ) from training dataset and adds its log probability (log p(L t |L s )) as the penalty of the beam score when the model predicts \"EOS\".", "labels": [], "entities": [{"text": "EOS", "start_pos": 251, "end_pos": 254, "type": "METRIC", "confidence": 0.9759960770606995}]}, {"text": "The BLEU score is sensitive to the value of BP, and we observe the same trend in that the character-based approaches generate a shorter sentence by the original beam search.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9646540582180023}, {"text": "BP", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9971296191215515}]}, {"text": "As a result, each of the character-based models can generate longer translation by +0.09 BP scores at least than System 1 using the original beam search.", "labels": [], "entities": [{"text": "BP", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9909045100212097}]}, {"text": "The word-based tree-to-sequece decoder model shows slightly better performance than the wordbased sequence-to-sequence ANMT model () in both of the scores.", "labels": [], "entities": []}, {"text": "The results of the baseline systems are the ones reported in.", "labels": [], "entities": []}, {"text": "Compared to these SMT baselines, each of the character-based models clearly outperforms the phrase-based system in both of the BLEU and RIBES scores.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9874005913734436}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9971143007278442}, {"text": "RIBES", "start_pos": 136, "end_pos": 141, "type": "METRIC", "confidence": 0.8152982592582703}]}, {"text": "Although the hierarchical phrase-based SMT system and the tree-to-string SMT system outperforms the single character-based model without phrase label inputs by +1.04 and by +1.92 BLEU scores, respectively, our best ensemble of character-based models shows better performance (+5.65 RIBES scores) than the tree-to-string SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8043789863586426}, {"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9979744553565979}, {"text": "RIBES", "start_pos": 282, "end_pos": 287, "type": "METRIC", "confidence": 0.995009183883667}, {"text": "SMT", "start_pos": 320, "end_pos": 323, "type": "TASK", "confidence": 0.8972595930099487}]}, {"text": "All the submitted systems are evaluated by pairwise cloudsourcing.", "labels": [], "entities": []}, {"text": "System 1 is ranked as the 9th out of the 10 submitted models, and System 2 is ranked as the 6th.", "labels": [], "entities": []}, {"text": "shows a comparison of the speeds to predict the next word between the word-based decoder and the character-based decoder when generating a sentence by abeam size of 1.", "labels": [], "entities": []}, {"text": "The character-based decoder is about 41 times faster than the word-based decoder.", "labels": [], "entities": []}, {"text": "It is because the time to output a word by using a softmax layer is roughly proportional to the vocabulary sizes of the decoders.", "labels": [], "entities": []}, {"text": "In addition to the low cost of predicting the outputs in the character-based model, the character-based decoder requires the smaller size of beam search than the word-based model.", "labels": [], "entities": []}, {"text": "The word-based decoder requires abeam size of 20 when decoding, but abeam size of 5 is enough for the character-based decoder.", "labels": [], "entities": []}, {"text": "It requires smaller beam size for the character-based decoder to find the best hypothesis.", "labels": [], "entities": []}, {"text": "We therefore conclude that the character-based model works more efficiently as a translation model than the word-based model in terms of the cost of the outputs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The details of dataset in the ASPEC corpus.", "labels": [], "entities": [{"text": "ASPEC corpus", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.8695524334907532}]}, {"text": " Table 3: The results of our systems and the baseline systems.", "labels": [], "entities": []}]}