{"title": [{"text": "Don't Let Notes Be Misunderstood: A Negation Detection Method for Assessing Risk of Suicide in Mental Health Records", "labels": [], "entities": [{"text": "Negation Detection", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.6884923577308655}, {"text": "Assessing Risk of Suicide in Mental Health", "start_pos": 66, "end_pos": 108, "type": "TASK", "confidence": 0.868782273360661}]}], "abstractContent": [{"text": "Mental Health Records (MHRs) contain free-text documentation about patients' suicide and suicidality.", "labels": [], "entities": [{"text": "Mental Health Records (MHRs)", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.7263135264317194}]}, {"text": "In this paper, we address the problem of determining whether grammatic variants (inflections) of the word \"suicide\" are affirmed or negated.", "labels": [], "entities": [{"text": "grammatic variants (inflections) of the word \"suicide\"", "start_pos": 61, "end_pos": 115, "type": "TASK", "confidence": 0.6346199078993364}]}, {"text": "To achieve this, we populate and annotate a dataset with over 6,000 sentences originating from a large repository of MHRs.", "labels": [], "entities": []}, {"text": "The resulting dataset has high Inter-Annotator Agreement (\u03ba 0.93).", "labels": [], "entities": [{"text": "Inter-Annotator Agreement", "start_pos": 31, "end_pos": 56, "type": "METRIC", "confidence": 0.8505666851997375}]}, {"text": "Furthermore, we develop and propose a negation detection method that leverages syntactic features of text 1.", "labels": [], "entities": [{"text": "negation detection", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9670082628726959}]}, {"text": "Using parse trees, we build a set of basic rules that rely on minimum domain knowledge and render the problem as binary classification (affirmed vs. negated).", "labels": [], "entities": []}, {"text": "Since the overall goal is to identify patients who are expected to beat high risk of suicide, we focus on the evaluation of positive (affirmed) cases as determined by our classifier.", "labels": [], "entities": []}, {"text": "Our negation detection approach yields a recall (sensitivity) value of 94.6% for the positive cases and an overall accuracy value of 91.9%.", "labels": [], "entities": [{"text": "negation detection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9831845462322235}, {"text": "recall (sensitivity)", "start_pos": 41, "end_pos": 61, "type": "METRIC", "confidence": 0.9493198692798615}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9993359446525574}]}, {"text": "We believe that our approach can be integrated with other clinical Natural Language Processing tools in order to further advance information extraction capabilities.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.8166840672492981}]}], "introductionContent": [{"text": "Suicide is a leading cause of death globally.", "labels": [], "entities": []}, {"text": "Approximately 10% of people report having suicidal thoughts at some point in their lives ( and each year 0.3% of the general population make a suicide attempt (.", "labels": [], "entities": []}, {"text": "Mental disorders (particularly depression, substance abuse, schizophrenia and other psychoses) are associated with approximately 90% of all suicides).", "labels": [], "entities": []}, {"text": "Assessment of suicide risk is therefore routine practice for clinicians in mental health services, but it is notoriously inaccurate as well as time-consuming (.", "labels": [], "entities": [{"text": "Assessment of suicide risk", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8149117976427078}]}, {"text": "Although individual risk factors associated with suicide have been reported in depth (e.g., integrating them into an algorithm to analyse signatures of suicidality has been beset with difficulties.", "labels": [], "entities": []}, {"text": "Clinicians document the progress of mental health patients in Mental Health Records (MHRs), predominantly using free text, with sparse structured information.", "labels": [], "entities": []}, {"text": "This poses new and interesting challenges for clinical Natural Language Processing (NLP) tool development that could assist in identifying which patients are most at risk of suicide, and when (.", "labels": [], "entities": [{"text": "clinical Natural Language Processing (NLP) tool development", "start_pos": 46, "end_pos": 105, "type": "TASK", "confidence": 0.6972572604815165}]}, {"text": "Developing a classifier to identify times of greatest risk for suicide at an individual patient level () would assist in targeting suicide prevention strategies to those patients who are most vulnerable ().", "labels": [], "entities": [{"text": "suicide prevention", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.6998946219682693}]}, {"text": "Negation can be used to denote absence or inversion of concepts.", "labels": [], "entities": []}, {"text": "As a linguistic feature it can play a prominent role in monitoring both symptom context and risk in psychological conditions ().", "labels": [], "entities": []}, {"text": "For instance, one study found that almost 50% of the clinical concepts in narrative reports were negated ().", "labels": [], "entities": []}, {"text": "In this paper, we address the long-term goal of de-veloping improved information retrieval systems for clinicians and researchers, with a specific focus on suicide risk assessment.", "labels": [], "entities": [{"text": "suicide risk assessment", "start_pos": 156, "end_pos": 179, "type": "TASK", "confidence": 0.7381608883539835}]}, {"text": "To achieve this, we focus on the problem of determining negation concerning mentions of suicide.", "labels": [], "entities": [{"text": "determining negation concerning mentions of suicide", "start_pos": 44, "end_pos": 95, "type": "TASK", "confidence": 0.6590992112954458}]}, {"text": "Clinical concepts are most often defined as nouns (\"suicide\") or noun phrases (\"suicide ideation\"), and a negation detection algorithm needs to model the surrounding context to correctly ascertain whether the concept is negated or not (\"patient has never expressed any suicidal ideation\" vs. \"patient expressing suicidal ideation\").", "labels": [], "entities": [{"text": "negation detection", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.9643604159355164}]}, {"text": "Modelling the surrounding context of words can be done in different ways.", "labels": [], "entities": []}, {"text": "Our work is motivated by the advances in Probabilistic Context Free Grammar Parsers (PCFGs), which allow us express widely generalisable negation patterns in terms of restrictions on constituents.", "labels": [], "entities": []}, {"text": "A solution to negation detection that uses different aspects of linguistic structure can provide richer and more informative features.", "labels": [], "entities": [{"text": "negation detection", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.978277862071991}]}, {"text": "As a next step we want to extend our work and extract other important features from MHRs, such as the statements in the form of subject-predicateobject, temporal characteristics or degree of suicidality.", "labels": [], "entities": []}, {"text": "We propose an automated method for determining negation in relation to documented suicidality in MHRs.", "labels": [], "entities": [{"text": "negation", "start_pos": 47, "end_pos": 55, "type": "TASK", "confidence": 0.6724697351455688}, {"text": "MHRs", "start_pos": 97, "end_pos": 101, "type": "TASK", "confidence": 0.7608053684234619}]}, {"text": "Our negation detection algorithm relies on syntactic information and is applied and evaluated on a manually annotated corpus of sentences containing mentions of suicide, or inflections thereof, from a repository of mental health notes.", "labels": [], "entities": [{"text": "negation detection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9874533414840698}]}, {"text": "Our paper makes the following contributions: \u2022 we create an annotated dataset containing over 6,000 sentences with mentions of suicide (affirmed or negated), \u2022 we propose anew method for incorporating syntactic information for automatically determining whether a mention of interest is affirmed or negated.", "labels": [], "entities": []}, {"text": "To our knowledge, no previous research has addressed the problem of negation detection in the domain of MHRs and suicidality.", "labels": [], "entities": [{"text": "negation detection", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.9925478994846344}, {"text": "MHRs", "start_pos": 104, "end_pos": 108, "type": "TASK", "confidence": 0.978323221206665}]}], "datasetContent": [{"text": "Pseudonymised and de-identified mental health records of all patients (both in and outpatients) from the Clinical Record Interactive Search (CRIS) database were used (.", "labels": [], "entities": [{"text": "Clinical Record Interactive Search (CRIS) database", "start_pos": 105, "end_pos": 155, "type": "DATASET", "confidence": 0.6261766627430916}]}, {"text": "CRIS has records from the South London and Maudsley NHS Foundation Trust (SLaM), one of the largest mental health providers in Europe.", "labels": [], "entities": [{"text": "CRIS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9229508638381958}, {"text": "South London and Maudsley NHS Foundation Trust (SLaM)", "start_pos": 26, "end_pos": 79, "type": "DATASET", "confidence": 0.8170128881931304}]}, {"text": "SLaM covers the Lambeth, Southwark, Lewisham and Croydon boroughs in South London.", "labels": [], "entities": [{"text": "SLaM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7864533066749573}, {"text": "Lambeth", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.9196352958679199}]}, {"text": "CRIS has full ethical approval as a database for secondary analysis (Oxford REC C, reference 08/H0606/71+5) under a robust, patientled and externally approved governance structure.", "labels": [], "entities": [{"text": "CRIS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9700294137001038}, {"text": "Oxford REC C, reference 08/H0606/71+5", "start_pos": 69, "end_pos": 106, "type": "DATASET", "confidence": 0.9438393910725912}]}, {"text": "Currently, CRIS contains mental health records for around 226K patients, and approximately 18.6 million documents with free text.", "labels": [], "entities": [{"text": "CRIS contains mental health records", "start_pos": 11, "end_pos": 46, "type": "DATASET", "confidence": 0.869796073436737}]}, {"text": "Out of these documents, 783K contain at least one mention of \"suicid*\" (111K patients).", "labels": [], "entities": []}, {"text": "Monitoring suicide risk is an important task for mental health teams, and therefore use of the term \"suicid*\" was expected to be common.", "labels": [], "entities": [{"text": "Monitoring suicide risk", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7779660026232401}]}, {"text": "The annotation task was defined on a conceptlevel: each target concept (\"suicid*\") in a sentence was to be marked as either negative (negated mention, e.g.\"denies suicidal thoughts\") or positive (affirmed mention, e.g.\"patient with suicidal thoughts\") . In clinical narratives, there are cases where this distinction is not necessarily straightforward.", "labels": [], "entities": []}, {"text": "For instance, in a sentence like \"low risk of suicide based on current mental state\", a clinician maybe inclined to interpret this as negated (this is not a patient at risk of suicide), while a linguistic interpretation would be that this is not negated (there is no linguistic negation marker in this example).", "labels": [], "entities": []}, {"text": "In this study, the annotators were asked to focus on linguistic negation markers, and disregard clinical interpretations, in order to create a well-defined and unambiguously annotated corpus.", "labels": [], "entities": []}, {"text": "They were also instructed to annotate mentions of suicide regardless of whether comments concerned the patient, their family member or a friend.", "labels": [], "entities": []}, {"text": "A collection of 5000 randomly selected MHRs was extracted, divided (segmented) into individual sentences, keeping only sentences containing the target concept.", "labels": [], "entities": []}, {"text": "This resulted in a corpus of 6066 sentence-instances.", "labels": [], "entities": []}, {"text": "One annotator (domain expert) annotated the entire corpus.", "labels": [], "entities": []}, {"text": "To assess the feasibility and estimate the upper performance levels that could be expected from an automated system, we employed a doubleannotation procedure on a portion of the corpus.", "labels": [], "entities": []}, {"text": "We calculated the Inter-Annotator Agreement (IAA) in order to examine if the task is well-defined.", "labels": [], "entities": [{"text": "Inter-Annotator Agreement (IAA)", "start_pos": 18, "end_pos": 49, "type": "METRIC", "confidence": 0.9425429344177246}]}, {"text": "A randomly selected subset (1244 sentences, >20% of the corpus) was given to a second annotator (NLP researcher) to calculate IAA.", "labels": [], "entities": [{"text": "IAA", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.5984570384025574}]}, {"text": "The IAA analysis showed that our annotators agreed on 97.9% of the instances (Cohen's \u03ba 0.93, agreement over 1218 sentences).", "labels": [], "entities": [{"text": "IAA analysis", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8411056697368622}]}, {"text": "From this result, we concluded that: 1) the annotation task was indeed defined in an unambiguous way and was wellunderstood by humans, and 2) there are still some cases that are inherently difficult to assess, due to a degree of ambiguity, which is to be expected in realworld settings.", "labels": [], "entities": []}, {"text": "The final corpus contains 2941 sentences annotated as positive (affirmation of suicide) and 3125 annotated as negative (i.e. suicide negated, 48.5% -51.5% positive to negative ratio).", "labels": [], "entities": []}, {"text": "We evaluate results with precision (positive predictive value), recall (sensitivity), F-measure (harmonic mean of precision and recall) and accuracy (correct classifications overall classifications).", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9993470311164856}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9994863271713257}, {"text": "F-measure", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9988271594047546}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.638344943523407}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9401648640632629}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9994437098503113}]}, {"text": "We also compare our algorithm against two other, openly available, lexical negation resolution approaches: pyConTextNLP (Chapman et al., 2011) 5  and the) 2009 python implementation . Since these approaches depend on lists of negation and termination cues, we compare results with three configurations: 1) NegEx as obtained from the online code repository, 2) pyConTextNLP with the negation and termination cues from configuration 1 (pyConTextNLP-N), and 3) pyContextNLP with the negation and termination cues created for our proposed approach (pyConTextNLP-O).", "labels": [], "entities": [{"text": "lexical negation resolution", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.7872117161750793}]}, {"text": "Furthermore, we provide a more detailed performance analysis with regards to the length (in words) of a sentence, since the syntactic parses are more error-prone for longer sentences (lower accuracy and time-out requests).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9971410036087036}]}], "tableCaptions": [{"text": " Table 1: Confusion matrix: Manually annotated (Class) vs.", "labels": [], "entities": []}]}