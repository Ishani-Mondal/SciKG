{"title": [{"text": "A Recurrent and Compositional Model for Personality Trait Recognition from Short Texts", "labels": [], "entities": [{"text": "Personality Trait Recognition from Short Texts", "start_pos": 40, "end_pos": 86, "type": "TASK", "confidence": 0.8767823179562887}]}], "abstractContent": [{"text": "Many methods have been used to recognise author personality traits from text, typically combining linguistic feature engineering with shallow learning models, e.g. linear regression or Support Vector Machines.", "labels": [], "entities": []}, {"text": "This work uses deep-learning-based models and atomic features of text, the characters, to build hierarchical, vectorial word and sentence representations for trait inference.", "labels": [], "entities": []}, {"text": "This method, applied to a corpus of tweets, shows state-of-the-art performance across five traits compared with prior work.", "labels": [], "entities": []}, {"text": "The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits.", "labels": [], "entities": []}], "introductionContent": [{"text": "Techniques falling under the umbrella of \"deep-learning\" are increasingly commonplace in the space of Natural Language Processing (NLP).", "labels": [], "entities": []}, {"text": "Such methods have been applied to a number of tasks from part-of-speech-tagging () to sentiment analysis (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.9702159762382507}]}, {"text": "Essentially, each of these tasks is concerned with learning representations of language at different levels.", "labels": [], "entities": []}, {"text": "The work we outline here is no different in essence, though we choose perhaps the highest level of representation -that of the author of a given text rather than the text itself.", "labels": [], "entities": []}, {"text": "This task, modelling people from their language, is one built on the long-standing foundation that language use is known to be influenced by sociodemographic characteristics such as gender and personality.", "labels": [], "entities": []}, {"text": "The study of personality traits in particular is supported by the notion that they are considered temporally stable (, and thus our modelling ability is enriched by the acquisition of more data overtime.", "labels": [], "entities": []}, {"text": "Computational personality recognition, and its broader applications, is becoming of increasing interest with workshops exploring the topic ().", "labels": [], "entities": [{"text": "Computational personality recognition", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8524963855743408}]}, {"text": "The addition of personality traits in the PAN Author Profiling challenge at CLEF in 2015 () is further evidence.", "labels": [], "entities": [{"text": "PAN Author Profiling challenge at CLEF in 2015", "start_pos": 42, "end_pos": 88, "type": "DATASET", "confidence": 0.6947685591876507}]}, {"text": "Much prior literature in this field has used some variation of enriched bag-of-words; e.g. the \"Open vocabulary\" approach (.", "labels": [], "entities": []}, {"text": "This is understandable as exploring the relationship between word use and traits has delivered significant insight into aspects of human behaviour.", "labels": [], "entities": []}, {"text": "Different levels of representation of language have been used such as syntactic, semantic, and higher-order such as the psychologically-derived lexica of the Linguistic Inquiry and Word Count (LIWC) tool.", "labels": [], "entities": [{"text": "Linguistic Inquiry and Word Count (LIWC)", "start_pos": 158, "end_pos": 198, "type": "TASK", "confidence": 0.7222925014793873}]}, {"text": "One drawback of this bag-of-linguistic-features approach is that considerable effort can be spent on feature engineering.", "labels": [], "entities": []}, {"text": "Another is an unspoken assumption that these features, like the traits to which they relate, are similarly stable: the same language features always indicate the same traits.", "labels": [], "entities": []}, {"text": "However, this is not the case.", "labels": [], "entities": []}, {"text": "As have shown, the relationship between language and personality is not consistent across all forms of communication and that it is more complex.", "labels": [], "entities": []}, {"text": "In order to better explore this complexity in this work we propose a novel deep-learning feature-engineering-free modelisation of the problem of personality trait recognition.", "labels": [], "entities": [{"text": "personality trait recognition", "start_pos": 145, "end_pos": 174, "type": "TASK", "confidence": 0.6854827404022217}]}, {"text": "The task is framed as one of supervised sequence regression based on a joint atomic representation of the text: specifically on the character and word level.", "labels": [], "entities": []}, {"text": "In this context, we are exploring short texts.", "labels": [], "entities": []}, {"text": "Typically, classification of such texts tends to be particularly challenging for state-of-the-art BoW based approaches due, in part, to the noisy nature of such data (.", "labels": [], "entities": []}, {"text": "To cope with this we propose a novel recurrent and compositional neural network architecture, capable of constructing representations at character, word and sentence level.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: after we consider previous approaches to the task of computational personality recognition, including those which have a deep-learning component, we describe our model.", "labels": [], "entities": [{"text": "computational personality recognition", "start_pos": 89, "end_pos": 126, "type": "TASK", "confidence": 0.6027362843354543}]}, {"text": "We report on two sets of experiments, the first of which demonstrates the effectiveness of the model in inferring personality for users, while the second reports on the short text level analysis.", "labels": [], "entities": []}, {"text": "In both settings, the proposed model achieves state-of-the-art performance across five personality traits.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report two sets of experiments: the first a comparison at the user level between our featureengineering-free approach and current state-of-the-art models which rely on linguistic features; the second designed to evaluate the performance of the proposed model against other feature-engineering-free approaches on individual short texts.", "labels": [], "entities": []}, {"text": "We show that in both settings, i.e., against models with or without feature engineering, our proposed model achieves better results across all personality traits.", "labels": [], "entities": []}, {"text": "We use the English data from the PAN 2015 Author Profiling task dataset (, collected from Twitter and consisting of 14, 166 tweets and 152 users.", "labels": [], "entities": [{"text": "PAN 2015 Author Profiling task dataset", "start_pos": 33, "end_pos": 71, "type": "DATASET", "confidence": 0.8475425442059835}]}, {"text": "For each user there is a set of tweets (average n = 100) and gold standard personality labels.", "labels": [], "entities": []}, {"text": "The five trait labels -scores between -0.5 and 0.5 -are calculated following the author's self-assessment responses to the short Big 5 test, BFI-10 ( which is the most widely accepted and exploited scheme for personality recognition and has the most solid grounding in language (.", "labels": [], "entities": [{"text": "BFI-10", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.8247323632240295}, {"text": "personality recognition", "start_pos": 209, "end_pos": 232, "type": "TASK", "confidence": 0.715113177895546}]}, {"text": "In our experiments, each tweet is tokenised using Twokenizer (, in order to preserve hashtag-preceded topics and user mentions.", "labels": [], "entities": [{"text": "Twokenizer", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.7999870181083679}]}, {"text": "Unlike the majority of the language used in a tweet, URLs and mentions are used for their targets, and not their surface forms.", "labels": [], "entities": []}, {"text": "Therefore each text is normalised by mapping these features to single characters (e.g., @username \u2192 @, http://t.co/ \u2192 \u02c6).", "labels": [], "entities": []}, {"text": "Thus we limit the risk of modelling, say, character usage which was not directly influenced by the personality of the author.", "labels": [], "entities": [{"text": "character usage", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7470940351486206}]}, {"text": "Due to the unavailability of the test corpus -withheld by the PAN 2015 organisers -we compare the kfold cross-validation performance (k = 5 or 10) on the available dataset.", "labels": [], "entities": [{"text": "PAN 2015 organisers", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.9425161083539327}]}, {"text": "Performance is measured using Root Mean Square Error (RMSE) on either the tweet level or user level depending on the granularity of the task: where T and U are the total numbers of tweets and users in the corpus, y s i and\u02c6yand\u02c6 and\u02c6y s i the true and estimated personality trait score of the i th tweet, similarly y user i and\u02c6yand\u02c6 and\u02c6y user i are their user-level counterparts.", "labels": [], "entities": [{"text": "Root Mean Square Error (RMSE)", "start_pos": 30, "end_pos": 59, "type": "METRIC", "confidence": 0.8249150429453168}]}, {"text": "Each tweet in the dataset inherits the same five trait scores as assigned to the author from whom they were drawn.", "labels": [], "entities": []}, {"text": "\u02c6 y user i = 1 Ti Ti j=1\u02c6yj=1\u02c6 j=1\u02c6y s j where Ti refers to the total number of tweets of user i . In Section 4.3 and 4.4, we present the results measured at the user and tweet level using RM SE user and RM SE tweet respectively.", "labels": [], "entities": []}, {"text": "It is important to note that, to enable direct comparison, we use exactly the same dataset and evaluation metric RM SE user as in the works of).", "labels": [], "entities": [{"text": "RM SE", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.4745045453310013}]}], "tableCaptions": [{"text": " Table 1: Pearson correlations for the five personality traits", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9626351892948151}]}, {"text": " Table 2: RM SE user across five traits. Bold highlights best performance. indicates N/A.", "labels": [], "entities": [{"text": "RM SE", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.7201786637306213}, {"text": "N/A", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.8572149078051249}]}, {"text": " Table 3: RM SE tweet across five traits level. Bold highlights best performance. indicates N/A.", "labels": [], "entities": [{"text": "RM", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.7984265089035034}, {"text": "SE", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.864628255367279}, {"text": "N/A", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9086584250132242}]}]}