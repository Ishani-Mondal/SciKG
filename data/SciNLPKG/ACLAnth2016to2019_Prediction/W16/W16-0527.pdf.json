{"title": [{"text": "Exploring the Intersection of Short Answer Assessment, Authorship Attribution, and Plagiarism Detection", "labels": [], "entities": [{"text": "Short Answer Assessment", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.6459148526191711}, {"text": "Authorship Attribution", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7271157801151276}]}], "abstractContent": [{"text": "In spite of methodological and conceptual parallels, the computational linguistic applications short answer scoring (Burrows et al., 2015), authorship attribution (Stamatatos, 2009), and plagiarism detection (Zesch and Gurevych, 2012) have not been linked in practice.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.5684727430343628}, {"text": "authorship attribution", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.7662883102893829}, {"text": "plagiarism detection", "start_pos": 187, "end_pos": 207, "type": "TASK", "confidence": 0.7722496092319489}]}, {"text": "This work explores the practical usefulness of the combination of features from each of these fields for two tasks: short answer assessment, and plagiarism detection.", "labels": [], "entities": [{"text": "short answer assessment", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.7444681525230408}, {"text": "plagiarism detection", "start_pos": 145, "end_pos": 165, "type": "TASK", "confidence": 0.7342205792665482}]}, {"text": "The experiments show that incorporating features from the other domain yields significant improvements.", "labels": [], "entities": []}, {"text": "A feature analysis reveals that robust lexical and semantic features are most informative for these tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite different ultimate goals, Short Answer Assessment, Plagiarism Detection, and Authorship Attribution are three domains of Computational Linguistics that share a range of methodology.", "labels": [], "entities": [{"text": "Short Answer Assessment", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7520274718602499}, {"text": "Plagiarism Detection", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.6704193353652954}, {"text": "Authorship Attribution", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.8015306293964386}]}, {"text": "However, these parallel have not been compared across domains.", "labels": [], "entities": []}, {"text": "This work explores the intersection of these areas in a practical context.", "labels": [], "entities": []}, {"text": "In the domain of authorship attribution, a set of texts and potential authors is given, and the goal is to \"distinguish between texts written by different authors\".", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7053439766168594}]}, {"text": "In the domain of short answer assessment, tools are designed to assess the meaning of a short answer by comparing it to a reference answer (, and thereby to its semantic appropriateness.", "labels": [], "entities": [{"text": "short answer assessment", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.779868503411611}]}, {"text": "In the domain of Plagiarism Detection, two main goals can be pursued (: in extrinsic plagiarism detection, a source and potentially plagiarized texts are compared as a whole unit with methods from the domain of authorship attribution.", "labels": [], "entities": [{"text": "Plagiarism Detection", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.88141268491745}, {"text": "extrinsic plagiarism detection", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.6890449921290079}]}, {"text": "The goal of intrinsic plagiarism detection is to detect stylistic changes within one document).", "labels": [], "entities": [{"text": "intrinsic plagiarism detection", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.7203826506932577}]}, {"text": "All three areas use textual similarity features on various levels of linguistic abstraction for nominal classifiers, but the distribution of features over three related dimensions differs: style, content, and structure.", "labels": [], "entities": []}, {"text": "While (learner language) short answer assessment systems put emphasis on content and ignore stylistic aspects, authorship attribution focuses on stylistic features.", "labels": [], "entities": []}, {"text": "Plagiarism detection systems use both content, structural, and stylistic similarity features to classify texts as plagiarizing other documents or not.", "labels": [], "entities": [{"text": "Plagiarism detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8484223186969757}]}, {"text": "The main task for short answer assessment and plagiarism detection is to evaluate the existence and quality of paraphrases of a source text.", "labels": [], "entities": [{"text": "short answer assessment", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7433042526245117}, {"text": "plagiarism detection", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7756848633289337}]}, {"text": "This work explores the effect of features used in the field of authorship attribution and plagiarism detection features for short answer assessment, as well as the effect of short answer assessment features for plagiarism detection.", "labels": [], "entities": [{"text": "authorship attribution and plagiarism detection", "start_pos": 63, "end_pos": 110, "type": "TASK", "confidence": 0.6900850713253022}, {"text": "short answer assessment", "start_pos": 124, "end_pos": 147, "type": "TASK", "confidence": 0.6064886550108591}, {"text": "plagiarism detection", "start_pos": 211, "end_pos": 231, "type": "TASK", "confidence": 0.7923308610916138}]}], "datasetContent": [{"text": "The following orthogonal hypotheses were tested: 1.", "labels": [], "entities": []}, {"text": "The accuracy for the learner language short answer assessment task increases when features from the domain of authorship attribution are added.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995582699775696}, {"text": "learner language short answer assessment task", "start_pos": 21, "end_pos": 66, "type": "TASK", "confidence": 0.6749528050422668}]}, {"text": "2. The accuracy for the plagiarism classification task increases when features from the short answer assessment system are added.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9996486902236938}, {"text": "plagiarism classification task", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.7863695323467255}]}], "tableCaptions": [{"text": " Table 1: Data distribution in the CREG-1032 and  Wikipedia Reuse Corpus data set.", "labels": [], "entities": [{"text": "CREG-1032", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.9551213383674622}, {"text": "Wikipedia Reuse Corpus data set", "start_pos": 50, "end_pos": 81, "type": "DATASET", "confidence": 0.9389711618423462}]}, {"text": " Table 3: CoMiC system features.", "labels": [], "entities": []}, {"text": " Table 5: Results for the binary classification tasks. *  denotes a significant improvement (\u03b1 = 0.1).", "labels": [], "entities": [{"text": "binary classification tasks", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.7313987612724304}]}, {"text": " Table 6: Ten most informative features for the  CREG-1032 and WRC data set.", "labels": [], "entities": [{"text": "CREG-1032", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.9579048156738281}, {"text": "WRC data set", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.8734060923258463}]}]}