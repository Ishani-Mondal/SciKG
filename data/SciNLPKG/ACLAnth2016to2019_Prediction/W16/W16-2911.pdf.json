{"title": [{"text": "Unsupervised Document Classification with Informed Topic Models", "labels": [], "entities": [{"text": "Unsupervised Document Classification", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.644586573044459}]}], "abstractContent": [{"text": "Document classification is an important and common application in natural language processing.", "labels": [], "entities": [{"text": "Document classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9283256232738495}, {"text": "natural language processing", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.6443482438723246}]}, {"text": "Scaling classification approaches to many targets faces a bottleneck in acquiring gold standard labels.", "labels": [], "entities": [{"text": "Scaling classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9658494591712952}]}, {"text": "In this work, we develop and evaluate a method for using informed topic models to noisily label documents, creating a noisy but usable set of labels for training discriminative classifiers.", "labels": [], "entities": []}, {"text": "We investigate multiple ways to train this noisy classifier, and the best performing method uses Wikipedia-seeded topic models to approximately label training instances without any supervision.", "labels": [], "entities": []}, {"text": "We evaluate these methods on the classification task as well as in an active learning setting, in which they are shown to improve learning rates over traditional active learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document classification is a standard task in machine learning and natural language processing which has been studied extensively).", "labels": [], "entities": [{"text": "Document classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9057277143001556}]}, {"text": "For many instances of this problem, standard supervised machine learning methods are now sufficient, so that any given document classification problem maybe considered an application or engineering task rather than an interesting research problem.", "labels": [], "entities": [{"text": "document classification problem", "start_pos": 119, "end_pos": 150, "type": "TASK", "confidence": 0.7840498487154642}]}, {"text": "Recent work related to this problem has come mainly from the machine learning community and has focused on a generalization of the task called multi-label classification, in which each instance has multiple categories that must be predicted).", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 143, "end_pos": 169, "type": "TASK", "confidence": 0.7406040132045746}]}, {"text": "That work has been concerned with the problem of how to best make use of correlations between the different labels, and using that information to perform the classifications non-independently.", "labels": [], "entities": []}, {"text": "In contrast, the work here is concerned with the more practical problem of obtaining these labels, and particularly the issue that ad hoc classification targets require obtaining supervised training data from scratch.", "labels": [], "entities": []}, {"text": "This problem may arise in any application area of natural language processing, but in the clinical domain this problem is potentially more pressing because expert annotators (physicians) are expensive and traditional cost-saving approaches such as crowdsourcing are not always viable due to privacy concerns.", "labels": [], "entities": []}, {"text": "A common use case for clinical document classification is physicians mining patient notes for diseases, then using genetic samples of that \"virtual cohort\" to do phenotype-genotype correlation studies.", "labels": [], "entities": [{"text": "clinical document classification", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.6405949493249258}]}, {"text": "Billing codes have high recall but varying precision depending on the disease.", "labels": [], "entities": [{"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9996253252029419}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9989222288131714}]}, {"text": "Thus, machine learning and NLP applied to the narrative text in the clinical record are now often used as a solution to this problem.", "labels": [], "entities": []}, {"text": "Our approach to this task is to use the unsupervised method of topic modeling, specifically Latent Dirichlet Allocation (LDA), which can learn word probabilities for semantically coherent topics, and by providing informed priors, we can steer topics to categories of interest and use these word lists like features in a classifier.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 92, "end_pos": 125, "type": "METRIC", "confidence": 0.8750937283039093}]}, {"text": "As a first step, we take advantage of the crowd-sourced knowledge contained in Wikipedia to build a representation of the category of interest.", "labels": [], "entities": []}, {"text": "We then use this category representation as an informed prior to LDA.", "labels": [], "entities": []}, {"text": "This informed LDA algorithm then finds the topics that best satisfy the data given the priors, including both informed topics and traditional uninformed topics.", "labels": [], "entities": []}, {"text": "In particular, we are able to guide the topic model to learn separate topics for similar categories if that is required by the categories we are interested in for classification.", "labels": [], "entities": []}, {"text": "The ability to extract pre-specified topics of varying granularity is interesting on its own, as it could be used for more guided data explorations of the kind that LDA is already in use for.", "labels": [], "entities": []}, {"text": "But we can also use the output of this process to generate classifiers, by treating the occurrence of these topics in a document as a noisy label for that document.", "labels": [], "entities": []}, {"text": "Given these noisy labels, we can immediately train a classifier, which performs much better than chance, without seeing a single gold standard training example.", "labels": [], "entities": []}, {"text": "Finally, we show that this has potential applications to active learning by using our noisy classifier's certainty estimates to select training examples, rather than first annotating a random seed set.", "labels": [], "entities": []}, {"text": "This method results in faster learning rates than passive learning, standard active learning, and a baseline method that uses the Wikipedia-trained priors directly.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the effectiveness of this method, we will start with one brief qualitative evaluation to inspect the topics found, and then proceed to two quantitative evaluations.", "labels": [], "entities": []}, {"text": "The first evaluation attempts to get a preliminary look at how the informed LDA method works on topics that are superficially similar, to gauge how adding information can guide the model to make difficult distinctions.", "labels": [], "entities": []}, {"text": "The first quantitative evaluation is a simple set of unsupervised classification experiments.", "labels": [], "entities": []}, {"text": "We build a bronze standard for the 20 Newsgroups and i2b2 data sets, then train classifiers for each category and evaluate the classifier.", "labels": [], "entities": [{"text": "Newsgroups and i2b2 data sets", "start_pos": 38, "end_pos": 67, "type": "DATASET", "confidence": 0.7910227537155151}]}, {"text": "The second quantitative evaluation examines the use case of active learning.", "labels": [], "entities": []}, {"text": "Our experiment uses the unsupervised classifiers from the previous experiment to evaluate whether active learning can be made even faster by using those classifiers to select examples at the start of the active learning procedure, when the gold standard training data is still quite small.", "labels": [], "entities": []}, {"text": "shows the results of inspecting a few sports-related topics from the 20 Newsgroups corpus.", "labels": [], "entities": [{"text": "20 Newsgroups corpus", "start_pos": 69, "end_pos": 89, "type": "DATASET", "confidence": 0.7880369325478872}]}, {"text": "This is to simply see if this method can address the issue discussed in Section 2, the conflation of similar topics.", "labels": [], "entities": []}, {"text": "The first column shows the words in a sports-related topic using standard LDA.", "labels": [], "entities": []}, {"text": "It clearly finds words related to both hockey and baseball, with no other topics containing any significant amount of hockey or baseball content.", "labels": [], "entities": []}, {"text": "In contrast, the last two columns show the informed topics for baseball and hockey using informed LDA.", "labels": [], "entities": []}, {"text": "In addition to the sport names there are additional terms that are discriminative, including hit, runs, and wins (a pitching statistic) for baseball, and canada, nhl, and cup for hockey.", "labels": [], "entities": []}, {"text": "Informed LDA also did not have any other topics containing significant amount of hockey or baseball content.", "labels": [], "entities": [{"text": "Informed LDA", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7347964346408844}]}, {"text": "This kind of evaluation is of limited use, but it does verify that the algorithm is able to find closely aligned topics.", "labels": [], "entities": []}, {"text": "The data sets used for evaluation are the 20 Newsgroups data set 3 and the 2008 i2b2 Challenge data set described above.", "labels": [], "entities": [{"text": "20 Newsgroups data set 3", "start_pos": 42, "end_pos": 66, "type": "DATASET", "confidence": 0.8188571393489837}, {"text": "2008 i2b2 Challenge data set", "start_pos": 75, "end_pos": 103, "type": "DATASET", "confidence": 0.9204598546028138}]}, {"text": "The 20 Newsgroups data set contains around 11,000 training documents, partitioned into 20 topics, which are used as labels for the documents.", "labels": [], "entities": [{"text": "20 Newsgroups data set", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.791979968547821}]}, {"text": "These include labels such as alt.atheism for atheism-related conversations, sci.crypt for cryptography-related discussions, and so forth.", "labels": [], "entities": []}, {"text": "While each document may have multiple \"topics\" in the strict semantic sense, it will have one topic label -in other words, a classifier must choose a single category from 20 possibilities.", "labels": [], "entities": []}, {"text": "The 2008 i2b2 Challenge data consists of clin-ical discharge summaries from patients at an obesity clinic.", "labels": [], "entities": [{"text": "2008 i2b2 Challenge data", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.5934282429516315}]}, {"text": "This data contains 730 notes in the training set, with each note being labeled for 16 disease categories, with both textual and intuitive labels.", "labels": [], "entities": []}, {"text": "We use the more challenging intuitive label set, which did not require explicit confirmation of a diagnosis in the text.", "labels": [], "entities": []}, {"text": "We discard two labels, hypertriglyceridemia and venous insufficiency, after preliminary work on the training set indicated that those two labels could not be learned satisfactorily even with fully supervised approach.", "labels": [], "entities": []}, {"text": "The likely cause of the difficulty is that these two categories contained the fewest number of positive examples, an important issue but one we will have to reserve for future work.", "labels": [], "entities": []}, {"text": "In contrast to the 20 Newsgroups data, in the i2b2 data the labels are not mutually exclusive, so we frame the task as 14 binary classification problems.", "labels": [], "entities": [{"text": "Newsgroups data", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.8878275156021118}]}, {"text": "We used the Weka machine learning toolkit ( ) during development, and evaluated many different classifiers on both datasets, including Adaboost, support vector machines, logistic regression, and naive bayes.", "labels": [], "entities": [{"text": "Weka machine learning toolkit", "start_pos": 12, "end_pos": 41, "type": "DATASET", "confidence": 0.8670344650745392}, {"text": "Adaboost", "start_pos": 135, "end_pos": 143, "type": "DATASET", "confidence": 0.8945642709732056}]}, {"text": "We use the Adaboost algorithm) with decision stumps as the weak learner for the i2b2 data.", "labels": [], "entities": []}, {"text": "For the 20 Newsgroups data we used a support vector machine with linear kernel for the classification experiment and switched to Naive Bayes for the active learning experiments for speed reasons.", "labels": [], "entities": [{"text": "20 Newsgroups data", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.6716632048288981}, {"text": "classification", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.9638469815254211}]}, {"text": "Besides being relatively accurate, using boosting with decision trees has the beneficial property that the models it builds have some degree of transparency, which clinical researchers appreciate.", "labels": [], "entities": []}, {"text": "For the first experiment we evaluate the effectiveness of informed LDA on generating labels that can train a classifier.", "labels": [], "entities": []}, {"text": "We compare first to a random labeling baseline (labeled RandL), that generates a random labeling, trains a classifier with those labels, and then uses it to classify the training set.", "labels": [], "entities": []}, {"text": "This is not intended to be a competitive baseline, as much as it is a check to set a lower bound on what kind of performance we would get if informed LDA labeling had no signal whatsoever.", "labels": [], "entities": []}, {"text": "We also compare to a standard random classifier (RandC) which is based on a recall of 0.5 and a precision of the category's prevalence.", "labels": [], "entities": [{"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9990754127502441}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9945001602172852}]}, {"text": "This baseline is important for the binary classifier to make sure our classifier is learning more than just how to do random guessing based on our evenly split labels.", "labels": [], "entities": []}, {"text": "In the main experimental condition (Bronze), we use informed LDA to generate a bronze standard label set for the training data as described in Section 3.3, train a classifier with those labels and evaluate it on those same examples from the training set.", "labels": [], "entities": [{"text": "Bronze", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9480739831924438}]}, {"text": "The upper bound we compare against is a 5-fold cross-validation of the training set using gold labels.", "labels": [], "entities": []}, {"text": "The next experiment examines the usefulness of these unsupervised classifiers in an augmented active learning scheme described in Section 3.5.", "labels": [], "entities": []}, {"text": "We use the two baselines of passive learning and standard active learning.", "labels": [], "entities": []}, {"text": "The passive learning baseline is equivalent to just plotting a learning curve fora machine learning problem with random ordering of the instances.", "labels": [], "entities": []}, {"text": "The active learning baseline uses an initial seed set of 25 examples from within the pool set.", "labels": [], "entities": []}, {"text": "We use uncertainty sampling to select the next example, which uses the example which has the smallest difference in probability estimates between the two most likely classes.", "labels": [], "entities": []}, {"text": "The condition we are testing is labeled Bronze.", "labels": [], "entities": [{"text": "Bronze", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.8741310238838196}]}, {"text": "This condition does not use a seed set, but starts with a classifier trained on the entire bronzelabeled pool set.", "labels": [], "entities": []}, {"text": "Learning proceeds by finding examples in the pool set that the current iteration of the classifier is uncertain about and uncovering the gold label (i.e. simulating annotation).", "labels": [], "entities": []}, {"text": "This means that, in the active learning curve, the x-axis, which traditionally indicates the size of the training data used to train the classifier, now indicates the number of gold instances in the training data (the remaining instances still have bronze labels).", "labels": [], "entities": []}, {"text": "We give gold and bronze instances different weights to reflect varying quality of the labels.", "labels": [], "entities": []}, {"text": "This weight is used in calculating the cost function during training -a higher weight on gold labels means the classifier will try harder to get goldlabeled instances correct.", "labels": [], "entities": []}, {"text": "Here we use a weight of 0.1 for bronze-labeled instances and a weight of 1.0 for gold-labeled instances.", "labels": [], "entities": []}, {"text": "shows the results of the 14 binary classifiers on the i2b2 data.", "labels": [], "entities": []}, {"text": "The random labeling gives rise to a classifier that never obtains an F1 score better than 0.11.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9880418479442596}]}, {"text": "The bronze labeling, performs much better than the RandL classifier, with a low performance of 0.26 (for depression) and a high performance of 0.83 (for diabetes).", "labels": [], "entities": [{"text": "bronze labeling", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.616236001253128}, {"text": "RandL classifier", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.8945034146308899}]}, {"text": "The bronze-: Multi-way classifier accuracy on the 20 Newsgroups dataset using random labels (RL), a random classifier (RC), bronze labels obtained from informed LDA (Bronze) and a supervised cross-validation (CV).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9855418801307678}, {"text": "20 Newsgroups dataset", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.8655952016512553}]}], "tableCaptions": [{"text": " Table 2: F1 scores for traditional supervised clas- sifier (CV) vs. unsupervised classifier trained us- ing informed LDA (Bronze), classifiers trained  with random labels (RandL), and a classifier that  makes random guesses (RandC). (CAD=Coronary  Artery Disease, CHF=Congestive Heart Fail- ure, GERD=Gastroesophageal Reflux Disease,  HC=Hypercholesterolemia, HTN=Hypertension,  OA=Osteoarthritis, OSA=Obstructive Sleep Ap- nea, PVD=Peripheral Vascular Disease)", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9995579123497009}, {"text": "OSA=Obstructive Sleep Ap- nea", "start_pos": 399, "end_pos": 428, "type": "METRIC", "confidence": 0.7697263700621468}]}, {"text": " Table 3: Multi-way classifier accuracy on the 20  Newsgroups dataset using random labels (RL),  a random classifier (RC), bronze labels obtained  from informed LDA (Bronze) and a supervised  cross-validation (CV).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.980994462966919}, {"text": "20  Newsgroups dataset", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.7710670828819275}]}, {"text": " Table 4: Performance of augmented active learn- ing on 14 categories from the 2008 i2b2 Challenge  data .", "labels": [], "entities": [{"text": "2008 i2b2 Challenge  data", "start_pos": 79, "end_pos": 104, "type": "DATASET", "confidence": 0.7435853779315948}]}, {"text": " Table 5: Performance of augmented active learn- ing on 20-way classifier for 20 Newsgroups data.  Unit is Area Under the Learning curve (ALC).", "labels": [], "entities": []}]}