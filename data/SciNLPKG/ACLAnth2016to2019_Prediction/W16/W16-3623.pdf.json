{"title": [{"text": "Identifying Teacher Questions Using Automatic Speech Recognition in Classrooms", "labels": [], "entities": [{"text": "Identifying Teacher Questions", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8940074443817139}, {"text": "Automatic Speech Recognition", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.655901312828064}]}], "abstractContent": [{"text": "We investigate automatic question detection from recordings of teacher speech collected in live classrooms.", "labels": [], "entities": [{"text": "question detection", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7362035065889359}]}, {"text": "Our corpus contains audio recordings of 37 class sessions taught by 11 teachers.", "labels": [], "entities": []}, {"text": "We automatically segment teacher speech into utterances using an amplitude envelope thresholding approach followed by filtering non-speech via automatic speech recognition (ASR).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 143, "end_pos": 177, "type": "TASK", "confidence": 0.7562239368756613}]}, {"text": "We manually code the segmented utterances as containing a teacher question or not based on an empirically-validated scheme for coding classroom discourse.", "labels": [], "entities": []}, {"text": "We compute domain-independent natural language processing (NLP) features from transcripts generated by three ASR engines (AT&T, Bing Speech, and Azure Speech).", "labels": [], "entities": []}, {"text": "Our teacher-independent supervised machine learning model detects questions with an overall weighted F 1 score of 0.59, a 51% improvement over chance.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9891215761502584}]}, {"text": "Furthermore, the proportion of automatically-detected questions per class session strongly correlates (Pearson's r = 0.85) with human-coded question rates.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.8964643279711405}]}, {"text": "We consider our results to reflect a substantial (37%) improvement over the state-of-the-art in automatic question detection from naturalistic audio.", "labels": [], "entities": [{"text": "question detection", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.6911397725343704}]}, {"text": "We conclude by discussing applications of our work for teachers , researchers, and other stakeholders.", "labels": [], "entities": []}], "introductionContent": [{"text": "Questions are powerful tools that can inspire thought and inquiry at deeper levels of comprehension (.", "labels": [], "entities": []}, {"text": "There is a large body of work supporting a positive relationship between the use of certain types of questions with increased student engagement and achievement.", "labels": [], "entities": []}, {"text": "But not all questions are the same.", "labels": [], "entities": []}, {"text": "Questions that solicit surface-level facts (called test questions) are far less predictive of achievement compared to more open-ended (or dialogic) questions).", "labels": [], "entities": []}, {"text": "Fortunately, providing teachers with training and feedback on their use of instructional practices (including question-asking) can help them adopt techniques known to be associated with student achievement.", "labels": [], "entities": []}, {"text": "However, automatic computational methods are required to analyze classroom instruction on a large scale.", "labels": [], "entities": []}, {"text": "Although there are well-known coding schemes for manual coding of questions in classroom environments () research on automatically identifying these questions in live classrooms is in its infancy and is the focus of this work.", "labels": [], "entities": [{"text": "manual coding of questions in classroom environments", "start_pos": 49, "end_pos": 101, "type": "TASK", "confidence": 0.7632476346833366}, {"text": "automatically identifying these questions in live classrooms", "start_pos": 117, "end_pos": 177, "type": "TASK", "confidence": 0.7302977400166648}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Results of best models for question detection.", "labels": [], "entities": [{"text": "question detection", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8658855855464935}]}, {"text": " Table 3. Confusion matrix of combined ASR model  for Question (Q) and Utterances (U).", "labels": [], "entities": [{"text": "ASR", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8502761721611023}, {"text": "Question (Q) and Utterances (U)", "start_pos": 54, "end_pos": 85, "type": "METRIC", "confidence": 0.7352864212459989}]}, {"text": " Table 4. Confusion matrix showing a comparison of the ASR and Human models.", "labels": [], "entities": [{"text": "ASR", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9092216491699219}]}]}