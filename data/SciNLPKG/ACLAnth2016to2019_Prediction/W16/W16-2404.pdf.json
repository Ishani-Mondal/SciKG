{"title": [{"text": "Distributed representation and estimation of WFST-based n-gram models", "labels": [], "entities": [{"text": "Distributed representation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8637745380401611}]}], "abstractContent": [{"text": "We present methods for partitioning a weighted finite-state transducer (WFST) representation of an n-gram language model into multiple blocks or shards, each of which is a stand-alone WFST n-gram model in its own right, allowing processing with existing algorithms.", "labels": [], "entities": []}, {"text": "After independent estimation, including normal-ization, smoothing and pruning on each shard, the shards can be reassembled into a single WFST that is identical to the model that would have resulted from estimation without sharding.", "labels": [], "entities": [{"text": "WFST", "start_pos": 137, "end_pos": 141, "type": "DATASET", "confidence": 0.8702050447463989}]}, {"text": "We then present an approach that uses data partitions in conjunction with WFST sharding to estimate models on orders-of-magnitude more data than would have otherwise been feasible with a single process.", "labels": [], "entities": []}, {"text": "We present some numbers on shard characteristics when large models are trained from a very large data set.", "labels": [], "entities": []}, {"text": "Functionality to support distributed n-gram modeling has been added to the open-source OpenGrm library.", "labels": [], "entities": [{"text": "distributed n-gram modeling", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.6611541410287222}, {"text": "OpenGrm library", "start_pos": 87, "end_pos": 102, "type": "DATASET", "confidence": 0.9384698271751404}]}], "introductionContent": [{"text": "Training n-gram language models on ever increasing amounts of text continues to yield large model improvements for tasks as diverse as machine translation (MT), automatic speech recognition (ASR) and mobile text entry.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.842996883392334}, {"text": "automatic speech recognition (ASR)", "start_pos": 161, "end_pos": 195, "type": "TASK", "confidence": 0.8033812244733175}, {"text": "mobile text entry", "start_pos": 200, "end_pos": 217, "type": "TASK", "confidence": 0.596297045548757}]}, {"text": "One approach to scaling n-gram model estimation to peta-byte scale data sources and beyond, is to distribute the storage, processing and serving of n-grams.", "labels": [], "entities": []}, {"text": "In some scenarios -most notably ASR -a very common approach is to heavily prune models trained on large resources, and then pre-compose the resulting model off-line with other models (e.g., a pronunciation lexicon) in order to optimize the model for use at time of firstpass decoding ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9874573349952698}]}, {"text": "Among other things, this approach can impact the choice of smoothing for the first-pass model (, and the resulting model is generally stored as a weighted finite-state transducer (WFST) in order to take advantage of known operations such as determinization, minimization and weight pushing.", "labels": [], "entities": []}, {"text": "Even though the resulting model in such scenarios is generally of modest size, there is a benefit to training on very large samples, since model pruning generally aims to minimize the KL divergence from the unpruned model.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 184, "end_pos": 197, "type": "TASK", "confidence": 0.6655371487140656}]}, {"text": "Storing such a large n-gram model in a single WFST prior to model pruning is not feasible in many situations.", "labels": [], "entities": []}, {"text": "For example, speech recognition first pass models maybe trained as a mixture of models from many domains, each of which are trained on billions or tens of billions of sentences (.", "labels": [], "entities": [{"text": "speech recognition first pass", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.8154320567846298}]}, {"text": "Even with modest count thresholding, the size of such models before entropybased pruning would be on the order of tens of billions of n-grams.", "labels": [], "entities": []}, {"text": "Storing this model in the WFST n-gram format of the OpenGrm library () allocates an arc for every n-gram (other than end-ofstring n-grams) and a state for every n-gram prefix.", "labels": [], "entities": [{"text": "WFST n-gram format", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.8436461488405863}, {"text": "OpenGrm library", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9185954630374908}]}, {"text": "Even using very efficient specialized n-gram representations (), a single FST representing this model would require on the order of 400GB of storage, making it difficult to access and process on a single processor.", "labels": [], "entities": []}, {"text": "In this paper, we present methods for the distributed representation and processing of large WFST-based n-gram language models by partitioning them into multiple blocks or shards.", "labels": [], "entities": []}, {"text": "Our sharding approach meets two key desiderata: 1) each sub-model shard is a stand-alone \"canonical format\" WFST-based model in its own right, providing correct probabilities fora particular subset of the n-grams from the full model; and 2) once ngram counts have been sharded, downstream pro-  cessing such as model normalization, smoothing and pruning, can occur on each shard independently.", "labels": [], "entities": []}, {"text": "Methods, utilities and convenience scripts have been added to the OpenGrm NGram library 1 to permit distributed processing.", "labels": [], "entities": [{"text": "OpenGrm NGram library 1", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.9483332335948944}]}, {"text": "In addition to presenting design principles and algorithms in this paper, we will also outline the relevant library functionality.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Sharding characteristics and time to estimate under different training scenarios. As noted in Section 6, times are not", "labels": [], "entities": [{"text": "Sharding", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9718821048736572}]}, {"text": " Table 2: Counting time broken down between stages occur-", "labels": [], "entities": [{"text": "Counting", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.8894991874694824}]}]}