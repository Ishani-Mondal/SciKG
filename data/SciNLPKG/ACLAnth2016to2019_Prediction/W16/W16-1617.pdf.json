{"title": [{"text": "Learning Text Similarity with Siamese Recurrent Networks", "labels": [], "entities": [{"text": "Learning Text Similarity", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.671909769376119}]}], "abstractContent": [{"text": "This paper presents a deep architecture for learning a similarity metric on variable-length character sequences.", "labels": [], "entities": []}, {"text": "The model combines a stack of character-level bidi-rectional LSTM's with a Siamese architecture.", "labels": [], "entities": []}, {"text": "It learns to project variable-length strings into a fixed-dimensional embedding space by using only information about the similarity between pairs of strings.", "labels": [], "entities": []}, {"text": "This model is applied to the task of job title normalization based on a manually annotated taxonomy.", "labels": [], "entities": [{"text": "job title normalization", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.6362239519755045}]}, {"text": "A small data set is incrementally expanded and augmented with new sources of variance.", "labels": [], "entities": []}, {"text": "The model learns a representation that is selective to differences in the input that reflect semantic differences (e.g., \"Java developer\" vs. \"HR manager\") but also invariant to non-semantic string differences (e.g., \"Java de-veloper\" vs. \"Java programmer\").", "labels": [], "entities": []}], "introductionContent": [{"text": "Text representation plays an important role in natural language processing (NLP).", "labels": [], "entities": [{"text": "Text representation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7650148272514343}, {"text": "natural language processing (NLP)", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.8094642062981924}]}, {"text": "Tasks in this field rely on representations that can express the semantic similarity and dissimilarity between textual elements, be they viewed as sequences of words or characters.", "labels": [], "entities": []}, {"text": "Such representations and their associated similarity metrics have many applications.", "labels": [], "entities": []}, {"text": "For example, word similarity models based on dense embeddings ( ) have recently been applied in diverse settings, such as sentiment analysis and recommender systems (.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.7503389418125153}, {"text": "sentiment analysis", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.9649668335914612}]}, {"text": "Semantic textual similarity measures have been applied to tasks such as automatic summarization (, debate analysis (Boltuzic and\u0160najderand\u02c7and\u0160najder, 2015) and paraphrase detection).", "labels": [], "entities": [{"text": "summarization", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.722981870174408}, {"text": "debate analysis (Boltuzic and\u0160najderand\u02c7and\u0160najder, 2015)", "start_pos": 99, "end_pos": 156, "type": "TASK", "confidence": 0.6018030904233456}, {"text": "paraphrase detection", "start_pos": 161, "end_pos": 181, "type": "TASK", "confidence": 0.9226049780845642}]}, {"text": "Measuring the semantic similarity between texts is also fundamental problem in Information Extraction (IE)).", "labels": [], "entities": [{"text": "Information Extraction (IE))", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.8358429551124573}]}, {"text": "An important step in many applications is normalization, which puts pieces of information in a standard format, so that they can be compared to other pieces of information.", "labels": [], "entities": [{"text": "normalization", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9799525141716003}]}, {"text": "Normalization relies crucially on semantic similarity.", "labels": [], "entities": []}, {"text": "An example of normalization is formatting dates and times in a standard way, so that \"12pm\", \"noon\" and \"12.00h\" all map to the same representation.", "labels": [], "entities": [{"text": "formatting dates and times", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.8360622078180313}]}, {"text": "Normalization is also important for string values.", "labels": [], "entities": []}, {"text": "Person names, for example, maybe written in different orderings or character encodings depending on their country of origin.", "labels": [], "entities": []}, {"text": "A sophisticated search system may need to understand that the strings \"\", \"Lee, Junfan\" and \"Bruce Lee\" all refer to the same person and so need to be represented in away that indicates their semantic similarity.", "labels": [], "entities": []}, {"text": "Normalization is essential for retrieving actionable information from free, unstructured text.", "labels": [], "entities": []}, {"text": "In this paper, we present a system for job title normalization, a common task in information extraction for recruitment and social network analysis ().", "labels": [], "entities": [{"text": "job title normalization", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.6044334669907888}, {"text": "information extraction", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7497522830963135}, {"text": "social network analysis", "start_pos": 124, "end_pos": 147, "type": "TASK", "confidence": 0.6544782519340515}]}, {"text": "The task is to receive an input string and map it to one of a finite set of job codes, which are predefined externally.", "labels": [], "entities": []}, {"text": "For example, the string \"software architectural technician Java/J2EE\" might need to be mapped to \"Java developer\".", "labels": [], "entities": []}, {"text": "This task can be approached as a highly multi-class classification problem, but in this study, the approach we take focuses on learning a representation of the strings such that synonymous job titles are close together.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.6814137101173401}]}, {"text": "This approach has the advantage that it is flexible, i.e., the representation can function as the input space to a subsequent classifier, but can also be used to find closely related job titles or explore job title clusters.", "labels": [], "entities": []}, {"text": "In addition, the architecture of the learning model allows us to learn useful representations with limited supervision.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct a set of experiments to test the model's capabilities.", "labels": [], "entities": []}, {"text": "We start from a small data set based on a handmade taxonomy of job titles.", "labels": [], "entities": []}, {"text": "In each subsequent experiment the data set is augmented by adding new sources of variance.", "labels": [], "entities": []}, {"text": "We test the model's behavior in a set of unit tests, reflecting desired capabilities of the model, taking our cue from (.", "labels": [], "entities": []}, {"text": "This section discusses the data augmentation strategies, the composition of the unit tests, and the results of the experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracy of the baseline and models on each of the four test cases. The best performing neural  network in each column is indicated in bold. Note that the performance of the n-gram match system (*)  on the Extra Words test is 1.00 by construction.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9899513125419617}]}]}