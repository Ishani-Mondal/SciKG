{"title": [{"text": "Evaluation Dataset (DT-Grade) and Word Weighting Approach towards Constructed Short Answers Assessment in Tutorial Dialogue Context", "labels": [], "entities": [{"text": "Word Weighting", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.6848413497209549}, {"text": "Approach", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.5149257183074951}, {"text": "Constructed Short Answers Assessment in Tutorial Dialogue Context", "start_pos": 66, "end_pos": 131, "type": "TASK", "confidence": 0.6984758451581001}]}], "abstractContent": [{"text": "Evaluating student answers often requires contextual information, such as previous utterances in conversational tutoring systems.", "labels": [], "entities": []}, {"text": "For example, students use coreferences and write elliptical responses, i.e. incomplete but can be interpreted in context.", "labels": [], "entities": []}, {"text": "The DT-Grade corpus which we present in this paper consists of short constructed answers extracted from tuto-rial dialogues between students and an Intelligent Tutoring System and annotated for their correctness in the given context and whether the contextual information was useful.", "labels": [], "entities": [{"text": "DT-Grade corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.872545063495636}]}, {"text": "The dataset contains 900 answers (of which about 25% required contextual information to properly interpret them).", "labels": [], "entities": []}, {"text": "We also present a base-line system developed to predict the correct-ness label (such as correct, correct but incomplete) in which weights for the words are assigned based on context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Constructed short answers are responses produced by students to questions, e.g. in a test or in the middle of a tutorial dialogue.", "labels": [], "entities": [{"text": "Constructed short answers are responses produced by students to questions, e.g. in a test or in the middle of a tutorial dialogue", "start_pos": 0, "end_pos": 129, "type": "Description", "confidence": 0.7863494727922522}]}, {"text": "Such constructed answers are very different form answers to multiple choice questions where students just choose an option from the given list of choices.", "labels": [], "entities": []}, {"text": "In this paper, we present a corpus called DT-Grade 1 which contains constructed short answers generated during interaction with a state-of-the-art conversational Intelligent Tutoring System (ITS) called DeepTutor ().", "labels": [], "entities": []}, {"text": "The main instructional task during tutoring was conceptual problem Available at http://language.memphis.edu/dt-grade solving in the area of Newtonian physics.", "labels": [], "entities": []}, {"text": "The answers in our data set are shorter than 100 words.", "labels": [], "entities": []}, {"text": "We annotated the instances, i.e. the student generated responses, for correctness using one of the following labels: correct, correct-but-incomplete, contradictory, or incorrect.", "labels": [], "entities": []}, {"text": "The student answers were evaluated with respect to target/ideal answers provided by Physics experts while also considering the context of the student-tutor interaction which consists of the Physics problem description and the dialogue history related to that problem.", "labels": [], "entities": []}, {"text": "In fact, during annotation we only limited our context to the immediately preceding tutor question and problem description.", "labels": [], "entities": []}, {"text": "This decision was based on previous work by Niraula and colleagues that showed that most of the referring expressions can be resolved by looking at the past utterance; that is, looking at just the previous utterance could be sufficient for our task as considering the full dialogue context would be computationally very expensive.", "labels": [], "entities": []}, {"text": "Automatic answer assessment systems typically assess student responses by measuring how much of the targeted concept is present in the student answer.", "labels": [], "entities": [{"text": "Automatic answer assessment", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7561886111895243}]}, {"text": "To this end, subject matter experts create target (or reference) answers to questions that students will be prompted to answer.", "labels": [], "entities": []}, {"text": "Almost always, the student responses depend on the context (at least broadly on the context of a particular domain) but it is more prominent in some situations.", "labels": [], "entities": []}, {"text": "Particularly in conversational tutoring systems, the meanings of students' responses often depend on the dialogue context and problem/task description.", "labels": [], "entities": []}, {"text": "For example, students frequently use pronouns, such as they, he, she, and it, in their response to tutors' questions or other prompts.", "labels": [], "entities": []}, {"text": "In an analysis of tutorial conversation logs, found that 68% of the pronouns used by students were referring to entities in the previous utterances or in the problem description.", "labels": [], "entities": []}, {"text": "In addition to anaphora, complex coreferences are also employed by students.", "labels": [], "entities": []}, {"text": "Also, in tutorial dialogues students react often with very short answers which are easily interpreted by human tutors as the dialogue context offers support to fill-in the blanks or untold parts.", "labels": [], "entities": []}, {"text": "Such elliptical utterances are common in conversations even when the speakers are instructed to produce more syntactically and semantically complete utterances.", "labels": [], "entities": []}, {"text": "By analyzing 900 student responses given to DeepTutor tutoring systems, we have found that about 25% of the answers require some contextual information to properly interpret them.", "labels": [], "entities": []}, {"text": "Problem description: A car windshield collides with a mosquito, squashing it.", "labels": [], "entities": []}, {"text": "Tutor question: How do the amounts of the force exerted on the windshield by the mosquito and the force exerted on the mosquito by the windshield compare?", "labels": [], "entities": []}, {"text": "Reference answer: The force exerted by the windshield on the mosquito and the force exerted by the mosquito on the windshield are an action-reaction pair.", "labels": [], "entities": []}, {"text": "The force of the bug hitting the window is much less than the force that the window exerts on the bug A3.", "labels": [], "entities": []}, {"text": "they are equal and opposite in direction A4.", "labels": [], "entities": []}, {"text": "equal and opposite: A problem and student answers to the given question.", "labels": [], "entities": []}, {"text": "As illustrated in the, the student answers may vary greatly.", "labels": [], "entities": []}, {"text": "For instance, answer A1 is elliptical.", "labels": [], "entities": [{"text": "A1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.7384846806526184}]}, {"text": "The \"bug\" in A2 is referring to the mosquito and \"they\" in A3 is referring to the amount of forces exerted to each other.", "labels": [], "entities": [{"text": "A2", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9528469443321228}]}, {"text": "In order to foster research in automatic answer assessment in context (also in general), we have annotated 900 student responses gathered from an experiment with the DeepTutor intelligent tutoring system (.", "labels": [], "entities": [{"text": "automatic answer assessment", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6407812138398489}]}, {"text": "Each response was annotated for: (a) their correctness, (b) whether the contextual information was helpful in understanding the student answer, and (c) whether the student answer contains important extra information.", "labels": [], "entities": []}, {"text": "The annotation labels, which are similar to the ones proposed by, were chosen such that there is a balance between the level of specificity and the amount of effort required for the annotation.", "labels": [], "entities": []}, {"text": "We also developed a baseline system using semantic similarity approach with word weighting scheme utilizing contextual information.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}