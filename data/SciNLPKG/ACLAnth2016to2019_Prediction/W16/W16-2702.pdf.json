{"title": [{"text": "Multi-source named entity typing for social media", "labels": [], "entities": []}], "abstractContent": [{"text": "Typed lexicons that encode knowledge about the semantic types of an entity name, e.g., that 'Paris' denotes a geoloca-tion, product, or person, have proven useful for many text processing tasks.", "labels": [], "entities": []}, {"text": "While lexicons maybe derived from large-scale knowledge bases (KBs), KBs are inherently imperfect, in particular they lack coverage with respect to long tail entity names.", "labels": [], "entities": []}, {"text": "We infer the types of a given entity name using multi-source learning , considering information obtained by alignment to the Freebase knowledge base, Web-scale distributional patterns, and global semi-structured contexts retrieved by means of Web search.", "labels": [], "entities": [{"text": "Freebase knowledge base", "start_pos": 125, "end_pos": 148, "type": "DATASET", "confidence": 0.9499412576357523}]}, {"text": "Evaluation in the challenging domain of social media shows that multi-source learning improves performance compared with rule-based KB lookups, boosting typing results for some semantic categories.", "labels": [], "entities": []}], "introductionContent": [{"text": "Typed lexicons associate lexical entity names with a set of semantic types, e.g., the name 'Paris' maybe mapped into the semantic categories of geolocation, product, or person.", "labels": [], "entities": []}, {"text": "Such world knowledge has proven valuable for various text processing tasks, including the classification of search queries), question answering (), named entity recognition (, entity linking, and relation extraction (.", "labels": [], "entities": [{"text": "classification of search queries", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.8448793292045593}, {"text": "question answering", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.8970558643341064}, {"text": "named entity recognition", "start_pos": 148, "end_pos": 172, "type": "TASK", "confidence": 0.6042012373606364}, {"text": "entity linking", "start_pos": 176, "end_pos": 190, "type": "TASK", "confidence": 0.7708296775817871}, {"text": "relation extraction", "start_pos": 196, "end_pos": 215, "type": "TASK", "confidence": 0.8154037594795227}]}, {"text": "Over the last years, large scale knowledge bases (KBs) have become available, like Freebase (), and, which organize entities and their lexical aliases into semantic categories.", "labels": [], "entities": []}, {"text": "KBs are inherently incomplete however in their coverage of world knowledge).", "labels": [], "entities": []}, {"text": "This is partly because updates to the database with emerging entities and facts exhibit some delay ().", "labels": [], "entities": []}, {"text": "In addition, the 'long tail' of unpopular, local or domain-specific concepts is under-represented in general knowledge bases (.", "labels": [], "entities": []}, {"text": "In this work, we seek to associate semantic types of interest with a given entity name.", "labels": [], "entities": []}, {"text": "We refer to this problem as named entity typing.", "labels": [], "entities": []}, {"text": "Our main focus is on names mentioned on social media.", "labels": [], "entities": []}, {"text": "In this domain, local contextual information is often very limited, so that background world knowledge is especially useful for semantic processing purposes.", "labels": [], "entities": [{"text": "semantic processing", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.8370214104652405}]}, {"text": "For example, the commercial system described by performs named entity recognition in tweets by matching the tweet text against known entity names in a reference knowledge base (KB).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.6792391836643219}]}, {"text": "But, emerging and long tail entity names, which are frequently mentioned on social media, e.g., the names of local businesses, as well as informal names, abbreviations, and acronyms, are often missing from.", "labels": [], "entities": []}, {"text": "Previously, researchers have proposed to infer the semantic types of entity names based on local distributional evidence harvested from Web documents (, however such distributional evidence is sparse and costly to obtain for long tail names.", "labels": [], "entities": []}, {"text": "We claim that in order to infer the semantic types of infrequent entity names, it is neces-sary to model global, semi-structured, contextual evidence of the respective name mentions.", "labels": [], "entities": []}, {"text": "Specifically, we consider in this work paragraph-level contexts, Webpage titles and URLs, obtained by means of Web search.", "labels": [], "entities": []}, {"text": "As suggested by, Web search has several advantages as a source of background knowledge in cross-domain settings.", "labels": [], "entities": []}, {"text": "We believe this work to be the first to examine Web search as an information source for the task of entity name typing, and the first to evaluate this task in the social media domain.", "labels": [], "entities": [{"text": "entity name typing", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.6183509727319082}]}, {"text": "We apply a discriminative learning framework using an ensemble of information sources, naming this approach as Multi-Source Named Entity Typing (MS-NET).", "labels": [], "entities": [{"text": "Multi-Source Named Entity Typing (MS-NET)", "start_pos": 111, "end_pos": 152, "type": "TASK", "confidence": 0.696753489119666}]}, {"text": "Relevant evidence about a given entity name is extracted from the various sources and represented as features in a joint feature space.", "labels": [], "entities": []}, {"text": "Specifically, we model as features in this work related Freebase category tags, distributional information in the form of ReVerb lexico-syntactic patterns, and semi-structured Web search results.", "labels": [], "entities": []}, {"text": "In our experiments, we consider a set of nine target semantic entity types found to be highly popular on social media ().", "labels": [], "entities": []}, {"text": "While training examples are obtained by means of distant supervision, our evaluation of MS-NET is conducted using a gold-labeled dataset of entity names extracted from tweets, in which each name string has been annotated with its full set of semantic types.", "labels": [], "entities": []}, {"text": "We make this evaluation dataset available to the research community.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work to combine diverse information sources, including Web search results, for the purpose of entity name typing.", "labels": [], "entities": [{"text": "entity name typing", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.7617143392562866}]}, {"text": "Our results clearly show that the information sources modeled are complimentarylearning using all sources yields the best overall typing performance with respect to both recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9989563226699829}, {"text": "precision", "start_pos": 181, "end_pos": 190, "type": "METRIC", "confidence": 0.9922845363616943}]}, {"text": "MS-NET improves overall performance compared with a plausible baseline of rule-based alignment to a KB.", "labels": [], "entities": [{"text": "MS-NET", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7285901308059692}]}, {"text": "In particular, a boost in performance is obtained for some of the semantic types evaluated, such as facility and product, which apply to along tail of infrequent entity names.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each target type \u2113 \u2208 L, we sampled 900 entity names from relevant Freebase categories.", "labels": [], "entities": []}, {"text": "In order to assess and correct possible representation bias, we further sampled 100 additional entity names per category from manually identified Web lists.", "labels": [], "entities": []}, {"text": "We found that 18% of the latter names were missing from Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.990025520324707}]}, {"text": "Overall, the constructed training dataset includes about 1,000 unique names per target category.", "labels": [], "entities": []}, {"text": "In learning a binary classifier c j , we uniformly sample an equal number of entity names (1,000) associated with the other classes as negative examples.", "labels": [], "entities": []}, {"text": "We used Google API to perform Web-scale search.", "labels": [], "entities": []}, {"text": "Following tuning experiments, in which we examined cross-validation results using the training data, we set the number of top search results modeled per entity name to k = 15.", "labels": [], "entities": []}, {"text": "We found performance to be relatively insensitive to value of k, as long as at least 5 search results were modeled.", "labels": [], "entities": []}, {"text": "Arguably, search engines inflect bias over the produced rankings.", "labels": [], "entities": []}, {"text": "We experimented with shuffling of the top 50 search results (prior to selecting the top 15 results) and found performance to be robust with respect to ranking variance.", "labels": [], "entities": []}, {"text": "Furthermore, in our experiments, we ignore exact ranking information, assigning equal importance to each of the selected search results.", "labels": [], "entities": []}, {"text": "We report the results of a strict yet realistic learning setting, in which the classifiers trained using the examples obtained from Freebase and specialized Web lists were applied across domains to a test dataset, which includes entity names found on twitter.", "labels": [], "entities": []}, {"text": "We experimented with several classification algorithms using Weka (, and report our results using SVM, which was found to perform best.", "labels": [], "entities": [{"text": "Weka", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9483179450035095}]}, {"text": "In the following section we first describe the test dataset, and then turn to discuss the experimental results.", "labels": [], "entities": []}, {"text": "For evaluation purposes, we manually constructed a gold-labeled dataset of entity names mentioned: Statistics of the gold-labeled twitter dataset detailed by semantic type: no. of labels, interannotator agreement, and ratio and examples of entity names for which no match in FB was found.", "labels": [], "entities": [{"text": "FB", "start_pos": 275, "end_pos": 277, "type": "METRIC", "confidence": 0.893566906452179}]}, {"text": "We considered a corpus of 2,400 tweets collected by.", "labels": [], "entities": []}, {"text": "They have identified the name mentions in this corpus, and annotated these mentions with their contextualized meaning with respect to the following set of types: music artist, company, facility, geolocation, movie, product, sportsteam, tv-show and person.", "labels": [], "entities": []}, {"text": "These types were found to be most popular in the given tweets.", "labels": [], "entities": []}, {"text": "A similar set of categories has been used in other works on social media (.", "labels": [], "entities": []}, {"text": "In order to use this resource for the evaluation of multi-label named entity typing, we have annotated the entity names in the corpus with their full set of types, using the same target category set.", "labels": [], "entities": [{"text": "multi-label named entity typing", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.5831820964813232}]}, {"text": "Labeling was performed by a graduate student, who was allowed to use any resource available, including Web searches, to determine whether an entity belonged to each of the target classes.", "labels": [], "entities": [{"text": "Labeling", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9472099542617798}]}, {"text": "A random subset of 100 entities has been coannotated by the first author in order to assess inter-agreement rates.", "labels": [], "entities": []}, {"text": "Cohen's kappa agreement scores with respect to each of the target classes are detailed in.", "labels": [], "entities": []}, {"text": "As shown, agreement ranges between 0.69-1, denoting substantial to perfect agreement (.", "labels": [], "entities": [{"text": "agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9969624876976013}]}, {"text": "Interestingly, the lowest agreement was observed for the music artist category.", "labels": [], "entities": [{"text": "agreement", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9982748031616211}]}, {"text": "We found that disagreements mainly occurred for short, and therefore highly ambiguous, entity names; e.g., 'Justin' mayor may not refer to the music artist \"justin bieber\", and 'MAC' is possibly an alias for \"The Mac Band\".", "labels": [], "entities": []}, {"text": "Somewhat more expectedly, moderate agreement was observed for the product and facility categories.", "labels": [], "entities": [{"text": "agreement", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9761407375335693}]}, {"text": "It is not clear, for example, if 'Twitter' and 'Facebook' are services or products.", "labels": [], "entities": []}, {"text": "The resulting dataset includes 965 distinct entity names and 1,442 label assignments overall.", "labels": [], "entities": []}, {"text": "About 27% of the entity names were assigned two classes, and 11% of the entity names were labeled  with three types or more.", "labels": [], "entities": []}, {"text": "The corpus includes misspelled, abbreviated, and 'long tail' entity names, such as local restaurants and hotels.", "labels": [], "entities": []}, {"text": "details the total number, as well as concrete examples, of names per category for which no matching entry was found in Freebase ('Out-of-FB'), despite using a proximate alignment procedure.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.9588369131088257}]}, {"text": "This dataset maybe first to include gold mappings of entity names into a set of types of interest in the social media domain.", "labels": [], "entities": []}, {"text": "We will make the dataset freely available to the research community.", "labels": [], "entities": []}, {"text": "details our test results on the gold-labeled twitter dataset.", "labels": [], "entities": [{"text": "gold-labeled twitter dataset", "start_pos": 32, "end_pos": 60, "type": "DATASET", "confidence": 0.6433330476284027}]}, {"text": "We report the following evaluation measures: (i) example-level macro average precision, recall, and F1: these measures are first computed with respect to the set of types assigned to each entity name, and are then averaged overall entity names in the dataset; (ii) coverage: the ratio of entity names for which type predictions were generated.", "labels": [], "entities": [{"text": "example-level macro average precision", "start_pos": 49, "end_pos": 86, "type": "METRIC", "confidence": 0.5182543322443962}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9986379742622375}, {"text": "F1", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.9998476505279541}, {"text": "coverage", "start_pos": 265, "end_pos": 273, "type": "METRIC", "confidence": 0.9983394145965576}]}, {"text": "(iii) accuracy: the ratio of names perfectly classified, i.e., having all their types correctly predicted, with no incorrect types assigned to them.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9991682767868042}]}, {"text": "As baseline, we consider the plausible strategy of rule-based mapping, aligning SF (e), the set of Freebase tags extracted for name e (Alg. 1), with the target labels L.", "labels": [], "entities": [{"text": "rule-based mapping", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.6622786074876785}]}, {"text": "We manually determined a set of alignment rules for the purposes of this study, utilizing and expanding rules made previously available by other researchers ().", "labels": [], "entities": []}, {"text": "As shown, manual mapping applies to 80% of the entity names in the twitter dataset, for which at least one possibly matching entry in Freebase was found.", "labels": [], "entities": [{"text": "twitter dataset", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.8824026584625244}, {"text": "Freebase", "start_pos": 134, "end_pos": 142, "type": "DATASET", "confidence": 0.9501404762268066}]}, {"text": "The resulting instance-level precision and recall are .54 and .60.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.99265456199646}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9994151592254639}]}, {"text": "While manual rules tend to be precise, a main source of noise lies in the proximate matching of string e against FB entities.", "labels": [], "entities": []}, {"text": "And, hand-picking FB categories for alignment hurts recall.", "labels": [], "entities": [{"text": "FB", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.6716850996017456}, {"text": "alignment", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.907589852809906}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9981556534767151}]}], "tableCaptions": [{"text": " Table 2: Statistics of the gold-labeled twitter dataset detailed by semantic type: no. of labels, inter- annotator agreement, and ratio and examples of entity names for which no match in FB was found.", "labels": [], "entities": [{"text": "twitter dataset", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.648930087685585}, {"text": "FB", "start_pos": 188, "end_pos": 190, "type": "METRIC", "confidence": 0.7486823797225952}]}, {"text": " Table 3: Instance-level performance using various  information sources and features. Results that im- prove over the baseline are bold faced.", "labels": [], "entities": []}, {"text": " Table 4: Classifier performance by type. Results that improve over the baseline are bold faced.", "labels": [], "entities": []}]}