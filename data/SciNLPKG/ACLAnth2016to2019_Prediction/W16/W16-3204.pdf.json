{"title": [{"text": "Leveraging Captions in the Wild to Improve Object Detection", "labels": [], "entities": [{"text": "Leveraging Captions in the Wild", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7852450251579285}, {"text": "Improve Object Detection", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.7770518859227499}]}], "abstractContent": [{"text": "In this study, we explore whether the captions in the wild can boost the performance of object detection in images.", "labels": [], "entities": [{"text": "object detection in images", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.8075705394148827}]}, {"text": "Captions that accompany images usually provide significant information about the visual content of the image, making them an important resource for image understanding.", "labels": [], "entities": [{"text": "image understanding", "start_pos": 148, "end_pos": 167, "type": "TASK", "confidence": 0.7849762141704559}]}, {"text": "However, captions in the wild are likely to include numerous types of noises which can hurt visual estimation.", "labels": [], "entities": []}, {"text": "In this paper, we propose data-driven methods to deal with the noisy captions and utilize them to improve object detection.", "labels": [], "entities": [{"text": "object detection", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.858809769153595}]}, {"text": "We show how a pre-trained state-of-the-art object detector can take advantage of noisy captions.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that captions provide promising cues about the visual content of the images and can aid in improving object detection.", "labels": [], "entities": [{"text": "object detection", "start_pos": 129, "end_pos": 145, "type": "TASK", "confidence": 0.8265030980110168}]}], "introductionContent": [{"text": "Visual data on the Internet is generally coupled with descriptive text such as tags, keywords or captions.", "labels": [], "entities": []}, {"text": "While tags and keywords are typically composed of single words, or phrases, and generally depict the main entities in an image (e.g. objects, places, etc.), a caption is a complete sentence which is intended to describe the image in a holistic manner.", "labels": [], "entities": []}, {"text": "It can reveal information about not just the existing objects or the corresponding event but also the relationships between the objects/scene elements, their attributes or the actions in a scene).", "labels": [], "entities": []}, {"text": "In this respect, captions provide a much richer source of information in order to understand the image content.", "labels": [], "entities": []}, {"text": "This has recently motivated researchers to automate the task of describing images in natural languages using captions (.", "labels": [], "entities": []}, {"text": "However, most of these studies employ carefully collected image descriptions which are obtained by services like Amazon's Mechanical Turk ().", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 113, "end_pos": 137, "type": "DATASET", "confidence": 0.9029183834791183}]}, {"text": "Little has been done on utilizing captions in the wild, i.e. the captions that accompany images readily available on the Web.", "labels": [], "entities": []}, {"text": "Although captions are rich, there are some challenges that limit their use in computer vision, and related language tasks.", "labels": [], "entities": [{"text": "computer vision", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.7243522107601166}]}, {"text": "First, a caption may not be a visual depiction of the scene, but rather a sort of comment not directly related to the visual content of the image).", "labels": [], "entities": []}, {"text": "The users might avoid explaining the obvious, but talk about more indirect aspects, abstract concepts and/or feelings.", "labels": [], "entities": []}, {"text": "Third, the caption maybe poorly written, which makes it difficult to understand the meaning of the text associated with the image.", "labels": [], "entities": []}, {"text": "On the other hand, there is also a major advantage in having image-caption pairs on the Web; billions of them are freely available online.", "labels": [], "entities": []}, {"text": "Collectively considering image-caption pairs associated with a certain query image may allow to eliminate noisy information.", "labels": [], "entities": []}, {"text": "Researchers have used this idea to collect a large scale images-captions dataset consisting of clean, descriptive texts paired with images.", "labels": [], "entities": []}, {"text": "When noisy captions are eliminated, the rest can serve as an excellent source of information for what is available in the visual world.", "labels": [], "entities": []}, {"text": "In this paper, we investigate whether we can leverage captions in the wild to improve object detection.", "labels": [], "entities": [{"text": "object detection", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.8327498733997345}]}, {"text": "Object detection has seen some significant advances in recent years thanks to convolutional neural networks (.", "labels": [], "entities": [{"text": "Object detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9726099967956543}]}, {"text": "But in some cases, even state-of-the-art object detectors may fail to accurately locate objects or may produce false positives (see.", "labels": [], "entities": []}, {"text": "For such situations, we propose to utilize captions as an alternative source of information to determine what the big yellow horse in Prague beautiful young woman sitting near her bicycle under the tree in forest with map in her hands above clouds airplane window Hey diddle diddle...", "labels": [], "entities": []}, {"text": "the cat and the fiddle...", "labels": [], "entities": []}, {"text": "the cow jumped over the M00N!!: Left: Examples of good captions, carrying rich information about the visual content of the image such as existence, sizes, attributes of objects, or their spatial organization.", "labels": [], "entities": [{"text": "M00N", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.8624587655067444}]}, {"text": "Right: Examples of noisy captions, where the mentioned objects may not exist visually (magenta for existing, red for nonexisting objects). is present in the image.", "labels": [], "entities": []}, {"text": "Due to the reasons stated above, however, leveraging captions directly may result in errors.", "labels": [], "entities": []}, {"text": "Therefore, we suggest to use datadriven methods which can eliminate the noise in the captions and inform about which objects are available in the image.", "labels": [], "entities": []}, {"text": "For our purpose, we first consider a constrained scenario where we assume access to test image captions and run detectors for objects mentioned in the caption, as previously motivated by.", "labels": [], "entities": []}, {"text": "Then, we proceed to explore a more general setting where we observe captions only at training stage and infer possible objects within the test image using similar training images and their captions.", "labels": [], "entities": []}, {"text": "In finding similar images/captions, we propose to use three different approaches, based on nearest neighbors, 2-view Canonical Correlation Analysis (CCA) and 3-view CCA.", "labels": [], "entities": []}, {"text": "When the visual input is combined with caption information, these approaches not only help us to eliminate the noise in the captions, but also to infer about possible objects not even mentioned in the caption of a test image (see).", "labels": [], "entities": []}, {"text": "Our experimental results show that utilizing noisy captions of visually similar images in the proposed ways can indeed help in improving the performance of the object detection.", "labels": [], "entities": [{"text": "object detection", "start_pos": 160, "end_pos": 176, "type": "TASK", "confidence": 0.8126741349697113}]}], "datasetContent": [{"text": "We restrict ourselves to the images containing captions where the object classes from the PAS-CAL challenge) are mentioned such as dog, aeroplane, car, etc.", "labels": [], "entities": []}, {"text": "To that end, we queried the dataset considering these PAS-CAL classes as well as their synonyms (e.g., motorbike, motorcycle).", "labels": [], "entities": []}, {"text": "We also favoured imagecaption pairs that include place prepositions such as in, on, under, front and behind coupled with the query noun (e.g., dog under the tree) if exist.", "labels": [], "entities": []}, {"text": "This ensures the image-caption pairs to be used for exploring the effect of spatial information in captions and images as well.", "labels": [], "entities": []}, {"text": "We observed that captions that are short (e.g., max 4 words) or in the form of phrases tend to be cleaner than longer captions.", "labels": [], "entities": []}, {"text": "However, as our main aim is to leverage captions in the wild for object detection, we uniformly sampled captions that have different lengths between tokens, preventing the bias against caption lengths.", "labels": [], "entities": [{"text": "object detection", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.8347403705120087}]}, {"text": "We sampled 3.2k of such images for annotation and collected object-level bounding boxes for each and every PASCAL object available in the image.", "labels": [], "entities": []}, {"text": "shows the distribution of the number of object instances along with their visibility rates which is measured as the conditional probability given that a class name is mentioned in a caption, how frequent it actually exists in the image.", "labels": [], "entities": []}, {"text": "As can be seen, animate objects like dogs, horses and cats appear frequently when mentioned while vehicles like aeroplane, bus and train have low visibilities.", "labels": [], "entities": []}, {"text": "For experiments, we split our dataset as 50%-50% as training and test.", "labels": [], "entities": []}, {"text": "We use Faster R-CNN ( as our base object detector.", "labels": [], "entities": []}, {"text": "The detector itself is trained on the PASCAL VOC 2012 data ().", "labels": [], "entities": [{"text": "PASCAL VOC 2012 data", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.866919532418251}]}, {"text": "We emply PAS-CAL () conventions while evaluating the methods and also set the set of possible object classes C to Pascal classes (excluding the person class, due to the high level of ambiguity of the captions of this class), so we have 19 classes in total.", "labels": [], "entities": []}, {"text": "Following the regular detection experimental settings, we measure intersectionover-union (IoU) between detection and annotation windows and count the detections as positive detections if their IoU exceeds the threshold 0.50%.", "labels": [], "entities": [{"text": "intersectionover-union (IoU)", "start_pos": 66, "end_pos": 94, "type": "METRIC", "confidence": 0.9342297166585922}]}, {"text": "We evaluate the performance using average precision (AP).", "labels": [], "entities": [{"text": "average precision (AP)", "start_pos": 34, "end_pos": 56, "type": "METRIC", "confidence": 0.8318309307098388}]}, {"text": "While selecting the similar images, the number of nearest images N is assigned to different values of.", "labels": [], "entities": []}, {"text": "The first experiments evaluate the performance of Faster R-CNN by running the detector for every object class without considering any textual information, referred as All classes.", "labels": [], "entities": []}, {"text": "In the second experiment, we assume that we have access to the captions of the test images and run the detector only for the objects mentioned in these captions.", "labels": [], "entities": []}, {"text": "This experiment can be interpreted as using an unreliable oracle, since the objects mentioned All classes", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics. For each object class, we provide the number of instances in the dataset and  their visibility rates p(visible|mentioned).", "labels": [], "entities": []}, {"text": " Table 4: Mean Average precision (mAP) values of the Faster R-CNN run with all classes, classes men- tioned in the captions and the predicted candidate object classes.  Predicted classes  Method All classes Mentioned classes Single view 2-view CCA 3-view CCA  AP  0.304  0.508  0.504  0.512  0.518", "labels": [], "entities": [{"text": "Mean Average precision (mAP)", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9569443166255951}]}]}