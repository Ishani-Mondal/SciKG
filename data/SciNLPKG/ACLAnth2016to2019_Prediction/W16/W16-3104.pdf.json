{"title": [{"text": "Learning to Answer Biomedical Questions: OAQA at BioASQ 4B", "labels": [], "entities": [{"text": "Learning to Answer Biomedical Questions", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6883008003234863}, {"text": "OAQA", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.6606664061546326}, {"text": "BioASQ 4B", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.6419434249401093}]}], "abstractContent": [{"text": "This paper describes the OAQA system evaluated in the BioASQ 4B Question Answering track.", "labels": [], "entities": [{"text": "OAQA", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.47951632738113403}, {"text": "BioASQ 4B Question Answering", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.7519200295209885}]}, {"text": "The system extends the Yang et al.", "labels": [], "entities": []}, {"text": "(2015) system and integrates additional biomedical and general-purpose NLP annotators, machine learning modules for search result scoring, collective answer reranking, and yes/no answer prediction.", "labels": [], "entities": [{"text": "search result scoring", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.6335824032624563}, {"text": "collective answer reranking", "start_pos": 139, "end_pos": 166, "type": "TASK", "confidence": 0.567005048195521}, {"text": "yes/no answer prediction", "start_pos": 172, "end_pos": 196, "type": "TASK", "confidence": 0.6423793435096741}]}, {"text": "We first present the overall architecture of the system, and then focus on describing the main extensions to the Yang et al.", "labels": [], "entities": []}, {"text": "Before the official evaluation, we used the development dataset (excluding the 3B Batch 5 subset) for training.", "labels": [], "entities": [{"text": "3B Batch 5 subset", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.7643840909004211}]}, {"text": "We present initial evaluation results on a subset of the development data set to demonstrate the effectiveness of the proposed new methods, and focus on performance analysis of yes/no question answering.", "labels": [], "entities": [{"text": "yes/no question answering", "start_pos": 177, "end_pos": 202, "type": "TASK", "confidence": 0.6177588284015656}]}], "introductionContent": [{"text": "The BioASQ QA challenge () evaluates automatic question answering technologies and systems in the biomedical domain.", "labels": [], "entities": [{"text": "BioASQ QA challenge", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.550872415304184}, {"text": "question answering", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.6958446055650711}]}, {"text": "It consists of two phases: in Phase A, the task requires to retrieve relevant document, snippets, concepts, and triples given a natural language question, and evaluates the retrieval results in terms of mean average precision (MAP); in Phase B, the task requires to generate ideal answers for the questions, which are evaluated using accuracy and mean reciprocal rank (MRR), as well as exact answers, which are evaluated based on manual judgment.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 203, "end_pos": 231, "type": "METRIC", "confidence": 0.8730209767818451}, {"text": "accuracy", "start_pos": 334, "end_pos": 342, "type": "METRIC", "confidence": 0.9986819624900818}, {"text": "mean reciprocal rank (MRR)", "start_pos": 347, "end_pos": 373, "type": "METRIC", "confidence": 0.8597697218259176}]}, {"text": "The OAQA team participated in, and 5 of BioASQ 4B, in the categories of document, snippet, and concept retrieval, factoid, list and yes/no question answering (exact answer generation).", "labels": [], "entities": [{"text": "document, snippet, and concept retrieval", "start_pos": 72, "end_pos": 112, "type": "TASK", "confidence": 0.5806463829108647}, {"text": "yes/no question answering", "start_pos": 132, "end_pos": 157, "type": "TASK", "confidence": 0.5908805966377259}, {"text": "exact answer generation", "start_pos": 159, "end_pos": 182, "type": "TASK", "confidence": 0.56268443663915}]}, {"text": "The source code of the participating system can be downloaded from our GitHub repository . We follow the same general hypothesis expressed in and, specifically that informatics challenges like BioASQ are best met through careful design of a flexible and extensible architecture, coupled with continuous, incremental experimentation and optimization over various combinations of existing state-of-the-art components, rather than relying on a single \"magic\" component or single component combination.", "labels": [], "entities": []}, {"text": "This year, the number of labeled questions in the development set has grown to 1,307 (up from 810 in last year's dataset), which allows further exploration of a) the potential of supervised learning methods, and b) the effectiveness of various biomedical NLP tools in various phases of the system, from relevant concept and document retrieval to snippet extraction, and from answer text identification to answer prediction.", "labels": [], "entities": [{"text": "snippet extraction", "start_pos": 346, "end_pos": 364, "type": "TASK", "confidence": 0.7143528610467911}, {"text": "answer text identification", "start_pos": 375, "end_pos": 401, "type": "TASK", "confidence": 0.7371920545895895}, {"text": "answer prediction", "start_pos": 405, "end_pos": 422, "type": "TASK", "confidence": 0.8733120262622833}]}, {"text": "First, we use TmTool 2 (, in addition to MetaMap 3 , to identify possible biomedical named entities, especially out-of-vocabulary concepts.", "labels": [], "entities": []}, {"text": "We also extract frequent multi-word terms from relevant snippets () to further improve the recall of concept and candidate answer text extraction.", "labels": [], "entities": [{"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9965381622314453}, {"text": "candidate answer text extraction", "start_pos": 113, "end_pos": 145, "type": "TASK", "confidence": 0.4939503148198128}]}, {"text": "Second, we propose a supervised learning method to rerank the answer candidates for factoid and list questions based on the relation between each candidate answer and other candidate answers, which we refer to as collective reranking in this paper.", "labels": [], "entities": []}, {"text": "Third, we implement a yes/no question answering pipeline combining various heuristics, e.g. negation words, sentiment of the statements, the biomedical con-", "labels": [], "entities": [{"text": "yes/no question answering pipeline", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.7042636424303055}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Collective Answer Reranking Features", "labels": [], "entities": [{"text": "Collective Answer Reranking", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6593003968397776}]}, {"text": " Table 2: Yes/No Question Answering Features", "labels": [], "entities": [{"text": "Yes/No Question Answering", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.669494116306305}]}, {"text": " Table 3: Retrieval Result Reranking via Relevant Classification Features", "labels": [], "entities": [{"text": "Retrieval Result Reranking", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.9503545761108398}, {"text": "Relevant Classification", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.6165656447410583}]}, {"text": " Table 4: Evaluation results on BioASQ 3B Batch  5 Phase A subset. LR represents a Logistic Re- gression based reranking method is used, and NO  means no operation is performed, i.e. original re- trieval scores are used.", "labels": [], "entities": [{"text": "NO", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9978925585746765}]}, {"text": " Table 5: Evaluation results on BioASQ 3B Batch  5 Phase B subset. OF and AF represent Orig- inal or Additional features are used in training  and predicting answer scorers for factoid and list  questions. CR represents the Logistic Regression  based Collective Reranking is used. TP means  a hard Threshold is used", "labels": [], "entities": [{"text": "OF", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9886067509651184}, {"text": "AF", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.7737990021705627}, {"text": "TP", "start_pos": 281, "end_pos": 283, "type": "METRIC", "confidence": 0.9873048663139343}, {"text": "Threshold", "start_pos": 298, "end_pos": 307, "type": "METRIC", "confidence": 0.9562227129936218}]}]}