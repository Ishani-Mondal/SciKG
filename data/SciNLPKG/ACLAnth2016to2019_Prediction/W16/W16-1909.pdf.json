{"title": [{"text": "Generalization in Artificial Language Learning: Modelling the Propensity to Generalize", "labels": [], "entities": [{"text": "Generalization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9638810753822327}, {"text": "Generalize", "start_pos": 76, "end_pos": 86, "type": "TASK", "confidence": 0.7265576720237732}]}], "abstractContent": [{"text": "Experiments in Artificial Language Learning have revealed much about the cogni-tive mechanisms underlying sequence and language learning inhuman adults, in infants and in non-human animals.", "labels": [], "entities": []}, {"text": "This paper focuses on their ability to generalize to novel grammatical instances (i.e., instances consistent with a familiarization pattern).", "labels": [], "entities": []}, {"text": "Notably, the propensity to generalize appears to be negatively correlated with the amount of exposure to the artificial language, a fact that has been claimed to be contrary to the predictions of statistical models (Pe\u00f1a et al. (2002); Endress and Bonatti (2007)).", "labels": [], "entities": []}, {"text": "In this paper, we propose to model generalization as a three-step process, and we demonstrate that the use of statistical models for the first two steps, contrary to widespread intuitions in the ALL-field, can explain the observed decrease of the propensity to generalize with exposure time.", "labels": [], "entities": [{"text": "generalization", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.9754953980445862}]}], "introductionContent": [{"text": "In the last twenty years, experiments in Artificial Language Learning (ALL) have become increasingly popular for the study of the basic mechanisms that operate when subjects are exposed to language-like stimuli.", "labels": [], "entities": [{"text": "Artificial Language Learning (ALL)", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.6744678368171056}]}, {"text": "Thanks to these experiments, we know that 8 month old infants can segment a speech stream by extracting statistical information of the input, such as the transitional probabilities between adjacent syllables (;).", "labels": [], "entities": []}, {"text": "This ability also seems to be present inhuman adults (, and to some extent in nonhuman animals like cotton-top tamarins) and rats).", "labels": [], "entities": []}, {"text": "Even though this statistical mechanism is well attested for segmentation, it has been claimed that it does not suffice for generalization to novel stimuli or rule learning . Ignited by a study by, which postulated the existence of an additional rule-based mechanism for generalization, a vigorous debate emerged around the question of whether the evidence from ALL-experiments supports the existence of a specialized mechanism for generalization (;;;;), echoing earlier debates about the supposed dichotomy between rules and statistics.", "labels": [], "entities": [{"text": "generalization", "start_pos": 270, "end_pos": 284, "type": "TASK", "confidence": 0.9626537561416626}]}, {"text": "From a Natural Language Processing perspective, the dichotomy between rules and statistics is unhelpful.", "labels": [], "entities": []}, {"text": "In this paper, we therefore propose a different conceptualization of the steps involved in generalization in ALL.", "labels": [], "entities": []}, {"text": "In the following sections, we will first review some of the experimental data that has been interpreted as evidence for an additional generalization mechanism (;;).", "labels": [], "entities": []}, {"text": "We then reframe the interpretation of those results with our 3-step approach, a proposal of the main steps that are required for generalization, involving: (i) memorization of segments of the input, (ii) computation of the probability for unseen sequences, and (iii) distribution of this probability among particular unseen sequences.", "labels": [], "entities": []}, {"text": "We model the first step with the Retention&Recognition model (.", "labels": [], "entities": [{"text": "Retention&Recognition", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8873509168624878}]}, {"text": "We propose that a rational charac-terization of the second step can be accomplished with the use of smoothing techniques (which we further demonstrate with the use of the Simple Good-Turing method, (;).", "labels": [], "entities": []}, {"text": "We then argue that the modelling results shown in these two steps already account for the key aspects of the experimental data; and importantly, it removes the need to postulate an additional, separate generalization mechanism.", "labels": [], "entities": []}, {"text": "conduct a series of Artificial Language Learning experiments in which Frenchspeaking adults are familiarized to a synthesized speech stream consisting of a sequence of artificial words.", "labels": [], "entities": []}, {"text": "Each of these words contains three syllables A i XC i such that the A i syllable always cooccurs with the Ci syllable (as indicated by the subindex i).", "labels": [], "entities": []}, {"text": "This forms a consistent pattern (a \"rule\") consisting in a non-adjacent dependency between A i and Ci , with a middle syllable X that varies.", "labels": [], "entities": []}, {"text": "The order of the words in the stream is randomized, with the constraint that words do not appear consecutively if they either: (i) belong to the same \"family\" (i.e., they have the same A i and Ci syllables), or (ii) they have the same middle syllable X.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}