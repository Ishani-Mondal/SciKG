{"title": [{"text": "Improved Semantic Representation for Domain-Specific Entities", "labels": [], "entities": [{"text": "Improved Semantic Representation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8995537956555685}]}], "abstractContent": [{"text": "Most existing corpus-based approaches to semantic representation suffer from inaccurate modeling of domain-specific lexical items which either have low frequencies or are non-existent in open-domain corpora.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.877935141324997}]}, {"text": "We put forward a technique that improves word embeddings in specific domains by first transforming a given lexical item to a sorted list of representative words and then modeling the item by combining the em-beddings of these words.", "labels": [], "entities": []}, {"text": "Our experiments show that the proposed technique can significantly improve some of the recent word embedding techniques while modeling a set of lexical items in the biomedical domain , i.e., phenotypes.", "labels": [], "entities": [{"text": "word embedding", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.7015990167856216}]}], "introductionContent": [{"text": "Semantic representation is one of the oldest, yet most active, research areas in Natural Language Processing (NLP) owing to the central role it plays in many applications ( . The field has experienced a resurgence of interest in recent years with the introduction of lowdimensional continuous space models that leverage neural networks for learning semantic representations.) is a good example which despite its recent invention has found its way prominently into literature, mainly thanks to its ability to be quickly and effectively trained on large amounts of text.", "labels": [], "entities": [{"text": "Semantic representation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8653091192245483}, {"text": "Natural Language Processing (NLP)", "start_pos": 81, "end_pos": 114, "type": "TASK", "confidence": 0.7487237354119619}]}, {"text": "However, since most of these corpus-based techniques base their representation only on the co-occurrence statistics derived from text corpora, they fall short of effectively modeling lexical items for which not many statistical clues can be obtained from the underlying corpus.", "labels": [], "entities": []}, {"text": "Several attempts have been made to improve word embeddings with the help of knowledge derived from other resources ( or by including arbitrary contexts in the training process ().", "labels": [], "entities": []}, {"text": "However, most of these techniques still suffer from another deficiency of word embeddings that they inherit from their countbased ancestors: they conflate the different meanings of a word into a single vector representation.", "labels": [], "entities": []}, {"text": "Attempts have been made to tackle the meaning conflation issue of word-level representations.", "labels": [], "entities": [{"text": "meaning conflation", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7004774659872055}]}, {"text": "A series of approaches cluster the context of a word prior to representation) whereas others exploit lexical knowledge bases for sense-specific information.", "labels": [], "entities": []}, {"text": "We propose a model that addresses both these issues through a mapping of a lexical item to a sorted list of representative words that brings about two advantages.", "labels": [], "entities": []}, {"text": "Firstly, it pinpoints with an inherent disambiguation the meaning of the given lexical item at a deeper semantic level.", "labels": [], "entities": []}, {"text": "Secondly, by casting the representation of the item as that of a set of potentially more frequent words, our approach can provide a more reliable representation of domain-specific items based on significantly more statistical knowledge.", "labels": [], "entities": []}, {"text": "Our experiments show that the proposed model can provide a considerable improvement over some of the stateof-the-art word embedding approaches in a semantic similarity-based task.", "labels": [], "entities": []}, {"text": "The final goal of this paper is to improve the semantic representation of domain-specific terms and phrases which usually have low frequencies (or are non-existent) in open-domain corpora and hence have a lower chance of being effectively modeled by existing word representation techniques.", "labels": [], "entities": []}, {"text": "Therefore, for our experiments we retrieved terms and phrases from a domain-specific ontology in the biomedical domain.", "labels": [], "entities": []}, {"text": "Specifically, as our dataset in the experiments we opted for Human Phenotype Ontology which is a standardized vocabulary of phenotypic abnormalities encountered inhuman disease.", "labels": [], "entities": [{"text": "Human Phenotype Ontology", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6130115687847137}]}, {"text": "Semantic modeling of phenotypes has several applications in the biomedical domain such as profiling heritable diseases or understanding the genetic origins of diseases ().", "labels": [], "entities": [{"text": "Semantic modeling of phenotypes", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8435524553060532}, {"text": "profiling heritable diseases", "start_pos": 90, "end_pos": 118, "type": "TASK", "confidence": 0.8940117557843527}]}], "datasetContent": [{"text": "We evaluate our model in the semantic representation of phenotypes in the HPO ontology.", "labels": [], "entities": [{"text": "HPO ontology", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.94025057554245}]}, {"text": "As of February 2016, the HPO ontology comprises of 11,591 phenotypic abnormalities.", "labels": [], "entities": [{"text": "HPO ontology", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7780136466026306}]}, {"text": "Each of these concepts is provided with a title (with an average length of four words) and about 35% of all these concepts are associated with synonymous titles (by average, each of these concepts has 1.94 synonyms).", "labels": [], "entities": []}, {"text": "For example, Keratoconjunctivitis sicca is a phenotype for which three synonymous titles are provided by the ontology: Dry eye syndrome, Keratitis sicca, and Xerophthalmia.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation results for the synonym identification task. We report mean and median rank (lower  better) and the percentage of phenotypes for which the rank was equal to one (first match; higher better).", "labels": [], "entities": [{"text": "synonym identification task", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.9606345494588217}]}]}