{"title": [{"text": "Composition of Compound Nouns Using Distributional Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "The use of distributional semantics to represent the meaning of a single word has proven to be very effective, but there still is difficulty representing the meaning of larger constituents , such as a noun phrase.", "labels": [], "entities": []}, {"text": "In general, it is unclear how to find a representation of phrases that preserves syntactic distinctions and the relationship between a compound's constituents.", "labels": [], "entities": []}, {"text": "This paper is an attempt to find the best representation of nominal compounds in Spanish and English, and evaluates the performance of different compositional models by using correlations with human similarity judgments and by using compositional representations as input into an SVM classifying the semantic relation between nouns within a compound.", "labels": [], "entities": []}, {"text": "This paper also evaluates the utility of different function's compositional representations , which give our model a slight advantage inaccuracy over other state-of-the-art semantic relation classifiers.", "labels": [], "entities": []}], "introductionContent": [{"text": "The use of distributional semantics has become increasingly popular due to its effectiveness in a range of NLP tasks.", "labels": [], "entities": []}, {"text": "The vector-based representation is computed by looking at the context of every instance of a specific word within a large corpus, which is based on the idea that the meaning of a word is determined by its associations with other words.", "labels": [], "entities": []}, {"text": "Despite the success of vector-based representation in a wide variety on contexts, this method still has difficulty handling larger phrase structures and function words, as opposed to just isolated content words.", "labels": [], "entities": []}, {"text": "Vectors for larger phrases cannot be reliably used due to the sparseness of data.", "labels": [], "entities": []}, {"text": "Ways of representing compositional models for constituents larger than a single word that preserve the lexical and syntactic function of a word in a phrase and best represent the relation between the constituents of a phrase is desired in creating a more general and powerful framework for natural language semantics.", "labels": [], "entities": []}, {"text": "(, and) have compared and empirically tested the effectiveness of different mathematical compositions in representing adjective-noun, verb-object, and noun-noun compounds, but there has been little research into representing nominal compounds that are longer than two words, and the vast majority of research has been in English, without crosslinguistic inquiries.", "labels": [], "entities": []}, {"text": "This paper will investigate the effectiveness of a variety of different compositional functions using two metrics: correlation of the model's cosine similarity predictions with human similarity judgments for two, three and four word Spanish and English noun compounds, and by using the composition of two vectors as input into an SVM used to classify the relations between constituent nouns for two-word English noun compounds.", "labels": [], "entities": []}, {"text": "For the human correlation task, this paper builds on) by analyzing compounds longer than two words, which is a previously unexplored topic, and by analyzing the composition of Spanish compound nouns.", "labels": [], "entities": [{"text": "human correlation", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.6187161505222321}]}, {"text": "As far as we know, compositional models have never been applied to Spanish word vectors be-20 fore.", "labels": [], "entities": []}, {"text": "Previous work have utilized word embeddings as input for relation classification, but we use the composed vectors as input as well, which also has never before been tested.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.9144873321056366}]}, {"text": "This paper is also the first to use noun-relation classification accuracy as a metric for the utility of compositional functions, which gives anew level of insight into the", "labels": [], "entities": [{"text": "noun-relation classification", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.8316796123981476}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.7409049868583679}]}], "datasetContent": [{"text": "For the human similarity judgments, we calculated intersubject agreement using Spearman's \u03c1, using leave-out one resampling as employed by, with the results given in  For the two-word English set, we see that the similarity judgment is consistent with previous work, where) achieved a Spearman's correlation coefficient of 0.49.", "labels": [], "entities": [{"text": "Spearman's correlation coefficient", "start_pos": 285, "end_pos": 319, "type": "METRIC", "confidence": 0.6495411619544029}]}, {"text": "As a general trend, inter-subject agreement declines as the compounds get longer.", "labels": [], "entities": []}, {"text": "We evaluated the similarity of two compounds by taking the cosine of their vectors, a commonly used metric.", "labels": [], "entities": []}, {"text": "To test if a composition model's results were consistent with human judgments, we used Spearman's correlation, where we compared the cosine with the average human similarity judgment.", "labels": [], "entities": [{"text": "Spearman's correlation", "start_pos": 87, "end_pos": 109, "type": "METRIC", "confidence": 0.5060767332712809}]}, {"text": "Similar to, the results indicate that the similarity judgment task was relatively difficult, but there still was a decent amount of consistency between partic-26 ipants.", "labels": [], "entities": [{"text": "similarity judgment task", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.7428537209828695}, {"text": "consistency", "start_pos": 132, "end_pos": 143, "type": "METRIC", "confidence": 0.9790303111076355}]}, {"text": "Our study finds that this task becomes more difficult as the compounds get longer.", "labels": [], "entities": []}, {"text": "For noun relation classification, we used two metrics.", "labels": [], "entities": [{"text": "noun relation classification", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.9387358029683431}]}, {"text": "We performed a ten-fold cross-validation on the training set, and also tested each model on the unseen test set.", "labels": [], "entities": []}, {"text": "For the parameterized functions, we used the optimized parameter values from the corresponding human judgment correlation test.", "labels": [], "entities": []}, {"text": "Since the optimal normalized parameters from the 2-word English set was 0.5 and 0.5, we did not perform a test for the normalized weighted additive set, since the proportions are the same as the simplified additive model.", "labels": [], "entities": []}, {"text": "We also did not test the tensor product model, due to constraints in dimensionality.", "labels": [], "entities": []}, {"text": "shows the model's predictions correlated with the human judgment using Spearman's \u03c1.", "labels": [], "entities": []}, {"text": "Consistent with the work of (Mitchell and Lapata, 2010), all compositional models outperform the head-only and modifier-only models, indicating the utility of the composition functions.", "labels": [], "entities": []}, {"text": "The simple additive model and the multiplicative model yield comparable results for two-word compounds, but the effectiveness of the multiplicative model declines for longer compounds.", "labels": [], "entities": []}, {"text": "This could be due to the previously discussed fact that zero or low-valued entries in the vector can essentially \"throw away\" data in the component vector, leading to poor results as more vectors are composed.", "labels": [], "entities": []}, {"text": "As more and more vectors are composed, this problem is exacerbated and begins to affect performance.", "labels": [], "entities": []}, {"text": "Likewise, the tensor product performs well on two-word compounds in comparison with the additive model, but less soon longer compounds, especially three-word compounds.", "labels": [], "entities": []}, {"text": "This may imply that in addition to dimensionality challenges, the tensor product may face similar limitations to the multiplicative model for composing larger phrases.", "labels": [], "entities": []}, {"text": "For the optimized weighted additive and combined models, the results are very comparable, with the optimized additive model slightly outperforming the normalized additive model.", "labels": [], "entities": []}, {"text": "The combined and weighted additive models yield the most promising results, especially since their accuracy is relatively consistent for handling longer phrases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9992982149124146}]}, {"text": "The increasing inaccuracy of the multiplicative and tensor models and the consistency of the combined and weighted additive models for longer compounds are new insights for the effectiveness of these models, which has serious consequences for attempting to build models that can handle longer phrase structures in general.", "labels": [], "entities": []}, {"text": "This work suggests that the utility of each function can vary with the length of the sentence, which suggests the importance of performing more work on structures longer than two-words, which has been the standard for work in compound nouns until now.", "labels": [], "entities": []}, {"text": "This paper presents strong evidence that the multiplicative model, although promising in previous work handling two-word phrases, has serious shortcomings for handling more complex phrases.", "labels": [], "entities": []}, {"text": "gives the results for each tested function on the different word embeddings, including the concatenation of all the different embeddings.", "labels": [], "entities": []}, {"text": "The CV column represents the 10-fold cross-validation accuracy, and the test set is comprised of unseen noun compounds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.978675127029419}]}, {"text": "Input with only the constituent vector embeddings without the composition function was also tested to give a baseline.", "labels": [], "entities": []}, {"text": "Adding the composition function improves the performances for every type of embedding, with the most dramatic improvement in the concatenated word2vec+HPCA+CW+GloVe model.", "labels": [], "entities": []}, {"text": "We achieved the best results using the concatenation of the word2vec, HPCA, CW, and GloVe embeddings.", "labels": [], "entities": [{"text": "HPCA", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.9131790399551392}]}, {"text": "Adding the composition function improves this models performance by as much as 2.02% using the multiplicative function, demonstrating the utility of using a compositional function during classification.", "labels": [], "entities": []}, {"text": "The simple additive and weighted additive models actually perform worse in cross-validation than using no composition function at all.", "labels": [], "entities": []}, {"text": "The combined models \u03b3 parameter was 9.697, so the multiplicative component of the combined model mostly overpowers the additive components, which explains why its performance is similar to that of the multiplicative model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Overview of different embeddings", "labels": [], "entities": []}, {"text": " Table 3:  Parameters for the combined, optimized  weighted additive, and normalized weighted addtive  models", "labels": [], "entities": []}, {"text": " Table 4: Intersubject Agreement for Human Similarity  Judgments", "labels": [], "entities": [{"text": "Human Similarity  Judgments", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6836607952912649}]}, {"text": " Table 5: Spearman's correlation between human simi- larity judgments and cosine similarity predictions", "labels": [], "entities": []}, {"text": " Table 6: Cross-validation accuracy and accuracy on an unseen test set for semantic relation classification", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9886419177055359}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9992368221282959}, {"text": "semantic relation classification", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.7671894232432047}]}]}