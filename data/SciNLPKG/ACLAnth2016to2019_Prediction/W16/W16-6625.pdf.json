{"title": [{"text": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing", "labels": [], "entities": [{"text": "Paraphrase Generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9192255139350891}, {"text": "Semantic Parsing", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.7110575139522552}]}], "abstractContent": [{"text": "One of the limitations of semantic parsing approaches to open-domain question answering is the lexicosyntactic gap between natural language questions and knowledge base entries-there are many ways to ask a question, all with the same answer.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7590680718421936}, {"text": "open-domain question answering", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.6723519762357076}]}, {"text": "In this paper we propose to bridge this gap by generating paraphrases of the input question with the goal that at least one of them will be correctly mapped to a knowledge-base query.", "labels": [], "entities": []}, {"text": "We introduce a novel grammar model for paraphrase generation that does not require any sentence-aligned paraphrase corpus.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.9485252797603607}]}, {"text": "Our key idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases.", "labels": [], "entities": []}, {"text": "We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.9472028017044067}]}, {"text": "Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser improves over strong baselines.", "labels": [], "entities": [{"text": "WebQuestions benchmark dataset", "start_pos": 34, "end_pos": 64, "type": "DATASET", "confidence": 0.9338196714719137}]}], "introductionContent": [{"text": "Semantic parsers map sentences ontological forms that can be used to query databases), instruct robots), extract information (, or describe visual scenes.", "labels": [], "entities": [{"text": "Semantic parsers map sentences ontological forms", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7926615377267202}]}, {"text": "In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.8508273959159851}]}, {"text": "Current systems accomplish this by learning task-specific grammars, strongly-typed CCG grammars (), or neural networks without requiring any grammar ().", "labels": [], "entities": []}, {"text": "These methods are sensitive to the words used in a question and their word order, making them vulnerable to unseen words and phrases.", "labels": [], "entities": []}, {"text": "Furthermore, mismatch between natural language and Freebase makes the problem even harder.", "labels": [], "entities": []}, {"text": "For example, Freebase expresses the fact that \"Czech is the official language of Czech Republic\" (encoded as a graph), whereas to answer a question like \"What do people in Czech Republic speak?\" one should infer people in Czech Republic refers to Czech Republic and What refers to the language and speak refers to the predicate official language.", "labels": [], "entities": []}, {"text": "We address the above problems by using paraphrases of the original question.", "labels": [], "entities": []}, {"text": "Paraphrasing has shown to be promising for semantic parsing).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.8389365077018738}]}, {"text": "We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs).", "labels": [], "entities": []}, {"text": "Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA;, or hand annotated grammars for KB-based QA).", "labels": [], "entities": [{"text": "paraphrasing", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.9648832082748413}, {"text": "phrase-based machine translation", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.6079603433609009}]}, {"text": "We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (MT)", "start_pos": 13, "end_pos": 62, "type": "TASK", "confidence": 0.7341999709606171}]}, {"text": "Unlike MT based approaches, our system does not require aligned parallel paraphrase corpora.", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9842759370803833}]}, {"text": "In addition we do not require hand annotated grammars for paraphrase generation but instead learn the grammar directly from a large scale question corpus.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.8959738314151764}]}, {"text": "The main contributions of this paper are twofold.", "labels": [], "entities": []}, {"text": "First, we present an algorithm ( \u00a72) to generate paraphrases using latent-variable PCFGs.", "labels": [], "entities": []}, {"text": "We use the spectral method of to estimate L-PCFGs on a large scale question treebank.", "labels": [], "entities": []}, {"text": "Our grammar model leads to a robust and an efficient system for paraphrase generation in opendomain question answering.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.8772381842136383}, {"text": "opendomain question answering", "start_pos": 89, "end_pos": 118, "type": "TASK", "confidence": 0.5503581066926321}]}, {"text": "While CFGs have been explored for paraphrasing using bilingual parallel corpus (, ours is the first implementation of CFG that uses only monolingual data.", "labels": [], "entities": []}, {"text": "Second, we show that generated paraphrases can be used to improve semantic parsing of questions into Freebase logical forms ( \u00a73).", "labels": [], "entities": [{"text": "semantic parsing of questions into Freebase logical forms", "start_pos": 66, "end_pos": 123, "type": "TASK", "confidence": 0.7209175862371922}]}, {"text": "We build on a strong baseline of and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "Below, we give details on the evaluation dataset and baselines used for comparison.", "labels": [], "entities": []}, {"text": "We also describe the model features and provide implementation details.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the WebQuestions dataset ().", "labels": [], "entities": [{"text": "WebQuestions dataset", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.9610291719436646}]}, {"text": "WebQuestions consists of 5,810 question-answer pairs where questions represents real Google search queries.", "labels": [], "entities": [{"text": "WebQuestions", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8896552920341492}]}, {"text": "We use the standard train/test splits, with 3,778 train and 2,032 test questions.", "labels": [], "entities": []}, {"text": "For our development experiments we tune the models on held-out data consisting of 30% training questions, while for final testing we use the complete training data.", "labels": [], "entities": []}, {"text": "We use average precision (avg P.), average recall (avg R.) and average F 1 (avg F 1 ) proposed by as evaluation metrics.", "labels": [], "entities": [{"text": "average precision (avg P.)", "start_pos": 7, "end_pos": 33, "type": "METRIC", "confidence": 0.842497706413269}, {"text": "recall (avg R.)", "start_pos": 43, "end_pos": 58, "type": "METRIC", "confidence": 0.9062369227409363}, {"text": "average F 1 (avg F 1 )", "start_pos": 63, "end_pos": 85, "type": "METRIC", "confidence": 0.8489770367741585}]}], "tableCaptions": [{"text": " Table 1: Oracle statistics and results on the WebQues-", "labels": [], "entities": []}, {"text": " Table 2: Results on WebQuestions test dataset.", "labels": [], "entities": [{"text": "WebQuestions test dataset", "start_pos": 21, "end_pos": 46, "type": "DATASET", "confidence": 0.9498749772707621}]}]}