{"title": [{"text": "Improving word alignment for low resource languages using English monolingual SRL", "labels": [], "entities": [{"text": "Improving word alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9025492469469706}, {"text": "SRL", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.650684654712677}]}], "abstractContent": [{"text": "We introduce anew statistical machine translation approach specifically geared to learning translation from low resource languages, that exploits monolingual English semantic parsing to bias inversion transduction grammar (ITG) induction.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.6252188583215078}, {"text": "learning translation from low resource languages", "start_pos": 82, "end_pos": 130, "type": "TASK", "confidence": 0.8310332198937734}, {"text": "English semantic parsing", "start_pos": 158, "end_pos": 182, "type": "TASK", "confidence": 0.6615650653839111}, {"text": "bias inversion transduction grammar (ITG) induction", "start_pos": 186, "end_pos": 237, "type": "TASK", "confidence": 0.7251793965697289}]}, {"text": "We show that in contrast to conventional statistical machine translation (SMT) training methods, which rely heavily on phrase memorization, our approach focuses on learning bilingual correlations that help translating low resource languages , by using the output language semantic structure to further narrow down ITG constraints.", "labels": [], "entities": [{"text": "statistical machine translation (SMT) training", "start_pos": 41, "end_pos": 87, "type": "TASK", "confidence": 0.8208347473825727}, {"text": "translating low resource languages", "start_pos": 206, "end_pos": 240, "type": "TASK", "confidence": 0.8246006220579147}]}, {"text": "This approach is motivated by previous research which has shown that injecting a semantic frame based objective function while training SMT models improves the translation quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 136, "end_pos": 139, "type": "TASK", "confidence": 0.9859827160835266}]}, {"text": "We show that including a monolingual semantic objective function during the learning of the translation model leads towards a semantically driven alignment which is more efficient than simply tuning loglinear mixture weights against a semantic frame based evaluation metric in the final stage of statistical machine translation training.", "labels": [], "entities": [{"text": "statistical machine translation training", "start_pos": 296, "end_pos": 336, "type": "TASK", "confidence": 0.7058158740401268}]}, {"text": "We test our approach with three different language pairs and demonstrate that our model biases the learning towards more semantically correct alignments.", "labels": [], "entities": []}, {"text": "Both GIZA++ and ITG based techniques fail to capture meaningful bilingual constituents, which is required when trying to learn translation models for low resource languages.", "labels": [], "entities": []}, {"text": "In contrast, our proposed model not only improve translation by injecting a monolingual objective function to learn bilingual correlations during early training of the translation model, but also helps to learn more meaningful correlations with a relatively small data set, leading to a better alignment compared to either conventional ITG or traditional GIZA++ based approaches.", "labels": [], "entities": [{"text": "translation", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9804221987724304}]}], "introductionContent": [{"text": "In this paper we introduce anew approach for inversion transduction grammar (ITG) induction for low resource languages.", "labels": [], "entities": [{"text": "inversion transduction grammar (ITG) induction", "start_pos": 45, "end_pos": 91, "type": "TASK", "confidence": 0.869714651788984}]}, {"text": "Our induction algorithm uses the output language (English) semantic frames.", "labels": [], "entities": []}, {"text": "Recent research showed that including a semantic frame based objective function at an early stage of training statistical machine translation (SMT) systems helps to learn more meaningful word alignments) rather than relying on tuning against a semantic based objective function such as MEANT ( , which improves the translation adequacy).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 110, "end_pos": 147, "type": "TASK", "confidence": 0.7885619153579077}, {"text": "MEANT", "start_pos": 286, "end_pos": 291, "type": "METRIC", "confidence": 0.9179555177688599}]}, {"text": "We show that integrating a semantic based objective function much earlier in the training pipeline not only helps to learn more semantically correct alignments, but also helps us get rid of the heavy memorization used in conventional training methods, which is paramount for low resource languages where data sparseness makes memorization ineffective.", "labels": [], "entities": []}, {"text": "Our approach is also motivated by the fact that inversion transduction grammar alignments have previously been empirically shown to cover 100% of crosslingual semantic frame alternations, while ruling out the majority of incorrect alignments).", "labels": [], "entities": []}, {"text": "We experiment on three different language pairs from the DARPA LORELEI study on efficient learning under low resource conditions: Chinese, Hausa, Uzbek, always translating into English.", "labels": [], "entities": [{"text": "DARPA LORELEI study", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.71929399172465}]}, {"text": "We show that integrating a semantic frame based objective function much earlier in the training pipeline not only produces more semantically correct alignments but also helps to learn bilingual correlations without memorizing from a huge amount of parallel corpora.", "labels": [], "entities": []}, {"text": "We believe that low resource conditions are more interesting than high resource conditions because they are both scientifically and socioeconomically more interesting as they emphasize issues of efficient generalization as opposed to mere memorization from big data collections.", "labels": [], "entities": []}, {"text": "We report results and examples showing that this way for inducing ITGs gives better translation quality compared to the conventional ITG (  and GIZA++ ) alignments.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The size of the different data sets in sentence pairs (foreign-English).  Uzbek  Hausa Chinese  Training  148,190 76,910 39,953  Development 1,200  1,000  1,512  Test  600  500  489", "labels": [], "entities": []}, {"text": " Table 4: Translation quality of an Uzbek-English phrase based SMT system build on three different  alignment methods.  cased/uncased  Alignments  BLEU  METEOR TER  WER  PER  CDER  GIZA++  16.28/17.09 40.7/42.8 82.20/80.91 88.51/87.71 66.70/64.61 79.47/78.11  BITG  16.85/17.66 38.8/40.9 79.75/78.12 85.53/84.60 65.04/62.89 76.93/75.51  Monolingual English SRL", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9523669481277466}, {"text": "Uzbek-English phrase based SMT", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.5139774903655052}, {"text": "BLEU  METEOR TER  WER  PER  CDER  GIZA++", "start_pos": 147, "end_pos": 187, "type": "METRIC", "confidence": 0.8255148604512215}, {"text": "BITG", "start_pos": 260, "end_pos": 264, "type": "METRIC", "confidence": 0.9766474962234497}]}]}