{"title": [{"text": "Error analysis for anaphora resolution in Russian: new challenging issues for anaphora resolution task in a morphologically rich language", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7295899093151093}, {"text": "anaphora resolution", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7471312880516052}]}], "abstractContent": [{"text": "This paper presents a quantitative and qualitative error analysis of Russian anaphora re-solvers which participated in the RU-EVAL event.", "labels": [], "entities": [{"text": "RU-EVAL event", "start_pos": 123, "end_pos": 136, "type": "TASK", "confidence": 0.6026691496372223}]}, {"text": "Its aim is to identify and characterize a set of challenging errors common to state-of-the-art systems dealing with Russian.", "labels": [], "entities": []}, {"text": "We examined three types of pronouns: 3rd person pronouns, reflexive and relative pronouns.", "labels": [], "entities": []}, {"text": "The investigation has shown that a high level of grammatical ambiguity, specific features of reflexive pronouns, free word order and special cases of non-referential pronouns in Rus-sian impact the quality of anaphora resolution systems.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 209, "end_pos": 228, "type": "TASK", "confidence": 0.7379578649997711}]}, {"text": "Error analysis reveals some specific features of anaphora resolution for morphologically rich and free word order languages with alack of gold standard resources.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7232480347156525}]}], "introductionContent": [{"text": "Anaphora resolution, or the task of identifying noun-phrase antecedents of pronouns and adjectival anaphors in a text, is an essential step in the text-processing pipeline of NLP.", "labels": [], "entities": [{"text": "Anaphora resolution", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6911625862121582}]}, {"text": "Still, building an anaphora resolution module is challenging for textmining systems, as it requires a high level of morphological and syntactic analysis at the first stages of the NLP pipeline.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7355243861675262}]}, {"text": "Nevertheless, this task has along history of development and evaluation (e.g. the MUC-6 conference in 1995), and different aspects of anaphora resolution are well studied and have rich resource support, especially for English.", "labels": [], "entities": [{"text": "MUC-6 conference in 1995", "start_pos": 82, "end_pos": 106, "type": "DATASET", "confidence": 0.8810348063707352}, {"text": "anaphora resolution", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.8751477003097534}]}, {"text": "However, Russian (as well as other Slavic languages) poses additional challenges for anaphora resolution, in particular, it has rich morphology, free word order and lacks articles.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.90318363904953}]}, {"text": "Furthermore, Russian is a relatively low-resourced language () due to the lack of freely distributable gold standard corpora for different NLP tasks.", "labels": [], "entities": []}, {"text": "In our paper, we analyze the performance of Russian anaphora resolvers which participated in the RU-EVAL-2014 evaluation campaign ().", "labels": [], "entities": [{"text": "Russian anaphora resolvers", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.5106841325759888}, {"text": "RU-EVAL-2014 evaluation", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.531017616391182}]}, {"text": "RU-EVAL-2014 was dedicated both to anaphora and coreference resolution, but our study focuses only on anaphora resolution, as there were more participants in this task and the results obtained were more reliable.", "labels": [], "entities": [{"text": "RU-EVAL-2014", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8775290250778198}, {"text": "coreference resolution", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.9492766261100769}, {"text": "anaphora resolution", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7090573459863663}]}, {"text": "The aim of this paper is to present quantitative and qualitative error analysis for different pronoun types (reflexives, 3rd person pronouns and relative pronouns).", "labels": [], "entities": []}, {"text": "We identify and characterize a set of challenging errors common to state-of-the-art systems dealing with Russian.", "labels": [], "entities": []}, {"text": "Error analysis enables us to compare the efficiency of different NLP approaches and detect errors that occur either due to language-specific issues or system defects that could be fixed.", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.694686084985733}]}, {"text": "In Section 2, we discuss the previous experience of anaphora and coreference resolution error analysis that we took into account.", "labels": [], "entities": [{"text": "coreference resolution error analysis", "start_pos": 65, "end_pos": 102, "type": "TASK", "confidence": 0.8988522440195084}]}, {"text": "In the section 3, we give a short overview of RU-EVAL-2014, describe the data used for evaluation (RuCoref corpus) and the annotation scheme.", "labels": [], "entities": [{"text": "RU-EVAL-2014", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.8838196396827698}, {"text": "RuCoref corpus)", "start_pos": 99, "end_pos": 114, "type": "DATASET", "confidence": 0.9186646739641825}]}, {"text": "Then, we briefly describe systems that took part in RU-EVAL-2014, the evaluation principles and systems' general performance.", "labels": [], "entities": [{"text": "RU-EVAL-2014", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.7163026928901672}]}, {"text": "The qualitative and quantitative error analysis presented in section 4 reveals language specific features influencing system performance such as particular types of morphological ambiguity, lack of animacy 74 opposition in pronouns, some specific features of syntactic binding for reflexives in Russian, special cases of \"antecedentless\" pronouns (when pronouns show semantic reinterpretation) and others.", "labels": [], "entities": []}, {"text": "We also focus on some issues that are common for other languages, such as syntactic ambiguity in the case of NP embedding and some cases of referential conflicts.", "labels": [], "entities": []}, {"text": "In Section 5, we present our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the pronominal anaphora resolution task, performance on only 3 types of pronominal NPs was evaluated: 1) 3rd person pronouns, 2) the relative pronoun (kotoryj 'which') and 3) reflexive pronouns.", "labels": [], "entities": [{"text": "pronominal anaphora resolution", "start_pos": 7, "end_pos": 37, "type": "TASK", "confidence": 0.7815307974815369}]}, {"text": "The zero anaphora was not evaluated.", "labels": [], "entities": []}, {"text": "As in Evalita-2011 (Poesio and Uryupina, 2011), we used a weak criterion for antecedent identification.", "labels": [], "entities": [{"text": "Evalita-2011", "start_pos": 6, "end_pos": 18, "type": "DATASET", "confidence": 0.8046225905418396}, {"text": "antecedent identification", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.590597927570343}]}, {"text": "It was not required to link a pronoun to its linear closest nonpronominal antecedent.", "labels": [], "entities": []}, {"text": "We treat as true positives the pair of a pronoun and any mention belonging to the same coreference chain which matches the corresponding mention in the gold standard.", "labels": [], "entities": []}, {"text": "For instance, in (1), the following pairs: 'him -Vagner', 'himprofessor' or 'him -he' are allowed.", "labels": [], "entities": []}, {"text": "(1) The evaluation was based on the principle of lenient matching of NPs: a system antecedent matches an NP in the gold standard corpus (GS) if it includes one of possible heads annotated for this gold standard NP.", "labels": [], "entities": [{"text": "gold standard corpus (GS)", "start_pos": 115, "end_pos": 140, "type": "DATASET", "confidence": 0.8169110864400864}]}, {"text": "This makes it possible to compare the results of the systems that differ in principles of antecedent mark-up (cf. NP heads vs. full NPs vs.  We present all of the runs.", "labels": [], "entities": []}, {"text": "The variation in the results for different runs of one system is not as significant as difference between systems, in spite of the different algorithms employed in different runs.", "labels": [], "entities": []}, {"text": "The rule-based runs generally show better results than those based on machine learning techniques; the top three results are achieved by rule-based systems.", "labels": [], "entities": []}, {"text": "Incorporating semantics into analysis leads to better results.", "labels": [], "entities": []}, {"text": "The runs involving semantic role labeling, named entity recognition or ontological information achieve higher F-measure scores.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.6593988140424093}, {"text": "named entity recognition", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.6291539271672567}, {"text": "F-measure", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9783489108085632}]}], "tableCaptions": [{"text": " Table 1: Evaluation results of RU-EVAL-2014", "labels": [], "entities": [{"text": "RU-EVAL-2014", "start_pos": 32, "end_pos": 44, "type": "TASK", "confidence": 0.46604040265083313}]}, {"text": " Table 2: Statistics on pronoun types", "labels": [], "entities": []}, {"text": " Table 3: Precision error rate for different pronoun types", "labels": [], "entities": [{"text": "Precision error rate", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.937531590461731}]}]}