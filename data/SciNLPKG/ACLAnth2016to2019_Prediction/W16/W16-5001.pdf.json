{"title": [{"text": "'Who would have thought of that!': A Hierarchical Topic Model for Extraction of Sarcasm-prevalent Topics and Sarcasm Detection", "labels": [], "entities": [{"text": "Sarcasm Detection", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.6320472657680511}]}], "abstractContent": [{"text": "Topic Models have been reported to be beneficial for aspect-based sentiment analysis.", "labels": [], "entities": [{"text": "aspect-based sentiment analysis", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.7472007075945536}]}, {"text": "This paper reports a simple topic model for sarcasm detection, a first, to the best of our knowledge.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.9654484391212463}]}, {"text": "Designed on the basis of the intuition that sarcastic tweets are likely to have a mixture of words of both sentiments as against tweets with literal sentiment (either positive or negative), our hierarchical topic model discovers sarcasm-prevalent topics and topic-level sentiment.", "labels": [], "entities": []}, {"text": "Using a dataset of tweets labeled using hashtags, the model estimates topic-level, and sentiment-level distributions.", "labels": [], "entities": []}, {"text": "Our evaluation shows that topics such as 'work', 'gun laws', 'weather' are sarcasm-prevalent topics.", "labels": [], "entities": []}, {"text": "Our model is also able to discover the mixture of sentiment-bearing words that exist in a text of a given sentiment-related label.", "labels": [], "entities": []}, {"text": "Finally, we apply our model to predict sarcasm in tweets.", "labels": [], "entities": [{"text": "predict sarcasm in tweets", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.7875820845365524}]}, {"text": "We outperform two prior work based on statistical classifiers with specific features, by around 25%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sarcasm detection is the computational task of predicting sarcasm in text.", "labels": [], "entities": [{"text": "Sarcasm detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9202819764614105}, {"text": "predicting sarcasm in text", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.917783573269844}]}, {"text": "Past approaches in sarcasm detection rely on designing classifiers with specific features (to capture sentiment changes or incorporate context about the author, environment, etc.)", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.9370039999485016}]}, {"text": "(, or model conversations using the sequence labeling-based approach by.", "labels": [], "entities": []}, {"text": "Approaches, in addition to this statistical classifier-based paradigm are: deep learning-based approaches as in the case of Silvio or rule-based approaches such as.", "labels": [], "entities": []}, {"text": "This work employs a machine learning technique that, to the best of our knowledge, has not been used for computational sarcasm.", "labels": [], "entities": []}, {"text": "Specifically, we introduce a topic model for extraction of sarcasm-prevalent topics and as a result, for sarcasm detection.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.8956669270992279}]}, {"text": "Our model based on a supervised version of the Latent Dirichlet Allocation (LDA) model () is able to discover clusters of words that correspond to sarcastic topics.", "labels": [], "entities": []}, {"text": "The goal of this work is to discover sarcasm-prevalent topics based on sentiment distribution within text, and use these topics to improve sarcasm detection.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.844508558511734}]}, {"text": "The key idea of the model is that (a) some topics are more likely to be sarcastic than others, and (b) sarcastic tweets are likely to have a different distribution of positive-negative words as compared to literal positive or negative tweets.", "labels": [], "entities": []}, {"text": "Hence, distribution of sentiment in a tweet is the central component of our model.", "labels": [], "entities": [{"text": "distribution of sentiment in a tweet", "start_pos": 7, "end_pos": 43, "type": "TASK", "confidence": 0.7606256107489268}]}, {"text": "Our sarcasm topic model is learned on tweets that are labeled with three sentiment labels: literal positive, literal negative and sarcastic.", "labels": [], "entities": []}, {"text": "In order to extract sarcasm-prevalent topics, the model uses three latent variables: a topic variable to indicate words that are prevalent in sarcastic discussions, a sentiment variable for sentiment-bearing words related to a topic, and a switch variable that switches between the two kinds of words (topic and sentiment-bearing words).", "labels": [], "entities": []}, {"text": "Using a dataset of 166,955 tweets, our model is able to discover words corresponding to topics that are found in our corpus of positive, negative and sarcastic tweets.", "labels": [], "entities": []}, {"text": "We evaluate our model in two steps: a qualitative evaluation that ascertains sarcasm-prevalent topics based on the ones extracted, and a quantitative evaluation that evaluates sub-components of the model.", "labels": [], "entities": []}, {"text": "We also demonstrate how it can be used for sarcasm detection.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.9735982120037079}]}, {"text": "To do so, we compare our model with two prior work, and observe a significant improvement of around 25% in the F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9980021119117737}]}], "datasetContent": [{"text": "We create a dataset of English tweets for our topic model.", "labels": [], "entities": []}, {"text": "We do not use datasets reported in past work (related to classifiers) because topic models typically require larger datasets than classifiers.", "labels": [], "entities": []}, {"text": "The tweets are downloaded from twitter using the twitter API 2 using hashtag-based supervision.", "labels": [], "entities": []}, {"text": "Hashtag-based supervision is common in sarcasm-labeled datasets ( . Tweets containing hashtags #happy, #excited are labeled as positive tweets.", "labels": [], "entities": []}, {"text": "Tweets with #sad, #angry are labeled as negative tweets.", "labels": [], "entities": []}, {"text": "Tweets with #sarcasm and #sarcastic are labeled as sarcastic tweets.", "labels": [], "entities": []}, {"text": "The tweets are converted to lowercase, and the hashtags used for supervision are removed.", "labels": [], "entities": []}, {"text": "Function words 3 , punctuation, hashtags, author names and hyperlinks are removed from the tweets.", "labels": [], "entities": []}, {"text": "Duplicate tweets (same tweet text repeated for multiple tweets) and re-tweets (tweet text with the 'RT' added in the beginning) are discarded.", "labels": [], "entities": []}, {"text": "Finally, words which occur less than three times in the vocabulary are also removed.", "labels": [], "entities": []}, {"text": "As a result, the tweets that have less than 3 words are removed.", "labels": [], "entities": []}, {"text": "This results in a dataset of 166,955 tweets.", "labels": [], "entities": []}, {"text": "Out of these, 70,934 are positive, 20,253 are negative and the remaining 75,769 are sarcastic.", "labels": [], "entities": []}, {"text": "A total of 35398 tweets are used for testing, out of which 26,210 are of positive sentiment, 5535 are of negative sentiment and 3653 are sarcastic.", "labels": [], "entities": []}, {"text": "We repeat that these labels are determined based on hashtags, as stated above.", "labels": [], "entities": []}, {"text": "The total number of distinct labels (L) is 3, and the total number of distinct sentiment (S) is 2.", "labels": [], "entities": []}, {"text": "The total number of distinct topics (Z) is experimentally determined as 50.", "labels": [], "entities": []}, {"text": "We use block-based Gibbs sampling to estimate the distributions.", "labels": [], "entities": []}, {"text": "The block-based sampler samples all latent variables together based on their joint distributions.", "labels": [], "entities": []}, {"text": "We set asymmetric priors based on sentiment word-list from.", "labels": [], "entities": []}, {"text": "A key parameter of the model is \u03b7 w since it drives the split of a word as a topic or a sentiment word.", "labels": [], "entities": []}, {"text": "SentiWordNet () is used to learn the distribution \u03b7 w prior to estimating the model.", "labels": [], "entities": []}, {"text": "We average across multiple senses of a word.", "labels": [], "entities": []}, {"text": "Based on the SentiWordNet scores to all senses of a word, we determine this probability.: Topics estimated when the topic model is learned on only sarcastic tweets  The goal of this section is to present topics discovered by our sarcasm topic model.", "labels": [], "entities": []}, {"text": "We do so in two steps.", "labels": [], "entities": []}, {"text": "We first describe the topics generated when only sarcastic tweets from our corpus are used to estimate the distributions, followed by the ones when the full corpus is used.", "labels": [], "entities": []}, {"text": "In case of the former, since only sarcastic tweets are used, the topics generated here indicate words corresponding to sarcasm-prevalent topics.", "labels": [], "entities": []}, {"text": "In case of the latter, the sentiment-topic distributions in the model capture the prevalence of sarcasm.", "labels": [], "entities": []}, {"text": "The model estimates the \u03c6 and \u03c7 distributions corresponding to topic words and sentiment words.", "labels": [], "entities": []}, {"text": "Top five words fora subset of topics (as estimated by \u03c6) are shown in.", "labels": [], "entities": []}, {"text": "The headings in boldface are manually assigned . Sarcasm-prevalent topics, as discovered by our topic model, are work, party, weather, women, etc.", "labels": [], "entities": []}, {"text": "The corresponding sentiment topics for each of these sarcasm-prevalent topics (as estimated by \u03c7) are given in.", "labels": [], "entities": []}, {"text": "The headings in boldface are manually assigned.", "labels": [], "entities": []}, {"text": "For topics corresponding to 'party' and 'women', we observe that the two columns contain words from opposing sentiment polarities.", "labels": [], "entities": []}, {"text": "An example sarcastic tweet about work is 'Yaay!", "labels": [], "entities": []}, {"text": "Another night spent at office!", "labels": [], "entities": []}, {"text": "I love working late night'.", "labels": [], "entities": []}, {"text": "The previous set of topics are all from sarcastic text.", "labels": [], "entities": []}, {"text": "We now show the topics extracted by our model from the full corpus.", "labels": [], "entities": []}, {"text": "These topics will indicate peculiarity of topics for each of the three labels, allowing: Topics estimated when the topic model is learned on full corpus us to infer what topics are sarcasm-prevalent.", "labels": [], "entities": []}, {"text": "shows the top 5 topic words for the topics discovered (as estimated in \u03c6) from the full corpus (i.e., containing tweets of all three tweet-level sentiment labels: positive, negative and sarcastic).", "labels": [], "entities": []}, {"text": "shows the top 3 sentiment words for each sentiment (as estimated by \u03c7) of each of the topics discovered.", "labels": [], "entities": []}, {"text": "Like in the previous case, the heading in boldface is manually assigned.", "labels": [], "entities": []}, {"text": "One of the topic discovered was 'Music'.", "labels": [], "entities": [{"text": "'Music'", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.6721149881680807}]}, {"text": "The top 5 topic words for the topic 'Music' are Pop, Country, Rock, Bluegrass and Beatles.", "labels": [], "entities": []}, {"text": "The corresponding sentiment words for Music are 'love', 'happy', 'good' on the positive side and 'sad', 'passion' and 'pain' on the negative side.", "labels": [], "entities": []}, {"text": "The remaining sections present results when the model is learned on the full corpus.", "labels": [], "entities": []}, {"text": "In this section, we answer three questions: (A) What is the likely sentiment label, if a user is talking about a particular topic?", "labels": [], "entities": []}, {"text": "(Section 6.2.1), (B) We hypothesize that sarcastic text tends to have mixed-polarity", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Sentiment-related topics estimated when the topic model is learned on full corpus", "labels": [], "entities": [{"text": "Sentiment-related topics", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9078372716903687}]}, {"text": " Table 6: Probability of sentiment label for various discovered topics", "labels": [], "entities": [{"text": "sentiment label", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.8018879890441895}]}, {"text": " Table 7. The values are averaged over the two classes. Both prior work  show poor F-score (around 18-19%) while our sampling based approach achieves the best F-score of  46.80%. The low values, in general, may be because our corpus is large in size, and is diverse in terms  of the topics. Also, features in Liebrecht et al. (2013) are unigrams, bigrams and trigrams which may  result in sparse features.", "labels": [], "entities": [{"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9988478422164917}, {"text": "F-score", "start_pos": 159, "end_pos": 166, "type": "METRIC", "confidence": 0.9966394901275635}]}, {"text": " Table 7: Comparison of Various Approaches for Sarcasm Detection", "labels": [], "entities": [{"text": "Sarcasm Detection", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.9249968826770782}]}]}