{"title": [{"text": "Generating Clinically Relevant Texts: A Case Study on Life-changing Events", "labels": [], "entities": [{"text": "Generating Clinically Relevant Texts", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7388426437973976}]}], "abstractContent": [{"text": "The need to protect privacy poses unique challenges to behavioral research.", "labels": [], "entities": []}, {"text": "For instance , researchers often cannot use examples drawn directly from such data to explain or illustrate key findings.", "labels": [], "entities": []}, {"text": "In this research , we use data-driven models to synthesize realistic-looking data, focusing on discourse produced by social-media participants announcing life-changing events.", "labels": [], "entities": []}, {"text": "We comparatively explore the performance of distinct techniques for generating synthetic linguistic data across different linguistic units and topics.", "labels": [], "entities": []}, {"text": "Our approach offers utility not only for reporting on qualitative behavioral research on such data, where directly quoting a partici-pant's content can unintentionally reveal sensitive information about the participant, but also for clinical computational system developers , for whom access to realistic synthetic data maybe sufficient for the software development process.", "labels": [], "entities": []}, {"text": "Accordingly, the work also has implications for computational linguistics at large.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.7321583926677704}]}], "introductionContent": [{"text": "Behavioral research using personal data, such as that from social media or clinical studies, must continually balance insights gained with respect for privacy.", "labels": [], "entities": []}, {"text": "Ethical and legal demands also come into play.", "labels": [], "entities": []}, {"text": "Deidentification involves removing information such as named entities, address-specific information and social security numbers.", "labels": [], "entities": []}, {"text": "However, naive approaches are often prone to privacy attacks.", "labels": [], "entities": []}, {"text": "Such de-identified data will often still contain information that, when combined with other data from different resources, can point to the individual who generated it.", "labels": [], "entities": []}, {"text": "For example, if a de-identified dataset contains detailed demographic information, it could then be possible to extract a small list of people matching this information and to identify a specific person using other, publicly available data.", "labels": [], "entities": []}, {"text": "One approach that strikes a good balance is to synthesize realistic-looking data with the same statistical properties as actual data.", "labels": [], "entities": []}, {"text": "Our contribution is to compare different techniques for synthesizing behavioral data.", "labels": [], "entities": []}, {"text": "Specifically, we explore this problem in a case study with social media texts that involve social media participants making announcements about life-changing events, which are personal in nature and which also may affect, positively or negatively, a person's well-being.", "labels": [], "entities": []}, {"text": "Two immediate applications to clinical research that motivate this approach are: qualitative results reporting involving textual data and data access issues for software development purposes.", "labels": [], "entities": [{"text": "qualitative results reporting", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.6224753459294637}]}, {"text": "Neither readers of scientific reports nor software developers need access to the original data as long as realistic looking synthetic data is available.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each event category, we divided the dataset of 2000 tweets into 1800 training and 200 testing instances.", "labels": [], "entities": []}, {"text": "We used the machine translation quality metric BLEU () to measure the similarity between machine generated tweets and the held out tests sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.6586205959320068}]}, {"text": "For each model, we generated ten sets of 200 tweets.", "labels": [], "entities": []}, {"text": "We calculated BLEU scores (without the brevity penalty) using the full 200-tweet test set as the reference for each candidate tweet and report the average of the BLEU scores of all ten sets of tweets generated by a given model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9992345571517944}, {"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9992005228996277}]}, {"text": "To gain further insight into the effectiveness of the machine generated data, we asked human annotators to evaluate the generated tweets.", "labels": [], "entities": []}, {"text": "We selected 800 tweets by randomly sampling: 400 human generated tweets (100 from each category), and 400 machine generated tweets.", "labels": [], "entities": []}, {"text": "The 400 machine gener- ated tweets consisted of 25 tweets for each combination of model (LM-char, LM-word, LSTM-char, LSTM-word) and category (birth, marriage, death, general).", "labels": [], "entities": []}, {"text": "For each tweet, the annotators indicated if they thought the tweet was generated by a human or machine, and they rated the quality of the tweet on the basis of syntax and semantics.", "labels": [], "entities": []}, {"text": "Also, they indicated which topic category they thought the tweet belonged to.", "labels": [], "entities": []}, {"text": "A randomized set of 800 tweets, both real and synthetic, from all four topic categories was submitted to a panel of annotators (co-authors).", "labels": [], "entities": []}, {"text": "Each annotator was asked to decide whether the tweet was real  (i.e., produced by a human) or synthetic (i.e., generated by one of the LSTM or n-gram language models).", "labels": [], "entities": []}, {"text": "Each tweet was also rated in terms of its syntax and semantics on a five point Likert scale.", "labels": [], "entities": []}, {"text": "In addition, the annotators were asked to select the intended topic category (birth, death, marriage, or general) of the tweet.", "labels": [], "entities": []}, {"text": "shows the ability of human annotators to accurately identify a tweet's topic.", "labels": [], "entities": []}, {"text": "In general, the annotators were able to identify the topic of the human tweets, with the weakest performance in the general category.", "labels": [], "entities": []}, {"text": "Identifying the intended topic of the synthetic tweets was more challenging for the annotators, but accuracy was quite high in all topics other than general.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9988803267478943}]}, {"text": "We note that the general category was not filtered to remove tweets that could have belonged to the other topics, which could explain this discrepancy.", "labels": [], "entities": []}, {"text": "show the distribution of each annotator's syntax and semantics scores for each model.", "labels": [], "entities": []}, {"text": "These boxplots show that there was significant variance in the annotators' evaluation of the syntactic and semantic quality of the tweets.", "labels": [], "entities": []}, {"text": "We note, however, that the models yielding the highest BLEU scores, char-LSTM and word-LM, tended to receive more favorable scores for syntactic and semantic quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9983587861061096}]}, {"text": "The character-based LM model, whose BLEU scores were significantly lower than other models, consistently received the most unsatisfactory evaluation of syntactic and semantic quality by all four annotators.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9993059635162354}]}, {"text": "It also seems that the LSTM models produce output that is more consistent in its semantic and syntactic quality, with smaller annotator to annotator variance than the LM models.", "labels": [], "entities": []}, {"text": "With regard to, Annotators 1 and 2 rated 283 (selected randomly) tweets, while Annotators 3 and 4 rated all 800 tweets; and with regard to, all annotators rated 283 tweets.", "labels": [], "entities": []}, {"text": "Annotators 1 and 2 have an academic background in linguistics, while the other two annotators do not have prior linguistic training, perhaps explaining why annotators 1 and 2 generally were better able to identify the topic category.", "labels": [], "entities": []}, {"text": "Annotators 1 and 2 tended to have similar distributions of semantic and syntactic quality scores across models, which again is likely related to their previous training in linguistics and linguistic annotation.", "labels": [], "entities": []}, {"text": "Annotator 4 may have been less forgiving about non-standard language use in the human-composed tweets, while annotator 3 was more tolerant of the syntax and semantics of machine-generated tweets.", "labels": [], "entities": []}, {"text": "RT @USER The new part prigials give birth to bely son Junt and I'm delined a hape proud (Char LSTM Generated) I'm so sorry for your loss and world harry gotting to my funeral it was without URL (Word LM Generated) shows the percent of instances a human annotator marked a synthetic tweet as human generated.", "labels": [], "entities": [{"text": "RT", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8491201400756836}]}, {"text": "shows some of the tweets that were generated by language models but were identified by all four annotators as human generated.", "labels": [], "entities": []}, {"text": "A few example tweets that were correctly identified by all four annotators as synthetic tweets are displayed in.", "labels": [], "entities": []}], "tableCaptions": []}