{"title": [{"text": "Focus Annotation of Task-based Data: Establishing the Quality of Crowd Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "We explore the annotation of information structure in German and compare the quality of expert annotation with crowd-sourced annotation taking into account the cost of reaching crowd consensus.", "labels": [], "entities": []}, {"text": "Concretely, we discuss a crowd-sourcing effort annotating focus in a task-based corpus of German containing reading comprehension questions and answers.", "labels": [], "entities": []}, {"text": "Against the backdrop of a gold standard reference resulting from adjudicated expert annotation, we evaluate a crowd sourcing experiment using majority voting to determine a baseline performance.", "labels": [], "entities": [{"text": "crowd sourcing", "start_pos": 110, "end_pos": 124, "type": "TASK", "confidence": 0.6796777695417404}]}, {"text": "To refine the crowd-sourcing setup, we introduce the Consensus Cost as a measure of agreement within the crowd.", "labels": [], "entities": [{"text": "Consensus Cost", "start_pos": 53, "end_pos": 67, "type": "METRIC", "confidence": 0.9231995940208435}]}, {"text": "We investigate the usefulness of Consensus Cost as a measure of crowd annotation quality both intrinsically, in relation to the expert gold standard, and extrinsically, by integrating focus annotation information into a system performing Short Answer Assessment taking into account the Consensus Cost.", "labels": [], "entities": []}, {"text": "We find that low Consensus Cost in crowd sourcing indicates high quality, though high cost does not necessarily indicate low accuracy but increased variability.", "labels": [], "entities": [{"text": "Consensus Cost", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.8580220639705658}, {"text": "crowd sourcing", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7185606956481934}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9982642531394958}]}, {"text": "Overall , taking Consensus Cost into account improves both intrinsic and extrinsic evaluation measures.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper addresses the question of how to explore and evaluate the annotation of information structural concepts to support the analysis of authentic data.", "labels": [], "entities": []}, {"text": "While the formal pragmatic concepts in information structure, such as the focus of an utterance, are precisely defined in theoretical linguistics and potentially very useful in conceptual and practical terms, it has turned out to be difficult to reliably annotate such notions in corpus data (.", "labels": [], "entities": []}, {"text": "Theoretical linguists have discussed the notion of focus for decades (cf., e.g.,.", "labels": [], "entities": []}, {"text": "Following the work of, one of the widely used definitions of focus is that \"Focus indicates the presence of alternatives that are relevant for the linguistic expressions\" (cf..", "labels": [], "entities": []}, {"text": "Which part of an utterance is in the focus thus depends on the context of the utterance, as illustrated by the question-answers pairs in examples (1) and (2).", "labels": [], "entities": []}, {"text": "Since focus is signalled by prosodic prominence in an intonation language like English, the answers also show different prominence patterns, as indicated by the pitch accents on picture in (1) and Mary in (2).", "labels": [], "entities": []}, {"text": "The linguistic discussions of focus phenomena generally are based on few example sentences, without an apparent exploration of substantial amounts of authentic data.", "labels": [], "entities": []}, {"text": "Only few attempts at systematically identifying focus in authentic data have been made (.", "labels": [], "entities": []}, {"text": "They generally ran into significant problems trying to reach good inter-annotator agreement, as they tried to identify focus in newspaper text or other data types where no explicit questions are available, making the task of determining the question under discussion, and thus reliably annotating focus, particularly difficult.", "labels": [], "entities": []}, {"text": "More recently, showed that reliable focus annotation is feasible, even for somewhat ill-formed learner language, if one has access to explicit questions and takes them into account in an incremental annotation scheme.", "labels": [], "entities": []}, {"text": "They demonstrate the effectiveness of the approach by reporting both substantial inter-annotator agreement and a substantial extrinsic improvement resulting from integration of focus information into a Short Answer Assessment system.", "labels": [], "entities": []}, {"text": "However, manual focus annotation by experts is time consuming, both for annotator training and the annotation itself.", "labels": [], "entities": []}, {"text": "Additionally, in computational linguistics it has been argued) that annotation of theoretical linguistic notions by experts should be complemented by external grounding, either in the form of extrinsic evaluation, as reported above, or by using crowdsourcing: by formulating the annotation task in such away that non-experts can understand it and carry it out, one ensures that the task does not depend on implicit knowledge shared only by a team of experts.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of crowdsourcing -which has been shown to work well fora number of linguistic tasks (see, e.g.,) -for focus annotation.", "labels": [], "entities": [{"text": "focus annotation", "start_pos": 136, "end_pos": 152, "type": "TASK", "confidence": 0.8023470044136047}]}, {"text": "We investigate how systematically the untrained crowd can identify a meaning-based linguistic notion like focus in authentic data and which characteristics of the data and context lead to consistent annotation results.", "labels": [], "entities": []}, {"text": "Having established the general feasibility of non-expert focus annotation, we refine the crowdsourcing approach by taking into account the variability within the set of crowd judgements.", "labels": [], "entities": []}, {"text": "The approach is based on the idea that sentences with little variation in the annotation provided by the crowd are more reliably annotated, i.e., are of a higher quality.", "labels": [], "entities": []}, {"text": "We spell out a measure of crowd diversity, Consensus Cost, and investigate its usefulness both intrinsically, by relating it to the expertbased gold-standard, and extrinsically, by integrating cost-based focus annotation data in a Short Answer Assessment system.", "labels": [], "entities": []}], "datasetContent": [{"text": "To study non-expert focus annotation, we implemented a crowd-sourcing task using the crowdsourcing platform CrowdFlower 2 to collect focus annotations from crowd workers.", "labels": [], "entities": []}, {"text": "CrowdFlower makes it possible to require workers to come from German speaking countries, a feature that other platforms like Amazon Mechanical Turk do not provide as transparently, and it has a built-in quality control mechanism ensuring that workers throughout the entire job maintain a certain level of accuracy on interspersed test items.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 305, "end_pos": 313, "type": "METRIC", "confidence": 0.9968361854553223}]}, {"text": "As data for our crowd-sourcing experiment, we used 5,597 question-answer pairs from the CREG-5K corpus and 100 manually constructed test question-answer pairs.", "labels": [], "entities": [{"text": "CREG-5K corpus", "start_pos": 88, "end_pos": 102, "type": "DATASET", "confidence": 0.9541555941104889}]}, {"text": "The task of the crowd workers was to mark those words in an answer sentence that \"contain the information asked for in the question\".", "labels": [], "entities": []}, {"text": "Workers were shown five questionanswer pairs at a time.", "labels": [], "entities": []}, {"text": "One of those five was from our set of hand-crafted test question-answer pairs.", "labels": [], "entities": []}, {"text": "The workers were paid two cents per annotated sentence.", "labels": [], "entities": []}, {"text": "Since CREG-5K consists of reading comprehension questions and answers provided by learners of German, there are cases where a student response does not answer a given question at all, for example, when the learner misunderstood the question.", "labels": [], "entities": []}, {"text": "In the gold standard annotation described in section 2.1, the annotators had the option to mark such cases as \"question ignored\".", "labels": [], "entities": []}, {"text": "Since we also wanted to provide the crowd workers with this option, we included a checkbox \"Frage nicht beantwortet\" (\"question not answered\").", "labels": [], "entities": []}, {"text": "When this option is selected, no word in the answer sentence can be marked as focus.", "labels": [], "entities": []}, {"text": "shows an example CrowdFlower task with the marked words in yellow.", "labels": [], "entities": []}, {"text": "These marked words are the ones that we counted as focus.", "labels": [], "entities": []}, {"text": "The English translation shown below was not part of the CrowdFlower task.", "labels": [], "entities": []}, {"text": "We collected 11 focus annotations per answer sentence and crowd workers had to maintain an accuracy of 60% on the test question-answer pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.999516487121582}]}, {"text": "Altogether we collected 62,247 annotated sentences.", "labels": [], "entities": []}, {"text": "To evaluate the quality of our crowd focus annotation, we wanted to find out how the annotations produced by the crowd workers compare to the gold standard expert annotation described in section 2.1.", "labels": [], "entities": []}, {"text": "We therefore chose to calculate all possibilities of combining one through eleven workers into one \"virtual\" annotator using majority voting on individual word judgments.", "labels": [], "entities": []}, {"text": "Ties in voting are resolved by random assignment.", "labels": [], "entities": [{"text": "voting", "start_pos": 8, "end_pos": 14, "type": "TASK", "confidence": 0.9153993725776672}]}, {"text": "The procedure is similar to the approach described by.", "labels": [], "entities": []}, {"text": "We did not employ any bias correction or other types of weighting schemes, as discussed, e.g., by, but plan to do so in future research.", "labels": [], "entities": []}, {"text": "In measuring agreement between crowd workers and the expert gold-standard on the word level, for the following reasons we opted for percentage agreement instead of Kappa or other measures that include a notion of expected agreement: i) Kappa assumes the annotators to be the same across all instances and this is systematically violated by the crowd-sourcing setup, and ii) calculating Kappa on a per-answer basis is not sensible in cases where only one class occurs, as in all-focus and no-focus answers.", "labels": [], "entities": []}, {"text": "To identify patterns that show which types of data can be annotated with focus most consistently by crowd workers compared to the experts, we particularly want to look at properties of our data that take characteristics of the context into accountwhich in our case is the question context in which an answer annotated with focus occurs.", "labels": [], "entities": []}, {"text": "We therefore investigated the impact of different types of questions on annotation agreement.", "labels": [], "entities": []}, {"text": "We carried out the comparison for the specific question form subtypes distinguishing surface forms of wh-questions as annotated in CREG).", "labels": [], "entities": [{"text": "CREG", "start_pos": 131, "end_pos": 135, "type": "DATASET", "confidence": 0.8878888487815857}]}, {"text": "shows how the different question form subtypes impact the agreement between the crowd and the gold-standard focus annotation.", "labels": [], "entities": []}, {"text": "As reference, the dotted lines again show the percentage agreements between the two expert annotators for the different question forms.", "labels": [], "entities": []}, {"text": "The question forms make the answers fall into three broad categories in terms of worker-gold agreement: the most concrete ones (who, when and where) in terms of surface realization in answers come out on top with percentage agreements at 91% (where), 87% (who), and 86% (when).", "labels": [], "entities": []}, {"text": "The third group consists only of why-questions at an agreement level of 71%.", "labels": [], "entities": []}, {"text": "For such questions asking for reasons, the range of possible answer realizations arguably is the greatest given that reasons are typically expressed by whole clauses.", "labels": [], "entities": []}, {"text": "However, for the gold expert-annotation, the more explicit guidelines seem to have paid off in this case, as why-questions come out at a much higher agreement level of 86%.", "labels": [], "entities": []}, {"text": "To test whether more explicit guidelines could also help the crowd annotators to be more systematic in their focus annotation, we conducted a small additional crowdsourcing annotation study with a smaller data set only containing answers to why and what-questions.", "labels": [], "entities": []}, {"text": "While the general setup was the same as described in section 3.1, we provided the crowd workers with more examples illustrating focus in different kind of answers.", "labels": [], "entities": []}, {"text": "The result was only a small improvement in agreement between crowd and gold standard annotation, with answers to what-questions 1% higher than before, and 2% higher for why-questions.", "labels": [], "entities": [{"text": "agreement", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9979694485664368}]}, {"text": "Even more explicit guidelines thus do not seem to help the nonexperts to handle answers occurring with whyquestions when annotating focus.", "labels": [], "entities": []}, {"text": "Summing up the results so far, the crowd annotation study shows that i. the percentage agreement improves the more crowd workers are taken into account, and ii. majority voting on crowd worker judgments compared to the expert gold annotation can reach the expert level for specific cases (e.g., where-questions).", "labels": [], "entities": []}, {"text": "To externally establish the relevance and quality of the crowd focus annotation, we extrinsically evaluated the expert gold standard annotation in an independent task, Short Answer Assessment, specifically the automatic assessment of answers to reading comprehension questions.", "labels": [], "entities": []}, {"text": "For this purpose, we employed the CoMiC system (Meurers et al., 2011), which assesses student answers by analyzing the quantity and quality of alignment links it finds between the student and the target answer.", "labels": [], "entities": [{"text": "Meurers et al., 2011)", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.81521408756574}]}, {"text": "Our goal here is twofold: on the one hand, we want to find out whether the previously introduced Consensus Cost measure is helpful in determining the quality of focus annotation as measured by its impact on Short Answer Assessment.", "labels": [], "entities": [{"text": "Short Answer Assessment", "start_pos": 207, "end_pos": 230, "type": "TASK", "confidence": 0.6846645673116049}]}, {"text": "On the other hand, it is interesting to determine whether the state of the art in automatic answer assessment can be advanced by integrating non-expert annotation of focus (as a step towards automatic focus annotation developed using the crowd-annotated data).", "labels": [], "entities": [{"text": "automatic answer assessment", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.6861893733342489}]}, {"text": "To cleanly separate the data used for testing the Answer Assessment system CoMiC from the data used for training CoMiC, we randomly sampled approximately 20% of the CREG-5K data set and set it aside as the final test set.", "labels": [], "entities": [{"text": "Answer Assessment system CoMiC", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.7487090229988098}, {"text": "CREG-5K data set", "start_pos": 165, "end_pos": 181, "type": "DATASET", "confidence": 0.9363798101743063}]}, {"text": "The remaining 80% was used as training set.", "labels": [], "entities": []}, {"text": "In exploring the impact of different Consensus Costs, we used the same four cutoffs as before: 0.25, 0.5, 0.75 and the maximum value 1.0.", "labels": [], "entities": []}, {"text": "For each cutoff, we picked the answers with crowd focus annotations satisfying the cutoff constraint in training and test set, and ran CoMiC on the resulting data excerpt, aligning only words in student and target answer that are focused.", "labels": [], "entities": []}, {"text": "For the rest of the data, which did not meet the Consensus Cost criterion or for which no focus annotation was available, we used the standard version of CoMiC that only aligns words not previously mentioned in the question.", "labels": [], "entities": [{"text": "Consensus Cost criterion", "start_pos": 49, "end_pos": 73, "type": "METRIC", "confidence": 0.8543924689292908}]}, {"text": "We then calculated a weighted average (by number of test instances) of both system accuracies in order to arrive at an overall system result for the respective Consensus Cost value.", "labels": [], "entities": []}, {"text": "The results are displayed in: Results on the \"unseen answers\" test set The 'train/test' column shows the number of training and test instances each system was run on, and the '%' column shows the classification accuracy achieved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9684422612190247}]}, {"text": "The 'base' row gives the baseline resulting from using CoMiC as-is, without any focus information.", "labels": [], "entities": []}, {"text": "Looking at the results for the focus partition of the data, one can see that accuracy drops when taking into account focus annotation with higher Consensus Cost, even though thereby in principle more training data is becoming available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9995194673538208}, {"text": "Consensus Cost", "start_pos": 146, "end_pos": 160, "type": "METRIC", "confidence": 0.9759660959243774}]}, {"text": "For the 'Given' column, when data with higher Consensus Cost is used for the 'Focus' version of the system and thereby less data is available for training the 'Given' system, accuracy of the latter decreases.", "labels": [], "entities": [{"text": "Consensus Cost", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.9665209054946899}, {"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9995019435882568}]}, {"text": "Overall, a Consensus Cost cutoff of 0.75 gives the optimal trade-off between both system variants, yielding 83.2% classification accuracy.", "labels": [], "entities": [{"text": "Consensus Cost cutoff", "start_pos": 11, "end_pos": 32, "type": "METRIC", "confidence": 0.8351735671361288}, {"text": "classification", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.9299226403236389}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9352781772613525}]}, {"text": "Test with answers to unseen questions Ina second experiment, we also compiled a questionbased train/test split, meaning that for approximately 20% of randomly picked questions in CREG-5K, all answers were held out as the test set.", "labels": [], "entities": [{"text": "CREG-5K", "start_pos": 179, "end_pos": 186, "type": "DATASET", "confidence": 0.936314582824707}]}, {"text": "This is a much harder benchmark since the system in the test has to classify answers to previously unseen questions, providing some indication of the system's ability to learn something general rather than about specific question-answer pairs.", "labels": [], "entities": []}, {"text": "The remainder of the testing procedure was the same as described above, yielding the results detailed in: Results on the \"unseen questions\" test set The accuracies are generally lower due to the harder test scenario.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 153, "end_pos": 163, "type": "METRIC", "confidence": 0.9955718517303467}]}, {"text": "Moreover, the clear trends observed above with regard to training and test size do not seem to apply as clearly here, likely again owing to the 'unseen questions' scenario.", "labels": [], "entities": []}, {"text": "Given the many different types of potential questions and the relatively small number of different questions the system sees during training, it is more important for which questions the system has seen answers, than how many.", "labels": [], "entities": []}, {"text": "However, despite the differences to the previous experiment, the optimal result is again achieved with a Consensus Cost of 0.75, supporting the conclusion that Consensus Cost supports a systematic characterization of annotation quality.", "labels": [], "entities": [{"text": "Consensus Cost", "start_pos": 105, "end_pos": 119, "type": "METRIC", "confidence": 0.9848591983318329}]}], "tableCaptions": [{"text": " Table 1: Results on the \"unseen answers\" test set", "labels": [], "entities": []}, {"text": " Table 2: Results on the \"unseen questions\" test set", "labels": [], "entities": []}]}