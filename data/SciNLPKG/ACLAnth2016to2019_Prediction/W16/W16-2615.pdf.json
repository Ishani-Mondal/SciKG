{"title": [{"text": "LTL-UDE @ EmpiriST 2015: Tokenization and PoS Tagging of Social Media Text", "labels": [], "entities": [{"text": "LTL-UDE @ EmpiriST 2015", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.8582771569490433}, {"text": "PoS Tagging of Social Media Text", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.8270073185364405}]}], "abstractContent": [{"text": "We present a detailed description of our submission to the EmpiriST shared task 2015 for tokenization and part-of-speech tagging of German social media text.", "labels": [], "entities": [{"text": "EmpiriST shared task 2015", "start_pos": 59, "end_pos": 84, "type": "DATASET", "confidence": 0.9291560351848602}, {"text": "tokenization", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.9681942462921143}, {"text": "part-of-speech tagging of German social media text", "start_pos": 106, "end_pos": 156, "type": "TASK", "confidence": 0.8453773217541831}]}, {"text": "As relatively little training data is provided, neither tokenization nor PoS tagging can be learned from the data alone.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.9687784314155579}, {"text": "PoS tagging", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.7839494347572327}]}, {"text": "For tok-enization, our system uses regular expressions for general cases and word lists for exceptions.", "labels": [], "entities": []}, {"text": "For PoS tagging, adding unsu-pervised knowledge beyond the available training data is the most important factor for reaching acceptable tagging accuracy.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9197591841220856}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9604083299636841}]}, {"text": "A learning curve experiment shows furthermore that more in-domain training data is very likely to further increase accuracy .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9974803328514099}]}], "introductionContent": [{"text": "Tokenization and part-of-speech (PoS) tagging are two fundamental NLP tasks.", "labels": [], "entities": [{"text": "part-of-speech (PoS) tagging", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.6484370470046997}]}, {"text": "Tokenization aims at detecting word and sentence boundaries in text while PoS tagging uses the recognized words and assigns each word its syntactical category.", "labels": [], "entities": [{"text": "detecting word and sentence boundaries in text", "start_pos": 21, "end_pos": 67, "type": "TASK", "confidence": 0.7420258522033691}, {"text": "PoS tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.7340767830610275}]}, {"text": "Both tasks are especially challenging when applied on noisy social media texts.", "labels": [], "entities": []}, {"text": "The main challenge when tokenizing social media text is the ambiguity of punctuation characters which occurs more frequently than in other domains.", "labels": [], "entities": [{"text": "tokenizing social media text", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.912838876247406}]}, {"text": "A major source of ambiguity are emoticons that show a surprising degree of complexity ranging from two-character emoticons such as :) to ncharacter emoticons such as \\(*.*#).", "labels": [], "entities": []}, {"text": "Additionally challenges are introduced by missing whitespace characters and the use of non-standard abbreviations such as in aus meiner (Doz.)Sicht.:).", "labels": [], "entities": []}, {"text": "For PoS tagging, the main source of error are the frequently occurring unknown word forms that are spelling variations of words found in the dictionary.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9186312854290009}]}, {"text": "Those spelling variations are usually not contained in the (newswire) training data of the model which leads to a strong decline inaccuracy on social media data (.", "labels": [], "entities": []}, {"text": "There has been little work for German social media processing, the EmpiriST () provides for both tasks two data sets composing of dialogical and monological text of the social media domain to help the development of robust tools for German.", "labels": [], "entities": [{"text": "German social media processing", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.5816953405737877}, {"text": "EmpiriST", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.5763974785804749}]}, {"text": "The results of our approaches for tokenization and PoS tagging are reported under the name LTL-UDE in the EmpiriST rankings.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9811020493507385}, {"text": "PoS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.861560732126236}, {"text": "EmpiriST rankings", "start_pos": 106, "end_pos": 123, "type": "DATASET", "confidence": 0.973854660987854}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 5: Results of applying our trained PoS tagger against the released gold test data, we present  additional to the overall result the accuracy gain of adding 100k token Tiger and the gains of adding  each individual resource compared to training on Empiri+Tiger. We compare our performance against  the German TreeTagger model.", "labels": [], "entities": [{"text": "gold test data", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.7548709313074747}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9994328618049622}, {"text": "Empiri+Tiger", "start_pos": 254, "end_pos": 266, "type": "DATASET", "confidence": 0.9467258453369141}, {"text": "German TreeTagger model", "start_pos": 308, "end_pos": 331, "type": "DATASET", "confidence": 0.9102814594904581}]}, {"text": " Table 6: Accuracy per word class with an accuracy  of less than 50%. PoS tags newly added in the  extended STTS tagset are highlighted in grey.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9954485297203064}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.998783528804779}, {"text": "STTS tagset", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.7792566418647766}]}, {"text": " Table 6. Notewor- thy is that seven word classes have an accuracy of  zero. Five of those classes are newly added tags", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9988399147987366}]}]}