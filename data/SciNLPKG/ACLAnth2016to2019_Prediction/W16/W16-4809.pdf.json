{"title": [{"text": "Automatic Detection of Arabicized Berber and Arabic Varieties", "labels": [], "entities": [{"text": "Automatic Detection of Arabicized Berber and Arabic Varieties", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.8032348155975342}]}], "abstractContent": [{"text": "Automatic Language Identification (ALI) is the detection of the natural language of an input text by a machine.", "labels": [], "entities": [{"text": "Automatic Language Identification (ALI)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7577402939399084}, {"text": "detection of the natural language of an input text", "start_pos": 47, "end_pos": 97, "type": "TASK", "confidence": 0.6742194692293803}]}, {"text": "It is the first necessary step to do any language-dependent natural language processing task.", "labels": [], "entities": [{"text": "language-dependent natural language processing task", "start_pos": 41, "end_pos": 92, "type": "TASK", "confidence": 0.6943980872631073}]}, {"text": "Various methods have been successfully applied to a wide range of languages, and the state-of-the-art automatic language identifiers are mainly based on character n-gram models trained on huge corpora.", "labels": [], "entities": []}, {"text": "However, there are many languages which are not yet automatically processed , for instance minority and informal languages.", "labels": [], "entities": []}, {"text": "Many of these languages are only spoken and do not exist in a written format.", "labels": [], "entities": []}, {"text": "Social media platforms and new technologies have facilitated the emergence of written format for these spoken languages based on pronunciation.", "labels": [], "entities": []}, {"text": "The latter are not well represented on the Web, commonly referred to as under-resourced languages, and the current available ALI tools fail to properly recognize them.", "labels": [], "entities": []}, {"text": "In this paper, we revisit the problem of ALI with the focus on Arabicized Berber and dialectal Arabic short texts.", "labels": [], "entities": []}, {"text": "We introduce new resources and evaluate the existing methods.", "labels": [], "entities": []}, {"text": "The results show that machine learning models combined with lexicons are well suited for detecting Arabicized Berber and different Arabic varieties and distinguishing between them, giving a macro-average F-score of 92.94%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 204, "end_pos": 211, "type": "METRIC", "confidence": 0.9740965962409973}]}], "introductionContent": [{"text": "Automatic Language Identification (ALI) is a well-studied field in computational linguistics, since early 1960's, where various methods achieved successful results for many languages.", "labels": [], "entities": [{"text": "Automatic Language Identification (ALI)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7897672851880392}, {"text": "computational linguistics", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.7165943384170532}]}, {"text": "ALI is commonly framed as a categorization.", "labels": [], "entities": []}, {"text": "However, the rapid growth and wide dissemination of social media platforms and new technologies have contributed to the emergence of written forms of some varieties which are either minority or colloquial languages.", "labels": [], "entities": []}, {"text": "These languages were not written before social media and mobile phone messaging services, and they are typically under-resourced.", "labels": [], "entities": []}, {"text": "The state-of-the-art available ALI tools fail to recognize them and represent them by a unique category; standard language.", "labels": [], "entities": []}, {"text": "For instance, whatever is written in Arabic script, and is clearly not Persian, Pashto or Urdu, is considered as Arabic, Modern Standard Arabic (MSA) precisely, even though there are many Arabic varieties which are considerably different from each other.", "labels": [], "entities": []}, {"text": "There are also other less known languages written in Arabic script but which are completely different from all Arabic varieties.", "labels": [], "entities": []}, {"text": "In North Africa, for instance, Berber or Tamazight 2 , which is widely used, is also written in Arabic script mainly in Algeria, Libya and Morocco.", "labels": [], "entities": []}, {"text": "Arabicized Berber (BER) or Berber written in Arabic script is an under-resourced language and unknown to all available ALI tools which misclassify it as Arabic (MSA).", "labels": [], "entities": []}, {"text": "Arabicized Berber does not use special characters and it coexists with Maghrebi Arabic where the dialectal contact has made it hard for non-Maghrebi people to distinguish 1 Assigning a predefined category to a given text based on the presence or absence of some features.", "labels": [], "entities": []}, {"text": "An Afro-Asiatic language widely spoken in North Africa and different from Arabic.", "labels": [], "entities": []}, {"text": "It has 13 varieties and each has formal and informal forms.", "labels": [], "entities": []}, {"text": "It has its unique script called Tifinagh but for convenience Latin and Arabic scripts are also used.", "labels": [], "entities": []}, {"text": "Using Arabic script to transliterate Berber has existed since the beginning of the Islamic Era (L.).", "labels": [], "entities": []}, {"text": "3 Among the freely available language identification tools, we tried Google Translator, Open Xerox language and Translated labs at http://labs.translated.net.", "labels": [], "entities": [{"text": "language identification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7319875657558441}]}, {"text": "it from local Arabic dialects.", "labels": [], "entities": []}, {"text": "For instance each word in the Arabicized Berber sentence 'AHml sAqwl mA$y dwl kAn' 5 which means 'love is from heart and not just a word' has a false friend in MSA and all Arabic dialects.", "labels": [], "entities": []}, {"text": "In MSA, the sentence means literally 'I carry I will say going countries was' which does not mean anything.", "labels": [], "entities": [{"text": "MSA", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.7980381846427917}]}, {"text": "In this study, we deal with the automatic detection of Arabicized Berber and distinguishing it from the most popular Arabic varieties.", "labels": [], "entities": [{"text": "automatic detection of Arabicized Berber", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.6769102573394775}]}, {"text": "We consider only the seven most popular Arabic dialects, based on the geographical classification, plus MSA.", "labels": [], "entities": [{"text": "MSA", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.8859231472015381}]}, {"text": "There are many local dialects due to the linguistic richness of the Arab world, but it is hard to deal with all of them for two reasons: it is hard to get enough data, and it is hard to find reliable linguistic features as these local dialects are very hard to describe and full of unpredictability and hybridization.", "labels": [], "entities": []}, {"text": "We start the paper by a brief overview about the related work done for Arabicized Berber and dialectal Arabic ALI in Section 2.", "labels": [], "entities": []}, {"text": "We then describe the process of building the linguistic resources (dataset and lexicons) used in this paper and motivate the adopted classification in Section 3.", "labels": [], "entities": []}, {"text": "We next describe the experiments and analyze the results in Sections 4 and 5, and finally conclude with the findings and future plans.", "labels": [], "entities": []}], "datasetContent": [{"text": "For Arabicized Berber, two Berber native speakers collected 503 documents (5,801 words) from north African countries mainly from forums, blogs and Facebook.", "labels": [], "entities": []}, {"text": "For more data, we have selected varied texts from Algerian newspapers and segmented them.", "labels": [], "entities": []}, {"text": "Originally the news texts are short, around 1,500 words each, so we considered each paragraph as a document (maximum 178 words).", "labels": [], "entities": []}, {"text": "The selected newspapers use various Berber standard varieties written in Arabic script.", "labels": [], "entities": []}, {"text": "For each Arabic variety, two native speakers have manually collected content from various social media platforms (forums, blogs and micro-blogs) where each user's comment is counted as a single document/text.", "labels": [], "entities": []}, {"text": "We gave instructions, for instance 'Collect only what is clearly written in your dialect, i.e. texts containing at least one clear dialectal word and you can easily understand it and reproduce the same in your daily interactions'.", "labels": [], "entities": []}, {"text": "We have also compiled a list of dialectal words for each Arabic variety based on our knowledge.", "labels": [], "entities": []}, {"text": "We then used a script with the compiled words as keywords to collect more data.", "labels": [], "entities": []}, {"text": "Likewise, we collected 1,000 documents (around 54,150 words) for each dialect, roughly published between 2012-2016 in various platforms (micro-blogs, forums, blogs and online newspapers) from allover the Arab world.", "labels": [], "entities": []}, {"text": "The same native speakers have been asked to clean the data following the same set of instructions.", "labels": [], "entities": []}, {"text": "We ended up with an unbalanced corpus of between 2,430 documents (64,027 words) and 6,000 documents or (170,000 words) for each dialect.", "labels": [], "entities": []}, {"text": "In total, the collected dataset contains 579,285 words.", "labels": [], "entities": []}, {"text": "In terms of data source distribution, the majority of the content comes from blogs and forums where users are trying to promote their dialects; roughly 50%, around 30% of the data comes from popular YouTube channels and the rest is collected from micro-blogs.", "labels": [], "entities": []}, {"text": "The selection of the data sources is based on the quality of the dialectal content, i.e. we know that the content of the selected forums and blogs is dialectal which is used to teach or promote dialects between users.", "labels": [], "entities": []}, {"text": "Ideally we would have looked at just some data resources and harvest content as much as possible either manually or using a script.", "labels": [], "entities": []}, {"text": "But given the fact that data depends on the platform it is used in 12 and our goal that is to build a general system which will be able to handle various domain/topic independent data, we have used various data domains dealing with quite varied topics like cartoons, cooking, health/body care, movies, music, politics and social issues.", "labels": [], "entities": []}, {"text": "We labeled each document with the corresponding Arabic variety.", "labels": [], "entities": []}, {"text": "We introduced necessary pre-processing rules such as tokenization, normalization and removal of nondiscriminative words including punctuation, emoticons, any word occurring in the MSA data more than 100 times (prepositions, verbs, common nouns, proper nouns, adverbs, etc.) and Named Entities (NE).", "labels": [], "entities": [{"text": "MSA data more than 100 times (prepositions, verbs, common nouns, proper nouns, adverbs, etc.)", "start_pos": 180, "end_pos": 273, "type": "Description", "confidence": 0.8104881175926754}]}, {"text": "Removing non-discriminative words is motivated by the fact that these words are either prevalent in all Arabic varieties or they do not carry any important linguistic information like emoticons and punctuation.", "labels": [], "entities": []}, {"text": "The choice of removing NE is motivated by the fact that NE are either dialect (region) specific or prevalent; i.e. they exist in many regions, so they are weak discriminants.", "labels": [], "entities": []}, {"text": "Moreover, we want the system to be robust and effective by learning the language variety and not heuristics about a given region.", "labels": [], "entities": []}, {"text": "The pre-processing step was done manually because of the absence of the appropriate tools.", "labels": [], "entities": []}, {"text": "To assess the reliability of the annotated data, we have conducted a human evaluation.", "labels": [], "entities": [{"text": "reliability", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.954064667224884}]}, {"text": "As a sample, we have picked up randomly 100 documents for each language from the collection, removed the labels, shuffled and put all in one file (900 unlabeled documents in total).", "labels": [], "entities": []}, {"text": "We asked two native speakers for each language, not the same ones who collected the original data, to pick out what s/he thinks is written in his/her dialect, i.e. can understand easily and can produce the same in his/her daily life.", "labels": [], "entities": []}, {"text": "All the annotators are educated, either have already finished their university or are still students.", "labels": [], "entities": []}, {"text": "This means that all of them are expected to properly distinguish between MSA and dialectal Arabic.", "labels": [], "entities": []}, {"text": "To interpret the results, we computed the inter-annotator agreement for each language to see how often the annotators agree.", "labels": [], "entities": []}, {"text": "Since we have two annotators per language, we computed the Cohen's kappa coefficient which is a standard metric used to evaluate the quality of a set of annotations in classification tasks by assessing the annotators' agreement.", "labels": [], "entities": []}, {"text": "Overall, the data quality is 'satisfactory' for Algerian, Gulf and Tunisian dialects by interpreting the kappa metric which is between 0.6-0.8.", "labels": [], "entities": []}, {"text": "The quality of the rest of the dialectal data is 'really good', kappa 0.8-1.", "labels": [], "entities": []}, {"text": "We use supervised machine learning, namely Cavnar's Text classification, support vector machines (SVM) and Prediction by Partial Matching (PPM) methods.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.6700206100940704}]}, {"text": "For features, we use both character-based ngram and word-based n-gram 16 models, then we combine them.", "labels": [], "entities": []}, {"text": "We also use the words of the compiled lexicons as features.", "labels": [], "entities": []}, {"text": "We focus more on social media short texts, so we limit the text maximum length to 140 characters (which is the maximum length of a tweet) assuming that if a method works for short texts, it should work better for longer texts as there will be access to more information.", "labels": [], "entities": []}, {"text": "We use a balanced dataset containing 18,000 documents (2,000 documents, between 60,000 and 170,000 words, for each language) where we used 80% (total of 14,400 documents or 1,600 for each language) for training and 20%, total of 3,600 documents or 131,412 words (400 documents for each language), for evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Cavnar's method performance using character 3-grams.", "labels": [], "entities": []}, {"text": " Table 3: SVM performance combining character-based 5-grams and 6-grams with lexicons.", "labels": [], "entities": []}]}