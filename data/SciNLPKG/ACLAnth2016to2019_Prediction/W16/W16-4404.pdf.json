{"title": [{"text": "Combining Lexical and Semantic-based Features for Answer Sentence Selection", "labels": [], "entities": [{"text": "Answer Sentence", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.9360205829143524}]}], "abstractContent": [{"text": "Question answering is always an attractive and challenging task in natural language processing area.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9401691257953644}]}, {"text": "There are some open domain question answering systems, such as IBM Waston, which take the unstructured text data as input, in some ways of humanlike thinking process and a mode of artificial intelligence.", "labels": [], "entities": [{"text": "question answering", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7120881974697113}, {"text": "IBM Waston", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.8523248136043549}]}, {"text": "At the conference on Natural Language Processing and Chinese Computing (NLPCC) 2016, China Computer Federation hosted a shared task evaluation about Open Domain Question Answering.", "labels": [], "entities": [{"text": "Open Domain Question Answering", "start_pos": 149, "end_pos": 179, "type": "TASK", "confidence": 0.6780724972486496}]}, {"text": "We achieve the 2nd place at the document-based subtask.", "labels": [], "entities": []}, {"text": "In this paper, we present our solution, which consists of feature engineering in lexical and semantic aspects and model training methods.", "labels": [], "entities": []}, {"text": "As the result of the evaluation shows, our solution provides a valuable and brief model which could be used in modelling question answering or sentence semantic relevance.", "labels": [], "entities": [{"text": "question answering", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.819060206413269}, {"text": "sentence semantic relevance", "start_pos": 143, "end_pos": 170, "type": "TASK", "confidence": 0.7393609980742136}]}, {"text": "We hope our solution would contribute to this vast and significant task with some heuristic thinking.", "labels": [], "entities": []}], "introductionContent": [{"text": "Selection-based question answering (QA) is a task in question answering to pick out one or several parts in a context containing an answer to an open-domain question, where the context comprises of one or more sentences.", "labels": [], "entities": [{"text": "Selection-based question answering (QA)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7870939274628957}, {"text": "question answering to pick out one or several parts in a context containing an answer to an open-domain question, where the context comprises of one or more sentences", "start_pos": 53, "end_pos": 219, "type": "Description", "confidence": 0.8155144113918831}]}, {"text": "Commonly, atypical pipeline of open-domain question answering systems is composed of three high level major steps: a) question analysis and retrieval of candidate passages; b) ranking and selecting of passages which contain the answer; and optionally c) extracting and verifying the answer.", "labels": [], "entities": [{"text": "question answering", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.6897282004356384}, {"text": "question analysis", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7459996044635773}]}, {"text": "In this paper, we pay close attention to the answer sentence selection.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.7812159657478333}]}, {"text": "Being considered as a key subtask of QA, the selection is to identify the answer-bearing sentences from all candidate sentences.", "labels": [], "entities": []}, {"text": "The selected sentences should be relevant to and answer the input questions (.", "labels": [], "entities": []}, {"text": "Several corpora have been created for these tasks like TREC-QA , WikiQA (, allowing researchers to build effective question answering systems.", "labels": [], "entities": [{"text": "TREC-QA", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.6869855523109436}, {"text": "WikiQA", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.8326654434204102}, {"text": "question answering", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7657682597637177}]}, {"text": "The nature of this task is to match not only the words but also the meaning between question and answer sentences.", "labels": [], "entities": []}, {"text": "For example, the answer to \"Where was James born ?\"is more likely to be \"He came from New York .\"than \"James was born in summer.\", even though the latter is more similar in the superficial level.", "labels": [], "entities": []}, {"text": "Further, the crisis of the task is to find the sentence most closely related to the intention of the question.", "labels": [], "entities": []}, {"text": "There have been many works towards the sentence selection task.", "labels": [], "entities": [{"text": "sentence selection task", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.8902789354324341}]}, {"text": "Basicly, those models could be divided into two categories: the lexical models and semantic-based models.", "labels": [], "entities": []}, {"text": "The relatedness between the question-answer sentence pair measured by lexical models is mostly based on some metrics such as Longest common substring (LCS), Bag-of-Words (BOW) and Word Overlap Ratio as well as: Some samples in the dataset.", "labels": [], "entities": [{"text": "Longest common substring (LCS)", "start_pos": 125, "end_pos": 155, "type": "METRIC", "confidence": 0.9272524317105612}, {"text": "Bag-of-Words (BOW)", "start_pos": 157, "end_pos": 175, "type": "METRIC", "confidence": 0.6531646251678467}]}, {"text": "The identifier \"\\t\" splits each line into 3 parts: the question, the candidate answer and the label, where 0 is incorrect while 1 is the right answer.", "labels": [], "entities": []}, {"text": "\\t \u4e2d\u6587\u540d: \u80a1\u9cde\u8713\u8725 \\t 0 \u8713\u8725\u5c5e\u5728\u54ea\u91cc\u6709\u5206\u5e03\uff1f", "labels": [], "entities": []}, {"text": "\\t \u4fd7\u540d\u522b\u540d: \\t 0 \u8713\u8725\u5c5e\u5728\u54ea\u91cc\u6709\u5206\u5e03\uff1f", "labels": [], "entities": []}, {"text": "\\t \u82f1\u6587\u540d: SouthChina forest skink \\t 0 \u8713\u8725\u5c5e\u5728\u54ea\u91cc\u6709\u5206\u5e03\uff1f", "labels": [], "entities": [{"text": "SouthChina forest skink", "start_pos": 8, "end_pos": 31, "type": "DATASET", "confidence": 0.9816699822743734}]}, {"text": "\\t \u62c9\u4e01\u5b66\u540d: Sphenomorphus incognitus \\t 0 \u8713\u8725\u5c5e\u5728\u54ea\u91cc\u6709\u5206\u5e03\uff1f", "labels": [], "entities": []}, {"text": "\\t \u5730\u7406\u5206\u5e03: \u5206\u5e03\u5728\u53f0\u6e7e\u5357\u90e8\u4e0e\u4e1c\u90e8\u3002", "labels": [], "entities": []}, {"text": "\\t 1 some complex syntactic matching degree.", "labels": [], "entities": []}, {"text": "The semantic-based models usually use some neural network framework to obtain the distributed representation between the sentences . However, both the two categories get some disadvantages.", "labels": [], "entities": []}, {"text": "The former could just capture the similarity in literal level , losing sight of the deep semantic information and latent correlation; Meanwhile, the semantic-based models often take much time to train and rely heavily on the provided data.", "labels": [], "entities": []}, {"text": "When the train dataset is insufficient or there are some unseen works in test phase, the performance is hard to guarantee.", "labels": [], "entities": []}, {"text": "To solve those problems, we present a model that emphasizes the intention analysis of the question through a feature engineering method.", "labels": [], "entities": [{"text": "intention analysis of the question", "start_pos": 64, "end_pos": 98, "type": "TASK", "confidence": 0.795515114068985}]}, {"text": "The critical part of the model is to build some efficient lexical features integrated with semantic-based methods to measure the relevance between Chinese question and the answering sentences.", "labels": [], "entities": []}, {"text": "Our contributions are three-fold: \u2022 We propose a supervised approach by combining lexical and semantic features to solve the sentence selection task in open-domain QA.", "labels": [], "entities": [{"text": "sentence selection task", "start_pos": 125, "end_pos": 148, "type": "TASK", "confidence": 0.7957124511400858}]}, {"text": "\u2022 We explore a feature named Intention Analysis Window Feature which can flexibly construct a strong semantic relation between question and answer sentences.", "labels": [], "entities": []}, {"text": "The feature is also capable of integrating kinds of external resources, which could reinforce the performance and effectiveness.", "labels": [], "entities": []}, {"text": "\u2022 An efficient Topic Word Extraction method is exploited in our model to successfully filter irrelevant information in answer sentence selection process.", "labels": [], "entities": [{"text": "Topic Word Extraction", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.7666905124982198}, {"text": "answer sentence selection", "start_pos": 119, "end_pos": 144, "type": "TASK", "confidence": 0.7552328109741211}]}, {"text": "Our model is simple, low-cost in computation and commonly adaptive to various questions.", "labels": [], "entities": []}, {"text": "As the result of the evaluation completion shows, the full model is highly efficient, outperforming almost all other models except one with external knowledge resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are totally 8772 questions in the training dataset of this task, and each of the question-answer pairs has a handcrafted label.", "labels": [], "entities": []}, {"text": "To evaluate our model, we divide the 8772 questions after shuffle into training ones and test ones with a ratio of 7:3.", "labels": [], "entities": []}, {"text": "And we made 3 pairs of this training-test dataset to evaluate our model with some cross validation method.", "labels": [], "entities": []}, {"text": "Besides the Intension Analysis Window Feature, we also build some conventional features to contrast and work together.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the training dataset. Each pair denotes a question-candidate answer pair. Average  Pairs is the average number of pairs in one question.One2One means the question only has one answer  while One2Many means at least 2 answers.", "labels": [], "entities": [{"text": "Average  Pairs", "start_pos": 98, "end_pos": 112, "type": "METRIC", "confidence": 0.9239316880702972}]}, {"text": " Table 4: The evalutaion results of some baseline method and our soulutions. ACC means the binary  classification accuracy.", "labels": [], "entities": [{"text": "ACC", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9951795339584351}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9567230343818665}]}, {"text": " Table 5. We find that the parameter tuning is an important process to affect the final metric.  However, the parameters with a reasonable range are easy to find after a few attempts. Parameters with  reasonable range could almost reach limit of the features and plenty of fine tuning could at most affect  the MAP or MRR by only 1.5%. That is to say, parameter tuning should not be regarded as the key point  of the system and the features themselves are the critical factor.", "labels": [], "entities": [{"text": "MAP", "start_pos": 311, "end_pos": 314, "type": "METRIC", "confidence": 0.9001248478889465}, {"text": "MRR", "start_pos": 318, "end_pos": 321, "type": "METRIC", "confidence": 0.879176139831543}, {"text": "parameter tuning", "start_pos": 352, "end_pos": 368, "type": "TASK", "confidence": 0.7874132692813873}]}]}