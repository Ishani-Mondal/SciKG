{"title": [{"text": "Neural Attention Model for Classification of Sentences that Support Promoting/Suppressing Relationship", "labels": [], "entities": [{"text": "Neural Attention", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7309485375881195}, {"text": "Classification of Sentences", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.8779585957527161}, {"text": "Promoting/Suppressing Relationship", "start_pos": 68, "end_pos": 102, "type": "TASK", "confidence": 0.6487731263041496}]}], "abstractContent": [{"text": "Evidences that support a claim \"a subject phrase promotes or suppresses a value\" help in making a rational decision.", "labels": [], "entities": []}, {"text": "We aim to construct a model that can classify if a particular evidence supports a claim of a promoting/suppressing relationship given an arbitrary subject-value pair.", "labels": [], "entities": []}, {"text": "In this paper , we propose a recurrent neural network (RNN) with an attention model to classify such evidences.", "labels": [], "entities": []}, {"text": "We incorporated a word embedding technique in an attention model such that our method generalizes for never-encountered subjects and value phrases.", "labels": [], "entities": []}, {"text": "Benchmarks showed that the method outperforms conventional methods in evidence classification tasks.", "labels": [], "entities": [{"text": "evidence classification tasks", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.8827334841092428}]}], "introductionContent": [{"text": "With recent trend of big data and electronic records, it is getting increasingly important to collect evidences that support a claim, which usually comes along with a decision, for rational decision making.", "labels": [], "entities": [{"text": "rational decision making", "start_pos": 181, "end_pos": 205, "type": "TASK", "confidence": 0.6649133364359537}]}, {"text": "Argument mining can be utilized for this purpose because an argument itself is an opinion of the author that supports the claim, and an argument usually consists of evidences that support the claim.", "labels": [], "entities": [{"text": "Argument mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7871178984642029}]}, {"text": "Identification of a claim has been rigorously studied in argument mining including extraction of arguments () and classification of claims (.", "labels": [], "entities": [{"text": "Identification of a claim", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9144067317247391}, {"text": "argument mining", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.8044742345809937}, {"text": "classification of claims", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.8604133327802023}]}, {"text": "Our goal is to achieve classification of positive and negative effects of a subject in a form \"a subject phrase S promotes/suppresses a value V.\"", "labels": [], "entities": []}, {"text": "For example, given a subject S = gambling, a value V = crime and a text X = casino increases theft, we can say that X supports a claim of gambling (S) promotes crime (V) relationship.", "labels": [], "entities": []}, {"text": "Such a technique is important because it allows extracting both sides of an opinion to be used in decision makings (.", "labels": [], "entities": [{"text": "decision makings", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.8889456689357758}]}, {"text": "We take a deep learning approach for this evidence classification, which has started to outperform conventional methods in many linguistic tasks.", "labels": [], "entities": [{"text": "evidence classification", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7269648611545563}]}, {"text": "Our work is based on a neural attention model, which had promising result in a translation task ( and in a sentiment classification task (.", "labels": [], "entities": [{"text": "translation task", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.9019760191440582}, {"text": "sentiment classification task", "start_pos": 107, "end_pos": 136, "type": "TASK", "confidence": 0.8548179070154825}]}, {"text": "The neural attention model achieved these by focusing on important phrases; e.g. when V is economy and X is Gambling boosts the city's revenue., the attention layer focuses near the phrase boosts the city's revenue.", "labels": [], "entities": []}, {"text": "The neural attention model was previously applied to aspect-based sentiment analysis (ABSA) (, which has some similarity to the evidence classification in that it classifies sentimental polarities towards a subject S given an aspect (corresponding to V) (.", "labels": [], "entities": [{"text": "aspect-based sentiment analysis (ABSA)", "start_pos": 53, "end_pos": 91, "type": "TASK", "confidence": 0.8160670797030131}]}, {"text": "A limitation of was that the learned attention layer is tightly attached to each S or V and does not generalize for neverencountered subjects/values.", "labels": [], "entities": []}, {"text": "This means that it requires manually labeled data for all possible subjects and values, which is not practicable.", "labels": [], "entities": []}, {"text": "Instead, when we train a model to classify an evidence that supports a claim of a relationship between, for example, gambling and crime, we want the same learned model to work for other Sand V pairs such as smoking and health.", "labels": [], "entities": []}, {"text": "In other words, we want the model to learn how to classify evidences that support a relationship of Sand V, rather than learning the relationship itself.", "labels": [], "entities": []}, {"text": "In this paper, we propose a neural attention  model that can learn to focus on important phrases from text even when Sand V are never encountered, allowing the neural attention model to be applied to the evidence classification.", "labels": [], "entities": []}, {"text": "We extend the neural attention model by modeling the attention layer using a distributed representation of words in which similar words are treated in a similar manner.", "labels": [], "entities": []}, {"text": "We also report benchmarks of the method against previous works in both neural and lexiconbased approaches.", "labels": [], "entities": []}, {"text": "We show that the method can effectively generalize to an evidence classification task with never-encountered phrases.", "labels": [], "entities": [{"text": "evidence classification task", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.8380251924196879}]}], "datasetContent": [{"text": "The purpose of this experiment was to test if the proposed RNN with word embedding-based attention model could perform well in a evidence classification task.", "labels": [], "entities": [{"text": "evidence classification task", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.8713791569073995}]}, {"text": "We benchmarked our method to the RNN without an attention model and conventional lexicon-based classification methods.", "labels": [], "entities": [{"text": "RNN", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7815852761268616}]}, {"text": "We chose seven subject phrases and one or two value phrases for each subject phrase (total of 13 pairs) as shown in.", "labels": [], "entities": []}, {"text": "For each pair of Sand V, we extracted sentences having both Sand V within two adjacent sentences from Annotated . From candidates of 7000 sentences, we manually extracted and labeled 1,085 self-contained sentences that support promoting/suppressing relationship.", "labels": [], "entities": []}, {"text": "We allowed sentences in which S, V did not appear.", "labels": [], "entities": []}, {"text": "We chose five subject phrases as training data and other two as test data.", "labels": [], "entities": []}, {"text": "Notice that only a fraction of the test data had overlapping value phrases with the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Hyperparameters of BiRNN and  BiRNN+ATT (our method)", "labels": [], "entities": [{"text": "ATT", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.6888510584831238}]}, {"text": " Table 3: Performance of the classifiers. The best  result for each metric is shown bold.", "labels": [], "entities": []}]}