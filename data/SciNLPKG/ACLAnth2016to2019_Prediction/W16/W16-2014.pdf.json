{"title": [{"text": "Read my points: Effect of animation type when speech-reading from EMA data", "labels": [], "entities": []}], "abstractContent": [{"text": "Three popular vocal-tract animation paradigms were tested for intelligibility when displaying videos of pre-recorded Electromagnetic Articulography (EMA) data in an online experiment.", "labels": [], "entities": []}, {"text": "EMA tracks the position of sensors attached to the tongue.", "labels": [], "entities": []}, {"text": "The conditions were dots with tails (where only the coil location is presented), 2D animation (where the dots are connected to form 2D representations of the lips, tongue surface and chin), and a 3D model with coil locations driving facial and tongue rigs.", "labels": [], "entities": []}, {"text": "The 2D animation (recorded in VisArtico) showed the highest identification of the prompts.", "labels": [], "entities": [{"text": "VisArtico", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.890333354473114}]}], "introductionContent": [{"text": "Electromagnetic Articulography (EMA) is a popular vocal-tract motion capture technique used increasingly for second language learning and speech therapy purposes.", "labels": [], "entities": []}, {"text": "In this situation, an instructor aids the subject to reach a targeted vocal tract configuration by showing them a live augmented visualization of the trajectories of (some of) the subject's articulators, alongside a targeted configuration.", "labels": [], "entities": []}, {"text": "Current research into how subjects respond to this training uses a variety of different visualizations:  and used a 'mouse-controlled drawing tool' to indicate target areas as circles on the screen, with the former displaying an 'image of [the] current tongue position', the latter displaying a 'tongue trace'.", "labels": [], "entities": []}, {"text": "displayed a mid-sagittal representation of the tongue surface as a spline between three sensors along the tongue, as well as showing a palate trace and lip coil positions and targets as circles.", "labels": [], "entities": []}, {"text": "used a 3D avatar with a transparent face mesh, pink tongue rig, including colored shapes that lit when touched as targets.", "labels": [], "entities": []}, {"text": "For audiovisual feedback scenarios the optimal manner of presenting the stimuli has not yet been explicitly studied, but rather the experiments have reflected recent software developments.", "labels": [], "entities": []}, {"text": "Meanwhile, different tools) have emerged as state of the art software for offline processing and visualization.", "labels": [], "entities": []}, {"text": "The claim that subjects make gains in tongue gesture awareness only after a practice period with the visualization (Ouni, 2011) underlies the need for research into how EMA visualizations can best be presented to subjects in speech therapy or L2-learning settings.", "labels": [], "entities": [{"text": "tongue gesture awareness", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6756824254989624}]}, {"text": "The main inspiration for this work is the finding of that showing normallyobscured articulators (as opposed to a full face, with and without the tongue) has a positive effect on the identification of VCV stimuli.", "labels": [], "entities": [{"text": "identification of VCV stimuli", "start_pos": 182, "end_pos": 211, "type": "TASK", "confidence": 0.8470442444086075}]}, {"text": "An established body of research already focuses on quantifying the intelligibility-benefit or realism of animated talking heads, ideally as compared to a video-realistic standard ().", "labels": [], "entities": []}, {"text": "However, as the articulators that researchers/teachers wish to present to their subjects in the aforementioned scenario are generally outside the line of sight, these evaluation methods cannot be directly applied to intra-oral visualizations.", "labels": [], "entities": []}, {"text": "We aim to fill this gap by comparing commonly-used EMA visualizations to determine which is most intelligible, 1 hoping this may guide future research into the presentation of EMA data in a visual feedback setting.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Coefficient, standard error and signifi- cance and of fixed effects for the mixed model of  the identification dataset. 2D animations (SYSV)  improve identification significantly over the base- line (3D animations). Table created with texreg  (Leifeld, 2013).", "labels": [], "entities": [{"text": "Coefficient", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9583955407142639}, {"text": "standard error", "start_pos": 23, "end_pos": 37, "type": "METRIC", "confidence": 0.8958957493305206}]}, {"text": " Table 2. The 2D ani- mation was significantly better-identified than the  3D animation. The Dots animation was slightly  (but not significantly) less well-performing than  the 3D animation.  Even within the most intelligible system (2D  graphics), it is evident that there is much variabil- ity in how well participants are able to identify the  various prompts (see", "labels": [], "entities": []}, {"text": " Table 3: Identification strategy frequency by num- ber of mentions over all participants.", "labels": [], "entities": []}]}