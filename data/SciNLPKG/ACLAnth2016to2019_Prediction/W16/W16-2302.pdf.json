{"title": [{"text": "Results of the WMT16 Metrics Shared Task", "labels": [], "entities": [{"text": "WMT16 Metrics Shared Task", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.6231473982334137}]}], "abstractContent": [{"text": "This paper presents the results of the WMT16 Metrics Shared Task.", "labels": [], "entities": [{"text": "WMT16 Metrics Shared Task", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6817989945411682}]}, {"text": "We asked participants of this task to score the outputs of the MT systems involved in the WMT16 Shared Translation Task.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9116919636726379}, {"text": "WMT16 Shared Translation Task", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.7627450227737427}]}, {"text": "We collected scores of 16 metrics from 9 research groups.", "labels": [], "entities": []}, {"text": "In addition to that, we computed scores of 9 standard metrics (BLEU, SentBLEU, NIST, WER, PER, TER and CDER) as baselines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9987373948097229}, {"text": "NIST", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.47211211919784546}, {"text": "WER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9396050572395325}, {"text": "PER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.8892266154289246}, {"text": "TER", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9231101870536804}]}, {"text": "The collected scores were evaluated in terms of system-level correlation (how well each metric's scores correlate with WMT16 official manual ranking of systems) and in terms of segment level correlation (how often a metric agrees with humans in comparing two translations of a particular sentence).", "labels": [], "entities": [{"text": "WMT16", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.8360011577606201}, {"text": "segment level correlation", "start_pos": 177, "end_pos": 202, "type": "METRIC", "confidence": 0.6421990295251211}]}, {"text": "This year there are several additions to the setup: large number of language pairs (18 in total), datasets from different domains (news, IT and medical), and different kinds of judgments: relative ranking (RR), direct assessment (DA) and HUME manual semantic judgments.", "labels": [], "entities": [{"text": "HUME manual semantic judgments", "start_pos": 238, "end_pos": 268, "type": "TASK", "confidence": 0.6158538982272148}]}, {"text": "Finally, generation of large number of hybrid systems was trialed for provision of more conclusive system-level metric rankings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic evaluation of machine translation quality is essential in the development and selection of machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7885906398296356}, {"text": "machine translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7416308522224426}]}, {"text": "Many different automatic MT quality metrics are available and the Metrics Shared Task 1 is held annually at WMT to assess their quality, starting with and following up to Stanojevi\u00b4c . http://www.statmt.org/wmt16/ metrics-task/ Metrics participating in the metrics task rely on the existence of reference translations with which MT outputs are compared, and the metrics task itself then needs manual judgments of translation quality in order to check the extent to which the automatic metrics can approximate the judgment.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9775086641311646}, {"text": "WMT", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.9343138337135315}, {"text": "MT outputs", "start_pos": 329, "end_pos": 339, "type": "TASK", "confidence": 0.8927239179611206}]}, {"text": "A related WMT task on quality estimation assesses the performance of methods where no reference translations are needed, requiring only the manual quality judgments (.", "labels": [], "entities": [{"text": "WMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9116837382316589}, {"text": "quality estimation", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.5765893757343292}]}, {"text": "This year, we keep the two main types of metric evaluation: system-level, where a metric is expected to provide a quality score for the whole translated document, and segment-level, where the score is needed for every individual sentence.", "labels": [], "entities": []}, {"text": "We experiment with several novelties.", "labels": [], "entities": []}, {"text": "Specifically, test sets this year come from three domains: news, IT and medical/health-related texts.", "labels": [], "entities": []}, {"text": "The added domains bring in an extended set of languages.", "labels": [], "entities": []}, {"text": "In sum, the metrics task this year includes 18 language pairs, English paired with Basque, Bulgarian, Czech, Dutch, Finnish, German, Polish, Portuguese, Romanian, Russian, Spanish, and Turkish, in one or both directions.", "labels": [], "entities": []}, {"text": "On the evaluation side, we rely on three golden truths of manual judgment: \u2022 Relative Ranking (RR) of up to 5 different translation candidates at a time, as collected in WMT in the past, \u2022 Direct Assessment (DA) evaluating the adequacy of a translation candidate on an absolute scale in isolation from other translations, \u2022 HUME, a composite segment-level score aggregated over manual judgments of translation quality of semantic units of the source sentence.", "labels": [], "entities": [{"text": "Relative Ranking (RR)", "start_pos": 77, "end_pos": 98, "type": "METRIC", "confidence": 0.9698062777519226}, {"text": "HUME", "start_pos": 324, "end_pos": 328, "type": "METRIC", "confidence": 0.9395026564598083}]}, {"text": "Additional changes to the task evaluation include a change in the way we compute confidence: Overview of \"tracks\" of the WMT16 metrics task.", "labels": [], "entities": [{"text": "WMT16 metrics task", "start_pos": 121, "end_pos": 139, "type": "DATASET", "confidence": 0.6846430500348409}]}, {"text": "\"\u2022\" indicates language pairs covered in the evaluation, \"\u00b7\" are language pairs planned but abandoned due to difficulties in obtaining human judgments.", "labels": [], "entities": []}, {"text": "intervals for metric correlations with human assessment, resulting in more reliable conclusions as to which metrics outperform others.", "labels": [], "entities": []}, {"text": "The official method of evaluation remains unchanged, relying on RR in both the system-level (TrueSkill) and segment-level (Kendall's \u03c4 ) metrics, see below for details and references.", "labels": [], "entities": [{"text": "RR", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9353176951408386}]}, {"text": "Our datasets are described in Section 2.", "labels": [], "entities": []}, {"text": "This includes the test sets, system outputs, human judgments of translation quality as well as participating metrics across the tasks.", "labels": [], "entities": []}, {"text": "Results of system-level metric evaluation are provided in Section 3.1 and Section 3.2, the results of the segment-level evaluation are provided in Section 3.3.", "labels": [], "entities": []}, {"text": "provides the complete picture of the golden truths, test sets, translation systems and language pairs involved in the metrics task this year.", "labels": [], "entities": []}, {"text": "For simplicity, we called each of these setups a \"track\", indicating the underlying type of golden truth (RR/DA/HUME), system-or segment-level evaluation (sys/seg) and the particular test set.", "labels": [], "entities": [{"text": "golden truth (RR/DA/HUME)", "start_pos": 92, "end_pos": 117, "type": "METRIC", "confidence": 0.7291506859991286}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 6: Absolute Pearson correlation of cs-en and  en-cs system-level metric scores with human as- sessment variant RR + TT, i.e. standard WMT  relative ranking including tuning task systems.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.780206541220347}, {"text": "RR + TT", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.680487871170044}, {"text": "WMT", "start_pos": 141, "end_pos": 144, "type": "TASK", "confidence": 0.885067343711853}]}, {"text": " Table 4: Absolute Pearson correlation of to-English system-level metric scores with human assessment  variants: RR = standard WMT relative ranking; DA = direct assessment of translation adequacy.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8483264446258545}, {"text": "RR", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.9908292293548584}, {"text": "DA", "start_pos": 149, "end_pos": 151, "type": "METRIC", "confidence": 0.973168134689331}]}, {"text": " Table 7: Absolute Pearson correlation of system-level metric scores with 10K hybrid systems: DA Hy- brid = direct assessment of translation adequacy of 10K hybrid MT systems.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.821085532506307}, {"text": "DA Hy- brid", "start_pos": 94, "end_pos": 105, "type": "METRIC", "confidence": 0.9354550689458847}]}, {"text": " Table 8: System-level metric results (ittest2016): Pearson correlation of system-level metric scores with  human assessment computed over standard WMT relative ranking (RR) human assessments; absolute  values of correlation coefficients reported for all metrics.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 52, "end_pos": 71, "type": "METRIC", "confidence": 0.8555004298686981}, {"text": "WMT relative ranking (RR) human", "start_pos": 148, "end_pos": 179, "type": "TASK", "confidence": 0.6274821417672294}]}, {"text": " Table 9: Segment-level metric results for to-English language pairs (newstest2016): Correlation of  segment-level metric scores with human assessment variants, where \u03c4 are official results computed sim- ilar to Kendall's \u03c4 and over standard WMT relative ranking (RR) human assessments; r are Pearson  correlation coefficients of metric scores with direct assessment (DA) of absolute translation adequacy;  absolute value of correlation coefficients reported for all metrics.", "labels": [], "entities": [{"text": "WMT relative ranking (RR) human", "start_pos": 242, "end_pos": 273, "type": "TASK", "confidence": 0.6909477284976414}]}, {"text": " Table 11: Pearson correlation of segment-level metric scores with HUME human assessment variant.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9715079665184021}]}]}