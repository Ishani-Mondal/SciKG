{"title": [{"text": "Results of the 4 th edition of BioASQ Challenge", "labels": [], "entities": [{"text": "BioASQ Challenge", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.6341052651405334}]}], "abstractContent": [{"text": "The goal of this task is to push the research frontier towards hybrid information systems.", "labels": [], "entities": []}, {"text": "We aim to promote systems and approaches that are able to deal with the whole diversity of the Web, especially for, but not restricted to, the context of bio-medicine.", "labels": [], "entities": []}, {"text": "This goal is pursued by the organization of challenges.", "labels": [], "entities": []}, {"text": "The fourth challenge, as the previous challenges, consisted of two tasks: semantic indexing and question answering.", "labels": [], "entities": [{"text": "semantic indexing", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7921435236930847}, {"text": "question answering", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8698057532310486}]}, {"text": "16 systems participated by 7 different participating teams for the semantic indexing task.", "labels": [], "entities": [{"text": "semantic indexing task", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.8960774739583334}]}, {"text": "The question answering task was tackled by 37 different systems, developed by 11 different teams.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.9079716602961222}]}, {"text": "25 of the systems participated in the phase A of the task, while 12 participated in phase B.", "labels": [], "entities": []}, {"text": "3 of the teams participated in both phases of the question answering task.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.9096739093462626}]}, {"text": "Overall, as in previous years, the best systems were able to out-perform the strong baselines.", "labels": [], "entities": []}, {"text": "This suggests that advances over the state of the art were achieved through the BIOASQ challenge but also that the benchmark in itself is very challenging.", "labels": [], "entities": [{"text": "BIOASQ", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.665377676486969}]}, {"text": "In this paper, we present the data used during the challenge as well as the technologies which were at the core of the participants' frameworks.", "labels": [], "entities": []}], "introductionContent": [{"text": "The aim of this paper is twofold.", "labels": [], "entities": []}, {"text": "First, we aim to give an overview of the data issued during the BioASQ challenge in 2016.", "labels": [], "entities": [{"text": "BioASQ challenge in 2016", "start_pos": 64, "end_pos": 88, "type": "DATASET", "confidence": 0.7954567819833755}]}, {"text": "In addition, we aim to present the systems that participated in the challenge and for which we received system descriptions, as well as evaluate their performance.", "labels": [], "entities": []}, {"text": "To achieve these goals, we begin by giving a brief overview of the tasks, including the timing of the different tasks and the challenge data.", "labels": [], "entities": []}, {"text": "Thereafter, we give an overview of the systems which participated in the challenge and provided us with an overview of the technologies they relied upon.", "labels": [], "entities": []}, {"text": "Detailed descriptions of some of the systems are given in lab proceedings.", "labels": [], "entities": []}, {"text": "The evaluation of the systems, which was carried out by using state-ofthe-art measures or manual assessment, is the last focal point of this paper.", "labels": [], "entities": []}, {"text": "The conclusion sums up the results of this challenge.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Statistics on the training and test datasets  of Task 4b. All the numbers for the documents,  snippets, concepts and triples refer to averages.", "labels": [], "entities": []}, {"text": " Table 1: Statistics on the test datasets of Task 4a.", "labels": [], "entities": []}, {"text": " Table 2: Correspondence of reference and submitted systems for Task 4a.", "labels": [], "entities": []}, {"text": " Table 5: Results for batch 1 for documents in phase A of Task 4b.", "labels": [], "entities": []}, {"text": " Table 6: Results for batch 1 for snippets in phase A of Task 4b.", "labels": [], "entities": []}, {"text": " Table 7: Results for batch 3 for exact answers in phase B of Task 4b.", "labels": [], "entities": []}]}