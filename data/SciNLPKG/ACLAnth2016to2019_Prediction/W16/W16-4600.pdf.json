{"title": [], "abstractContent": [{"text": "Recently Neural Machine Translation (NMT) systems are reported to outperform other approaches in machine translation.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.7667433321475983}, {"text": "machine translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7488196492195129}]}, {"text": "However, NMT systems are known to be computationally expensive both in training and in translation inference -sometimes prohibitively so in the case of very large data sets and large models.", "labels": [], "entities": [{"text": "translation inference", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.9776070713996887}]}, {"text": "Several authors have also charged that NMT systems lack robustness, particularly when input sentences contain rare words.", "labels": [], "entities": []}, {"text": "These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential.", "labels": [], "entities": [{"text": "NMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.7874358296394348}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9992238283157349}, {"text": "speed", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.9764713048934937}]}, {"text": "In this talk, I present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6242122054100037}]}, {"text": "Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using residual connections as well as attention connections from the decoder network to the encoder.", "labels": [], "entities": []}, {"text": "To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder.", "labels": [], "entities": []}, {"text": "To accelerate the final translation speed, we employ low-precision arithmetic during inference computations.", "labels": [], "entities": []}, {"text": "To improve handling of rare words, we divide words into a limited set of common subword units (\"wordpieces\") for both input and output.", "labels": [], "entities": []}, {"text": "On the WMT'14 English-to-French and Englishto-German benchmarks, GNMT achieves competitive results to state-of-the-art.", "labels": [], "entities": [{"text": "WMT'14 English-to-French and Englishto-German benchmarks", "start_pos": 7, "end_pos": 63, "type": "DATASET", "confidence": 0.7014857411384583}, {"text": "GNMT", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.7431410551071167}]}, {"text": "Using a human sideby-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60phrase-based production system.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}