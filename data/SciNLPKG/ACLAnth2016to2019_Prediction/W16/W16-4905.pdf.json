{"title": [{"text": "A Comparison of Word Embeddings for English and Cross-Lingual Chinese Word Sense Disambiguation", "labels": [], "entities": [{"text": "Cross-Lingual Chinese Word Sense Disambiguation", "start_pos": 48, "end_pos": 95, "type": "TASK", "confidence": 0.6772625803947449}]}], "abstractContent": [{"text": "Word embeddings are now ubiquitous forms of word representation in natural language processing.", "labels": [], "entities": [{"text": "word representation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7389668822288513}]}, {"text": "There have been applications of word embeddings for monolingual word sense disambigua-tion (WSD) in English, but few comparisons have been done.", "labels": [], "entities": [{"text": "monolingual word sense disambigua-tion (WSD)", "start_pos": 52, "end_pos": 96, "type": "TASK", "confidence": 0.7350994220801762}]}, {"text": "This paper attempts to bridge that gap by examining popular embeddings for the task of monolingual English WSD.", "labels": [], "entities": [{"text": "monolingual English WSD", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.48898465434710187}]}, {"text": "Our simplified method leads to comparable state-of-the-art performance without expensive retraining.", "labels": [], "entities": []}, {"text": "Cross-Lingual WSD-where the word senses of a word in a source language e come from a separate target translation language f-can also assist in language learning; for example, when providing translations of target vocabulary for learners.", "labels": [], "entities": [{"text": "WSD-where the word senses of a word in a source language e", "start_pos": 14, "end_pos": 72, "type": "TASK", "confidence": 0.7396975407997767}]}, {"text": "Thus we have also applied word embed-dings to the novel task of cross-lingual WSD for Chinese and provide a public dataset for further benchmarking.", "labels": [], "entities": []}, {"text": "We have also experimented with using word embeddings for LSTM networks and found surprisingly that a basic LSTM network does notwork well.", "labels": [], "entities": []}, {"text": "We discuss the ramifications of this outcome.", "labels": [], "entities": []}], "introductionContent": [{"text": "A word takes on different meanings, largely dependent on the context in which it is used.", "labels": [], "entities": []}, {"text": "For example, the word \"bank\" could mean \"slope beside a body of water\", or a \"depository financial institution\" . Word Sense Disambiguation (WSD) is the task of identifying the contextually appropriate meaning of the word.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 114, "end_pos": 145, "type": "TASK", "confidence": 0.7847212900718054}]}, {"text": "WSD is often considered a classification task, in which the classifier predicts the sense from a possible set of senses, known as a sense inventory, given the target word and the contextual information of the target word.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9568938612937927}]}, {"text": "Existing WSD systems can be categorised into either data-driven supervised or knowledge-rich approaches.", "labels": [], "entities": []}, {"text": "Both approaches are considered to be complementary to each other.", "labels": [], "entities": []}, {"text": "Word embeddings have become a popular word representation formalism, and many tasks can be done using word embeddings.", "labels": [], "entities": [{"text": "word representation formalism", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.7592431008815765}]}, {"text": "The effectiveness of using word embeddings has been shown in several NLP tasks.", "labels": [], "entities": []}, {"text": "The goal of our work is to apply and comprehensively compare different uses of word embeddings, solely with respect to WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.7178932428359985}]}, {"text": "We perform evaluation of the effectiveness of word embeddings on monolingual WSD tasks from Senseval-2 (held in 2001),, and SemEval-2007.", "labels": [], "entities": [{"text": "WSD tasks", "start_pos": 77, "end_pos": 86, "type": "TASK", "confidence": 0.8926721811294556}]}, {"text": "After which, we evaluate our approach on English-Chinese Cross-Lingual WSD using a dataset that we constructed for evaluating our approach on the translation task used in educational applications for language learning.", "labels": [], "entities": [{"text": "translation task", "start_pos": 146, "end_pos": 162, "type": "TASK", "confidence": 0.8974952697753906}]}], "datasetContent": [{"text": "As far as we know, there is no existing publicly available English-Chinese Cross-Lingual WSD dataset.", "labels": [], "entities": [{"text": "English-Chinese Cross-Lingual WSD dataset", "start_pos": 59, "end_pos": 100, "type": "DATASET", "confidence": 0.5605088025331497}]}, {"text": "To evaluate our proposal, therefore, we hired human annotators to construct such an evaluation dataset using sentences from recent news articles.", "labels": [], "entities": []}, {"text": "As the dataset is constructed using recent news data, it is a   To obtain the gold standard for this data set, we hired 18 annotators to select the right translations fora given word and its context.", "labels": [], "entities": []}, {"text": "There are 697 instances in total in our dataset, with a total of 251 target words to disambiguate, that were each multiply-annotated by 3 different annotators.", "labels": [], "entities": []}, {"text": "Each annotator disambiguated 110+ instances (15 annotators with 116 instances, 3 with 117) in hard-copy.", "labels": [], "entities": []}, {"text": "The annotators are all bilingual undergraduate students, who are native Chinese speakers.", "labels": [], "entities": []}, {"text": "For each instance, which contains a single English target word to disambiguate, we include the sentence it appears in and its adjacent sentences as its context.", "labels": [], "entities": []}, {"text": "Each instance contains possible translations of the word.", "labels": [], "entities": []}, {"text": "The annotators selected all Chinese words that had an identical meaning to the English target word.", "labels": [], "entities": []}, {"text": "If the word cannot be appropriately translated, we instructed annotators to leave the annotation blank.", "labels": [], "entities": []}, {"text": "The annotators provided their own translations if they believe that there is a suitable translation, but which was not provided by the crawled dictionary.", "labels": [], "entities": [{"text": "crawled dictionary", "start_pos": 135, "end_pos": 153, "type": "DATASET", "confidence": 0.8926356136798859}]}, {"text": "The concept of a sense is a human construct, and therefore, as earlier elaborated on when discussing sense granularity, it is maybe difficult for human annotators to agree on the correct answer.", "labels": [], "entities": []}, {"text": "Our annotation task differs from the usual since we allow users to select multiple labels and can also add new labels to each case if they do not agree with any label provided.", "labels": [], "entities": []}, {"text": "As such, applying the Cohen's Kappa as it is for measuring the inter-annotator agreement as it is does notwork for our annotated dataset.", "labels": [], "entities": []}, {"text": "We are also unable to compute the probably of chance agreement byword, since there are few test instances per word in our dataset.", "labels": [], "entities": [{"text": "probably", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9676141738891602}]}, {"text": "The Kappa equation is given as \u03ba = p A \u2212p E 1\u2212p E . To compute p A for \u03ba, we use a simplified, optimistic approach where we select one annotated label out of possibly multiple selected labels for each annotator.", "labels": [], "entities": []}, {"text": "We always choose the label that results in an agreement between the pair, if such a label exist.", "labels": [], "entities": []}, {"text": "For p E (the probability of chance agreement), as the labels of each case are different, we consider the labels in terms of how frequent they occur in the training data.", "labels": [], "entities": []}, {"text": "We only consider the top 3 most frequent senses for each word due to the skewness of the sense distribution.", "labels": [], "entities": []}, {"text": "We first compute the probability of an annotator selecting each of the top three frequent senses, p E is then equals to the sum of the probability that both annotators selected one of the three top senses by chance.", "labels": [], "entities": []}, {"text": "The pairwise value by this proposed method of \u03ba is obtained is 0.42.", "labels": [], "entities": []}, {"text": "We interpreted this as a moderate level of agreement.", "labels": [], "entities": []}, {"text": "We note that there is a large number of possible labels for each case, which is known to affect the value of \u03ba negatively.", "labels": [], "entities": []}, {"text": "This is exacerbated as we allow the annotators to add new labels.", "labels": [], "entities": []}, {"text": "In this annotation task, as we consider the possible translations as fine-grained, the value of agreement is likely to be underestimated in this case.", "labels": [], "entities": [{"text": "agreement", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9817758202552795}]}, {"text": "Hence, we believe that clustering of similar translations during annotation is required in order to deal with the issue of sense granularity in Cross-Lingual WSD.", "labels": [], "entities": [{"text": "Cross-Lingual WSD", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.4515027403831482}]}, {"text": "To overcome this problem, we used different configurations of granularity during evaluation of our system.", "labels": [], "entities": []}, {"text": "For all configurations, we remove instances from the dataset if it does not have a correct sense.", "labels": [], "entities": []}, {"text": "We also noticed that some target words were part of a proper noun, such as the word 'white' in 'White House'.", "labels": [], "entities": []}, {"text": "This led to some confusion among annotators, so we omitted instances where the target word is part of a proper noun.", "labels": [], "entities": []}, {"text": "Statistics of the test dataset after filtering out different cases are given in.", "labels": [], "entities": []}, {"text": "As previously described, IMS is a supervised system requiring training data before use.", "labels": [], "entities": [{"text": "IMS", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9558579921722412}]}, {"text": "We constructed data by processing a parallel corpus, the news section of the UM-Corpus (, and performing word alignment.", "labels": [], "entities": [{"text": "UM-Corpus", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.9134743213653564}, {"text": "word alignment", "start_pos": 105, "end_pos": 119, "type": "TASK", "confidence": 0.7434114217758179}]}, {"text": "We used the dictionary provided by) as the sense inventory, which we further expanded using translations from Bing Translator and Google Translate.", "labels": [], "entities": []}, {"text": "For construction of the training dataset, word alignment is used to assign Chinese words as training labels for each English target word.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.7091728746891022}]}, {"text": "GIZA++) is used for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.8292132318019867}]}, {"text": "To evaluate our system, we compare the results of the method described in, which uses Bing Translator and word alignment to obtain translations.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7185614705085754}]}, {"text": "We use the configuration where every annotation is considered to be correct for our main evaluation since this is closer to a coarse-grained evaluation.", "labels": [], "entities": []}, {"text": "It can be seen that word embeddings improves the performance on Cross-Lingual WSD.", "labels": [], "entities": [{"text": "Cross-Lingual WSD", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.49578598141670227}]}, {"text": "Similar to our observations for monolingual WSD, the use of both CBOW and GLoVe improved performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9090460538864136}]}, {"text": "However, the improvements from the word embeddings feature type over IMS was not statistically significant at 95% confidence level.", "labels": [], "entities": []}, {"text": "This is attributed to the small size of the dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. We evaluate the effect of varying the scaling factor with the feature of the sum of the  surrounding word vectors, and find that the summation feature works optimally with 50 dimensions.", "labels": [], "entities": []}, {"text": " Table 1: Effects on accuracy when varying scaling factor on C&W embeddings", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9993232488632202}]}, {"text": " Table 2: Comparison of systems by their accuracy score on both Lexical Sample and All Words tasks.  Rank 1 system refers to the top ranked system in the respective shared tasks.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 41, "end_pos": 55, "type": "METRIC", "confidence": 0.9779072701931}]}, {"text": " Table 3: Accuracy of adding word embeddings to IMS, with different parameters, on SE-2, SE-3 LS and  AW tasks and SE-2007 AW task  Type  Size Compose Scaling SE-2 SE-3 SE-2 SE-3 SE-2007 SE-2007  LS  LS  AW  AW  Fine-Coarse- grained grained", "labels": [], "entities": [{"text": "SE-2007 AW task  Type  Size Compose Scaling SE-2 SE-3 SE-2 SE-3 SE-2007 SE-2007  LS  LS  AW  AW", "start_pos": 115, "end_pos": 210, "type": "TASK", "confidence": 0.6900017559528351}]}, {"text": " Table 4: Accuracy of a basic LSTM approach on the Lexical Sample and All Words tasks.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9805629253387451}]}, {"text": " Table 5: Statistics of our new annotated Chinese-English crosslingual WSD dataset. Out-of-vocabulary  (OOV) annotations refer to annotations added by the annotators", "labels": [], "entities": [{"text": "Chinese-English crosslingual WSD dataset", "start_pos": 42, "end_pos": 82, "type": "DATASET", "confidence": 0.6118013709783554}]}, {"text": " Table 6: Results of our systems on the Cross-Lingual WSD dataset, excluding named entities. Instances  with out-of-vocabulary annotations are removed. All annotations are considered correct answers.", "labels": [], "entities": [{"text": "Cross-Lingual WSD dataset", "start_pos": 40, "end_pos": 65, "type": "DATASET", "confidence": 0.6109103759129842}]}]}