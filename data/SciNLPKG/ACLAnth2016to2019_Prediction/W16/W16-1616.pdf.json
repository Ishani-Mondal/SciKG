{"title": [{"text": "Learning Semantic Relatedness in Community Question Answering Using Neural Models", "labels": [], "entities": [{"text": "Learning Semantic Relatedness in Community Question Answering", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.5758054980209896}]}], "abstractContent": [{"text": "Community Question Answering forums, such as Quora and Stackoverflow contain millions of questions and answers.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6851983815431595}]}, {"text": "Automatically finding the relevant questions from the existing questions and finding the relevant answers to anew question are Natural Language Processing tasks.", "labels": [], "entities": []}, {"text": "In this paper, we aim to address these tasks, which we refer to as similar-Question Retrieval and Answer Selection.", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.9135383069515228}]}, {"text": "We present a neural-based model with stacked bidirec-tional LSTMs and MLP to address these tasks.", "labels": [], "entities": []}, {"text": "The model generates the vector representations of the question-question or question-answer pairs and computes their semantic similarity scores, which are then employed to rank and predict relevancies.", "labels": [], "entities": []}, {"text": "Extensive experiments demonstrate our results outperform the baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Community Question Answering (cQA) websites such as Quora and Stackoverflow 2 are rapidly expanding.", "labels": [], "entities": [{"text": "Community Question Answering (cQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7196746120850245}]}, {"text": "Managing such platforms has become increasingly difficult because of the exponential growth in content, triggered by wider access to the internet.", "labels": [], "entities": []}, {"text": "Traditionally, websites used to keep track of a list of frequently asked questions (FAQ) that they expect visitors to consult before asking a question.", "labels": [], "entities": [{"text": "track of a list of frequently asked questions (FAQ)", "start_pos": 37, "end_pos": 88, "type": "TASK", "confidence": 0.6662525317885659}]}, {"text": "Now, with a wider range of questions being asked, a need has emerged fora better and more scalable system to automatically identify similarities between any two questions on the platform.", "labels": [], "entities": []}, {"text": "In addition, with many users contributing to a single question, it has become harder to identify which answers are more relevant than others.", "labels": [], "entities": []}, {"text": "We summarize these two problems as follows: \u2022 Question Retrieval: given anew question and a list of questions, we automatically rank the questions in the list according to their relevancy to the new question.", "labels": [], "entities": [{"text": "Question Retrieval", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7259600907564163}]}, {"text": "\u2022 Answer Selection: given a cQA thread containing a question and a list of answers, we automatically rank the answers according to their relevance to the question.", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 2, "end_pos": 18, "type": "TASK", "confidence": 0.8442046642303467}]}, {"text": "The increase in the number of communitybased Q&A platforms has lead to a rapid buildup of large archives of user-generated questions and answers.", "labels": [], "entities": []}, {"text": "When anew question is asked on the platform, the system searches for questions that are semantically similar in the archives.", "labels": [], "entities": []}, {"text": "If a similar question is found, the corresponding correct answer is retrieved and returned immediately to the user as the final answer.", "labels": [], "entities": []}, {"text": "The quality of the answer depends on the effectiveness of the question-similarity calculation.", "labels": [], "entities": []}, {"text": "However, measuring semantic relatedness between questions and answers is not trivial.", "labels": [], "entities": [{"text": "measuring semantic relatedness between questions and answers", "start_pos": 9, "end_pos": 69, "type": "TASK", "confidence": 0.742864055292947}]}, {"text": "Sometimes, similar questions or relevant answers use very different wording.", "labels": [], "entities": []}, {"text": "For instance, the two questions \"Is downloading movies illegal?\" and \"Can I share a copy of a DVD online\" have an almost identical meaning but are lexically very different.", "labels": [], "entities": []}, {"text": "Traditional text-based similarity metrics for measuring sentence distance such as the Jaccard coefficient and the overlap coefficient), perform poorly.", "labels": [], "entities": [{"text": "overlap coefficient", "start_pos": 114, "end_pos": 133, "type": "METRIC", "confidence": 0.9741897284984589}]}, {"text": "In this paper, we present a neural-based model including stacked Bidirectional Long Short-Term Memory (BLSTM) networks and Multi-Layer Perceptron (MLP) to address the question retrieval and answer selection problems.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.7878976166248322}, {"text": "answer selection", "start_pos": 190, "end_pos": 206, "type": "TASK", "confidence": 0.7679020464420319}]}, {"text": "The model computes the representations of the Q&As and then their semantic simi-larity scores.", "labels": [], "entities": []}, {"text": "These scores are subsequently employed to rank the list of existing questions and answers with respect to the given question.", "labels": [], "entities": []}, {"text": "We evaluate our model on a public benchmark cQA data (, and show that the results of our model outperform the baselines.", "labels": [], "entities": [{"text": "cQA data", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.7547959685325623}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The hyper-parameters of the stacked bidirectional  LSTM model.", "labels": [], "entities": []}, {"text": " Table 2: The statistics for the cQA data (Nakov et al., 2016)  that we employ to evaluate our neural model.", "labels": [], "entities": [{"text": "cQA data", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9231036603450775}]}, {"text": " Table 4: Results on (a) development and (b) test data for answer selection task in cQA.", "labels": [], "entities": [{"text": "answer selection task", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.8856004675229391}, {"text": "cQA", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.9441230893135071}]}, {"text": " Table 5: Results on (a) development and (b) test data for question retrieval task in cQA.", "labels": [], "entities": [{"text": "question retrieval task", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.8189501762390137}, {"text": "cQA", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9296653866767883}]}]}