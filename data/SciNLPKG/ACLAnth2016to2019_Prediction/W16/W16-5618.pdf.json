{"title": [{"text": "Are You a Racist or Am I Seeing Things? Annotator Influence on Hate Speech Detection on Twitter", "labels": [], "entities": [{"text": "Hate Speech Detection", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.790540486574173}]}], "abstractContent": [{"text": "Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016).", "labels": [], "entities": []}, {"text": "For this reason, there has been both an academic and an industry interest in detection of hate speech.", "labels": [], "entities": [{"text": "detection of hate speech", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.8697299212217331}]}, {"text": "The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts.", "labels": [], "entities": []}, {"text": "In this paper, we provide an examination of the influence of annotator knowledge of hate speech on classification models by comparing classification results obtained from training on expert and amateur annotations.", "labels": [], "entities": []}, {"text": "We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016).", "labels": [], "entities": []}, {"text": "We find that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Large amounts of hate speech on exists on platforms that allow for user generated documents, which creates a need to detect and filter it (, and to create data sets that contain hate speech and are annotated for the occurrence of hate speech.", "labels": [], "entities": []}, {"text": "The need for corpus creation must be weighted against the psychological tax of being exposed to large amounts of abusive language.", "labels": [], "entities": [{"text": "corpus creation", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8225565552711487}]}, {"text": "A number of studies on profanity and hate speech detection, have crowdsourced their annotations due to the resources required to annotate large data sets and the possibility of distributing the load onto the crowd (.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.6450821856657664}]}, {"text": "investigate annotator reliability for hate speech annotation, concluding that \"hate speech is a fuzzy construct that requires significantly better definitions and guidelines in order to be annotated reliably\".", "labels": [], "entities": [{"text": "hate speech annotation", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7470773458480835}]}, {"text": "Hate speech is hard to detect for humans, which warrants a thorough understanding of the benefits and pitfalls of crowdsourced annotation.", "labels": [], "entities": [{"text": "Hate speech", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7615546882152557}]}, {"text": "This need is reinforced by previous studies, which utilize crowdsourcing of hate speech without knowledge on the quality of crowdsourced annotations for hate speech labeling.", "labels": [], "entities": [{"text": "hate speech labeling", "start_pos": 153, "end_pos": 173, "type": "TASK", "confidence": 0.7001619338989258}]}, {"text": "In addition, it is important to understand how different manners of obtaining labeling can influence the classification models and how it is possible to obtain good annotations, while ensuring that annotators are not likely to experience adverse effects of annotating hate speech.", "labels": [], "entities": []}, {"text": "Our contribution We provide annotations of 6, 909 tweets for hate speech by annotators from CrowdFlower and annotators that have a theoretical and applied knowledge of hate speech, henceforth amateur and expert annotators 1 . Our data set extends the data set by 4, 033 tweets.", "labels": [], "entities": []}, {"text": "We also illustrate, how amateur and expert annotations influence classification efforts.", "labels": [], "entities": [{"text": "classification", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.9816269278526306}]}, {"text": "Finally, we show the effects of allowing majority voting on classification and agreement between the amateur and expert annotators.", "labels": [], "entities": [{"text": "classification", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.9513456225395203}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Label distributions of the three annotation groups and", "labels": [], "entities": []}, {"text": " Table 4: Scores for each individual feature on amateur (majority voting) and expert annotations.", "labels": [], "entities": []}, {"text": " Table 5: Scores obtained for each of the feature sets.", "labels": [], "entities": []}]}