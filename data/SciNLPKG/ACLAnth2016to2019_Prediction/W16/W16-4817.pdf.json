{"title": [{"text": "Classifying ASR Transcriptions According to Arabic Dialect", "labels": [], "entities": [{"text": "Classifying ASR Transcriptions", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7731190721193949}]}], "abstractContent": [{"text": "We describe several systems for identifying short samples of Arabic dialects, which were prepared for the shared task of the 2016 DSL Workshop (Malmasi et al., 2016).", "labels": [], "entities": []}, {"text": "Our best system, an SVM using character tri-gram features, achieved an accuracy on the test data for the task of 0.4279, compared to a baseline of 0.20 for chance guesses or 0.2279 if we had always chosen the same most frequent class in the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9995467066764832}]}, {"text": "This compares with the results of the team with the best weighted F1 score, which was an accuracy of 0.5117.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9737122058868408}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9994217157363892}]}, {"text": "The team entries seem to fall into cohorts, with the all the teams in a cohort within a standard-deviation of each other, and our three entries are in the third cohort, which is about seven standard deviations from the top.", "labels": [], "entities": []}], "introductionContent": [{"text": "In 2016 the Distinguishing Similar Languages workshop () added a shared task to classify short segments of text as one of five Arabic dialects.", "labels": [], "entities": [{"text": "Distinguishing Similar Languages workshop", "start_pos": 12, "end_pos": 53, "type": "TASK", "confidence": 0.9012056589126587}]}, {"text": "The workshop organizers provided a training file and a schedule.", "labels": [], "entities": []}, {"text": "After allowing the participants development time, they distributed a test file, and evaluated the success of participating systems.", "labels": [], "entities": []}, {"text": "We built several systems for dialect classification, and submitted runs from three of them.", "labels": [], "entities": [{"text": "dialect classification", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7440667450428009}]}, {"text": "Interestingly, our results on the workshop test data were not consistent with our tests on reserved training data.", "labels": [], "entities": [{"text": "workshop test data", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.715328594048818}]}, {"text": "Our accuracy rates cluster around 40%; the rates of the best systems were a little better than 50%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993267059326172}]}, {"text": "If we take the raw scores as drawn from a binomial distribution, the standard deviation is p(1 \u2212 p)n.", "labels": [], "entities": []}, {"text": "With n = 1540, and p = 0.5 or p = 0.4 the standard deviation is 19.2 or 19.6 correct answers respectively, corresponding to a difference inaccuracy of about 1.25%.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 42, "end_pos": 60, "type": "METRIC", "confidence": 0.9315389394760132}]}, {"text": "Since the best overall accuracy score is 51.33%, our best score is 6.9 standard deviations below it.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 23, "end_pos": 37, "type": "METRIC", "confidence": 0.9788287281990051}]}, {"text": "(The scores of the top three teams don't seem to be significantly different from each other.)", "labels": [], "entities": []}, {"text": "On reserved training data, our systems all scored much better than they did on the test data, with our best system achieving an accuracy rate of 57%.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 128, "end_pos": 141, "type": "METRIC", "confidence": 0.9904033243656158}]}, {"text": "No doubt the best systems in the trial also scored better in training.", "labels": [], "entities": []}, {"text": "In addition to describing our systems, we speculate what factors might account for the difference in training and test results.", "labels": [], "entities": []}], "datasetContent": [{"text": "After the results of the test runs came back, we conducted some experiments to see whether using all 7619 segments of the training data would have made any difference to our SVM models.", "labels": [], "entities": []}, {"text": "Three different orders of n-grams, 2, 3 and 4-gram, are used to model the sentences of training and testing data set.", "labels": [], "entities": []}, {"text": "The n-gram feature vectors produced from training data are used to train Multi-class SVM model.", "labels": [], "entities": []}, {"text": "This results in three SVMs; 2, 3 and 4-gram based models.", "labels": [], "entities": []}, {"text": "After removing the n-gram components with zero counts overall training data, the dimensions of resulted feature vectors are 2601, 13656 and 82790 for 2-gram, 3-gram and 4-gram, respectively.", "labels": [], "entities": []}, {"text": "Three SVM systems were trained on bigram features, trigram features and 4-gram features respectively, and evaluated on testing data (1540 sentences.)", "labels": [], "entities": []}, {"text": "The bigram system achieved a score of 515 out of 1540 correct, or 0.3344.", "labels": [], "entities": []}, {"text": "The trigram system's score was 597 out of 1540, or 0.3877.", "labels": [], "entities": []}, {"text": "The 4-gram system's score was 649 out of 1540 or 0.4214, essentially the same as our best submitted system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Training and Test data by segment size", "labels": [], "entities": []}, {"text": " Table 3: Results for test set C (closed training).", "labels": [], "entities": []}]}