{"title": [{"text": "Using Embedding Masks for Word Categorization", "labels": [], "entities": [{"text": "Word Categorization", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7317837774753571}]}], "abstractContent": [{"text": "Word embeddings are widely used nowadays for many NLP tasks.", "labels": [], "entities": []}, {"text": "They reduce the dimensionality of the vocabulary space, but most importantly they should capture (part of) the meaning of words.", "labels": [], "entities": []}, {"text": "The new vector space used by the embeddings allows computation of semantic distances between words, while some word embed-dings also permit simple vector operations (e.g. summation, difference) resembling analogical reasoning.", "labels": [], "entities": [{"text": "summation", "start_pos": 171, "end_pos": 180, "type": "TASK", "confidence": 0.9570903778076172}]}, {"text": "This paper proposes anew operation on word embed-dings aimed to capturing categorical information by first learning and then applying an embedding mask for each analyzed category.", "labels": [], "entities": []}, {"text": "Thus, we conducted a series of experiments related to categorization of words based on their embeddings.", "labels": [], "entities": []}, {"text": "Several classical approaches were compared together with the one introduced in the paper which uses different embedding masks learnt for each category.", "labels": [], "entities": []}], "introductionContent": [{"text": "The idea of using vector representations of words for various natural language processing (NLP) and machine learning tasks has become more and more popular in the last years.", "labels": [], "entities": []}, {"text": "Most of these representations are based on the idea that the meaning of a word can be determined by the context in which each word is used.", "labels": [], "entities": []}, {"text": "Sometimes, additional information about the words is available or can be computed and this might be used along with the embedding for each word.", "labels": [], "entities": []}, {"text": "This information may consist of relations between words (e.g. syntactic dependencies), part of speech (POS) tags, word categories, word senses, etc.", "labels": [], "entities": []}, {"text": "In this paper, we propose to encode this extra information in the form of a vector mask that can be applied on the word embedding before being used as an input by any classifier, such as a neural network, or before computing any semantic distance between the word embeddings.", "labels": [], "entities": []}, {"text": "We explore the possibility of using vector masks for assigning WordNet categories to words.", "labels": [], "entities": []}, {"text": "We define a word category as one of the top concepts in the WordNet taxonomy as will be later explained in more detail.", "labels": [], "entities": [{"text": "WordNet taxonomy", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.9295122921466827}]}, {"text": "Using the trained masks fora subset of words, we then test whether they improve the accuracy of determining the correct category for new words that are not part of the training corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9992327690124512}]}], "datasetContent": [{"text": "The solutions below were tested in similar conditions on the generated dataset.", "labels": [], "entities": []}, {"text": "A brief description of each method was added when needed.", "labels": [], "entities": []}, {"text": "Most of the models (Support Vector Machines -SVM, random forest and logistic regression) were developed in Weka 1 , while the neural networks were implemented in Tensorflow 2 . Cosine Similarity A common way of comparing embeddings is using the cosine similarity.", "labels": [], "entities": [{"text": "Weka", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.9485910534858704}]}, {"text": "A threshold can be used as a boundary between positive and negative examples.", "labels": [], "entities": []}, {"text": "For word categorization, the best threshold was chosen based on the training set.", "labels": [], "entities": [{"text": "word categorization", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8292129635810852}]}], "tableCaptions": [{"text": " Table 1: Accuracy of the compared methods.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989883303642273}]}]}