{"title": [{"text": "Joint Learning of Sentence Embeddings for Relevance and Entailment", "labels": [], "entities": []}], "abstractContent": [{"text": "We consider the problem of Recognizing Textual Entailment within an Information Retrieval context, where we must simultaneously determine the relevancy as well as degree of entailment for individual pieces of evidence to determine a yes/no answer to a binary natural language question.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment within an Information Retrieval context", "start_pos": 27, "end_pos": 97, "type": "TASK", "confidence": 0.8289564400911331}]}, {"text": "We compare several variants of neural networks for sentence embeddings in a setting of decision-making based on evidence of varying relevance.", "labels": [], "entities": []}, {"text": "We propose a basic model to integrate evidence for entailment, show that joint training of the sentence embeddings to model relevance and entail-ment is feasible even with no explicit per-evidence supervision, and show the importance of evaluating strong baselines.", "labels": [], "entities": []}, {"text": "We also demonstrate the benefit of carrying over text comprehension model trained on an unrelated task for our small datasets.", "labels": [], "entities": []}, {"text": "Our research is motivated primarily by anew open dataset we introduce, consisting of binary questions and news-based evidence snippets.", "labels": [], "entities": []}, {"text": "We also apply the proposed relevance-entailment model on a similar task of ranking multiple-choice test answers, evaluating it on a preliminary dataset of school test questions as well as the standard MCTest dataset, where we improve the neural model state-of-art.", "labels": [], "entities": [{"text": "MCTest dataset", "start_pos": 201, "end_pos": 215, "type": "DATASET", "confidence": 0.9582400023937225}]}], "introductionContent": [{"text": "Let us consider the goal of building machine reasoning systems based on knowledge from fulltext data like encyclopedic articles, scientific papers or news articles.", "labels": [], "entities": []}, {"text": "Such machine reasoning systems, like humans researching a problem, must be able to recover evidence from large amounts of retrieved but mostly irrelevant information and judge the evidence to decide the answer to the question at hand.", "labels": [], "entities": []}, {"text": "A typical approach, used implicitly in information retrieval (and its extensions, like IR-based Question Answering systems), is to determine evidence relevancy by a keyword overlap feature (like tf-idf or BM-25 () and prune the evidence by the relevancy score.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7650130987167358}, {"text": "IR-based Question Answering", "start_pos": 87, "end_pos": 114, "type": "TASK", "confidence": 0.6016908685366312}]}, {"text": "On the other hand, textual entailment systems that seek to confirm hypotheses based on evidence ())) are typically provided with only a single piece of evidence or only evidence pre-determined as relevant, and are often restricted to short and simple sentences without open-domain named entity occurences.", "labels": [], "entities": []}, {"text": "In this work, we seek to fuse information retrieval and textual entaiment recognition by defining the Hypothesis Evaluation task as deciding the truth value of a hypothesis by integrating numerous pieces of evidence, not all of it equally relevant.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.7150559425354004}, {"text": "textual entaiment recognition", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.7845591505368551}]}, {"text": "As a specific instance, we introduce the Argus Yes/No Question Answering task.", "labels": [], "entities": [{"text": "Argus Yes/No Question Answering task", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.7285775031362262}]}, {"text": "The problem is, given a real-world event binary question like Did Donald Trump announce he is running for president? and numerous retrieved news article fragments as evidence, to determine the answer for the question.", "labels": [], "entities": []}, {"text": "Our research is motivated by the Argus automatic reporting system for the Augur prediction market platform.", "labels": [], "entities": [{"text": "Argus automatic reporting", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.5527321398258209}, {"text": "Augur prediction market platform", "start_pos": 74, "end_pos": 106, "type": "DATASET", "confidence": 0.8800453096628189}]}, {"text": "() Therefore, we consider the question answering task within the constraints of a practical scenario that has limited available dataset and only minimum supervision.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.9152926802635193}]}, {"text": "Hence, authentic news sentences are the evidence (with noise like segmentation errors, irrelevant participial phrases, etc.), and whereas we have gold standard for the correct answers, the model must do without explicit super-vision on which individual evidence snippets are relevant and what do they entail.", "labels": [], "entities": []}, {"text": "To this end, we introduce an open dataset of questions and newspaper evidence, and a neural model within the Sentence Pair Scoring framework () that (A) learns sentence embeddings for the question and evidence, (B) the embeddings represent both relevance and entailment characteristics as linear classifier inputs, and (C) the model aggregates all available evidence to produce a binary signal as the answer, which is the only training supervision.", "labels": [], "entities": [{"text": "Sentence Pair Scoring", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.7157928347587585}]}, {"text": "We also evaluate our model on a related task that concerns ranking answers of multiple-choice questions given a set of evidencing sentences.", "labels": [], "entities": []}, {"text": "We consider the MCTest dataset and the AI2-8grade/CK12 dataset that we introduce below.", "labels": [], "entities": [{"text": "MCTest dataset", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.9771164357662201}, {"text": "AI2-8grade/CK12 dataset", "start_pos": 39, "end_pos": 62, "type": "DATASET", "confidence": 0.7834388315677643}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "2, we formally outline the Argus question answering task, describe the question-evidence dataset, and describe the multiple-choice questions task and datasets.", "labels": [], "entities": [{"text": "Argus question answering task", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.6794293373823166}]}, {"text": "3, we briefly survey the related work on similar problems, whereas in Sec.", "labels": [], "entities": []}, {"text": "4 we propose our neural models for joint learning of sentence relevance and entailment.", "labels": [], "entities": []}, {"text": "We present the results in Sec.", "labels": [], "entities": []}, {"text": "5 and conclude with a summary, model usage recommendations and future work directions in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "Formally, the Hypothesis Evaluation task is to build a function y i = f h (H i ), where y i \u2208 [0, 1] is a binary label (no towards yes) and H i = (q i , E i ) is a hypothesis instance in the form of question text q i and a set of E i = {e ij } evidence texts e ij as extracted from an evidence-carrying corpus.", "labels": [], "entities": [{"text": "Hypothesis Evaluation", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.8556766211986542}]}, {"text": "Our main aim is to propose a solution to the Argus Task, where the Argus system (Baudis, 2015) () is to automatically analyze and answer questions in the context of the Augur prediction market platform.", "labels": [], "entities": [{"text": "Augur prediction market platform", "start_pos": 169, "end_pos": 201, "type": "DATASET", "confidence": 0.7256279289722443}]}, {"text": "Ina prediction market, users pose questions about future events whereas others bet on the yes or no answer, with the assumption that the bet price reflects the real probability of the event.", "labels": [], "entities": []}, {"text": "At a specified moment (e.g. after the date of a to-be-predicted sports match), the correct answer is retroactively determined and the bets are paid off.", "labels": [], "entities": []}, {"text": "At a larger vol-ume of questions, determining the bet results may present a significant overhead for running of the market.", "labels": [], "entities": []}, {"text": "This motivates the Argus system, which should partially automate this determinationdeciding questions related to recent events based on open news sources.", "labels": [], "entities": [{"text": "Argus", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.4798341691493988}]}, {"text": "To train a machine learning model for the f h function, we have created a dataset of questions with gold labels, and produced sets of evidence texts from a variety of newspaper using a preexisting IR (information retrieval) component of the Argus system.", "labels": [], "entities": [{"text": "Argus system", "start_pos": 241, "end_pos": 253, "type": "DATASET", "confidence": 0.9776047170162201}]}, {"text": "We release this dataset openly.", "labels": [], "entities": []}, {"text": "To pose a reproducible task for the IR component, the time domain of questions was restricted from September 1, 2014 to September 1, 2015, and topic domain was focused to politics, sports and the stock market.", "labels": [], "entities": [{"text": "IR", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.97906094789505}]}, {"text": "To build the question dataset, we have used several sources: \u2022 We asked Amazon Mechanical Turk users to pose questions, together with a golden label and a news article reference.", "labels": [], "entities": []}, {"text": "This seeded the dataset with initial, somewhat redundant 250 questions.", "labels": [], "entities": []}, {"text": "\u2022 We manually extended this dataset by derived questions with reversed polarity (to obtain an opposite answer).", "labels": [], "entities": []}, {"text": "\u2022 We extended the data with questions autogenerated from 26 templates, pertaining top sporting event winners and US senate or gubernatorial elections.", "labels": [], "entities": []}, {"text": "To build the evidence dataset, we used the Syphon preprocessing component () of the Argus implementation 3 to identify semantic roles of all question tokens and produce the search keywords if a role was assigned to each token.", "labels": [], "entities": [{"text": "Argus implementation 3", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.8775493303934733}]}, {"text": "We then used the IR component to query a corpus of newspaper articles, and kept sentences that contained at least 2/3 of all the keywords.", "labels": [], "entities": []}, {"text": "Our corpus of articles contained articles from The Guardian (all articles) and from the New York Times (Sports, Politics and Business sections  For the final dataset, we kept only questions whereat least a single evidence was found (i.e. we successfuly assigned a role to each token, found some news stories and found at least one sentence with 2/3 of question keywords within).", "labels": [], "entities": [{"text": "The Guardian", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.920378714799881}]}, {"text": "The final size of the dataset is outlined in and some examples are shown in.", "labels": [], "entities": []}, {"text": "The AI2 Elementary School Science Questions (no-diagrams variant) released by the Allen Institute cover 855 basic four-choice questions regarding high school science and follows up to the Allen AI Science Kaggle challenge.", "labels": [], "entities": [{"text": "AI2 Elementary School Science Questions", "start_pos": 4, "end_pos": 43, "type": "DATASET", "confidence": 0.6358163297176361}, {"text": "Allen Institute", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.8829114139080048}, {"text": "Allen AI Science Kaggle challenge", "start_pos": 188, "end_pos": 221, "type": "DATASET", "confidence": 0.7895426154136658}]}, {"text": "The vocabulary includes scientific jargon and named entities, and many questions are not factoid, requiring realworld reasoning or thought experiments.", "labels": [], "entities": []}, {"text": "We have combined each answer with the respective question (by substituting the wh-word in the question by each answer) and retrieved evidence sentences for each hypothesis using Solr search in a collection of CK-12 \"Concepts B\" textbooks.", "labels": [], "entities": [{"text": "CK-12 \"Concepts B\" textbooks", "start_pos": 209, "end_pos": 237, "type": "DATASET", "confidence": 0.8507568637530009}]}, {"text": "525 questions attained any supporting evidence, examples are shown in.", "labels": [], "entities": []}, {"text": "We consider this dataset as preliminary since it was not reviewed by a human and many hypotheses are apparently unprovable by the evidence we have gathered (i.e. the theoretical top accuracy is much lower than 1.0).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.8544278144836426}]}, {"text": "However, we released it to the public 7 and still included it in the comparison as these qualities reflect many realistic datasets of unknown qualities, so we find relative performances of models on such datasets instructive.", "labels": [], "entities": []}, {"text": "The Machine Comprehension Test () dataset has been introduced to provide a challenge for researchers to come up with models that approach human-level reading comprehen-sion, and serve as a higher-level alternative to semantic parsing tasks that enforce a specific knowledge representation.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 217, "end_pos": 233, "type": "TASK", "confidence": 0.7640196084976196}]}, {"text": "The dataset consists of a set of 660 stories spanning multiple sentences, written in simple and clean language (but with less restricted vocabulary than e.g. the bAbI dataset).", "labels": [], "entities": [{"text": "bAbI dataset", "start_pos": 162, "end_pos": 174, "type": "DATASET", "confidence": 0.9410317838191986}]}, {"text": "Each story is accompanied by four questions and each of these lists four possible answers; the questions are tagged as based on just one in-story sentence, or requiring multiple sentence inference.", "labels": [], "entities": []}, {"text": "We use an official extension of the dataset for RTE evaluation that again textually merges questions and answers.", "labels": [], "entities": [{"text": "RTE evaluation", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.9313369691371918}]}, {"text": "The dataset is split in two parts, MC-160 and MC-500, based on provenance but similar in quality.", "labels": [], "entities": []}, {"text": "We train all models on a joined training set.", "labels": [], "entities": []}, {"text": "The practical setting differs from the Argus task as the MCTest dataset contains relatively restricted vocabulary and well-formed sentences.", "labels": [], "entities": [{"text": "MCTest dataset", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9444955289363861}]}, {"text": "Furthermore, the goal is to find the single key point in the story to focus on, while in the Argus setting we may have many pieces of evidence supporting an answer; another specific characteristics of MCTest is that it consists of stories where the ordering and proximity of evidence sentences matters.", "labels": [], "entities": [{"text": "Argus", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.9361681938171387}]}, {"text": "We implement the differentiable model in the Keras framework and train the whole network from word embeddings to output evidence-integrated hypothesis label using the binary cross-entropy loss as an objective and the Adam optimization algorithm).", "labels": [], "entities": []}, {"text": "We apply L 2 = 10 \u22124 regularization and a p = 1/3 dropout.", "labels": [], "entities": []}, {"text": "Following the recommendation of (  In, we report the model performance on the Argus task, showing that the Ubuntu Dialogue transfer RNN outperforms other proposed models by a large margin.", "labels": [], "entities": [{"text": "Argus task", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.8434388339519501}, {"text": "Ubuntu Dialogue transfer RNN", "start_pos": 107, "end_pos": 135, "type": "DATASET", "confidence": 0.8997980058193207}]}, {"text": "However, a comparison of evidence integration approaches in shows that evidence integration is not the major deciding factor and there are no staticially meaningful differences between the evaluated approaches.", "labels": [], "entities": [{"text": "evidence integration", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7457499802112579}, {"text": "evidence integration", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.7557448446750641}]}, {"text": "We measured high correlation between classification and relevance scores with Pearson's r = 0.803, showing that our model does not learn a separate evidence weighing function on this task.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.8794484535853068}]}, {"text": "In, we look at the model performance on the AI2-8grade/CK12 task, repeating the story of Ubuntu Dialogue transfer RNN dominating other models.", "labels": [], "entities": [{"text": "AI2-8grade/CK12 task", "start_pos": 44, "end_pos": 64, "type": "DATASET", "confidence": 0.6217520833015442}]}, {"text": "However, on this task our proposed evidence weighing scheme improves over simpler approaches -but just on the best model, as shown in.", "labels": [], "entities": [{"text": "evidence weighing", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.8876749575138092}]}, {"text": "On the other hand, the simplest averaging model benefits from at least BM25 information to    select relevant evidence, apparently.", "labels": [], "entities": [{"text": "BM25", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9089841842651367}]}, {"text": "For the MCTest dataset, compares our proposed models with the current state-of-art ensemble of hand-crafted syntactic and framesemantic features (, as well as past neural models from the literature, all using attention mechanisms -the Attentive Reader of ( , Neural Reasoner of) and the HABCNN model family of.", "labels": [], "entities": [{"text": "MCTest dataset", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.9382145404815674}, {"text": "HABCNN model family", "start_pos": 287, "end_pos": 306, "type": "DATASET", "confidence": 0.9065544803937277}]}, {"text": "We see that averaging-based models are surprisingly effective on this task, and in particular on the MC-500 dataset it can beat even the best so far reported model of HABCNN-TE.", "labels": [], "entities": [{"text": "MC-500 dataset", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.9714696407318115}, {"text": "HABCNN-TE", "start_pos": 167, "end_pos": 176, "type": "DATASET", "confidence": 0.884229838848114}]}, {"text": "Our proposed transfer model is statistically equivalent to the best model on both datasets (furthermore, previous work did not include confidence intervals, even though their models should also be stochastically initialized).", "labels": [], "entities": []}, {"text": "As expected, our models did badly on the multiple-evidence class of questions -we made no attempt to model information flow across ad-  jacent sentences in our models as this aspect is unique to MCTest in the context of our work.", "labels": [], "entities": []}, {"text": "Interestingly, evidence weighing does play an important role on the MCTest task as shown in, significantly boosting model accuracy.", "labels": [], "entities": [{"text": "evidence weighing", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8259734809398651}, {"text": "MCTest", "start_pos": 68, "end_pos": 74, "type": "TASK", "confidence": 0.8401761054992676}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9942017197608948}]}, {"text": "This confirms that a mechanism to allocate attention to different sentences is indeed crucial for this task.", "labels": [], "entities": []}], "tableCaptions": []}