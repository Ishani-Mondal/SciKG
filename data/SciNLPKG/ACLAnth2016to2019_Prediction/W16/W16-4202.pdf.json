{"title": [{"text": "Bidirectional LSTM-CRF for Clinical Concept Extraction", "labels": [], "entities": [{"text": "Clinical Concept Extraction", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.6133536895116171}]}], "abstractContent": [{"text": "Automated extraction of concepts from patient clinical records is an essential facilitator of clinical research.", "labels": [], "entities": [{"text": "Automated extraction of concepts from patient clinical", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8469105022294181}]}, {"text": "For this reason, the 2010 i2b2/VA Natural Language Processing Challenges for Clinical Records introduced a concept extraction task aimed at identifying and classifying concepts into predefined categories (i.e., treatments, tests and problems).", "labels": [], "entities": [{"text": "VA Natural Language Processing Challenges for Clinical Records", "start_pos": 31, "end_pos": 93, "type": "DATASET", "confidence": 0.6475029028952122}, {"text": "concept extraction task", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.7963211933771769}]}, {"text": "State-of-the-art concept extraction approaches heavily rely on handcrafted features and domain-specific resources which are hard to collect and define.", "labels": [], "entities": [{"text": "concept extraction", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7662351429462433}]}, {"text": "For this reason, this paper proposes an alternative, streamlined approach: a recurrent neural network (the bidirectional LSTM with CRF decoding) initialized with general-purpose, off-the-shelf word embeddings.", "labels": [], "entities": []}, {"text": "The experimental results achieved on the 2010 i2b2/VA reference corpora using the proposed framework outperform all recent methods and ranks closely to the best submission from the original 2010 i2b2/VA challenge.", "labels": [], "entities": [{"text": "i2b2/VA reference corpora", "start_pos": 46, "end_pos": 71, "type": "DATASET", "confidence": 0.6134484291076661}]}], "introductionContent": [{"text": "Patient clinical records typically contain longitudinal data about patients' health status, diseases, conducted tests and response to treatments.", "labels": [], "entities": []}, {"text": "Analysing such information can prove of immense value not only for clinical practice, but also for the organisation and management of healthcare services.", "labels": [], "entities": []}, {"text": "Concept extraction (CE) aims to identify mentions to medical concepts such as problems, test and treatments in clinical records (e.g., discharge summaries and progress reports) and classify them into predefined categories.", "labels": [], "entities": [{"text": "Concept extraction (CE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8798664748668671}]}, {"text": "The concepts in clinical records are often expressed with unstructured, \"free\" text, making their automatic extraction a challenging task for clinical Natural Language Processing (NLP) systems.", "labels": [], "entities": []}, {"text": "Traditional approaches have extensively relied on rule-based systems and lexicons to recognise the concepts of interest.", "labels": [], "entities": []}, {"text": "Typically, the concepts represent drug names, anatomical nomenclature and other specialized names and phrases which are not part of everyday vocabularies.", "labels": [], "entities": []}, {"text": "For instance, \"resp status\" should be interpreted as \"response status\".", "labels": [], "entities": []}, {"text": "Such use of abbreviated phrases and acronyms is very common within the medical community, with many abbreviations having a specific meaning that differ from that of other lexicons.", "labels": [], "entities": []}, {"text": "Dictionary-based systems perform concept extraction by looking up terms on medical ontologies such as the Unified Medical Language System (UMLS).", "labels": [], "entities": [{"text": "concept extraction", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7560895681381226}]}, {"text": "Intrinsically, dictionary-and rule-based systems are laborious to implement and inflexible to new cases and misspellings (.", "labels": [], "entities": []}, {"text": "Although these systems can achieve high precision, they tend to suffer from low recall (i.e., they may miss a significant number of concepts).", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9965921640396118}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9992424249649048}]}, {"text": "To overcome these limitations, various machine learning approaches have been proposed (e.g., conditional random fields (CRFs), maximum-entropy classifiers and support vector machines) to simultaneously exploit the textual and contextual information while reducing the reliance on lexicon lookup (.", "labels": [], "entities": []}, {"text": "State-of-the-art machine learning approaches usually follow a two-step process of feature engineering and classification.", "labels": [], "entities": []}, {"text": "The feature engineering task is, in its own right, very laborious and demanding on expert knowledge, and it can become the bottleneck of the overall approach.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8873386681079865}]}, {"text": "For this reason, this paper proposes a highly streamlined alternative: to employ a contemporary neural network -the bidirectional LSTM-CRF -initialized with general-purpose, off-the-shelf word embeddings such as GloVe () and Word2Vec ().", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 225, "end_pos": 233, "type": "DATASET", "confidence": 0.946230947971344}]}, {"text": "The experimental results over the authoritative 2010 i2b2/VA benchmark show that the proposed approach outperforms all recent approaches and ranks closely to the best from the literature.", "labels": [], "entities": [{"text": "2010 i2b2/VA benchmark", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.5406503230333328}]}], "datasetContent": [{"text": "The 2010 i2b2/VA Natural Language Processing Challenges for Clinical Records include a concept extraction task focused on the extraction of medical concepts from patient reports.", "labels": [], "entities": [{"text": "i2b2/VA Natural Language Processing Challenges", "start_pos": 9, "end_pos": 55, "type": "TASK", "confidence": 0.6371773992265973}, {"text": "concept extraction task", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.7994614044825236}, {"text": "extraction of medical concepts from patient reports", "start_pos": 126, "end_pos": 177, "type": "TASK", "confidence": 0.7703886117253985}]}, {"text": "For the challenge, a total of 394 concept-annotated reports for training, 477 for testing, and 877 unannotated reports were deidentified and released to the participants alongside a data use agreement).", "labels": [], "entities": []}, {"text": "However, part of this data set is no longer being distributed due to restrictions later introduced by the Institutional Review Board (IRB).", "labels": [], "entities": [{"text": "Institutional Review Board (IRB)", "start_pos": 106, "end_pos": 138, "type": "DATASET", "confidence": 0.755162517229716}]}, {"text": "Thus, summarizes the basic statistics of the training and test data sets which are currently publicly available and that we have used in our experiments.", "labels": [], "entities": []}, {"text": "All weight matrices were randomly initialized from the uniform distribution within range [\u22121, 1].", "labels": [], "entities": []}, {"text": "The word embeddings were either initialized randomly in the same way or fetched from Word2Vec and GloVe (.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.9721047878265381}]}, {"text": "Approximately 25% of the tokens were alphanumeric, abbreviated or domain-specific strings that were not available as pre-trained embeddings and were always randomly initialized.", "labels": [], "entities": []}, {"text": "Early stopping of training was set to 50 epochs to mollify over-fitting, and the model that gave the best performance on the validation set was retained.", "labels": [], "entities": []}, {"text": "The accuracy is reported in terms of microaverage F 1 score computed using the CoNLL score function (Nadeau and Sekine, 2007).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999638557434082}, {"text": "microaverage F 1 score", "start_pos": 37, "end_pos": 59, "type": "METRIC", "confidence": 0.9222979098558426}, {"text": "CoNLL score function", "start_pos": 79, "end_pos": 99, "type": "METRIC", "confidence": 0.871049185593923}]}, {"text": "shows the performance comparison between state-of-the-art CE systems and the proposed bidirectional LSTM-CRF with different initialization strategies.", "labels": [], "entities": []}, {"text": "As a first note, the bidirectional LSTM-CRF initialized with GloVe outperforms all recent approaches.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.7872365117073059}]}, {"text": "On the other hand, the best submission from the 2010 i2b2/VA challenge (deBruijn et al., 2011) still outperforms our approach.", "labels": [], "entities": [{"text": "i2b2/VA challenge (deBruijn et al., 2011", "start_pos": 53, "end_pos": 93, "type": "DATASET", "confidence": 0.7768434852361679}]}, {"text": "However, based on the description provided in), these results are not directly comparable since the experiments in) had used the original dataset which has a significantly larger number of training samples.", "labels": [], "entities": []}, {"text": "Using general-purpose, pre-trained embeddings improves the F 1 score by over 5 percentage points over a random initialization.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9853923122088114}]}, {"text": "In general, the results achieved with the proposed approach are close and in many cases above the results achieved by systems based on hand-engineered features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the training and test data sets used for the 2010-i2b2/VA concept extraction.", "labels": [], "entities": [{"text": "2010-i2b2/VA concept extraction", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.7667957663536071}]}, {"text": " Table 3: Performance comparison between the bidirectional LSTM-CRF (bottom three lines) and state- of-the-art systems (top five lines) over the 2010 i2b2/VA concept extraction task.", "labels": [], "entities": [{"text": "i2b2/VA concept extraction task", "start_pos": 150, "end_pos": 181, "type": "TASK", "confidence": 0.630234976609548}]}]}