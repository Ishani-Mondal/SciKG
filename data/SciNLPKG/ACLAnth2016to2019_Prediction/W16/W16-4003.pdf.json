{"title": [{"text": "A Dataset for Multimodal Question Answering in the Cultural Heritage Domain", "labels": [], "entities": [{"text": "Multimodal Question Answering", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.6425703366597494}]}], "abstractContent": [{"text": "Multimodal question answering in the cultural heritage domain allows visitors to museums, landmarks or other sites to ask questions in a more natural way.", "labels": [], "entities": [{"text": "Multimodal question answering", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6218419869740804}]}, {"text": "This in turn provides better user experiences.", "labels": [], "entities": []}, {"text": "In this paper, we propose the construction of a golden standard dataset dedicated to aiding research into multimodal question answering in the cultural heritage domain.", "labels": [], "entities": [{"text": "question answering", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7029623836278915}]}, {"text": "The dataset, soon to be released to the public, contains multimodal content about the fascinating old-Egyptian Amarna period, including images of typical artworks, documents about these artworks (con-taining images) and over 800 multimodal queries integrating visual and textual questions.", "labels": [], "entities": []}, {"text": "The multimodal questions and related documents are all in English.", "labels": [], "entities": []}, {"text": "The multimodal questions are linked to relevant paragraphs in the related documents that contain the answer to the multimodal query.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimodal Question Answering (MQA) invokes answering a query that is formed using different modalities.", "labels": [], "entities": [{"text": "Multimodal Question Answering (MQA) invokes answering a query", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.8606225311756134}]}, {"text": "This topic combines Computer Vision (CV), Natural Language Processing (NLP) and possibly Speech Recognition (SR).", "labels": [], "entities": [{"text": "Speech Recognition (SR)", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.8406673669815063}]}, {"text": "With the increasing use of mobile devices, taking pictures becomes an easy and natural way for people to interact with cultural objects.", "labels": [], "entities": []}, {"text": "Therefore, we consider a question composed of a textual query combined with a picture of a cultural objector part thereof, where the picture and the natural language question can provide complementary information.", "labels": [], "entities": []}, {"text": "Interest in MQA dramatically increased in recent years (.", "labels": [], "entities": [{"text": "MQA", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.8791772127151489}]}, {"text": "show techniques for the joint processing of images and natural language sentences, introduce object-level visual question answering research, which has some similarity with the visual question answering that we propose in this paper.", "labels": [], "entities": [{"text": "object-level visual question answering", "start_pos": 93, "end_pos": 131, "type": "TASK", "confidence": 0.6082784533500671}, {"text": "visual question answering", "start_pos": 177, "end_pos": 202, "type": "TASK", "confidence": 0.8134207725524902}]}, {"text": "Yet, no studies exist that regard MQA in the cultural heritage domain, with the objective of improving the user experience.", "labels": [], "entities": [{"text": "MQA", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.89840167760849}]}, {"text": "Moreover, state-of-the-art research in the area of cultural heritage is uni-modal and either only demonstrates the recognition of images of a cultural object (), or is only concerned with the processing of a vocal input (.", "labels": [], "entities": []}, {"text": "This paper reports on anew dataset constructed to facilitate MQA in the cultural domain.", "labels": [], "entities": [{"text": "MQA", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9869683384895325}]}, {"text": "The dataset includes images of 16 fascinating artworks from the old-Egyptian Amarna period, documents with regard to this period, and 805 multimodal questions composed of both the full image and part-of-image level queries with regard to the artworks.", "labels": [], "entities": []}, {"text": "A part-of-image level query is a question posed in natural language that regards part of the image of a cultural heritage object.", "labels": [], "entities": []}, {"text": "In our dataset, the multimodal questions correspond to linked paragraphs in documents that are part of a larger document collection.", "labels": [], "entities": []}, {"text": "Part-ofimage level queries ask for more detailed information about an image than the identity of the object it shows.", "labels": [], "entities": []}, {"text": "Their resolution requires the joint processing of the images and the natural language questions.", "labels": [], "entities": []}, {"text": "All images and their related documents are collected from external web sources and the multimodal questions are collected from people of different ages and backgrounds, via a survey.", "labels": [], "entities": []}, {"text": "The dataset is stored as a database using the unrelational database management system MongoDB.", "labels": [], "entities": [{"text": "MongoDB", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9787105321884155}]}, {"text": "The remainder of the paper is organized as follows: Section 2 discusses some advantages of using multimodal queries during cultural heritage visits.", "labels": [], "entities": []}, {"text": "Section 3 describes how we have built the golden standard dataset and offers some analysis.", "labels": [], "entities": [{"text": "golden standard dataset", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.7006760040918986}]}, {"text": "Section 4 discusses a particular application of the dataset, Section 5 provides concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the context described so far, personalized and important questions of visitors are supposed to be expressed as a multimodal query.", "labels": [], "entities": []}, {"text": "Such query is composed of a photo taken by the visitor of an artwork or of a detail of it, augmented by a question expressed in natural language, as shows.", "labels": [], "entities": []}, {"text": "Number shows some statistics for the dataset.", "labels": [], "entities": []}, {"text": "'Related paragraphs' denote the number of paragraphs linked to the multimodal questions, and the 'document collection size' describes the size of the document collection by the number of English words it contains.", "labels": [], "entities": []}, {"text": "55% of the questions in the dataset can be answered purely by the related document collection in the dataset.", "labels": [], "entities": []}, {"text": "On the other hand, several questions such as 'How many colors does the image contain?'", "labels": [], "entities": []}, {"text": "can be answered separately by low-level image analysis.", "labels": [], "entities": []}, {"text": "Additionally, questions about cultural heritage objects are sometimes difficult to answer due to the lack of historical knowledge.", "labels": [], "entities": []}, {"text": "Also, a lot of questions such as 'Why is the woman so ugly?'", "labels": [], "entities": []}, {"text": "'Do they really love each other?' are difficult to answer even fora human.", "labels": [], "entities": []}, {"text": "We still keep these questions in our dataset as these questions are examples of what humans would ask.", "labels": [], "entities": []}, {"text": "Some example questions are shown in.", "labels": [], "entities": []}, {"text": "The questions in this dataset are diverse and can be used for various artificial intelligence tasks, especially for natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.6712499856948853}]}, {"text": "They contain simple image understanding questions with regard to the object class (e.g., 'Is this a woman or man?') and the attributes of the objects (e.g., 'What material is the sculpture made of?').", "labels": [], "entities": [{"text": "image understanding", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7556627988815308}]}, {"text": "Some questions are complex and need deep and commonsense reasoning.", "labels": [], "entities": []}, {"text": "For example, to answer the question 'Why do they give each other a hand?', we should know that this question is about the two persons who wear a similar crown in the image.", "labels": [], "entities": []}, {"text": "Based on our commonsense, the two persons wearing a similar crown and giving each other a hand should be a couple or more specifically king and queen, and a couple in this ancient culture like to give each other a hand to show their feelings and relationship.", "labels": [], "entities": []}, {"text": "We categorize the questions based on the method of () into 9 types, the statistics of which are shown in(b): 1) What: questions about the attributes and features of the object.", "labels": [], "entities": []}, {"text": "2) When: questions related to time with regard to the subject.", "labels": [], "entities": [{"text": "When", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9778409600257874}]}, {"text": "3) Who : questions about the identity of a person.", "labels": [], "entities": []}, {"text": "4) Why: questions about the reason of some phenomenon.", "labels": [], "entities": [{"text": "Why", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9512848854064941}]}, {"text": "5) Where: questions about the location of the object.", "labels": [], "entities": []}, {"text": "6) Which: questions that need reasoning about the object.", "labels": [], "entities": []}, {"text": "The questions' length in the dataset ranges from 2 to 24 English words and the average length of the questions is 6.56 words.", "labels": [], "entities": []}, {"text": "A histogram of the questions' length in the dataset is shown in.", "labels": [], "entities": []}, {"text": "Compared to other application areas of MQA such as in-door scenes and wild-life animals, MQA tasks in the cultural heritage domain are more difficult.", "labels": [], "entities": [{"text": "MQA tasks", "start_pos": 89, "end_pos": 98, "type": "TASK", "confidence": 0.8753359615802765}]}, {"text": "Due to uniqueness of the artworks and historical reasons, it is hard to obtain a large amount of data for each artwork and thus few training data.", "labels": [], "entities": []}, {"text": "As no MQA dataset in the cultural heritage domain has been released so far, our dataset, which will be made public soon, can be regarded as the first benchmark for MQA research in that field.", "labels": [], "entities": [{"text": "MQA dataset in the cultural heritage domain", "start_pos": 6, "end_pos": 49, "type": "DATASET", "confidence": 0.7579333101000104}, {"text": "MQA", "start_pos": 164, "end_pos": 167, "type": "TASK", "confidence": 0.9659159183502197}]}, {"text": "This dataset is explicitly constructed for facilitating MQA.", "labels": [], "entities": [{"text": "MQA", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9329808354377747}]}, {"text": "In this case, the answer estimation problem can be formulated as the probability of an answer a conditioned on a multimodal question q composed of an image q i and its corresponding textual question qt , as shown in the formula below.", "labels": [], "entities": [{"text": "answer estimation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.9026676416397095}]}, {"text": "In this formula, \u03b8 denotes a vector of all parameters to learn.", "labels": [], "entities": []}, {"text": "q i and qt can be represented as real valued vectors: or other forms that the computers can directly use, q is a joint representation of q i and qt . With this formula, answers with the highest k probabilities will be retrieved as a ranked answer set A.", "labels": [], "entities": []}, {"text": "Note that A can bean empty list if all probabilities are zero or are considered too low to yield a valuable answer.", "labels": [], "entities": []}, {"text": "Compared to other popular multimodal datasets composed of visual and language data (e.g., DAQUAR, VQA), which provide only question-answer pairs that can be easily collected by online crowdsourcing platforms such as Amazon Mechanical Turk (AMT) 6 , the dataset described in this paper, which contains a large set of documents and diverse multimodal questions, can give better perspectives for MQA research: 1) Natural language processing of the questions and textual documentation Query formation involves keywords selection from raw textual questions.", "labels": [], "entities": [{"text": "MQA", "start_pos": 393, "end_pos": 396, "type": "TASK", "confidence": 0.9786179065704346}, {"text": "Query formation", "start_pos": 481, "end_pos": 496, "type": "TASK", "confidence": 0.8301817774772644}]}, {"text": "To detect answer types in the dataset, named entity recognition and more specifically the recognition of person and location names and other types of entity information in the question and textual documents can be implemented.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.6664667824904124}]}, {"text": "Also, the natural language questions and documents can be used to research coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.9627568125724792}]}], "tableCaptions": [{"text": " Table 1. List of example sources that form the related documents for the artworks.", "labels": [], "entities": []}, {"text": " Table 2. Statistics of the dataset.", "labels": [], "entities": []}]}