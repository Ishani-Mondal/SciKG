{"title": [{"text": "Adjusting Word Embeddings with Semantic Intensity Orders", "labels": [], "entities": []}], "abstractContent": [{"text": "Semantic lexicons such as WordNet and PPDB have been used to improve the vector-based semantic representations of words by adjusting the word vectors.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9516659379005432}]}, {"text": "However, such lexicons lack semantic intensity information, inhibiting adjustment of vector spaces to better represent semantic intensity scales.", "labels": [], "entities": []}, {"text": "In this work, we adjust word vectors using the semantic intensity information in addition to synonyms and antonyms from WordNet and PPDB, and show improved performance on judging semantic intensity orders of adjective pairs on three different human annotated datasets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 120, "end_pos": 127, "type": "DATASET", "confidence": 0.9468403458595276}]}], "introductionContent": [{"text": "Word embedding models that represent words as real-valued vectors have been directly used in word-level NLP tasks such as word similarity (), antonym detection (, knowledge relations (, and semantic scale inference.", "labels": [], "entities": [{"text": "antonym detection", "start_pos": 142, "end_pos": 159, "type": "TASK", "confidence": 0.706153616309166}]}, {"text": "Word embedding models such as Word2Vec (continuous bag-of-words (CBOW) and skip-gram) () and GloVe (, widely used to generate word vectors, are trained following the distributional hypothesis which assumes that the meaning of words can be represented by their context.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.9543012976646423}, {"text": "GloVe", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.884900689125061}]}, {"text": "However, word embedding models based solely on the distributional hypothesis often place words improperly in vector spaces.", "labels": [], "entities": []}, {"text": "For example, in a vector space, a word and its antonym should be sufficiently far apart, but they can be quite close because they can have similar contexts in many cases.", "labels": [], "entities": []}, {"text": "For better semantic representations, different approaches using semantic lexicons as well as lexical knowledge to adjust word vectors have recently been introduced.", "labels": [], "entities": []}, {"text": "adjusted each word vector to be in the middle between the initial position and its synonymous words.", "labels": [], "entities": []}, {"text": "Mrk\u0161i\u00b4c used max-margin approaches to adjust each word vector with synonyms and antonyms while keeping the relative similarities to the neighbors.", "labels": [], "entities": []}, {"text": "While these two approaches are post-processing models that adjust preexisting word vectors,,, and jointly train models that augment the skip-gram () objective function to include knowledge from semantic lexicons.", "labels": [], "entities": []}, {"text": "The common goal in these approaches is to make semantically close words closer and semantically distant words farther apart while keeping each word vector not to be too far from the original position.", "labels": [], "entities": []}, {"text": "Although the joint training models can even indirectly adjust words that are not listed in the semantic lexicons, the post-processing models are much more efficient and can be applied to word vectors from any kinds of models, which can eventually perform better than the joint training models.", "labels": [], "entities": []}, {"text": "Although, Mrk\u0161i\u00b4c,,, and's adjustment approaches have been shown to represent word semantics better in vector spaces, their coarse modeling of words as synonyms or antonyms maybe insufficient for modeling words lying along a semantic intensity scale.", "labels": [], "entities": []}, {"text": "For example, assume that \"great\" is erroneously between \"bad\" and \"good\" in a vector space (\"bad\" should be closer to \"good\" than \"great\").", "labels": [], "entities": []}, {"text": "Since semantic lexicons such as and the Paraphrase Database (PPDB)) only inform us that \"good\" and \"great\" are semantically similar and \"good\" is semantically opposite to \"bad\", adjusting word vectors with those semantic lexicons does not permit to retrieve the appropriate semantic intensity ordering: bad < good < great.", "labels": [], "entities": []}, {"text": "Accurate representation of such semantic intensity scales can help correct processing in downstream tasks that require robust textual understanding.", "labels": [], "entities": []}, {"text": "For instance, given an assertion such as the movie is outstanding, statements that contain a semantically weaker expression (e.g., the movie is good, the movie is okay) are entailed, whereas the movie is okay does not entail that the movie is outstanding.", "labels": [], "entities": []}, {"text": "Similarly, correct information about semantic scales can also provide accurate inferences: when answers to a yes/no question that contains a gradable adjective does not explicitly contain a yes or a no, we can derive the intended answer by figuring out whether the answer entails or implicates the question.", "labels": [], "entities": []}, {"text": "For example, for the question Was the talk good?, if the answer is It was excellent, the answer entails \"yes\", but if the answer is It was okay, \"no\" will be implied.", "labels": [], "entities": []}, {"text": "To deal with the representation of semantic intensity scales, we infer semantic intensity orders with de's approach and then use the intensity orders to adjust the word vectors.", "labels": [], "entities": []}, {"text": "Evaluating on three different human annotated datasets, we show that the adjustment with intensity orders in addition to adjustments with synonyms and antonyms performs best in representing semantic intensities.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the representation of semantic intensities on the three following human-annotated datasets.", "labels": [], "entities": []}, {"text": "In our evaluation of the semantic orderings of adjective pairs, we decide which adjective in a pair <A, B> is semantically stronger following Kim and de Marneffe (2013)'s approach.", "labels": [], "entities": []}, {"text": "First, we look   for an antonym of A.", "labels": [], "entities": []}, {"text": "2 Then, we check whether the word vector for B is more similar to the vector for A than to the vector for A's antonym, or whether the vector for B is more similar to the vector for A's antonym.", "labels": [], "entities": []}, {"text": "We infer a \"yes\" answer in the former case, and a \"no\" in the other case.", "labels": [], "entities": []}, {"text": "Expanding on the results in, as the baselines, we used three different 300 dimensional off-the-shelf word vectors: GloVe, 4 CBOW, and Paragram-SL999.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.8707253336906433}]}, {"text": "Following Mrk\u0161i\u00b4c, for each of the word vector sets, we extracted word vectors corresponding to the 76,427 most frequent words from Open-Subtitles.", "labels": [], "entities": []}, {"text": "Table indicates whether the differences in performance of the adjustment methods in are statistically significant (McNemar's \u03c7 2 test with pvalue < 0.05).", "labels": [], "entities": []}, {"text": "In the table, \"merged\" columns are the results of the concatenation of all the datasets.", "labels": [], "entities": []}, {"text": "For each comparison, '+' denotes that the performance of the latter is significantly higher than that of the former, and '-' denotes the opposite, whereas no value indicates that the difference in performance is not statistically significant.", "labels": [], "entities": []}, {"text": "For Paragram vectors, only one case (\"baseline\" vs \"syn&ant,same_ord\") is significantly different.", "labels": [], "entities": []}, {"text": "In, \"baseline\" shows the performance of the baseline word vectors without any adjustments.", "labels": [], "entities": []}, {"text": "Since Paragram-SL999 are optimized to perform best on evaluating SimLex-999 dataset, the baseline performance of Paragram-SL999 on SimLex-999 as well as two of the other datasets are noticeably better than word vectors from GloVe and CBOW.", "labels": [], "entities": [{"text": "SimLex-999 dataset", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.8893560767173767}, {"text": "CBOW", "start_pos": 234, "end_pos": 238, "type": "DATASET", "confidence": 0.8856118321418762}]}, {"text": "In \"syn&ant\", corresponding to the optimization with equation 4, 15,509 words are adjusted with the synonyms and 6,162 words are adjusted with the antonyms.", "labels": [], "entities": []}, {"text": "This adjustment significantly, IQAP, and de Melo & Bansal (dM&B) datasets, as well as concatenating the three datasets (merged).", "labels": [], "entities": [{"text": "IQAP", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.5254243612289429}, {"text": "de Melo & Bansal (dM&B) datasets", "start_pos": 41, "end_pos": 73, "type": "DATASET", "confidence": 0.6501807183027267}]}, {"text": "For xv. y, '+' denotes that y's score is significantly higher than that of x, ''-' denotes the opposite, and no value denotes that the difference is not statistically significant.", "labels": [], "entities": []}, {"text": "improves the performance of CBOW vectors and Paragram vectors on the IQAP and de Melo and Bansal (2013)'s datasets.", "labels": [], "entities": [{"text": "IQAP and de Melo and Bansal (2013)'s datasets", "start_pos": 69, "end_pos": 114, "type": "DATASET", "confidence": 0.73824096809734}]}, {"text": "Specifically, for the IQAP dataset, where many of the pairs are either synonyms or antonyms, \"syn&ant\" showed better performance than including adjustments with semantic intensity orders.", "labels": [], "entities": [{"text": "IQAP dataset", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.9362793266773224}]}, {"text": "However, this adjustment makes GloVe vectors yield significantly worse performance on the WordNet synset pair dataset.", "labels": [], "entities": [{"text": "WordNet synset pair dataset", "start_pos": 90, "end_pos": 117, "type": "DATASET", "confidence": 0.9356699436903}]}, {"text": "This shows that the adjustment with just synonyms and antonyms can worsen the representation of subtle semantics considering intensities.", "labels": [], "entities": []}, {"text": "In this case, using just the adjustment with semantic intensity orders can be helpful.", "labels": [], "entities": []}, {"text": "\"same_ord (kmeans only)\", corresponding to equation 5, adjusts word vectors by just making vectors of words with the same intensity order to be more similar without using synonyms and antonyms.", "labels": [], "entities": []}, {"text": "For GloVe vectors, \"same_ord (kmeans only)\" showed the highest score for the WordNet synset pair dataset.", "labels": [], "entities": [{"text": "WordNet synset pair dataset", "start_pos": 77, "end_pos": 104, "type": "DATASET", "confidence": 0.9219485223293304}]}, {"text": "For adjustments with semantic intensity orders, 616 words are adjusted when WordNet dumbbells and Equivalence relations from PPDB word pairs are used as the clusters.", "labels": [], "entities": [{"text": "WordNet dumbbells", "start_pos": 76, "end_pos": 93, "type": "DATASET", "confidence": 0.9014642238616943}, {"text": "Equivalence", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.9285024404525757}]}, {"text": "When clusters from kmeans++ are used, several hundreds of words are adjusted, where the adjusted words vary depending on the vector space for each iteration.", "labels": [], "entities": []}, {"text": "For the WordNet synset pair dataset and de's dataset, where the subtle semantic intensity differences are more critical, using synonyms, antonyms, and semantic intensity orders altogether (\"syn&ant,same_ord,diff_ord\") showed significantly higher scores than \"syn&ant\" in many settings.", "labels": [], "entities": [{"text": "WordNet synset pair dataset", "start_pos": 8, "end_pos": 35, "type": "DATASET", "confidence": 0.8621936440467834}]}, {"text": "Here, \"diff_ord\" corresponds to equation 6.", "labels": [], "entities": []}, {"text": "shows the adjective pairs whose intensity judgements were changed by including adjustments with semantic intensity orders.", "labels": [], "entities": []}, {"text": "The pairs are from the WordNet synset pairs and baseline v.", "labels": [], "entities": [{"text": "WordNet synset pairs", "start_pos": 23, "end_pos": 43, "type": "DATASET", "confidence": 0.952382504940033}]}, {"text": "same_ord (kmeans only) syn&ant v. syn&ant, same_ord,diff_ord(+kmeans) satisfactory < superb mediocre < severe unfavorable < poor troublesome < rocky crazy < ardent upfront < blunt outspoken < expansive solid < redeeming sad < tragic warm < uneasy deserving < sacred valuable < sacred: Adjective pairs whose incorrect decisions with the former models are corrected by the latter models.", "labels": [], "entities": []}, {"text": "For those model comparisons, there were no pairs that were correctly judged with the former models but not with the latter models.", "labels": [], "entities": []}, {"text": "GloVe vectors were used as the baseline.", "labels": [], "entities": []}, {"text": "\"baseline\" is compared to \"same_ord (kmeans only)\" in the first column and \"syn&ant\" is compared to \"syn&ant,same_ord,diff_ord(+kmeans)\".", "labels": [], "entities": []}, {"text": "In both cases, we observe that some of the incorrectly judged pairs are corrected when adding the adjustment with semantic intensity orders.", "labels": [], "entities": []}, {"text": "In these cases, there were no pairs that were correctly judged by the adjustments without semantic intensity orders but incorrectly judged with semantic intensity orders.", "labels": [], "entities": []}, {"text": "Since the numbers of adjectives pairs in the datasets and the numbers of words that are adjusted with semantic intensity orders are small, not all the cases comparing the adjustments using just synonyms and antonyms to the adjustments including semantic intensity orders were significant for p-value < 0.05, as shown in.", "labels": [], "entities": []}, {"text": "However, since many of them are slightly insignificant (like p-value=0.07) and the scores noticeably increased in many cases, using semantic intensity orders for the adjustments seem promising.", "labels": [], "entities": []}, {"text": "In addition, to show that the adjustments are not harmful for the representation of the general semantics of the words, we also evaluated on SimLex-999 (, where 999 word  pairs were annotated on Mechanical Turk to score the degree of semantic similarities.", "labels": [], "entities": []}, {"text": "This dataset has been widely used to evaluate the quality of semantic representations of words.", "labels": [], "entities": []}, {"text": "shows Spearman's \u03c1 scores on the SimLex-999 dataset for the different adjustment methods.", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 17, "end_pos": 18, "type": "METRIC", "confidence": 0.6585211157798767}, {"text": "SimLex-999 dataset", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.9784310758113861}]}, {"text": "Since SimLex-999 dataset is not directly related to semantic intensities compared to the other evaluation datasets, there were no significant gains for the adjustments with semantic intensity orders.", "labels": [], "entities": [{"text": "SimLex-999 dataset", "start_pos": 6, "end_pos": 24, "type": "DATASET", "confidence": 0.9222759902477264}]}, {"text": "However, no significant drops indicate that the adjustments with semantic intensity orders are not harmful for the representation of general word semantics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Percentage of adjective pairs and the  maximum number of Turkers who agree with each  other on the annotation.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.965651273727417}]}, {"text": " Table 2: F1 scores for determining semantic intensity ordering on three datasets, across three baseline  models (GloVe, CBOW, Paragram), using different compositions of adjustment techniques, including  synonyms, antonyms, same intensity orders, and different intensity orders.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9986771941184998}, {"text": "determining semantic intensity ordering", "start_pos": 24, "end_pos": 63, "type": "TASK", "confidence": 0.5956684201955795}]}, {"text": " Table 3: The numbers of total adjective pairs, syn- onymous pairs, and antonymous pairs for each  dataset.", "labels": [], "entities": []}, {"text": " Table 6: Spearman's \u03c1 on SimLex-999.", "labels": [], "entities": []}]}