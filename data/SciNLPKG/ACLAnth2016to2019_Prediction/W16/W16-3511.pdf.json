{"title": [{"text": "Generating Paraphrases from DBPedia using Deep Learning", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9336749315261841}]}], "abstractContent": [{"text": "Recent deep learning approaches to Natural Language Generation mostly rely on sequence-to-sequence models.", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.672013501326243}]}, {"text": "In these approaches, the input is treated as a sequence whereas inmost cases, input to generation usually is either a tree or a graph.", "labels": [], "entities": []}, {"text": "In this paper, we describe an experiment showing how enriching a sequential input with structural information improves results and help support the generation of paraphrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Following work by), there has been much work recently on using deep learning techniques to generate text from data.) uses recurrent neural network to generate text from dialog speech acts.", "labels": [], "entities": []}, {"text": "Using biography articles and infoboxes from the WikiProject Biography, () learns a conditional neural language model to generate text from infoboxes. etc.", "labels": [], "entities": [{"text": "WikiProject Biography", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.9421066343784332}]}, {"text": "A basic feature of these approaches is that both the input and the output data is represented as a sequence so that generation can then be modeled using a Long Short Term Memory Model (LSTM) or a conditional language model.", "labels": [], "entities": []}, {"text": "Mostly however, the data taken as input by natural language generation systems is tree or graph structured, not linear.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a constrained generation approach where the input is enriched with constraints on the syntactic shape of the sentence to be generated.", "labels": [], "entities": []}, {"text": "As illustrated in, there is a strong correlation between the shape A was born in E.", "labels": [], "entities": []}, {"text": "She worked as an engineer.", "labels": [], "entities": []}, {"text": "A was born in E and worked as an engineer. of the input and the shape of the corresponding sentence.", "labels": [], "entities": []}, {"text": "The chaining structure T 1 where B is shared by two predications (mission and operator) will favour the use of a participial or a passive subject relative clause.", "labels": [], "entities": []}, {"text": "In contrast, the tree structure T 2 will favour the use of anew clause with pronominal subject or a coordinated VP.", "labels": [], "entities": []}, {"text": "Using synthetic data, we explore different ways of integrating structural constraints in the training data.", "labels": [], "entities": []}, {"text": "We focus on the following two questions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the results by computing the BLEU-4 score of the generated sentences against the reference sentence.", "labels": [], "entities": [{"text": "BLEU-4 score", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9807974398136139}]}, {"text": "The baseline and the R+I model have very low results.", "labels": [], "entities": []}, {"text": "For the baseline model, this indicates that training on a corpus where the same input is associated with several distinct paraphrases make it difficult to learn a good data-to-text generation model.", "labels": [], "entities": []}, {"text": "The marked difference between the R+I and the RI+C model shows that simply associating each input with an identifier labelling the syntactic structure of the associated sentence is not sufficient to learn a model that should predict different syntactic structures for differently labelled inputs.", "labels": [], "entities": []}, {"text": "Interestingly, training on a corpus where the input data is enriched with infixed connectors giving indications about the structure of the associated sentence yields much better results.", "labels": [], "entities": []}], "tableCaptions": []}