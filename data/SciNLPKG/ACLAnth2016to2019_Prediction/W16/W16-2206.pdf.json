{"title": [], "abstractContent": [{"text": "Neural machine translation (NMT) has emerged recently as a promising statistical machine translation approach.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8328309853871664}, {"text": "statistical machine translation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.6319564481576284}]}, {"text": "In NMT, neural networks (NN) are directly used to produce translations, without relying on a pre-existing translation framework.", "labels": [], "entities": []}, {"text": "In this work, we take a step towards bridging the gap between conventional word alignment models and NMT.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.7647964358329773}]}, {"text": "We follow the hidden Markov model (HMM) approach that separates the alignment and lexical models.", "labels": [], "entities": []}, {"text": "We propose a neural alignment model and combine it with a lexical neural model in a log-linear framework.", "labels": [], "entities": [{"text": "neural alignment", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7431323230266571}]}, {"text": "The models are used in a standalone word-based decoder that explicitly hypothesizes alignments during search.", "labels": [], "entities": []}, {"text": "We demonstrate that our system outperforms attention-based NMT on two tasks: IWSLT 2013 German\u2192English and BOLT Chinese\u2192English.", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English", "start_pos": 77, "end_pos": 102, "type": "DATASET", "confidence": 0.8051573395729065}, {"text": "BOLT", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.917792797088623}]}, {"text": "We also show promising results for realigning the training data using neural models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural networks have been gaining a lot of attention recently in areas like speech recognition, image recognition and natural language processing.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.760032594203949}, {"text": "image recognition", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7535832822322845}, {"text": "natural language processing", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.6348638435204824}]}, {"text": "In machine translation, NNs are applied in two main ways: In N -best rescoring, the neural model is used to score the first-pass decoding output, limiting the model to a fixed set of hypotheses ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7653650641441345}]}, {"text": "The second approach integrates the NN into decoding, potentially allowing it to directly determine the search space.", "labels": [], "entities": []}, {"text": "There are two approaches to use neural models in decoding.", "labels": [], "entities": []}, {"text": "The first integrates the models into phrase-based decoding, where the models are used to score phrasal candidates hypothesized by the decoder (.", "labels": [], "entities": []}, {"text": "The second approach is referred to as neural machine translation, where neural models are used to hypothesize translations, word byword, without relying on a pre-existing framework.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7050838073094686}]}, {"text": "In comparison to the former approach, NMT does not restrict NNs to predetermined translation candidates, and it does not depend on word alignment concepts that have been part of building state-of-the-art phrase-based systems.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.7369250804185867}]}, {"text": "In such systems, the HMM and the IBM models developed more than two decades ago are used to produce Viterbi word alignments, which are used to build standard phrase-based systems.", "labels": [], "entities": [{"text": "Viterbi word alignments", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.5439908107121786}]}, {"text": "Existing NMT systems either disregard the notion of word alignments entirely (, or rely on a probabilistic notion of alignments ( ) independent of the conventional alignment models.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7027723789215088}]}, {"text": "Most recently, designed neural models that incorporate concepts like fertility and Markov conditioning into their structure.", "labels": [], "entities": []}, {"text": "In this work, we also focus on the question whether conventional word alignment concepts can be used for NMT.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.715477243065834}, {"text": "NMT", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.8874448537826538}]}, {"text": "In particular, (1) We follow the HMM approach to separate the alignment and translation models, and use neural networks to model alignments and translation.", "labels": [], "entities": []}, {"text": "(2) We introduce a lexicalized alignment model to capture source reordering information.", "labels": [], "entities": []}, {"text": "(3) We bootstrap the NN training using Viterbi word alignments obtained from the HMM and IBM model training, and use the trained neural models to generate new alignments.", "labels": [], "entities": [{"text": "IBM model training", "start_pos": 89, "end_pos": 107, "type": "DATASET", "confidence": 0.8982652823130289}]}, {"text": "The new alignments are then used to re-train the neural networks.", "labels": [], "entities": []}, {"text": "(4) We design an alignment-based decoder that hypothesizes the alignment path along with the associated translation.", "labels": [], "entities": []}, {"text": "We show competitive results in comparison to attention-based models on the IWSLT 2013 German\u2192English and BOLT Chinese\u2192English task.", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English and BOLT Chinese\u2192English task", "start_pos": 75, "end_pos": 130, "type": "DATASET", "confidence": 0.7821592146700079}]}], "datasetContent": [{"text": "We carryout experiments on two tasks: the IWSLT 2013 German\u2192English shared translation task, 1 and the BOLT Chinese\u2192English task.", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English shared translation task", "start_pos": 42, "end_pos": 91, "type": "TASK", "confidence": 0.8057650402188301}, {"text": "BOLT", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9533277153968811}]}, {"text": "The corpora statistics are shown in.", "labels": [], "entities": []}, {"text": "The IWSLT phrase-based baseline system is trained on all available bilingual data, and uses a 4-gram LM with modified Kneser-Ney smoothing, trained with the SRILM toolkit).", "labels": [], "entities": []}, {"text": "As additional data sources for the LM, we selected parts of the Shuffled News and LDC English Gigaword corpora based on the cross-entropy difference (, resulting in a total of 1.7 billion running words for LM training.", "labels": [], "entities": [{"text": "Shuffled News and LDC English Gigaword corpora", "start_pos": 64, "end_pos": 110, "type": "DATASET", "confidence": 0.8259992514337812}]}, {"text": "The phrase-based baseline is a standard phrasebased SMT system () tuned with MERT and contains a hierarchical reordering model (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8743954300880432}]}, {"text": "The in-domain data consists of 137K sentences.", "labels": [], "entities": []}, {"text": "The BOLT Chinese\u2192English task is evaluated on the \"discussion forum\" domain.", "labels": [], "entities": [{"text": "BOLT", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8013641834259033}]}, {"text": "We use a 5-gram LM trained on 2.9 billion running words in total.", "labels": [], "entities": []}, {"text": "The in-domain data consists of a subset of 67.8K sentences.", "labels": [], "entities": []}, {"text": "We used a set of 1845 sentences as a tune set.", "labels": [], "entities": []}, {"text": "The evaluation set test1 contains 1844 and test2 contains 1124 sentences.", "labels": [], "entities": []}, {"text": "We use the FFNN architecture for the lexical and alignment models.", "labels": [], "entities": []}, {"text": "Both models use a window of 9 source words, and 5 target history words.", "labels": [], "entities": []}, {"text": "Both models use two hidden layers, the first has 1000 units and the second has 500 units.", "labels": [], "entities": []}, {"text": "The lexical model uses a class-factored output layer, with 1000 singleton classes dedicated to the most frequent words, and 1000 classes shared among the rest of the words.", "labels": [], "entities": []}, {"text": "The classes are trained using a separate tool to optimize the maximum likelihood training criterion with the bigram assumption.", "labels": [], "entities": []}, {"text": "The alignment model uses a small output layer of 201 nodes, determined by a maximum jump length of 100 (forward and backward).", "labels": [], "entities": []}, {"text": "300 nodes are used for word embeddings.", "labels": [], "entities": []}, {"text": "Each of the FFNN models is trained on CPUs using 12 threads, which takes up to 3 days until convergence.", "labels": [], "entities": [{"text": "FFNN", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.6245018243789673}]}, {"text": "We train with stochastic gradient descent using a batch size of 128.", "labels": [], "entities": []}, {"text": "The learning rate is halved when the development perplexity increases.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9536182284355164}]}, {"text": "Each BJM has 4 LSTM layers: two for the forward and backward states, one for the target state, and one after merging the source and target states.", "labels": [], "entities": [{"text": "BJM", "start_pos": 5, "end_pos": 8, "type": "DATASET", "confidence": 0.8207789063453674}]}, {"text": "The size of the word embeddings and hidden layers is 350 nodes.", "labels": [], "entities": []}, {"text": "The output layers are identical to those of the FFJM models.", "labels": [], "entities": [{"text": "FFJM", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.8041715621948242}]}, {"text": "We compare our system to an attention-based baseline similar to the networks described in).", "labels": [], "entities": []}, {"text": "All such systems use single models, rather than ensembles.", "labels": [], "entities": []}, {"text": "The word embedding dimension is 620, each direction of the encoder and the decoder has a layer of 1000 gated recurrent units ().", "labels": [], "entities": []}, {"text": "Unknowns and numbers are carried out from the source side to the target side based on the largest attention weight.", "labels": [], "entities": []}, {"text": "To speedup decoding of long sentences, the decoder hypothesizes 21 and 41 source positions around the diagonal, for the IWSLT and the BOLT tasks, respectively.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 120, "end_pos": 125, "type": "DATASET", "confidence": 0.611640214920044}, {"text": "BOLT", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9192836880683899}]}, {"text": "We choose these numbers such that the translation quality does not degrade.", "labels": [], "entities": []}, {"text": "The beam size is set to 16 in all experiments.", "labels": [], "entities": []}, {"text": "Larger beam sizes did not lead to improvements.", "labels": [], "entities": []}, {"text": "We apply part-of-speech-based long-range verb reordering rules to the German side in a preprocessing step for all German\u2192English systems), including the baselines.", "labels": [], "entities": [{"text": "part-of-speech-based long-range verb reordering", "start_pos": 9, "end_pos": 56, "type": "TASK", "confidence": 0.6212150007486343}]}, {"text": "The Chinese\u2192English systems use no such preordering.", "labels": [], "entities": []}, {"text": "We use the GIZA++ word alignments to train the models.", "labels": [], "entities": []}, {"text": "The networks are fine-tuned by training additional epochs on the in-domain data only . The LMs are only used in the phrase-based systems in both tasks, but not in the NMT systems.", "labels": [], "entities": []}, {"text": "All translation experiments are performed with the Jane toolkit ().", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9774283766746521}, {"text": "Jane toolkit", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9808251559734344}]}, {"text": "The alignment-based NNs are trained using an extension of the rwthlm toolkit ().", "labels": [], "entities": []}, {"text": "We use an implementation based on Blocks) and Theano () for the attention-based experiments.", "labels": [], "entities": []}, {"text": "All results are mea- shows the IWSLT German\u2192English results.", "labels": [], "entities": [{"text": "IWSLT German\u2192English", "start_pos": 31, "end_pos": 51, "type": "DATASET", "confidence": 0.8188328444957733}]}, {"text": "FFJM refers to feed-forward lexical model.", "labels": [], "entities": [{"text": "FFJM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8594961166381836}]}, {"text": "We compare against the phrase-based system with an LM trained on the target side of the bilingual data (row #1), the phrase-based system with an LM trained on additional monolingual data (row #2), the attention-based system (row #3), and the attention-based system after fine-tuning towards the in-domain data (row #4).", "labels": [], "entities": []}, {"text": "First, we experiment with a system using the FFJM as a lexical model and a linear distortion penalty (dp) to encourage monotone translation as the alignment model.", "labels": [], "entities": [{"text": "FFJM", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.886745810508728}, {"text": "linear distortion penalty (dp)", "start_pos": 75, "end_pos": 105, "type": "METRIC", "confidence": 0.9289439022541046}]}, {"text": "We also include a word penalty (wp).", "labels": [], "entities": [{"text": "word penalty (wp)", "start_pos": 18, "end_pos": 35, "type": "METRIC", "confidence": 0.9048000335693359}]}, {"text": "This system is shown in row #5.", "labels": [], "entities": []}, {"text": "In comparison, if the distortion penalty is replaced by the feed-forward alignment model (FFAM), we observe large improvements of 4.5% to 5.2% BLEU (row #5 vs. #6).", "labels": [], "entities": [{"text": "distortion", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9845948219299316}, {"text": "feed-forward alignment model (FFAM)", "start_pos": 60, "end_pos": 95, "type": "METRIC", "confidence": 0.7176010012626648}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9992831349372864}]}, {"text": "This highlights the significant role of the alignment model in our system.", "labels": [], "entities": []}, {"text": "Moreover, it indicates that the FFAM is able to model alignments beyond the simple monotone alignments preferred by the distortion penalty.", "labels": [], "entities": [{"text": "FFAM", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.4826372563838959}, {"text": "distortion", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.965174674987793}]}, {"text": "Fine-tuning the neural networks towards indomain data improves the system by up to 3.3% BLEU and 2.9% TER (row #6 vs #7).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9995476603507996}, {"text": "TER", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9989643096923828}]}, {"text": "The gain from fine-tuning is larger than the one observed for the attention-based system.", "labels": [], "entities": []}, {"text": "This is likely due to the fact that our system has two neural models, and each of them is fine-tuned.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: IWSLT 2013 German\u2192English results in  BLEU [%] and TER [%].", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.7693762302398681}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9993294477462769}, {"text": "TER", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9976931214332581}]}, {"text": " Table 3: BOLT Chinese\u2192English results in BLEU  [%] and TER [%].", "labels": [], "entities": [{"text": "BOLT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.96169513463974}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9993543028831482}, {"text": "TER", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.998339056968689}]}, {"text": " Table 4: Re-alignment results in BLEU [%] and  TER [%] on the IWSLT 2013 German\u2192English  in-domain data. Each system includes FFJM,  FFAM and word penalty.", "labels": [], "entities": [{"text": "Re-alignment", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9278984069824219}, {"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9994949102401733}, {"text": "TER", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9983417987823486}, {"text": "IWSLT 2013 German\u2192English  in-domain data", "start_pos": 63, "end_pos": 104, "type": "DATASET", "confidence": 0.9516000151634216}, {"text": "FFJM", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.47290658950805664}, {"text": "FFAM", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.5340681672096252}]}]}