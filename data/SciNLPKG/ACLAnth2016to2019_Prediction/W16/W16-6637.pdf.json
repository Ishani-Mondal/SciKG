{"title": [{"text": "An Analysis of the Ability of Statistical Language Models to Capture the Structural Properties of Language", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate the characteristics and quan-tifiable predispositions of both n-gram and recurrent neural language models in the framework of language generation.", "labels": [], "entities": [{"text": "language generation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.7202877104282379}]}, {"text": "In modern applications , neural models have been widely adopted, as they have empirically provided better results.", "labels": [], "entities": []}, {"text": "However, there is alack of deep analysis of the models and how they relate to real language and its structural properties.", "labels": [], "entities": []}, {"text": "We attempt to perform such an investigation by analyzing corpora generated by sampling from the models.", "labels": [], "entities": []}, {"text": "The results are compared to each other and to the results of the same analysis applied to the training corpus.", "labels": [], "entities": []}, {"text": "We carried out these experiments on varieties of Kneser-Ney smoothed n-gram models and basic recurrent neural language models.", "labels": [], "entities": []}, {"text": "Our results reveal a number of distinctive characteristics of each model, and offer insights into their behavior.", "labels": [], "entities": []}, {"text": "Our general approach also provides a framework in which to perform further analysis of language models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical language modelling is critical to natural language processing and many generation systems.", "labels": [], "entities": [{"text": "Statistical language modelling", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8340682784716288}, {"text": "natural language processing", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.6604368289311727}]}, {"text": "In recent years use has shifted from the previously prevalent n-gram model to the recurrent neural network paradigm that now dominates inmost applications.", "labels": [], "entities": []}, {"text": "Researchers have long sought to find the best language modeling solutions for particular applications, but it is important to understand the behavior of language models in a more generalizable way.", "labels": [], "entities": []}, {"text": "This is advantageous both in developing language models and in applying them practically.", "labels": [], "entities": []}, {"text": "Whether in tasks where statistical models are used to directly generate language or in cases where the model is used for ranking for surface realization, the statistical predispositions of the language model will be reflected in the results.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7520212829113007}]}, {"text": "In this paper we compare the behavior of n-gram models and Recurrent Neural Network Language Models (RNNLMs) with regard to properties of their generated language.", "labels": [], "entities": []}, {"text": "We use the SRILM toolkit for training and generating from n-gram models).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.8563321530818939}]}, {"text": "Our n-gram model is a modified KneserNey back-off interpolative model, unless otherwise stated).", "labels": [], "entities": []}, {"text": "We use Tomas Mikolov's implementation of an RNNLM, available at rnnlm.org.", "labels": [], "entities": []}, {"text": "This model has a single hidden recurrent layer, and three defining parameters: class size, hidden layer size, and backpropagation through time (BPTT) steps.", "labels": [], "entities": []}, {"text": "Classes are used to factor the vocabulary mappings to improve performance, by predicting a distribution over classes of words and then over words in a class).", "labels": [], "entities": []}, {"text": "BPTT steps determine how many times the recurrent layer of the network is unwrapped for training.", "labels": [], "entities": [{"text": "BPTT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.4293159544467926}]}, {"text": "Unless otherwise mentioned all neural models have class of 100 and use four BPTT steps.", "labels": [], "entities": [{"text": "BPTT", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.8846567273139954}]}, {"text": "We use the Penn Tree Bank (PTB), constructed from articles from the Wall Street Journal, as our primary training corpus, with the standard training split of 42068 sentences.", "labels": [], "entities": [{"text": "Penn Tree Bank (PTB)", "start_pos": 11, "end_pos": 31, "type": "DATASET", "confidence": 0.980821043252945}, {"text": "Wall Street Journal", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.8523412744204203}]}, {"text": "Correspondingly, our generated language corpora also contain 42068 sentences.", "labels": [], "entities": []}, {"text": "Novel sentences are easily sampled from trained language models by prompting with a start of sentence token, We select three primary metrics with which to evaluate the various resulting corpora.", "labels": [], "entities": []}, {"text": "The first is the distribution of sentence lengths.", "labels": [], "entities": []}, {"text": "Sentence length is compared visually and through the sum of error as compared to the length distribution from the training corpus.", "labels": [], "entities": []}, {"text": "The second metric is word frequency.", "labels": [], "entities": []}, {"text": "Word frequency is analyzed by fitting a Zipfian distribution, and comparing between the distributions for each model.", "labels": [], "entities": []}, {"text": "Third is pronoun frequency relative to distance from the start of a sentence.", "labels": [], "entities": [{"text": "frequency", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.7097752094268799}]}, {"text": "This was selected as a metric due to the fact that one-word pronouns area small class fairly easily identifiable regardless of context (though there area few that can be other parts of speech), partly avoiding the ambiguities and challenges that follow from part of speech taggers.", "labels": [], "entities": []}, {"text": "This is especially useful in a corpus with a restricted vocabulary resulting in the replacement of uncommon tokens with a single token, such as the PTB, and with generated language that is not always semantically sound.", "labels": [], "entities": [{"text": "PTB", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.8392512202262878}]}, {"text": "These experiments were repeated multiple times with small variations, ensuring the key patterns in the results were not a product of chance.", "labels": [], "entities": []}, {"text": "Through these three metrics we seek to develop some insights into the behavior of standard stochastic models in language generation.", "labels": [], "entities": [{"text": "language generation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7180711179971695}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Sum of errors for sentence lengths, including normal-", "labels": [], "entities": [{"text": "Sum", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8312652707099915}, {"text": "errors", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.7563197016716003}]}, {"text": " Table 2: Zipf fit parameters s with Log-Likelihood.", "labels": [], "entities": []}]}