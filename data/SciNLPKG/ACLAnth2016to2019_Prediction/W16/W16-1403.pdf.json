{"title": [{"text": "Cross-Lingual Question Answering Using Common Semantic Space", "labels": [], "entities": [{"text": "Cross-Lingual Question Answering", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6915374994277954}]}], "abstractContent": [{"text": "With the advent of Big Data concept, a lot of attention has been paid to structuring and giving semantic to this data.", "labels": [], "entities": []}, {"text": "Knowledge bases like DBPedia play an important role to achieve this goal.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9514847993850708}]}, {"text": "Question answering systems are common approach to address ex-pressivity and usability of information extraction from knowledge bases.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8830689191818237}, {"text": "information extraction from knowledge bases", "start_pos": 89, "end_pos": 132, "type": "TASK", "confidence": 0.8560541152954102}]}, {"text": "Recent researches focused only on monolingual QA systems while cross-lingual setting has still so many barriers.", "labels": [], "entities": []}, {"text": "In this paper we introduce anew cross-lingual approach using a unified semantic space among languages.", "labels": [], "entities": []}, {"text": "After keyword extraction, entity linking and answer type detection , we use cross lingual semantic similarity to extract the answer from knowledge base via relation selection and type matching.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7837844491004944}, {"text": "entity linking", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7688379287719727}, {"text": "answer type detection", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7849591573079427}, {"text": "type matching", "start_pos": 179, "end_pos": 192, "type": "TASK", "confidence": 0.7291205674409866}]}, {"text": "We have evaluated our approach on Persian and Spanish which are typologically different languages.", "labels": [], "entities": []}, {"text": "Our experiments are on DBPedia.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.9737108945846558}]}, {"text": "The results are promising for both languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Large scale knowledge bases like) and Freebase ( provide structured information in diverse domains.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9750999212265015}]}, {"text": "Such resources are worthwhile to answer opendomain questions using structured query.", "labels": [], "entities": []}, {"text": "In recent years, answering open-domain questions by querying knowledge bases has gained a lot of attentions ().", "labels": [], "entities": []}, {"text": "These systems exploit many diverse methods like semantic parsing, information extraction ( ) and deep learning (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7577240169048309}, {"text": "information extraction", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.8280601501464844}]}, {"text": "While existing approaches focused only on English language, there are so many difficulties to cope within cross lingual setting.", "labels": [], "entities": []}, {"text": "On the one hand, lack of tools and resources, and on the other hand, vocabulary gap between source and target languages, frustrate any effort to adapt the existing approaches for languages other than English.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a pipeline of stages for cross lingual question answering over knowledge bases.", "labels": [], "entities": [{"text": "cross lingual question answering", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.7045476585626602}]}, {"text": "In the first stage, using a MaxEnt Markov Model, keywords are extracted.", "labels": [], "entities": []}, {"text": "Syntactic and semantic features are utilized to do this job.", "labels": [], "entities": []}, {"text": "In the second stage, using an SVM classifier, keywords that mention an entity are distinguished from ones that determine the answer type.", "labels": [], "entities": []}, {"text": "In the next stage we try to find the most probable entity(s) in KB which can be linked to detected grounded entity(s).", "labels": [], "entities": []}, {"text": "Several sources are used to find entities in KB, like abstract of entities in KB, cross lingual dictionaries like BabelNet () and the KB own cross lingual links (whenever such links exist).", "labels": [], "entities": []}, {"text": "Also using extracted keywords we search in the ontology of the KB to predict type of entities that are answers.", "labels": [], "entities": []}, {"text": "In the last stage, answers are extracted using two kinds of information: 1.", "labels": [], "entities": []}, {"text": "Type of neighbours of found entities 2.", "labels": [], "entities": []}, {"text": "Semantic similarity between relation labels of found entities and extracted keywords.", "labels": [], "entities": []}, {"text": "Cross lingual semantic similarity are measured using the unified semantic space among languages proposed by Camacho-Collados (2015).", "labels": [], "entities": []}, {"text": "Our system doesn't rely on huge annotated data or any language specific resources except fora chunker.", "labels": [], "entities": []}, {"text": "Thus our main contributions are: \u2022 Introducing a staged cross lingual approach which can easily be adapted to any source language with an available chunker in that language.", "labels": [], "entities": []}, {"text": "\u2022 Reducing annotation effort and reliance on huge amount of training data which is a barrier for many resource scarce languages.", "labels": [], "entities": []}, {"text": "\u2022 Providing anew QA dataset for Persian and conducting experiences on two different languages.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.7440073192119598}]}], "datasetContent": [{"text": "QALD-5 is a multilingual QA dataset over DBPedia 2014 for QALD task at CLEF 2015.", "labels": [], "entities": [{"text": "QALD-5", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9558396339416504}, {"text": "DBPedia 2014", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.8774501085281372}, {"text": "QALD task at CLEF 2015", "start_pos": 58, "end_pos": 80, "type": "DATASET", "confidence": 0.5145629405975342}]}, {"text": "It contains 300 training questions in 7 languages with annotated keywords and queries to extract answers from DBPedia and 50 questions as test set.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.9580513834953308}]}, {"text": "To add Persian translation to these dataset, the questions were translated to Persian by a language expert outside development team.", "labels": [], "entities": [{"text": "Persian translation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7129273414611816}]}, {"text": "To annotate keywords of each Persian questions we have used majority voting among 5 annotators.", "labels": [], "entities": []}, {"text": "Each word has tagged as B, I or O.", "labels": [], "entities": [{"text": "B", "start_pos": 24, "end_pos": 25, "type": "METRIC", "confidence": 0.989370584487915}]}, {"text": "Also we have augmented this dataset with answer type tag.", "labels": [], "entities": []}, {"text": "Each keyword has tagged as type detector or neutral.", "labels": [], "entities": []}, {"text": "We have chosen these tags through majority voting among 5 annotators.", "labels": [], "entities": []}, {"text": "To evaluate our approach we have conducted experiments on Persian and Spanish.", "labels": [], "entities": []}, {"text": "we have used DBPedia 2014 as the KB that answers must be extracted from it.", "labels": [], "entities": [{"text": "DBPedia 2014", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.978165864944458}]}, {"text": "We have tested our system on QALD-5 test set.", "labels": [], "entities": [{"text": "QALD-5 test set", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9591052134831747}]}, {"text": "It contains 49 questions in both languages.", "labels": [], "entities": []}, {"text": "As a baseline we translate each question to English using Google Translate.", "labels": [], "entities": []}, {"text": "shows the result of our approach for both Persian and Spanish questions compared with results of the baseline.", "labels": [], "entities": []}, {"text": "Errors in translating of named entities in fully translating a question is one of main sources of errors in baseline with proportion of 64%.", "labels": [], "entities": [{"text": "translating of named entities in fully translating a question", "start_pos": 10, "end_pos": 71, "type": "TASK", "confidence": 0.7363281779819064}]}, {"text": "We have compared the performance of the monolingual version of our approach with the best participant of QALD-5 challenge.", "labels": [], "entities": []}, {"text": "Despite less annotation cost for training the model compared with Xser, our system improved F 1 by 2.2%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9925656020641327}]}, {"text": "Since our proposed approach consist of a pipeline of pre-processing, we have evaluated internal stages of our system.", "labels": [], "entities": []}, {"text": "shows the results for each stage.", "labels": [], "entities": []}, {"text": "The reported accuracies are average accuracy over 10-fold cross validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9988096952438354}]}, {"text": "We have also evaluated the influence of calculating semantic similarity using unified vectors on accuracy of our method.: Performance of our method with unified vectors and Word2vec semantic similarity translation of a Persian or Spanish phrase and an English phrase using Word2Vec was used as a baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9995682835578918}, {"text": "Word2vec semantic similarity translation of a Persian or Spanish phrase", "start_pos": 173, "end_pos": 244, "type": "TASK", "confidence": 0.6313594281673431}, {"text": "Word2Vec", "start_pos": 273, "end_pos": 281, "type": "DATASET", "confidence": 0.9320553541183472}]}, {"text": "The result for Persian are reported in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of our approach for Persian and Spanish", "labels": [], "entities": []}, {"text": " Table 3: Performance of each stage.", "labels": [], "entities": []}]}