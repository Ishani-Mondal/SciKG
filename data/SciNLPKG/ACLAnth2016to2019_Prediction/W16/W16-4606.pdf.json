{"title": [{"text": "Global Pre-ordering for Improving Sublanguage Translation", "labels": [], "entities": [{"text": "Improving Sublanguage Translation", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.7649305462837219}]}], "abstractContent": [{"text": "When translating formal documents, capturing the sentence structure specific to the sublanguage is extremely necessary to obtain high-quality translations.", "labels": [], "entities": []}, {"text": "This paper proposes a novel global reordering method with particular focus on long-distance reordering for capturing the global sentence structure of a sublanguage.", "labels": [], "entities": []}, {"text": "The proposed method learns global reordering models from a non-annotated parallel corpus and works in conjunction with conventional syntactic reordering.", "labels": [], "entities": []}, {"text": "Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations.", "labels": [], "entities": [{"text": "RIBES metric", "start_pos": 109, "end_pos": 121, "type": "METRIC", "confidence": 0.9612022638320923}, {"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9995065927505493}]}], "introductionContent": [{"text": "Formal documents such as legal and technical documents often form sublanguages.", "labels": [], "entities": []}, {"text": "Previous studies have highlighted that capturing the sentence structure specific to the sublanguage is extremely necessary for obtaining high-quality translations especially between distant languages ().", "labels": [], "entities": []}, {"text": "illustrates two pairs of bilingual sentences specific to the sublanguage of patent abstracts.", "labels": [], "entities": []}, {"text": "In both sentence pairs, the global sentence structure ABC in the source sentences must be reordered to CBA in the target sentences to produce a structurally appropriate translation.", "labels": [], "entities": []}, {"text": "Each of the components ABC must then be syntactically reordered to complete the reordering.", "labels": [], "entities": []}, {"text": "Various attempts have been made along this line of research.", "labels": [], "entities": []}, {"text": "One such method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees and uses conventional SMT to train global reordering).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.8126005033651987}]}, {"text": "However, the performance of this method is limited by syntactic parsing, therefore the global reordering has low accuracy where the accuracy of syntactic parsing is low.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.6058045625686646}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9991242289543152}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9990711212158203}, {"text": "syntactic parsing", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.6146809905767441}]}, {"text": "Another approach involves manually preparing synchronous context-free grammar rules for capturing the global sentence structure of the target sublanguage (.", "labels": [], "entities": []}, {"text": "However, this method requires manual preparation of rules.", "labels": [], "entities": []}, {"text": "Both methods are unsuitable for formal documents such as patent abstracts, because they fail to adapt to sentences with various expressions, for which manual preparation of rules is complex.", "labels": [], "entities": []}, {"text": "This paper describes a novel global reordering method for capturing sublanguage-specific global sentence structure to supplement the performance of conventional syntactic reordering.", "labels": [], "entities": []}, {"text": "The method learns a global pre-ordering model from non-annotated corpora without using syntactic parsing and uses this model to perform global pre-ordering on newly inputted sentences.", "labels": [], "entities": []}, {"text": "As the global pre-ordering method does not rely on syntactic parsing, it is not affected by the degradation of parsing accuracy, and is readily applicable to new sublanguages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.912916898727417}]}, {"text": "Globally pre-ordered sentence segments are then syntactically reordered before being translated by SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9611299633979797}]}, {"text": "In this empirical study on the patent abstract sublanguage in Japanese-to-English and English-toJapanese translations, the translation quality of the sublanguage was improved when global pre-ordering Pair 1 Japanese  was combined with syntactic pre-ordering.", "labels": [], "entities": []}, {"text": "A statistically significant improvement was observed against the syntactic pre-ordering alone, and a substantial gain of more than 25 points in RIBES score against the baseline was observed for both Japanese-to-English and English-to-Japanese translations, and the BLEU scores remained comparable.", "labels": [], "entities": [{"text": "RIBES score", "start_pos": 144, "end_pos": 155, "type": "METRIC", "confidence": 0.9611747860908508}, {"text": "BLEU", "start_pos": 265, "end_pos": 269, "type": "METRIC", "confidence": 0.9993884563446045}]}], "datasetContent": [{"text": "In this section, we first describe the reordering configuration for depicting the effect of global preordering.", "labels": [], "entities": []}, {"text": "We then describe the primary preparation of global reordering, followed by a description of the settings used in our translation experiment.", "labels": [], "entities": [{"text": "translation experiment", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.896553635597229}]}, {"text": "Data As our experimental data, we use the Patent Abstracts of Japan (PAJ), the English translations of Japanese patent abstracts.", "labels": [], "entities": [{"text": "Patent Abstracts of Japan (PAJ)", "start_pos": 42, "end_pos": 73, "type": "DATASET", "confidence": 0.6370004202638354}]}, {"text": "We automatically align () PAJ with the corresponding original Japanese abstracts, from which we randomly select 1,000,000 sentence pairs for training, 1,000 for development and 1,000 for testing.", "labels": [], "entities": []}, {"text": "This training data for the translation experiment are also used for training global reordering as described in the previous subsection.", "labels": [], "entities": [{"text": "translation", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.9664804935455322}]}, {"text": "Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation.", "labels": [], "entities": []}, {"text": "In our experiments, the number of sentences actually used for evaluation is 300.", "labels": [], "entities": []}, {"text": "Baseline SMT The baseline system for our experiment is Moses phrase-based SMT ( with the default distortion limit of six.", "labels": [], "entities": [{"text": "Baseline SMT", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.4803377091884613}, {"text": "SMT", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.7644621133804321}]}, {"text": "We use for training language models and SyMGIZA++ (Junczys-Dowmunt and Szal, 2010) for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.8149214684963226}]}, {"text": "The weights of the models are tuned with the n-best batch MIRA (Cherry and Foster, 2012).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.8842079639434814}]}, {"text": "As variants of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based () SMT with the default maximum chart span often.", "labels": [], "entities": []}, {"text": "Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berkeley Parser.", "labels": [], "entities": [{"text": "Berkeley Parser", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.9179339110851288}]}, {"text": "The input sentences are parsed using the Berkeley Parser, and the binary nodes are swapped by the classifier (.", "labels": [], "entities": []}, {"text": "As a variant of conventional reordering, we also use a reordering model based on the top-down bracketing transducer grammar (TDBTG).", "labels": [], "entities": []}, {"text": "We use the output of mkcls and SyMGIZA++ obtained during the preparation of the baseline SMT for training TDBTG-based reordering.", "labels": [], "entities": [{"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9806233644485474}]}, {"text": "Global pre-ordering Global pre-ordering consists of the detection of segment boundaries and the reordering of the detected segments.", "labels": [], "entities": []}, {"text": "Out of the 1,000,000 phrase-aligned sentence pairs in the training set for SMT, we use the first 100,000 sentence pairs for extracting the sentence pairs containing global reordering.", "labels": [], "entities": [{"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9944762587547302}]}, {"text": "We only use a portion of the SMT training data due to the slow execution speed of the current implementation of the software program for extracting sentence pairs containing global reordering.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.9005647301673889}]}, {"text": "We evaluate both the heuristic and the machine learning-based methods for comparison.", "labels": [], "entities": []}, {"text": "Evaluation metrics We use the RIBES) and the BLEU ( scores as evaluation metrics.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.8827188611030579}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9988788962364197}]}, {"text": "We use both metrics because n-gram-based metrics such as BLEU alone cannot fully illustrate the effects of global reordering.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9968829154968262}]}, {"text": "RIBES is an evaluation metric based on rank correlation which measures long-range relationships and is reported to show much higher correlation with human evaluation than BLEU for evaluating document translations between distant languages (Isozaki and Kouchi, 2015).", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8851693868637085}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9977699518203735}, {"text": "document translations between distant languages", "start_pos": 191, "end_pos": 238, "type": "TASK", "confidence": 0.8027082324028015}]}], "tableCaptions": [{"text": " Table 1: Evaluation of Japanese-to-English translation where glob-pre denotes global pre-ordering and  pre denotes conventional syntactic pre-ordering, dl denotes distortion limit, HPB denotes hierarchical  phrase-based SMT and TDBTG denotes reordering based on top-down bracketing transduction gram- mar. The bold numbers indicate a statistically insignificant difference from the best system performance  according to the bootstrap resampling method at p = 0.05", "labels": [], "entities": []}, {"text": " Table 2: Evaluation of English-to-Japanese translation  Reordering config  Settings  Results  glob-pre  pre  SMT  glob-pre  pre  RIBES  BLEU", "labels": [], "entities": [{"text": "SMT  glob-pre  pre  RIBES  BLEU", "start_pos": 110, "end_pos": 141, "type": "METRIC", "confidence": 0.5926229655742645}]}, {"text": " Table 3: Number of correct global reordering in 100 test sentences", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9676121473312378}]}]}