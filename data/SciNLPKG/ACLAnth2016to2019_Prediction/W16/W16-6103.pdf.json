{"title": [{"text": "Modelling Radiological Language with Bidirectional Long Short-Term Memory Networks", "labels": [], "entities": []}], "abstractContent": [{"text": "Motivated by the need to automate medical information extraction from free-text radiological reports, we present a bi-directional long short-term memory (BiLSTM) neural network architecture for modelling radiological language.", "labels": [], "entities": [{"text": "medical information extraction from free-text radiological reports", "start_pos": 34, "end_pos": 100, "type": "TASK", "confidence": 0.8117745688983372}]}, {"text": "The model has been used to address two NLP tasks: medical named-entity recognition (NER) and negation detection.", "labels": [], "entities": [{"text": "medical named-entity recognition (NER)", "start_pos": 50, "end_pos": 88, "type": "TASK", "confidence": 0.7767862131198248}, {"text": "negation detection", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.9893830716609955}]}, {"text": "We investigate whether learning several types of word embeddings improves BiLSTM's performance on those tasks.", "labels": [], "entities": []}, {"text": "Using a large dataset of chest x-ray reports, we compare the proposed model to a baseline dictionary-based NER system and a negation detection system that leverages the hand-crafted rules of the NegEx algorithm and the grammatical relations obtained from the Stanford Dependency Parser.", "labels": [], "entities": [{"text": "negation detection", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.9124916195869446}]}, {"text": "Compared to these more traditional rule-based systems, we argue that BiLSTM offers a strong alternative for both our tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Radiological reports represent a large part of all Electronic Medical Records (EMRs) held by medical institutions.", "labels": [], "entities": []}, {"text": "For instance, in England alone, upwards of 22 million plain radiographs were reported over the 12-month period from March 2015.", "labels": [], "entities": []}, {"text": "A radiological report is a written document produced by a Radiologist, a physician that specialises in interpreting medical images.", "labels": [], "entities": [{"text": "interpreting medical images", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.868229349454244}]}, {"text": "A report typically states any technical factors relevant to the acquired image as well as the presence or absence of radiological abnormalities.", "labels": [], "entities": []}, {"text": "When an abnormality is noted, the Radiologist often gives further description, including anatomical location and the extent of the disease.", "labels": [], "entities": [{"text": "anatomical location", "start_pos": 89, "end_pos": 108, "type": "METRIC", "confidence": 0.931759774684906}, {"text": "extent", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9590026140213013}]}, {"text": "Whilst Radiologists are taught to review radiographs in a systematic and comprehensive manner, their reporting style can vary quite dramatically () and the same findings can often be described in a multitude of different ways ().", "labels": [], "entities": []}, {"text": "The radiological reports may contain broken grammar and misspellings, which are often the result of voice recognition software or the dictation-transcript method).", "labels": [], "entities": []}, {"text": "Applying text mining techniques to these reports poses a number of challenges due to extensive variability in language, ambiguity and uncertainty, which are typical problems for natural language.", "labels": [], "entities": [{"text": "text mining", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.7332079112529755}]}, {"text": "In this work we are motivated by the need to automatically extract standardised clinical information from digitised radiological reports.", "labels": [], "entities": []}, {"text": "A system for the fully-automated extraction of this information could be used, for instance, to characterise the patient population and help health professionals improve dayto-day services.", "labels": [], "entities": []}, {"text": "The extracted structured data could also be used to build management dashboards) summarising and presenting the most prevalent conditions.", "labels": [], "entities": []}, {"text": "Another potential use is the automatic labelling of medical images, e.g. to support the development of computer-aided diagnosis software.", "labels": [], "entities": [{"text": "automatic labelling of medical images", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.7616430938243866}, {"text": "computer-aided diagnosis", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.7338241338729858}]}, {"text": "In this paper we propose a recurrent neural network (RNN) architecture for modelling radiological language and investigate its potential advantages on two different tasks: medical named-entity recognition (NER) and negation detection.", "labels": [], "entities": [{"text": "medical named-entity recognition (NER)", "start_pos": 172, "end_pos": 210, "type": "TASK", "confidence": 0.7900447100400925}, {"text": "negation detection", "start_pos": 215, "end_pos": 233, "type": "TASK", "confidence": 0.9900037050247192}]}, {"text": "The model, a bi-directional long short-term memory (BiLSTM) network, does not use any hand-engineered features, but learns them using a relatively small amount of labelled data and a larger but unlabelled corpus of radiological reports.", "labels": [], "entities": []}, {"text": "In addition, we explore the combined use of BiLSTM with other language models such as GloVe () and a novel variant of GloVe, proposed here, that makes use of a medical ontology.", "labels": [], "entities": []}, {"text": "The performance of the BiLSTM model is assessed comparatively to a rule-based system that has been optimised for the tasks at hand and builds upon well established techniques for medical NER and negation detection.", "labels": [], "entities": [{"text": "NER", "start_pos": 187, "end_pos": 190, "type": "TASK", "confidence": 0.8399004340171814}, {"text": "negation detection", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.9844472706317902}]}, {"text": "In particular, for NER, the system uses a baseline dictionary-based text mining component relying on a curated dictionary of medical terms.", "labels": [], "entities": [{"text": "NER", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8117755651473999}]}, {"text": "As a baseline for the negation detection task, the system implements a hybrid component based on the NegEx algorithm) in conjunction with grammatical relations obtained from the Stanford Dependency Parser.", "labels": [], "entities": [{"text": "negation detection task", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.9612841208775839}]}, {"text": "The article is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we provide a brief review of the existing body of work in NLP for medical information extraction and briefly discuss the use of artificial neural networks for NLP tasks.", "labels": [], "entities": [{"text": "medical information extraction", "start_pos": 79, "end_pos": 109, "type": "TASK", "confidence": 0.6178256571292877}]}, {"text": "In Section 3 we describe the datasets used for our experiments, and in Section 4 we introduce the BiLSTM model.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9153760671615601}]}, {"text": "The results are presented in Section 6 where we also compare BiLSTM against the rule-based baseline systems described in Section 5.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.5534223914146423}]}], "datasetContent": [{"text": "For this study, we produced an in-house radiology corpus consisting of 745, 480 historical chest Xray (radiographs) reports provided by Guy's and St Thomas' Trust (GSTT).", "labels": [], "entities": [{"text": "Guy's and St Thomas' Trust (GSTT)", "start_pos": 136, "end_pos": 169, "type": "DATASET", "confidence": 0.9628931879997253}]}, {"text": "This Trust runs two hospitals within the National Health Service (NHS) in England, serving a large area in South London.", "labels": [], "entities": [{"text": "National Health Service (NHS)", "start_pos": 41, "end_pos": 70, "type": "DATASET", "confidence": 0.9097524782021841}]}, {"text": "The reports cover the period between January 2005 and March 2016, and were generated by 276 different reporters including consultant Radiologists, trainee Radiologists and reporting Radiographers.", "labels": [], "entities": []}, {"text": "Our repository consists of text written or dictated by the clinicians after radiograph analysis, and do not contain any referral information or patientidentifying data, such as names, addresses or dates of birth.", "labels": [], "entities": []}, {"text": "However, many reports refer to the clinical history of the patient.", "labels": [], "entities": []}, {"text": "The reports had a minimum of 1 word and maximum of 311 words, with an average of 25.3 words and a standard deviation of 19.9 words.", "labels": [], "entities": []}, {"text": "On average there were 2.9 sentences per report.", "labels": [], "entities": []}, {"text": "After lemmatization, converting to lowercase, and discounting words that occur less than 3 times in the corpus, the resulting vocabulary contained 8, 031 words.", "labels": [], "entities": []}, {"text": "A sample of 2, 000 reports was randomly selected from the corpus for the purpose of creating a training and validation dataset for the NER and negation detection tasks, whilst the remaining of the reports were utilised for pre-training word embeddings.", "labels": [], "entities": [{"text": "NER", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.962422251701355}, {"text": "negation detection", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.8924658596515656}]}, {"text": "The reports selected for manual annotation were written for all types of patients, the word heart was associated with both Clinical Finding and Body Location classes.", "labels": [], "entities": [{"text": "Clinical Finding", "start_pos": 123, "end_pos": 139, "type": "TASK", "confidence": 0.6783232688903809}, {"text": "Body Location", "start_pos": 144, "end_pos": 157, "type": "TASK", "confidence": 0.7424022853374481}]}, {"text": "We have also introduced a negation attribute to indicate the absence of any of these entities.", "labels": [], "entities": []}, {"text": "We evaluated the BiLSTM model on the medical NER task by measuring the overlap between the predicted semantic groups and the ground truth labels.", "labels": [], "entities": [{"text": "NER task", "start_pos": 45, "end_pos": 53, "type": "TASK", "confidence": 0.7687646448612213}]}, {"text": "The evaluation was performed at the granularity of a single word and using 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "The BiLSTM model was always trained on 80% of the annotated corpus and tested on the remaining 20%.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.7729648351669312}]}, {"text": "mance of our baseline rule-based system.", "labels": [], "entities": []}, {"text": "Without fine-tuning, the BiLSTM NER model, that was initialised with the embeddings trained in an unsupervised manner using the BiLSTM language model, achieves the best F1-score (0.859), and outperforms the next best variant by 0.012.", "labels": [], "entities": [{"text": "BiLSTM NER", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.674967497587204}, {"text": "BiLSTM language model", "start_pos": 128, "end_pos": 149, "type": "DATASET", "confidence": 0.8812459508577982}, {"text": "F1-score", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9986757636070251}]}, {"text": "With fine-tuning, the same BiLSTM variant improves the F1-score by a further 0.015 and outperforms the baseline rulebased system by an F1-score of 0.172.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.5239203572273254}, {"text": "F1-score", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9996222257614136}, {"text": "F1-score", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9984301924705505}]}, {"text": "shows its performance measure for each of the semantic groups.", "labels": [], "entities": []}, {"text": "The evaluation of negation detection was measured on complete entities.", "labels": [], "entities": [{"text": "negation detection", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.9889038503170013}]}, {"text": "If any of the words within an entity were tagged with a I, B, E or S, that entity was considered to be negated.", "labels": [], "entities": []}, {"text": "As shown in, the BiLSTM (BiLSTM language model embeddings, fine-tuning allowed) achieved an F1-score of 0.902, which outperformed NegEx by 0.128.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9996080994606018}]}, {"text": "However, the best F1-score of 0.928 is achieved using the NegEx-Stanford system.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9995362758636475}]}], "tableCaptions": [{"text": " Table 1: Frequency distribution of entities by class in 2, 000", "labels": [], "entities": []}, {"text": " Table 2: Comparison of the BiLSTM model and rule-based sys-", "labels": [], "entities": []}, {"text": " Table 3: BiLSTM: performance metrics broken down by se-", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.5327197313308716}]}, {"text": " Table 4: Rule-based system: performance metrics broken down", "labels": [], "entities": []}, {"text": " Table 5: Comparison of BiLSTM, NegEx and NegEx-Stanford", "labels": [], "entities": [{"text": "NegEx-Stanford", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.7425185441970825}]}]}