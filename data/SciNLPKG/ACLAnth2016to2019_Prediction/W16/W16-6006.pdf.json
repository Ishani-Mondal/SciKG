{"title": [{"text": "Towards Broad-coverage Meaning Representation: The Case of Comparison Structures", "labels": [], "entities": [{"text": "Broad-coverage Meaning Representation", "start_pos": 8, "end_pos": 45, "type": "TASK", "confidence": 0.7704138159751892}]}], "abstractContent": [], "introductionContent": [{"text": "Representing the underlying meaning of text has been a long-standing topic of interest in computational linguistics.", "labels": [], "entities": [{"text": "Representing the underlying meaning of text", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8996929625670115}]}, {"text": "Recently there has been a renewed interest in computational modeling of meaning for various tasks such as semantic parsing).", "labels": [], "entities": [{"text": "computational modeling of meaning", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.7326099574565887}, {"text": "semantic parsing", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.7881028950214386}]}, {"text": "Opendomain and broad-coverage semantic representation () is essential for many language understanding tasks such as reading comprehension tests and question answering.", "labels": [], "entities": [{"text": "broad-coverage semantic representation", "start_pos": 15, "end_pos": 53, "type": "TASK", "confidence": 0.603746622800827}, {"text": "language understanding", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7281868755817413}, {"text": "question answering", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.8630538284778595}]}, {"text": "One of the most common way for expressing evaluative sentiment towards different entities is to use comparison.", "labels": [], "entities": []}, {"text": "Comparison can happen in very simple structures such as 'John is taller than Susan', or more complicated constructions such as 'The table is longer than the sofa is wide'.", "labels": [], "entities": []}, {"text": "So far the computational semantics of comparatives and how they affect the meaning of the surrounding text has not been studied effectively.", "labels": [], "entities": []}, {"text": "That is, the difference between the existing semantic and syntactic representation of comparatives has not been distinctive enough for enabling deeper understanding of a sentence.", "labels": [], "entities": []}, {"text": "For instance, the general logical form representation of the sentence 'John is taller than Susan' using the Boxer system) is the following: not fully capture the underlying semantics of the adjective 'tall' and what it means to be 'taller'.", "labels": [], "entities": []}, {"text": "A human reader can easily infer that the 'height' attribute of John is greater than Susan's.", "labels": [], "entities": []}, {"text": "Capturing the underlying meaning of comparison structures, as opposed to their surface wording, is crucial for accurate evaluation of qualities and quantities.", "labels": [], "entities": [{"text": "Capturing the underlying meaning of comparison structures", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.7957866958209446}]}, {"text": "Consider a more complex comparison example, 'The pizza was great, but it was still worse than the sandwich'.", "labels": [], "entities": []}, {"text": "The stateof-the-art sentiment analysis system) assigns an overall 'negative' sentiment value to this sentence, which clearly lacks the understanding of the comparison happening in the sentence.", "labels": [], "entities": [{"text": "stateof-the-art sentiment analysis", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.6768271426359812}]}, {"text": "As another example, consider the generic meaning representation of the sentence 'My Mazda drove faster than his Hyundai', according to frame semantic parsing using Semafor 1 tool () as depicted in.", "labels": [], "entities": [{"text": "frame semantic parsing", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.6053341130415598}]}, {"text": "It is evident that this meaning representation does not fully capture how the semantics of the adjective fast relates to the driving event, and what it actually means fora car to drive faster than another car.", "labels": [], "entities": []}, {"text": "More importantly, there is an ellipsis in this sentence, the resolution of which results incomplete reading of 'My Mazda drove faster than his Hyundai drove fast', which is in noway captured in 2 . Although the syntax and semantics of comparison in language have been studied in linguistics fora longtime, so far, computational modeling of the semantics of comparison components of natural language has not been developed fundamentally.", "labels": [], "entities": []}, {"text": "The lack of such a computational framework has left the deeper understanding of comparison structures still baffling to the currently existing NLP systems.", "labels": [], "entities": []}, {"text": "In this paper we summarize our efforts on defining a joint framework for comprehensive semantic representation of the comparison and ellipsis constructions.", "labels": [], "entities": []}, {"text": "We jointly model comparison and ellipsis as inter-connected predicate-argument structures, which enables automatic ellipsis resolution.", "labels": [], "entities": [{"text": "ellipsis resolution", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7444405257701874}]}, {"text": "In the upcoming sections we summarize our main contributions to this topic.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained our ILP model on the train-dev part of the dataset (70%), and tested on the test set (30%).", "labels": [], "entities": []}, {"text": "Evaluation is done against the reference gold annotation, with Exact and partial (Head) credits to annotating the constituency nodes.", "labels": [], "entities": [{"text": "Exact", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9417524337768555}]}, {"text": "We mainly report on two models: our comprehensive ILP model (detailed in Section 4), and a rule-based baseline.", "labels": [], "entities": []}, {"text": "In short, the baseline encodes the same linguistically motivated ILP constraints via rules and uses a few pattern extraction methods for finding comparison morphemes.", "labels": [], "entities": []}, {"text": "The average results on predicate prediction (across all types) is shown in.", "labels": [], "entities": [{"text": "predicate prediction", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.8967107534408569}]}, {"text": "As the results show, overall, the scores are high for predicting the predicates, what is not shown here is ellipsis predicates being the most challenging.", "labels": [], "entities": [{"text": "predicting the predicates", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.8527505000432333}]}, {"text": "The baseline is competitive, which shows that the linguistic patterns can capture many of the predicate types.", "labels": [], "entities": []}, {"text": "Our model performs the poorest on Equatives, achieving 71%/73% F1 score, which is a complex morpheme used in various linguistic constructions.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9448908269405365}]}, {"text": "Our analysis shows that the errors are often due to inaccuracies in automatically generated parse trees . As you can see in, The task of predicting arguments is a more demanding task.", "labels": [], "entities": [{"text": "predicting arguments", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.8965294063091278}]}, {"text": "The baseline performs very poorly at predicting the arguments.", "labels": [], "entities": []}, {"text": "Our comprehensive ILP model consistently outperforms the No Constraints model, showing the effectiveness of our linguistically motivated ILP constraints.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Predicate prediction Precision (P), Recall  (R) and F1 scores on test set, averaged across all  predicate types. Each cell contains scores according  to Exact/Head measurement.", "labels": [], "entities": [{"text": "Predicate prediction Precision (P)", "start_pos": 10, "end_pos": 44, "type": "METRIC", "confidence": 0.7443457593520483}, {"text": "Recall  (R)", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9593713879585266}, {"text": "F1", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.9882160425186157}, {"text": "Exact/Head measurement", "start_pos": 163, "end_pos": 185, "type": "METRIC", "confidence": 0.7120553627610207}]}, {"text": " Table 2: Results of argument prediction on test set, averaged across various argument types.", "labels": [], "entities": [{"text": "argument prediction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7662375867366791}]}]}