{"title": [{"text": "How Document Pre-processing affects Keyphrase Extraction Performance", "labels": [], "entities": [{"text": "Document Pre-processing", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8793525397777557}, {"text": "Keyphrase Extraction", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7382818758487701}]}], "abstractContent": [{"text": "The SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction.", "labels": [], "entities": [{"text": "SemEval-2010 benchmark dataset", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.7977982759475708}, {"text": "keyphrase extraction", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7333718091249466}]}, {"text": "This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 208, "end_pos": 228, "type": "TASK", "confidence": 0.8087570071220398}]}, {"text": "In previous work, a wide range of document preprocessing techniques were described but their impact on the overall performance of keyphrase extraction models is still unexplored.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.8410672247409821}]}, {"text": "Here, we reassess the performance of several keyphrase extraction models and measure their robustness against increasingly sophisticated levels of document preprocessing.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7505905628204346}]}], "introductionContent": [{"text": "Recent years have seen a surge of interest in automatic keyphrase extraction, thanks to the availability of the SemEval-2010 benchmark dataset (.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.7872900664806366}, {"text": "SemEval-2010 benchmark dataset", "start_pos": 112, "end_pos": 142, "type": "DATASET", "confidence": 0.8297406633694967}]}, {"text": "This dataset is composed of documents (scientific articles) that were automatically converted from PDF format to plain text.", "labels": [], "entities": []}, {"text": "As a result, most documents contain irrelevant pieces of text (e.g. muddled sentences, tables, equations, footnotes) that require special handling, so as to not hinder the performance of keyphrase extraction systems.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 187, "end_pos": 207, "type": "TASK", "confidence": 0.7324246317148209}]}, {"text": "In previous work, these are usually removed at the preprocessing step, but using a variety of techniques ranging from simple heuristics ( to sophisticated document logical structure detection on richly-formatted documents recovered from Google Scholar . Under such conditions, it may prove difficult to draw firm conclusions about which keyphrase extraction model performs best, as the impact of preprocessing on overall performance cannot be properly quantified.", "labels": [], "entities": [{"text": "document logical structure detection", "start_pos": 155, "end_pos": 191, "type": "TASK", "confidence": 0.6534744203090668}, {"text": "keyphrase extraction", "start_pos": 337, "end_pos": 357, "type": "TASK", "confidence": 0.7442191541194916}]}, {"text": "While previous work clearly states that efficient document preprocessing is a prerequisite for the extraction of high quality keyphrases, there is, to our best knowledge, no empirical evidence of how preprocessing affects keyphrase extraction performance.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 222, "end_pos": 242, "type": "TASK", "confidence": 0.7882002890110016}]}, {"text": "In this paper, we re-assess the performance of several state-of-the-art keyphrase extraction models at increasingly sophisticated levels of preprocessing.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7794223725795746}]}, {"text": "Three incremental levels of document preprocessing are experimented with: raw text, text cleaning through document logical structure detection, and removal of keyphrase sparse sections of the document.", "labels": [], "entities": [{"text": "text cleaning", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.7583137452602386}, {"text": "document logical structure detection", "start_pos": 106, "end_pos": 142, "type": "TASK", "confidence": 0.6297838389873505}]}, {"text": "In doing so, we present the first consistent comparison of different keyphrase extraction models and study their robustness over noisy text.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.7472617626190186}]}, {"text": "More precisely, our contributions are: \u2022 We show that performance variation across keyphrase extraction systems is, at least in part, a function of the (often unstated) effectiveness of document preprocessing.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7623776793479919}]}, {"text": "\u2022 We empirically show that supervised models are more resilient to noise, and point out that the performance gap between baselines and top performing systems is narrowing with the increase in preprocessing effort.", "labels": [], "entities": []}, {"text": "\u2022 We compare the previously reported results of several keyphrase extraction models with that of our re-implementation, and observe that baseline performance is underestimated because of the inconsistence in document preprocessing.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.748686820268631}]}, {"text": "\u2022 We release both anew version of the SemEval-2010 dataset 1 with preprocessed documents and our implementation of the state-of-the-art keyphrase extraction models 2 using the pke toolkit for use by the community.", "labels": [], "entities": [{"text": "SemEval-2010 dataset 1", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.8062406082948049}, {"text": "keyphrase extraction", "start_pos": 136, "end_pos": 156, "type": "TASK", "confidence": 0.7389900982379913}]}], "datasetContent": [{"text": "The SemEval-2010 benchmark dataset () is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers).", "labels": [], "entities": [{"text": "SemEval-2010 benchmark dataset", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.7721045613288879}, {"text": "ACM Digital Library", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.9360408584276835}]}, {"text": "The input papers ranged from 6 to 8 pages and were converted from PDF format to plain text using an off-the-shelf tool . The only preprocessing applied is a systematic dehyphenation at line breaks and removal of author-assigned keyphrases.", "labels": [], "entities": []}, {"text": "Scientific articles were selected from four different research areas as defined in the ACM classification, and were equally distributed into training (144 articles) and test (100 articles) sets.", "labels": [], "entities": [{"text": "ACM classification", "start_pos": 87, "end_pos": 105, "type": "DATASET", "confidence": 0.835540771484375}]}, {"text": "Gold standard keyphrases are composed of both author-assigned keyphrases collected from the original PDF files and reader-assigned keyphrases provided by student annotators.", "labels": [], "entities": []}, {"text": "Long documents such as those in the SemEval-2010 benchmark dataset are notoriously difficult to handle due to the large number of keyphrase candidates (i.e. phrases that are eligible to be keyphrases) that the systems have to cope with ().", "labels": [], "entities": [{"text": "SemEval-2010 benchmark dataset", "start_pos": 36, "end_pos": 66, "type": "DATASET", "confidence": 0.8064285516738892}]}, {"text": "Furthermore, noisy textual content, whether due to format conversion errors or to unusable elements (e.g. equations), yield many spurious keyphrase candidates that negatively affect keyphrase extraction performance.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 182, "end_pos": 202, "type": "TASK", "confidence": 0.8786168694496155}]}, {"text": "This is particularly true for systems that make use of core NLP tools to select candidates, that in turn exhibit poor performance on degraded text.", "labels": [], "entities": []}, {"text": "Filtering out irrelevant text is therefore needed for addressing these issues.", "labels": [], "entities": []}, {"text": "In this study, we concentrate our effort on re-assessing keyphrase extraction performance on three increasingly sophisticated levels of document preprocessing described below.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.8474323153495789}]}, {"text": "Level 1: We process each input file with the Stanford CoreNLP suite () . We use the default parameters and perform tokenization, sentence splitting and Part-Of-Speech (POS) tagging.", "labels": [], "entities": [{"text": "Stanford CoreNLP suite", "start_pos": 45, "end_pos": 67, "type": "DATASET", "confidence": 0.9353940089543661}, {"text": "sentence splitting", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7626973986625671}, {"text": "Part-Of-Speech (POS) tagging", "start_pos": 152, "end_pos": 180, "type": "TASK", "confidence": 0.677969217300415}]}, {"text": "Level 2: Similarly to (, we retrieve 6 the original PDF files from the ACM Digital Library.", "labels": [], "entities": [{"text": "ACM Digital Library", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.8565312425295512}]}, {"text": "We then extract the enriched 7 textual content of the PDF files using an Optical Character Recognition (OCR) system 8 , and perform document logical structure detection using ParsCit ( . We use the detected logical structure to remove authorassigned keyphrases and select only relevant elements: title, headers, abstract, introduction, related work, body text 10 and conclusion.", "labels": [], "entities": [{"text": "document logical structure detection", "start_pos": 132, "end_pos": 168, "type": "TASK", "confidence": 0.6386524215340614}]}, {"text": "We finally apply a systematic dehyphenation at line breaks and run the Stanford CoreNLP suite.", "labels": [], "entities": [{"text": "Stanford CoreNLP suite", "start_pos": 71, "end_pos": 93, "type": "DATASET", "confidence": 0.9593130548795065}]}, {"text": "Level 3: As pointed out by, considering only the keyphrase dense parts of the scientific articles allows to improve keyphrase extraction performance.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.8518467843532562}]}, {"text": "Accordingly we follow previous work and further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion.", "labels": [], "entities": []}, {"text": "Here, the idea is to achieve the best compromise between search space (number of candidates) and maximum performance (recall).", "labels": [], "entities": [{"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9985241293907166}]}, {"text": "1 https://github.com/boudinfl/semeval-2010-pre 2 https://github.com/boudinfl/pke 3 pdftotext, http://www.foolabs.com/xpdf/ 4 Valid hyphenated forms may have their hyphen removed by this process.", "labels": [], "entities": []}, {"text": "Use use Stanford CoreNLP v3.6.0.", "labels": [], "entities": [{"text": "Stanford CoreNLP v3.6.0", "start_pos": 8, "end_pos": 31, "type": "DATASET", "confidence": 0.9297733505566915}]}, {"text": "To ensure consistency, articles were manually collected.", "labels": [], "entities": [{"text": "consistency", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9729655385017395}]}, {"text": "Additional information such as font format or spacial layout is also extracted.", "labels": [], "entities": [{"text": "spacial layout", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.765360563993454}]}, {"text": "We use Omnipage v18, http://www.nuance.com/omnipage We use ParsCit v110505.", "labels": [], "entities": [{"text": "ParsCit v110505", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.8577623069286346}]}, {"text": "We further filter out tables, figures, captions, equations, notes, copyright and references.", "labels": [], "entities": []}, {"text": "shows the average number of sentences and words along with the maximum possible recall for each level of preprocessing.", "labels": [], "entities": [{"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9992766976356506}]}, {"text": "The maximum recall is obtained by computing the fraction of the reference keyphrases that occur in the documents.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9993727803230286}]}, {"text": "We observe that the level 2 preprocessing succeeds in eliminating irrelevant text by significantly reducing the number of words (-19%) while maintaining a high maximum recall (-2%).", "labels": [], "entities": [{"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.9959945678710938}]}, {"text": "Level 3 preprocessing drastically reduce the number of words to less than a quarter of the original amount while interestingly still preserving high recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.9968151450157166}]}, {"text": "We follow the evaluation procedure used in the SemEval-2010 competition and evaluate the performance of each model in terms of f-measure (F) at the top N keyphrases . We use the set of combined author-and reader-assigned keyphrases as reference keyphrases.", "labels": [], "entities": [{"text": "f-measure (F)", "start_pos": 127, "end_pos": 140, "type": "METRIC", "confidence": 0.7812714725732803}]}, {"text": "Extracted and reference keyphrases are stemmed to reduce the number of mismatches.", "labels": [], "entities": []}, {"text": "In the previous sections, we provided empirical evidence that document preprocessing weighs heavily on the outcome of keyphrase extraction models.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 118, "end_pos": 138, "type": "TASK", "confidence": 0.8282597661018372}]}, {"text": "This raises the question of whether further improvement might be gained from a more aggressive preprocessing.", "labels": [], "entities": []}, {"text": "To answer this question, we take another step beyond content filtering and further abridge the input text from level 3 preprocessed documents using an unsupervised summarization technique.", "labels": [], "entities": [{"text": "content filtering", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7347482144832611}]}, {"text": "More specifically, we keep the title and abstract intact, as they are the two most keyphrase dense parts within scientific articles , and select only the most content bearing sentences from the remaining contents.", "labels": [], "entities": []}, {"text": "To do this, sentences are ordered using TextRank () and the less informative ones, as determined by their TextRank scores normalized by their lengths in words, are filtered out.", "labels": [], "entities": []}, {"text": "Finding the optimal subset of sentences from already shortened documents is however no trivial task as maximum recall linearly decreases with the number of sentences discarded.", "labels": [], "entities": [{"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9833217859268188}]}, {"text": "Here, we simply set the reduction ratio to 0.865 so that the average maximum recall on the training set does not lose more than 5%.", "labels": [], "entities": [{"text": "reduction ratio", "start_pos": 24, "end_pos": 39, "type": "METRIC", "confidence": 0.973252683877945}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9969667792320251}]}, {"text": "shows the reduction in the average number of sentences and words compared to level 3 preprocessing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics computed at the different levels of document preprocessing on the training set.", "labels": [], "entities": []}, {"text": " Table 2: Maximum recall and average number of keyphrase candidates for each model.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.991969883441925}]}, {"text": " Table 3: F-scores computed at the top 10 extracted keyphrases for the unsupervised (Unsup.) and super- vised (Sup.) models at each preprocessing level. We also report the standard deviation across the five  models for each level (\u03c3 1 ) and the standard deviation across the three levels for each model (\u03c3 2 ). \u03b1 and  \u03b2 indicate significance at the 0.05 level using Student's t-test against level 1 and level 2 respectively.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9870931506156921}, {"text": "standard deviation", "start_pos": 172, "end_pos": 190, "type": "METRIC", "confidence": 0.9388363063335419}, {"text": "standard deviation", "start_pos": 245, "end_pos": 263, "type": "METRIC", "confidence": 0.9502263069152832}]}]}