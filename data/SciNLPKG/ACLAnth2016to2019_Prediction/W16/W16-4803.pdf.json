{"title": [], "abstractContent": [{"text": "Computational approaches for dialectometry employed Levenshtein distance to compute an aggregate similarity between two dialects belonging to a single language group.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 52, "end_pos": 72, "type": "METRIC", "confidence": 0.8175304532051086}]}, {"text": "In this paper, we apply a sequence-to-sequence autoencoder to learn a deep representation for words that can be used for meaningful comparison across dialects.", "labels": [], "entities": []}, {"text": "In contrast to the alignment-based methods, our method does not require explicit alignments.", "labels": [], "entities": []}, {"text": "We apply our architectures to three different datasets and show that the learned representations indicate highly similar results with the analyses based on Levenshtein distance and capture the traditional dialectal differences shown by dialectologists.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper proposes anew technique based on state-of-the-art machine learning methods for analyzing dialectal variation.", "labels": [], "entities": []}, {"text": "The computational/quantitative study of dialects, dialectometry, has been a fruitful approach for studying linguistic variation based on geographical or social factors.", "labels": [], "entities": []}, {"text": "A typical dialectometric analysis of a group of dialects involve calculating differences between pronunciations of a number of items (words or phrases) as spoken in a number of sites (geographical location, or another unit of variation of interest).", "labels": [], "entities": []}, {"text": "Once a difference metric is defined for individual items, item-by-item differences are aggregated to obtain site-by-site differences which form the basis of further analysis and visualizations of the linguistic variation based on popular computational methods such as clustering or dimensionality reduction.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 282, "end_pos": 306, "type": "TASK", "confidence": 0.6767737716436386}]}, {"text": "One of the key mechanisms for this type of analysis is the way item-by-item differences are calculated.", "labels": [], "entities": []}, {"text": "These distances are often based on Levenshtein distance between two phonetically transcribed variants of the same item).", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 35, "end_pos": 55, "type": "METRIC", "confidence": 0.8471216261386871}]}, {"text": "Levenshtein distance is often improved by weighting the distances based on pointwise mutual information (PMI) of the aligned phonemes (.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.47840164601802826}, {"text": "pointwise mutual information (PMI)", "start_pos": 75, "end_pos": 109, "type": "METRIC", "confidence": 0.7103348324696223}]}, {"text": "In this paper we propose an alternative way for calculating the distances between two phoneme sequences using unsupervised (or self supervised) deep learning methods, namely Long Short Term Memory (LSTM) autoencoders (see Section 2 for details).", "labels": [], "entities": []}, {"text": "The model is trained for predicting every pronunciation in the data using the pronunciation itself as the sole predictor.", "labels": [], "entities": []}, {"text": "Since the internal representation of the autoencoder is limited, it is forced to learn compact representations of words that are useful for reconstruction of the input.", "labels": [], "entities": []}, {"text": "The resulting internal representations, embeddings, are (dense) multi-dimensional vectors in a space where similar pronunciations are expected to lie in proximity of each other.", "labels": [], "entities": []}, {"text": "Then, we use the distances between these embedding vectors as the differences between the alternative pronunciations.", "labels": [], "entities": []}, {"text": "Since the distanced calculated in this manner are proper distances in an Euclidean space, the usual methods of clustering or multi-dimensional scaling can be applied without further transformations or normalizations.", "labels": [], "entities": []}, {"text": "There area number of advantages of the proposed model in comparison to methods based on Levenshtein distance.", "labels": [], "entities": []}, {"text": "First of all, proposed method does not need explicit alignments.", "labels": [], "entities": []}, {"text": "While learning to reconstruct the pronunciations, the model discovers a representation that places phonetically similar variants together.", "labels": [], "entities": []}, {"text": "However, unlike other alternatives, the present method does not need pairs of pronunciations of the same item.", "labels": [], "entities": []}, {"text": "Another advantage of the model is its ability to discover potential non-linear and long-distance dependencies within words.", "labels": [], "entities": []}, {"text": "The use of (deep) neural networks allow non-linear combinations of the input features, where LSTMs are particularly designed for learning relationships at a distance within a sequence.", "labels": [], "entities": []}, {"text": "The model can learn properties of input that depend on non-contiguous long-distance context features (e.g., as in vowel harmony) and it can also learn to combine features in a non-linear non-additive way, (e.g., when the effect of vowel harmony is canceled in presence of other contextual features).", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe our model and the reasons for the development and employment of such a model.", "labels": [], "entities": []}, {"text": "In section 3, we discuss our experimental settings and the results of our experiments.", "labels": [], "entities": []}, {"text": "We discuss our results in section 4.", "labels": [], "entities": []}, {"text": "We conclude the paper in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we limit T , the length of the sequence processed by the LSTM, to 10 for Dutch and German dialect datasets and 20 for Pennsylvanian dataset.", "labels": [], "entities": [{"text": "T", "start_pos": 29, "end_pos": 30, "type": "METRIC", "confidence": 0.9828712940216064}, {"text": "Dutch and German dialect datasets", "start_pos": 93, "end_pos": 126, "type": "DATASET", "confidence": 0.5565877139568329}, {"text": "Pennsylvanian dataset", "start_pos": 138, "end_pos": 159, "type": "DATASET", "confidence": 0.9535952508449554}]}, {"text": "We trained our autoencoder network for 20 epochs on each dataset and then used the encoder to predict a hidden representation of length 8 for each dataset.", "labels": [], "entities": []}, {"text": "We used the continuous vector representation to compute the similarities between words.", "labels": [], "entities": []}, {"text": "We used a batch size of 32 and the Adadelta optimizer) to train our neural networks.", "labels": [], "entities": []}, {"text": "All our experiments were performed using Keras (Chollet, 2015) and Tensorflow (Abadi et al., 2016).", "labels": [], "entities": []}], "tableCaptions": []}