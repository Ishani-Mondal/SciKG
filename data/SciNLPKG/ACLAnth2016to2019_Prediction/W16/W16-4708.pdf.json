{"title": [{"text": "A Study on the Interplay Between the Corpus Size and Parameters of a Distributional Model for Term Classification", "labels": [], "entities": [{"text": "Term Classification", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8953433036804199}]}], "abstractContent": [{"text": "We propose and evaluate a method for identifying co-hyponym lexical units in a terminological resource.", "labels": [], "entities": []}, {"text": "The principles of term recognition and distributional semantics are combined to extract terms from a similar category of concept.", "labels": [], "entities": [{"text": "term recognition", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.736456423997879}]}, {"text": "Given a set of candidate terms, random projections are employed to represent them as low-dimensional vectors.", "labels": [], "entities": []}, {"text": "These vectors are derived automatically from the frequency of the co-occurrences of the candidate terms and words that appear within windows of text in their proximity (context-windows).", "labels": [], "entities": []}, {"text": "Ina k-nearest neighbours framework , these vectors are classified using a small set of manually annotated terms which exemplify concept categories.", "labels": [], "entities": []}, {"text": "We then investigate the interplay between the size of the corpus that is used for collecting the co-occurrences and a number of factors that play roles in the performance of the proposed method: the configuration of context-windows for collecting co-occurrences, the selection of neighbourhood size (k), and the choice of similarity metric.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic term recognition (ATR) deals with the extraction of domain-specific lexical units from text.", "labels": [], "entities": [{"text": "Automatic term recognition (ATR)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8540318806966146}, {"text": "extraction of domain-specific lexical units from text", "start_pos": 48, "end_pos": 101, "type": "TASK", "confidence": 0.7108874235834394}]}, {"text": "The input of ATR is a large collection of documents, i.e., a special corpus, 1 and the output is a vocabulary that is used for communicating specialized knowledge.", "labels": [], "entities": []}, {"text": "This vocabulary comprises a collection of single-token and multi-token lexical units-respectively known as simple and complex terms-that form a terminological resource.", "labels": [], "entities": []}, {"text": "For example, in computational linguistics, lexicon and parsing are examples of simple terms, while multilingual corpus and information extraction are complex terms.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.738162025809288}]}, {"text": "Similarly, in molecular biology, collagen and cortisol are examples of simple terms, and I kappa B and plasma prednisolone are examples of complex terms.", "labels": [], "entities": []}, {"text": "Terms, extracted by an ATR system, represent abroad spectrum of concepts that exist in a domain knowledge.", "labels": [], "entities": []}, {"text": "Terms and their corresponding concepts, however, can be further organized in several categories to form a taxonomy; each category characterizes a group of terms from 'similar' concepts in the domain of study).", "labels": [], "entities": []}, {"text": "For example, in computational linguistics, the terms lexicon and multilingual corpus can be categorized under the category of language resources, while parsing and information extraction can be categorized under the concept of technologies.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 164, "end_pos": 186, "type": "TASK", "confidence": 0.6642422676086426}]}, {"text": "Likewise, in molecular biology, instances such as collagen and I kappa B are categorized as proteins, while cortisol and plasma prednisolone are classified as lipid substances.", "labels": [], "entities": []}, {"text": "If the concept categories are not known, a method is used to suggest an organization for terms (e.g.,);).", "labels": [], "entities": []}, {"text": "However, concept categories are usually known, or at least, a partial knowledge of them exists.", "labels": [], "entities": []}, {"text": "In these scenarios, typically a manually annotated corpus is employed to develop an entity tagger in a supervised fashion, often in the form of a sequence classifier.", "labels": [], "entities": []}, {"text": "Bio-entity tagging is an established example of this kind of tasks ().", "labels": [], "entities": [{"text": "Bio-entity tagging", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7900175154209137}]}, {"text": "These methods, however, rely heavily on manually annotated corpora, in which each mention of a term and its concept-: Venn diagram that illustrates the relationship between candidate terms, valid terms, and a particular category of terms C p . ATR targets the extraction of candidate terms and the identification of valid terms.", "labels": [], "entities": []}, {"text": "However, term classification targets the identification of terms that belong to a conceptscategory, i.e., a subset of valid terms.", "labels": [], "entities": [{"text": "term classification", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7529158592224121}]}, {"text": "Provided that enough training data is available, a reasonable performance can be attained in these recognition tasks).", "labels": [], "entities": []}, {"text": "Yet in several scenarios, the targeted concept categories (similar to entity recognition tasks) are known but no manual annotation is available for the training and development of an entity tagger.", "labels": [], "entities": [{"text": "entity recognition tasks", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.7914483944574991}]}, {"text": "This is a familiar problem when a terminological resource with a hierarchical structure must be constructed from scratch, a task with many practical applications (see e.g.) and renewed interests, e.g., as addressed in cold-start knowledge base population () and ontology learning.", "labels": [], "entities": [{"text": "ontology learning", "start_pos": 262, "end_pos": 279, "type": "TASK", "confidence": 0.8627467155456543}]}, {"text": "Similarly, this problem surfaces in maintaining terminologies, where constant update and extension is required to accommodate new vocabularies and their usages.", "labels": [], "entities": []}, {"text": "This paper suggests a method to address this situation: the extraction of terms from a particular class of concepts in the absence of training data for the development of an entity tagger.", "labels": [], "entities": []}, {"text": "The proposed method (similar to ATR and in contrast to entity recognition task) works at the corpus level and does not deal with individual term mentions.", "labels": [], "entities": [{"text": "entity recognition task", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.8010109066963196}]}, {"text": "However, in contrast to ATR (which extracts terms from diverse concept categories in a specific domain knowledge) and similar to entity tagging, the proposed method is designed to extract a subset of terms that belongs to a particular category of concepts in a domain knowledge (i.e., co-hyponym terms).", "labels": [], "entities": [{"text": "entity tagging", "start_pos": 129, "end_pos": 143, "type": "TASK", "confidence": 0.7215843498706818}]}, {"text": "Note that each category can be further organised into more refined subcategories to provide abstractions at different levels of granularity.", "labels": [], "entities": []}, {"text": "Since co-hyponymy is an inheritable relationship, terms under each category, disregarding the subcategory that they belong to, are still co-hyponym.", "labels": [], "entities": []}, {"text": "Since polysemy is less frequent in specialized vocabularies than in general vocabularies, the proposed approach is effective and useful.", "labels": [], "entities": []}, {"text": "We support this claim with a comparison between the proportion of polysemous entries in WordNet, i.e., a general vocabulary, and the terminological resource that is induced from the annotated terms in the GENIA corpus (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.955073893070221}, {"text": "GENIA corpus", "start_pos": 205, "end_pos": 217, "type": "DATASET", "confidence": 0.9627034366130829}]}, {"text": "In WordNet, approximately 17% of entries are polysemous.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.9573341608047485}]}, {"text": "The GENIA corpus (which is a well-known special corpus in the domain of molecular biology) provides manual concept-category annotations for 92,722 term mentions.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9610006213188171}]}, {"text": "These term mentions constitute a vocabulary of 34,077 distinct entries, of which only 1,373 are polysemous (i.e., their individual mentions are annotated with at least two concept categories).", "labels": [], "entities": []}, {"text": "Therefore, compared to WordNet, the GENIA terminological resource contains only a small fraction of polysemous entries, i.e., 1372 34077 = 4%.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.9653882384300232}, {"text": "GENIA terminological resource", "start_pos": 36, "end_pos": 65, "type": "DATASET", "confidence": 0.8630385597546896}]}, {"text": "2 The proposed term classification method is realized as an ad hoc term-weighting procedure on top of an ATR system.", "labels": [], "entities": [{"text": "term classification", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8159214556217194}]}, {"text": "ATR typically comprises a two-step procedure: candidate term extraction followed by term weighting and ranking).", "labels": [], "entities": [{"text": "ATR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9386580586433411}, {"text": "candidate term extraction", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.6569160521030426}]}, {"text": "Candidate term extraction deals with term formation and the extraction of candidate terms.", "labels": [], "entities": [{"text": "Candidate term extraction", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6760236620903015}, {"text": "term formation", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7145909816026688}]}, {"text": "Following the extraction of candidate terms, as stated byKageura and Umino (1996), an ATR system often combines scores that are known as unithood and termhood to weight terms.", "labels": [], "entities": [{"text": "ATR", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.8143638968467712}]}, {"text": "Unithood indicates the degree to which a sequence of tokens can be combined to form a complex term.", "labels": [], "entities": []}, {"text": "It characterizes syntagmatic relations between tokens to identify collocations (therefore is only defined for complex terms).", "labels": [], "entities": []}, {"text": "Termhood, however, \"is the degree that a linguistic unit is related to \u00b7 \u00b7 \u00b7 some domain-specific concepts\" (.", "labels": [], "entities": []}, {"text": "Hence, termhood is defined for both simple and complex terms.", "labels": [], "entities": []}, {"text": "From a linguistic perspective, termhood char-: Shown a context-window of size 3 tokens that extend around terms: the occurrences of the candidate term information extraction in different sentences of a corpus.", "labels": [], "entities": []}, {"text": "For each occurrence of the candidate term in each line, the context-window consists of words that are placed in rectangles.", "labels": [], "entities": []}, {"text": "To construct a model, these co-occurrences are collected, counted, and represented by a vector.", "labels": [], "entities": []}, {"text": "acterizes an associative relationship between terms and the communicative context that verbalizes their meaning (in this scenario, the corpus).", "labels": [], "entities": []}, {"text": "The major difference between the proposed term classification technique and a general ATR system is, therefore, the way they define termhood.", "labels": [], "entities": [{"text": "term classification", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7242057174444199}]}, {"text": "To actualize the proposed term classification task, a termhood measure that can identify co-hyponym terms must be devised.", "labels": [], "entities": [{"text": "term classification task", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.7857809265454611}]}, {"text": "To achieve this goal, we take a distributional approach.", "labels": [], "entities": []}, {"text": "We assume that the association of a term to a concept category is a kind of relation that can be modelled using the syntagmatic relation of the term and its co-occurred words in context-windows extended in the vicinity of the term's mentions in the corpus.", "labels": [], "entities": []}, {"text": "We, therefore, hypothesise that co-hyponym terms tend to have similar distributional properties in these context-windows.", "labels": [], "entities": []}, {"text": "Note that a similar hypothesis has been employed in many other distributional techniques for terminology extraction.", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.9262098371982574}]}, {"text": "In order to quantify these distributional similarities, vector space models are employed.", "labels": [], "entities": []}, {"text": "Words that appear in context-windows are represented by the elements of the standard basis of a vector space (i.e., informally each dimension of a vector space) and each candidate term is represented by a vector.", "labels": [], "entities": []}, {"text": "In this vector space, the co-occurrence frequency of words and candidate term in context-windows determines the coordinates of the vector that represent the candidate term.", "labels": [], "entities": []}, {"text": "Hence, the values assigned to the the vector's coordinates represent the correlation between the candidate term that the vector represents and the words in context-windows.", "labels": [], "entities": []}, {"text": "Consequently, we can use the proximity of candidate terms to compare their distributional similarities in this term-space model.", "labels": [], "entities": []}, {"text": "In this term-space model, we model a category of terms using a set of reference terms (shown by R s ), i.e., a small number of terms that are manually annotated with their corresponding concept category.", "labels": [], "entities": []}, {"text": "The averaged distance between vectors that represent candidate terms and the vectors that represent R sis assumed to determine the association of candidate terms to the concept categories represented by R s . This association is computed using a k-nearest neighbours (k-nn) method.", "labels": [], "entities": []}, {"text": "As explained by Daelemans and Van Den Bosch (2010), the memory-based k-nn technique provides us with a similaritybased reasoning framework that can be used to identify term categories without the need for formulating these associations using a meta-language such as rules.", "labels": [], "entities": []}, {"text": "Like other distributional methods, finding a configuration of context-windows (i.e., the way cooccurrence frequencies are collected) that best characterizes co-hyponym terms is a major research concern that must be investigated empirically.", "labels": [], "entities": []}, {"text": "Context-windows can be configured differently regarding the position of the candidate terms in them and the direction in which they are stretched.", "labels": [], "entities": []}, {"text": "They can be expanded (a) only to the left side of a candidate term to collect the co-occurrences of the candidate term with preceding words in each sentence of the corpus, (b) to the right side to collect co-occurrences with the succeeding words, or (c) around the candidate term, i.e., in both left and right directions.", "labels": [], "entities": []}, {"text": "The size of context-windows must also be decided, i.e., the extent of the region on either side of a term for collecting and counting its co-occurrences with neighbouring words.", "labels": [], "entities": []}, {"text": "In addition, information about the order of words in context-windows can be ignored or encoded in the constructed distributional model.", "labels": [], "entities": []}, {"text": "Independent of the configuration of context-windows in the proposed method, due to the Zipfian distribution of terms and words in context-windows, vectors that represent candidate terms are inevitably high-dimensional and sparse (i.e., most of the elements of vectors are zero).", "labels": [], "entities": []}, {"text": "The high-dimensionality of vectors hinders the computation of similarities, and their sparseness is likely to diminish the discriminatory power of the constructed model (i.e., the curse of dimensionality problem).", "labels": [], "entities": []}, {"text": "To avoid these problems, a dimensionality reduction technique is employed to reduce the dimension of vectors to a certain size.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6753818541765213}]}], "datasetContent": [{"text": "The proposed method is evaluated using the GENIA terminological resource.", "labels": [], "entities": [{"text": "GENIA terminological resource", "start_pos": 43, "end_pos": 72, "type": "DATASET", "confidence": 0.9050979018211365}]}, {"text": "Manually annotated term mentions from the GENIA corpus (Version 3.02) are collected to build a terminological resource.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.9745990037918091}]}, {"text": "This resource's entries are distinct pairs of lexical units and their annotations.", "labels": [], "entities": []}, {"text": "The annotations are employed to organize terms in a taxonomy similar to the one proposed by for evaluating bio-entity taggers.", "labels": [], "entities": []}, {"text": "To keep the reports to a manageable size, we limit the evaluation task to the identification of terms belonging to the category of proteins (see).", "labels": [], "entities": []}, {"text": "Using the the obtained frequencies in the GENIA corps and c-value measure (i.e., a widely used method for ranking terms in ATR systems () terms are ranked in a list.", "labels": [], "entities": [{"text": "GENIA corps", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.8045864105224609}]}, {"text": "From this sorted list, the top 100 terms and their annotations are used to form a set of reference vectors (R s ).", "labels": [], "entities": []}, {"text": "Consequently, in our evaluations, R s contains 36 protein terms: terms that are annotated as co-hyponyms under the concept category of 'protein' from the GENIA Ontology.", "labels": [], "entities": [{"text": "GENIA Ontology", "start_pos": 154, "end_pos": 168, "type": "DATASET", "confidence": 0.9737145304679871}]}, {"text": "shows the distribution of protein terms in the obtained sorted list of terms using the c-value measure with respect to a random baseline.", "labels": [], "entities": []}, {"text": "Except fora small number of terms at the top of the list, the proportion of protein terms in the c-value sorted list is similar to the random baseline.", "labels": [], "entities": []}, {"text": "We use the c-value ranking as one baseline in our evaluations.", "labels": [], "entities": []}, {"text": "To show that R sis not sufficient for developing an entity tagger, we verify the performance of a bioentity tagger when the employed R sis used for its training.", "labels": [], "entities": []}, {"text": "Namely, we employ the ABNER system, an entity tagger designed for analysing biology text.", "labels": [], "entities": [{"text": "ABNER", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.8230518698692322}]}, {"text": "It uses conditional random fields and a variety of orthographic and contextual features to perform its task.", "labels": [], "entities": []}, {"text": "If ABNER is trained using all the provided annotations for protein term mentions in the GENIA corpus, it achieves a reasonable performance (recall of 77.8 and precision of 68.1).", "labels": [], "entities": [{"text": "ABNER", "start_pos": 3, "end_pos": 8, "type": "METRIC", "confidence": 0.8143843412399292}, {"text": "GENIA corpus", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.9710493683815002}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9996790885925293}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.999626636505127}]}, {"text": "However, if it is trained using the mentions of terms in R s , the resulting model can only identify an additional 16 protein terms out of the remaining 8,864 terms.", "labels": [], "entities": []}, {"text": "Put simply, the 1,321 mentions of the 36 protein terms in R s are not sufficient to train ABNER.", "labels": [], "entities": [{"text": "ABNER", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.5873668193817139}]}, {"text": "Initially, we will construct vector spaces using the raw text from the GENIA corpus.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.9708915054798126}]}, {"text": "Besides normalising text to lower-case letters and a simple Penn Treebank tokenisation, no other text pre-processing is performed.", "labels": [], "entities": [{"text": "Penn Treebank tokenisation", "start_pos": 60, "end_pos": 86, "type": "DATASET", "confidence": 0.9124108751614889}]}, {"text": "This pre-processing results in 490,941 tokens and a vocabulary size of 19,576.", "labels": [], "entities": []}, {"text": "We then enlarge the corpus by fetching 223,316 abstracts from the PubMed repository, of which each abstract contains at least three of the terms in the terminological resource.", "labels": [], "entities": [{"text": "PubMed repository", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.9524339735507965}]}, {"text": "The enlarged corpus has more than 55 million tokens and a vocabulary of size 881,040.", "labels": [], "entities": []}, {"text": "Hereafter, we denote these two corpora by G o (for the original GENIA corpus) and Ge (for the enlarged corpus).", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.952531635761261}]}, {"text": "In this corpus, the terms employed in our experiments are mentioned more than 9 million times.", "labels": [], "entities": []}, {"text": "As expected, only a small number of terms are frequent and the majority of terms are mentioned a few times.", "labels": [], "entities": []}, {"text": "A large number of terms (i.e., about 40%) never appear in Ge (see.", "labels": [], "entities": []}, {"text": "Using the method explained in Section 2, we use these two corpora to collect the co-occurrences and build vector space models.", "labels": [], "entities": []}, {"text": "We perform our experiments with vector spaces that are constructed at the reduced dimension m = 2000.", "labels": [], "entities": []}, {"text": "Considering the number of term vectors in the model (i.e., 34077), m = 2000 is a conservative choice that guarantees a small distortion in pair-wise distances between vectors.", "labels": [], "entities": []}, {"text": "Similarly, because the vocabulary size | w| \u2265 19576, we use word vectors of 6 non-zero elements and 30 non-zero elements, respectively, for the construction 2 and 1 -normed spaces.", "labels": [], "entities": []}, {"text": "These values for the numbers of non-zero elements in word vectors are conservative choices that meet the criteria specified in Section 2 for the value of \u03b1 in Equation 2.", "labels": [], "entities": []}, {"text": "The construction of vector spaces is carried out by collecting co-occurrence frequencies in contextwindows that are configured differently regarding the direction and size in which they are stretched.", "labels": [], "entities": []}, {"text": "Moreover, we investigate the influence of the inclusion of word order information in the model using the permutation technique described in.", "labels": [], "entities": []}, {"text": "As suggested in research reports (see, e.g., and), narrow context-windows are more suitable to capture paradigmatic relations such as the one intended in this paper.", "labels": [], "entities": []}, {"text": "Accordingly, we report the performance of the method for context-windows of 1 \u2264 size \u2264 8 tokens, for three directions of around (hereafter, denoted by A), only to the left (denoted by L), or to the right (denoted by R) of candidate terms.", "labels": [], "entities": []}, {"text": "In addition, we construct vector spaces that encode information about the order of words in these contextwindows.", "labels": [], "entities": []}, {"text": "Hence, for each input corpus, 48 vector spaces are constructed to reflect each of the possible: The y-axis shows the observed NAP i for i = 8,900 (i.e., recall 100%).", "labels": [], "entities": [{"text": "NAP", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.9322338104248047}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9967325925827026}]}, {"text": "For each of the employed similarity measures, the x-axis shows the size of context windows.", "labels": [], "entities": []}, {"text": "The letters A, L, and R denote the direction in which context-windows are stretched (i.e., respectively, Around, Left, or Right side of the candidate terms).", "labels": [], "entities": []}, {"text": "Models that encode word order information are denoted using the on top.", "labels": [], "entities": []}, {"text": "The size of letters, however, shows the value of k.", "labels": [], "entities": []}, {"text": "The smallest size denotes k = 1 (black colour), while the largest size denotes k = 25 (grey colour); the medium size represents k = 7 (blue colour).", "labels": [], "entities": []}, {"text": "In these experiments, the computed NAP over c-value ranked terms (i.e., the baseline) is 0.27.", "labels": [], "entities": [{"text": "NAP", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.8842195868492126}]}, {"text": "For the sake of readability, for each configuration of context-windows size and the employed similarity metric, we plot only the best observed results (complete plots are provided as supplementary materials).", "labels": [], "entities": []}, {"text": "configurations of context-windows, listed above.", "labels": [], "entities": []}, {"text": "The performance of the proposed k-nn technique is affected by the value of k.", "labels": [], "entities": []}, {"text": "In the absence of a large training dataset, in the employed memory-based learning framework, a small value fork may lead to over-fitting and sensitivity to noise, while a large neighborhood estimation may reduce the discriminatory power of the classifier.", "labels": [], "entities": []}, {"text": "Therefore, we report the performance of the method for three values of neighborhood size, i.e., k \u2208 {1, 7, 25}.", "labels": [], "entities": []}, {"text": "As stated earlier, term weighting in Equation 1 is performed by the help of three different measures: the cosine similarity, the Euclidean, and the city block distance.", "labels": [], "entities": [{"text": "term weighting", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.652362585067749}]}], "tableCaptions": [{"text": " Table 1: Statistics of the terminological resource: terms and 'protein terms' are respectively abbreviated  by T and P (note P \u2282 T).", "labels": [], "entities": []}]}