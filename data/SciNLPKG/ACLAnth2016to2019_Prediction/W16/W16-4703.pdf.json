{"title": [{"text": "Recognition of non-domain phrases in automatically extracted lists of terms", "labels": [], "entities": [{"text": "Recognition of non-domain phrases in automatically extracted lists of terms", "start_pos": 0, "end_pos": 75, "type": "TASK", "confidence": 0.8070154428482056}]}], "abstractContent": [{"text": "In the paper, we address the problem of recognition of non-domain phrases in terminology lists obtained with an automatic term extraction tool.", "labels": [], "entities": [{"text": "recognition of non-domain phrases in terminology lists", "start_pos": 40, "end_pos": 94, "type": "TASK", "confidence": 0.8151917031833104}, {"text": "term extraction", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.7272111773490906}]}, {"text": "We focus on identification of multi-word phrases that are general terms and discourse function expressions.", "labels": [], "entities": []}, {"text": "We tested several methods based on domain corpora comparison and a method based on contexts of phrases identified in a large corpus of general language.", "labels": [], "entities": [{"text": "domain corpora comparison", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.692508856455485}]}, {"text": "We compared the results of the methods to manual annotation.", "labels": [], "entities": []}, {"text": "The results show that the task is quite hard as the inter-annotator agreement is low.", "labels": [], "entities": []}, {"text": "Several tested methods achieved similar overall results, although the phrase ordering varied between methods.", "labels": [], "entities": [{"text": "phrase ordering", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.7089646458625793}]}, {"text": "The most successful method with the precision about 0.75 at the half of the tested list was the context based method using a modified contextual diversity coefficient.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9984748959541321}]}], "introductionContent": [{"text": "Automatic term recognition (ATR) can be applied to achieve concept names which might be included in a domain ontology.", "labels": [], "entities": [{"text": "Automatic term recognition (ATR)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7713369578123093}]}, {"text": "However, lists of terms obtained in this way should be filtered to exclude terms belonging to different specialized domains which occurred within the text only by coincidence (e.g. citations); terms which are general, such as low level used in many different domains; and discourse markers like point of view.", "labels": [], "entities": []}, {"text": "It is difficult to consider that phrases such as low level or left side are domain specific, but they play an important role in several domains, e.g. medicine or technology.", "labels": [], "entities": []}, {"text": "Phrases like turning point or difficult question should be excluded from terminology lists.", "labels": [], "entities": []}, {"text": "While identification of domain terms has been addressed by several researchers, the problem of general terms identification has not been studied greatly, although it poses a much harder task to cope with.", "labels": [], "entities": [{"text": "identification of domain terms", "start_pos": 6, "end_pos": 36, "type": "TASK", "confidence": 0.845239594578743}, {"text": "general terms identification", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.6588182747364044}]}, {"text": "We propose identifying such phrases and building a separate resource to be combined with other domain specific ontologies.", "labels": [], "entities": []}, {"text": "The filtering out-of-domain terms has been the subject of several studies.", "labels": [], "entities": []}, {"text": "Most typical approaches are described in (), other attempts include ()) or (.", "labels": [], "entities": []}, {"text": "Discrimination of in-and out-of-domain terms was based on identifying terms occurring more frequently in the given domain related data than in other corpora.", "labels": [], "entities": []}, {"text": "Most of these approaches looked for terms which are more salient in particular corpora than in others and work relatively well for selecting specialized terms.", "labels": [], "entities": []}, {"text": "In this paper we focused our attention on terms which are nearly equally frequent in many corpora and thus are hard to classify either as domain specific or general.", "labels": [], "entities": []}, {"text": "We decided to focus on multi-word terms as most of them are not present in general wordnet-type datasets.", "labels": [], "entities": []}, {"text": "They are also easier to classify as either domain specific or general.", "labels": [], "entities": []}, {"text": "Thus, the evaluation of the proposed methods is more reliable.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our method we prepared two manually annotated lists.", "labels": [], "entities": []}, {"text": "The first one, called COM, consists of 7151 terms which occur in at least three of the six selected corpora.", "labels": [], "entities": []}, {"text": "Annotation was done by two annotators and then the third one resolved the conflicts to obtain the gold standard annotation (GS).", "labels": [], "entities": [{"text": "gold standard annotation (GS)", "start_pos": 98, "end_pos": 127, "type": "DATASET", "confidence": 0.5115104466676712}]}, {"text": "The annotators introduced five labels representing non-terms, general-terms, domain-terms-used-generally, domain-terms, improper-phrases.", "labels": [], "entities": []}, {"text": "At the evaluation stage as general-terms we treated the first three classes together.", "labels": [], "entities": []}, {"text": "includes the number of annotations of each type.", "labels": [], "entities": []}, {"text": "The difficulty of the task and the lack of the strict guidelines is reflected in a relatively low Cohen's kappa-coefficient which is equal to 0.45.", "labels": [], "entities": []}, {"text": "As the first test set contained a lot of phrases located very low on the ranked terminological lists, we also prepared the second test set (MFQ) to verify our context based method.", "labels": [], "entities": []}, {"text": "This test set is based on the first 1000 terms from the terminological lists obtained separately for all corpora except the medical one.", "labels": [], "entities": []}, {"text": "The resulting 3250 terms were annotated by the same two annotators.", "labels": [], "entities": []}, {"text": "To reduce the influence of the subjectivity of judgments (the kappa coefficient was 0.5), the final test set contains only 2341 terms which were annotated identically by both annotators.", "labels": [], "entities": [{"text": "kappa coefficient", "start_pos": 62, "end_pos": 79, "type": "METRIC", "confidence": 0.9143204689025879}]}, {"text": "964 terms are included in both test sets.", "labels": [], "entities": []}, {"text": "As our results are ranked lists, we had to introduce a threshold indicating which part of the lists should be treated as general terms.", "labels": [], "entities": []}, {"text": "For the first method, we selected terms which occur in at least 4 corpora; for the others, we treated 70% of the lists as general terms.", "labels": [], "entities": []}, {"text": "This is roughly the most desirable partition as the COM test set contains a little more than 73% of general terms.", "labels": [], "entities": [{"text": "COM test set", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.7809796730677286}]}, {"text": "gives the number of common annotations made using the above methods and the threshold.", "labels": [], "entities": []}, {"text": "For the evaluation of the IV method we performed the experiments in which we used two data sets and two lists of terms.", "labels": [], "entities": []}, {"text": "The first (art) corpus consisted of four of the corpora described in section 3 (all except the hospital data set -ChH).", "labels": [], "entities": [{"text": "hospital data set -ChH", "start_pos": 95, "end_pos": 117, "type": "DATASET", "confidence": 0.8567730844020843}]}, {"text": "It consists of about 845K tokens.", "labels": [], "entities": []}, {"text": "The second data set ( (nkjp+art) is much larger, with 1.3G words from the complete NKJP -National Corpus of Polish Language () added to the (art) corpus.", "labels": [], "entities": [{"text": "NKJP -National Corpus of Polish Language", "start_pos": 83, "end_pos": 123, "type": "DATASET", "confidence": 0.9145170535360064}]}, {"text": "The term list is the same list of 7151 terms described above.", "labels": [], "entities": []}, {"text": "While counting the diversity coefficient d(w) we only selected contexts which were   strings containing only lowercase letters.", "labels": [], "entities": []}, {"text": "We excluded named entities from this set.", "labels": [], "entities": []}, {"text": "We also disregarded the most common words (e.g. prepositions and pronouns).", "labels": [], "entities": []}, {"text": "For this purpose, we used the list of stop words from the Wikipedia page.", "labels": [], "entities": []}, {"text": "As the PPMI value is biased towards low frequency phenomena, we took into account only pairs which occur in NKJP more than 5 times.", "labels": [], "entities": []}, {"text": "For all methods we counted how many terms annotated as general in the GS file were found in each part of the ranked lists.", "labels": [], "entities": [{"text": "GS file", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.8407896757125854}]}, {"text": "The results for every 500 element segments are shown in, while shows the overall precision by steps of 500 terms.", "labels": [], "entities": [{"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9991770386695862}]}, {"text": "show that the most methods do not differ much.", "labels": [], "entities": []}, {"text": "The most stable results were achieved for IIa and the combination IIa+III.", "labels": [], "entities": []}, {"text": "For the latter method we tested several values of \u03b1 from 0.2 to 0.8 and the best results were obtained with \u03b1 0.4.", "labels": [], "entities": []}, {"text": "The methods I and III are shown to be the least consistent.", "labels": [], "entities": []}, {"text": "The method IV showed the quickest decrease of the percentage of the general terms for each five hundred positions, thus proving to be the most selective one.", "labels": [], "entities": [{"text": "quickest", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9894825220108032}]}, {"text": "In the second experiment, in which we check the contexts of the phrases, the results obtained fora small corpus containing four sets described in Section 3 (IV art ) turned out to be rather poor.", "labels": [], "entities": []}, {"text": "The list of terms with non-zero related contexts was very short -it contained only 301 elements.", "labels": [], "entities": []}, {"text": "The resulting precision was only 0.33.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9995456337928772}]}, {"text": "For this data set, the addition of similar terms (IV art add ) improved the results.", "labels": [], "entities": [{"text": "IV art add )", "start_pos": 50, "end_pos": 62, "type": "METRIC", "confidence": 0.9362922012805939}]}, {"text": "In this approach we found relevant contexts for 948 terms with a precision equal to 0.64 for the first 500 elements and 0.5 for the entire set.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9965291619300842}]}, {"text": "For the big corpus, the results achieved by adding similar terms (IV nkjp+art add ) were slightly worse, as was expected.", "labels": [], "entities": []}, {"text": "summarizes the results and presents the precision obtained by all our methods for the first 500 elements and for the entire set (* indicates that the method did not process the entire COM list).", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9990453124046326}]}, {"text": "In the next set of experiments we tested more extensively different variants of the IV method which is based on contextual information.", "labels": [], "entities": []}, {"text": "On two term test sets described above, apart form the basic version of the method, we tested the newly introduced d M coefficient and the non-uniform treatment of the context words.", "labels": [], "entities": []}, {"text": "Ina weighted d w schema we assigned smaller weights to context words which are more distant from the given term (in a 5 word window, the farthest word has weight equal to 0.2 while the closest neighbour has the weight of 1).", "labels": [], "entities": []}, {"text": "We performed tests on the big nkjp+art corpus.", "labels": [], "entities": [{"text": "nkjp+art corpus", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.674496628344059}]}, {"text": "The results shown in confirm improvement in cases where the d M coefficient was used.", "labels": [], "entities": []}, {"text": "The number of the general terms at the beginning of the list is higher and this proportion constantly decreases, which was not the case for the other methods.", "labels": [], "entities": []}, {"text": "The non-uniform weighting of context words caused deterioration of results.", "labels": [], "entities": []}, {"text": "shows how many terms were filtered out from the top part of terms in the 5 domain corpora.", "labels": [], "entities": []}, {"text": "We tested lists of, at most 1800, top general terms obtained by 9 methods separately.", "labels": [], "entities": []}, {"text": "We tested only the top parts of all domain term lists consisting of 10K terms.", "labels": [], "entities": []}, {"text": "It shows that method III is more efficient in eliminating phrases from the top of the term list than the other methods.", "labels": [], "entities": []}, {"text": "Unfortunately, it concerns both types of terms: out-of-domain terms and false positive out-of-domain terms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1.  We observed that although the total number of multi-word terms constitute about one third of all term  occurrences, the number of different phrases is much higher than one half of all of them.", "labels": [], "entities": []}, {"text": " Table 2: Common multi-word terms", "labels": [], "entities": []}, {"text": " Table 7: Common annotations for COM test set", "labels": [], "entities": []}, {"text": " Table 8: Precision of all the methods -COM test set", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9812589287757874}]}, {"text": " Table 9: Precision of different variants of IV method, nkjp+art corpus", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9684776663780212}]}, {"text": " Table 10: Filtered out out-of-domain terms in 10K top terms", "labels": [], "entities": []}]}