{"title": [{"text": "Collaborative development of a rule-based machine translator between Croatian and Serbian", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the development and current state of a bidirectional Croatian-Serbian machine translation system based on the open-source Apertium platform.", "labels": [], "entities": [{"text": "Croatian-Serbian machine translation", "start_pos": 74, "end_pos": 110, "type": "TASK", "confidence": 0.5615604817867279}]}, {"text": "It has been created inside the Abu-MaTran project with the aims of creating free linguistic resources as well as having non-experts and experts work together.", "labels": [], "entities": []}, {"text": "We describe the collaborative way of collecting the necessary data to build our system, which outperforms other available systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Croatian and Serbian are language varieties and official registers of the pluricentric Bosnian-Croatian-Montenegrin-Serbian (BCMS) language.", "labels": [], "entities": []}, {"text": "Although mutually intelligible, the national varieties are standardised differently, and both communities have a high interest to produce documentation that adheres to these standards, if for no other reason, then for the sake of producing standard documents for Serbian, the official language of an EU candidate state.", "labels": [], "entities": []}, {"text": "Thus it is sensible to make use of a related language of a recent member state and employ machine translation between these two language varieties to meet this aim.", "labels": [], "entities": []}, {"text": "Creating machine translation (MT) systems for South-Slavic languages, both between themselves and other languages, is also the aim of the Abu-MaTran project.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.8608034610748291}]}, {"text": "In the first phase of the project, the focus was on MT between English and Croatian, while MT between South-Slavic is the focus of the second phase.", "labels": [], "entities": [{"text": "MT between English and Croatian", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.8651077747344971}, {"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9781326055526733}]}, {"text": "The system presented in this paper will be used within the project to increase the amount of English -Serbian parallel data by translating the Croatian side of English-Croatian parallel data to Serbian.", "labels": [], "entities": []}, {"text": "It will also be added to another by-product of the Abu-MaTran project -AltLang -a service for translating between language varieties. and their open-source Apertium platform have shown that, when doing machine translation between language variants or closely-related languages like Spanish and Catalan, a rule-based shallow transfer approach is often sufficient to produce good quality translations.", "labels": [], "entities": [{"text": "translating between language varieties.", "start_pos": 94, "end_pos": 133, "type": "TASK", "confidence": 0.8372794538736343}]}, {"text": "Indeed, work has already been done in building rule based translators from BCMS into).", "labels": [], "entities": [{"text": "rule based translators from BCMS", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.5353906750679016}]}, {"text": "To our knowledge, however, no similar work has been done for the Croatian-Serbian language pair specifically.", "labels": [], "entities": []}, {"text": "The only accessible state of the art system for this pair is Google Translate, 6 which reaches a BLEU score of 82.27 in the Serbian-Croatian direction.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9806198477745056}]}, {"text": "However, the statistical approach that Google uses, which has also been explored in) but only using small corpora, is not a feasible option for us, as there are not enough parallel corpora available to train SMT systems that can deal with the minute differences between the two languages without introducing additional noise.", "labels": [], "entities": [{"text": "SMT", "start_pos": 208, "end_pos": 211, "type": "TASK", "confidence": 0.9890396595001221}]}], "datasetContent": [{"text": "Finally, we perform a comparative evaluation of our system, but we present an evaluation of only the Serbian to Croatian direction as this direction was the initial focus of the development and the other direction was still underdevelopment at the moment of presenting these results.", "labels": [], "entities": []}, {"text": "We compare our system to the output of Google Translate, 19 as this is the current state of the art system.", "labels": [], "entities": []}, {"text": "For our baseline we assume that the output is identical to the input, a setup which yields the lowest evaluation scores.", "labels": [], "entities": []}, {"text": "Our SMT baseline was constructed by training a phrase-based Moses system on 200k segments from the SETimes parallel corpus, with an additional 2 thousand segments of development data, while we use hrWaC2.0 for building the language model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.977378249168396}, {"text": "SETimes parallel corpus", "start_pos": 99, "end_pos": 122, "type": "DATASET", "confidence": 0.8509639898935953}]}, {"text": "For the evaluation we use a test set consisting of 351 Serbian sentences gathered from newspaper texts that were manually translated into Croatian by students.", "labels": [], "entities": []}, {"text": "We evaluate the system with BLEU () and TER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9995372295379639}, {"text": "TER", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9992642998695374}]}, {"text": "When compared to our baseline systems, the evaluation scores are decidedly positive.", "labels": [], "entities": []}, {"text": "When compared to Google's system, we also improve, but the question is whether this improvement is statistically significant.", "labels": [], "entities": []}, {"text": "To calculate this we use approximate randomisation with 1000 iterations, and while the reported 0.7 point improvement in BLEU yields a p-value of 0.384, which is too high to prove statistical significance, the improvement in TER by -0.0091 is in fact statistically significant, with a p-value of 0.018.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9983159303665161}, {"text": "TER", "start_pos": 225, "end_pos": 228, "type": "METRIC", "confidence": 0.9976104497909546}]}, {"text": "Given that BLEU is known to favour statistical machine translation in its evaluation, it is safe to claim that our system outperforms that of Google.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9376334547996521}, {"text": "statistical machine translation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.6665389835834503}]}], "tableCaptions": [{"text": " Table 1. Results of the MT evaluation. Statistically significantly better results are in bold.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9892628788948059}]}]}