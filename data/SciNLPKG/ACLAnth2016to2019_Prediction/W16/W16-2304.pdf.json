{"title": [], "abstractContent": [{"text": "This paper describes LIMSI's submissions to the shared WMT'16 task \"Transla-tion of News\".", "labels": [], "entities": [{"text": "WMT'16 task \"Transla-tion of News", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.5418071150779724}]}, {"text": "We report results for Romanian-English in both directions, for English to Russian, as well as preliminary experiments on reordering to translate from English into German.", "labels": [], "entities": []}, {"text": "Our submissions use mainly NCODE and MOSES along with continuous space models in a post-processing step.", "labels": [], "entities": [{"text": "NCODE", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.9502841830253601}, {"text": "MOSES", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.6325470209121704}]}, {"text": "The main novelties of this year's participation are the following: for the translation into Russian and Romanian, we have attempted to extend the output of the decoder with morphological variations and to use a CRF model to rescore this new search space; as for the translation into German, we have been experimenting with source-side pre-ordering based on a dependency structure allowing permutations in order to reproduce the target word order.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper documents LIMSI's participation to the shared task of machine translation of news for three language pairs: English to Russian, Romanian-English in both directions and English to German.", "labels": [], "entities": [{"text": "machine translation of news", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.8590082973241806}]}, {"text": "The reported experiments are mainly related to two challenging domains: inflection prediction and word order in morphologically rich languages.", "labels": [], "entities": [{"text": "inflection prediction", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.7199692726135254}]}, {"text": "In our systems translating from English into Romanian and Russian, we have attempted to address the difficulties that go along with translating into morphologically reach languages.", "labels": [], "entities": []}, {"text": "First, a baseline system outputs sentences in which we reconsider the choices previously made for inflected words by generating their full paradigm.", "labels": [], "entities": []}, {"text": "Second, a CRF model is expected to make better choices than the translation system.", "labels": [], "entities": []}, {"text": "For English to German, experiments are reported on the pre-ordering of the source sentence.", "labels": [], "entities": []}, {"text": "Using the dependency structure of the sentence, the model predicts permutations of source words that lead to an order that is as close as possible to the right order in the target language.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all our experiments, the MT systems are tuned using the kb-mira algorithm (Cherry and Foster, 2012) implemented in MOSES, including the reranking step.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9762020707130432}, {"text": "MOSES", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.7945149540901184}]}, {"text": "POS tagging is performed using the TreeTagger ( for English and Russian (Sharoff and Nivre, 2011), and TTL (Tufis\u00b8etTufis\u00b8Tufis\u00b8et al., 2008) for Romanian.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7781162261962891}, {"text": "TTL", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.8508740067481995}]}, {"text": "The experimental results were not conclusive, as, in the best configuration for Russian our model achieved the same results as the baseline and slightly worsened the NCODE+SOUL system (see  As for Romanian, our model performed worse than for Russian.", "labels": [], "entities": []}, {"text": "We assume that this must be partly due to the sparsity of the lexicon used for Romanian, with which we only generated partial paradigms, as opposed to full paradigms for Russian.", "labels": [], "entities": []}, {"text": "NCODE translates a sentence by first re-ordering the source sentence and then monotonically decoding it.", "labels": [], "entities": [{"text": "NCODE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9296146035194397}]}, {"text": "Reorderings of the source sentence are compactly encoded in a permutation lattice generated by iteratively applying POS-based reordering rules extracted from the parallel data.", "labels": [], "entities": []}, {"text": "In this year's WMT evaluation campaign we investigated ways to improve the re-ordering step by re-implementing the approach proposed by.", "labels": [], "entities": [{"text": "WMT evaluation", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.6163804531097412}]}, {"text": "This approach aims at taking advantage of the dependency structure of the source sentence to predict a permutation of the source words that is as close as possible to a correct syntactic word order in the target language: starting from the root of the dependency tree a classifier is used to recursively predict the order of anode and all its children.", "labels": [], "entities": []}, {"text": "More precisely, fora family 5 of size n, a multiclass classifier is used to select the best ordering of this family among its n!", "labels": [], "entities": []}, {"text": "A different classifier is trained for each possible family size.", "labels": [], "entities": []}, {"text": "Predicting the best re-ordering These experiments were only performed for English to German translation.", "labels": [], "entities": []}, {"text": "The source sentences were PoS-tagged and dependency parsed using the MATEPARSER (Bohnet and Nivre, 2012) trained on the UDT v2.0.", "labels": [], "entities": [{"text": "MATEPARSER", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9284184575080872}, {"text": "UDT v2.0", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.8434593975543976}]}, {"text": "The parallel source and target sentences were aligned in both directions with) and these alignments were merged with the intersection heuristic.", "labels": [], "entities": []}, {"text": "The training set used to learn the classifiers is generated as follows: during a depth-first traversal of each source sentence, an example is extracted from anode if each child of this node is aligned with exactly one word in the target sentence.", "labels": [], "entities": []}, {"text": "In this case, it is possible, by following the alignment links, to extract the order of the family members in the target language.", "labels": [], "entities": []}, {"text": "An example is therefore a permutation of n members (1 head and its n \u2212 1 children).", "labels": [], "entities": []}, {"text": "In practice, we did not extract training examples from families having more than 8 members and train 7 classifiers (one binary classifier for the family made of ahead and a single dependent and 6 multi-class classifiers able to discriminate between up to 5 040 classes).", "labels": [], "entities": []}, {"text": "Our experiments used Following (, we call family ahead in a dependency tree and all its children.", "labels": [], "entities": [{"text": "Following", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9906626343727112}]}, {"text": "6 Preliminary experiments with the gdfa heuristic showed that the symmetrization heuristic has no impact on the quality of the predicted pre-ordering.", "labels": [], "entities": []}, {"text": "Families with more than 8 members account for less than 0.5% of the extracted examples.", "labels": [], "entities": []}, {"text": "VOWPAL WABBIT, a very efficient implementation of the logistic regression capable to handle a large number of output classes.", "labels": [], "entities": [{"text": "VOWPAL", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7270283102989197}, {"text": "WABBIT", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.7790288925170898}]}, {"text": "The features used for training are the same as those proposed by: word forms, PoStags, relative positions of the head, children, their siblings and the gaps between them, etc.", "labels": [], "entities": []}, {"text": "Building permutation lattices In order to mitigate the impact of errouneously predicted word preorderings, we propose to build lattices of permutations rather than using just one reordering of the source sentence.", "labels": [], "entities": []}, {"text": "This lattice includes the two best predicted permutations at each node.", "labels": [], "entities": []}, {"text": "It is built as follows: starting from an automaton with a single arc between the initial state and the final state labeled with the ROOT token, each arc is successively substituted by two automata describing two possible re-orderings of the token t corresponding to this arc label and its children in the dependency tree.", "labels": [], "entities": []}, {"text": "Each of these automata has n + 1 arcs corresponding to then children oft in the dependency tree and t itself that appear in the predicted order.", "labels": [], "entities": []}, {"text": "The weight of the first arc is defined as the probability predicted by the classifier; all other arcs have a weight of 0.", "labels": [], "entities": []}, {"text": "MT experiments We report preliminary results for pre-ordering.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7553654909133911}]}, {"text": "All the source side of training data is reordered using the method described above.", "labels": [], "entities": []}, {"text": "Then, the reordered source side, along with the target side, are considered as the new parallel training data on which anew NCODE system is trained (including new word alignment, tuple extraction, ...).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 163, "end_pos": 177, "type": "TASK", "confidence": 0.7449508607387543}, {"text": "tuple extraction", "start_pos": 179, "end_pos": 195, "type": "TASK", "confidence": 0.6960297673940659}]}, {"text": "For tuning and test steps, the learned classifiers are used to generate a permutation lattice that will be decoded.", "labels": [], "entities": []}, {"text": "In the following experiments, we use only news-commentary and Europarl datasets as parallel training data; the development and test sets are, respectively, newstest-2014 and newstest-2015.", "labels": [], "entities": [{"text": "Europarl datasets", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.9789456725120544}]}, {"text": "These preliminary experiments show a significant decrease in BLEU score which deserves closer investigations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9753310978412628}]}, {"text": "This performance drop is more important when more reordering paths (\"2-best\" in) are proposed to the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.9277792572975159}]}, {"text": "A similar trend was also observed when using a dependency-based model only to predict the reordering lattices fora system trained on raw data and without the pre-ordering step.", "labels": [], "entities": []}, {"text": "As shown in: Translation results for pre-ordering on the English to German translation task the members of a family have the same order in the source and in the target languages, a trend that is probably amplified by our instance extraction strategy.", "labels": [], "entities": [{"text": "Translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.969977855682373}, {"text": "instance extraction", "start_pos": 221, "end_pos": 240, "type": "TASK", "confidence": 0.6764948964118958}]}, {"text": "Dealing with skewed classes is a challenging problem in machine learning and it is not surprising that the performance of the classifier is rather low for the minority classes (see results in).", "labels": [], "entities": []}, {"text": "It is interesting to note that the standard rule-based approach does not suffer from the class imbalance problem as all re-orderings observed in the training data are considered without taking into account their probability.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results (BLEU) for English-Russian  with NCODE and MOSES on the official test.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.965631902217865}, {"text": "NCODE", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.8211097121238708}, {"text": "MOSES", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9543148875236511}]}, {"text": " Table 2: Results (BLEU) for English:Romanian  with NCODE and MOSES on the official test.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9561693668365479}, {"text": "NCODE", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.5634585022926331}, {"text": "MOSES", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9815630912780762}]}, {"text": " Table 3: Translation results for pre-ordering on the  English to German translation task", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9485691785812378}]}, {"text": " Table 4: % of family that have the same order in English and German (% mono.), overall prediction  performance (prec.) as well as precision for monotonic and non-monotonic reordering.", "labels": [], "entities": [{"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9994495511054993}]}]}