{"title": [{"text": "Vanilla Classifiers for Distinguishing between Similar Languages", "labels": [], "entities": [{"text": "Distinguishing between Similar Languages", "start_pos": 24, "end_pos": 64, "type": "TASK", "confidence": 0.8539139926433563}]}], "abstractContent": [{"text": "In this paper we describe the submission of the UniBuc-NLP team for the Discriminating between Similar Languages Shared Task, DSL 2016.", "labels": [], "entities": [{"text": "Discriminating between Similar Languages Shared Task, DSL 2016", "start_pos": 72, "end_pos": 134, "type": "TASK", "confidence": 0.7734758257865906}]}, {"text": "We present and analyze the results we obtained in the closed track of sub-task 1 (Similar languages and language varieties) and sub-task 2 (Arabic dialects).", "labels": [], "entities": []}, {"text": "For sub-task 1 we used a logistic regression classifier with tf-idf feature weighting and for sub-task 2 a character-based string kernel with an SVM classifier.", "labels": [], "entities": []}, {"text": "Our results show that good accuracy scores can be obtained with limited feature and model engineering.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9993593096733093}]}, {"text": "While certain limitations are to be acknowledged, our approach worked surprisingly well for out-of-domain, social media data, with 0.898 accuracy (3 rd place) for dataset B1 and 0.838 accuracy (4 th place) for dataset B2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9970966577529907}, {"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9965214729309082}]}], "introductionContent": [{"text": "Automatic language identification is the task of determining the language in which apiece of text is written using computational methods.", "labels": [], "entities": [{"text": "Automatic language identification", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6498235662778219}]}, {"text": "In today's context of multilingualism, and given the rapid development of the online repositories of cross-language information, language identification is an essential task for many downstream applications (such as cross-language information retrieval or question answering), to route the documents to the appropriate NLP systems, based on their language.", "labels": [], "entities": [{"text": "language identification", "start_pos": 129, "end_pos": 152, "type": "TASK", "confidence": 0.7261160910129547}, {"text": "cross-language information retrieval", "start_pos": 216, "end_pos": 252, "type": "TASK", "confidence": 0.7109668850898743}, {"text": "question answering", "start_pos": 256, "end_pos": 274, "type": "TASK", "confidence": 0.8016100823879242}]}, {"text": "Although language identification has been intensively studied in the recent period, there are still questions to be answered.", "labels": [], "entities": [{"text": "language identification", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.7502955198287964}]}, {"text": "Language identification is still a challenging research problem for very similar languages and language varieties, for very short pieces of text, such as tweets, or for documents involving code-switching (the practice of mixing more languages within a single communication).", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7283681333065033}]}, {"text": "The DSL 2016 shared task () tackles two interesting aspects of language identification: similar language and language varieties (with in-domain and out-of-domain -social media datatest sets) and Arabic dialects.", "labels": [], "entities": [{"text": "DSL 2016 shared task", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8867335319519043}, {"text": "language identification", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.7396782338619232}]}, {"text": "In this paper we present the submission of the UniBuc-NLP team for the closed track (using only the training data provided by the organizers) of both sub-tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using the experimental setup previously described, we developed several systems for discriminating between similar languages and language varieties (sub-task 1) and between Arabic dialects (sub-task 2).", "labels": [], "entities": []}, {"text": "The organizers provided three test datasets, two for sub-task 1 and one for sub-task 2.", "labels": [], "entities": []}, {"text": "In we provide a brief characterization of the datasets:  Task # instances A In-domain: newspaper texts Sub-task 1 12,000 B1 Out of domain: social media data Sub-task 1 500 B2 Out of domain: social media data Sub-task 1 500 C ASR texts from Arabic dialects Sub-task 2 1,540 Sub-task 1 Our two runs for sub-task 1 are as follows: \u2022 Run 1: a one-level system.", "labels": [], "entities": []}, {"text": "The first system consists of a single logistic regression classifier that predicts the language or language variety.", "labels": [], "entities": []}, {"text": "\u2022 Run 2: a two-level system.", "labels": [], "entities": []}, {"text": "The second system consists of multiple logistic regression classifiers: we train a classifier to predict the language group (\"inter-group classifier\"), and one classifier for each language group (\"intra-group classifier\"), to predict the language or language variety within the group.", "labels": [], "entities": []}, {"text": "For the one-level system we obtained 0.8441 accuracy when evaluating on the development dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9992989301681519}]}, {"text": "For the two-level system we obtained 0.9972 accuracy for the inter-group classifier, and the following values for the intra-group classifiers: 0.7510 for es, 0.8940 for fr, 0.9207 for pt, 0.7848 for bs-hr-sr, 0.9820 for id-my.: The results of the UniBuc-NLP team for sub-task 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.999447762966156}]}, {"text": "In we report the results that we obtained for the test datasets.", "labels": [], "entities": []}, {"text": "Our best results for each dataset are as follows: 0.8648 accuracy (11 th place) for dataset A, 0.8980 accuracy (3 rd place) for dataset B1 and 0.8380 accuracy (4 th place) for dataset B2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9979068040847778}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9949304461479187}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9950049519538879}]}, {"text": "For two of the three datasets (A, B2), the two-level system obtained better results than the one-level system.", "labels": [], "entities": []}, {"text": "However, our highest accuracy (0.8990) was obtained by the one-level system for dataset B1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9991729855537415}]}, {"text": "In we report the most informative features for each class.", "labels": [], "entities": []}, {"text": "With few exceptions, most of the informative features are unigrams.", "labels": [], "entities": []}, {"text": "While for the language classifiers many of these features are named entities (such as references to geographical regions or names of persons), as expected, for the language group classifier) the situation is different: mostly very short words prove to have high discriminative power.", "labels": [], "entities": []}, {"text": "Among others, we identified definite and indefinite articles -\"los\" (es), \"le\" (fr) -and functional words -\"nao\" (pt), \"dalam\" (id-my) -ranked among the most informative features.", "labels": [], "entities": []}, {"text": "Despite the fact that quite many of the top features are named entities, which could a suggest a topic bias in classification, our systems obtain a good performance on out-of-domain data, ranking 3 rd and 4 th on the social media datasets.", "labels": [], "entities": []}, {"text": "Both our systems outperform significantly a random baseline that obtains 0.0883 F1 score for dataset A and 0.20 for datasets B1 and B2.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9879820048809052}]}, {"text": "The most informative features for the two-level system for sub-task 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the dataset of similar languages and language varieties (sub-task 1).", "labels": [], "entities": []}, {"text": " Table 2: Statistics for the dataset of Arabic dialects (sub-task 2).", "labels": [], "entities": []}, {"text": " Table 4: The results of the UniBuc-NLP team for sub-task 1.", "labels": [], "entities": []}, {"text": " Table 5: The results of the UniBuc-NLP team for sub-task 2.", "labels": [], "entities": []}]}