{"title": [{"text": "Effect of Data Annotation, Feature Selection and Model Choice on Spatial Description Generation in French", "labels": [], "entities": [{"text": "Feature Selection", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.6602081954479218}, {"text": "Spatial Description Generation", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.901680568854014}]}], "abstractContent": [{"text": "In this paper, we look at automatic generation of spatial descriptions in French, more particularly , selecting a spatial preposition fora pair of objects in an image.", "labels": [], "entities": [{"text": "automatic generation of spatial descriptions", "start_pos": 26, "end_pos": 70, "type": "TASK", "confidence": 0.8389378309249877}]}, {"text": "Our focus is on assessing the effect on accuracy of (i) increasing data set size, (ii) removing synonyms from the set of prepositions used for annotation, (iii) opti-mising feature sets, and (iv) training on best prepositions only vs. training on all acceptable prepositions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9985867738723755}]}, {"text": "We describe anew data set where each object pair in each image is annotated with the best and all acceptable prepositions that describe the spatial relationship between the two objects.", "labels": [], "entities": []}, {"text": "We report results for three new methods for this task, and find that the best, 75% Accuracy, is 25 points higher than our previous best result for this task.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9971622824668884}]}], "introductionContent": [{"text": "The research in this paper addresses the area of image description generation with applications in automatic image captioning and assistive technologies.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.8322052756945292}, {"text": "automatic image captioning", "start_pos": 99, "end_pos": 125, "type": "TASK", "confidence": 0.659805566072464}]}, {"text": "An important aspect, and long-standing research topic, is to identify the entities, or objects, in images.", "labels": [], "entities": []}, {"text": "However, a good image description will also say something about how entities relate to each other, not just list them.", "labels": [], "entities": []}, {"text": "Spatial relations, and prepositions to express them, are particularly important in this context, but until very recently there had been no research directly aimed at this subtask, although some research came close (.) did address the subtask, but with hardwired rules for just eight preposition.", "labels": [], "entities": []}, {"text": "The work reported by is closely related to our work and also uses geometric and label features to predict prepositions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To compare results in this paper, we use variants of Accuracy from our previous work (.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9988847374916077}]}, {"text": "The dimension along which the variants we use here differ is output rank.", "labels": [], "entities": []}, {"text": "Different variants, denoted Acc(n), where n = 1...4, return Accuracy rates for the top n outputs produced by systems, such that a system output is considered correct if a target (human-selected) output is among the top n outputs produced by the system (so for s = 1 the measure is just standard Accuracy).", "labels": [], "entities": [{"text": "Acc", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9973529577255249}, {"text": "Accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9991204142570496}, {"text": "Accuracy", "start_pos": 295, "end_pos": 303, "type": "METRIC", "confidence": 0.9982377290725708}]}, {"text": "2 Implemented using scikit-learn (http://scikit-learn.org).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Acc(1) results for the data with the larger (DS-21) and", "labels": [], "entities": [{"text": "Acc", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9990350008010864}]}, {"text": " Table 2: Acc(1) and Acc(2) results for the smaller (DS-17-2o)", "labels": [], "entities": [{"text": "Acc", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9994189739227295}, {"text": "Acc", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9992632269859314}]}, {"text": " Table 3: Acc(1), Acc(2) and Acc(3) results for DS-17, before and after feature optimisation, for the three best models.", "labels": [], "entities": [{"text": "Acc", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9993718266487122}, {"text": "Acc", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9955717325210571}, {"text": "Acc(3)", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9405588656663895}, {"text": "DS-17", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.8689529895782471}]}]}