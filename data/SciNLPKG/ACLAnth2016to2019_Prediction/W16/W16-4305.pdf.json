{"title": [{"text": "A graphical framework to detect and categorize diverse opinions from online news", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a framework to extract diverse opinionated sentences within a given news article, by introducing the concept of diversity in a graphical model for opinion detection.", "labels": [], "entities": [{"text": "opinion detection", "start_pos": 167, "end_pos": 184, "type": "TASK", "confidence": 0.7890757620334625}]}, {"text": "We conduct extensive evaluation and find that the proposed modification leads to impressive improvements in performance and makes the final results of the model more usable.", "labels": [], "entities": []}, {"text": "The proposed method (OP-D) not only performs much better than the other techniques used for opinion detection and introducing diversity, but is also able to select opinions from different categories (Asher et al., 2009).", "labels": [], "entities": [{"text": "opinion detection", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.8378430902957916}]}, {"text": "By developing a classification model which categorizes the identified sentences into various opinion categories, we find that OP-D is able to push opinions from different categories uniformly among the top opinions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Online publishing houses desire to develop engagement of users around the articles published on their websites.", "labels": [], "entities": []}, {"text": "An important aspect of user engagement is commenting on the article and subsequently building up a conversation around it.", "labels": [], "entities": []}, {"text": "In order to facilitate meaningful conversation, an option might be to identify and highlight specific relevant portions of the article, which may act as a seed for such conversation.", "labels": [], "entities": []}, {"text": "For ensuring wide engagement, it would be best if the sentences chosen are opinions expressed in the article, as unlike factual statements, opinions might easily kick-start discussions.", "labels": [], "entities": []}, {"text": "Further, to be able to engage a wide range of audience, it would be helpful if each chosen sentence expresses different context than the other.", "labels": [], "entities": []}, {"text": "In general, all opinions are not of the same type.", "labels": [], "entities": []}, {"text": "Opinions can be categorized into various categories and sub-categories (, and it would be ideal if the extracted opinions cover multiple such categories.", "labels": [], "entities": []}, {"text": "Some examples of these categories are provided below: 1) Report : e.g., Christie's staffs have denied Zimmer's allegation.", "labels": [], "entities": [{"text": "Report", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9861006140708923}, {"text": "Christie's staffs", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.9399827321370443}]}, {"text": "2) Judgment : e.g., McGreevey's lover was being paid 11000 Dollar even though he was wildly unqualified for the position.", "labels": [], "entities": [{"text": "Judgment", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9925001263618469}, {"text": "McGreevey", "start_pos": 20, "end_pos": 29, "type": "DATASET", "confidence": 0.8129724264144897}]}, {"text": "3) Advise : e.g., Let's shoot at the opposition not our own troops, one Insider pleaded.", "labels": [], "entities": []}, {"text": "4) Sentimental : e.g., So why do so many people enjoy ridiculing my New Jersey One word Jealousy?", "labels": [], "entities": []}, {"text": "Opinion analysis has been a major field of study in natural language processing and data mining for many years.", "labels": [], "entities": [{"text": "Opinion analysis", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.907120943069458}, {"text": "natural language processing", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6378259460131327}, {"text": "data mining", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.7159630954265594}]}, {"text": "Several works such as () focus on opinion mining.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.8591091930866241}]}, {"text": "Opinion mining is very similar to subjectivity classification where subjective nature indicates the tendency of expressing one's thoughts and opinions.", "labels": [], "entities": [{"text": "Opinion mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.829303503036499}, {"text": "subjectivity classification", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7272954881191254}]}, {"text": "Work has been done in the past for developing classifiers (Wiebe and) which separate subjective sentences from objective ones, using several features present in the sentences.", "labels": [], "entities": []}, {"text": "() describes how to predict certainty (factuality) of text (e.g., tweet) by using keywords collected from source introducing predicates (cues) and groups.", "labels": [], "entities": [{"text": "certainty", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.8923305869102478}]}, {"text": "These models focus only on the local context (takes no global context into account) from a sentence to measure its subjectivity.", "labels": [], "entities": []}, {"text": "Side by side, there have been works where graphical models have been proposed to capture the global context, where the sentences are treated as nodes in the graph and a similarity measure between sentences is defined to build this graph.", "labels": [], "entities": []}, {"text": "While some of the approaches use PageRank to model each node in a similar manner (), HITS framework has also been used that establishes a relationship between opinions and supporting facts by modeling opinions as hubs and facts as authorities ().", "labels": [], "entities": []}, {"text": "None of these works, however, focus on finding diverse opinions from an article.", "labels": [], "entities": []}, {"text": "Experiments on MPQA and Yahoo datasets (both are English datasets) show that this leads to a sub-optimal performance, while trying to extract the most opinionated sentences in a news article.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.9474728107452393}, {"text": "Yahoo datasets", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.8637062609195709}]}, {"text": "The graphical models end up choosing similar sentences -which is not ideal to enable wide-ranging user engagement.", "labels": [], "entities": []}, {"text": "We, therefore, attempt to modify a variant of graphical model proposed in) to introduce diversity.", "labels": [], "entities": []}, {"text": "The basic idea of our approach is that once anode (sentence) is selected as an opinion because of a high hub score (as per the HITS framework used in ()), we can discount the hub scores for the nodes it links to, as these might be sub-opinions supporting the main opinion, and discounting their hub scores might improve diversity.", "labels": [], "entities": []}, {"text": "We find that this simple modification to the earlier framework leads to impressive improvement in performance for (i) Classifying opinions and facts, and (ii) Identifying diverse opinion categories in both datasets.", "labels": [], "entities": [{"text": "Classifying opinions and facts", "start_pos": 118, "end_pos": 148, "type": "TASK", "confidence": 0.8999073654413223}, {"text": "Identifying diverse opinion categories", "start_pos": 159, "end_pos": 197, "type": "TASK", "confidence": 0.8811115473508835}]}, {"text": "Extensive experimental results are reported to show that as a result of this modification, the output opinions can be used more meaningfully.", "labels": [], "entities": []}, {"text": "Note that the proposed technique is unique from the general work on diversity) in that we introduce diversity in a model with two different kinds of nodes in the document graph, as opposed to the other algorithms, which treat all the sentences equally.", "labels": [], "entities": []}, {"text": "Further, to understand the distribution of extracted opinions from online news in various categories, e.g., Report, Judgment, Advise and Sentiment etc.", "labels": [], "entities": []}, {"text": "(, we develop an opinion classification model.", "labels": [], "entities": [{"text": "opinion classification", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7292451411485672}]}, {"text": "While classification of opinions into various sentiment levels such as positive, negative and neutral has been tried (, automated classification of opinions into various categories is not available.", "labels": [], "entities": []}, {"text": "Analysis using the opinion classification model shows that our algorithm (OP-D) actually adds diversity even at the category level by selecting opinions from different opinion categories.", "labels": [], "entities": [{"text": "opinion classification", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7243546843528748}]}], "datasetContent": [{"text": "To investigate the effectiveness of the proposed framework, experiments are conducted using two different datasets, a) the standard Multi-Perspective Question Answering (MPQA) dataset (contains 535 documents) and b) 120 news articles crawled from Yahoo news.", "labels": [], "entities": [{"text": "Multi-Perspective Question Answering (MPQA)", "start_pos": 132, "end_pos": 175, "type": "TASK", "confidence": 0.7475974361101786}]}, {"text": "Each document is a news article pertaining to some topic.", "labels": [], "entities": []}, {"text": "In the MPQA dataset, each sentence is classified as either opinionated or factual by checking for the presence of certain subjective elements as annotated by the authors of the corpus (Wiebe  From the 535 documents in the MPQA dataset, we randomly select 100 documents for the test set.", "labels": [], "entities": [{"text": "MPQA dataset", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.983542263507843}, {"text": "MPQA dataset", "start_pos": 222, "end_pos": 234, "type": "DATASET", "confidence": 0.9812949001789093}]}, {"text": "Similarly, 25 documents are selected randomly from the Yahoo dataset for testing.", "labels": [], "entities": [{"text": "Yahoo dataset", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.936424195766449}]}, {"text": "Note that training and test splits are required for the first stage NB classifier.", "labels": [], "entities": []}, {"text": "We first perform 5-fold cross validation experiments on the training sets.", "labels": [], "entities": []}, {"text": "Then the entire training set is used to train the NB classifier and the results are reported on the test set.: Variation of precision and recall for different k (% of edges) and weight (w) for OP-D Parameter Fixing: The parameters that we needed to fix for the proposed algorithm were: weight of the edges (absolute (wt) or thresholded (1/0)), k for the top k% edges if thresholded weights are used and the constant \u03bb in Equation 2.", "labels": [], "entities": [{"text": "Variation", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9818265438079834}, {"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9982079267501831}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.999354898929596}, {"text": "Equation", "start_pos": 421, "end_pos": 429, "type": "METRIC", "confidence": 0.9502948522567749}]}, {"text": "shows results obtained by OP-D for different k and weight.", "labels": [], "entities": []}, {"text": "In all the cases, we find that we get better results when the weight is properly thresholded rather than taking the absolute weight -we follow that henceforth.", "labels": [], "entities": []}, {"text": "k = 10 performs the best, so we take top 10% of the edges.", "labels": [], "entities": []}, {"text": "shows how the precision and recall values vary for different choices of \u03bb using 5-fold cross validation for all the experiments reported.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.999521017074585}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9987420439720154}]}, {"text": "Since the best results are obtained for \u03bb = 20, that has been fixed for all the experiments.", "labels": [], "entities": []}, {"text": "Baselines: We use the following baselines for comparison: a).", "labels": [], "entities": []}, {"text": "Random Baseline: Each sentence is randomly assigned to one of the two classes, opinion or fact . red We choose 3 and 5 sentences randomly to evaluate P@3 and P@5.", "labels": [], "entities": []}, {"text": "Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (NB): The next baseline is the first stage sentence-level classifier, as described earlier.", "labels": [], "entities": []}, {"text": "We also experimented with Logistic Regression, SMO (Sequential Minimal Optimization), Support Vector Machine (SVM), Repeated Incremental Pruning -Java version (JRip) as well as other classifiers, however NB gave the best results and was used as a baseline.", "labels": [], "entities": [{"text": "Logistic Regression", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8366948068141937}]}, {"text": "HITS (): We use the method proposed by) as another baseline.", "labels": [], "entities": [{"text": "HITS", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.45587337017059326}]}, {"text": "Note that the first two stages of our approach are similar to) except that we use a more extensive set of features as well as the network is unweighted.", "labels": [], "entities": []}, {"text": "We verified that these modifications indeed lead to performance gain, and used the modified approach as the baseline.", "labels": [], "entities": []}, {"text": "FactJudge (): FactJudge proposes a method to detect factuality of tweet.", "labels": [], "entities": []}, {"text": "We use the factuality score given by this approach as a baseline to detect opinions, i.e., higher the factuality score, lower is the probability of being an opinion.", "labels": [], "entities": []}, {"text": "Evaluation Metrics: We use precisions P @3, P @5 and recalls R@3 , R@5 as the evaluation measures, as we would like to obtain the best 3 or 5 opinions from the document.", "labels": [], "entities": [{"text": "precisions P @3", "start_pos": 27, "end_pos": 42, "type": "METRIC", "confidence": 0.9122491478919983}]}, {"text": "We perform a series of evaluations to compare the proposed approach with other baselines.", "labels": [], "entities": []}, {"text": "In the first evaluation, we verify if the top 3 or 5 sentences returned by the algorithm are actually opinions.", "labels": [], "entities": []}, {"text": "We also report the recall at top 3 or 5 places.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9986328482627869}]}, {"text": "We consider only those files (86 out of 100 for MPQA and 23 out of 25 for Yahoo) for testing which have at least one opinion.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.9437940716743469}]}, {"text": "shows the comparison results for the two datasets.", "labels": [], "entities": []}, {"text": "We see that the random baseline, along with () gives a precision close to 0.5.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9985585808753967}]}, {"text": "The first stage NB classifier performs better than these methods.", "labels": [], "entities": [{"text": "NB classifier", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.5596577525138855}]}, {"text": "The graphical framework (second stage) gives further improvements upon the NB classifier, and OP-D outperforms all these baselines consistently at least by 5%.", "labels": [], "entities": []}, {"text": "Performance on different buckets of opinion fraction: Since the fraction of opinions in each document varies, we wanted to investigate the performance at various sparsity levels and thus study the robustness of the proposed algorithm.", "labels": [], "entities": []}, {"text": "The test datasets were divided into various buckets according to the fraction of opinionated sentence (sparse, medium and dense) in the document.", "labels": [], "entities": []}, {"text": "The results shown in confirm that OP-D performs better consistently across various buckets.", "labels": [], "entities": []}, {"text": "Specifically, even for the documents with small fraction of opinions, it is able to improve performance from the NB and HITS baselines.", "labels": [], "entities": [{"text": "NB", "start_pos": 113, "end_pos": 115, "type": "DATASET", "confidence": 0.9695023894309998}, {"text": "HITS baselines", "start_pos": 120, "end_pos": 134, "type": "DATASET", "confidence": 0.8551112711429596}]}, {"text": "In general, for the documents with sparse opinions, the performance is poor (across methods) which is bringing down the overall performance.", "labels": [], "entities": []}, {"text": "This needs detailed future inspection.", "labels": [], "entities": []}, {"text": "Diversity Experiment: While these evaluations establish that the top 3-5 sentences extracted by OP-D contain more opinions than the baselines, they do not provide insights into whether these selected opinions are more important and diverse topics with respect to the entire article.", "labels": [], "entities": []}, {"text": "We, therefore, randomly select 50 MPQA articles and 25 Yahoo articles, provide all the sentences with gold standard opinion / fact labels to the annotators, and ask them to label 5 opinions, which they feel are important as well as diverse topics to cover the entire article.", "labels": [], "entities": []}, {"text": "Each article is provided to 3 annotators and we use a rank aggregation method to prepare a gold standard of 5 important and diverse opinions from these articles.", "labels": [], "entities": []}, {"text": "We now measure P @3 and P @5 (Total annotated important and diverse opinions per article is 5, so recall is a simple function of precision, therefore we omitted.) depending on what fraction of the top 3 or 5 sentences returned by various systems feature in the 5 important and diverse opinions, as selected by the annotators.", "labels": [], "entities": [{"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9985541701316833}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9992771744728088}]}, {"text": "We use the standard diversity algorithms, MMR) and Grasshopper (, both on the results of NB and HITS, as well as DivRank ( as baseline algorithms for diversity.", "labels": [], "entities": [{"text": "Grasshopper", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.9593258500099182}]}, {"text": "shows that OP-D outperforms other methods (sometimes even by 10%) in detecting diverse opinions.", "labels": [], "entities": [{"text": "detecting diverse opinions", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.8629718025525411}]}, {"text": "While both MMR and Grasshopper are able to achieve improvement over both NB and HITS classifiers, the order of improvement by OP-D over HITS is much higher, indicating that decreasing the hub scores of the authority of the selected hubs results eventually in more diverse opinions getting selected.", "labels": [], "entities": []}, {"text": "A goodness test of algorithms would be if the chosen sentences fall uniformly under various categories and sub-categories -this may instill diverse type of user engagement.", "labels": [], "entities": []}, {"text": "We took the top 5 sentences for 23 Articles from Yahoo Dataset detected by OP-D, DivRank, Grasshopper (on NB), Grasshopper (on HITS), MMR (on NB), MMR (on HITS) and then got each sentence (opinions) labeled by 2 anonymous human annotators for the category and subcategories it belongs to.", "labels": [], "entities": [{"text": "Yahoo Dataset detected", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.8896457155545553}, {"text": "Grasshopper", "start_pos": 111, "end_pos": 122, "type": "DATASET", "confidence": 0.9445642232894897}]}, {"text": "Any tie has been settled by another annotator.", "labels": [], "entities": []}, {"text": "show the distribution of opinions detected by these algorithms into various categories and subcategories respectively . Clearly, OP-D is able to select the opinionated sentences from various categories and subcategories much more uniformly than the other algorithms.", "labels": [], "entities": []}, {"text": "OP-D achieves the highest Shannon entropy among all the baselines reinforcing that claim.", "labels": [], "entities": []}, {"text": "The performance can be even better For the sake of space, shows only the best 3 algorithms.", "labels": [], "entities": []}, {"text": "appreciated if we can understand the overall distribution of opinions in an article.", "labels": [], "entities": []}, {"text": "However, manually classifying all opinionated sentences is not possible -hence we build an automated classification model, which has been described in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the MPQA and Yahoo datasets", "labels": [], "entities": [{"text": "MPQA and Yahoo datasets", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.7761228680610657}]}, {"text": " Table 2: Variation of precision and recall for different k (% of edges) and weight (w) for OP-D", "labels": [], "entities": [{"text": "Variation", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9560064077377319}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9977631568908691}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9995819926261902}, {"text": "OP-D", "start_pos": 92, "end_pos": 96, "type": "TASK", "confidence": 0.535811722278595}]}, {"text": " Table 3: Comparison results of the proposed OP-D framework with other baselines on MPQA and Yahoo  datasets", "labels": [], "entities": [{"text": "MPQA and Yahoo  datasets", "start_pos": 84, "end_pos": 108, "type": "DATASET", "confidence": 0.8117146492004395}]}, {"text": " Table 4: Comparison results of precision and recall on different buckets of opinion fractions", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9994041919708252}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.99883633852005}]}, {"text": " Table 5: Comparison results for the most diverse set of opinions", "labels": [], "entities": []}, {"text": " Table 6: Statistics of the Annotated MPQA and Yahoo dataset for automatic classification of opinion", "labels": [], "entities": [{"text": "Annotated MPQA", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.660385400056839}, {"text": "Yahoo dataset", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.8618438839912415}, {"text": "automatic classification of opinion", "start_pos": 65, "end_pos": 100, "type": "TASK", "confidence": 0.6706957966089249}]}, {"text": " Table 7: Comparison of 5-fold cross validation Accuracy (A), Precision (P), Recall (R), F1-Score (F)  results for automatic classification of opinions for MPQA and Yahoo datasets.", "labels": [], "entities": [{"text": "Accuracy (A)", "start_pos": 48, "end_pos": 60, "type": "METRIC", "confidence": 0.9228263795375824}, {"text": "Precision (P)", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9495287835597992}, {"text": "Recall (R)", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9609895348548889}, {"text": "F1-Score (F)", "start_pos": 89, "end_pos": 101, "type": "METRIC", "confidence": 0.9677405804395676}, {"text": "MPQA and Yahoo datasets", "start_pos": 156, "end_pos": 179, "type": "DATASET", "confidence": 0.7759493291378021}]}]}