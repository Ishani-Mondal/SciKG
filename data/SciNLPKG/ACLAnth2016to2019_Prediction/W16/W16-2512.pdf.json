{"title": [{"text": "Evaluating Embeddings using Syntax-based Classification Tasks as a Proxy for Parser Performance", "labels": [], "entities": [{"text": "Syntax-based Classification Tasks", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.7711847821871439}]}], "abstractContent": [{"text": "Most evaluations for vector space models are semantically motivated, e.g. by measuring how well they capture word similarity.", "labels": [], "entities": []}, {"text": "If one is interested in syntax-related downstream applications such as dependency parsing, a syntactically motivated evaluation seems preferable.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8395932912826538}]}, {"text": "As we show, the choice of embeddings has a noticeable impact on parser performance.", "labels": [], "entities": []}, {"text": "Since evaluating embeddings directly in a parser is costly, we analyze the correlation between the full parsing task and a simple linear classification task as a potential proxy.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many complex tasks in NLP are solved using embeddings as additional features.", "labels": [], "entities": []}, {"text": "In some pipelines, pre-trained embeddings are used as-is, in others they are learned as an integral part of training the pipeline (examples for this would bee. g. RNNLMs( or parsers that learn their own embeddings (e. g.)).", "labels": [], "entities": []}, {"text": "We focus on the first type.", "labels": [], "entities": []}, {"text": "If we want to run a system that can be enhanced using pre-trained embeddings, the question arises which embedding actually works best.", "labels": [], "entities": []}, {"text": "Since it is computationally infeasible to evaluate all embeddings on all pipelines, usually simple tasks are used to demonstrate the strengths of embeddings and an embedding for use in a pipeline is picked based on these proxy tasks.", "labels": [], "entities": []}, {"text": "Most of these tasks are semantically motivated and English dominates as language of choice for the tasks.", "labels": [], "entities": []}, {"text": "Last year, we proposed a more syntactically motivated evaluation task syneval, which uses morpho-syntactic information across a variety of languages (see Section 2).", "labels": [], "entities": []}, {"text": "Morphological information helps syntactic parsers, but usually there is no gold-standard information available during parsing (with the exception of parser evaluation).", "labels": [], "entities": []}, {"text": "Using embeddings that are good predictors of the missing morphological information should alleviate the problem of missing morphology.", "labels": [], "entities": []}, {"text": "It is reasonable to assume that the classification problems in syneval area good proxy for syntaxrelated tasks because they describe how well an embedding is able to capture morphological information which is helpful to the parser.", "labels": [], "entities": []}, {"text": "To test this assumption, we evaluate the performance of RBGParser ( ) using different embeddings as additional features.", "labels": [], "entities": []}, {"text": "The parser performance using a specific embedding should then reflect the embedding's performance on the classification tasks.", "labels": [], "entities": []}, {"text": "We know that embeddings yield only marginal improvements if the parser also has access to gold standard morphological information but benefits significantly if no morphological information is present ().", "labels": [], "entities": []}, {"text": "Therefore, we experiment with stripping the information that is used as classification target in syneval.", "labels": [], "entities": []}], "datasetContent": [{"text": "The basic idea is as follows: If an embedding encodes a morphological feature well, it should be a good drop-in replacement of that feature.", "labels": [], "entities": []}, {"text": "Therefore, if we strip a morphological feature from the data, using a well-performing embedding should yield higher parsing accuracies than using a worse performing one.", "labels": [], "entities": []}, {"text": "We use the following setups with each embedding (as well as without embeddings): \u2022 no case information (-case) \u2022 no tense information (-tense) \u2022 no number information (-number) \u2022 no PoS information (-PoS) \u2022 morphology (including PoS) completely stripped (-all) \u2022 all gold standard information as-is (+all) We train RBGParser on the gold standard for each language using the settings mentioned above, i. e. stripping the morphological information corresponding to the setting from both training and test data.", "labels": [], "entities": [{"text": "RBGParser", "start_pos": 315, "end_pos": 324, "type": "DATASET", "confidence": 0.9006332755088806}]}, {"text": "For each setting and language, we trained the parser in for modes: Without Embeddings, with dep, with GloVe, and with cbow.", "labels": [], "entities": []}, {"text": "The resulting accuracies are listed in.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9456239342689514}]}, {"text": "No embedding is able to provide a benefit to the parser with complete gold-standard annotations (the +all rows), which is consistent with previous findings.", "labels": [], "entities": []}, {"text": "Even when stripping morphological information, the embeddings only yield relevant improvements with respect to not using embeddings in -PoS and -all settings.", "labels": [], "entities": []}, {"text": "In both these cases, dep clearly outperforms the other embeddings, which is consistent with the syneval results).", "labels": [], "entities": []}, {"text": "In contrast, cbow, which performs better than GloVe in syneval, yields worse results than GloVe on average.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.7997922301292419}]}, {"text": "Both differences are significant (two-sided sign test; dep = glove: p < 0.05; GloVe = cbow: p < 0.01).", "labels": [], "entities": [{"text": "GloVe", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9691791534423828}]}, {"text": "The absolute difference between dep and GloVe is much larger than the absolute difference between GloVe and cbow.", "labels": [], "entities": []}, {"text": "The difference between GloVe and cbow is especially striking in the -PoS case where cbow outperforms GloVe by a large margin in syneval but is consistently beaten in the parsing task for every language, even those where cbow outperforms GloVe in the +all case.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 170, "end_pos": 182, "type": "TASK", "confidence": 0.8732505738735199}]}, {"text": "Stripping a single morphological feature (other than PoS) has little impact on parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9803803563117981}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9110194444656372}]}, {"text": "On the other hand, stripping all morphological information leads to much worse accuracies than just parsing without PoS.", "labels": [], "entities": []}, {"text": "This hints at some redundancy provided by the morphological annotations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Unlabeled parsing accuracies using different embeddings as well as no embeddings with varying  amounts of gold-standard morphological information available. Results better than the second best by a  margin of at least .1 are highlighted.", "labels": [], "entities": []}]}