{"title": [{"text": "Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction", "labels": [], "entities": [{"text": "Sentence-Level Grammatical Error Identification", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.738978311419487}]}], "abstractContent": [{"text": "We demonstrate that an attention-based encoder-decoder model can be used for sentence-level grammatical error identification for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016.", "labels": [], "entities": [{"text": "sentence-level grammatical error identification", "start_pos": 77, "end_pos": 124, "type": "TASK", "confidence": 0.635814979672432}, {"text": "Automated Evaluation of Scientific Writing (AESW) Shared Task", "start_pos": 133, "end_pos": 194, "type": "TASK", "confidence": 0.6731599152088166}]}, {"text": "The attention-based encoder-decoder models can be used for the generation of corrections, in addition to error identification, which is of interest for certain end-user applications.", "labels": [], "entities": [{"text": "generation of corrections", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.7300894856452942}, {"text": "error identification", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.7778283357620239}]}, {"text": "We show that a character-based encoder-decoder model is particularly effective, outperforming other results on the AESW Shared Task on its own, and showing gains over a word-based counterpart.", "labels": [], "entities": [{"text": "AESW Shared Task", "start_pos": 115, "end_pos": 131, "type": "DATASET", "confidence": 0.8585125009218851}]}, {"text": "Our final model-a combination of three character-based encoder-decoder models, one word-based encoder-decoder model, and a sentence-level CNN-is the highest performing system on the AESW 2016 binary prediction Shared Task.", "labels": [], "entities": [{"text": "AESW 2016 binary prediction Shared Task", "start_pos": 182, "end_pos": 221, "type": "DATASET", "confidence": 0.8193614184856415}]}], "introductionContent": [{"text": "The recent confluence of data availability and strong sequence-to-sequence learning algorithms has the potential to lead to practical tools for writing support.", "labels": [], "entities": []}, {"text": "Grammatical error identification is one such application of potential utility as a component of a writing support tool.", "labels": [], "entities": [{"text": "Grammatical error identification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.854840874671936}]}, {"text": "Much of the recent work in grammatical error identification and correction has made use of hand-tuned rules and features that augment data-driven approaches, or individual classifiers for human-designated subsets of errors.", "labels": [], "entities": [{"text": "grammatical error identification and correction", "start_pos": 27, "end_pos": 74, "type": "TASK", "confidence": 0.7807566523551941}]}, {"text": "Given a large, annotated dataset of scientific journal articles, we propose a fully data-driven approach for this problem, inspired by recent work in neural machine translation and more generally, sequence-tosequence learning).", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 150, "end_pos": 176, "type": "TASK", "confidence": 0.714129110177358}]}, {"text": "The Automated Evaluation of Scientific Writing (AESW) 2016 dataset is a collection of nearly 10,000 scientific journal articles (over 1 million sentences) published between 2006 and 2013 and annotated with corrections by professional, native English-speaking editors.", "labels": [], "entities": [{"text": "Automated Evaluation of Scientific Writing (AESW) 2016 dataset", "start_pos": 4, "end_pos": 66, "type": "TASK", "confidence": 0.6895749062299729}]}, {"text": "The goal of the associated AESW Shared Task is to identify whether or not a given unedited source sentence was corrected by the editor (that is, whether a given source sentence has one or more grammatical errors, broadly construed).", "labels": [], "entities": []}, {"text": "This system report describes our approach and submission to the AESW 2016 Shared Task, which establishes the current highest-performing public baseline for the binary prediction task.", "labels": [], "entities": [{"text": "AESW 2016 Shared Task", "start_pos": 64, "end_pos": 85, "type": "DATASET", "confidence": 0.8973840922117233}, {"text": "binary prediction task", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.7868314584096273}]}, {"text": "Our primary contribution is to demonstrate the utility of an attention-based encoder-decoder model for the binary prediction task.", "labels": [], "entities": [{"text": "binary prediction task", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.7888780832290649}]}, {"text": "We also provide evidence of tangible performance gains using a character-aware version of the model, building on the characteraware language modeling work of.", "labels": [], "entities": []}, {"text": "In addition to sentence-level classification, the models are capable of intra-sentence error identification and the generation of possible corrections.", "labels": [], "entities": [{"text": "sentence-level classification", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.7527349293231964}, {"text": "intra-sentence error identification", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.6338525513807932}]}, {"text": "We also obtain additional gains by using an ensemble of a generative encoder-decoder and a discriminative CNN classifier.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Here (and elsewhere)  RANDOM is the result of randomly assigning a sen- tence to one of the binary classes. For the CNN clas- sifiers, fine-tuning the word2vec embeddings im- proves performance. The encoder-decoder mod- els improve over the CNN classifiers, even though", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.8389037251472473}]}, {"text": " Table 1: Experimental results on the development set excluding the held-out 10k tuning subset.", "labels": [], "entities": []}, {"text": " Table 2: Results on the official development set. Here, RANDOM was provided by the Shared Task organizers.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9578943848609924}]}, {"text": " Table 3: Final submitted results on the Shared Task test set. COMBINATION was our final submitted system. RANDOM was", "labels": [], "entities": [{"text": "Shared Task test set", "start_pos": 41, "end_pos": 61, "type": "DATASET", "confidence": 0.7066344991326332}, {"text": "COMBINATION", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9282253980636597}, {"text": "RANDOM", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.6516175866127014}]}]}