{"title": [{"text": "Path-based vs. Distributional Information in Recognizing Lexical Semantic Relations", "labels": [], "entities": []}], "abstractContent": [{"text": "Recognizing various semantic relations between terms is beneficial for many NLP tasks.", "labels": [], "entities": []}, {"text": "While path-based and distributional information sources are considered complementary for this task, the superior results the latter showed recently suggested that the former's contribution might have become obsolete.", "labels": [], "entities": []}, {"text": "We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8470603227615356}]}, {"text": "The empirical results show that this method is effective in the multiclass setting as well.", "labels": [], "entities": []}, {"text": "We further show that the path-based information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated methods to recognize the lexical semantic relation the holds between terms are valuable for NLP applications.", "labels": [], "entities": []}, {"text": "Two main information sources are used to recognize such relations: path-based and distributional.", "labels": [], "entities": []}, {"text": "Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features.", "labels": [], "entities": []}, {"text": "Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (), which provide a distributional representation for each term.", "labels": [], "entities": []}, {"text": "These embedding-based methods were reported to perform well on several common datasets (), consistently outperforming other methods (.", "labels": [], "entities": []}, {"text": "While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution over the distributional ones.", "labels": [], "entities": []}, {"text": "Recently, however,  presented HypeNET, an integrated path-based and distributional method for hypernymy detection.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8561393618583679}]}, {"text": "They showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, notably improving results on anew dataset.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.8563672006130219}]}, {"text": "In this paper we present LexNET, an extension of HypeNET that recognizes multiple semantic relations.", "labels": [], "entities": []}, {"text": "We show that this integrated method is indeed effective also in the multiclass setting.", "labels": [], "entities": []}, {"text": "In the evaluations reported in this paper, LexNET performed better than each individual method on several common datasets.", "labels": [], "entities": [{"text": "LexNET", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.7094071507453918}]}, {"text": "Further, it was the best performing system in the semantic relation classification task of the CogALex 2016 shared task . We further assess the contribution of path-based information to semantic relation classification.", "labels": [], "entities": [{"text": "semantic relation classification task", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.8134855479001999}, {"text": "CogALex 2016 shared task", "start_pos": 95, "end_pos": 119, "type": "DATASET", "confidence": 0.812835305929184}, {"text": "semantic relation classification", "start_pos": 186, "end_pos": 218, "type": "TASK", "confidence": 0.806430439154307}]}, {"text": "Even though the distributional source is dominant across most datasets, path-based information always contributed to it.", "labels": [], "entities": []}, {"text": "In particular, path-based information seems to better capture the relationship between terms, rather than their individual properties, and can do so even for rare words or senses.", "labels": [], "entities": []}, {"text": "Our code and data are available at https://github.com/vered1986/LexNET.", "labels": [], "entities": [{"text": "LexNET", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.7665455937385559}]}, {"text": "Each dependency path is embedded using an LSTM, as illustrated in the top row of.", "labels": [], "entities": []}, {"text": "This results in a path vector space in which semantically-similar paths (e.g. X is defined as Y and X is described as Y) have similar vectors.", "labels": [], "entities": []}, {"text": "The vectors of all the paths that connect x and y are averaged to create v paths(x,y) . hypernym, co-hyponym, random 12,762 EVALution hypernym, meronym, attribute, synonym, antonym, holonym, substance meronym 7,378: The relation types and number of instances in each dataset, named by their WordNet equivalent where relevant.", "labels": [], "entities": [{"text": "EVALution", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.8350517153739929}]}, {"text": "showed that this new path representation outperforms prior path-based methods for hypernymy detection, and that the integrated model yields a substantial improvement over each individual model.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.8135580122470856}]}, {"text": "While HypeNET is designed for detecting hypernymy relations, it seems straightforward to extend it to classify term-pairs simultaneously to multiple semantic relations, as we describe next.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use four common semantic relation datasets that were created using semantic resources: K&H+N (Necsulescu et al., 2015) (an extension to), BLESS (Baroni and Lenci, 2011),, and ROOT09 (.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.992031455039978}, {"text": "ROOT09", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.7213266491889954}]}, {"text": "displays the relation types and number of instances in each dataset.", "labels": [], "entities": []}, {"text": "Most dataset relations are parallel to WordNet relations, such as hypernymy (cat, animal) and meronymy (hand, body), with an additional random relation for negative instances.", "labels": [], "entities": []}, {"text": "BLESS contains the event and attribute relations, connecting a concept with atypical activity/property (e.g. (alligator, swim) and (alligator, aquatic)).", "labels": [], "entities": [{"text": "BLESS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9620397090911865}]}, {"text": "EVALution contains a richer schema of semantic relations, with some redundancy: it contains both meronymy and holonymy (e.g. for bicycle and wheel), and the fine-grained substance-holonymy relation.", "labels": [], "entities": [{"text": "EVALution", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8733593821525574}]}, {"text": "We removed two relations with too few instances: Entails and MemberOf.", "labels": [], "entities": [{"text": "MemberOf", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.930505633354187}]}, {"text": "To prevent the lexical memorization effect (), Santus et al.", "labels": [], "entities": []}, {"text": "(2016) added negative switched hyponym-hypernym pairs (e.g. (apple, animal), (cat, fruit)) to ROOT09, which were reported to reduce this effect.", "labels": [], "entities": [{"text": "ROOT09", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.7196220755577087}]}], "tableCaptions": [{"text": " Table 2: Performance scores (precision, recall and F1) of each individual approach and the integrated models. To compute the  metrics we used scikit-learn (Pedregosa et al., 2011) with the \"averaged\" setup, which computes the metrics for each relation,  and reports their average, weighted by support (the number of true instances for each relation). Note that it can result in an F1  score that is not the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.998443067073822}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.993473470211029}, {"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9937431216239929}, {"text": "F1  score", "start_pos": 382, "end_pos": 391, "type": "METRIC", "confidence": 0.9767886400222778}, {"text": "precision", "start_pos": 425, "end_pos": 434, "type": "METRIC", "confidence": 0.9980810880661011}, {"text": "recall", "start_pos": 439, "end_pos": 445, "type": "METRIC", "confidence": 0.9740609526634216}]}]}