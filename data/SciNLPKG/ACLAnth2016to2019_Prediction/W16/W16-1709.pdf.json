{"title": [{"text": "Different Flavors of GUM: Evaluating Genre and Sentence Type Effects on Multilayer Corpus Annotation Quality", "labels": [], "entities": []}], "abstractContent": [{"text": "Genre and domain are well known co-variates of both manual and automatic annotation quality.", "labels": [], "entities": []}, {"text": "Comparatively less is known about the effect of sentence types, such as imperatives, questions or fragments , and how they interact with text type effects.", "labels": [], "entities": []}, {"text": "Using mixed effects models, we evaluate the relative influence of gen-re and sentence types on automatic and manual annotation quality for three related tasks in English data: POS tagging, dependency parsing and coreference resolution.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 176, "end_pos": 187, "type": "TASK", "confidence": 0.875984251499176}, {"text": "dependency parsing", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.8600132167339325}, {"text": "coreference resolution", "start_pos": 212, "end_pos": 234, "type": "TASK", "confidence": 0.9730387330055237}]}, {"text": "For the latter task, we also develop anew metric for the evaluation of individual regions of coreference annotation.", "labels": [], "entities": []}, {"text": "Our results show that while there are substantial differences between manual and automatic annotation in each task, sentence type is generally more important than genre in predicting errors within our data.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the availability of increasingly diverse language resources and the viability of processing almost unrestricted Web data, domain adaptation and coverage of novel domains have become a major concern in NLP and corpus creation (see e.g.).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7526654303073883}, {"text": "corpus creation", "start_pos": 214, "end_pos": 229, "type": "TASK", "confidence": 0.725302591919899}]}, {"text": "However, accuracy for both state of the art automatic tools and manual annotation of new tasks is typically reported on standard sources, typically newswire text, which often leads to overestimation of expected accuracy in both manual and automatic annotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9991143345832825}, {"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9722894430160522}]}, {"text": "Manning (2011) points out that although we expect 97% accuracy from POS taggers on newswire, such a rate indicates an error every other sentence even within the training domain, and more in other domains or epochs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9981721639633179}, {"text": "POS taggers", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.7396946549415588}]}, {"text": "A major cause of problems in adaptation is the presence of unknown words from outside the training domain, which maybe more influential than other aspects of the actual genre itself (cf..", "labels": [], "entities": [{"text": "adaptation", "start_pos": 29, "end_pos": 39, "type": "TASK", "confidence": 0.9660457372665405}]}, {"text": "It has also been suggested that at least part of the source for these problems lies in less frequent kinds of utterances within and across domains, i.e. that domain adaptation maybe folding in sentence type effects.", "labels": [], "entities": []}, {"text": "For example, in an evaluation of the English Web Treebank, explicitly intended to expand the text types covered by reference Treebank data, remark that \"[t]he most striking difference between the two types of data has to do with imperatives, which occur two orders of magnitude more often in the EWT.\"", "labels": [], "entities": [{"text": "English Web Treebank", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.9259392221768697}, {"text": "EWT", "start_pos": 296, "end_pos": 299, "type": "DATASET", "confidence": 0.9727746248245239}]}, {"text": "Specifically Silveira et al. found over 445 times more imperatives in EWT than in the Wall Street Journal corpus.", "labels": [], "entities": [{"text": "EWT", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8965726494789124}, {"text": "Wall Street Journal corpus", "start_pos": 86, "end_pos": 112, "type": "DATASET", "confidence": 0.9534708261489868}]}, {"text": "Despite this stark difference, there is remarkably little literature on sentence type as a factor in annotation quality or NLP tool performance.", "labels": [], "entities": []}, {"text": "While sentence type is known to be important in computational models of language acquisition (see), it has not been suggested that human annotators are affected by it.", "labels": [], "entities": [{"text": "sentence type", "start_pos": 6, "end_pos": 19, "type": "TASK", "confidence": 0.7240213453769684}, {"text": "language acquisition", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7353840172290802}]}, {"text": "In the development of automatic annotation tools, explicit partitioning of sentence types for differential treatment is also rare (for an exception see on machine translation).", "labels": [], "entities": [{"text": "differential treatment", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.7788047194480896}, {"text": "machine translation", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.7238055020570755}]}, {"text": "Indeed, it is not clear whether sentence type is actually pertinent to annotation quality, especially for human annotators, who are generally able to understand most sentences without difficulty.", "labels": [], "entities": []}, {"text": "The question we will be asking in this paper is therefore whether sentence types area better predictor of annotation quality than text type or genre, which is often postulated to be central without consideration of alternative explanations.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each of the three tasks, POS tagging, dependency annotation, and coreference resolution, we first split the corpus into each of the four text types and collate responses from manual, automatic and gold annotation in GUM.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.8721353113651276}, {"text": "coreference resolution", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.9704312384128571}, {"text": "GUM", "start_pos": 220, "end_pos": 223, "type": "DATASET", "confidence": 0.8458771109580994}]}, {"text": "Ordinarily, the manual annotation data released fora corpus is the same as the gold data -for this study we obtained uncorrected, single annotator versions of the data to approach annotation quality effects in an initial manually produced analysis.", "labels": [], "entities": []}, {"text": "Since GUM is a 'class-sourced' corpus, the unadjudicated annotations always represent work from relatively inexperienced student annotators, which was subsequently corrected by an experienced instructor.", "labels": [], "entities": []}, {"text": "These corrections will be considered the 'gold' data for our evaluation.", "labels": [], "entities": []}, {"text": "Once we have annotation graphs and labels from all three sources, we can easily compare manual and automatic annotation with the gold standard in each subcorpus.", "labels": [], "entities": []}, {"text": "However comparisons across sentence types can be less straightforward: while POS tags can be evaluated in the different sentence types in isolation, coreference annotation cannot be easily evaluated while ignoring certain parts of the text.", "labels": [], "entities": []}, {"text": "We therefore develop some extended metrics for the evaluation in Section 6.", "labels": [], "entities": []}, {"text": "For all data sets, we keep track of the documents (and by proxy annotators) that contain each annotation as a random effect, and we will consider some competing independent variables, such as sentence length, as alternative explanations for annotation quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Composition of the GUM corpus.", "labels": [], "entities": [{"text": "GUM corpus", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.8386710584163666}]}, {"text": " Table 3: Tagging performance by genre, with significance  in a simple linear model (*p<0.05;**p<0.001;***p<0.0001)", "labels": [], "entities": [{"text": "Tagging performance", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8780328929424286}]}, {"text": " Table 4: Tagging accuracy by sentence type for manual and automatic annotation. Significance only indicated for  deviations of more than 2% below the mean (with *) or above (with +).", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9760757684707642}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9758825302124023}, {"text": "Significance", "start_pos": 81, "end_pos": 93, "type": "METRIC", "confidence": 0.9586211442947388}]}, {"text": " Table 5: t values for mixed effects models with document,  genre, sentence and length effects (significant values bold).", "labels": [], "entities": []}, {"text": " Table 6: Parser and corrector accuracies.", "labels": [], "entities": [{"text": "corrector accuracies", "start_pos": 21, "end_pos": 41, "type": "METRIC", "confidence": 0.8287539780139923}]}, {"text": " Table 7: t values from mixed effects models for parsing  accuracy using sentence type, genre and length, with docu- ment random effects.", "labels": [], "entities": [{"text": "parsing", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.9543461203575134}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.8944846987724304}]}, {"text": " Table 8: Partitioned precision and recall p-link scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9955211877822876}, {"text": "recall p-link scores", "start_pos": 36, "end_pos": 56, "type": "METRIC", "confidence": 0.912575364112854}]}, {"text": " Table 9: t-values for mixed effects models of precision and  recall for manual and automatic annotation.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9995312690734863}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.99880051612854}]}]}