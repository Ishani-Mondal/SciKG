{"title": [{"text": "Using Centroids of Word Embeddings and Word Mover's Distance for Biomedical Document Retrieval in Question Answering", "labels": [], "entities": [{"text": "Biomedical Document Retrieval", "start_pos": 65, "end_pos": 94, "type": "TASK", "confidence": 0.7982591291268667}, {"text": "Question Answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.704712837934494}]}], "abstractContent": [{"text": "We propose a document retrieval method for question answering that represents documents and questions as weighted cen-troids of word embeddings and reranks the retrieved documents with a relaxation of Word Mover's Distance.", "labels": [], "entities": [{"text": "question answering", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7608334720134735}]}, {"text": "Using biomedical questions and documents from BIOASQ, we show that our method is competitive with PUBMED.", "labels": [], "entities": [{"text": "BIOASQ", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.9104734659194946}, {"text": "PUBMED", "start_pos": 98, "end_pos": 104, "type": "DATASET", "confidence": 0.8123564720153809}]}, {"text": "With a top-k approximation , our method is fast, and easily portable to other domains and languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Biomedical experts (e.g., researchers, clinical doctors) routinely need to search the biomedical literature to support research hypotheses, treat rare syndromes, follow best practices etc.", "labels": [], "entities": []}, {"text": "The most widely used biomedical search engine is PUBMED, with more than 24 million biomedical references and abstracts, mostly of journal articles.", "labels": [], "entities": [{"text": "PUBMED", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.821257472038269}]}, {"text": "To improve their performance, biomedical search engines often use large, manually curated ontologies, e.g., to identify biomedical terms and expand queries with related terms.", "labels": [], "entities": []}, {"text": "Biomedical experts, however, report that search engines often miss relevant documents and return many irrelevant ones.", "labels": [], "entities": []}, {"text": "There is also growing interest for biomedical question answering (QA) systems), which allow their users to specify their information needs more precisely, as natural language questions rather than Boolean queries, and aim to produce more concise answers.", "labels": [], "entities": [{"text": "biomedical question answering (QA)", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.8219531178474426}]}, {"text": "Document retrieval is particularly important in biomedical QA, since most of the information sought resides in documents and is essential in later stages.", "labels": [], "entities": [{"text": "Document retrieval", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9136451184749603}, {"text": "biomedical QA", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.7544195652008057}]}, {"text": "We propose anew document retrieval method.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7296426892280579}]}, {"text": "Instead of representing documents and questions as bags of words, we represent them as the centroids of their word embeddings () and retrieve the documents whose centroids are closer to the centroid of the question.", "labels": [], "entities": []}, {"text": "This allows retrieving relevant documents that may have no common terms with the question without query expansion.", "labels": [], "entities": []}, {"text": "Using biomedical questions from the BIOASQ competition (, we show that our method combined with a relaxation of the recently proposed Word Mover's Distance (WMD) () is competitive with PUBMED.", "labels": [], "entities": [{"text": "BIOASQ competition", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.8485155403614044}, {"text": "Word Mover's Distance (WMD)", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.4807370773383549}]}, {"text": "We also show that with a top-k approximation, our method is particularly fast, with no significant decrease in effectiveness.", "labels": [], "entities": []}, {"text": "Given that it does not require ontologies, term extractors, or manually labeled training data, our method could be easily ported to other domains (e.g., legal texts) and languages.", "labels": [], "entities": [{"text": "term extractors", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.6894121170043945}]}], "datasetContent": [{"text": "Figures 2-4 show Mean Interpolated Precision (MIP) at 11 recall levels, Mean Average Interpolated Precision (MAIP), Mean Average Precision (MAP), and Normalized Discounted Cumulative Gain (nDCG).", "labels": [], "entities": [{"text": "Mean Interpolated Precision (MIP)", "start_pos": 17, "end_pos": 50, "type": "METRIC", "confidence": 0.8813167810440063}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9933159947395325}, {"text": "Mean Average Interpolated Precision (MAIP)", "start_pos": 72, "end_pos": 114, "type": "METRIC", "confidence": 0.9497620037623814}, {"text": "Mean Average Precision (MAP)", "start_pos": 116, "end_pos": 144, "type": "METRIC", "confidence": 0.9613592823346456}]}, {"text": "Roughly speaking, MAIP is the area under the MIP curve, MAP is the same area without interpolation, and nDCG is an alternative We use relevance ranking (not recency) in PubMedSE.", "labels": [], "entities": [{"text": "PubMedSE", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.9364334344863892}]}, {"text": "The dump is available from https://www.nlm.", "labels": [], "entities": []}, {"text": "nih.gov/databases/license/license.html.", "labels": [], "entities": []}, {"text": "The 14 million articles do not include approx. 10 million articles for which only titles are provided.", "labels": [], "entities": []}, {"text": "There are hardly any title-only gold relevant documents, and PubMedSE very rarely returns title-only documents.", "labels": [], "entities": [{"text": "PubMedSE", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.9334484338760376}]}, {"text": "It is unclear to us if PUBMED also searches the full texts of the articles, which may put our methods at a disadvantage.", "labels": [], "entities": [{"text": "PUBMED", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.6921295523643494}]}, {"text": "All measures are widely used ().", "labels": [], "entities": []}, {"text": "We use binary relevance in nDCG, as in the BIOASQ dataset.: MAIP and MAP scores, fork (documents to retrieve) set to 1,000.", "labels": [], "entities": [{"text": "BIOASQ dataset.", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.9563425481319427}, {"text": "MAIP", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.7275131344795227}, {"text": "MAP scores", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.8841618299484253}]}, {"text": "Unless otherwise stated, the number of retrieved documents is set to k = 1,000.", "labels": [], "entities": []}, {"text": "shows that Cent performs much worse than CentIDF.", "labels": [], "entities": []}, {"text": "At low recall, CentIDF is as good as PubMedSE, but PubMedSE outperforms CentIDF at high recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9987872242927551}, {"text": "PubMedSE", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.919986367225647}, {"text": "PubMedSE", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.9193927049636841}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9955223798751831}]}, {"text": "Reranking the top-k documents of CentIDF by RWMD-Q has a significant impact, leading to a system (CentIDF-RWMD-Q) that performs better or as good as PubMedSE up to 0.7 recall.", "labels": [], "entities": [{"text": "PubMedSE", "start_pos": 149, "end_pos": 157, "type": "DATASET", "confidence": 0.9324991703033447}, {"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.9967411160469055}]}, {"text": "Reranking the top-k documents of PubMedSE by RWMD-Q (PubMedSE-RWMD-Q) also improves the performance of PubMedSE.", "labels": [], "entities": [{"text": "PubMedSE", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9342082142829895}, {"text": "PubMedSE", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.9216666221618652}]}, {"text": "Reranking the top-k documents of CentIDF by RWMD-D (or RWMD-MAX, not shown) leads to much worse results (CentIDF-RWMD-D), for reasons already explained.", "labels": [], "entities": [{"text": "RWMD-MAX", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.7469947934150696}]}, {"text": "11 Similar conclusions are reached by examining the MAIP, MAP, and nDCG scores.", "labels": [], "entities": [{"text": "MAIP", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.5096940994262695}, {"text": "MAP", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.5791136026382446}]}, {"text": "Keyword-based information retrieval may miss relevant documents that use different terms than the question, even with query expansion.", "labels": [], "entities": [{"text": "Keyword-based information retrieval", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6634269257386526}]}, {"text": "PubMedSE retrieves no documents for 35% (460/1307) of our questions.", "labels": [], "entities": [{"text": "PubMedSE", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9407126903533936}]}, {"text": "12 Further experiments (not reported), however, indicate that PubMedSE has higher precision than CentIDF-RWMD-Q, when PubMedSE returns documents, at the expense of lower recall.", "labels": [], "entities": [{"text": "PubMedSE", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.876987874507904}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9992477893829346}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9987711310386658}]}, {"text": "Hence, there is scope to combine PubMedSE with our methods.", "labels": [], "entities": [{"text": "PubMedSE", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.915219783782959}]}, {"text": "As a first, crude step, we tested a method (Hybrid) that returns the documents of CentIDF-RWMD-Q when PubMedSE retrieves no documents, and those of nDCG@20 nDCG@100: Average times (in seconds) overall the questions of the dataset (k = 1000).", "labels": [], "entities": []}, {"text": "Hybrid had the best results in our experiments; the only exception was its nDCG@100 score, which was slightly lower than the score of CentIDF-RWMD-Q. shows that an approximate top-k algorithm (ANN) in CentIDF-RWMD-Q (ANN-CentIDF-RWMD-Q) reduces dramatically the time to obtain the top-k documents, with a very small decrease in MAIP, MAP, and nDCG scores.", "labels": [], "entities": []}, {"text": "We also compared against the other participants of the second year of BIOASQ; the participant results of later years are not yet available.", "labels": [], "entities": [{"text": "BIOASQ", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.6182194352149963}]}, {"text": "The official BIOASQ score is MAP; MIP, MAIP, and nDCG scores are not provided.", "labels": [], "entities": [{"text": "BIOASQ score", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.7545724809169769}, {"text": "MAP", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9211288690567017}, {"text": "MIP", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.6811491847038269}]}, {"text": "Our best method was again Hybrid (avg. MAP over the five batches of the second year 16.18%).", "labels": [], "entities": [{"text": "avg. MAP", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.7344298660755157}]}, {"text": "It performed overall better than the BIOASQ 'baselines' (best avg.", "labels": [], "entities": [{"text": "BIOASQ 'baselines'", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.6908718744913737}]}, {"text": "MAP 15.60%) and all eight participants, except for the best one (avg. MAP 28.20%).", "labels": [], "entities": [{"text": "MAP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7467527985572815}]}, {"text": "The best system () used dependency IR models, combined with UMLS and query expansion heuristics (e.g., adding We use Annoy (https://github.com/spotify/ annoy), 100 trees, 1,000 neighbors, search-k = 10 \u00b7 |trees| \u00b7 |neighbors|.", "labels": [], "entities": []}, {"text": "Times on a server with 4 Intel Xeon E5620 CPUs (16 cores total), at 2.4 GHz, with 128 GB RAM.", "labels": [], "entities": []}, {"text": "We used the evaluation platform of BIOASQ (http:// participants-area.bioasq.org/oracle).", "labels": [], "entities": [{"text": "BIOASQ", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.8225088119506836}]}, {"text": "the titles of the top-k initially retrieved documents to the query).", "labels": [], "entities": []}, {"text": "The 'baselines' are actually very competitive; no system beat them in the first year, and only one was better in the second year.", "labels": [], "entities": []}, {"text": "They are PubMedSE, but using BIOASQ-specific heuristics (e.g., instructing PubMedSE to ignore types of articles the experts did not consider).", "labels": [], "entities": []}, {"text": "Our system is simpler and does not use heuristics; hence, it can be ported more easily to other domains.", "labels": [], "entities": []}, {"text": "reports that a k-NN classifier that represents articles as IDF-weighted centroids (Eq.", "labels": [], "entities": []}, {"text": "1) of 200-dimensional word embeddings (200 features) is as good at assigning semantic labels (MeSH headings) to biomedical articles as when using millions of bag-of-word features, reducing significantly the training and classification times.", "labels": [], "entities": [{"text": "assigning semantic labels (MeSH headings) to biomedical articles", "start_pos": 67, "end_pos": 131, "type": "TASK", "confidence": 0.8140333771705628}]}, {"text": "To our knowledge, our work is the first attempt to use IDF-weighted centroids of word embeddings in information retrieval, and the first to use WMD to rerank the retrieved documents.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.6841395944356918}]}, {"text": "More elaborate methods to encode texts as vectors have been proposed ( and they could be used as alternatives to centroids of word embeddings, though the latter are simpler and faster to compute.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average times (in seconds) over all the  questions of the dataset (k = 1000).", "labels": [], "entities": [{"text": "Average times (in seconds)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8940417865912119}]}]}