{"title": [{"text": "Improving the Morphological Analysis of Classical Sanskrit", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9856113195419312}, {"text": "Morphological Analysis of Classical Sanskrit", "start_pos": 14, "end_pos": 58, "type": "TASK", "confidence": 0.9048393487930297}]}], "abstractContent": [{"text": "The paper describes anew tagset for the morphological disambiguation of Sanskrit, and compares the accuracy of two machine learning methods (CRF, deep recurrent neural networks) for this task, with a special focus on how to model the lexicographic information.", "labels": [], "entities": [{"text": "morphological disambiguation of Sanskrit", "start_pos": 40, "end_pos": 80, "type": "TASK", "confidence": 0.8485067039728165}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9973164200782776}]}, {"text": "It reports a significant improvement over previously published results.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "This section reports how results are influenced by feature and model selection, and examines which linguistic phenomena are mainly responsible for errors made by the morphological disambiguation.", "labels": [], "entities": []}, {"text": "If not mentioned otherwise, evaluation only considers the 42% of morphologically ambiguous forms.", "labels": [], "entities": []}, {"text": "The \"final\" accuracy rate that also considers forms with only one possible solution is clearly higher (refer to the last row of).", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.928168773651123}]}, {"text": "contrasts the results of CRF and LSTM for different lexicalizations in \"fast mode\".", "labels": [], "entities": []}, {"text": "Remarkably, LSTM outperforms the CRF in all evaluation measures.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.6964516639709473}]}, {"text": "A test with a higher-order CRF (not reported) shows that the accuracy of the CRF cannot be improved relevantly when wider ranges of output label transitions are considered, and increasing the range of input features also does not improve over the reported results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9993910789489746}]}, {"text": "9 So, the deep NN seems to be more appropriate for this task than a CRF.", "labels": [], "entities": []}, {"text": "Comparing the previous large and the new smaller tagset yields consistent results for CRF and LSTM.", "labels": [], "entities": []}, {"text": "While the previous tagset performs better for some low-frequency classes (higher F score), the new tagset produces a higher overall accuracy, and requires less time for training due to its lower dimensionality.", "labels": [], "entities": [{"text": "F score", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9933979213237762}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9991783499717712}]}, {"text": "In addition, demonstrates the high influence of the lexical representation.", "labels": [], "entities": []}, {"text": "While the unlexicalized variant (none) suffers especially from low recall, the values of morfessor are clearly closer to the lexicalized than to the unlexicalized version, indicating that this approach may turnout to be useful for (ancient) Indian languages for which no extensive lexical resources, but large unannotated corpora are available.", "labels": [], "entities": [{"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.998508632183075}]}, {"text": "Finally, the variants using word embeddings (w2v-sparse and -dense) produce lower accuracy rates than the one-hot-encodings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.998704195022583}]}, {"text": "Adding the broad word-semantic classes further improves the accuracy of the 1h encoding, although the difference to 1h is not significant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9995637536048889}]}, {"text": "The weight of anode is defined as the number of occurrences of the concept linked to the node, plus the sum of this number for all its subnodes.", "labels": [], "entities": []}, {"text": "7 Example: Although the node \"mountain\" has a very high weight, it is further collapsed into a parent node \"elements of the landscape\", which covers related concepts such as \"lake\" or \"river\".", "labels": [], "entities": []}, {"text": "8 Example: The subclasses of the concept \"person\" were widely scattered over the original tree and could, therefore, not be subsumed automatically under one common ancestor.", "labels": [], "entities": []}, {"text": "A first-order CRF with a feature window of 7 instead of 5 words produces P = 82.84, R = 64.13, and F = 68.71.", "labels": [], "entities": [{"text": "R", "start_pos": 84, "end_pos": 85, "type": "METRIC", "confidence": 0.9794822335243225}, {"text": "F", "start_pos": 99, "end_pos": 100, "type": "METRIC", "confidence": 0.9940168857574463}]}, {"text": "Results for the tagset used in Hellwig: Macro-average P(recision), R(ecall) and F(-score), and A(ccuracy) for all words with more than one morphological reading, \"fast mode\".", "labels": [], "entities": [{"text": "F(-score)", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.8914451599121094}, {"text": "A", "start_pos": 95, "end_pos": 96, "type": "METRIC", "confidence": 0.98583984375}]}, {"text": "Note that macro-average measures tend to overemphasize (bad) results of small classes.", "labels": [], "entities": []}, {"text": "To make the results comparable with those reported for other languages, the second row of reports the performance of the two models when trained with the 1h feature on the full data set.", "labels": [], "entities": []}, {"text": "Remarkably, the CRF benefits more clearly from the increased training set, although its results are clearly worse than those of the LSTM., which splits the results of the best LSTM model from according to coarse POS classes, confirms that the correct decisions were made when reducing the size of the tagset.", "labels": [], "entities": []}, {"text": "The class of finite verbal forms, whose tense distinction strongly increased the size of the tagset, produce low error rates, while infinite declinable verbal forms are in a similar error range as adjectives and nouns.", "labels": [], "entities": [{"text": "error", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9823655486106873}]}], "tableCaptions": [{"text": " Table 1: Macro-average P(recision), R(ecall) and F(-score), and A(ccuracy) for all words with more than  one morphological reading, \"fast mode\". Note that macro-average measures tend to overemphasize (bad)  results of small classes.", "labels": [], "entities": [{"text": "F(-score)", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.8510767370462418}, {"text": "A", "start_pos": 65, "end_pos": 66, "type": "METRIC", "confidence": 0.9739355444908142}]}, {"text": " Table 2: Macro-average PRF and accuracy on the full training set, features: 1h. First row: Results for  ambiguous words; second row: Results for all words.", "labels": [], "entities": [{"text": "PRF", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8558573722839355}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9996628761291504}]}, {"text": " Table 3: Accuracy per coarse POS class for the best model from Table 1 for ambiguous and unambiguous  words. Numbers are subsumed under the class A. V.fin.: finite verbal forms; V.inf.: infinite verbal forms", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986087679862976}]}, {"text": " Table 4: P, R, and F of the full LSTM model for the most frequent nominal categories. Bold numbers  are higher than the best results reported in Hellwig (2015). Note that the F score may be better than in  Hellwig (2015), even if neither P nor R are better, because Hellwig (2015) considers these values for two  models.", "labels": [], "entities": [{"text": "F score", "start_pos": 176, "end_pos": 183, "type": "METRIC", "confidence": 0.9846662878990173}]}]}