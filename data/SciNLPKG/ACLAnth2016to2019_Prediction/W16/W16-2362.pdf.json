{"title": [], "abstractContent": [{"text": "Bidirectional Recurrent Neural Networks (BiRNNs) have shown outstanding results on sequence-to-sequence learning tasks.", "labels": [], "entities": []}, {"text": "This architecture becomes specially interesting for multimodal machine translation task, since BiRNNs can deal with images and text.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.6225627561410269}]}, {"text": "On most translation systems the same word embedding is fed to both BiRNN units.", "labels": [], "entities": []}, {"text": "In this paper, we present several experiments to enhance a base-line sequence-to-sequence system (Elliott et al., 2015), for example, by using double embeddings.", "labels": [], "entities": []}, {"text": "These embeddings are trained on the forward and backward direction of the input sequence.", "labels": [], "entities": []}, {"text": "Our system is trained, validated and tested on the Multi30K dataset (Elliott et al., 2016) in the context of the WMT 2016 Multimodal Translation Task.", "labels": [], "entities": [{"text": "Multi30K dataset (Elliott et al., 2016)", "start_pos": 51, "end_pos": 90, "type": "DATASET", "confidence": 0.8967171178923713}, {"text": "WMT 2016 Multimodal Translation Task", "start_pos": 113, "end_pos": 149, "type": "TASK", "confidence": 0.6886697351932526}]}, {"text": "The obtained results show that the double-embedding approach performs significantly better than the traditional single-embedding one.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence-to-sequence learning is anew common approach to translation problems ().", "labels": [], "entities": [{"text": "Sequence-to-sequence learning", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9281047582626343}, {"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9748149514198303}]}, {"text": "The basic idea consists in mapping the input sentence into a vector of fixed dimensionality with a Recurrent Neural Network (RNN) and, then, do the reverse step to map the vector to the target sequence.", "labels": [], "entities": []}, {"text": "From this new perspective, multimodal translation) has become a feasible task.", "labels": [], "entities": [{"text": "multimodal translation", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.760780930519104}]}, {"text": "In particular, we are referring to the WMT 2016 multimodal task that consists in translating English sentences into German, given the English sentence itself and the image that it describes.", "labels": [], "entities": [{"text": "WMT 2016 multimodal task", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.5979689657688141}]}, {"text": "This paper describes our participation in this task using a translation scheme based on Bidirectional RNNs (BiRNNs) which allows to combine both information from image and text.", "labels": [], "entities": []}, {"text": "In this paper, we take as baseline system the one from) and focus on experimenting with the word embedding system and encoding techniques.", "labels": [], "entities": []}, {"text": "The rest of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes related work on image captioning and machine translation.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7901929020881653}, {"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8188880980014801}]}, {"text": "Section 3 gives details about the architecture of the multimodal translation system.", "labels": [], "entities": [{"text": "multimodal translation", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.6785427033901215}]}, {"text": "Section 4 reports details on the experimental framework including the parameters of our model and the results obtained.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes and comments on further work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: BLEU and METEOR Results. Of- ficial baseline 1 GroundedTranslation C kindly  provided by the organisers.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986072182655334}, {"text": "METEOR", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9765908718109131}, {"text": "GroundedTranslation C kindly", "start_pos": 57, "end_pos": 85, "type": "METRIC", "confidence": 0.794353187084198}]}]}