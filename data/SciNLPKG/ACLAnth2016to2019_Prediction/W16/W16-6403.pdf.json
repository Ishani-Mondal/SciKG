{"title": [{"text": "A Hybrid Approach for Deep Machine Translation", "labels": [], "entities": [{"text": "Deep Machine Translation", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.6554861267407736}]}], "abstractContent": [{"text": "This paper presents a Hybrid Approach to Deep Machine Translation in the language direction from English to Bulgarian.", "labels": [], "entities": [{"text": "Deep Machine Translation", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.6138838231563568}]}, {"text": "The setup uses pre-and post-processing modules as well as two-level transfer.", "labels": [], "entities": []}, {"text": "The language resources that have been incorporated are: WordNets for both languages; a valency lexicon for Bulgarian; aligned parallel corpora.", "labels": [], "entities": []}, {"text": "The architecture comprises a predominantly statistical component (factor-based SMT in Moses) with some focused rule-based elements.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.8350170254707336}]}, {"text": "The experiments show promising results and room for further improvements within the MT architecture.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9718167185783386}]}], "introductionContent": [{"text": "The paper presents a hybrid approach for Deep Semantic Machine Translation.", "labels": [], "entities": [{"text": "Deep Semantic Machine Translation", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.6716380640864372}]}, {"text": "For that purpose, however, the linguistic phenomena that constitute deep semantics have to be defined.", "labels": [], "entities": []}, {"text": "A list of such phenomena have been considered in and, among others.", "labels": [], "entities": []}, {"text": "All the mentioned phenomena represent various levels of granularity and different linguistic dimensions.", "labels": [], "entities": []}, {"text": "In our deep Machine Translation (MT) system we decided to exploit the following components in the transfer phase: Lexical Semantics (WSD), MultiWord Expression (MWE), Named Entities (NE) and Logical Form (LF).", "labels": [], "entities": [{"text": "deep Machine Translation (MT)", "start_pos": 7, "end_pos": 36, "type": "TASK", "confidence": 0.8081411520640055}]}, {"text": "For the incorporation of Lexical Semantics through the exploitation of WordNet and Valency dictionary the knowledge-based approach to WSD has been accepted.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9662325978279114}, {"text": "WSD", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.879278302192688}]}, {"text": "Concerning the LF, we rely on Minimal Recursion Semantics (MRS) in its two variants -the full one (MRS) and the more underspecified one (Robust MRS (RMRS)).", "labels": [], "entities": [{"text": "Minimal Recursion Semantics (MRS)", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.6546842058499655}, {"text": "MRS", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.8125953674316406}]}, {"text": "The MWE and NE are parts of the lexicons.", "labels": [], "entities": [{"text": "MWE", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.909799337387085}]}, {"text": "We should note that there are also other appropriate LF frameworks that are briefly mentioned below.", "labels": [], "entities": []}, {"text": "One of the MRS-related semantic formalisms is the Abstract Meaning Representation (AMR 1 ), which aims at achieving whole-sentence deep semantics instead of addressing various isolated holders of semantic information (such as, NER, coreferences, temporal anchors, etc.).", "labels": [], "entities": [{"text": "MRS-related semantic formalisms", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.9327343503634135}, {"text": "Abstract Meaning Representation (AMR", "start_pos": 50, "end_pos": 86, "type": "TASK", "confidence": 0.6941603600978852}, {"text": "NER, coreferences, temporal anchors", "start_pos": 227, "end_pos": 262, "type": "TASK", "confidence": 0.600212350487709}]}, {"text": "AMR also builds on the available syntactic trees, thus contributing to the efforts on sembanking.", "labels": [], "entities": [{"text": "AMR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8956805467605591}, {"text": "sembanking", "start_pos": 86, "end_pos": 96, "type": "TASK", "confidence": 0.9665176868438721}]}, {"text": "It is English-dependent and it makes an extensive use of PropBank framesets) and ().", "labels": [], "entities": [{"text": "PropBank framesets", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.9702739417552948}]}, {"text": "Its concepts are either English words or special keywords.", "labels": [], "entities": []}, {"text": "AMR uses approximately 100 relations.", "labels": [], "entities": [{"text": "AMR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6155390739440918}]}, {"text": "They include: frame arguments, general semantic relations, relations for quantities and date-entities, etc.", "labels": [], "entities": []}, {"text": "The Groningen Meaning Bank (GMB) integrates various phenomena in one formalism.", "labels": [], "entities": [{"text": "Groningen Meaning Bank (GMB)", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.7494475841522217}]}, {"text": "It has a linguistically motivated, theoretically solid (CCG 2 /DRT 3 ) background.", "labels": [], "entities": []}, {"text": "In this paper the NLP strategies are presented for Hybrid Deep Machine Translation in the direction from English-to-Bulgarian.", "labels": [], "entities": [{"text": "Hybrid Deep Machine Translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.6500318050384521}]}, {"text": "Under Hybrid MT we understand the usage of the automatic Moses system together with a rule-based component at the transfer phase.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.6181036233901978}]}, {"text": "The paper is structured as follows:in section 2 the components of the hybrid MT architecture is presented.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9599586129188538}]}, {"text": "Section 3 discusses the deep semantic processing.", "labels": [], "entities": [{"text": "deep semantic processing", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.619743545850118}]}, {"text": "Section 4 reports on the current experiments and results.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}