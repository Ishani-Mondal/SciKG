{"title": [{"text": "Semi-supervised Named Entity Recognition in noisy-text", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.7611344854036967}]}], "abstractContent": [{"text": "Many of the existing Named Entity Recognition (NER) solutions are built based on news corpus data with proper syntax.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.807598759730657}]}, {"text": "These solutions might not lead to highly accurate results when being applied to noisy, user generated data, e.g., tweets, which can feature sloppy spelling, concept drift, and limited contextualization of terms and concepts due to length constraints.", "labels": [], "entities": []}, {"text": "The models described in this paper are based on linear chain conditional random fields (CRFs), use the BIEOU encoding scheme, and leverage random feature dropout for up-sampling the training data.", "labels": [], "entities": [{"text": "BIEOU", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9849029779434204}]}, {"text": "The considered features include word clusters and pre-trained distributed word representations, updated gazetteer features, and global context predictions.", "labels": [], "entities": [{"text": "global context predictions", "start_pos": 128, "end_pos": 154, "type": "TASK", "confidence": 0.5727437237898508}]}, {"text": "The latter feature allows for ingesting the meaning of new or rare tokens into the system via unsupervised learning and for alleviating the need to learn lexicon based features, which usually tend to be high dimensional.", "labels": [], "entities": []}, {"text": "In this paper, we report on the solution [ST] we submitted to the WNUT 2016 NER shared task.", "labels": [], "entities": [{"text": "WNUT 2016 NER shared task", "start_pos": 66, "end_pos": 91, "type": "DATASET", "confidence": 0.8516634941101074}]}, {"text": "We also present an improvement over our original submission [SI], which we built by using semi-supervised learning on labelled training data and pre-trained resourced constructed from unlabelled tweet data.", "labels": [], "entities": []}, {"text": "Our ST solution achieved an F1 score of 1.2% higher than the baseline (35.1% F1) for the task of extracting 10 entity types.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9880543947219849}, {"text": "F1", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9949817061424255}]}, {"text": "The SI resulted in an increase of 8.2% in F1 score over the baseline (7.08% over ST).", "labels": [], "entities": [{"text": "SI", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.6086191534996033}, {"text": "F1 score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9603487849235535}]}, {"text": "Finally, the SI model's evaluation on the test data achieved a F1 score of 47.3% (~1.15% increase over the 2 nd best submitted solution).", "labels": [], "entities": [{"text": "SI", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.8496586084365845}, {"text": "F1 score", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9880577921867371}]}, {"text": "Our experimental setup and results are available as a standalone twitter NER tool at https://github.com/napsternxg/TwitterNER.", "labels": [], "entities": []}], "introductionContent": [{"text": "A common task in information extraction is the identification of named entities from free text, also referred to as Named Entity Recognition (NER).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.8087776899337769}, {"text": "identification of named entities from free text", "start_pos": 47, "end_pos": 94, "type": "TASK", "confidence": 0.812975321497236}, {"text": "Named Entity Recognition (NER)", "start_pos": 116, "end_pos": 146, "type": "TASK", "confidence": 0.8014819622039795}]}, {"text": "In the machine learning and data mining literature, NER is typically formulated as a sequence prediction problem, where fora given sequence of tokens, an algorithm or model need to predict the correct sequence of labels.", "labels": [], "entities": [{"text": "NER", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9839885830879211}, {"text": "sequence prediction", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.715467244386673}]}, {"text": "Additionally, most of the NER systems are designed or trained based on monolingual newswire corpora, which are written with proper linguistic syntax.", "labels": [], "entities": []}, {"text": "However, noisy and user generated text data, which are common on social media, pose several challenges for generic NER systems, such as shorter and multilingual texts, ever evolving word forms and vocabulary, improper grammar, and shortened or incorrectly spelled words.", "labels": [], "entities": []}, {"text": "Let us consider a fictional tweet: \"r u guyz goin to c da #coldplay show @madisonsqrgrdn\uf04a?\".", "labels": [], "entities": []}, {"text": "This tweet contains two named entities, namely: \"Coldplay\", a music band, and \"Madison Square Garden, NYC, USA\", a geolocation, which references the place at which the band is playing.", "labels": [], "entities": [{"text": "Coldplay", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9284143447875977}]}, {"text": "Many of the terms present in the exemplary tweet would be considered as out of vocabulary (OOV) terms by traditional NER systems.", "labels": [], "entities": []}, {"text": "Furthermore, using a large set of such OOV tokens for training a classifier is likely to result in a sparse and high dimensional feature space, thereby increasing computing time.", "labels": [], "entities": []}, {"text": "The phenomenon of concept-drift, i.e., the meaning of terms shifting overtime, has also been found to affect the accuracy of NER systems overtime, resulting in poor performance of a classifier trained on older data).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9989186525344849}, {"text": "NER systems overtime", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.8616238236427307}]}, {"text": "The Workshop on \"Noisy User-generated Text\" (WNUT) continued its 2015 shared task on NER on tweets () in 2016.", "labels": [], "entities": []}, {"text": "In 2016, the task was divided into two parts: (1) identification of named entities in tweets, and (2) NER on 10 types of entities, namely person, geo-location, other, company, sports-team, facility, product, music-artist, movie, and tv-show.", "labels": [], "entities": [{"text": "identification of named entities in tweets", "start_pos": 50, "end_pos": 92, "type": "TASK", "confidence": 0.8256334960460663}, {"text": "NER", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.7461897134780884}]}, {"text": "In this paper we introduce two solutions to perform NER on tweets.", "labels": [], "entities": [{"text": "NER", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.989522397518158}]}, {"text": "The first system, which we will refer to as the submitted solution, was submitted as an entry to the WNUT 2016 NER shared task.", "labels": [], "entities": [{"text": "WNUT 2016 NER shared task", "start_pos": 101, "end_pos": 126, "type": "DATASET", "confidence": 0.7664742827415466}]}, {"text": "It uses random feature dropout for up-sampling the dataset.", "labels": [], "entities": []}, {"text": "This system was improved into a semisupervised solution (our 2 nd solution), which uses additional, unsupervised features.", "labels": [], "entities": []}, {"text": "These features have been found to be useful in prior information extraction and NER tasks.", "labels": [], "entities": [{"text": "prior information extraction", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.6734871566295624}, {"text": "NER tasks", "start_pos": 80, "end_pos": 89, "type": "TASK", "confidence": 0.9318998157978058}]}, {"text": "The semi-supervised approach circumvents the need to include word n-gram features from any tweets, and builds upon the successful usage of word representations, and word clusters) for NER by utilizing large amounts of unlabelled data or models pre-trained on a large vocabulary.", "labels": [], "entities": [{"text": "NER", "start_pos": 184, "end_pos": 187, "type": "TASK", "confidence": 0.9258852005004883}]}, {"text": "The SI system was designed to mitigate the various issues mentioned above, and utilizes the unlabelled tokens from the all the available datasets (including unlabelled test data) to improve the prediction quality on the evaluation datasets, a form of transductive learning).", "labels": [], "entities": [{"text": "SI", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9388809204101562}]}, {"text": "The SI system outperforms ST by ~7% (F1 score) when using the development set for evaluation, and by ~11% when using the test set (1% higher than the 2 nd best team in the task).", "labels": [], "entities": [{"text": "ST", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9494194388389587}, {"text": "F1 score)", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9868366320927938}]}, {"text": "The SI model does not utilize any word n-gram lexical features.", "labels": [], "entities": []}, {"text": "We believe that the approach taken for SI is useful for situations that require refinement or adaptation of an existing classifier to perform well on anew test set.", "labels": [], "entities": [{"text": "SI", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9919568300247192}]}, {"text": "We have released our experimental setup and code at https://github.com/napsternxg/TwitterNER.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Change in F1 score for the NER classifier on the development dataset on incremental  addition of different types of features (from left to right). ST refers to submitted solution, BL refers to  baseline solution provided by the organizers. Bolded values are the best scores across classifiers.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9833517074584961}, {"text": "BL", "start_pos": 190, "end_pos": 192, "type": "METRIC", "confidence": 0.9433730244636536}]}]}