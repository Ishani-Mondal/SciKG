{"title": [], "abstractContent": [{"text": "We explore a novel application of Question Generation (QG) for authentication use, where questions are widely used to verify user identity for online accounts.", "labels": [], "entities": [{"text": "Question Generation (QG)", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.837255722284317}]}, {"text": "In our approach, we prompt users to provide a few sentences about their personal life events.", "labels": [], "entities": []}, {"text": "We transform user-provided input sentences into a set of simple fact-based authentication questions.", "labels": [], "entities": []}, {"text": "We compared our approach with previous QG systems, and evaluation results show that our approach yielded better performance and the promise of future personalized authentica-tion question generation.", "labels": [], "entities": [{"text": "authentica-tion question generation", "start_pos": 163, "end_pos": 198, "type": "TASK", "confidence": 0.5863243838151296}]}], "introductionContent": [{"text": "An authentication question (also known as a security question), such as \"What is your mother's maiden name?\" is widely used for verifying user identity for many online accounts -such as email, banking, e-commerce and social networking.", "labels": [], "entities": []}, {"text": "However, past numerous breaches on security questions identify the weakness of the current fixed set of authentication questions.", "labels": [], "entities": []}, {"text": "Answers to some of those authentication questions are easy to guess based on simple commonsense, with little or no prior knowledge about the individual.", "labels": [], "entities": []}, {"text": "Since current security questions are not personalized, users can choose from a finite set of questions whose answers are easily guessed.", "labels": [], "entities": []}, {"text": "Also, not all questions are applicable to all users.", "labels": [], "entities": []}, {"text": "Motivated by the research of, in our study we automatically generate security questions from user-provided short texts from personal life events.", "labels": [], "entities": []}, {"text": "Given user-provided text such as, \"I visited Beijing in 2001 with John,\" we generate more meaningful authentication questions, such as: \"What city did you visit?\"", "labels": [], "entities": []}, {"text": "\"What year did you visit?\"", "labels": [], "entities": []}, {"text": "\"Who were you with?\"", "labels": [], "entities": []}, {"text": "These are more difficult to guess than the maiden name of a user's mother.", "labels": [], "entities": []}, {"text": "The contribution of this work is to automatically generate rule-based, concise, simple, fact-based shallow WH* questions, where we explore 1) dependency parsing based, and 2) semantic role labeling (SRL) based approaches to generating questions.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.728454664349556}, {"text": "semantic role labeling (SRL)", "start_pos": 175, "end_pos": 203, "type": "TASK", "confidence": 0.7592740952968597}]}], "datasetContent": [{"text": "We compared our systems to two other QG systems developed by and Heilman and Smith (2010b).", "labels": [], "entities": []}, {"text": "Both of those approaches over-generate questions and rank them to provide the best QA pairs.", "labels": [], "entities": []}, {"text": "We calculated the average precision, recall, and F1 score based on an exact word match for each question and answer pair.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.999339759349823}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9991810917854309}, {"text": "F1 score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9861167967319489}]}, {"text": "The evaluation results are shown in, where the dependency parsing system is denoted as DepPar, the SRL-based approach is denoted as SRL, the system by is denoted as OA, and the system by Heilman and Smith (2010b) is denoted as H&S.: Precision, recall, and F1 score for Generated Answers based on an exact word match manually generated Q&A pairs and Q&A pairs generated from each approach.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.6994517743587494}, {"text": "OA", "start_pos": 165, "end_pos": 167, "type": "METRIC", "confidence": 0.904678225517273}, {"text": "Precision", "start_pos": 233, "end_pos": 242, "type": "METRIC", "confidence": 0.9669927358627319}, {"text": "recall", "start_pos": 244, "end_pos": 250, "type": "METRIC", "confidence": 0.9972203969955444}, {"text": "F1 score", "start_pos": 256, "end_pos": 264, "type": "METRIC", "confidence": 0.9833745360374451}]}, {"text": "From, we observe that the DepPar system performs better than OA and H&S.", "labels": [], "entities": []}, {"text": "The dependency parser approach is better in capturing objects, time, and locations from simplified sentences, constructing better what, when and where questions, covering all the QTypes from manually generated data.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7530123293399811}]}, {"text": "The SRL-based system has the second-best performance.", "labels": [], "entities": [{"text": "SRL-based", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.6713969707489014}]}, {"text": "On the other hand, H&S has the lowest recall and performed poorly since it only generated 70% of the required QA set.", "labels": [], "entities": [{"text": "H&S", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.47984779874483746}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9996254444122314}, {"text": "QA set", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9569753706455231}]}, {"text": "The reason that OA performed poorly is that it generates the longest questions with an average of 9.8 words per question, while the average number of words in the manual dataset, H&S, DepPar, and SRL is 7.3, 7.2, 7.5, and 8.8 words per question, respectively.", "labels": [], "entities": []}, {"text": "Hence, extra words in OA are penalized for precision, where the length of generated sentences is critical for the calculation of these evaluation metrics.", "labels": [], "entities": [{"text": "OA", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.8372081518173218}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9976209998130798}]}, {"text": "Also, we evaluated the generated answers from each approach in, with manually generated answers based on an exact word match.", "labels": [], "entities": []}, {"text": "Both dependency and SRL-based approaches were better at capturing the candidate answers for date, location, people, subject, and object.", "labels": [], "entities": []}, {"text": "Hence, those approaches constructed better authentication questions.", "labels": [], "entities": []}, {"text": "On the other hand, other approaches missed required answers, and their F1 scores were lower as a result.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.980163186788559}]}], "tableCaptions": [{"text": " Table 2: Precision, recall, and F1 score for Gener- ated Questions based on an exact word match", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9986100196838379}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9959920048713684}, {"text": "F1 score", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9635206162929535}]}, {"text": " Table 3: Precision, recall, and F1 score for Gener- ated Answers based on an exact word match", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9985900521278381}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9968982934951782}, {"text": "F1 score", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9629831910133362}, {"text": "Gener- ated Answers", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.5208188220858574}]}]}