{"title": [], "abstractContent": [{"text": "We are presenting a hybrid MT approach in the WMT2016 Shared Translation Task for the IT-Domain.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9908373355865479}, {"text": "WMT2016 Shared Translation Task", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.7065452188253403}]}, {"text": "Our work consists of several translation components based on rule-based and statistical approaches that feed into an informed selection mechanism.", "labels": [], "entities": []}, {"text": "Additions to last year's submission include a WSD component, a syntactically-enhanced component and several improvements to the rule-based component, relevant to the particular domain.", "labels": [], "entities": [{"text": "WSD", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.7758049964904785}]}, {"text": "We also present detailed human evaluation on the output of all translation components, focusing on particular systematic errors.", "labels": [], "entities": []}], "introductionContent": [{"text": "We are presenting extensions on our hybrid MT approach from the WMT 2015 translation task in the generic-domain ().", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9888839721679688}, {"text": "WMT 2015 translation task", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.7281894385814667}]}, {"text": "The system combines several SMT and RBMT components that feed into an informed selection mechanism.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9554404020309448}, {"text": "RBMT", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.5841237902641296}]}, {"text": "For WMT 2016, several new system components have been submitted to the IT-task that are described in more detail in this paper.", "labels": [], "entities": [{"text": "WMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7879146933555603}]}, {"text": "In our work, detailed evaluation of translation quality using a wide variety of methods from automatic scores to human error annotation is an active part of the MT development process.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8248423039913177}, {"text": "MT development", "start_pos": 161, "end_pos": 175, "type": "TASK", "confidence": 0.9062268733978271}]}, {"text": "Already in previous work), we have argued for an approach to MT research and development (R&D) that makes a more direct use of the knowledge and expertise of language professionals.", "labels": [], "entities": [{"text": "MT research and development (R&D)", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.9392083750830756}]}, {"text": "One of the reasons is that it is difficult to build hybrid architectures (that take advantage of the fact that different engines make different errors) solely based on the rough feedback provided by automatic scores.", "labels": [], "entities": []}, {"text": "As scores like BLEU) are not suitable for comparison across different types of engines like Statistical Machine Translation (SMT) and Rule-based Machine Translation (RBMT), we have included human feedback by a language professional in the development of the components reported in this paper.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9976813793182373}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 92, "end_pos": 129, "type": "TASK", "confidence": 0.7579554965098699}, {"text": "Rule-based Machine Translation (RBMT)", "start_pos": 134, "end_pos": 171, "type": "TASK", "confidence": 0.7975166539351145}]}, {"text": "To this end, we complement our system development with specific manual analysis.", "labels": [], "entities": []}, {"text": "We have identified and manually inspected phenomena in the given domain that frequently lead to errors in our engines.", "labels": [], "entities": []}, {"text": "We are using the insights gained from this detailed analysis to guide further improvements of our engines and selection mechanism, some of which are detailed below.", "labels": [], "entities": []}, {"text": "Therefore, the components developed follow the direction of addressing some of the most observed systematic issues.", "labels": [], "entities": []}, {"text": "Nevertheless, the systems submitted to this task are only a stage in the continuous development effort.", "labels": [], "entities": []}, {"text": "The short paper is structured as follows: Section 2 includes a description of the individual components and the hybridization mechanism, section 3 presents a detailed manual evaluation focusing on systematic errors, whereas conclusions and ideas for further work are given in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Apart from the automatic evaluation scores, we include manual evaluation performed by a professional German linguist.", "labels": [], "entities": []}, {"text": "The manual evaluation was performed in four phases: \u2022 The annotator reads through the development set translated by all systems and identifies the phenomena where often errors occur.", "labels": [], "entities": []}, {"text": "\u2022 For each one of the prominent linguistic phenomena, the annotator selects 100 source segments including the respective phenomenon that is prone to MT errors.", "labels": [], "entities": [{"text": "MT", "start_pos": 149, "end_pos": 151, "type": "TASK", "confidence": 0.9873286485671997}]}, {"text": "\u2022 The total occurrences of each phenomenon in all source segments are counted (each phenomenon may occur more than once in a segment, and each segment may contain more than one sentences).", "labels": [], "entities": []}, {"text": "\u2022 Consequently, the annotator counts the times each phenomenon has been translated correctly.", "labels": [], "entities": []}, {"text": "For a translation to be correct it does not have to be identical with the reference translation.", "labels": [], "entities": []}, {"text": "This is repeated for the output of every MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9613980054855347}]}, {"text": "The accuracy is calculated as the ratio of the correct translations of the phenomenon divided by the occurrences of the phenomenon in the source.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995406866073608}]}, {"text": "The most prominent error categories were found to be imperatives, compounds, quotation marks, menu item sequences (separated by \">\"), missing verbs, phrasal verbs and terminology.", "labels": [], "entities": []}, {"text": "In these 7 categories, 657 source segments were chosen from development set Batch 2 to demonstrate the phenomena bound to the frequent errors 1 . Many segments contained multiple instances of the respective phenomena, resulting in 2104 instances of phenomena in overall.", "labels": [], "entities": []}, {"text": "The results appear in table 4.", "labels": [], "entities": []}, {"text": "The two baseline systems SMT and RBMT seem to have complementary behavior regarding the investigated phenomena.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9709181785583496}, {"text": "RBMT", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.5431170463562012}]}, {"text": "SMT performs well on terminology, menu items and quotation marks, but seems to suffer on imperatives, missing verbs, phrasal verbs and generation of compounds.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9792189002037048}]}, {"text": "On the contrary, RBMT does relatively well with imperatives, compounds, verbs and phrasal verbs, whereas it has issues with menu items and is relatively worse with terminology.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.8319504261016846}]}, {"text": "The linear combination system RBMT\u2192SMT manages to successfully combine the performance of the two systems regarding imperatives and maintains almost the same performance on verbs and terminology, whereas all other phenomena deteriorate, despite achieving higher automatic scores in overall.", "labels": [], "entities": [{"text": "RBMT\u2192SMT", "start_pos": 30, "end_pos": 38, "type": "TASK", "confidence": 0.46816694736480713}]}, {"text": "The SMT-syntax and the SMT-WSD systems seem to have relatively lower performance in all categories.", "labels": [], "entities": [{"text": "SMT-syntax", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.864222526550293}]}, {"text": "Since the performance of the WSD analyzer has already been confirmed, the failure of the SMT-WSD system to achieve a good performance on terminology and high n-gram-based automatic scores maybe an indication that the current data setting does not face ambiguity issues and the senses probably only add additional complexity.", "labels": [], "entities": [{"text": "WSD analyzer", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.8897823095321655}, {"text": "SMT-WSD", "start_pos": 89, "end_pos": 96, "type": "TASK", "confidence": 0.9068779945373535}]}, {"text": "The selection mechanism (which in its current version only included SMT-WSD, RBMT and RBMT\u2192SMT) performs better with the terminology and the quotation marks, whereas it maintains the good performance of its components on verbs and menu items.", "labels": [], "entities": []}, {"text": "Performance on phrasal verbs nevertheless suffers.", "labels": [], "entities": []}, {"text": "Additionally it achieves the highest accuracy on the selected phenomena, with 2% less errors than its best component, the baseline RBMT system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9991021156311035}, {"text": "errors", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9595165252685547}]}, {"text": "The two improved versions of the RBMT system appear to have solved the problems they were developed for, namely the compounded menu items and one of them also does better with the quotation marks.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.6917017698287964}]}, {"text": "The performance on imperatives, verbs and terminology remains the same, but the deterioration on phrasal verbs is obvious.", "labels": [], "entities": []}, {"text": "A postmortem analysis attributes this loss to a logical bug in the menu items detection, which often erroneously included title-cased verbs in the beginning of the sentence, preventing them from being translated as an active part of the sentence.: Translation accuracy on manually evaluated sentences focusing on particular phenomena.", "labels": [], "entities": [{"text": "Translation", "start_pos": 248, "end_pos": 259, "type": "TASK", "confidence": 0.9583693742752075}, {"text": "accuracy", "start_pos": 260, "end_pos": 268, "type": "METRIC", "confidence": 0.9773275852203369}]}, {"text": "Testsets consist of hand-picked source sentences of Batch 2 that include the respective phenomenon.", "labels": [], "entities": []}, {"text": "Boldface indicates best systems on each phenomenon (row) with a 0.95 confidence level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of corpora used for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9959751963615417}]}, {"text": " Table 1.  The text has been tokenized and truecased  (", "labels": [], "entities": []}, {"text": " Table 2: Automatic scores for factored SMT vari- ants with WSD. (*) indicates the version included  in the selection mechanism.", "labels": [], "entities": [{"text": "SMT vari- ants", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.8813437968492508}, {"text": "WSD", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.5969772934913635}]}, {"text": " Table 3: Improvements on the RBMT system. (*) indicates the submitted variations.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.7801453471183777}]}, {"text": " Table 4: Translation accuracy on manually evaluated sentences focusing on particular phenomena. Test- sets consist of hand-picked source sentences of Batch 2 that include the respective phenomenon. Bold- face indicates best systems on each phenomenon (row) with a 0.95 confidence level.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9720370769500732}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9705421924591064}]}, {"text": " Table 5: Human ranks and automatic scores of our  submitted systems on the tests, as a result of the  official evaluation. Ranks are given in a range in  order to account for confidence intervals.", "labels": [], "entities": []}]}