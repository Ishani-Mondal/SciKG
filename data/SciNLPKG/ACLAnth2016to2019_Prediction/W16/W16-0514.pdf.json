{"title": [], "abstractContent": [{"text": "In this paper we investigate how well the systems developed for automated evaluation of written responses perform when applied to spoken responses.", "labels": [], "entities": []}, {"text": "We compare two state of the art systems for automated writing evaluation and a state of the art system for evaluating spoken responses.", "labels": [], "entities": [{"text": "writing evaluation", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.6776457875967026}]}, {"text": "We find that the systems for writing evaluation achieve very good performance when applied to transcriptions of spoken responses but show degradation when applied to ASR output.", "labels": [], "entities": [{"text": "writing evaluation", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9366305470466614}, {"text": "ASR output", "start_pos": 166, "end_pos": 176, "type": "TASK", "confidence": 0.8822722136974335}]}, {"text": "The system based on sparse n-gram features appears to be more robust to such degradation.", "labels": [], "entities": []}, {"text": "We further explore the role of ASR accuracy and the performance and construct coverage of the combined model which includes all three engines.", "labels": [], "entities": [{"text": "ASR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9766252040863037}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9663060307502747}]}], "introductionContent": [{"text": "In this paper we evaluate how well the systems developed for automated evaluation of written responses perform when applied to spoken responses.", "labels": [], "entities": []}, {"text": "We use a corpus of spoken responses to an English language proficiency test and compare the performance of two state-of-the-art systems for evaluating writing and a state of the art system for evaluating spoken responses.", "labels": [], "entities": []}, {"text": "Automated speech scoring, until recently, primarily focused on evaluating pronunciation and prosody of highly constrained read speech).", "labels": [], "entities": []}, {"text": "With the improvement in automatic speech recognition technology, automated scoring has lately also been applied to constructed responses where the content of the response may not be known in advance ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7139314711093903}]}, {"text": "Earlier scoring systems for such responses still primarily evaluated delivery aspect of the response, but there has also been a growing amount of work on automatic evaluation of grammar, vocabulary and content of spoken responses.", "labels": [], "entities": []}, {"text": "While automatic evaluation of these high-level aspects of language proficiency is a relatively new field in automated speech scoring, there exists a substantial body of research on evaluating these constructs in written responses including several systems already used operationally for scoring responses to high-stakes language proficiency tests (see fora comprehensive overview).", "labels": [], "entities": [{"text": "automated speech scoring", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.6626370847225189}]}, {"text": "Automated scoring systems for spoken and written responses generally share a common structure: they extract a set of features measuring different aspects of language proficiency and use a machine learning algorithm to map those features to a human score.", "labels": [], "entities": []}, {"text": "There is also a substantial overlap in the criteria used to score grammar and vocabulary of spoken and written responses.", "labels": [], "entities": []}, {"text": "Therefore, it is not unreasonable to expect that some of the features developed for evaluating writing will also be applicable to scoring spoken responses (cf.).", "labels": [], "entities": []}, {"text": "On the other hand, the performance of such features can be affected by a number of factors.", "labels": [], "entities": []}, {"text": "First of all, many grammatical features rely on knowledge of sentence boundaries in order to parse the response into syntactic constituents.", "labels": [], "entities": []}, {"text": "In written responses the sentence boundaries can be established based on punctuation.", "labels": [], "entities": []}, {"text": "In spoken responses, however, these have to be estimated using machine learning algorithms such as the ones described in.", "labels": [], "entities": []}, {"text": "Furthermore, sentence boundaries in speech are often ambiguous.", "labels": [], "entities": []}, {"text": "These factors may lead to a decrease in feature performance.", "labels": [], "entities": []}, {"text": "Second, in automated speech scoring the transcription of the spoken responses necessary to evaluate grammar and vocabulary is obtained using automated speech recognition (ASR)).", "labels": [], "entities": [{"text": "automated speech scoring", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.5979057749112447}, {"text": "automated speech recognition (ASR))", "start_pos": 141, "end_pos": 176, "type": "TASK", "confidence": 0.7939852476119995}]}, {"text": "These systems may incorrectly recognize certain words introducing additional noise into the feature input and consequently lowering their performance.", "labels": [], "entities": []}, {"text": "Finally, spoken and written discourse differ in what is considered appropriate in terms of language use.", "labels": [], "entities": []}, {"text": "Thus, for example, sentence fragments typically considered inappropriate for written language are generally very common in unscripted spoken responses.", "labels": [], "entities": []}, {"text": "This may also impact how well the features developed for written responses perform on spoken responses.", "labels": [], "entities": []}, {"text": "We first introduce three state-of-the-art operational systems for automated scoring.", "labels": [], "entities": [{"text": "automated scoring", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.5439560860395432}]}, {"text": "We then apply the engines for evaluating writing to a corpus of spoken responses.", "labels": [], "entities": []}, {"text": "Finally, we evaluate whether combining different engines leads to further improvement in system performance and construct coverage.", "labels": [], "entities": []}, {"text": "2 Automated scoring systems 2.1 e-rater R e-rater R (E) is an engine that can automatically provide feedback on students' writing, as well as automatically assign a score to that writing.", "labels": [], "entities": [{"text": "R e-rater R (E)", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.7061023662487665}]}, {"text": "Using statistical and rule-based NLP methods, E identifies and extracts several feature classes for model building and essay scoring ().", "labels": [], "entities": [{"text": "model building", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7512198984622955}, {"text": "essay scoring", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.6829926520586014}]}, {"text": "Individual feature classes typically represent an aggregate of a larger feature set and are designed to capture a specific aspect of the construct being measured.", "labels": [], "entities": []}, {"text": "The feature classes used in this paper include the following: (a) grammatical errors (e.g., subject-verb agreement errors), (b) word usage errors (e.g., their versus there), (c) presence of essay-based discourse elements (e.g., thesis statement, main points, supporting details, and conclusions), (d) development of essay-based discourse elements, (e) a feature that considers correct usage of prepositions and collocations, and (f) sentence variety.", "labels": [], "entities": [{"text": "sentence variety", "start_pos": 433, "end_pos": 449, "type": "TASK", "confidence": 0.6901070773601532}]}, {"text": "To train anew scoring model, features are extracted from a training data set, and a linear model (roughly equivalent to non-negative least squares regression) is learned.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The total number of responses in each score category,", "labels": [], "entities": []}, {"text": " Table 2: Summary of performance (Pearson's r between the predicted and human score) of all 19 models evaluated in this study.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9263584017753601}, {"text": "Pearson's r", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9670902093251547}]}]}