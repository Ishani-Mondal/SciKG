{"title": [], "abstractContent": [{"text": "In this paper, we address the problem of quantifying the overall extent to which a test-taker's essay deals with the topic it is assigned (prompt).", "labels": [], "entities": []}, {"text": "We experiment with a number of models for word topicality, and a number of approaches for aggregating word-level indices into text-level ones.", "labels": [], "entities": [{"text": "word topicality", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7581042647361755}]}, {"text": "All models are evaluated for their ability to predict the holistic quality of essays.", "labels": [], "entities": []}, {"text": "We show that the best text-topicality model provides a significant improvement in a state-of-art essay scoring system.", "labels": [], "entities": []}, {"text": "We also show that the findings of the relative merits of different models generalize well across three different datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The instruction to \"stay on topic\" oft given to developing writers seems intuitively unproblematic, yet the question of the best way to measure this property of a text is far from settled, and little is known about the interaction of topicality and other properties of text, such as length.", "labels": [], "entities": []}, {"text": "We develop text topicality indices and evaluate them in the context of automated scoring of essays.", "labels": [], "entities": []}, {"text": "Specifically, we investigate the relationship between the extent to which the essay engages the topic provided in the essay question (prompt) and the quality of the essay as quantified by a human-provided holistic score.", "labels": [], "entities": []}, {"text": "In the existing literature, topicality has been addressed as a control flag to identify off-topic essays or spoken responses) or as an element in the overall coherence of the essay.", "labels": [], "entities": []}, {"text": "annotated essays for promptadherence, and found that achieving inter-rater reliability was very challenging, reporting Pearson r = 0.243 between two raters.", "labels": [], "entities": [{"text": "promptadherence", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.9453583359718323}, {"text": "Pearson r = 0.243", "start_pos": 119, "end_pos": 136, "type": "METRIC", "confidence": 0.9682352691888809}]}, {"text": "We address the relationship between a continuous topicality score and the holistic quality of an essay.", "labels": [], "entities": []}, {"text": "Generally, one can think of the topicality of a given word won a given topic T as the extent to which w occurs more often in texts addressing T than in otherwise comparable texts addressing a different topic.", "labels": [], "entities": []}, {"text": "We consider three models of word topicality from the literature: the significancetest approach as in topic signatures), the score-product approach as described in the essay scoring literature (), and a simple cutoff-based approach relying on difference in probabilities.", "labels": [], "entities": []}, {"text": "Given a definition of word topicality, the question arises how to quantify the topicality of the whole text.", "labels": [], "entities": []}, {"text": "Specifically, is topicality a property of the vocabulary of a text (of word types) or a property of both the vocabulary and the unfolding discourse (of word tokens)?", "labels": [], "entities": []}, {"text": "Thus, do the sentences \"I hate restaurants, abhor restaurants, loath restaurants, and love restaurants\" and \"I hate restaurants, abhor waiters, loath menus, and love food\" address the topic of restaurants to the same extent (this would be the prediction of the token-based model), or does the latter sentence address the topic to a greater extent than the former (this would be the prediction of the typebased model)?", "labels": [], "entities": []}, {"text": "The second sentence seems to en-gage more with the topic because it attends to more aspects (or details) of the topic.", "labels": [], "entities": []}, {"text": "In this paper, we implement type-based and token-based approaches to text topicality, using a number of different models for word topicality.", "labels": [], "entities": [{"text": "text topicality", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7153435200452805}, {"text": "word topicality", "start_pos": 125, "end_pos": 140, "type": "TASK", "confidence": 0.7402255833148956}]}, {"text": "All models are evaluated for their ability to predict the holistic quality of an essay.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "First, assuming a number of common definitions of word topicality and an application of predicting holistic quality of essays, we show that text-level topicality is most effectively modeled (a) as a property of word types rather than tokens in the text; (b) taking essay length into account.", "labels": [], "entities": []}, {"text": "Second, we show that when word topicality is defined using a simple cutoff-based measure and text-topicality is modeled as in (a),(b) above, we obtain a predictor of essay score that yields a statistically significant improvement in a state-of-art essay scoring system.", "labels": [], "entities": [{"text": "word topicality", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.6900297105312347}]}, {"text": "Third, we show that the characteristics of the best topicality model and its effectiveness in improving essay scoring generalize across different kinds of essays.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.7623109519481659}]}], "datasetContent": [{"text": "In this section, we present an evaluation of the best topicality index for each of the three datasets as a feature in a comprehensive, state-of-art essay scoring system.", "labels": [], "entities": []}, {"text": "The baseline engine (e-rater R , described in) computes more than 100 micro-features, which are aggregated into macro-features aligned with specific aspects of the writing construct.", "labels": [], "entities": []}, {"text": "The system incorporates macrofeatures measuring grammar, usage, mechanics, organization, development, etc; shows the nine macro-features, with examples of micro-features.", "labels": [], "entities": []}, {"text": "In addition, we put essay length (number of words) as the 10th macro-feature into the baseline model, to ascertain that any gains observed in the experimental condition are not due to the introduction of length as part of the scaling in the topicality feature.", "labels": [], "entities": []}, {"text": "In the baseline condition, a scoring model is built over the ten macro-features using linear regression on the Train set and evaluated on the Test set, for each of the datasets.", "labels": [], "entities": [{"text": "Train set", "start_pos": 111, "end_pos": 120, "type": "DATASET", "confidence": 0.9304554164409637}, {"text": "Test set", "start_pos": 142, "end_pos": 150, "type": "DATASET", "confidence": 0.840983122587204}]}, {"text": "In the experimental condition, the topicality index is added as the 11th macrofeature into the linear regression model; the experimental system is also trained on Train set and evaluated on Test set, for each of the datasets.", "labels": [], "entities": [{"text": "Train set", "start_pos": 163, "end_pos": 172, "type": "DATASET", "confidence": 0.8623762726783752}]}, {"text": "We evaluate essay scoring performance using Pearson correlation with human holistic score.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 44, "end_pos": 63, "type": "METRIC", "confidence": 0.936827152967453}]}, {"text": "To test statistical significance of the improvements, we use Wilcoxon signed-rank test for matched pairs.", "labels": [], "entities": []}, {"text": "We calculate the baseline and experimental performance on each prompt separately, and use the 76 pairs of values (for each of Sets 1 and 2) and 8 pairs of values (for TOEFL) as inputs  for the test.", "labels": [], "entities": [{"text": "TOEFL", "start_pos": 167, "end_pos": 172, "type": "METRIC", "confidence": 0.7244275808334351}]}, {"text": "We use VassarStats for performing the significance tests.", "labels": [], "entities": [{"text": "VassarStats", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9578355550765991}, {"text": "significance", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.965690553188324}]}, {"text": "We find that the addition of the topicality feature leads to a statistically significant improvement over the baseline for each of the three datasets.", "labels": [], "entities": []}, {"text": "In an additional set of experiments, we removed essay length from both the baseline and the experimental conditions to check whether the topicality feature would improve upon a state-of-art essay scoring system as-is; we found an improvement in all the three datasets, at the same significance levels as those reported in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sizes of the data partitions for each dataset. In  the N \u00d7 M notation, N = # prompts, M = # essays per  prompt. In TOEFL train and test sets, we show average  numbers of essays per prompt.", "labels": [], "entities": [{"text": "TOEFL train and test sets", "start_pos": 125, "end_pos": 150, "type": "DATASET", "confidence": 0.8659879088401794}]}, {"text": " Table 2: Distribution of essay scores, and average (std) of essay", "labels": [], "entities": [{"text": "average (std) of essay", "start_pos": 44, "end_pos": 66, "type": "METRIC", "confidence": 0.8360568185647329}]}]}