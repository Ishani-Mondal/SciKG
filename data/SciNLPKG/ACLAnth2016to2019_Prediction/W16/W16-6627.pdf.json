{"title": [{"text": "The aNALoGuE Challenge: Non Aligned Language GEneration", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a shared task based on recent advances in learning to generate natural language from meaning representations using semantically unaligned data.", "labels": [], "entities": []}, {"text": "The aNALoGuE challenge aims to evaluate and compare recent corpus-based methods with respect to their scalability to data size and target complexity, as well as to assess predictive quality of automatic evaluation metrics.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We will provide two types of baseline systems, which are frequently used by previous corpus-based methods, e.g. (): a challenging hand-crafted generator and n-gram Language Models, following early work by).", "labels": [], "entities": []}, {"text": "To evaluate the results, both objective and subjective metrics will be used.", "labels": [], "entities": []}, {"text": "We will explore automatic measures, such as BLEU-4 () and NIST) scores, which are widely used in a machine translation and NLG research, and will allow comparing the results of this challenge with previous work.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.997806966304779}, {"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7312026917934418}]}, {"text": "Since automatic metrics may not consistently agree with human perception, human evaluation will be used to assess subjective quality of generated utterances.", "labels": [], "entities": []}, {"text": "Human judges will be recruited using CrowdFlower.", "labels": [], "entities": []}, {"text": "Judges will be asked to compare utterance generated by different systems and score them in terms of informativeness (\"Does the utterance contains all the information specified in the MR?\"), naturalness (\"Could the utterance have been produced by a native speaker?\") and phrasing (\"Do you like the way the utterance has been expressed?\").", "labels": [], "entities": []}, {"text": "Here, we will explore different experimental setups for evaluation following previous shared tasks, e.g. ().", "labels": [], "entities": []}, {"text": "The challenge will also benefit from a national research grant on Domain Independent NLG (EP/M005429/1) which will provide funds for crowd-based evaluation.", "labels": [], "entities": [{"text": "Domain Independent NLG (EP/M005429/1)", "start_pos": 66, "end_pos": 103, "type": "DATASET", "confidence": 0.8830410003662109}]}], "tableCaptions": []}