{"title": [{"text": "Word Order Sensitive Embedding Features/Conditional Random Field- based Chinese Grammatical Error Detection", "labels": [], "entities": [{"text": "Chinese Grammatical Error Detection", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.6738324612379074}]}], "abstractContent": [{"text": "This paper discusses how to adapt two new word embedding features to build a more efficient Chinese Grammatical Error Diagnosis (CGED) systems to assist Chinese foreign learners (CFLs) in improving their written essays.", "labels": [], "entities": [{"text": "Chinese Grammatical Error Diagnosis (CGED)", "start_pos": 92, "end_pos": 134, "type": "TASK", "confidence": 0.6817152116979871}]}, {"text": "The major idea is to apply word order sensitive Word2Vec approaches including (1) structured skip-gram and (2) continuous window (CWindow) models, because they are more suitable for solving syntax-based problems.", "labels": [], "entities": []}, {"text": "The proposed new features were evaluated on the Test of Chinese as a Foreign Language (TOCFL) learner database provided by NLP-TEA-3&CGED shared task.", "labels": [], "entities": [{"text": "Test of Chinese as a Foreign Language (TOCFL) learner database", "start_pos": 48, "end_pos": 110, "type": "DATASET", "confidence": 0.5366622333725294}, {"text": "CGED shared task", "start_pos": 133, "end_pos": 149, "type": "DATASET", "confidence": 0.7938849925994873}]}, {"text": "Experimental results showed that the new features did work better than the traditional word order insensitive Word2Vec approaches.", "labels": [], "entities": []}, {"text": "Moreover, according to the official evaluation results, our system achieved the lowest (0.1362) false positive (FA) and the highest precision rates in all three measurements among all participants.", "labels": [], "entities": [{"text": "false positive (FA)", "start_pos": 96, "end_pos": 115, "type": "METRIC", "confidence": 0.8639666914939881}, {"text": "precision rates", "start_pos": 132, "end_pos": 147, "type": "METRIC", "confidence": 0.9879906177520752}]}], "introductionContent": [{"text": "In recent years, the rise of Asian economies and nearly 20 years of rapid development of China has led to a corresponding interest in the study of Standard Chinese (\"Mandarin\") as a foreign language, the official language of mainland China and Taiwan.", "labels": [], "entities": []}, {"text": "However, it might be a great challenge for those CFLs to learn how to write an essay or report in Chinese.", "labels": [], "entities": []}, {"text": "Because approximately 3,000 Chinese characters and 5,000 words are required for receiving Test of Chinese as a Foreign Language (TOCFL) certificate in advanced level . Beside, Chinese is an analytic language, in that they depend on syntax (word order and sentence structure) rather than morphology, i.e., changes inform of a word, to indicate the word's function in a sentence.", "labels": [], "entities": []}, {"text": "And Chinese also makes heavy use of grammatical particles to indicate aspect and mood, such as like \u4e86 (le , perfective), \u9084 (h\u00e1i, still), \u5df2\u7d93 (y\u01d0j\u012bng, already), and soon.", "labels": [], "entities": []}, {"text": "CFLs often make four types of grammatical errors, including (1) disorder, (2) missing, (3) redundant and (4) selection, for example: l Disorder: \u6211\u8981\u9001\u7d66\u4f60\u4e00\u500b\u6176\u795d\u79ae\u7269\u3002\u8981\u662f\u5169\u3001\u4e09\u5929\u665a\u4e86\uff0c\u8acb\u5225\u751f\u6c23 (\"\u5169\u3001\u4e09\u5929\u665a\u4e86\" should be \"\u665a\u4e86\u5169\u3001\u4e09\u5929\") l Missing: \u6211\u807d\u8aaa\u4f60\u627e\u5230\u5de5\u4f5c\u3002\u606d\u559c\u606d\u559c\uff01(\"\u5de5\u4f5c\" should be \"\u5de5\u4f5c\u4e86\") l Redundant: \u4eca\u5929\u662f\u6211\u5927\u5b78\u7562\u696d\u4e86 (\"\u4eca\u5929\u662f\" should be \"\u4eca\u5929\") l Selection: \u6211\u7b49\u5728\u6559\u5ba4\u6c92\u90a3\u9ebc\u4e45\u8001\u5e2b\u5c31\u4f86\u4e86 (\"\u90a3\u9ebc\" should be \"\u591a\") To detect those grammatical errors is not an easy task.", "labels": [], "entities": [{"text": "Disorder", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.8348976373672485}, {"text": "Selection", "start_pos": 297, "end_pos": 306, "type": "METRIC", "confidence": 0.9613146781921387}]}, {"text": "Recently, researchers have proposed many approaches for CGED task.", "labels": [], "entities": [{"text": "CGED task", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.7440318763256073}]}, {"text": "They could be roughly divided into two categories including (1) hybrid linguistic rules+language modelling and (2) pure classification-based methods.", "labels": [], "entities": []}, {"text": "For example, applied a set of handcrafted linguistic rules with syntactic information to detect errors occurred in Chinese sentences written by CFLs.", "labels": [], "entities": []}, {"text": "then further implemented a sentence judgement system that integrated both rule-based an and n-gram statistical methods to detect grammatical errors in Chinese sentences.", "labels": [], "entities": [{"text": "sentence judgement", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7091816514730453}]}, {"text": "proposed a system which measured the likelihood of sentences generated by deleting, inserting, or exchanging characters or words in which two sentence likelihood functions were proposed based on frequencies of space removed version of Google n-grams.", "labels": [], "entities": []}, {"text": "On the other hand, Xiang (2015) utilized an ensemble classifier random feature subspace method for CGED task.", "labels": [], "entities": []}, {"text": "proposed a CRF-based method to detect word ordering errors and a ranking SVM-based model to suggest the proper corrections.", "labels": [], "entities": [{"text": "word ordering errors", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7429728209972382}]}, {"text": "Finally, and also adopt CRFs and collected a set of common grammatical error rules for building CGED systems.", "labels": [], "entities": []}, {"text": "Among these two methods, the classification-based approach, especially the CRF-based one is quite promising.", "labels": [], "entities": []}, {"text": "Because, CRFs treat the CGED problem as a sequence-to-sequence mapping task, it could then model well the word ordering and sentence structure.", "labels": [], "entities": []}, {"text": "However, traditional CRF-based approaches often only take current and few neighbouring words and their POS tags as the input features.", "labels": [], "entities": []}, {"text": "This may limit CRFs' horizon vision.", "labels": [], "entities": [{"text": "CRFs' horizon vision", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.8383051554361979}]}, {"text": "Besides, word-based features will result in the sparse training data problem, since the total number of Chinese words is more than 160,000 2 . To alleviate these two difficulties, this paper would like to discuss how to adapt word embedding features to alleviate the sparseness issue and especially how to extract two new word order sensitive embedding features proposed by  to capture ordering information.", "labels": [], "entities": []}, {"text": "The major idea is to apply word order sensitive Word2Vec approaches including (1) Structured Skip-gram and (2) CWindow models.", "labels": [], "entities": []}, {"text": "Because they seriously take word ordering information into account and are therefore more suitable for solving syntax-based problems.", "labels": [], "entities": [{"text": "word ordering information", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7733883758385977}]}, {"text": "By this way, we hope we could build a more efficient CGED system.", "labels": [], "entities": []}, {"text": "2 System Implementation for NLP-TEA-3&CGED shared task The block diagram of our proposed system is shown in.", "labels": [], "entities": []}, {"text": "It has a CRF-based traditional Chinese parser for word segmentation and POS tagging frontend and a CRF-based CGED backend.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7664209604263306}, {"text": "POS tagging frontend", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7346190313498179}]}, {"text": "But the major enhancement comparing with other CRF-based approaches is that it applies the word order sensitive Word2Vec module to extract word embedding vectors and then does word clustering to generate input features for CRF-based CGED module.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 176, "end_pos": 191, "type": "TASK", "confidence": 0.7342161536216736}]}, {"text": "In the following subsections, several system components will be discussed in more detail, including (1) traditional Chinese parser, (2) word order sensitive Word2Vec, (3) grammatical rule and (4) CRFbased CGED models.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 157, "end_pos": 165, "type": "DATASET", "confidence": 0.8246398568153381}]}], "datasetContent": [{"text": "First of all, shows the impact of (with and without) error-prone words evaluated using a CWindow/CRF-based system.", "labels": [], "entities": []}, {"text": "According the results, it indicates that those special words did help to improve the performance of our CRF-based CGED system.", "labels": [], "entities": []}, {"text": "Therefore, those error-prone words will be considered in all systems reported below.", "labels": [], "entities": []}, {"text": "Approach Accuracy Precision Recall F1 Without 89.91% 52.17% 10.69% 17.75% With 89.89% 52.32% 10.89% 18.03%: Performance comparison on the effectiveness of adding the error-prone words feature templates on a CWindow/CRF-based CGED system.", "labels": [], "entities": [{"text": "Approach Accuracy Precision Recall F1", "start_pos": 0, "end_pos": 37, "type": "METRIC", "confidence": 0.7665373682975769}, {"text": "CWindow/CRF-based CGED system", "start_pos": 207, "end_pos": 236, "type": "DATASET", "confidence": 0.743796420097351}]}, {"text": "Second, showed the performance of the Structured Skip-gram-, CWindow-, Skip-gram-and CBOW-based CGED systems (all take error-prone words into account).", "labels": [], "entities": []}, {"text": "It is found that CWindow achieved the best F1-score.", "labels": [], "entities": [{"text": "CWindow", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.8014812469482422}, {"text": "F1-score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9985697269439697}]}, {"text": "Because F1-score is the most balanced performance measurement, all our submissions will use the CWindow-based approach.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.998589813709259}]}, {"text": "Three runs (NCTU+NTUT-Run1, Run2 and Run3) were submitted to NLP-TEA 2016 CGED shared task for official evaluation.", "labels": [], "entities": [{"text": "NCTU+NTUT-Run1", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.7359674970308939}, {"text": "NLP-TEA 2016 CGED shared task", "start_pos": 61, "end_pos": 90, "type": "DATASET", "confidence": 0.8933673858642578}]}, {"text": "All submissions are CWindow-based systems, since CWindow achieved the best performance in preliminary experiments.", "labels": [], "entities": []}, {"text": "The only difference between these three runs is that they have different FA performance (i.e., different operating points).", "labels": [], "entities": [{"text": "FA", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9980713725090027}]}, {"text": "shows the official evaluation results of our three submissions.", "labels": [], "entities": []}, {"text": "Among three submissions, Run1 has the lowest FA and highest precision rate in all three measurements comparing with other participants.", "labels": [], "entities": [{"text": "Run1", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.7796851992607117}, {"text": "FA", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9996652603149414}, {"text": "precision rate", "start_pos": 60, "end_pos": 74, "type": "METRIC", "confidence": 0.9899996221065521}]}, {"text": "Especially, Run1 achieved 0.1362 FA, 0.4603 accuracy, 0.2542 precision and 0.0483 recall rate in position-level.", "labels": [], "entities": [{"text": "FA", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9985515475273132}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.997408926486969}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9961422085762024}, {"text": "recall rate", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.9824163019657135}]}, {"text": "Since FA is the most important factor that influences users' experiences on CGED applications, the proposed approach is quite promising.: Official TOCFL evaluation results of NLP-TEA3&CGED shared task.", "labels": [], "entities": [{"text": "FA", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.9973286390304565}, {"text": "TOCFL", "start_pos": 147, "end_pos": 152, "type": "METRIC", "confidence": 0.6765310764312744}]}], "tableCaptions": [{"text": " Table 4:Performance of the Structured Skip-gram-, CWindow-, Skip-gram-and CBOW-based CGED  systems (all take error-prone words into account).", "labels": [], "entities": []}]}