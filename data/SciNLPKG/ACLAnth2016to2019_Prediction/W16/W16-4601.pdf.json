{"title": [{"text": "Overview of the 3rd Workshop on Asian Translation", "labels": [], "entities": [{"text": "Asian Translation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.6896830499172211}]}], "abstractContent": [{"text": "This paper presents the results of the shared tasks from the 3rd workshop on Asian translation (WAT2016) including J\u2194E, J\u2194C scientific paper translation subtasks, C\u2194J, K\u2194J, E\u2194J patent translation subtasks, I\u2194E newswire subtasks and H\u2194E, H\u2194J mixed domain subtasks.", "labels": [], "entities": [{"text": "Asian translation (WAT2016)", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.7046398818492889}]}, {"text": "For the WAT2016, 15 institutions participated in the shared tasks.", "labels": [], "entities": [{"text": "WAT2016", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.5122461318969727}]}, {"text": "About 500 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated .", "labels": [], "entities": []}], "introductionContent": [{"text": "The Workshop on Asian Translation (WAT) is anew open evaluation campaign focusing on Asian languages.", "labels": [], "entities": [{"text": "Asian Translation (WAT)", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.8391997456550598}]}, {"text": "Following the success of the previous workshops WAT2014 () and WAT2015 ( ), WAT2016 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation.", "labels": [], "entities": [{"text": "WAT2014", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.8859637975692749}, {"text": "WAT2015", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.8240677118301392}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7569755017757416}, {"text": "machine translation", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.7452263832092285}]}, {"text": "We are working toward the practical use of machine translation among all Asian countries.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7798082828521729}]}, {"text": "For the 3rd WAT, we adopt new translation subtasks with English-Japanese patent description, Indonesian-English news description and Hindi-English and Hindi-Japanese mixed domain corpus in addition to the subtasks that were conducted in WAT2015.", "labels": [], "entities": [{"text": "WAT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.5819652080535889}, {"text": "WAT2015", "start_pos": 237, "end_pos": 244, "type": "DATASET", "confidence": 0.8627752661705017}]}, {"text": "Furthermore, we invited research papers on topics related to the machine translation, especially for Asian languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8085507452487946}]}, {"text": "The submissions of the research papers were peer reviewed by at least 2 program committee members and the program committee accepted 7 papers that cover wide variety of topics such as neural machine translation, simultaneous interpretation, southeast Asian languages and soon.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 184, "end_pos": 210, "type": "TASK", "confidence": 0.6628921926021576}, {"text": "simultaneous interpretation", "start_pos": 212, "end_pos": 239, "type": "TASK", "confidence": 0.7786738872528076}]}, {"text": "WAT is unique for the following reasons: \u2022 Open innovation platform The test data is fixed and open, so evaluations can be repeated on the same data set to confirm changes in translation accuracy overtime.", "labels": [], "entities": [{"text": "WAT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.661195695400238}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9039868116378784}]}, {"text": "WAT has no deadline for automatic translation quality evaluation (continuous evaluation), so translation results can be submitted at anytime.", "labels": [], "entities": [{"text": "WAT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6332970261573792}, {"text": "translation quality evaluation", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.8098666667938232}]}, {"text": "\u2022 Domain and language pairs WAT is the world's first workshop that uses scientific papers as the domain, and Chinese \u2194 Japanese, Korean \u2194 Japanese and Indonesian \u2194 English as language pairs.", "labels": [], "entities": [{"text": "WAT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.758087694644928}]}, {"text": "In the future, we will add more Asian languages, such as Vietnamese, Thai, Burmese and soon.", "labels": [], "entities": []}, {"text": "\u2022 Evaluation method Evaluation is done both automatically and manually.", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 20, "end_pos": 30, "type": "TASK", "confidence": 0.8451676964759827}]}, {"text": "For human evaluation, WAT uses pairwise evaluation as the first-stage evaluation.", "labels": [], "entities": [{"text": "WAT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9804714918136597}]}, {"text": "Also, JPO adequacy evaluation is conducted for the selected submissions according to the pairwise evaluation results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We calculated automatic evaluation scores for the translation results by applying three metrics: BLEU (), RIBES () and AMFM (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9989603757858276}, {"text": "RIBES", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9904430508613586}, {"text": "AMFM", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9031860828399658}]}, {"text": "BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (; RIBES scores were calculated using RIBES.py version 1.02.4 12 ; AMFM scores were calculated using scripts created by technical collaborators of WAT2016.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9799787402153015}, {"text": "WAT2016", "start_pos": 232, "end_pos": 239, "type": "DATASET", "confidence": 0.8564746975898743}]}, {"text": "All scores for each task were calculated using one reference.", "labels": [], "entities": []}, {"text": "Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7038722932338715}]}, {"text": "For Japanese segmentation, we used three different tools: Juman version 7.0 (, KyTea 0.4.6) with Full SVM model and MeCab 0.996 () with IPA dictionary 2.7.0 14 . For Chinese segmentation we used two different tools: KyTea 0.4.6 with Full SVM Model in MSR model and Stanford Word Segmenter version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model 15).", "labels": [], "entities": [{"text": "Japanese segmentation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.5620394796133041}, {"text": "Chinese segmentation", "start_pos": 166, "end_pos": 186, "type": "TASK", "confidence": 0.7303652763366699}, {"text": "Chinese Penn Treebank (CTB)", "start_pos": 313, "end_pos": 340, "type": "DATASET", "confidence": 0.8133336901664734}]}, {"text": "For Korean segmentation we used mecabko . For English and Indonesian segmentations we used tokenizer.perl 17 in the Moses toolkit.", "labels": [], "entities": [{"text": "Korean segmentation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6506334245204926}]}, {"text": "For Hindi segmentation we used Indic NLP Library . Detailed procedures for the automatic evaluation are shown on the WAT2016 evaluation web page .  The participants submit translation results via an automatic evaluation system deployed on the WAT2016 web page, which automatically gives evaluation scores for the uploaded results.", "labels": [], "entities": [{"text": "Hindi segmentation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7984971404075623}, {"text": "WAT2016 evaluation web page", "start_pos": 117, "end_pos": 144, "type": "DATASET", "confidence": 0.9227917492389679}, {"text": "WAT2016 web page", "start_pos": 243, "end_pos": 259, "type": "DATASET", "confidence": 0.962891697883606}]}, {"text": "shows the submission interface for participants.", "labels": [], "entities": []}, {"text": "The system requires participants to provide the following information when they upload translation results: \u2022 Subtask: \u2022 Method (SMT, RBMT, SMT and RBMT, EBMT, NMT, Other); \u2022 Use of other resources in addition to ASPEC / JPC / BPPT Corpus / IITB Corpus; \u2022 Permission to publish the automatic evaluation scores on the WAT2016 web page.", "labels": [], "entities": [{"text": "ASPEC / JPC / BPPT Corpus / IITB Corpus", "start_pos": 213, "end_pos": 252, "type": "DATASET", "confidence": 0.7675778998268975}, {"text": "WAT2016 web page", "start_pos": 317, "end_pos": 333, "type": "DATASET", "confidence": 0.9377279281616211}]}, {"text": "The server for the system stores all submitted information, including translation results and scores, although participants can confirm only the information that they uploaded.", "labels": [], "entities": []}, {"text": "Information about translation results that participants permit to be published is disclosed on the web page.", "labels": [], "entities": []}, {"text": "In addition to submitting translation results for automatic evaluation, participants submit the results for human evaluation using the same web interface.", "labels": [], "entities": []}, {"text": "This automatic evaluation system will remain available even after WAT2016.", "labels": [], "entities": [{"text": "WAT2016", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.5841991901397705}]}, {"text": "Anybody can register to use the system on the registration web page 20 .  In WAT2016, we conducted 2 kinds of human evaluations: pairwise evaluation and JPO adequacy evaluation.", "labels": [], "entities": [{"text": "WAT2016", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.7429826855659485}, {"text": "JPO adequacy evaluation", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.5791513621807098}]}, {"text": "The pairwise evaluation is the same as the last year, but not using the crowdsourcing this year.", "labels": [], "entities": []}, {"text": "We asked professional translation company to do pairwise evaluation.", "labels": [], "entities": []}, {"text": "The cost of pairwise evaluation per sentence is almost the same to that of last year.", "labels": [], "entities": []}, {"text": "We randomly chose 400 sentences from the Test set for the pairwise evaluation.", "labels": [], "entities": []}, {"text": "We used the same sentences as the last year for the continuous subtasks.", "labels": [], "entities": []}, {"text": "Each submission is compared with the baseline translation (Phrase-based SMT, described in Section 3) and given a Pairwise score 21 .  We conducted pairwise evaluation of each of the 400 test sentences.", "labels": [], "entities": [{"text": "Phrase-based SMT", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.5647241324186325}, {"text": "Pairwise score", "start_pos": 113, "end_pos": 127, "type": "METRIC", "confidence": 0.9416416585445404}]}, {"text": "The input sentence and two translations (the baseline and a submission) are shown to the annotators, and the annotators are asked to judge which of the translation is better, or if they are of the same quality.", "labels": [], "entities": []}, {"text": "The order of the two translations are at random.", "labels": [], "entities": []}, {"text": "The participants' systems, which achieved the top 3 highest scores among the pairwise evaluation results of each subtask , were also evaluated with the JPO adequacy evaluation.", "labels": [], "entities": [{"text": "JPO", "start_pos": 152, "end_pos": 155, "type": "DATASET", "confidence": 0.6493911147117615}]}, {"text": "The JPO adequacy evaluation was carried out by translation experts with a quality evaluation criterion for translated patent documents which the Japanese Patent Office (JPO) decided.", "labels": [], "entities": [{"text": "Japanese Patent Office (JPO)", "start_pos": 145, "end_pos": 173, "type": "DATASET", "confidence": 0.6486300726731619}]}, {"text": "For each system, two annotators evaluate the test sentences to guarantee the quality.", "labels": [], "entities": []}, {"text": "The number of test sentences for the JPO adequacy evaluation is 200.", "labels": [], "entities": [{"text": "JPO", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.5961923003196716}]}, {"text": "The 200 test sentences were randomly selected from the 400 test sentences of the pairwise evaluation.", "labels": [], "entities": []}, {"text": "The test sentence include the input sentence, the submitted system's translation and the reference translation.", "labels": [], "entities": []}, {"text": "shows the JPO adequacy criterion from 5 to 1.", "labels": [], "entities": [{"text": "JPO", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6522168517112732}]}, {"text": "The evaluation is performed subjectively.", "labels": [], "entities": []}, {"text": "\"Important information\" represents the technical factors and their relationships.", "labels": [], "entities": []}, {"text": "The degree of importance of each element is also considered to evaluate.", "labels": [], "entities": []}, {"text": "The percentages in each grade are rough indications for the transmission degree of the source sentence meanings.", "labels": [], "entities": []}, {"text": "The detailed criterion can be found on the JPO document (in Japanese) . shows the list of participants for WAT2016.", "labels": [], "entities": [{"text": "JPO document", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.9714946746826172}, {"text": "WAT2016", "start_pos": 107, "end_pos": 114, "type": "TASK", "confidence": 0.7198663949966431}]}, {"text": "This includes not only Japanese organizations, but also some organizations from outside Japan.", "labels": [], "entities": []}, {"text": "15 teams submitted one or more translation results to the automatic evaluation server or human evaluation.", "labels": [], "entities": []}, {"text": "The   In this section, the evaluation results for WAT2016 are reported from several perspectives.", "labels": [], "entities": [{"text": "WAT2016", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.743418276309967}]}, {"text": "Some of the results for both automatic and human evaluations are also accessible at the WAT2016 website 24 ..", "labels": [], "entities": [{"text": "WAT2016 website", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.8288667500019073}]}, {"text": "The weights for the weighted \u03ba) is defined as |Evaluation1 \u2212 Evaluation2|/4.", "labels": [], "entities": []}, {"text": "From the evaluation results, the following can be observed: \u2022 Neural network based translation models work very well also for Asian languages.", "labels": [], "entities": [{"text": "Neural network based translation", "start_pos": 62, "end_pos": 94, "type": "TASK", "confidence": 0.581125907599926}]}, {"text": "\u2022 None of the automatic evaluation measures perfectly correlate to the human evaluation result (JPO adequacy).", "labels": [], "entities": [{"text": "JPO adequacy)", "start_pos": 96, "end_pos": 109, "type": "METRIC", "confidence": 0.7727709213892618}]}, {"text": "\u2022 The JPO adequacy evaluation result of IITB E\u2192H shows an interesting tendency: the system which achieved the best average score has the lowest ratio of the perfect translations and vice versa.", "labels": [], "entities": [{"text": "IITB E\u2192H", "start_pos": 40, "end_pos": 48, "type": "TASK", "confidence": 0.5684503465890884}]}, {"text": "Tables 9, 10, 11 and 12 show the results of statistical significance testing of ASPEC subtasks,, 15, 16 and 17 show those of JPC subtasks, 18 shows those of BPPT subtasks and 19 shows those of JPC subtasks.", "labels": [], "entities": [{"text": "BPPT", "start_pos": 157, "end_pos": 161, "type": "DATASET", "confidence": 0.9040331840515137}]}, {"text": "\u226b, \u226b and > mean that the system in the row is better than the system in the column at a significance level of p < 0.01, 0.05 and 0.1 respectively.", "labels": [], "entities": []}, {"text": "Testing is also done by the bootstrap resampling as follows: 1.", "labels": [], "entities": []}, {"text": "randomly select 300 sentences from the 400 pairwise evaluation sentences, and calculate the Pairwise scores on the selected sentences for both systems 2.", "labels": [], "entities": []}, {"text": "iterate the previous step 1000 times and count the number of wins (W ), losses (L) and ties (T )  The translation quality of JPC-CJ does not so much varied from the last year, but that of JPC-KJ is much worse.", "labels": [], "entities": [{"text": "translation", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.8886144161224365}, {"text": "JPC-CJ", "start_pos": 125, "end_pos": 131, "type": "DATASET", "confidence": 0.9314876794815063}, {"text": "JPC-KJ", "start_pos": 188, "end_pos": 194, "type": "DATASET", "confidence": 0.9402094483375549}]}, {"text": "Unfortunately, the best systems participated last year did not participate this year, so it is not directly comparable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Statistics for IITB Corpus.", "labels": [], "entities": [{"text": "IITB Corpus", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.8593902289867401}]}, {"text": " Table 8: JPO adequacy evaluation results in detail.", "labels": [], "entities": [{"text": "JPO adequacy evaluation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.5559129814306895}]}, {"text": " Table 13: Statistical significance testing of the JPC-JE Pairwise scores.", "labels": [], "entities": [{"text": "JPC-JE Pairwise scores", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.8495265444119772}]}, {"text": " Table 20: The Fleiss' kappa values for the pairwise evaluation results.", "labels": [], "entities": []}]}