{"title": [{"text": "Automatic label generation for news comment clusters", "labels": [], "entities": [{"text": "Automatic label generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6646813750267029}]}], "abstractContent": [{"text": "We present a supervised approach to automatically labelling topic clusters of reader comments to online news.", "labels": [], "entities": [{"text": "labelling topic clusters of reader comments to online news", "start_pos": 50, "end_pos": 108, "type": "TASK", "confidence": 0.7242564327187009}]}, {"text": "We use a feature set that includes both features capturing properties local to the cluster and features that capture aspects from the news article and from comments outside the cluster.", "labels": [], "entities": []}, {"text": "We evaluate the approach in an automatic and a manual, task-based setting.", "labels": [], "entities": []}, {"text": "Both evaluations show the approach to outperform a baseline method, which uses tf*idf to select comment-internal terms for use as topic labels.", "labels": [], "entities": []}, {"text": "We illustrate how cluster labels can be used to generate cluster summaries and present two alternative summary formats: a pie chart summary and an ab-stractive summary.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many application domains such as search engine snippet clustering (, summarising YouTube video comments () or online comments to news, grouping unlinked text segments by topic has been identified as a major requirement towards enabling efficient search or exploration of text collections.", "labels": [], "entities": [{"text": "search engine snippet clustering", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.6987492814660072}, {"text": "summarising YouTube video comments", "start_pos": 72, "end_pos": 106, "type": "TASK", "confidence": 0.8828700482845306}]}, {"text": "In the online news domain, thousands of reader comments are produced daily.", "labels": [], "entities": []}, {"text": "Identifying topics in comment streams is vitally important to providing an overview of what readers are saying.", "labels": [], "entities": [{"text": "Identifying topics in comment streams", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8685409307479859}]}, {"text": "However, merely clustering comments is not enough: topic clusters should also be given labels that accurately reflect their content, and that are accessible to users.", "labels": [], "entities": []}, {"text": "Producing \"good labels\" is challenging, as what constitutes a good label is not well defined.", "labels": [], "entities": []}, {"text": "A common method of labelling topic clusters with the top-n key terms characterising the topic is reported as less suitable than generating \"textual labels\" not consisting of key terms, to meaningfully represent the topic ().", "labels": [], "entities": []}, {"text": "In most studies, such textual labels are still extractive, i.e. the methods rely on labels being present within the textual sources (.", "labels": [], "entities": []}, {"text": "To overcome this limitation, many studies use external resources, most notably Wikipedia, for deriving topic labels., for example, present a graph-based approach to labeling using DBpedia concepts.", "labels": [], "entities": []}, {"text": "An advantage of such approaches is the potential to provide labels that are more abstract, and hence more akin to labels humans might produce.", "labels": [], "entities": []}, {"text": "apply such an approach to the online news domain, and evaluate it via an information retrieval task (similar to the evaluation in).", "labels": [], "entities": []}, {"text": "However, low recall figures were reported due to the abstractedness of the labels.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9993547797203064}]}, {"text": "also argue that external resources like Wikipedia titles are too broad for their e-mail and blog domain, as shown by the fact that none of the human-created labels in their development set appears in a Wikipedia title.", "labels": [], "entities": []}, {"text": "use human generated labels for social media posts in Google+, suggesting that post-internal information is not suitable for deriving labels.", "labels": [], "entities": []}, {"text": "In our work, we investigated label extraction from both the comments and from external sources, in our case the news article itself.", "labels": [], "entities": [{"text": "label extraction", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7290402799844742}]}, {"text": "This is motivated by two factors.", "labels": [], "entities": []}, {"text": "First, in this domain, the news article triggers the comments, so it is plausible that the article will contain terms suitable for labelling the topics of 61 some comment clusters.", "labels": [], "entities": []}, {"text": "Second, comments do not only discuss topics from the article, but may drift away from them.", "labels": [], "entities": []}, {"text": "Hence, using comment-internal terms as labels maybe useful too.", "labels": [], "entities": []}, {"text": "Thus we hypothesise that combining these two resources for label extraction should lead to a better performance.", "labels": [], "entities": [{"text": "label extraction", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.8278227150440216}]}, {"text": "We test this hypothesis using a baseline that extracts labels from the comment clusters only.", "labels": [], "entities": []}, {"text": "We adopt phrase or term as the most suitable linguistic unit to represent labels as evidenced by several previous studies.", "labels": [], "entities": []}, {"text": "This paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our dataset.", "labels": [], "entities": []}, {"text": "Section 3 discusses our labeling approach.", "labels": [], "entities": [{"text": "labeling", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.9635393023490906}]}, {"text": "The experimental setup as well the description of our baseline method are reported in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5 we present and discuss the results.", "labels": [], "entities": []}, {"text": "Section 6 presents how labels are used generate cluster summaries.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper and outlines directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the quality of automatic labels, we used two different evaluations: automatic and manual.", "labels": [], "entities": []}, {"text": "Inboth, we compare the performance of our proposed method SCL to our baseline method of tf*idf-based labeling, which is described below.", "labels": [], "entities": []}, {"text": "For the automatic evaluation we compare the gold standard labels to the machine generated ones.", "labels": [], "entities": []}, {"text": "For this purpose we use cosine similarity with and without Word2Vec word embeddings.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9298366904258728}]}, {"text": "We chose this approach for two reasons.", "labels": [], "entities": []}, {"text": "First, when humans and machine select labels that are the same or very similar, this can be captured by cosine similarity without Word2Vec word embeddings.", "labels": [], "entities": []}, {"text": "Second, humans and machine labels could have similar meaning but use different words, due to synonymy, in which case the use of Word2Vec word embeddings will help cosine to capture the semantic similarity between the labels.", "labels": [], "entities": []}, {"text": "Because of these reasons we use cosine with and without Word2Vec word embeddings.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9331009387969971}]}, {"text": "The cosine similarity between two labels L 1 and L 2 is computed as follows: where V (.) is -depending on whether Word2Vec embeddings are used -either the word vector holding the frequency counts of the words in the respective label or the 400 dimension Word2Vec vector holding the word embeddings.", "labels": [], "entities": []}, {"text": "Stop-words are removed before computing this metric.", "labels": [], "entities": []}, {"text": "The metric returns a value from 0 (no similarity) to 1 (100% sim-  In our manual evaluation, we used an online interface where the assessors could first read the news article and assess the quality of the labels based on the scenario shown in.", "labels": [], "entities": []}, {"text": "Four assessors took part in the evaluation; all were fluent in English and had a background in Computer Science.", "labels": [], "entities": []}, {"text": "All assessors evaluated the entire set of 20 clusters.", "labels": [], "entities": []}, {"text": "The manual evaluation was divided into three parts.", "labels": [], "entities": []}, {"text": "In the first part, assessors were asked to read the comments in the given cluster and to suggest a relevant label to this cluster (referred to as \"assessor labels\").", "labels": [], "entities": []}, {"text": "In the second part, three different labels (gold standard label, baseline label, and the label generated using our SCL method) were then shown in a random order.", "labels": [], "entities": []}, {"text": "For each label, assessors were asked to answer three questions using a 5-point Likert Scale (1: strongly disagree, 5: strongly agree): i) Q1: I can understand this label, ii) Q2: This label is a complete phrase, and iii) Q3: This label accurately reflects the content of the comment cluster.", "labels": [], "entities": []}, {"text": "Lastly, assessors were asked to provide any comments of all the labels they have assessed.", "labels": [], "entities": []}, {"text": "Imagine you want to gain a quick overview of what is said in the comments of the news article, but have only a limited amount of time (e.g. a coffee break).", "labels": [], "entities": [{"text": "overview of what is said in the comments of the news article", "start_pos": 33, "end_pos": 93, "type": "TASK", "confidence": 0.6662285476922989}]}, {"text": "The system groups comments into clusters (relating to the same topic), and provides a label, which is a word or phrase that briefly indicates the content of the cluster.", "labels": [], "entities": []}, {"text": "A good label should give you a sense of the topics discussed in a cluster, perhaps helping you to decide whether or not to read those comments.", "labels": [], "entities": []}, {"text": "The results of the automatic evaluation are shown in.", "labels": [], "entities": []}, {"text": "From the table we can see that both baseline and the proposed approach achieve very similar scores measured using cosine without Word2Vec embeddings.", "labels": [], "entities": []}, {"text": "Both scores are below 10% indicating that they have very little word overlap between the gold standard labels.", "labels": [], "entities": []}, {"text": "When Word2Vec embeddings are used we seethe SCL method achieves higher Word2Vec cosine similarity than the baseline method.", "labels": [], "entities": [{"text": "Word2Vec cosine similarity", "start_pos": 71, "end_pos": 97, "type": "METRIC", "confidence": 0.7004817326863607}]}, {"text": "The difference between the methods is also significant (p < 0.05).", "labels": [], "entities": []}, {"text": "According to this SCL is a better choice in terms of automatic cluster labeling.", "labels": [], "entities": [{"text": "cluster labeling", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.5941830426454544}]}, {"text": "We gathered judgments of 20 cluster labels from each method: gold-standard (GS), baseline, and SCL.", "labels": [], "entities": []}, {"text": "This results in the judgments of 60 cluster labels given by each assessor.", "labels": [], "entities": []}, {"text": "These labels were evaluated on three aspects as described in Section 4.3.", "labels": [], "entities": []}, {"text": "shows the average scores given by the four assessors for the evaluation questions, where Q1 identifies whether the label can be understood, Q2 represents the phrase completeness of the label, and Q3 represents the accuracy of the label.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9990578293800354}]}, {"text": "As we can see from the results the average scores with respect to the Q1 and Q2 are for both the baseline and our SCL method close to the gold label scores.", "labels": [], "entities": []}, {"text": "This shows that both automatic labels can be understood and that they are both complete phrases.", "labels": [], "entities": []}, {"text": "The results for the Q3, however, are for both systems much lower than the gold label figures.", "labels": [], "entities": []}, {"text": "The baseline system achieves on average 1.98, the SCL 2.43 and the gold labels 4.26.", "labels": [], "entities": []}, {"text": "The results between the baseline and SCL present a stable bias across all questions towards the SCL method.", "labels": [], "entities": []}, {"text": "In all questions the SCL method outperforms the baseline approach by on average 0.27-0.45 points.", "labels": [], "entities": []}, {"text": "We measure inter-assessor agreement using Krippendorff's alpha coefficient.", "labels": [], "entities": []}, {"text": "Agreements in Q1 and Q2 are 0.423 and 0.372, respectively, while, higher agreement of \u03b1 = 0.699 is achieved in Q3.", "labels": [], "entities": [{"text": "agreement", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9967420697212219}]}, {"text": "Overall, 91.67% cases in Q3 were assigned the identical or a majority score by the four assessors.", "labels": [], "entities": []}, {"text": "These figures were 88.3% and 85% for Q1 and Q2, respectively.", "labels": [], "entities": [{"text": "Q1", "start_pos": 37, "end_pos": 39, "type": "DATASET", "confidence": 0.8868535757064819}, {"text": "Q2", "start_pos": 44, "end_pos": 46, "type": "DATASET", "confidence": 0.4787626266479492}]}, {"text": "Disagreements in Q1 occurred when the labels included errors or were grammatically incorrect, such as 'threat so network rail'.", "labels": [], "entities": []}, {"text": "The assessors differed in their judgment as to whether the error was relevant to their understanding of the label.", "labels": [], "entities": []}, {"text": "A further source of disagreement in Q1 were general labels ('design stage'), or abstract labels ('bath of snobbery').", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Automatic evaluation results", "labels": [], "entities": []}, {"text": " Table 3: Error analysis: example labels along with their average judgment scores. 'Assessor Labels' lists the labels proposed by", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7054027765989304}]}, {"text": " Table 4: Comparison of label lengths", "labels": [], "entities": []}]}