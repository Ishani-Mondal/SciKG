{"title": [{"text": "Automatic Annotation of Structured Facts in Images", "labels": [], "entities": []}], "abstractContent": [{"text": "Motivated by the application of fact-level image understanding, we present an automatic method for data collection of struc-tured visual facts from images with captions.", "labels": [], "entities": [{"text": "fact-level image understanding", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.6493503848711649}, {"text": "data collection of struc-tured visual facts from images with captions", "start_pos": 99, "end_pos": 168, "type": "TASK", "confidence": 0.8064906239509583}]}, {"text": "Example structured facts include attributed objects (e.g., <flower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man, walking, dog>), and positional information (e.g., <vase, on, table>).", "labels": [], "entities": []}, {"text": "The collected annotations are in the form of fact-image pairs (e.g.,<man, walking, dog> and an image region containing this fact).", "labels": [], "entities": []}, {"text": "With a language approach , the proposed method is able to collect hundreds of thousands of visual fact annotations with accuracy of 83% according to human judgment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9989757537841797}]}, {"text": "Our method automatically collected more than 380,000 visual fact annotations and more than 110,000 unique visual facts from images with captions and localized them in images in less than one day of processing time on standard CPU platforms.", "labels": [], "entities": []}, {"text": "We will make the data publically available.", "labels": [], "entities": []}], "introductionContent": [{"text": "People generally acquire visual knowledge by exposure to both visual facts and to semantic or language-based representations of these facts, e.g., by seeing an image of \"a person petting dog\" and observing this visual fact associated with its language representation . In this work, we focus on methods for collecting structured facts that we define as structures that provide attributes about an object, and/or the actions and interactions this object may have with other objects.", "labels": [], "entities": []}, {"text": "We introduce the idea of automatically collecting annotations for second order visual facts and third order visual facts where second order facts <S,P> are attributed objects (e.g., <S: car, P: red>) and singleframe actions (e.g., <S: person, P: jumping>), and third order facts specify interactions (i.e., <boy, petting, dog>).", "labels": [], "entities": []}, {"text": "This structure is helpful for designing machine learning algorithms that learn deeper image semantics from caption data and allow us to model the relationships between facts.", "labels": [], "entities": []}, {"text": "In order to enable such a setting, we need to collect these structured fact annotations in the form of (language view, visual view) pairs (e.g., <baby, sitting on, chair> as the language view and an image with this fact as a visual view) to train models.", "labels": [], "entities": []}, {"text": "( showed that visual concepts, from a predefined ontology, can be learned by querying the web about these concepts using image-web search engines.", "labels": [], "entities": []}, {"text": "More recently,) presented an approach to learn concepts related to a particular object by querying the web with Google-N-gram data that has the concept name.", "labels": [], "entities": []}, {"text": "There are three limitations to these approaches.", "labels": [], "entities": []}, {"text": "(1) It is difficult to define the space of visual knowledge and then search for it.", "labels": [], "entities": []}, {"text": "It is further restricting to define it based on a predefined ontology such as) or a particular object such as (.", "labels": [], "entities": []}, {"text": "Using image search is not reliable to collect data for concepts with few images on the web.", "labels": [], "entities": []}, {"text": "These methods assume that the top retrieved examples by imageweb search are positive examples and that there are images available that are annotated with the searched concept.", "labels": [], "entities": []}, {"text": "(3) These concepts/facts are not structured and hence annotations lacks information like \"jumping\" is the action part in <person, jumping >, or \"man' and \"horse\" are interacting in <person, riding, horse >.", "labels": [], "entities": []}, {"text": "This structure is important for deeper understanding of visual data, which is one of the main motivations of this work.", "labels": [], "entities": []}, {"text": "The problems in the prior work motivate us to propose a method to automatically annotate struc-1: Structured Fact Automatic Annotation tured facts by processing image caption data since facts in image captions are highly likely to be located in the associated images.", "labels": [], "entities": []}, {"text": "We show that a large quantity of high quality structured visual facts could be extracted from caption datasets using natural language processing methods.", "labels": [], "entities": []}, {"text": "Caption writing is free-form and an easier task for crowd-sourcing workers than labeling second-and third-order tasks, and such free-form descriptions are readily available in existing image caption datasets.", "labels": [], "entities": [{"text": "Caption writing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9645507037639618}]}, {"text": "We focused on collecting facts from the MS COCO image caption dataset () and the newly collected Flickr30K entities).", "labels": [], "entities": [{"text": "MS COCO image caption dataset", "start_pos": 40, "end_pos": 69, "type": "DATASET", "confidence": 0.9011142730712891}, {"text": "Flickr30K entities", "start_pos": 97, "end_pos": 115, "type": "DATASET", "confidence": 0.9447583854198456}]}, {"text": "We automatically collected more than 380,000 structured fact annotations in high quality from both the 120,000 MS COCO scenes and 30,000 Flickr30K scenes.", "labels": [], "entities": [{"text": "MS COCO scenes", "start_pos": 111, "end_pos": 125, "type": "DATASET", "confidence": 0.8715614080429077}, {"text": "Flickr30K scenes", "start_pos": 137, "end_pos": 153, "type": "DATASET", "confidence": 0.9143678545951843}]}, {"text": "The main contribution of this paper is an accurate, automatic, and efficient method for extraction of structured fact visual annotations from image-caption datasets, as illustrated in.", "labels": [], "entities": [{"text": "extraction of structured fact visual annotations", "start_pos": 88, "end_pos": 136, "type": "TASK", "confidence": 0.8042985498905182}]}, {"text": "Our approach (1) extracts facts from captions associated with images and then (2) localizes the extracted facts in the image.", "labels": [], "entities": []}, {"text": "For fact extraction from captions, We propose anew method called SedonaNLP for fact extraction to fill gaps in existing fact extraction from sentence methods like Clausie.", "labels": [], "entities": [{"text": "fact extraction from captions", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8569657206535339}, {"text": "fact extraction", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.7928357422351837}]}, {"text": "SedonaNLP produces more facts than Clausie, especially <subject,attribute> facts, and thus enables collecting more visual annotations than using Clausie alone.", "labels": [], "entities": []}, {"text": "The final set of automatic annotations are the set of successfully localized facts in the associated images.", "labels": [], "entities": []}, {"text": "We show that these facts are extracted with more than 80% accuracy according to human judgment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9989150762557983}]}], "datasetContent": [{"text": "We propose three questions to evaluate each annotation: (Q1) Is the extracted fact correct (Yes/No)?", "labels": [], "entities": []}, {"text": "The purpose of this question is to evaluate errors captured by the first step, which extracts facts by Sedona or Clausie.", "labels": [], "entities": []}, {"text": "(Q2) Is the fact located in the image (Yes/No)?", "labels": [], "entities": []}, {"text": "In some cases, there might be a fact mentioned in the caption that does not exist in the image and is mistakenly considered as an annotation.", "labels": [], "entities": []}, {"text": "(Q3) How accurate is the box assigned to a given fact (a to g)?", "labels": [], "entities": []}, {"text": "a (about right), b (a bit big), c (a bit small), d (too small), e (too big), f (totally wrong box), g (fact does not exist or other).", "labels": [], "entities": []}, {"text": "Our instructions on these questions to the participants can be found in this url.", "labels": [], "entities": []}, {"text": "We evaluate these three questions for the facts that were successfully assigned a box in the image, because the main purpose of this evaluation is to measure the usability of the collected annotations as training data for our model.", "labels": [], "entities": []}, {"text": "We created an Amazon Mechanical Turk form to ask these three questions.", "labels": [], "entities": []}, {"text": "So far, we collected a total of 10,786 evaluation responses, which are an evaluation of 3,595 (f v , fl ) pairs (3 responses/ pair).", "labels": [], "entities": []}, {"text": "Table 2 shows the evaluation results, which indicate that the data is useful for training, since\u224883.1% of them are correct facts with boxes that are either about right, or a bit big or small (a,b,c).", "labels": [], "entities": []}, {"text": "We further some evaluation responses that we collected from volunteer researchers in showing similar results.", "labels": [], "entities": []}, {"text": "shows some successful qualitative results that include four extracted structured facts from MS COCO dataset (e.g., <person, using, phone>, <person, standing>, etc).", "labels": [], "entities": [{"text": "MS COCO dataset", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.901690661907196}]}, {"text": "show a negative example where there is a wrong fact among the extracted facts (i.e., <house, ski>).", "labels": [], "entities": []}, {"text": "The main reason for this failure case is that \"how\" is mistyped as \"house\"; see The supplementary materials) includes all the captions of these examples and also additional qualitative examples.", "labels": [], "entities": []}, {"text": "In order to study how the method behave in both easy and hard examples.", "labels": [], "entities": []}, {"text": "This section present statistics of the successfully extracted facts and relate it to the hardness of the extraction of these facts.", "labels": [], "entities": []}, {"text": "We start by defining hardness of an extracted fact in our case and its dependency on the fact type.", "labels": [], "entities": []}, {"text": "Our method collect both second-and third-order facts.", "labels": [], "entities": []}, {"text": "We refer to candidate subjects as all instances of the entity in the image that match the subject type of either a second-order fact <S,P> or a third-order fact <S,P,O>.", "labels": [], "entities": []}, {"text": "We refer to candidate objects as all instances in the image that match the object type of a third-order fact <S,P,O>.", "labels": [], "entities": []}, {"text": "The selection of the candidate subjects and candidate objects is apart of our method that we detailed in Sec 5.", "labels": [], "entities": []}, {"text": "We define the hardness for second order facts by the number of candidate subjects and the hardness of third order facts by the number of candidate subjects multiplied by the In, the Y axis is the number of facts for each bin.", "labels": [], "entities": []}, {"text": "The X axis shows the bins that correspond to hardness that we defined for both second and third order fats.", "labels": [], "entities": []}, {"text": "shows a histogram of the difficulties for all Mturk evaluated examples including both the successful and the failure cases.", "labels": [], "entities": []}, {"text": "shows a similar histogram but for but for subset of facts verified by the Turkers with Q3 as (about right).", "labels": [], "entities": [{"text": "Turkers", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.8907960057258606}]}, {"text": "The figures show that the method is able to handle difficulty cases even with more than 150 possibilities for grounding.", "labels": [], "entities": []}, {"text": "We show these results broken out for MSCOCO and Flickr30K Entities datasets and for each fact types in the supplementary materials ().", "labels": [], "entities": [{"text": "MSCOCO", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.9306845664978027}, {"text": "Flickr30K Entities datasets", "start_pos": 48, "end_pos": 75, "type": "DATASET", "confidence": 0.9498377839724222}]}], "tableCaptions": [{"text": " Table 1: Human Subject Evaluation by MTurk workers %", "labels": [], "entities": [{"text": "Human Subject Evaluation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7445672750473022}]}, {"text": " Table 2: Human Subject Evaluation by Volunteers % (This is another set of annotations different from  those evaluated by MTurkers)", "labels": [], "entities": [{"text": "Volunteers", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.984808087348938}, {"text": "MTurkers", "start_pos": 122, "end_pos": 130, "type": "DATASET", "confidence": 0.7812080979347229}]}]}