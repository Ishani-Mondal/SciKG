{"title": [{"text": "Extending AIDA framework by incorporating coreference resolution on detected mentions and pruning based on popularity of an entity", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9599736332893372}]}], "abstractContent": [{"text": "Named Entity Disambiguation (NED) is gaining popularity due to its applications in the field of information extraction.", "labels": [], "entities": [{"text": "Named Entity Disambiguation (NED)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8032984932263693}, {"text": "information extraction", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.8800958395004272}]}, {"text": "Entity linking or Named Entity Disambigua-tion is the task of discovering entities such as persons, locations, organizations, etc. and is challenging due to the high ambiguity of entity names in natural language text.", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7812351882457733}]}, {"text": "In this paper, we propose a modification to the existing state of the art for NED, Accurate Online Disambiguation of Entities (AIDA) framework.", "labels": [], "entities": []}, {"text": "As a mention's name in a text can appear many times in shorter forms, we propose to use corefer-ence resolution on the detected mentions.", "labels": [], "entities": []}, {"text": "Entity mentions within the document are clustered to their longer form.", "labels": [], "entities": []}, {"text": "We use the popularity of candidate entities to prune them and based on the similarity measure of AIDA the entity fora mention is chosen.", "labels": [], "entities": [{"text": "AIDA", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.8793162107467651}]}, {"text": "The mentions are broadly classified into four categories person, location, organization and miscellaneous and the effect of coreference and pruning were analyzed on each category.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the unsolved problems in computer science is understanding and producing natural language by machines.", "labels": [], "entities": []}, {"text": "The goal of fully understanding is out of reach but there have been significant advances recently.", "labels": [], "entities": []}, {"text": "Systems are able to understand words or phrases of text by explicitly representing their meaning.", "labels": [], "entities": []}, {"text": "Once the meanings of individual words are known, next is to find the relation among them.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments were carried on two data sets.", "labels": [], "entities": []}, {"text": "First one is the TIPSTER 1 data set from which 45 documents were randomly chosen.", "labels": [], "entities": [{"text": "TIPSTER 1 data set", "start_pos": 17, "end_pos": 35, "type": "DATASET", "confidence": 0.781559094786644}]}, {"text": "These documents were related to news.", "labels": [], "entities": []}, {"text": "The second dataset is the IITB dataset by, out of which 50 documents were taken.", "labels": [], "entities": [{"text": "IITB dataset", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.9787310063838959}]}, {"text": "The IITB documents were collected from online news sources and are not well formatted and sometimes had comments of online users.", "labels": [], "entities": [{"text": "IITB documents", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.89588862657547}]}, {"text": "The CoNLL 2003 dataset used by the AIDA is copyright protected but the annotations are available.", "labels": [], "entities": [{"text": "CoNLL 2003 dataset", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9592445691426595}]}, {"text": "We have manually annotated all the documents, i.e. both 45 documents of Tipster dataset and 50 documents of IITB dataset.", "labels": [], "entities": [{"text": "Tipster dataset", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9790247976779938}, {"text": "IITB dataset", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.9858463704586029}]}, {"text": "The properties of the dataset are given in  Only the relevant mentions retrieved by AIDA are considered.", "labels": [], "entities": [{"text": "AIDA", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.8927949666976929}]}, {"text": "Bermuda-based company ....\" where AIDA retrieves \"Bermudabased\" as mention which is considered irrelevant.", "labels": [], "entities": [{"text": "AIDA retrieves \"Bermudabased\"", "start_pos": 34, "end_pos": 63, "type": "DATASET", "confidence": 0.6508148670196533}]}, {"text": "Similarly, \"....Cuban-Soviet friendship...\" is retrieved as \"Cuban-Soviet\" which is irrelevant.", "labels": [], "entities": []}, {"text": "The mention mappings can be of four types: A mention whose entity is not registered in the database and mapping gives NULL, a mention whose entity is not registered in the database and maps to some entity, a mention whose entity is registered in the database and maps to an incorrect entity and mention whose entity is registered in the database and maps to the correct entity.", "labels": [], "entities": [{"text": "NULL", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9669121503829956}]}, {"text": "Precision is the fraction of mention entity mappings that match the ground truth assignments.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9872012138366699}]}, {"text": "Macro average precision is the average of precision of each document.", "labels": [], "entities": [{"text": "Macro average precision", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.4685904085636139}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9974254965782166}]}, {"text": "Micro average precision is the fraction of mention entity mappings in all documents that match the ground truth assignments.", "labels": [], "entities": [{"text": "Micro average precision", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.5514296988646189}]}, {"text": "give the details of NULL entities in ground truth.", "labels": [], "entities": []}, {"text": "The recall remains the same for the AIDA and for the experiments done as both use the same retrieval methods.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9994390606880188}, {"text": "AIDA", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.6594634652137756}]}, {"text": "The mentions were retrieved using the Stanford NER which classifies the men-40   tions into four categories namely person (p), location (l), organization (o) and miscellaneous (m).", "labels": [], "entities": [{"text": "Stanford NER", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9546429812908173}, {"text": "miscellaneous (m)", "start_pos": 162, "end_pos": 179, "type": "METRIC", "confidence": 0.9204538762569427}]}, {"text": "The mentions were also annotated for their labels manually.", "labels": [], "entities": []}, {"text": "gives the Confusion Matrix for both the datasets.", "labels": [], "entities": [{"text": "Confusion Matrix", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9668489396572113}]}, {"text": "The column is the actual label and the row is the labels predicted by NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8202659487724304}]}, {"text": "AIDA was run with four settings: \u2022 LocalDisambigautionSetting()-uses prior and similarity with a prior test, described in Section 3; \u2022 LocalDisambiguationWithNullSettings()-uses the above method but uses a threshold to find NULL entities; \u2022 CocktailDisambiguationSettings()-uses the graph method, described in Section 3; \u2022 CocktailDisambiguationWithNullSettings()-uses the above method but uses a threshold to find NULL entities;  The experiments were carried outwith the following 8 methods: \u2022 Method 1 (AG): AIDA graph Disambiguation.", "labels": [], "entities": [{"text": "AIDA graph Disambiguation", "start_pos": 510, "end_pos": 535, "type": "TASK", "confidence": 0.7325994968414307}]}, {"text": "\u2022 Method 2 (AGN): AIDA graph Disambiguation with NULL settings.", "labels": [], "entities": []}, {"text": "\u2022 Method 3 (AL): AIDA local Disambiguation.", "labels": [], "entities": [{"text": "AIDA local Disambiguation", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.6001359025637308}]}, {"text": "\u2022 Method 4 (ALN): AIDA local Disambiguation with NULL settings.", "labels": [], "entities": [{"text": "AIDA local Disambiguation", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.6580653587977091}]}, {"text": "\u2022 Method 5 (NP): No Pruning-The method is run based on Algorithm 1 except the lines 4, 13 to 18 are not performed.", "labels": [], "entities": []}, {"text": "\u2022 Method 6 (WP): With Pruning-The method is run based on Algorithm 1 except the line 4 is not performed.", "labels": [], "entities": []}, {"text": "\u2022 Method 7 (CNCP): Coreference without labeling condition and pruning-The method is run based on Algorithm 1 but inline 4 short forms are mapped to longer forms of mention without the condition that both the forms should have the same label and pruning is done for all mentions labeled as location, organization, misc and not done for mentions labeled as persons.", "labels": [], "entities": []}, {"text": "\u2022 Method 8 (CCP): Coreference with labeling condition and pruning-The method is run based on Algorithm 1 but inline 4 short forms are mapped to longer forms of mention with the condition that both the forms should have the same label and pruning is done for all mentions labeled as location, organization, misc and not done for mentions labeled as persons.   misc.", "labels": [], "entities": []}, {"text": "Thus, AIDA graph disambiguation performs well for person and organization while local disambiguation performs well for location and misc.", "labels": [], "entities": [{"text": "AIDA graph disambiguation", "start_pos": 6, "end_pos": 31, "type": "TASK", "confidence": 0.5796481470266978}]}, {"text": "The method 5 (NP) maps the mentions with the highest similarity, without considering the prior probability.", "labels": [], "entities": []}, {"text": "When the method 6-with pruning is compared with method 5-no pruning, for Tipster dataset there is an increase inaccuracy for location, organization and misc and decrease for person.", "labels": [], "entities": [{"text": "Tipster dataset", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9137819409370422}]}, {"text": "For IITB dataset there is an increase inaccuracy for location and misc and decrease for person and organization.", "labels": [], "entities": [{"text": "IITB dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7483436763286591}]}, {"text": "So for method 7 (CNCP) and method 8 (CCP) pruning was done for location, organization, misc.", "labels": [], "entities": []}, {"text": "shows the results for various methods for both the datasets.", "labels": [], "entities": []}, {"text": "Comparing the results, Coreference helps increase the accuracy of mapping especially for person, pruning for location while AIDA performs well on organization, for misc pruning increased accuracy on Tipster dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9992949962615967}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9987952709197998}, {"text": "Tipster dataset", "start_pos": 199, "end_pos": 214, "type": "DATASET", "confidence": 0.963856965303421}]}, {"text": "Pruning decreases the accuracy for organization showing that some potential entity that could be mapped is removed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9995550513267517}]}, {"text": "Coreferenece decreases accuracy for location on Tipster dataset while not much effective on IITB dataset because the shorter forms accuracy depends on the longer form it is mapped to.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9992105960845947}, {"text": "Tipster dataset", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9645327627658844}, {"text": "IITB dataset", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.9708260893821716}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9959192872047424}]}, {"text": "For misc every method is equally competitive.", "labels": [], "entities": []}, {"text": "After the modification, the mentions whose longer forms are mapped as NULL are ensured that the shorter forms too are mapped as NULL but in the case of AIDA, the shorter forms were mapped to some other entity in Yago2.", "labels": [], "entities": [{"text": "Yago2", "start_pos": 212, "end_pos": 217, "type": "DATASET", "confidence": 0.9688159227371216}]}, {"text": "For mentions whose longer forms are mapped to the right entity, the shorter forms are mapped to the right entities by both the methods AIDA and CCP.", "labels": [], "entities": [{"text": "AIDA", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.7520424723625183}]}, {"text": "In one of the documents, \"Naomi Foner, who wrote....", "labels": [], "entities": []}, {"text": "Her own experiences made Foner.....\"", "labels": [], "entities": []}, {"text": "AIDA just gives only Naomi Foner Gyllenhaal as candidate entity of mention \"Naomi Foner\", but the mention \"Foner\" doesn't contain Naomi Foner Gyllenhaal as one of its candidate entity.", "labels": [], "entities": [{"text": "AIDA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9645471572875977}]}, {"text": "This might be because of some error in retrieval by AIDA.", "labels": [], "entities": [{"text": "AIDA", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.8760441541671753}]}, {"text": "Coreferencing them ensured that mention \"Foner\" is mapped to the right entity.", "labels": [], "entities": []}, {"text": "If the longer surface form is mapped to a wrong entity, then all the shorter forms too are mapped to the wrong entity.", "labels": [], "entities": []}, {"text": "Thus, the accuracy depends on the mapping of longer forms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995877146720886}]}, {"text": "\"Nicholas Calas a poet and.....", "labels": [], "entities": []}, {"text": "The longer form \"Nicholas Calas\" has no candidate entities and shorter form \"Calas\" has been mapped to the right entity Nicolas Calas.: Results of various methods on IITB dataset coreferencing shorter form \"Calas\" to longer form maps it to NULL.", "labels": [], "entities": [{"text": "IITB dataset coreferencing shorter form", "start_pos": 166, "end_pos": 205, "type": "DATASET", "confidence": 0.8138078451156616}, {"text": "NULL", "start_pos": 240, "end_pos": 244, "type": "DATASET", "confidence": 0.8854324221611023}]}, {"text": "Instead of \"Nicholas\" if it had been \"Nicolas\" it would had mapped to right entity.", "labels": [], "entities": []}, {"text": "The mention was just misspelled.", "labels": [], "entities": []}, {"text": "The coherence graph algorithm of AIDA makes sense.", "labels": [], "entities": [{"text": "AIDA", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.841275155544281}]}, {"text": "Gadi, commander of a battalion....\"", "labels": [], "entities": []}, {"text": "When only similarity is considered it maps to \"Gadi Brumer (Israeli footballer)\" but with coherence, it maps to \"Gadi Eizenkot (Chief of general staff of Israel Defence Forces)\".", "labels": [], "entities": []}, {"text": "All mentions with the same syntax are mapped to the same entity.", "labels": [], "entities": []}, {"text": "Francisco and Washington ...\", here, \"San Francisco\" should be mapped to the teams San Francisco 49ers and \"Washington\" to Washington Redskins but are mapped to places.", "labels": [], "entities": []}, {"text": "These entities occur at the top when sorted with respective to the similarity measure.", "labels": [], "entities": []}, {"text": "If it is known that these mentions represent a team (organization), other candidate entities which are not team (organization) could be pruned by finding the yago types.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Mentions whose entity mappings marked  as NULL and not NULL for Tipster dataset", "labels": [], "entities": [{"text": "Tipster dataset", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.8053925335407257}]}, {"text": " Table 3: Mentions whose entity mappings marked  as NULL and not NULL for IITB dataset", "labels": [], "entities": [{"text": "IITB dataset", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.8652598261833191}]}, {"text": " Table 4: Confusion Matrix for Tipster dataset", "labels": [], "entities": [{"text": "Tipster dataset", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.7389796227216721}]}, {"text": " Table 5: Confusion Matrix for IITB dataset", "labels": [], "entities": [{"text": "IITB dataset", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.7482147812843323}]}, {"text": " Table 6: Percentage of correct mappings for each category by various methods on Tipster dataset", "labels": [], "entities": [{"text": "Tipster dataset", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.9221007227897644}]}, {"text": " Table 7: Percentage of correct mappings for each category by various methods on IITB dataset", "labels": [], "entities": [{"text": "IITB dataset", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.9418037533760071}]}, {"text": " Table 8: Results of various methods on Tipster dataset", "labels": [], "entities": [{"text": "Tipster dataset", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.8360611498355865}]}, {"text": " Table 9: Results of various methods on IITB dataset", "labels": [], "entities": [{"text": "IITB dataset", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.7635708451271057}]}]}