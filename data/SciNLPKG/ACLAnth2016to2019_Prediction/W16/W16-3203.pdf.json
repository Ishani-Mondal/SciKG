{"title": [{"text": "Focused Evaluation for Image Description with Binary Forced-Choice Tasks", "labels": [], "entities": [{"text": "Image Description", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.828432708978653}]}], "abstractContent": [{"text": "Current evaluation metrics for image description maybe too coarse.", "labels": [], "entities": [{"text": "image description", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8872131705284119}]}, {"text": "We therefore propose a series of binary forced-choice tasks that each focus on a different aspect of the captions.", "labels": [], "entities": []}, {"text": "We evaluate a number of different off-the-shelf image description systems.", "labels": [], "entities": []}, {"text": "Our results indicate strengths and shortcomings of both generation and ranking based approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Image description, i.e. the task of automatically associating photographs with sentences that describe what is depicted in them, has been framed in two different ways: as a natural language generation problem (where each system produces novel captions, see e.g.), and as a ranking task (where each system is required to rank the same pool of unseen test captions for each test image, see e.g.).", "labels": [], "entities": [{"text": "Image description", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8111559152603149}, {"text": "natural language generation", "start_pos": 173, "end_pos": 200, "type": "TASK", "confidence": 0.7190883159637451}]}, {"text": "But although the numbers reported in the literature make it seem as though this task is quickly approaching being solved (on the recent MSCOCO challenge, 1 the best models outperformed humans according to some metrics), evaluation remains problematic for both approaches.", "labels": [], "entities": []}, {"text": "Caption generation requires either automated metrics (), most of which have been shown to correlate poorly with human judgments and fail to capture the variety inhuman captions, while human evaluation is subjective (especially when reduced to simple questions such as \"Which is a better caption?\"), expensive, and difficult to replicate.", "labels": [], "entities": [{"text": "Caption generation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9836695492267609}]}, {"text": "Ranking-based evaluation suffers from the 1 http://mscoco.org/dataset/#captions-challenge2015 problem that the pool of candidate captions may, on the one hand, be too small to contain many meaningful and interesting distractors, and may, on the other hand, contain other sentences that are equally valid descriptions of the image.", "labels": [], "entities": []}, {"text": "To illustrate just how much is still to be done in this field, this paper examines a series of binary forced-choice tasks that are each designed to evaluate a particular aspect of image description.", "labels": [], "entities": [{"text": "image description", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.6953462660312653}]}, {"text": "Items in each task consist of one image, paired with one correct and one incorrect caption; the system has to choose the correct caption over the distractor.", "labels": [], "entities": []}, {"text": "These tasks are inspired both by ranking-based evaluations of image description as well as by more recent work on visual question answering (e.g.), but differ from these in that the negatives are far more restricted and focused than in the generic ranking task.", "labels": [], "entities": [{"text": "image description", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7285032272338867}, {"text": "visual question answering", "start_pos": 114, "end_pos": 139, "type": "TASK", "confidence": 0.7308090726534525}]}, {"text": "Since most of our tasks are simple enough that they could be solved by a very simple decision rule, our aim is not to examine whether models could be trained specifically for these tasks.", "labels": [], "entities": []}, {"text": "Instead, we wish to use these tasks to shed light on which aspects of image captions these models actually \"understand\", and how models trained for generation differ from models trained for ranking.", "labels": [], "entities": []}, {"text": "The models we compare consist of a number of simple baselines, as well as some publicly available models that each had close to state-ofthe-art performance on standard tasks when they were published.", "labels": [], "entities": []}, {"text": "More details and discussion can be found in Hodosh (2015).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper, we evaluate image description systems with a series of binary (two-alternative) forced choice tasks.", "labels": [], "entities": [{"text": "image description", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7806394398212433}]}, {"text": "The items in each task consist of one image from the test or development part of the Flickr30K dataset (: The \"switch people\" task with one correct and one incorrect caption, and the system has to choose (i.e. assign a higher score to) the correct caption over the distractor.", "labels": [], "entities": [{"text": "Flickr30K dataset", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.9871534705162048}]}, {"text": "The correct caption is either an original caption or apart of an original caption for the image.", "labels": [], "entities": []}, {"text": "Distractors are shorter phrases that occur in the original caption, complete captions for different images that share some aspect of the correct caption, or are artificially constructed sentences based on the original caption.", "labels": [], "entities": []}, {"text": "While all distractors are constructed around the people or scene mentions in the original caption, each task is designed to focus on a particular aspect of image description.", "labels": [], "entities": []}, {"text": "We focus on scene and people mentions because both occur frequently in Flickr30K.", "labels": [], "entities": [{"text": "Flickr30K", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.9676979780197144}]}, {"text": "Unlike MSCOCO, all images in Flickr30K focus on events and activities involving people or animals.", "labels": [], "entities": [{"text": "MSCOCO", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.881446361541748}, {"text": "Flickr30K", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.9150807857513428}]}, {"text": "Scene terms (\"beach\", \"city\", \"office\", \"street\", \"park\") tend to describe very visual, unlocalized components that can often be identified by the overall layout or other global properties of the image.", "labels": [], "entities": []}, {"text": "At the same time, they restrict what kind of entities and events are likely to occur in the image.", "labels": [], "entities": []}, {"text": "For instance, people do not \"run\" , \"jump\", or \"swim\" in an \"office\".", "labels": [], "entities": []}, {"text": "Hence, models trained and tested on standard caption datasets do not necessarily need to model what \"jumping in an office\" might look like.", "labels": [], "entities": []}, {"text": "We therefore suspect that much of the generic ranking task can be solved by identifying the visual appearance of scene terms.", "labels": [], "entities": [{"text": "generic ranking task", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.9040522972742716}]}, {"text": "Some tasks require the system to choose between two captions that provide similar descriptions of the main actor or the scene.", "labels": [], "entities": []}, {"text": "In others, the distractor is not a full sentence, but consists only: The \"replace scene\" task of the main actor or scene description.", "labels": [], "entities": []}, {"text": "We also evaluate a converse task in which the distractor describes the scene correctly (but everything else in the sentence is wrong), while the correct answer consists only of the NP that describes the scene.", "labels": [], "entities": []}, {"text": "Finally, we consider a task in which the distractor swaps two people mentions, reversing their corresponding semantic roles while keeping the same vocabulary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies of the different models on our tasks", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9969484210014343}]}]}