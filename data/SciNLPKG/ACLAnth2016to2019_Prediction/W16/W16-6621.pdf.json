{"title": [{"text": "Enabling text readability awareness during the micro planning phase of NLG applications 1", "labels": [], "entities": [{"text": "Enabling text readability awareness", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7632102444767952}]}], "abstractContent": [{"text": "Currently, there is alack of text complexity awareness in NLG systems.", "labels": [], "entities": [{"text": "text complexity awareness", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.8048213322957357}]}, {"text": "Much attention has been given to text simplification.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8605685234069824}]}, {"text": "However, based upon results of an experiment, we unveiled that sophisticated readers in fact would rather read more sophisticated text, instead of the simplest text they could get.", "labels": [], "entities": []}, {"text": "Therefore, we propose a technique that considers different readability levels during the micro planning phase of an NLG system.", "labels": [], "entities": []}, {"text": "Our technique considers grammatical and syntactic choices, as well as lexical items, when generating text.", "labels": [], "entities": []}, {"text": "The application uses the domain of descriptive summaries of line graphs as its use case.", "labels": [], "entities": []}, {"text": "The technique proposed uses learning for identifying features of text complexity; a graph search algorithm for efficient aggregation given a target reading level, and a combination of language modeling and word vectors for the creation of a domain-aware synset which allows the creation of disambiguated lexicon that is appropriate to different reading levels.", "labels": [], "entities": []}, {"text": "We found that generating text at different target reading levels is indeed preferred by readers with varying reading abilities.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first time readability awareness is considered in the micro planning phase of NLG systems.", "labels": [], "entities": [{"text": "readability awareness", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.8365194499492645}]}], "introductionContent": [{"text": "Prior work has concentrated on simplifying text in order to make it accessible to people with cognitive disabilities or low literacy levels.", "labels": [], "entities": []}, {"text": "On the other hand, as stated by), most NLG systems generate text for readers with good reading ability.", "labels": [], "entities": []}, {"text": "Our contention, however, is that NLG systems will be much more effective if they can target their output to the preferences of the reader.", "labels": [], "entities": []}, {"text": "This not only enables easy comprehension, but it also makes the experience more enjoyable for them.", "labels": [], "entities": []}, {"text": "Based on that claim, we propose an approach that considers a target reading level in order to decide on the syntactic and grammatical structure of the generated text and to select appropriate lexical items.", "labels": [], "entities": []}, {"text": "Our overall goal is to generate text at a target reading level.", "labels": [], "entities": []}, {"text": "The process of generating text takes in a number of propositions and outputs a set of English sentences which are realizations of these propositions.", "labels": [], "entities": []}, {"text": "Each proposition can be realized in several different ways, e.g., as a single sentence, aggregated with another proposition as an adjective attached to a noun, as a relative clause, or as another noun phrase in a coordination.", "labels": [], "entities": []}, {"text": "In addition, different lexical items can be used to describe a term, and these might also vary in complexity and grade level appropriateness.", "labels": [], "entities": []}, {"text": "The devised approach is applied to the domain of line graph description.", "labels": [], "entities": [{"text": "line graph description", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.6899890899658203}]}, {"text": "Information graphics (non-pictorial images such as line graphs, bar and pie charts) are commonly used by authors in order to convey a message or to make a point regarding the topic being discussed in the document or article.", "labels": [], "entities": []}, {"text": "To efficiently select realizations at a particular reading level, we devised an approach that uses a graph search algorithm guided by a heuristic.", "labels": [], "entities": []}, {"text": "To construct the heuristic, the features of text complexity were identified through machine learning.", "labels": [], "entities": []}, {"text": "The lexical choice implements a concept expansion phase followed by an approach which combines language modeling and word vectors to disambiguate domain-relevant concepts.", "labels": [], "entities": []}, {"text": "In the last step a grade level appropriate lexicon is applied.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first effort made during the micro planning phase of an NLG system that both considers different reading abilities when generating text and presents automated approaches in order to do it.", "labels": [], "entities": []}, {"text": "The next section describes related work on text readability and text simplification.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8088232278823853}]}, {"text": "Sections 3 to 5 discuss the graph search algorithm for the aggregation phase, the learning of feature measurements and the identification of grade level appropriate and domain aware lexicons.", "labels": [], "entities": []}, {"text": "Section 6 shows some examples of summaries generated by the system.", "labels": [], "entities": [{"text": "summaries generated", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.9023313820362091}]}, {"text": "Section 7 presents the evaluations of the system and Sections 8 and 9 provide conclusions and thoughts on future work, respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "Four different evaluations were performed to assess the effectives of the system on generating summaries at different target reading levels.", "labels": [], "entities": []}, {"text": "These evaluations intended to assess: 1) The ability of the system, given a target reading level, to generate a summary that is as close as possible to that target.", "labels": [], "entities": []}, {"text": "For this experiment we used a set of 11 line graphs.", "labels": [], "entities": []}, {"text": "We ran the system five times, generating five slightly different summaries at the reading level identified for the article in which the graphs appeared.", "labels": [], "entities": []}, {"text": "These five summaries differ since, on each iteration of the system, the lexical choice randomly selects lexical items from the pool of appropriate options.", "labels": [], "entities": []}, {"text": "The average grade level was used as the final reading level.", "labels": [], "entities": []}, {"text": "63.6% of the graphs had their summaries produced by the system matching their target reading level exactly.", "labels": [], "entities": []}, {"text": "27.3% of the graphs had their summaries generated by the system really close to the target reading level, having 1 summary produced at grade level 8.8 with a target 9 th -10 th and 2 summaries produced at grade levels 10.7 and 10.9 with a target 11 th -College.", "labels": [], "entities": []}, {"text": "And only one of the graphs had the summary generated at 2 grade levels lower than the target reading level (7.2 with a target of 9 th -10 th ).", "labels": [], "entities": []}, {"text": "2) The ability of the system to generate different summaries appropriate for different grade levels for any given graph in the experiment set.", "labels": [], "entities": []}, {"text": "The system was able to successfully generate summaries for all 11 graphs with increasing complexity as the target reading level increased (this also used the average of five runs).", "labels": [], "entities": []}, {"text": "It generated 11% of the summaries at a reading level that did not change, having generated summaries at 9 th -10 th grade level that targeted the 11 th -college grade group.", "labels": [], "entities": []}, {"text": "This was due to the lack of enough propositions to perform grammatical combinations that would lead to a higher reading level.", "labels": [], "entities": []}, {"text": "3) The ability of the system on varying the text complexity as perceived by human readers.", "labels": [], "entities": []}, {"text": "For this experiment, 90 Human Intelligent Tasks (HITs) were undertaken through Amazon Mechanical Turk (10 graphs, 9 turkers per graph).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 79, "end_pos": 101, "type": "DATASET", "confidence": 0.9215472141901652}]}, {"text": "Each HIT produced an ordering which corresponded to the grade level the turkers believed the summaries belonged to.", "labels": [], "entities": []}, {"text": "Each summary could be associated to only one grade level.", "labels": [], "entities": []}, {"text": "Since choosing one wrong grade level to a summary would lead to another misclassification, a pair-wise relationship approach was applied to analyze the results.", "labels": [], "entities": []}, {"text": "From 348 valid pairwise relationship results, 252 had a correct ordering, yielding a similarity between human readers and the system's perception of text complexity of 72%.", "labels": [], "entities": [{"text": "similarity", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9633603096008301}]}, {"text": "Another evaluation made on the results provided by the turkers was through calculating the average of the nDCGs obtained on the orderings.", "labels": [], "entities": []}, {"text": "Using the formula presented in, the results obtained were the ones presented in.", "labels": [], "entities": []}, {"text": "The results of applying nDCG are higher than the ones gotten from the pairwise relationship approach.", "labels": [], "entities": []}, {"text": "Although the nDCG score is a useful metric for evaluating relevance ranking, it might not be the most appropriate metric for evaluating the results of the task performed with the turkers since it penalizes top ranked results more and we would like to penalize misplaced assigned summary grades according to their distance from the target reading level.", "labels": [], "entities": []}, {"text": "4) The usability of summaries generated at different reading levels for users with different reading skills.", "labels": [], "entities": []}, {"text": "For this evaluation 16 students at the 5 th grade and 34 freshmen college students were recruited.", "labels": [], "entities": []}, {"text": "They received two summaries for each of nine different graphs: one at the 4 th -5 th and the other at the 11 th -college reading level.", "labels": [], "entities": []}, {"text": "They were asked to choose which summary they preferred and why.", "labels": [], "entities": []}, {"text": "Results per grade and per graph are presented in for the 5 th graders and in for college students.", "labels": [], "entities": []}, {"text": "Additionally, they were asked to circle things they did not like in either summary.", "labels": [], "entities": []}, {"text": "From 73 responses collected from the 5 th graders, 57 chose the summaries at their reading level and from the 163 responses collected from the college students, 115 chose the summaries at their reading level.", "labels": [], "entities": []}, {"text": "shows that the results are statistically significant given p = 3.67816E-12 calculated using the chi-squared test.", "labels": [], "entities": []}, {"text": "From these results we conclude that the system is able to successfully generate summaries that match the reading level of the articles on which the line graphs appear and that its perception of text complexity matches that of human readers at a rate of 72%.", "labels": [], "entities": []}, {"text": "In order to assess how good this result is, another possible experiment could contain the same tasks, but compare the results of our system with those obtained from a baseline.", "labels": [], "entities": []}, {"text": "Such baseline currently does not exist.", "labels": [], "entities": []}, {"text": "One possibility could be to provide them with summaries generated using Benetech guidelines for line graph description as they are made available, for example.", "labels": [], "entities": [{"text": "line graph description", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.6131380895773569}]}, {"text": "We also confirmed our initial contention that readers with different reading abilities prefer text that matches their reading skills, instead of always reading the simplest text they can get.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of applying nDCG to orderings provided by  the turkers.", "labels": [], "entities": []}, {"text": " Table 2: Results from reading level experiment  with 5th graders.", "labels": [], "entities": []}, {"text": " Table 3: Results from reading level experiment  with freshmen College students.", "labels": [], "entities": []}, {"text": " Table 4: Statistical significance data.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.7275181412696838}]}]}