{"title": [], "abstractContent": [{"text": "This paper describes our systems for Task 1 of the WMT16 Shared Task on Quality Estimation.", "labels": [], "entities": [{"text": "WMT16 Shared Task on Quality Estimation", "start_pos": 51, "end_pos": 90, "type": "TASK", "confidence": 0.6012925406297048}]}, {"text": "Our submissions use (i) a continuous space language model (CSLM) to extract sentence embeddings and cross-entropy scores, (ii) a neural network machine translation (NMT) model, (iii) a set of QuEst features, and (iv) a combination of features produced by QuEst and with CSLM and NMT.", "labels": [], "entities": []}, {"text": "Our primary submission achieved third place in the scoring task and second place in the ranking task.", "labels": [], "entities": []}, {"text": "Another interesting finding is the good performance obtained from using as features only CSLM sentence em-beddings, which are learned in an unsuper-vised fashion without any additional hand-crafted features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality Estimation (QE) aims at measuring the quality of the output of Machine Translation (MT) systems without reference translations.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6976567447185517}, {"text": "Machine Translation (MT)", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.8519003987312317}]}, {"text": "Generally, QE is addressed with various features indicating fluency, adequacy and complexity of the source and translation texts.", "labels": [], "entities": []}, {"text": "Such features are used along with Machine Learning methods in order to learn prediction models.", "labels": [], "entities": []}, {"text": "Features play a key role in QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9566489458084106}]}, {"text": "A wide range of features from the source segments and their translations, often processed using external resources and tools, have been proposed.", "labels": [], "entities": []}, {"text": "These go from simple, language-independent features, to advanced, linguistically motivated features.", "labels": [], "entities": []}, {"text": "They include features that rely on information from the MT system that generated the translations, and features that are oblivious to the way translations were produced.", "labels": [], "entities": []}, {"text": "This leads to a potential bottleneck: feature engineering can be time consuming, particularly because the impact of features vary across datasets and language pairs.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7917558550834656}]}, {"text": "Also, most features in the literature are extracted from segment pairs in isolation, ignoring contextual clues from other segments in the text.", "labels": [], "entities": []}, {"text": "The focus of our contributions this year is to explore anew set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks.", "labels": [], "entities": []}, {"text": "Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information.", "labels": [], "entities": []}, {"text": "For instance, neural network language models () have been successfully explored in many problems including Automatic Speech Recognition ( and Machine Translation.", "labels": [], "entities": [{"text": "Automatic Speech Recognition", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.7014391918977102}, {"text": "Machine Translation", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.8489161729812622}]}, {"text": "In this paper, we extend our previous work) to investigate the use of sentence embeddings extracted from a neural network language model along with cross entropy scores as features for QE.", "labels": [], "entities": []}, {"text": "We also investigate the use of a neural machine translation model to extract the log likelihood of sentences as QE features.", "labels": [], "entities": []}, {"text": "The features extracted from such resources are used in isolation or combined with hand-crafted features from QuEst to learn prediction models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In what follows we present our experiments on the WMT16 QE Task 1 with CSLM and NMT features.", "labels": [], "entities": [{"text": "WMT16 QE Task 1", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.8558445423841476}]}, {"text": "Task 1's English-German dataset consists respectively of a training set and development set with 12, 000 and 1, 000 source segments, their machine translations, the post-editions of the latter, and the edit distance scores between the MT and its postedited version (HTER).", "labels": [], "entities": []}, {"text": "The test set consists of 2, 000 English-German source-MT pairs.", "labels": [], "entities": []}, {"text": "Each of the translations was post-edited by professional translators, and HTER labels were computed using the TER tool (settings: tokenised, case insensitive, exact matching only, with scores capped to 1).", "labels": [], "entities": [{"text": "HTER labels", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.9612235724925995}, {"text": "TER", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9847099184989929}]}], "tableCaptions": [{"text": " Table 1, reports detailed statistics on the mono- lingual data used to train the back-off LM and  CSLM. The training dataset consists of WMT16  translation task monolingual corpora with the  Moore-Lewis data selection method (Moore and  Lewis, 2010) to select the CSLM training data  with respect to the task's development set. The", "labels": [], "entities": [{"text": "WMT16  translation task monolingual", "start_pos": 138, "end_pos": 173, "type": "TASK", "confidence": 0.5564588084816933}]}, {"text": " Table 3: Results on the development set of Task 1. Systems in bold are used as official submissions.", "labels": [], "entities": []}, {"text": " Table 4: Official results on the test set of Task 1. The superscript shows the overall ranking of the system  against various official evaluation metrics.", "labels": [], "entities": []}]}