{"title": [{"text": "Local-Global Vectors to Improve Unigram Terminology Extraction", "labels": [], "entities": [{"text": "Improve Unigram Terminology Extraction", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.7389797270298004}]}], "abstractContent": [{"text": "The present paper explores a novel method that integrates efficient distributed representations with terminology extraction.", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.8641610741615295}]}, {"text": "We show that the information from a small number of observed instances can be combined with local and global word embeddings to remarkably improve the term extraction results on unigram terms.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 151, "end_pos": 166, "type": "TASK", "confidence": 0.6657540053129196}]}, {"text": "To do so, we pass the terms extracted by other tools to a filter made of the local-global embeddings and a classifier which in turn decides whether or not a term candidate is a term.", "labels": [], "entities": []}, {"text": "The filter can also be used as a hub to merge different term extraction tools into a single higher-performing system.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7298249304294586}]}, {"text": "We compare filters that use the skip-gram architecture and filters that employ the CBOW architecture for the task at hand.", "labels": [], "entities": []}], "introductionContent": [{"text": "The terminology of a domain encodes the existing knowledge in that domain.", "labels": [], "entities": []}, {"text": "Hence understanding and interpreting a message belonging to a domain cannot be fully achieved without knowing its terminology.", "labels": [], "entities": [{"text": "interpreting a message belonging to a domain", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.7784212742533002}]}, {"text": "This makes Automatic Terminology Extraction (ATE) an important task in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Automatic Terminology Extraction (ATE)", "start_pos": 11, "end_pos": 49, "type": "TASK", "confidence": 0.7908283720413843}, {"text": "Natural Language Processing (NLP)", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.6953283449014028}]}, {"text": "ATE methods have been conventionally classified as linguistic, statistical, and hybrid.", "labels": [], "entities": [{"text": "ATE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8123477697372437}]}, {"text": "Linguistic methods implement formal rules to detect terms; statistical methods exploit some measures based on relative frequency of terms in general and target corpora by means of which they can tell apart a term from a word in its generic sense; and, the hybrid methods combine the advantages of both of these techniques ().", "labels": [], "entities": []}, {"text": "These methods often regard words in a document as atomic elements; that is, they are manifested as their symbolic alphabetical form in the algorithm (such as in 'a' below) and/or as some measure of their relative frequency (as in 'b' below).", "labels": [], "entities": []}, {"text": "But, in a distributed approach (as in 'c' below) each word has tensor hundreds of realvalued components, as opposed to a single linguistic form or a termhood score . The idea is that such finer-granularity may grant more access to the information that a word contains, potentially resulting in a better detection of terms in a document.", "labels": [], "entities": []}, {"text": "where n is often between 50 and 1000 for different types of word embeddings, almost similar to other vector space models such as Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA)", "start_pos": 129, "end_pos": 159, "type": "TASK", "confidence": 0.7723343670368195}]}, {"text": "In addition to the richer representation, the rise of distributed methods in NLP, especially the recent word embeddings surge, makes it relevant to explore the ways current model architectures may fit into the This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": []}, {"text": "Licence details: http:// creativecommons.org/licenses/by/4.0/ 1 Termhood is the degree of a linguistic unit being related to a domain-specific concept automatic terminology extraction picture.", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 161, "end_pos": 183, "type": "TASK", "confidence": 0.635019987821579}]}, {"text": "However, the fact that makes them particularly appealing is their computational efficiency and scalability as compared to the available alternatives, including LSA and LDA).", "labels": [], "entities": []}, {"text": "We present a simple method that harnesses the rich distributed representation acquired by a log-bilinear regression model called, as well as the efficiency of log-linear models with CBOW 2 and skip-gram architectures (, which is a step forward towards language-independent ATE.", "labels": [], "entities": []}, {"text": "In our method, the GloVe model is used to preserve the global 3 scope of a word in a general corpus (i.e., its general sense(s)) and the CBOW or the skip-gram model is used to capture the local scope fora word in a technical corpus (i.e., its technical usage).", "labels": [], "entities": []}, {"text": "Currently, we use our method as a filter on top of a previously-developed hybrid term extraction algorithm, namely, TermoStat) along with two simpler methods (refer to section 4 for further details) and focus on the unigram 4 term extraction.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.7481404840946198}, {"text": "unigram 4 term extraction", "start_pos": 216, "end_pos": 241, "type": "TASK", "confidence": 0.620483286678791}]}, {"text": "TermoStat has been previously tested on mathematics domain where it performed well on the extraction of multi-word expressions, but lower on unigram terms, hence the present work is an attempt to improve unigram term extraction for the same domain.", "labels": [], "entities": [{"text": "unigram term extraction", "start_pos": 204, "end_pos": 227, "type": "TASK", "confidence": 0.7016889850298563}]}, {"text": "As mentioned, the target domain of the present study is mathematics textbooks.", "labels": [], "entities": []}, {"text": "A significant component of any academic subject is its terminology.", "labels": [], "entities": []}, {"text": "Knowledge of the terminology of afield enables students to engage with their discipline more effectively by enhancing their ability to understand the related academic texts and lectures, and allowing them to use the subject-specific terminology in their discussions, presentations, and assignments.", "labels": [], "entities": []}, {"text": "Therefore, generating lists of terms specific to various fields of study is a significant endeavor.", "labels": [], "entities": []}, {"text": "However, these lists have often been generated manually or through corpus-based studies, which are time consuming, labor-intensive, and prone to human error.", "labels": [], "entities": []}, {"text": "This can be facilitated by a great extent with high-performance automatic term extraction.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.7286333590745926}]}, {"text": "To the best of our knowledge, the present method is the first to successfully apply neural network word embeddings to the terminology extraction task.", "labels": [], "entities": [{"text": "terminology extraction task", "start_pos": 122, "end_pos": 149, "type": "TASK", "confidence": 0.9109249711036682}]}, {"text": "This method can be combined with any term extraction algorithm for any non-polysynthetic 5 language and any domain.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.7080698311328888}]}], "datasetContent": [{"text": "First, we aim to find the best-performing classifier(s), out of the ones tested, to be used in our system for each of the two architectures (CBOW and skip grams).", "labels": [], "entities": []}, {"text": "We noticed that three classifiers, namely, SMO, logistic regression, and the multi-layer perceptron consistently outperformed the rest of the classifiers we examined (the full list is provided in section 4).", "labels": [], "entities": [{"text": "SMO", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.7081729769706726}]}, {"text": "JRip performed well, but its performance was consistently lower than the above-mentioned three classifiers.", "labels": [], "entities": [{"text": "JRip", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8268185257911682}]}, {"text": "It should be noted, however, that we also have a greater dimension size for our vectors than, that is, 150 versus 19.", "labels": [], "entities": []}, {"text": "Also the nature of the vectors is different in that they used feature vectors but we used embeddings.", "labels": [], "entities": []}, {"text": "Nevertheless, we only show the results for these three classifiers.", "labels": [], "entities": []}, {"text": "depicts the classifiers' performance with local-global vectors (LGVs) where the local vectors are trained with the CBOW architecture, and depicts the classifiers' performance with local-global vectors where the local vectors are trained with the skip-gram architecture.", "labels": [], "entities": []}, {"text": "The classifiers' performance is presented as a function of the number of observed instances (the amount of training data used), and the classifiers are tested on the rest of the instances (954 minus the number of observed instances).", "labels": [], "entities": []}, {"text": "Instances are chosen randomly for training with a positive/negative ratio proportional to the dataset (i.e., 1/2 respectively).", "labels": [], "entities": []}, {"text": "All of the instances in the entire dataset are unique candidate terms.", "labels": [], "entities": []}, {"text": "The performance is measured by F-measure in the figures.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9987490177154541}]}, {"text": "We compute only relative recall throughout the experiments at this stage.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9761808514595032}]}, {"text": "As shown in the CBOW architecture with the logistic regression classifier generalizes really well with as little data as only 9 instances.", "labels": [], "entities": []}, {"text": "However, as soon as we add more instances, the multi-layer perceptron and SMO take the lead, outperforming one another in the process.", "labels": [], "entities": [{"text": "SMO", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.5742459893226624}]}, {"text": "However, the logistic regression classifier shows less improvement when subjected to more training data.", "labels": [], "entities": []}, {"text": "Overall, we did not notice any considerable difference between the skip-gram and the CBOW architectures across the classifiers used for the purpose of unigram term extraction.", "labels": [], "entities": [{"text": "unigram term extraction", "start_pos": 151, "end_pos": 174, "type": "TASK", "confidence": 0.7244811256726583}]}, {"text": "In practice, we prefer to show the system as little data as possible since extracting a few high-precision terms is relatively easy in real-world ATE; hence, we choose the local CBOW architecture with logistic The numbers shown on the X axes of (i.e., are the results of splitting training and test data such that the training data is approximately 1%, 5%, 10%, 20%, and 50% of the entire dataset respectively.", "labels": [], "entities": [{"text": "ATE", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.6787671446800232}]}, {"text": "The most notable improvement is when we increase the training set from 9 to 47 and that is only 4% variation in the size of the test set but 10% improvement of performance on average for LGV's with local CBOW) and an average of 13% improvement of performance for LGV's with local skip-gram.", "labels": [], "entities": []}, {"text": "The reason for resorting to relative recall is that having annotators go through the entire corpus to compute recall is timeconsuming and labor-intensive at this phase of the project.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.912009596824646}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9663448929786682}]}, {"text": "regression classifier (trained on only 9 instances) as one configuration (our quickest learner), and the local skip-gram architecture coupled with multi-layer perceptron as the other configuration of our system (performs best among those trained on up to 47 instances) for the next experiment.", "labels": [], "entities": []}, {"text": "We compare these two system configurations with a baseline and the initial term extraction tools, all tested on 907 (i.e., 954 -47) remaining instances that are unseen to all of the systems under experiment.", "labels": [], "entities": []}, {"text": "compares the results of our system in unigram term extraction with individual term extraction tools and a frequency baseline that uses a stop-word filter (refer to section 4 for further details on the tools and the baseline).", "labels": [], "entities": [{"text": "unigram term extraction", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7000126043955485}]}, {"text": "The results show that both of our system configurations achieve a substantial improvement over the other tools.", "labels": [], "entities": []}], "tableCaptions": []}