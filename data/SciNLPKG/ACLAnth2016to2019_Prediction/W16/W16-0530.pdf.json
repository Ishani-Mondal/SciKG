{"title": [{"text": "Candidate re-ranking for SMT-based grammatical error correction", "labels": [], "entities": [{"text": "SMT-based grammatical error correction", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.9176240712404251}]}], "abstractContent": [{"text": "We develop a supervised ranking model to re-rank candidates generated from an SMT-based grammatical error correction (GEC) system.", "labels": [], "entities": [{"text": "SMT-based grammatical error correction (GEC)", "start_pos": 78, "end_pos": 122, "type": "TASK", "confidence": 0.8493457521711077}]}, {"text": "A range of novel features with respect to GEC are investigated and implemented in our re-ranker.", "labels": [], "entities": [{"text": "GEC", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.4937969744205475}]}, {"text": "We train a rank preference SVM model and demonstrate that this outperforms both Minimum Bayes-Risk and Multi-Engine Machine Translation based re-ranking for the GEC task.", "labels": [], "entities": [{"text": "Multi-Engine Machine Translation", "start_pos": 103, "end_pos": 135, "type": "TASK", "confidence": 0.5814916690190634}, {"text": "GEC task", "start_pos": 161, "end_pos": 169, "type": "TASK", "confidence": 0.5127535611391068}]}, {"text": "Our best system yields a significant improvement in I-measure when testing on the publicly available FCE test set (from 2.87% to 9.78%).", "labels": [], "entities": [{"text": "I-measure", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.715071439743042}, {"text": "FCE test set", "start_pos": 101, "end_pos": 113, "type": "DATASET", "confidence": 0.9552126924196879}]}, {"text": "It also achieves an F 0.5 score of 38.08% on the CoNLL-2014 shared task test set, which is higher than the best original result.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9929573138554891}, {"text": "CoNLL-2014 shared task test set", "start_pos": 49, "end_pos": 80, "type": "DATASET", "confidence": 0.8351566672325135}]}, {"text": "The oracle score (upper bound) for the re-ranker achieves over 40% I-measure performance , demonstrating that there is considerable room for improvement in the re-ranking component developed here, such as incorporating features able to capture long-distance dependencies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical error correction (GEC) has attracted considerable interest in recent years.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8564905722935995}]}, {"text": "Unlike classifiers built for specific error types (e.g. determiner or preposition errors), statistical machine translation (SMT) systems are trained to deal with all error types simultaneously.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 91, "end_pos": 128, "type": "TASK", "confidence": 0.8072329312562943}]}, {"text": "An SMT system thus learns to translate incorrect English into correct English using a parallel corpus of corrected sentences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9903151392936707}]}, {"text": "The SMT framework has been successfully used for GEC, as demonstrated by the top-performing systems in the CoNLL-2014 shared task ( ).", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9791750311851501}, {"text": "GEC", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.7350195646286011}]}, {"text": "However, the best candidate produced by an SMT system is not always the best correction.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9929813146591187}]}, {"text": "An example is given in.", "labels": [], "entities": []}, {"text": "Since SMT was not originally designed for GEC, many standard features do not perform well on this task.", "labels": [], "entities": [{"text": "SMT", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9917045831680298}, {"text": "GEC", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.827928900718689}]}, {"text": "It is necessary to add new local and global features to help the decoder distinguish good from bad corrections.", "labels": [], "entities": []}, {"text": "used Levenshtein distance to limit the changes made by their SMT system, given that most words translate into themselves and errors are often similar to their correct forms.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 5, "end_pos": 25, "type": "METRIC", "confidence": 0.6336103677749634}, {"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9899875521659851}]}, {"text": "Junczys-Dowmunt and Grundkiewicz (2014) also augmented their SMT system with Levenshtein distance and other sparse features that were extracted from edit operations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9915621876716614}, {"text": "Levenshtein distance", "start_pos": 77, "end_pos": 97, "type": "METRIC", "confidence": 0.7151500880718231}]}, {"text": "However, the integration of additional models/features into the decoding process may affect the dynamic programming algorithm used in SMT, because it does not support some complex features, such as those computed from an n-best list.", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9934751391410828}]}, {"text": "An alternative to performing integrated decoding is to use additional information to re-rank an SMT decoder's output.", "labels": [], "entities": [{"text": "SMT decoder's output", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.8894055336713791}]}, {"text": "The aim of n-best list re-ranking is to re-rank the translation candidates produced by the SMT system using a rich set of features that are not used by the SMT decoder, so that better candidates can be selected as 'optimal' translations.", "labels": [], "entities": [{"text": "n-best list re-ranking", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.590075691541036}, {"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9749566316604614}]}, {"text": "This has several advantages: 1) it allows the introduction of new features that are tailored for GEC; 2) unlike in SMT, we can use various types of features without worrying about fine-grained smoothing issues and it is easier to use global features; 3) re-ranking is easy to implement, and the existing decoder does not need to be modified; and 4) the decoding process in SMT Source There are some informations you have asked me about.", "labels": [], "entities": [{"text": "SMT", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9599188566207886}, {"text": "SMT", "start_pos": 373, "end_pos": 376, "type": "TASK", "confidence": 0.8992959260940552}]}, {"text": "Reference There is some information you have asked me about.", "labels": [], "entities": []}, {"text": "10 best list 1st: There are some information you have asked me about.", "labels": [], "entities": []}, {"text": "2nd: There is some information you have asked me about.", "labels": [], "entities": []}, {"text": "3rd: There are some information you asked me about.", "labels": [], "entities": []}, {"text": "4th: There are some information you have asked me.", "labels": [], "entities": []}, {"text": "5th: There are some information you have asked me for.", "labels": [], "entities": []}, {"text": "6th: There are some information you have asked me about it.", "labels": [], "entities": []}, {"text": "7th: There is some information you asked me about.", "labels": [], "entities": []}, {"text": "8th: There are some information you asked me for.", "labels": [], "entities": []}, {"text": "9th: There were some information you have asked me about.", "labels": [], "entities": []}, {"text": "10th: There is some information you have asked me.: In this example, there are two errors in the sentence (marked in bold): an agreement error (are \u2192 is) and amass noun error (informations \u2192 information).", "labels": [], "entities": []}, {"text": "The best output is the one with highest probability, which only corrects the mass noun error, but misses the agreement error.", "labels": [], "entities": []}, {"text": "However, the 2nd-ranked candidate corrects both errors and matches the reference (marked in italics).", "labels": [], "entities": []}, {"text": "The source sentence and error annotation are taken from the FCE dataset, and the 10-best list is from an SMT system trained on the whole CLC.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.9888435304164886}, {"text": "SMT", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.9680049419403076}]}, {"text": "More details about the datasets and system are presented in Section 3.", "labels": [], "entities": []}, {"text": "only needs to be performed once, which allows for fast experimentation.", "labels": [], "entities": []}, {"text": "Most previous work on GEC has used evaluation methods based on precision (P), recall (R), and Fscore (e.g. the CoNLL 2013 and 2014 shared tasks).", "labels": [], "entities": [{"text": "GEC", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.6539409756660461}, {"text": "precision (P)", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.9368161708116531}, {"text": "recall (R)", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9576151818037033}, {"text": "Fscore", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9995682835578918}, {"text": "CoNLL 2013", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.8754703104496002}]}, {"text": "However, they do not provide an indicator of improvement on the original text so there is noway to compare GEC systems with a 'do-nothing' baseline.", "labels": [], "entities": [{"text": "GEC", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.7659929394721985}]}, {"text": "Since the aim of GEC is to improve text quality, we use the Improvement (I) score calculated by the Imeasure, which tells us whether a system improves the input.", "labels": [], "entities": [{"text": "Improvement (I) score", "start_pos": 60, "end_pos": 81, "type": "METRIC", "confidence": 0.9789610505104065}, {"text": "Imeasure", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8916586637496948}]}, {"text": "The main contributions of our work are as follows.", "labels": [], "entities": []}, {"text": "First, to the best of our knowledge, we are the first to use a supervised discriminative re-ranking model in SMT for GEC, showing that n-best list re-ranking can be used to improve sentence quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.989327073097229}]}, {"text": "Second, we propose and investigate a range of easily computed features for GEC re-ranking.", "labels": [], "entities": [{"text": "GEC re-ranking", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.5433111190795898}]}, {"text": "Finally, we report results on two well-known publicly available test sets that can be used for cross-system comparisons.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the publicly available FCE dataset), which is apart of the CLC.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.9599015414714813}, {"text": "CLC", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9469995498657227}]}, {"text": "The FCE dataset is a set of 1,244 scripts written by learners of English taking the First Certificate in English (FCE) examination around the world between.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9559334516525269}, {"text": "First Certificate in English (FCE) examination", "start_pos": 84, "end_pos": 130, "type": "TASK", "confidence": 0.4641701988875866}]}, {"text": "The texts have been manually error-annotated with a taxonomy of approximately 80 error types.", "labels": [], "entities": []}, {"text": "The FCE dataset covers a wide variety of L1s and was used in the HOO-2012 error correction shared task ().", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9298316538333893}, {"text": "HOO-2012 error correction shared task", "start_pos": 65, "end_pos": 102, "type": "TASK", "confidence": 0.7414938032627105}]}, {"text": "Compared to the National University of Singapore Corpus of Learner English (NU-CLE)) used in the CoNLL 2013 and 2014 shared tasks, which contains essays written by students at the National University of Singapore, the FCE dataset is a more representative test set of learner writing, which is why we use it for our experiments.", "labels": [], "entities": [{"text": "National University of Singapore Corpus of Learner English (NU-CLE))", "start_pos": 16, "end_pos": 84, "type": "DATASET", "confidence": 0.7868783094666221}, {"text": "CoNLL 2013 and 2014 shared tasks", "start_pos": 97, "end_pos": 129, "type": "DATASET", "confidence": 0.7896474699179331}, {"text": "FCE dataset", "start_pos": 218, "end_pos": 229, "type": "DATASET", "confidence": 0.9465375244617462}]}, {"text": "The performance of our model on the CoNLL-2014 shared task test data is also presented in Section 3.7.", "labels": [], "entities": [{"text": "CoNLL-2014 shared task test data", "start_pos": 36, "end_pos": 68, "type": "DATASET", "confidence": 0.8713833570480347}]}, {"text": "To overcome this problem, JunczysDowmunt and Grundkiewicz (2014) introduced examples collected from the language exchange social networking website Lang-8, and were able to improve system performance by 6 F-score points.", "labels": [], "entities": [{"text": "F-score", "start_pos": 205, "end_pos": 212, "type": "METRIC", "confidence": 0.9971428513526917}]}, {"text": "As noticed by them, Lang-8 data maybe too noisy and error-prone, so we decided to add examples from the fully annotated learner corpus CLC to our training set (approx. 1,965,727 pairs of parallel sentences and 29,219,128 tokens on the target side).", "labels": [], "entities": [{"text": "learner corpus CLC", "start_pos": 120, "end_pos": 138, "type": "DATASET", "confidence": 0.74022709329923}]}, {"text": "Segmentation and tokenisation are performed using RASP (), which is expected to perform better on learner data than a system developed exclusively from high quality copy-edited text such as the Wall Street Journal.", "labels": [], "entities": [{"text": "tokenisation", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.9629570841789246}, {"text": "Wall Street Journal", "start_pos": 194, "end_pos": 213, "type": "DATASET", "confidence": 0.9415222605069479}]}, {"text": "System performance is evaluated using the Imeasure proposed by, which is designed to address problems with previous evaluation methods and reflect any improvement on the original sentence after applying a system's corrections.", "labels": [], "entities": [{"text": "Imeasure", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.729759931564331}]}, {"text": "The M 2 Scorer was the official scorer in the CoNLL 2013 and 2014 shared tasks, with the latter using F 0.5 as the system ranking metric.", "labels": [], "entities": [{"text": "M 2 Scorer", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.7292529741923014}, {"text": "CoNLL 2013", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.8970666527748108}, {"text": "F 0.5", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.981128066778183}]}, {"text": "GLEU is a simple variant of BLEU (), which shows better correlation with human judgments on the CoNLL-2014 shared task test set.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9320299625396729}, {"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9978783130645752}, {"text": "CoNLL-2014 shared task test set", "start_pos": 96, "end_pos": 127, "type": "DATASET", "confidence": 0.8997053742408753}]}], "tableCaptions": [{"text": " Table 2: SMT system performance on the FCE test set (in percentages). The best results are marked in bold.", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9892219305038452}, {"text": "FCE test set", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.9759148557980856}]}, {"text": " Table 3: Results of 10-best list re-ranking on the FCE test set", "labels": [], "entities": [{"text": "FCE test set", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9801140824953715}]}, {"text": " Table 5: Performance of SMT best, SVM re-ranker, oracle", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9857776761054993}]}, {"text": " Table 6: System performance on the CoNLL-2014 test set with-", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.9832814335823059}]}]}