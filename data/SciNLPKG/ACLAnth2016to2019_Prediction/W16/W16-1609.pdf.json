{"title": [{"text": "An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Recently, Le and Mikolov (2014) proposed doc2vec as an extension to word2vec (Mikolov et al., 2013a) to learn document-level embeddings.", "labels": [], "entities": []}, {"text": "Despite promising results in the original paper , others have struggled to reproduce those results.", "labels": [], "entities": []}, {"text": "This paper presents a rigorous empirical evaluation of doc2vec over two tasks.", "labels": [], "entities": []}, {"text": "We compare doc2vec to two baselines and two state-of-the-art document embedding methodologies.", "labels": [], "entities": []}, {"text": "We found that doc2vec performs robustly when using models trained on large external corpora, and can be further improved by using pre-trained word embed-dings.", "labels": [], "entities": []}, {"text": "We also provide recommendations on hyper-parameter settings for general-purpose applications, and release source code to induce document embeddings using our trained doc2vec models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural embeddings were first proposed by, in the form of a feed-forward neural network language model.", "labels": [], "entities": []}, {"text": "Modern methods use a simpler and more efficient neural architecture to learn word vectors (word2vec:; GloVe:), based on objective functions that are designed specifically to produce high-quality vectors.", "labels": [], "entities": []}, {"text": "Neural embeddings learnt by these methods have been applied in a myriad of NLP applications, including initialising neural network models for objective visual recognition) or machine translation ( ), as well as directly modelling word-to-word relationships (; Vylomova et al., to appear), Paragraph vectors, or doc2vec, were proposed by as a simple extension to word2vec to extend the learning of embeddings from words to word sequences.", "labels": [], "entities": [{"text": "objective visual recognition", "start_pos": 142, "end_pos": 170, "type": "TASK", "confidence": 0.7633684873580933}, {"text": "machine translation", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.7877829968929291}]}, {"text": "1 doc2vec is agnostic to the granularity of the word sequence -it can equally be a word n-gram, sentence, paragraph or document.", "labels": [], "entities": []}, {"text": "In this paper, we use the term \"document embedding\" to refer to the embedding of a word sequence, irrespective of its granularity.", "labels": [], "entities": []}, {"text": "doc2vec was proposed in two forms: dbow and dmpv.", "labels": [], "entities": []}, {"text": "dbow is a simpler model and ignores word order, while dmpv is a more complex model with more parameters (see Section 2 for details).", "labels": [], "entities": []}, {"text": "Although found that as a standalone method dmpv is a better model, others have reported contradictory results.", "labels": [], "entities": []}, {"text": "2 doc2vec has also been reported to produce sub-par performance compared to vector averaging methods based on informal experiments.", "labels": [], "entities": []}, {"text": "Additionally, while report state-of-theart results over a sentiment analysis task using doc2vec, others (including the second author of the original paper in follow-up work) have struggled to replicate this result.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.9339317679405212}]}, {"text": "To this end, we present a formal and rigorous evaluation of doc2vec over two extrinsic tasks.", "labels": [], "entities": []}, {"text": "Our findings reveal that dbow, despite being the simpler model, is superior to dmpv.", "labels": [], "entities": []}, {"text": "When trained overlarge external corpora, with pre-trained word embeddings and hyper-parameter tuning, we find that doc2vec performs very strongly compared to both a simple word embedding averaging and n-gram baseline, as well as two state-of-the-art document embedding approaches, and that doc2vec performs particularly strongly over longer documents.", "labels": [], "entities": []}, {"text": "We additionally release source code for replicating our experiments, and for inducing document embeddings using our trained models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate doc2vec in two task settings, specifically chosen to highlight the impact of document length on model performance.", "labels": [], "entities": []}, {"text": "For all tasks, we split the dataset into 2 partitions: development and test.", "labels": [], "entities": []}, {"text": "The development set is used to optimise the hyper-parameters of doc2vec, and results are reported on the test set.", "labels": [], "entities": []}, {"text": "We use all documents in the development and test set (and potentially more background documents, where explicitly mentioned) to train doc2vec.", "labels": [], "entities": []}, {"text": "Our rationale for this is that the doc2vec training is completely unsupervised, i.e. the model takes only raw text and uses no supervised or annotated information, and thus there is no need to holdout the test data, as it is unlabelled.", "labels": [], "entities": []}, {"text": "We ultimately relax this assumption in the next section (Section 4), when we train doc2vec using large external corpora.", "labels": [], "entities": []}, {"text": "After training doc2vec, document embeddings are generated by the model.", "labels": [], "entities": []}, {"text": "For the word2vec baseline, we compute a document embedding by taking the component-wise mean of its component word embeddings.", "labels": [], "entities": []}, {"text": "We experiment with both variants of doc2vec (dbow and dmpv) and word2vec (skip-gram and cbow) for all tasks.", "labels": [], "entities": []}, {"text": "In addition to word2vec, we experiment with another baseline model that converts a document into a distribution over words via maximum likelihood estimation, and compute pairwise document similarity using the Jensen Shannon divergence.", "labels": [], "entities": []}, {"text": "For word types we explore n-grams of order n = {1, 2, 3, 4} and find that a combination of unigrams, bigrams and trigrams achieves the best results.", "labels": [], "entities": []}, {"text": "Henceforth, this second baseline will be referred to as ngram.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROC AUC scores for each subforum.  Boldface indicates the best score in each row.", "labels": [], "entities": [{"text": "ROC AUC", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.39799946546554565}]}, {"text": " Table 2: Pearson's r of the STS task across 5 do- mains. DLS is the overall best system in the com- petition. Boldface indicates the best results be- tween doc2vec and word2vec in each row.", "labels": [], "entities": [{"text": "STS task", "start_pos": 29, "end_pos": 37, "type": "TASK", "confidence": 0.6433902084827423}]}, {"text": " Table 4: Optimal doc2vec hyper-parameter values used for each tasks. \"Training size\" is the total word  count in the training data. For Q-Dup training size is an average word count across all subforums.", "labels": [], "entities": [{"text": "Training size\"", "start_pos": 71, "end_pos": 85, "type": "METRIC", "confidence": 0.8575924634933472}]}, {"text": " Table 5: Results over all two tasks using models trained with external corpora.", "labels": [], "entities": []}, {"text": " Table 6: Comparison of dbow performance using  pre-trained WIKI and AP-NEWS skip-gram em- beddings.", "labels": [], "entities": [{"text": "WIKI", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.9001618027687073}]}]}