{"title": [{"text": "Policy Networks with Two-Stage Training for Dialogue Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose to use deep policy networks which are trained with an advantage actor-critic method for statistically optimised dialogue systems.", "labels": [], "entities": []}, {"text": "First, we show that, on summary state and action spaces, deep Reinforcement Learning (RL) outperforms Gaussian Processes methods.", "labels": [], "entities": []}, {"text": "Summary state and action spaces lead to good performance but require pre-engineering effort, RL knowledge , and domain expertise.", "labels": [], "entities": []}, {"text": "In order to remove the need to define such summary spaces, we show that deep RL can also be trained efficiently on the original state and action spaces.", "labels": [], "entities": []}, {"text": "Dialogue systems based on partially observable Markov decision processes are known to require many dialogues to train, which makes them unappealing for practical deployment.", "labels": [], "entities": []}, {"text": "We show that a deep RL method based on an actor-critic architecture can exploit a small amount of data very efficiently.", "labels": [], "entities": []}, {"text": "Indeed, with only a few hundred dialogues collected with a handcrafted policy, the actor-critic deep learner is considerably boot-strapped from a combination of supervised and batch RL.", "labels": [], "entities": []}, {"text": "In addition, convergence to an optimal policy is significantly sped up compared to other deep RL methods initialized on the data with batch RL.", "labels": [], "entities": []}, {"text": "All experiments are performed on a restaurant domain derived from the Dialogue State Tracking Challenge 2 (DSTC2) dataset.", "labels": [], "entities": [{"text": "Dialogue State Tracking Challenge 2 (DSTC2) dataset", "start_pos": 70, "end_pos": 121, "type": "DATASET", "confidence": 0.6364494926399655}]}], "introductionContent": [{"text": "The statistical optimization of dialogue management in dialogue systems through Reinforcement Learning (RL) has been an active thread of research for more than two decades (;.", "labels": [], "entities": [{"text": "statistical optimization of dialogue management", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.6543830692768097}, {"text": "Reinforcement Learning (RL)", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.7242609143257142}]}, {"text": "Dialogue management has been successfully modelled as a Partially Observable Markov Decision Process (POMDP), which leads to systems that can learn from data and which are robust to noise.", "labels": [], "entities": [{"text": "Dialogue management", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8217500448226929}]}, {"text": "In this context, a dialogue between a user and a dialogue system is framed as a sequential process where, at each turn, the system has to act based on what it has understood so far of the user's utterances.", "labels": [], "entities": []}, {"text": "Unfortunately, POMDP-based dialogue managers have been unfit for online deployment because they typically require several thousands of dialogues for training.", "labels": [], "entities": [{"text": "POMDP-based dialogue managers", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.7528319160143534}]}, {"text": "Nevertheless, recent work has shown that it is possible to train a POMDP-based dialogue system on just a few hundred dialogues corresponding to online interactions with users).", "labels": [], "entities": []}, {"text": "However, in order to do so, pre-engineering efforts, prior RL knowledge, and domain expertise must be applied.", "labels": [], "entities": [{"text": "RL", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9630250930786133}]}, {"text": "Indeed, summary state and action spaces must be used and the set of actions must be restricted depending on the current state so that notoriously bad actions are prohibited.", "labels": [], "entities": []}, {"text": "In order to alleviate the need fora summary state space, deep RL ( has recently been applied to dialogue management () in the context of negotiations.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7510090172290802}]}, {"text": "It was shown that deep RL performed significantly better than other heuristic or supervised approaches.", "labels": [], "entities": [{"text": "RL", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.6882732510566711}]}, {"text": "The authors performed learning over a large action space of 70 actions and they also had to use restricted action sets in order to learn efficiently over this space.", "labels": [], "entities": []}, {"text": "Besides, deep RL was not compared to other RL methods, which we do in this paper.", "labels": [], "entities": []}, {"text": "In, a simplistic implementation of deep Q Networks is presented, again with no comparison to other RL methods.", "labels": [], "entities": []}, {"text": "In this paper, we propose to efficiently alleviate the need for summary spaces and restricted actions using deep RL.", "labels": [], "entities": []}, {"text": "We analyse four deep RL models: Deep Q Networks (DQN) (, Double DQN (DDQN), Deep Advantage Actor-Critic (DA2C) () and aversion of DA2C initialized with supervised learning (TDA2C) 1 (similar idea to).", "labels": [], "entities": []}, {"text": "All models are trained on a restaurant-seeking domain.", "labels": [], "entities": []}, {"text": "We use the Dialogue State Tracking Challenge 2 (DSTC2) dataset to train an agenda-based user simulator ( for online learning and to perform batch RL and supervised learning.", "labels": [], "entities": [{"text": "Dialogue State Tracking Challenge 2 (DSTC2) dataset", "start_pos": 11, "end_pos": 62, "type": "DATASET", "confidence": 0.6867371565765805}]}, {"text": "We first show that, on summary state and action spaces, deep RL converges faster than Gaussian Processes SARSA (GPSARSA)).", "labels": [], "entities": []}, {"text": "Then we show that deep RL enables us to work on the original state and action spaces.", "labels": [], "entities": []}, {"text": "Although GPSARSA has also been tried on original state space, it is extremely slow in terms of wall-clock time due to its growing kernel evaluations.", "labels": [], "entities": [{"text": "GPSARSA", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.5810393691062927}]}, {"text": "Indeed, contrary to methods such as GPSARSA, deep RL performs efficient generalization over the state space and memory requirements do not increase with the number of experiments.", "labels": [], "entities": [{"text": "GPSARSA", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.8256487846374512}]}, {"text": "On the simple domain specified by DSTC2, we do not need to restrict the actions in order to learn efficiently.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.9667761325836182}]}, {"text": "In order to remove the need for restricted actions in more complex domains, we advocate for the use of TDA2C and supervised learning as a pre-training step.", "labels": [], "entities": []}, {"text": "We show that supervised learning on a small set of dialogues (only 706 dialogues) significantly bootstraps TDA2C and enables us to start learning with a policy that already selects only valid actions, which makes fora safe user experience in deployment.", "labels": [], "entities": [{"text": "TDA2C", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.8775215148925781}]}, {"text": "Therefore, we conclude that TDA2C is very appealing for the practical deployment of POMDP-based dialogue systems.", "labels": [], "entities": []}, {"text": "In Section 2 we briefly review POMDP, RL and GPSARSA.", "labels": [], "entities": [{"text": "POMDP", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.8079989552497864}, {"text": "GPSARSA", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.8004660606384277}]}, {"text": "The value-based deep RL models investigated in this paper (DQN and DDQN) are described in Section 3.", "labels": [], "entities": []}, {"text": "Policy networks and DA2C are discussed in Section 4.", "labels": [], "entities": []}, {"text": "We then introduce the two-stage training of DA2C in Section 5.", "labels": [], "entities": []}, {"text": "Experimental results are presented in Section 6.", "labels": [], "entities": []}, {"text": "Finally, Section 7 concludes the paper and makes suggestions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Similarly to the previous example, we work on a restaurant domain and use the DSTC2 specifications.", "labels": [], "entities": [{"text": "DSTC2 specifications", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.964926689863205}]}, {"text": "We use \u2212greedy exploration for all four algorithms with starting at 0.5 and being linearly annealed at a rate of \u03bb = 0.99995.", "labels": [], "entities": []}, {"text": "To speedup the learning process, the actions select-pricerange, select-area, and select-food are excluded from exploration.", "labels": [], "entities": []}, {"text": "Note that this set does not depend on the state and is meant for exploration only.", "labels": [], "entities": []}, {"text": "All the actions can be performed by the system at any moment.", "labels": [], "entities": []}, {"text": "We derived two datasets from DSTC2.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9646813869476318}]}, {"text": "The first dataset contains the 2118 dialogues of DSTC2.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.8874917030334473}]}, {"text": "We had these dialogues rated by a human expert, based on the quality of dialogue management and on a scale of 0 to 3.", "labels": [], "entities": []}, {"text": "The second dataset only contains the dialogues with a rating of 3 (706 dialogues).", "labels": [], "entities": []}, {"text": "The underlying assumption is that these dialogues correspond to optimal policies.", "labels": [], "entities": []}, {"text": "We compare the convergence rates of the deep RL models in different settings.", "labels": [], "entities": []}, {"text": "First, we compare DQN, DDQN and DA2C without any pretraining.", "labels": [], "entities": [{"text": "DQN", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.8492233753204346}]}, {"text": "Then, we compare DQN, DDQN and TDA2C with an RL initialization on the DSTC2 dataset.", "labels": [], "entities": [{"text": "DSTC2 dataset", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.9875922799110413}]}, {"text": "Finally, we focus on the advantage actor-critic models and compare DA2C, TDA2C, TDA2C with batch initialization on DSTC2, and TDA2C with batch initialization on the expert dialogues).", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 115, "end_pos": 120, "type": "DATASET", "confidence": 0.9566997289657593}]}], "tableCaptions": []}