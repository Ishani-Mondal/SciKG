{"title": [{"text": "Named Entity Recognition and Hashtag Decomposition to Improve the Classification of Tweets", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7616027891635895}, {"text": "Hashtag Decomposition", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7657436728477478}, {"text": "Classification of Tweets", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.735781212647756}]}], "abstractContent": [{"text": "In social networks services like Twitter, users are overwhelmed with huge amount of social data, most of which are short, unstructured and highly noisy.", "labels": [], "entities": []}, {"text": "Identifying accurate information from this huge amount of data is indeed a hard task.", "labels": [], "entities": []}, {"text": "Classification of tweets into organized form will help the user to easily access these required information.", "labels": [], "entities": [{"text": "Classification of tweets into organized form", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8127891918023428}]}, {"text": "Our first contribution relates to filtering parts of speech and preprocessing this kind of highly noisy and short data.", "labels": [], "entities": []}, {"text": "Our second contribution concerns the named entity recognition (NER) in tweets.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.7895107368628184}]}, {"text": "Thus, the adaptation of existing language tools for natural languages, noisy and not accurate language tweets, is necessary.", "labels": [], "entities": []}, {"text": "Our third contribution involves segmentation of hashtags and a semantic enrichment using a combination of relations from WordNet, which helps the performance of our classification system, including disambiguation of named entities, abbreviations and acronyms.", "labels": [], "entities": [{"text": "segmentation of hashtags", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.8995563586552938}, {"text": "WordNet", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.9254867434501648}]}, {"text": "Graph theory is used to cluster the words extracted from WordNet and tweets, based on the idea of connected components.", "labels": [], "entities": []}, {"text": "We test our automatic classification system with four categories: politics, economy, sports and the medical field.", "labels": [], "entities": []}, {"text": "We evaluate and compare several automatic classification systems using part or all of the items described in our contributions and found that filtering by part of speech and named entity recognition dramatically increase the classification precision to 77.3 %.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 174, "end_pos": 198, "type": "TASK", "confidence": 0.6787900726000468}, {"text": "precision", "start_pos": 240, "end_pos": 249, "type": "METRIC", "confidence": 0.9237993359565735}]}, {"text": "Moreover , a classification system incorporating segmentation of hashtags and semantic enrichment by two relations from WordNet, synonymy and hyperonymy, increase classification precision up to 83.4 %.", "labels": [], "entities": [{"text": "precision", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.9572840332984924}]}], "introductionContent": [{"text": "The automatic classification of text and the approaches for the extraction of hidden subjects have good performance when there is enough meta-information, the context is extended using knowledge from big collections, like Wikipedia ( or it uses meta-information from external sources such as) and include the use of lexical ontologies, like.", "labels": [], "entities": []}, {"text": "However, those approaches need online queries, what makes their performance decrease, and the extraction of knowledge from those external collections demand complex algorithms.", "labels": [], "entities": []}, {"text": "Moreover, the use of those approaches makes the classification algorithms less general.", "labels": [], "entities": []}, {"text": "We present a classification method that uses offline knowledge extracted from WordNet to disambiguate and enrich information in tweets and also to group semantically connected words of tweets in order to decrease the size of our training matrix.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9378800988197327}]}, {"text": "In Section 2 we present a review of some works on the classification of tweets and short texts in general.", "labels": [], "entities": [{"text": "classification of tweets and short texts", "start_pos": 54, "end_pos": 94, "type": "TASK", "confidence": 0.8654767572879791}]}, {"text": "In Section 3 we detail the main steps in the pre-processing: tweet normalization, hashtag decomposition and named entity recognition.", "labels": [], "entities": [{"text": "tweet normalization", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7685017585754395}, {"text": "hashtag decomposition", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.6990601718425751}, {"text": "named entity recognition", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.6574339071909586}]}, {"text": "Section 4 presents our methodology: how WordNet is used to disambiguate words in tweets, how we build a graph using the words in tweets and semantic relations extracted from WordNet and how connected components extracted from this graph is used to decrease the number of dimensions in our training matrix.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.929611086845398}]}, {"text": "In Section 5 we present the experiments and results obtained in the classification of tweets, in terms of precision.", "labels": [], "entities": [{"text": "classification of tweets", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.8957434495290121}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9989873766899109}]}, {"text": "And in Section 6 and Section 7 we present the evaluation and conclusion, respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Twitter API to build our corpus, passing as queries words related to the chosen domains (sport, politics, economics and medicine).", "labels": [], "entities": []}, {"text": "presents the number of tweets, terms and lemmas in the corpus for each domain.: Number of terms, lemmas and tweets for each category  The hashtags carry important information about the tweets subjects.", "labels": [], "entities": []}, {"text": "Moreover, words extracted from them enrich the connected components that contain those words.", "labels": [], "entities": []}, {"text": "For example, a tweet that contains the hashtag #ParisClimateConference does not share any word with the following connected component: climate\u2192 environmental condition, clime, climate.", "labels": [], "entities": [{"text": "ParisClimateConference", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.9668722748756409}]}, {"text": "However, after the hashtag segmentation, the word climate appears in the vector representing the tweet that contains this hashtag: The tweets containing the word climate in their texts or in their hashtags will share the same connected component.", "labels": [], "entities": []}, {"text": "Keeping only verbs, adjectives and adverbs (POS filter) helps improving the classification precision since the sense of the text is usually given by words in those grammatical categories.", "labels": [], "entities": [{"text": "POS filter", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.8854190111160278}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9730475544929504}]}, {"text": "Despite the existence of polysemic words, the disambiguation using WordNet, as explained in section 4.1, helps us find the correct sense of a word.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.955418050289154}]}, {"text": "Not all named entities can be detected by WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9786075353622437}]}, {"text": "However, the most common names of people, places and organizations used in tweets can be successfully identified.", "labels": [], "entities": []}, {"text": "The application of NER helps increase the precision.", "labels": [], "entities": [{"text": "NER", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.7606181502342224}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9987930059432983}]}, {"text": "For example, in tweets we find \"United States of America\", \"United States\" and \"USA\".", "labels": [], "entities": [{"text": "USA", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.9097034335136414}]}, {"text": "Identifying that the three expressions area unique entity helps understand the connection between the tweets that contain them.", "labels": [], "entities": []}, {"text": "We show how NER can affect the identification of the subject of a text.", "labels": [], "entities": [{"text": "NER", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9784846901893616}, {"text": "identification of the subject of a text", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.8870374645505633}]}, {"text": "Without NER, we could have the following connected components:", "labels": [], "entities": [{"text": "NER", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.6655280590057373}]}], "tableCaptions": [{"text": " Table 2: Number of terms, lemmas and tweets for each category", "labels": [], "entities": []}, {"text": " Table 3: Comparison of the precision for different classifications and the gain when named entity recog- nition is applied.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9991771578788757}]}]}