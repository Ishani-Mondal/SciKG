{"title": [{"text": "Japanese Text Normalization with Encoder-Decoder Model", "labels": [], "entities": [{"text": "Japanese Text Normalization", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7141270637512207}]}], "abstractContent": [{"text": "Text normalization is the task of transforming lexical variants to their canonical forms.", "labels": [], "entities": [{"text": "Text normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.737195149064064}]}, {"text": "We model the problem of text normalization as a character-level sequence to sequence learning problem and present a neural encoder-decoder model for solving it.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7515941262245178}]}, {"text": "To train the encoder-decoder model, many sentences pairs are generally required.", "labels": [], "entities": []}, {"text": "However, Japanese non-standard canonical pairs are scarce in the form of parallel corpora.", "labels": [], "entities": []}, {"text": "To address this issue, we propose a method of data augmentation to increase data size by converting existing resources into synthesized non-standard forms using handcrafted rules.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7025564014911652}]}, {"text": "We conducted an experiment to demonstrate that the synthesized corpus contributes to stably train an encoder-decoder model and improve the performance of Japanese text normalization.", "labels": [], "entities": [{"text": "Japanese text normalization", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.6059287289778391}]}], "introductionContent": [{"text": "With the rapid spread of the social media, many texts have been uploaded to the internet.", "labels": [], "entities": []}, {"text": "As such, social media texts are considered important language resources owing to an increasing demand for information extraction and text mining ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7380518019199371}, {"text": "text mining", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.7968320846557617}]}, {"text": "However, these texts include lexical variants such as insertions, phonetic substitutions and internet slangs.", "labels": [], "entities": []}, {"text": "These non-standard forms adversely affect language analysis tools that are trained on a clean corpus.", "labels": [], "entities": [{"text": "language analysis", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7091516852378845}]}, {"text": "Since Japanese has no explicit boundaries between words, word segmentation and part-of-speech (POS) tagging are extremely important in Japanese language processing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7439507842063904}, {"text": "part-of-speech (POS) tagging", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.7028764963150025}, {"text": "Japanese language processing", "start_pos": 135, "end_pos": 163, "type": "TASK", "confidence": 0.6021660069624583}]}, {"text": "For example, the output obtained using the Japanese morphological analyzer: MeCab 1 fora non-standard sentence: \"\" is as follows: Input: (kono apuri sugeee; This app is greeeat!)", "labels": [], "entities": [{"text": "MeCab 1", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9178094863891602}]}, {"text": "Output: (This, Prenominal adjective) / (a, Filler) / (UNK) / (Symbol) where the slashes indicate word boundaries.", "labels": [], "entities": [{"text": "Output", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9667242169380188}]}, {"text": "Although \"\" (apuri; app) is originally written in Katakana form, it is written in Hiragana form \"\" (apuri; app) in this example.", "labels": [], "entities": []}, {"text": "Japanese has several writing scripts: Kanji, Hiragana and Katakana.", "labels": [], "entities": []}, {"text": "Such interchanging between scripts in social media texts often occurs.", "labels": [], "entities": []}, {"text": "Furthermore, \"\" (sugeeee; greeeat) is derived from \"\" (sugoi; great) by changing some vowels from /oi/ to /eee/.", "labels": [], "entities": []}, {"text": "These non-standard forms such as \"\" and \"\" are not registered in the morphological analysis dictionary.", "labels": [], "entities": []}, {"text": "For this reason, if a word does not exist in the morphological analysis dictionary, the traditional system cannot predict the correct word segmentation or POS tags.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.7300752997398376}]}, {"text": "If we could normalize all non-standard forms, then the canonical sentence is considerd as \"\".", "labels": [], "entities": []}, {"text": "As a result, we can obtain the correct result of morphological analysis for this normalized sentence as follows: Input:(kono apuri sugoi; This app is great!)", "labels": [], "entities": []}, {"text": "Output: (This, Prenominal adjective) / (app, Noun) / (great, Adjective) /(Symbol) As the above example shows, we would expect improvement in the result of the morphological analysis by transforming non-standard forms into their standard ones.", "labels": [], "entities": []}, {"text": "In this work, we adopt the character-level encoder-decoder model as a method of Japanese text normalization.", "labels": [], "entities": [{"text": "Japanese text normalization", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.6897661685943604}]}, {"text": "Since the encoder-decoder model was proposed in the field of machine translation, it has achieved good results in morphological inflection generation and error correction (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8043818473815918}, {"text": "morphological inflection generation", "start_pos": 114, "end_pos": 149, "type": "TASK", "confidence": 0.679089883963267}, {"text": "error correction", "start_pos": 154, "end_pos": 170, "type": "TASK", "confidence": 0.6906495094299316}]}, {"text": "As these models are capable of capturing sequential patterns, it is possible to apply the encoder-decoder model to Japanese text normalization.", "labels": [], "entities": [{"text": "Japanese text normalization", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.5715697606404623}]}, {"text": "As mentioned previously, Japanese has no explicit boundaries between words.", "labels": [], "entities": []}, {"text": "Thus, Japanese text normalization is naturally posed to a character-level sequence to sequence learning.", "labels": [], "entities": [{"text": "Japanese text normalization", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.6044619381427765}]}, {"text": "Although the encoder-decoder model has been shown its effectiveness in large datasets, it is much less effective for small datasets such as low-resource languages (.", "labels": [], "entities": []}, {"text": "Unfortunately, contrary to machine translation, Japanese non-standard canonical pairs are scarce in the form of parallel corpora.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7232666909694672}]}, {"text": "To address this issue, we propose a method of data augmentation to increase the data size by converting existing resources into synthesized non-standard forms using handcrafted rules.", "labels": [], "entities": []}, {"text": "We conducted an experiment to demonstrate that the synthesized corpus provides stable training of the encoder-decoder model and improves the performance of Japanese text normalization.", "labels": [], "entities": [{"text": "Japanese text normalization", "start_pos": 156, "end_pos": 183, "type": "TASK", "confidence": 0.5848946273326874}]}, {"text": "The contributions of this research can be summarized by citing two points.", "labels": [], "entities": []}, {"text": "First, the proposed data augmentation methods can provide stable training of the encoder-decoder model.", "labels": [], "entities": []}, {"text": "Second, it can improve the performance of Japanese text normalization by adding the synthesized corpus.", "labels": [], "entities": [{"text": "Japanese text normalization", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.624673585096995}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the related work to our research including Japanese text normalization and the neural encoder-decoder model.", "labels": [], "entities": [{"text": "Japanese text normalization", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.6682584881782532}]}, {"text": "Section 3 introduces the model architecture in this research.", "labels": [], "entities": []}, {"text": "Section 4 describes how to construct a synthesized corpus.", "labels": [], "entities": []}, {"text": "Section 5 discusses experiments that we have performed and our corresponding analyses of the experimental results.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper with a brief summary and a mention of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the effectiveness of our normalization method, we conducted experiments on a microblog data to confirm the effectiveness of our model.", "labels": [], "entities": [{"text": "normalization", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9701050519943237}]}, {"text": "For the experiments, we used the Microblog corpus and the Synthesized corpus for training.", "labels": [], "entities": [{"text": "Microblog corpus", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.9747622013092041}, {"text": "Synthesized corpus", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.7882066071033478}]}, {"text": "Both corpora consist of canonical and non-standard sentence pairs.", "labels": [], "entities": []}, {"text": "The Synthesized corpus comprised 213,618 non-standard and canonical sentences pairs.", "labels": [], "entities": [{"text": "Synthesized corpus", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.7367190718650818}]}, {"text": "The Microblog corpus was constructed from Japanese Twitter data by.", "labels": [], "entities": [{"text": "Microblog corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9729195833206177}]}, {"text": "They collected 17,386 Japanese tweets using the Twitter Api.", "labels": [], "entities": [{"text": "Twitter Api", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.646356850862503}]}, {"text": "Among these, 1000 tweets were randomly selected, and they annotated the 1831 sentences with canonical forms and morphological information.", "labels": [], "entities": []}, {"text": "In our work, we extracted 506 sentences that included at least one non-standard form from the Microblog corpus.", "labels": [], "entities": [{"text": "Microblog corpus", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.9710267186164856}]}, {"text": "These extracted sentences comprised 697 non-standard canonical pairs.", "labels": [], "entities": []}, {"text": "In the preprocessing, repetitions of more than one character of long sound symbols, namely \"\", \"\" and the double consonant \"\" are reduced back to one character, whereas the others are reduced to three characters.", "labels": [], "entities": []}, {"text": "We used these sentences as the Microblog corpus.: Results for test sets on CER: Scatter plot of the relationship between Test CER and the training data size  As our evaluation metric, we used a character error rate (CER), which is defined as the Levenshtein edit distance between the predicted character sequence x and the target character sequence y, normalized by the total number of characters in the target.", "labels": [], "entities": [{"text": "Microblog corpus.", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9575479030609131}, {"text": "character error rate (CER)", "start_pos": 194, "end_pos": 220, "type": "METRIC", "confidence": 0.8664033710956573}, {"text": "Levenshtein edit distance", "start_pos": 246, "end_pos": 271, "type": "METRIC", "confidence": 0.7788699865341187}]}, {"text": "CER indicates the closeness of the normalized sentence toward the target sentence.", "labels": [], "entities": [{"text": "CER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9703534841537476}]}, {"text": "In the experiments, we performed 5-fold cross validation.", "labels": [], "entities": []}, {"text": "By dividing the Microblog corpus into the test data, development data and training data, we tested on only the Microblog corpus to evaluate the effectiveness of each method on all the microblog data.", "labels": [], "entities": [{"text": "Microblog corpus", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.9380952715873718}, {"text": "Microblog corpus", "start_pos": 111, "end_pos": 127, "type": "DATASET", "confidence": 0.9757182002067566}]}, {"text": "shows the results of our experiments on test CER.", "labels": [], "entities": [{"text": "CER", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.6712279915809631}]}, {"text": "No-operation indicates a baseline that leaves the input sentence unchanged.", "labels": [], "entities": []}, {"text": "This demonstrates that the input sentence has about two non-standard characters in itself.", "labels": [], "entities": []}, {"text": "As shown in, the Rule-based method reduces errors by 2.08 , as compared with No-operation.", "labels": [], "entities": [{"text": "errors", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9982011318206787}]}, {"text": "CRF trained on only the Microblog corpus: CRF (Microblog corpus) reduces errors by 3.44 , as compared with No-operation.", "labels": [], "entities": [{"text": "CRF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8942629098892212}, {"text": "Microblog corpus", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.9747974276542664}, {"text": "CRF (Microblog corpus", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.7301333695650101}, {"text": "errors", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9975571632385254}]}, {"text": "With these results, when the encoder-decoder model trained on only the Microblog corpus, we found that the EncDec (Microblog corpus) is worse than No-operation, then the results in CER from 17.15 to 87.07 . As shows, it generated entirely different strings of the input sentence.", "labels": [], "entities": [{"text": "Microblog corpus", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.9720467031002045}, {"text": "EncDec (Microblog corpus)", "start_pos": 107, "end_pos": 132, "type": "DATASET", "confidence": 0.651692795753479}, {"text": "CER", "start_pos": 181, "end_pos": 184, "type": "DATASET", "confidence": 0.7567775249481201}]}, {"text": "This indicates that the Microblog corpus is too small for training the encoder-decoder model; however, the Synthesized corpus contributes to the stable training of the encoder-decoder model.", "labels": [], "entities": [{"text": "Microblog corpus", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.9797276556491852}, {"text": "Synthesized corpus", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.8584409058094025}]}, {"text": "Additionally, EncDec (Synthesized corpus) is able to normalize the non-standard form \"\" (sugee; greeat) into the canonical form \"\" (sugoi; great) without generating different strings of the input sentence.", "labels": [], "entities": []}, {"text": "shows the relation between test CER and the training data size for the encoder-decoder model.", "labels": [], "entities": []}, {"text": "The first dotted line indicates that it needs more than 10,000 sentences to reduce the error rate than No-operation.", "labels": [], "entities": [{"text": "error rate", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9710878431797028}]}, {"text": "The second dotted line represents that about 200,000 sentences reach CER on CRF (Microblog corpus).", "labels": [], "entities": [{"text": "CER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.8719456195831299}, {"text": "CRF (Microblog corpus)", "start_pos": 76, "end_pos": 98, "type": "DATASET", "confidence": 0.7996955156326294}]}, {"text": "As we have combined the Microblog corpus and the Synthesized corpus, the Combined corpus contributes to improving the performance of Japanese text normalization in CRF and EncDec.", "labels": [], "entities": [{"text": "Microblog corpus", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.9612432420253754}, {"text": "Synthesized corpus", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.7114095836877823}, {"text": "Japanese text normalization", "start_pos": 133, "end_pos": 160, "type": "TASK", "confidence": 0.5858346919218699}]}, {"text": "Both methods are able to reduce errors by 3.72 , as compared with No-operation.", "labels": [], "entities": [{"text": "errors", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9953938722610474}]}, {"text": "Other examples of our normalization system output are illustrated in.", "labels": [], "entities": [{"text": "normalization", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9610956311225891}]}, {"text": "Example (1) shows that our method is able to normalize the spoken form \"\" (ssu; is) into its canonical form \"\" (desu; is).", "labels": [], "entities": []}, {"text": "Examples (2) and (3) demonstrate that our method is inability to generate correct normalization sentence.", "labels": [], "entities": []}, {"text": "Example (2) shows an incorrect example of over-translation, which generates the same character repeatedly: The Kanji character \"\" was generated twice.", "labels": [], "entities": []}, {"text": "Example (3) shows an incorrect example of the low frequency of a character: The Kanji character \"\" appears once in the training data.", "labels": [], "entities": []}, {"text": "Thus, the wrong character \"\" was generated instead of \"\" in this example.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for test sets on CER", "labels": [], "entities": [{"text": "CER", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.9518106579780579}]}]}