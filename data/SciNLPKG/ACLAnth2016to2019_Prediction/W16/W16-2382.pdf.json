{"title": [], "abstractContent": [{"text": "Referential translation machines (RTMs) pioneer a language independent approach for predicting translation performance and to all similarity tasks with top performance in both bilingual and monolingual settings and remove the need to access any task or domain specific information or resource.", "labels": [], "entities": [{"text": "Referential translation machines (RTMs", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8785730004310608}, {"text": "predicting translation", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.9074583649635315}]}, {"text": "RTMs achieve to become 1st in document-level, 4th system at sentence-level according to mean absolute error, and 4th in phrase-level prediction of translation quality in quality estimation task.", "labels": [], "entities": []}, {"text": "1 Referential Translation Machines Prediction of translation performance can help in estimating the effort required for correcting the translations during post-editing by human translators if needed.", "labels": [], "entities": [{"text": "Referential Translation Machines Prediction", "start_pos": 2, "end_pos": 45, "type": "TASK", "confidence": 0.7614474445581436}]}, {"text": "Referential translation machines achieve top performance in automatic and accurate prediction of machine translation performance independent of the language or domain of the prediction task.", "labels": [], "entities": [{"text": "Referential translation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7268033921718597}]}, {"text": "Each referential translation machine (RTM) model is a data translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation.", "labels": [], "entities": [{"text": "referential translation machine (RTM)", "start_pos": 5, "end_pos": 42, "type": "TASK", "confidence": 0.8285244107246399}, {"text": "data translation prediction", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.8032975097497305}]}, {"text": "RTMs are powerful enough to be applicable in different domains and tasks while achieving top performance in both monolingual (Bi\u00e7ici and Way, 2015) and bilingual settings (Bi\u00e7ici et al., 2015b).", "labels": [], "entities": []}, {"text": "Figure 1 depicts RTMs and explains the model building process (Bi\u00e7ici, 2016).", "labels": [], "entities": [{"text": "RTMs", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.8828829526901245}]}, {"text": "RTMs use ParFDA (Bi\u00e7ici et al., 2015a) for selecting instances and interpretants, data close to the task instances for building prediction models and machine translation performance prediction system (MTPPS) (Bi\u00e7ici and Way, 2015) for generating features.", "labels": [], "entities": [{"text": "ParFDA", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.8788530230522156}, {"text": "machine translation performance prediction", "start_pos": 150, "end_pos": 192, "type": "TASK", "confidence": 0.715454712510109}]}, {"text": "We improve our RTM models (Bi\u00e7ici et Figure 1: RTM depiction: ParFDA selects inter-pretants close to the training and test data using parallel corpus in bilingual settings and mono-lingual corpus in the target language or just the monolingual target corpus in monolingual settings ; an MTPPS uses interpretants and training data to generate training features and another uses interpretants and test data to generate test features in the same feature space; learning and prediction takes place taking these features as input.", "labels": [], "entities": [{"text": "RTM depiction", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.8125025629997253}]}, {"text": "al., 2015b) with numeric expression identification using regular expressions and replace them with a label (Bi\u00e7ici, 2016).", "labels": [], "entities": [{"text": "numeric expression identification", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.6857625842094421}]}, {"text": "2 RTM in the Quality Estimation Task We develop RTM models for all of the four sub-tasks of the quality estimation task (QET) in WMT16 (Bojar et al., 2016) (QET16), which include English to Spanish (en-es), English to Ger-man (en-de), and German to English (de-en) translation directions.", "labels": [], "entities": [{"text": "WMT16 (Bojar et al., 2016) (QET16)", "start_pos": 129, "end_pos": 163, "type": "DATASET", "confidence": 0.8207902637394991}]}, {"text": "The subtasks are: sentence-level prediction (Task 1), word-level prediction (Task 2), phrase-level prediction (Task 2p), and document-level prediction (Task 3).", "labels": [], "entities": [{"text": "sentence-level prediction", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.7246232628822327}, {"text": "word-level prediction", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.6715463995933533}, {"text": "phrase-level prediction", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.7377188950777054}, {"text": "document-level prediction", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.6891545653343201}]}, {"text": "Task 1 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006) scores of sentence translations, Task 2 is about binary classification of word-level quality, Task 2p is about binary classification of phrase-level quality, and 777", "labels": [], "entities": [{"text": "predicting HTER", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.6476696282625198}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Training performance of the top 2 individual RTM models prepared for different tasks.", "labels": [], "entities": []}, {"text": " Table 3: RTM Task 2 training results where GLMd  parallelized over 4 splits is referred as GLMd s4", "labels": [], "entities": [{"text": "RTM Task 2 training", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8085021525621414}]}, {"text": " Table 4: Test performance of the top 2 individual RTM models prepared for different tasks.", "labels": [], "entities": []}, {"text": " Table 5: RTM Task 2 results on the test set. wF 1 is  the average weighted F 1 score. bold results obtain  top performance.", "labels": [], "entities": [{"text": "RTM Task", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.4750933200120926}, {"text": "F 1 score", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9623774488766988}]}, {"text": " Table 6: RTM top predictor testing results for Task 1 optimized for r.", "labels": [], "entities": [{"text": "RTM top predictor testing", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6631423458456993}]}]}