{"title": [{"text": "Infusing NLU into Automatic Question Generation", "labels": [], "entities": [{"text": "NLU", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.7131723761558533}, {"text": "Automatic Question Generation", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.6306319932142893}]}], "abstractContent": [{"text": "We present afresh approach to automatic question generation that significantly increases the percentage of acceptable questions compared to prior state-of-the-art systems.", "labels": [], "entities": [{"text": "automatic question generation", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.6227883100509644}]}, {"text": "In our evaluation of the top 20 questions, our system generated 71% more acceptable questions by informing the generation process with Natural Language Understanding techniques.", "labels": [], "entities": []}, {"text": "The system also introduces our DeconStructure algorithm which creates an intuitive and practical structure for easily accessing sentence functional constituents in NLP applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question generation has been described as a dialogue and discourse task, drawing on both Natural Language Understanding and Natural Language Generation ( ).", "labels": [], "entities": [{"text": "Question generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8205605745315552}, {"text": "Natural Language Generation", "start_pos": 124, "end_pos": 151, "type": "TASK", "confidence": 0.634904811779658}]}, {"text": "However, current state-of-the-art question generation systems pay scant attention to the NLU aspect, an issue we address in this work.", "labels": [], "entities": [{"text": "question generation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.8327085077762604}]}, {"text": "The question generator we present explores means of infusing NLU analysis into the task of automatically generating questions from expository text for educational purposes.", "labels": [], "entities": []}, {"text": "Ginzburg's work on Questions Under Discussion (2012) frames discourse as a series of questions to be addressed.", "labels": [], "entities": []}, {"text": "Expository text could be viewed from this perspective: it is a monologue from which the author hopes the reader would be able to answer a set of questions.", "labels": [], "entities": []}, {"text": "Automatic question generation, then, could be viewed as a process of discovering unasked questions within the monologue.", "labels": [], "entities": [{"text": "question generation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6996338218450546}]}], "datasetContent": [{"text": "There is no standard way to evaluate automatically generated questions.", "labels": [], "entities": []}, {"text": "Recent work in QG and other NLP applications favors evaluation by crowdsourcing which has proven to be both cost and time efficient and to achieve results comparable to human evaluators (.", "labels": [], "entities": []}, {"text": "We compared our system performance to the most-frequently cited prior question generation system by.", "labels": [], "entities": []}, {"text": "The evaluation was conducted using Amazon's Mechanical Turk Service.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk Service", "start_pos": 35, "end_pos": 67, "type": "DATASET", "confidence": 0.9235857963562012}]}, {"text": "Workers were selected with at least 90% approval rating and who were located in the US and proficient in US English.", "labels": [], "entities": [{"text": "approval rating", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.972560465335846}]}, {"text": "To monitor quality, work was submitted in small batches, manually inspected, and run through software to detect workers whose ratings did not correspond well with fellow workers.", "labels": [], "entities": []}, {"text": "Each question was rated on a 1-5 scale by 4 workers.", "labels": [], "entities": []}, {"text": "The four scores were averaged.", "labels": [], "entities": []}, {"text": "Agreement between each set of workers and the average had a Pearson's correlation r = .71, showing high agreement.", "labels": [], "entities": [{"text": "Pearson's correlation r", "start_pos": 60, "end_pos": 83, "type": "METRIC", "confidence": 0.949762374162674}]}], "tableCaptions": [{"text": " Table 2: Comparing Parser Outputs: Phrase Structure Grammar, Semantic Role Label, Dependency", "labels": [], "entities": []}, {"text": " Table 3: Front End DeconStructure for Sentence in", "labels": [], "entities": []}, {"text": " Table 5: Test Data and Questions Generated", "labels": [], "entities": []}, {"text": " Table 6: Average Scores for Top 20 Questions", "labels": [], "entities": [{"text": "Average Scores", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9808870851993561}]}]}