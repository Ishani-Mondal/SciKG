{"title": [{"text": "Investigating Active Learning for Short-Answer Scoring", "labels": [], "entities": [{"text": "Short-Answer Scoring", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7029658406972885}]}], "abstractContent": [{"text": "Active learning has been shown to be effective for reducing human labeling effort in supervised learning tasks, and in this work we explore its suitability for automatic short answer assessment on the ASAP corpus.", "labels": [], "entities": [{"text": "automatic short answer assessment", "start_pos": 160, "end_pos": 193, "type": "TASK", "confidence": 0.5367175117135048}, {"text": "ASAP corpus", "start_pos": 201, "end_pos": 212, "type": "DATASET", "confidence": 0.8004043400287628}]}, {"text": "We systematically investigate a wide range of AL settings, varying not only the item selection method but also size and selection of seed set items and batch size.", "labels": [], "entities": []}, {"text": "Comparing to a random baseline and a recently-proposed diversity-based baseline which uses cluster centroids as training data, we find that uncertainty-based sampling methods can be beneficial, especially for data sets with particular properties.", "labels": [], "entities": []}, {"text": "The performance of AL, however, varies considerably across individual prompts.", "labels": [], "entities": [{"text": "AL", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.5606176853179932}]}], "introductionContent": [{"text": "Methods for automatically scoring short, written, free-text student responses have the potential to greatly reduce the workload of teachers.", "labels": [], "entities": []}, {"text": "This task of automatically assessing such student responses (as opposed to, e.g., gap-filling questions) is widely referred to as short answer scoring (SAS), and automatic methods have been developed for tasks ranging from science assessments to reading comprehension, and for such varied domains as foreign language learning, citizenship exams, and more traditional classrooms.", "labels": [], "entities": [{"text": "short answer scoring (SAS)", "start_pos": 130, "end_pos": 156, "type": "TASK", "confidence": 0.5540781964858373}]}, {"text": "Most existing automatic SAS systems rely on supervised machine learning techniques that require large amounts of manually labeled training data to achieve reasonable performance, and recent work (, among others) has begun to investigate the influence of the quantity and quality of training data for SAS.", "labels": [], "entities": [{"text": "SAS", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.7390920519828796}, {"text": "SAS", "start_pos": 300, "end_pos": 303, "type": "TASK", "confidence": 0.9704997539520264}]}, {"text": "In this paper we take the next logical step and investigate the applicability of active learning for teacher workload reduction in automatic SAS.", "labels": [], "entities": [{"text": "automatic SAS", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.5556362271308899}]}, {"text": "As for most supervised learning scenarios, automatic SAS systems perform more accurate scoring as the amount of data available for learning increases.", "labels": [], "entities": []}, {"text": "Particularly in the educational context, though, simply labeling more data is an unsatisfying and often impractical recommendation.", "labels": [], "entities": []}, {"text": "New questions or prompts with new sets of responses are generated on a regular basis, and there's a need for automatic scoring approaches that can do accurate assessment with much smaller amounts of labeled data ('labeling' here generally means human grading).", "labels": [], "entities": []}, {"text": "One solution to this problem is to develop generic scoring models which do not require re-training in order to do assessment fora new data set (i.e. anew question/prompt plus responses).", "labels": [], "entities": []}, {"text": "apply such a model for scoring short reading comprehension responses written by learners of German.", "labels": [], "entities": [{"text": "scoring short reading comprehension responses written by learners of German", "start_pos": 23, "end_pos": 98, "type": "TASK", "confidence": 0.7216134577989578}]}, {"text": "This system crucially relies on features which directly compare learner responses to target answers provided as part of the data set, and the responses are mostly one sentence or phrase.", "labels": [], "entities": []}, {"text": "In this work we are concerned with longer responses generated from a wide range of prompt types, from questions asking for list-like responses to those seeking coherent multi-sentence texts (details in Section 3).", "labels": [], "entities": []}, {"text": "For such questions, there is generally no single best response, and thus the system cannot rely on comparisons to a single target answer per question.", "labels": [], "entities": []}, {"text": "Rather systems need features which capture lexical properties of responses to the prompt at hand.", "labels": [], "entities": []}, {"text": "In other words, anew scoring model is built for each individual prompt.", "labels": [], "entities": []}, {"text": "A second solution involves focused selection of items to be labeled, with the aim of comparable performance with less labeled data.", "labels": [], "entities": []}, {"text": "investigate whether carefully selected training data are beneficial in an SAS task.", "labels": [], "entities": [{"text": "SAS task", "start_pos": 74, "end_pos": 82, "type": "TASK", "confidence": 0.9260620772838593}]}, {"text": "For each prompt, they first cluster the entire set of responses and then train a classifier on the labeled instances that are closest to the centroids of the clusters produced.", "labels": [], "entities": []}, {"text": "The intuition -that a training data set constructed in this way captures the lexical diversity of the responses -is supported by results on a data set with shorter responses, but on the ASAP data set, the approach fails to improve over random selection.", "labels": [], "entities": [{"text": "ASAP data set", "start_pos": 186, "end_pos": 199, "type": "DATASET", "confidence": 0.9597181479136149}]}, {"text": "The natural next step is to use active learning (AL, Settles) for informed selection of training instances.", "labels": [], "entities": []}, {"text": "In AL, training corpora are built up incrementally by successive selection of instances according to the current state of the classifier (a detailed description appears in Section 4).", "labels": [], "entities": []}, {"text": "In other words, the machine learner is queried to determine regions of uncertainty, instances in that region are sampled and labeled, these are added to the training data, the classifier is retrained, and the cycle repeats.", "labels": [], "entities": []}, {"text": "Our approach differs from that of in two important ways.", "labels": [], "entities": []}, {"text": "First, rather than selecting instances according to the lexical diversity of the training data, we select them according to the output of the classifier.", "labels": [], "entities": []}, {"text": "Second, we select instances and retrain the classifier in an incremental, cyclical fashion, such that each new labeled instance contributes to the knowledge state which leads to selection of the next instance.", "labels": [], "entities": []}, {"text": "Sample selection via AL involves setting a number of parameters, and there is no single best-for-alltasks AL setting.", "labels": [], "entities": [{"text": "Sample selection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9602691233158112}]}, {"text": "Thus we explore a wide range of AL scenarios, implementing a number of established methods for selecting candidates.", "labels": [], "entities": []}, {"text": "We consider three families of methods.", "labels": [], "entities": []}, {"text": "The first are uncertainty-based methods, which target items about which the classifier is least confident.", "labels": [], "entities": []}, {"text": "Next, diversity-based methods aim to cover the feature space as broadly as possible; the cluster-centroid selection method described above is most similar to this type of sample selection.", "labels": [], "entities": []}, {"text": "Finally, representativeness-based methods select items that are prototypical for the data set at hand.", "labels": [], "entities": []}, {"text": "Our results show a clear win for uncertaintybased methods, with the caveat that performance varies greatly across prompts.", "labels": [], "entities": []}, {"text": "To date, there are no clear guidelines for matching AL parameter settings to particular classification tasks or data sets.", "labels": [], "entities": []}, {"text": "To better understand the varying performance of different sample selection methods, we present an initial investigation of two properties of the various data sets.", "labels": [], "entities": []}, {"text": "Perhaps unsurprisingly, we see that uncertainty-based sampling brings stronger gains for data sets with skewed class distributions, as well as for those with more cleanly separable classes according to language model perplexity.", "labels": [], "entities": []}, {"text": "In sum, active learning can be used to reduce the amount of training data required for automatic SAS on longer written responses without representative target answers, but the methods and parameters need to be chosen carefully.", "labels": [], "entities": [{"text": "SAS", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.8160630464553833}]}, {"text": "Further investigation is needed to formulate recommendations for matching AL settings to individual data sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the data set, features, and classifier used in our experiments.", "labels": [], "entities": []}, {"text": "We evaluate all of our SAS systems using Cohen's linearly weighted kappa.", "labels": [], "entities": []}, {"text": "Each result reported fora given combination of item selection and seed selection methods is the average over 10 runs, each with a different seed set.", "labels": [], "entities": []}, {"text": "The seed sets remain fixed across conditions.", "labels": [], "entities": []}, {"text": "In order to evaluate the overall performance of an AL method, we need to measure the performance gain over a baseline.", "labels": [], "entities": []}, {"text": "Rather than computing this atone fixed point in the learning curve, we follow in looking at averaged performance over a set of points early in the learning curve.", "labels": [], "entities": []}, {"text": "This is where AL produces the biggest gains; once many more items have been labeled, the differences between the systems reduces.", "labels": [], "entities": []}, {"text": "We slightly adapt Melville and Mooney's method and compute the average percent error reduction (that is, error reduction on kappa values) over the first 300 labeled instances (18-26% of all items, depending on the size of the data set).", "labels": [], "entities": [{"text": "error reduction", "start_pos": 79, "end_pos": 94, "type": "METRIC", "confidence": 0.9388472735881805}, {"text": "error reduction", "start_pos": 105, "end_pos": 120, "type": "METRIC", "confidence": 0.9328948259353638}]}, {"text": "The first experiment compares the different item selection methods outlined in Section 4.1, using small seedsets and varying batch sizes.", "labels": [], "entities": [{"text": "item selection", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.6698986291885376}]}, {"text": "To give a global picture of differences between the methods, shows the learning curves for all sample selection methods, averaged overall prompt and seed sets.", "labels": [], "entities": []}, {"text": "Especially in early parts of the learning curve until about 500 items are labeled, uncertainty-based methods show improvement over the random baseline.", "labels": [], "entities": []}, {"text": "Both representativeness and diversity-based sampling perform far worse than random.", "labels": [], "entities": []}, {"text": "On average, the systems trained on cluster centroids perform at or below the random baseline, The picture changes a bit when we look at the performance of AL methods per prompt and with different seed selection methods.", "labels": [], "entities": []}, {"text": "shows the percent error reduction (compared to the random baseline) per prompt and seed selection method, averaged over the first 300 labeled items.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 18, "end_pos": 33, "type": "METRIC", "confidence": 0.8818871080875397}]}, {"text": "Most noticeable is that we see a wide variety in the performance of the sample selection methods for the various prompts.", "labels": [], "entities": []}, {"text": "For some -most pronouncedly prompt 2, 5, 6 and 10 -there is a consistent improvement for uncertainty sampling methods, while other prompts seem to be almost completely resistent to AL.", "labels": [], "entities": [{"text": "AL", "start_pos": 181, "end_pos": 183, "type": "METRIC", "confidence": 0.9360420107841492}]}, {"text": "When looking at individual averaged AL curves, we can see some improvement for prompts 7 to 9 that peaks only after 300 items are labeled.", "labels": [], "entities": []}, {"text": "For prompt 3, none of the AL methods ever beats the baseline, at any point in the learning process.", "labels": [], "entities": []}, {"text": "We also observe variability in the performance across seed sets for one prompt, as can be seen from the standard deviation.", "labels": [], "entities": []}, {"text": "The question of which AL method is most effective for this task can be answered at least partially: if any method yields a substantial improvement, it is an uncertainty-based method.", "labels": [], "entities": []}, {"text": "On average, boosted entropy gives the highest gains in both seed selection settings.", "labels": [], "entities": []}, {"text": "Comparing random to equal seed selection, performance is rather consistently better when AL starts with a seed set that covers all classes equally.: Error reduction rates over random sampling for different seed set sizes, averaging overall prompts.", "labels": [], "entities": []}, {"text": "Experiment 1 shows a clear benefit for using equal rather than random seeds.", "labels": [], "entities": []}, {"text": "Ina real life scenario, however, balanced seed sets are harder to produce than purely random ones.", "labels": [], "entities": []}, {"text": "One might argue that using a larger randomly-selected seed set increases the likelihood of covering all classes in the seed data and provides a better initialization for AL, without the additional overhead of creating balanced seed sets.", "labels": [], "entities": []}, {"text": "This motivates the next experiment, in which learning begins with seed sets of 20 randomlyselected labeled items, but otherwise follows the same procedure.", "labels": [], "entities": []}, {"text": "We compare the performance of systems intialized with these larger seed sets to both random and equal small seed sets, considering only the more promising uncertainty-based item selection methods, and again using varying batch sizes.", "labels": [], "entities": []}, {"text": "We can see, that the performance for margin and entropy sampling is slightly better than the small random seed set (curiously not for boosted entropy), but it is still below that of the small equal seed set.", "labels": [], "entities": [{"text": "entropy sampling", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7066241949796677}]}, {"text": "However, the trend across items is not completely clear.", "labels": [], "entities": []}, {"text": "We still take it as an indicator that seeds of good quality cannot be outweight by quantity.", "labels": [], "entities": []}, {"text": "In experiment 1 we used varying batch sizes that learn anew model after each individual labeled item in the beginning and allow larger batches only later in the AL process.", "labels": [], "entities": []}, {"text": "Ina real life application, larger batch sizes might be in general preferrable.", "labels": [], "entities": []}, {"text": "Therefore we test an alternative setup where we sample and label 20 items per batch before retraining.", "labels": [], "entities": []}, {"text": "presents results for uncertainty-based sampling methods, averaged over the first 300 labeled instances.", "labels": [], "entities": []}, {"text": "Compared to the varying batch size setup (numbers in parentheses), performance goes down, indicating that fine-grained sampling really does provide a benefit, especially early in the learn-  ing process.", "labels": [], "entities": []}, {"text": "Where larger batch sizes may lead to selection of instances in the same region of uncertainty, a smaller batch size allows the system to resolve a certain region of uncertainty with fewer labeled training instances.", "labels": [], "entities": []}, {"text": "On average, it is clear that uncertainty-based active learning methods are able to provide an advantage in classification performance over random or clustercentroid baselines.", "labels": [], "entities": []}, {"text": "If we look at the result for the different prompts, though, it is equally clear that AL performance varies tremendously across data sets for individual prompts.", "labels": [], "entities": []}, {"text": "In order to deploy AL effectively for SAS, we need to better understand why AL works so much better for some data sets than for others.", "labels": [], "entities": [{"text": "SAS", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9505937695503235}]}, {"text": "In we see that AL is especially effective for prompts 5 and 6.", "labels": [], "entities": [{"text": "AL", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9359375238418579}]}, {"text": "Cross-referencing, it becomes clear that these are the two ASAP prompts with the highest degree of class imbalance.", "labels": [], "entities": []}, {"text": "shows the changes in the distribution of the individual classes among the labeled data for prompt 6 as AL (here with entropy item selection) proceeds.", "labels": [], "entities": []}, {"text": "We see clearly that uncertainty sampling at early stages selects the different classes in away that is more balanced than the overall distribution for the full data set and thus increases the classifier's accuracy in labeling minority class items.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.9978787899017334}]}, {"text": "For comparison, a plot for random sampling would ideally consist of four lines parallel to the x axis, and both diversity and representativeness sampling tend to select items from the majority class, explaining their bad performance.", "labels": [], "entities": []}, {"text": "Class imbalance explains some of the variable performance of AL across prompts, but clearly there is more to the story.", "labels": [], "entities": []}, {"text": "Next, we use language model (LM) perplexity (computed using the SRILM toolkit) as a measurement of how similar shows the results.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.86970254778862}]}, {"text": "We see that for those answers that work well under AL, again prominently prompts 5 and 6, at least some classes separate very well against the other classes.", "labels": [], "entities": []}, {"text": "They show a high average perplexity, indicating that the answer is not well modeled by other answers with different scores.", "labels": [], "entities": []}, {"text": "In comparison, for some other data sets where the uncertainty curves do not clearly beat random sampling, especially 3 and 4, we see that the classes are not well separated from each other.", "labels": [], "entities": []}, {"text": "They are among those with the lowest perplexity across scores.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9700026512145996}]}, {"text": "This result, while preliminary and dependent on knowing the true scores of the data, suggests that uncertainty sampling profits from classes that are well separated from one another, such that clear regions of uncertainty can emerge.", "labels": [], "entities": []}, {"text": "An intriguing future direction is to seek out other approaches to characterizing unlabeled data sets, in order to determine: (a) whether AL is a suitable strategy for workload reduction, and (b) if so, which AL setting will give the strongest performance gains for the data set at hand.", "labels": [], "entities": [{"text": "workload reduction", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.7208686769008636}]}], "tableCaptions": [{"text": " Table 1: Data set sizes and label distributions for training and test splits. '-' indicates a score does not occur for that data set.", "labels": [], "entities": []}, {"text": " Table 3: Error reduction rates over random sampling for differ-", "labels": [], "entities": [{"text": "Error reduction", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9375802576541901}]}]}