{"title": [{"text": "NEAL: A Neurally Enhanced Approach to Linking Citation and Reference", "labels": [], "entities": [{"text": "Linking Citation and Reference", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.9089023023843765}]}], "abstractContent": [{"text": "As away to tackle Task 1A in CL-SciSumm 2016, we introduce a composite model consisting of TFIDF and Neural Network (NN), the latter being a adaptation of the embedding model originally proposed for the Q/A domain [2, 7].", "labels": [], "entities": []}, {"text": "We discuss an experiment using a development data, results thereof, and some remaining issues.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper provides an overview of our efforts to tackle Task 1A at CL-SciSumm 2016, whose stated goal is to locate part of a reference paper (RP) most relevant to a given citation made by a citing paper (CP).", "labels": [], "entities": []}, {"text": "To give an idea of what it is about, consider.", "labels": [], "entities": []}, {"text": "In it, you have a sentence that reads: On the other hand, researchers from the visualization community have designed to a number of topic visualization techniques ...", "labels": [], "entities": []}, {"text": "Your job is to find passages in the relevant literature (what the authors call 9, 16, 17, and 18), which are most pertinent to the sentence in question.", "labels": [], "entities": []}, {"text": "(We denote a passage in referred-to papers linked with a citation by a citation target or simply target, below and throughout the paper.)", "labels": [], "entities": []}, {"text": "As away to solve the task, we work with a hybrid of two models: one that is based on TFIDF and another on a single layer Neural Network (NN).", "labels": [], "entities": []}, {"text": "Formally, the present approach looks like the following.", "labels": [], "entities": []}, {"text": "where h represents a neural network and ta TFIDF based model; dis a citation instance and r a sentence in RP.", "labels": [], "entities": []}, {"text": "For a given citation instance d, we rank every sentence r in RP in accordance with \u03c3 (while dismissing those with two or less words) and select two highest ranked sentences as a target ford.", "labels": [], "entities": []}, {"text": "We then remove redundancies in the output with an MMR-like measure: we take a candidate sentence off the output if its similarity with those preceding it exceeds a certain threshold (\u03b3).", "labels": [], "entities": []}, {"text": "Thus, the number of the output sentences will be further cut down to one in case they are found to contain redundancies.", "labels": [], "entities": []}, {"text": "In the final run, we set \u03b3 to 0.24 and \u03bb to 0.1.", "labels": [], "entities": []}, {"text": "We call the current setup as 'a neurally enhanced approach to linking citation and reference,' or NEAL for short.", "labels": [], "entities": [{"text": "linking citation and reference", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.8697975873947144}]}, {"text": "Our adding the TFIDF component to NN in \u03c3 is meant to compensate for the latter's inability to handle exact word matches effectively due to the low dimensionality of hidden layers into which word features are mapped.", "labels": [], "entities": []}, {"text": "One significant consequence of using NN is that it will relieve us from the drudgery of contriving every feature that one needs to train a classifier on: NN learns by itself whatever feature it finds necessary to satisfy an objective function.", "labels": [], "entities": []}, {"text": "In what follows, we discuss the NN portion of \u03c3, which is basically an adaptation of the neural embedding models to the current task.", "labels": [], "entities": []}, {"text": "We built the TFIDF part based on statistics collected from the final test data that CL-SciSumm 2016 released.", "labels": [], "entities": []}], "datasetContent": [{"text": "Prior to the actual run, we conducted an experiment using DSA to see how well NEAL works.", "labels": [], "entities": [{"text": "DSA", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8585502505302429}, {"text": "NEAL", "start_pos": 78, "end_pos": 82, "type": "TASK", "confidence": 0.8886984586715698}]}, {"text": "The dataset comes with ten topic clusters, each of which consists of one reference paper and a number of papers that contain citations to that paper.", "labels": [], "entities": []}, {"text": "Following a leave-one-out cross validation scheme, we split DSA into two blocks, one containing nine topic (RP) clusters and the other one.", "labels": [], "entities": []}, {"text": "We used the former for training NEAL (or h, to be precise), and tested it out on the remaining block.", "labels": [], "entities": [{"text": "NEAL", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.5641106367111206}]}, {"text": "The performance was measured by ROUGE-LCS, which produces the normalized length of a longest, possibly discontiguous, string of words shared by predicted and true targets.", "labels": [], "entities": [{"text": "ROUGE-LCS", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9849807024002075}]}, {"text": "illustrates a citation and a corresponding target (made available by CL-SciSumm 2016 as part of gold standard data).", "labels": [], "entities": [{"text": "citation", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9031386971473694}, {"text": "CL-SciSumm 2016", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.9265279769897461}, {"text": "gold standard data", "start_pos": 96, "end_pos": 114, "type": "DATASET", "confidence": 0.720086137453715}]}, {"text": "The area shaded in green represents a citation in CP and one in yellow a target in RP.", "labels": [], "entities": [{"text": "citation", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9464090466499329}]}, {"text": "A citation and a target can span an arbitrary number of sentences.", "labels": [], "entities": []}, {"text": "Some statistics on DSA are shown in.", "labels": [], "entities": [{"text": "DSA", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.5022326707839966}]}, {"text": "RP refers to a reference paper, #CP the count of relevant citing papers, |D| the the number of instances used for training.", "labels": [], "entities": [{"text": "CP", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9480466842651367}]}, {"text": "|E| indicates how many instances are processed over the entire span of epochs, and |T | the number of citation-target pairs we used to test NEAL.", "labels": [], "entities": [{"text": "T", "start_pos": 84, "end_pos": 85, "type": "METRIC", "confidence": 0.9517580270767212}, {"text": "NEAL", "start_pos": 140, "end_pos": 144, "type": "TASK", "confidence": 0.7512239813804626}]}, {"text": "Term and document frequencies (to be used for t(\u00b7, \u00b7) in \u03c3) were collected from DSA.", "labels": [], "entities": [{"text": "Term", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9563489556312561}, {"text": "DSA", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.9651009440422058}]}, {"text": "K, N v and Ne \u2212 parameters that define the shape of NN \u2212 were set to (we also used the settings for the final run)..", "labels": [], "entities": []}, {"text": "Plot of Performance vs. \u03bb for DSA.", "labels": [], "entities": [{"text": "DSA", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.48587480187416077}]}, {"text": "The title of each strip (e.g. C02-1025) represents a designator fora given reference paper, which has 9 to 25 citing papers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Dry-Run Test Set (DSA)", "labels": [], "entities": [{"text": "Dry-Run Test Set (DSA)", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.8093194762865702}]}]}