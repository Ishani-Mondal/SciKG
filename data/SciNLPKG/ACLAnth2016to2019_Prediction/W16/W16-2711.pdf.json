{"title": [{"text": "Target-Bidirectional Neural Models for Machine Transliteration", "labels": [], "entities": [{"text": "Machine Transliteration", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.6979771107435226}]}], "abstractContent": [{"text": "Our purely neural network-based system represents a paradigm shift away from the techniques based on phrase-based statistical machine translation we have used in the past.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 101, "end_pos": 145, "type": "TASK", "confidence": 0.587853267788887}]}, {"text": "The approach exploits the agreement between a pair of target-bidirectional LSTMs, in order to generate balanced targets with both good suffixes and good prefixes.", "labels": [], "entities": []}, {"text": "The evaluation results show that the method is able to match and even surpass the current state-of-the-art on most language pairs, but also exposes weaknesses on some tasks motivating further study.", "labels": [], "entities": []}, {"text": "The Janus toolkit that was used to build the systems used in the evaluation is publicly available at https://github.com/lemaoliu/Agtarbidir.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our primary system for the NEWS shared evaluation on transliteration generation is different in character from all our previous systems.", "labels": [], "entities": [{"text": "NEWS shared evaluation", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.5168146789073944}, {"text": "transliteration generation", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.8073521554470062}]}, {"text": "In past years, all our systems have been based on phrase-based statistical machine translation (PB-SMT) techniques, stemming from the system proposed in.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (PB-SMT)", "start_pos": 50, "end_pos": 103, "type": "TASK", "confidence": 0.6717778486864907}]}, {"text": "This year's system is a pure end-to-end neural network transducer.", "labels": [], "entities": []}, {"text": "In () auxiliary neural network language models (both monolingual and bilingual () were introduced as features to augment the log-linear model of a phrasebased transduction system, and led to modest gains in system performance.", "labels": [], "entities": []}, {"text": "In the NEWS 2015 workshop () neural transliteration systems using attention-based sequence-to-sequence neural network transducers ( were applied to transliteration generation.", "labels": [], "entities": [{"text": "NEWS 2015 workshop", "start_pos": 7, "end_pos": 25, "type": "DATASET", "confidence": 0.830829381942749}, {"text": "transliteration generation", "start_pos": 148, "end_pos": 174, "type": "TASK", "confidence": 0.9509965777397156}]}, {"text": "In isolation, the performance was found to be lower than that of the phrase-based system on all of the tasks, however we observed that the neural network transducer was very effective when used as a model for re-scoring the output of the phrasebased transduction process, and this led to respectable improvements relative to previous systems on most of the tasks.", "labels": [], "entities": []}, {"text": "Our focus this year has been on the development of an end-to-end purely neural network-based system capable of competitive performance.", "labels": [], "entities": []}, {"text": "The changes and improvements over the sequence-tosequence neural transducer used in NEWS2015 are as follows: \u2022 A target-bidirectional agreement model was employed.", "labels": [], "entities": [{"text": "NEWS2015", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9173457622528076}]}, {"text": "\u2022 Ensembles of neural networks were used rather than just a single network.", "labels": [], "entities": []}, {"text": "\u2022 The ensembles were selected from different training runs and different training epochs according to their performance on development (and test) data.", "labels": [], "entities": []}, {"text": "In all our experiments we have taken a strictly language independent approach.", "labels": [], "entities": []}, {"text": "Each of the language pairs was processed automatically from the character sequence representation supplied for the shared tasks, with no language specific treatment for any of the language pairs.", "labels": [], "entities": []}, {"text": "Furthermore no preprocessing was performed on any of the data with the exception of uppercasing the English to ensure consistency among the data sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "The official scores for our system are given in Table 1, alongside the scores of our previous systems on the same test set, and the scores of the official baseline system.", "labels": [], "entities": []}, {"text": "The highest scores are highlighted in bold, and it is clear that this year's system has attained higher accuracy than the systems from previous years on most of the language pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.999242901802063}]}, {"text": "For some pairs, such as EnglishKatakana, English-Thai and Thai-English, the improvement is substantial.", "labels": [], "entities": []}, {"text": "However, there are also tasks in which the neural system was notable to match the performance of the previous system, notably English-Japanese Kanji, English-Hangul and Arabic-English.", "labels": [], "entities": []}, {"text": "The first two of these tasks have quite large vocabularies on the target side, and this may make them less suitable fora neural approach.", "labels": [], "entities": []}, {"text": "The Arabic-English task has no such issues, and furthermore has afar larger training corpus available which ought to favor the neural method, however it differs from the other tasks in that short vowels are not represented in written Arabic, but must still be generated on the target side.", "labels": [], "entities": []}, {"text": "Further research is necessary to determine the true cause, but our conjecture is that phrase-based systems, which effectively memorize the training data in a piecewise manner, are consequently more successful on this task than neural networks which are geared more towards generalization rather than memorization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The official evaluation results in terms of the top-1 accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9878315329551697}]}]}