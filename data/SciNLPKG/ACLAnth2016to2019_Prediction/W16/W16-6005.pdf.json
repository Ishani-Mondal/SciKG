{"title": [{"text": "Unsupervised Event Coreference for Abstract Words", "labels": [], "entities": [{"text": "Unsupervised Event Coreference", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5583425561587015}]}], "abstractContent": [{"text": "We introduce a novel approach for resolving coreference when the trigger word refers to multiple (sometimes non-contiguous) clauses.", "labels": [], "entities": [{"text": "resolving coreference", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.6628173589706421}]}, {"text": "Our approach is completely unsupervised, and our experiments show that Neural Network models perform much better (about 20% more accurate) than traditional feature-rich baseline models.", "labels": [], "entities": []}, {"text": "We also present anew dataset for Biomedical Language Processing which, with only about 25% of the original corpus vocabulary , still captures the essential distributional semantics of the corpus.", "labels": [], "entities": [{"text": "Biomedical Language Processing", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8214446902275085}]}], "introductionContent": [{"text": "Event coreference is a key module in many NLP applications, especially those that involve multisentence discourse.", "labels": [], "entities": [{"text": "Event coreference", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.717636376619339}]}, {"text": "Current event coreference systems restrict the problem to finding a correspondence between trigger words or phrases and their fully coreferent event (word or phrase).", "labels": [], "entities": []}, {"text": "This approach is rather limited since it does not handle the case when the trigger refers to several events as a group, as in We worked hard all our lives.", "labels": [], "entities": []}, {"text": "But one year we went on vacation.", "labels": [], "entities": []}, {"text": "There was boating, crazy adventure sports, and pro-golfing.", "labels": [], "entities": []}, {"text": "We also spent time in the evenings strolling around the park.", "labels": [], "entities": []}, {"text": "But eventually we had to go home.", "labels": [], "entities": []}, {"text": "There couldn't have been a better vacation.", "labels": [], "entities": []}, {"text": "In this paper we generalize the idea of coreference to 3 levels based on the degree of abstraction of the coreference trigger: 1.", "labels": [], "entities": []}, {"text": "Level 1 -Direct Mention: The trigger phrase is specific and usually matches the referring event(s) word-for-word or phrase-for-phrase.", "labels": [], "entities": []}, {"text": "2. Level 2 -Single Clause: While there is a similar word-to-phrase or word-to-word relationship as in level 1, the trigger is a more generic event compared to level 1.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Cloze-test evaluation is inspired by the reading comprehension evaluation from QuestionAnswering research.", "labels": [], "entities": [{"text": "QuestionAnswering research", "start_pos": 83, "end_pos": 109, "type": "DATASET", "confidence": 0.8923228085041046}]}, {"text": "In this evaluation, the system first reads a passage and then attempts to predict missing words from sentences that contain information from the passage.", "labels": [], "entities": []}, {"text": "For our evaluation, we use a slightly modified version of the Cloze-test, in which the model is trained for each coreference with sentences that appear before and after the coreference.", "labels": [], "entities": []}, {"text": "Currently, we arbitrarily limit the number of sentences in the antecedent and precedent span for coreference to 5.", "labels": [], "entities": [{"text": "coreference", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.9617248177528381}]}, {"text": "Also, we consider only 6 labels for now, namely these changes, these responses, this analysis, this context,this finding, this observation.", "labels": [], "entities": []}, {"text": "In order to maintain the even distribution of coreference candidates, we derived our dataset from the PubMed corpus by selecting 1000 samples of each of the 6 coreferent labels fora total of 6000 training samples, each sample containing the coreference trigger and we pick antecedent sentences based on the following criteria.", "labels": [], "entities": [{"text": "PubMed corpus", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.970522552728653}]}, {"text": "If the coreference occurs in the same paragraph, the number of antecedent sentences are limited to sentences from the start of the paragraph or upto five antecedent sentence candidates otherwise.", "labels": [], "entities": []}, {"text": "For the MLP model, we use a 70-30 train-test split and apply the early stopping criteria based on accuracy drop on the validation dataset.", "labels": [], "entities": [{"text": "MLP", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.5369805693626404}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9988622665405273}]}], "tableCaptions": [{"text": " Table 4: Results for various baselines and our work", "labels": [], "entities": []}]}