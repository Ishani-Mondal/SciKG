{"title": [], "abstractContent": [{"text": "We propose a method for evaluating em-beddings against dictionaries with tensor hundreds of thousands of entries, covering the entire gamut of the vocabulary.", "labels": [], "entities": []}], "introductionContent": [{"text": "Continuous vector representations (embeddings) are, to a remarkable extent, supplementing and potentially taking over the role of detail dictionaries in abroad variety of tasks ranging from POS tagging) and parsing) to MT (, and beyond).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 190, "end_pos": 201, "type": "TASK", "confidence": 0.8517161905765533}, {"text": "parsing", "start_pos": 207, "end_pos": 214, "type": "TASK", "confidence": 0.7438703179359436}, {"text": "MT", "start_pos": 219, "end_pos": 221, "type": "TASK", "confidence": 0.9254963994026184}]}, {"text": "Yet an evaluation method that directly compares embeddings on their ability to handle word similarity at the entire breadth of a dictionary has been lacking, which is all the more regrettable in light of the fact that embeddings are normally generated from gigaword or larger corpora, while the state of the art test sets surveyed in range between a low of 30 (MC-30) and a high of 3,000 word pairs (MEN).", "labels": [], "entities": [{"text": "word pairs (MEN)", "start_pos": 388, "end_pos": 404, "type": "METRIC", "confidence": 0.6140076220035553}]}, {"text": "We propose to develop a dictionary-based standard in two steps.", "labels": [], "entities": []}, {"text": "First, given a dictionary such as the freely available Collins-COBUILD, which has over 77,400 headwords, or Wiktionary (162,400 headwords), we compute a frequency list F that lists the probabilities of the headwords (this is standard, and discussed only briefly), and a dense similarity matrix M or an embedding \u03c8, this is discussed in Section 2.", "labels": [], "entities": [{"text": "Collins-COBUILD", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.9861491918563843}]}, {"text": "Next, in Section 3 we consider an arbitrary embedding \u03c6, and we systematically compare both its frequency and its similarity predictions to the gold standard embodied in F and \u03c8, building on the insights of.", "labels": [], "entities": [{"text": "similarity predictions", "start_pos": 114, "end_pos": 136, "type": "METRIC", "confidence": 0.9411014020442963}]}, {"text": "Pilot studies conducted along these lines are discussed in Section 4.", "labels": [], "entities": []}, {"text": "Before turning to the details, in the rest of this Introduction we attempt to evaluate the proposed evaluation itself, primarily in terms of the criteria listed in the call.", "labels": [], "entities": []}, {"text": "As we shall see, our method is highly replicable for other researchers for English, and to the extent monolingual dictionaries are available, for other other languages as well.", "labels": [], "entities": []}, {"text": "Low resource languages will typically lack a monolingual dictionary, but this is less of a perceptible problem in that they also lack larger corpora so building robust embeddings is already out of the question for these.", "labels": [], "entities": []}, {"text": "The costs are minimal, since we are just running software on preexisting dictionaries.", "labels": [], "entities": []}, {"text": "Initially, dictionaries are hard to assemble, require a great deal of manual labor, and are often copyrighted, but here our point is to leverage the manual (often crowdsourced) work that they already embody.", "labels": [], "entities": []}, {"text": "The proposed algorithm, as we present it here, is aimed primarily at word-level evaluation, but there are standard methods for extending these from word to sentence similarity (.", "labels": [], "entities": []}, {"text": "Perhaps the most attractive downstream application we see is MT, in particular word sense disambiguation during translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9927614331245422}, {"text": "word sense disambiguation during translation", "start_pos": 79, "end_pos": 123, "type": "TASK", "confidence": 0.6762120604515076}]}, {"text": "As for linguistic/semantic/psychological properties, dictionaries, both mono-and bilingual, are crucial resources not only for humans (language learners, translators, etc.) but also fora variety of NLP applications, including MT, cross-lingual information retrieval, cross-lingual QA, computer-assisted language learning, and many more.", "labels": [], "entities": [{"text": "MT", "start_pos": 226, "end_pos": 228, "type": "TASK", "confidence": 0.9894022941589355}, {"text": "cross-lingual information retrieval", "start_pos": 230, "end_pos": 265, "type": "TASK", "confidence": 0.6217305560906728}, {"text": "cross-lingual QA", "start_pos": 267, "end_pos": 283, "type": "TASK", "confidence": 0.6291106045246124}]}, {"text": "The mandate of lexicographers is to capture a huge number of linguistic phenomena ranging from gross synonymy to subtle meaning distinctions, and at the semantic level the inter-annotator agreement is very high, a point we discuss in greater detail below.", "labels": [], "entities": []}, {"text": "quote that \"human linguistic judgments (...) are subject to over 50 potential linguistic, psychologi-cal, and social confounds\", and many of these taint the crowd-sourced dictionaries, but lexicographers are annotators of a highly trained sort, and their work gives us valuable data, as near to laboratory purity as it gets.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 S R for dictionary-based embeddings", "labels": [], "entities": []}, {"text": " Table 2: Comparing embeddings by Simlex-999, dictionary S R , MEN, and RareWord", "labels": [], "entities": [{"text": "RareWord", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.937221348285675}]}]}