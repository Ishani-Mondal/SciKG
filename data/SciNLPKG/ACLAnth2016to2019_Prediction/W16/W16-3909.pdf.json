{"title": [{"text": "Name Variation in Community Question Answering Systems", "labels": [], "entities": [{"text": "Name Variation", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8670101463794708}, {"text": "Community Question Answering", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.5688090721766154}]}], "abstractContent": [{"text": "Community question answering systems are forums where users can ask and answer questions in various categories.", "labels": [], "entities": [{"text": "Community question answering", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5719027916590372}]}, {"text": "Answers, Quora, and Stack Overflow.", "labels": [], "entities": []}, {"text": "A common challenge with such systems is that a significant percentage of asked questions are left unanswered.", "labels": [], "entities": []}, {"text": "In this paper, we propose an algorithm to reduce the number of unanswered questions in Yahoo!", "labels": [], "entities": []}, {"text": "Answers by reusing the answer to the most similar past resolved question to the unanswered question , from the site.", "labels": [], "entities": []}, {"text": "Semantically similar questions could be worded differently, thereby making it difficult to find questions that have shared needs.", "labels": [], "entities": []}, {"text": "For example, Who is the best player for the Reds? and Who is currently the biggest star at Manchester United?", "labels": [], "entities": []}, {"text": "have a shared need but are worded differently; also, Reds and Manchester United are used to refer to the soccer team Manchester United football club.", "labels": [], "entities": [{"text": "Manchester United", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.9049071967601776}]}, {"text": "In this research, we focus on question categories that contain a large number of named entities and entity name variations.", "labels": [], "entities": []}, {"text": "We show that in these categories, entity linking can be used to identify relevant past resolved questions with shared needs as a given question by disambiguating named entities and matching these questions based on the dis-ambiguated entities, identified entities, and knowledge base information related to these entities.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7398431599140167}]}, {"text": "We evaluated our algorithm on anew dataset constructed from Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.8974929749965668}]}, {"text": "The dataset contains annotated question pairs, (Q given , [Q past , Answer]).", "labels": [], "entities": []}, {"text": "We carried out experiments on several question categories and show that an entity-based approach gives good performance when searching for similar questions in entity rich categories.", "labels": [], "entities": []}], "introductionContent": [{"text": "In community question answering (CQA) systems, users prefer asking other users questions because (I) their questions are personal and require a direct answer from users with similar experiences or users familiar with the question (II) no single web page can answer their question, and (III) users want to communicate and exchange ideas with other users.", "labels": [], "entities": [{"text": "community question answering (CQA)", "start_pos": 3, "end_pos": 37, "type": "TASK", "confidence": 0.8153332869211832}]}, {"text": "One of the challenges with such systems is that some questions are left unanswered because: \u2022 they are short and lack relevant content \u2022 they are not clearly expressed \u2022 they are not appropriately assigned to a user that is able to answer the question Approximately 15% of incoming English questions in Yahoo!", "labels": [], "entities": []}, {"text": "Answers do not receive any answer and leave the user that asked the question (asker) unsatisfied.", "labels": [], "entities": []}, {"text": "One approach to reducing).", "labels": [], "entities": []}, {"text": "Another approach automatically extracts answers from a knowledge base (KB) such as Wikipedia, text passage, or the web (.", "labels": [], "entities": []}, {"text": "In certain question categories in Yahoo!", "labels": [], "entities": []}, {"text": "Answers, approximately 25% of questions are recurrent.", "labels": [], "entities": []}, {"text": "A third approach takes advantage of this question recurrence by reusing past resolved questions (PARQ) from within Yahoo!", "labels": [], "entities": []}, {"text": "Answers to satisfy unanswered questions.", "labels": [], "entities": []}, {"text": "used this third approach to satisfy unanswered questions in the Beauty & Style, Health, and Pets question categories by matching new questions to PARQ's if they had a cosine similarity score above a threshold (0.9); features were then extracted from the new question and PARQ's to train a classifier.", "labels": [], "entities": []}, {"text": "Certain question categories such as Sports have a high occurrence of named entities and entity name variations.", "labels": [], "entities": [{"text": "Sports", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.954016387462616}]}, {"text": "For example, a sports team can be referred to by its official name, the name of the city it plays in or by any of several nicknames.", "labels": [], "entities": []}, {"text": "Also, the vocabulary in questions in these categories can be diverse and questions are often very short ().", "labels": [], "entities": []}, {"text": "The contribution of this paper is to propose an alternative approach to reducing the number of unanswered questions in question categories that contain a large number of entities by taking advantage of the recent successes in entity linking.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 226, "end_pos": 240, "type": "TASK", "confidence": 0.7144917398691177}]}, {"text": "We now have systems that can disambiguate named entities to a KB.", "labels": [], "entities": []}, {"text": "Matching questions and answers based on these disambiguated entities, entities, and KB information related to these entities finds most of the relevant answers to a given question.", "labels": [], "entities": []}, {"text": "We investigate the validity of using an entity-based approach in entity rich categories by first analyzing", "labels": [], "entities": []}], "datasetContent": [{"text": "For this research we used a repository of PARQ from Yahoo!", "labels": [], "entities": [{"text": "PARQ from Yahoo!", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.7775838673114777}]}, {"text": "Since we are interested in finding PARQ with answers that can satisfy a given question, we selected the best answers for each question in  We measure the precision, recall, and accuracy of the proposed algorithm.", "labels": [], "entities": [{"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9996088147163391}, {"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9991980195045471}, {"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.999512791633606}]}, {"text": "Precision: the fraction of returned answers that are correct i.e. potential answers.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9923753142356873}]}, {"text": "Recall: the fraction of the labelled potential answer question pairs that where returned by the system.", "labels": [], "entities": []}, {"text": "Accuracy: the overall fraction of potential answer question pairs classified correctly.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.993930459022522}]}], "tableCaptions": [{"text": " Table 1: Number of questions with named entities or entity variations out of 150 questions from each  category", "labels": [], "entities": []}, {"text": " Table 5. This shows that iden-", "labels": [], "entities": []}, {"text": " Table 5: Precision and recall of ENTITY-ALCHEMY and two baselines", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9968559741973877}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9993543028831482}, {"text": "ENTITY-ALCHEMY", "start_pos": 34, "end_pos": 48, "type": "METRIC", "confidence": 0.9012911915779114}]}, {"text": " Table 6: Accuracy of ENTITY-ALCHEMY and the baselines", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992824196815491}, {"text": "ENTITY-ALCHEMY", "start_pos": 22, "end_pos": 36, "type": "METRIC", "confidence": 0.9198272824287415}]}]}