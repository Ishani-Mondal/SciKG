{"title": [{"text": "Automatic Construction of Large Readability Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "This work presents a framework for the automatic construction of large Web corpora classified by readability level.", "labels": [], "entities": []}, {"text": "We compare different Machine Learning classifiers for the task of readabil-ity assessment focusing on Portuguese and English texts, analysing the impact of variables like the feature inventory used in the resulting corpus.", "labels": [], "entities": []}, {"text": "Ina comparison between shallow and deeper features, the former already produce F-measures of over 0.75 for Portuguese texts, but the use of additional features results in even better results, inmost cases.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9984322190284729}]}, {"text": "For English, shallow features also perform well as do classic readability formulas.", "labels": [], "entities": []}, {"text": "Comparing different classifiers for the task, logistic regression obtained, in general, the best results, but with considerable differences between the results for two and those for three-classes, especially regarding the intermediary class.", "labels": [], "entities": []}, {"text": "Given the large scale of the resulting corpus, for evaluation we adopt the agreement between different classifiers as an indication of readability assessment certainty.", "labels": [], "entities": []}, {"text": "As a result of this work, a large corpus for Brazilian Portuguese was built 1 , including 1.7 million documents and about 1.6 billion tokens, already parsed and annotated with 134 different textual attributes, along with the agreement among the various classifiers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text readability assessment refers to measuring how easy it is fora reader to read and understand a given text.", "labels": [], "entities": [{"text": "Text readability assessment", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7836097876230875}]}, {"text": "In this context methods for automatic readability assessment have received considerable attention from the research community).", "labels": [], "entities": [{"text": "automatic readability assessment", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.6869601905345917}]}, {"text": "The task of attributing a readability level to a text has a wide range of applications, including support for student reading material selection or help for clinical patients.", "labels": [], "entities": [{"text": "student reading material selection", "start_pos": 110, "end_pos": 144, "type": "TASK", "confidence": 0.5980213731527328}]}, {"text": "It can also be used for ensuring that instructions and policies are written in an easily comprehensible way even for readers with low education.", "labels": [], "entities": []}, {"text": "It can also contribute to the task of text simplification, evaluating the obtained version to indicate if further simplification is needed ( . Recently, authors such as, and  have started treating this task as one of text classification, using corpora manually annotated with readability classifications to train automatic learning models, based on a large set of text metrics, including deeper features, for example derived from n-gram language models and parse trees.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7666075527667999}, {"text": "text classification", "start_pos": 217, "end_pos": 236, "type": "TASK", "confidence": 0.7437343597412109}]}, {"text": "However, an important limitation to this approach is the small availability of reliably annotated train data.", "labels": [], "entities": []}, {"text": "Moreover, this task is known to be very subjective, and even human annotators present a high disagreement rate in their evaluations.", "labels": [], "entities": []}, {"text": "In this work, we aim to develop large corpora classified by readability levels.", "labels": [], "entities": []}, {"text": "To achieve this objective we present a study of different Machine Learning approaches to the task of readability assessment of texts, focusing on Portuguese, and apply the relatively recent concept of building corpora from the Web () to automatically generate large corpora classified by readability levels.", "labels": [], "entities": []}, {"text": "For that, we follow the framework proposed by Wagner, where a readability classifier was incorporated into a crawler, but changing the classifier position in the pipeline so that we can work with both low and high-cost complexity features.", "labels": [], "entities": []}, {"text": "We also experiment with learning models trained on several different reference corpora, in both Portuguese and English, and investigate the relevance of the agreement between them.", "labels": [], "entities": []}, {"text": "We focus our study on two hypothesis: (H1) a learning model trained in a reference annotated corpus is able to classify anew corpus so that its classes present significant linguistic differences and (H2) the use of syntactic attributes contributes to a better classification.", "labels": [], "entities": []}, {"text": "As a result of this work, anew Portuguese corpus of around 1.6 billion tokens was built and annotated with four different readability classifiers.", "labels": [], "entities": [{"text": "Portuguese corpus", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.8300226628780365}]}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss some relevant work in the literature and, in Section 3, we present our materials and methods, especially our training data, features and classification algorithms and our Web corpus collection framework.", "labels": [], "entities": [{"text": "Web corpus collection framework", "start_pos": 196, "end_pos": 227, "type": "DATASET", "confidence": 0.811223492026329}]}, {"text": "In Section 4, we apply our methodology and validate our hypothesis trough a series of experiments.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 we present our conclusions and ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present a comparison of different features categories (Section 4.1); evaluation of classification models (Section 4.2); and assess the generalisation of these models (Section 4.3).", "labels": [], "entities": []}, {"text": "Finally, in Section 4.4, we create an large corpus of Web content, which we classify with our models in Section 4.5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Description of the readability corpora", "labels": [], "entities": []}, {"text": " Table 2: Average rank of feature classes in the different training corpora (smaller values indicate a bigger  relevance to a given class)", "labels": [], "entities": []}, {"text": " Table 5: Average difference per feature category between the simple and complex classes in the PSFL  train corpus and in different classifications of our Web corpus", "labels": [], "entities": [{"text": "PSFL  train corpus", "start_pos": 96, "end_pos": 114, "type": "DATASET", "confidence": 0.8712680141131083}]}]}