{"title": [{"text": "Can Text Simplification Help Machine Translation?", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.734315812587738}, {"text": "Machine Translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7193925827741623}]}], "abstractContent": [{"text": "This article explores the use of text simplification as a pre-processing step for statistical machine translation of grammatically complex under-resourced languages.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7044467628002167}, {"text": "statistical machine translation of grammatically complex under-resourced languages", "start_pos": 82, "end_pos": 164, "type": "TASK", "confidence": 0.8192016780376434}]}, {"text": "Our experiments on English-to-Serbian translation show that this approach can improve grammaticality (fluency) of the translation output and reduce technical post-editing effort (number of post-edit operations).", "labels": [], "entities": [{"text": "English-to-Serbian translation", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.6997990608215332}]}, {"text": "Furthermore, the use of more aggressive text simplification methods (which do not only simplify the given sentence but also discard irrelevant information thus producing syntactically very simple sentences) also improves meaning preservation (adequacy) of the translation output.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation for under-resourced languages is facing a number of problems.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8444584906101227}]}, {"text": "First, there is not enough parallel data to build robust statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.7850128610928854}]}, {"text": "Second, most of these languages (including Serbian) have a very rich morphology and suffer from data sparsity when it comes to less frequently used cases, tenses, etc.", "labels": [], "entities": []}, {"text": "Third, there is a number of syntactic differences which are difficult to capture.", "labels": [], "entities": []}, {"text": "For English-to-Serbian SMT, a number of language related problems has been identified so far).", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.6446331143379211}]}, {"text": "Most of them are related to syntactic differences, e.g. missing verb parts due to distinct structure of certain verb tenses, incorrect prepositions, or incorrect translations of English sequences of nouns.", "labels": [], "entities": []}, {"text": "In this paper, we explore whether it is possible to improve the performance of the machine translation for under-resourced languages by introducing a pre-processing step in which source sentences are first simplified by an automatic text simplification (ATS) system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7261561006307602}, {"text": "text simplification (ATS)", "start_pos": 233, "end_pos": 258, "type": "TASK", "confidence": 0.7371537566184998}]}, {"text": "We focus on English-to-Serbian MT and apply two state-of-the-art ATS systems as a pre-processing step for simplifying the original English sentence before feeding it into a phrase-based SMT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.7483261227607727}, {"text": "ATS", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.8545186519622803}, {"text": "SMT", "start_pos": 186, "end_pos": 189, "type": "TASK", "confidence": 0.8708885312080383}]}, {"text": "We exploit two different types of ATS systems, a more conservative one (which, while simplifying the input sentence lexically and syntactically, retain all the information contained in the original sentence), and the more aggressive one (which, while sim-plifying the input sentence lexically and syntactically, also tries to reduce the amount of information by discarding irrelevant information and high-level details).", "labels": [], "entities": []}, {"text": "In this way, we address two different usage scenarios in MT: (1) when it is important to maintain all the information contained in the source text (e.g. translations of whole texts or documents); and (2) when it is enough to get a gist of the source text (e.g. skimming through news articles and looking for the most important news).", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9742753505706787}]}, {"text": "The results of the human evaluation of the news articles translated using the two above-mentioned approaches, in terms of grammaticality (fluency) and meaning preservation (accuracy) of the output, and the analysis of the post-editing effort (number of post-edit operations) shows that both approaches improve the MT output.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.6498554348945618}, {"text": "MT", "start_pos": 314, "end_pos": 316, "type": "TASK", "confidence": 0.9951850771903992}]}, {"text": "The remainder of the article is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly reports on the existing approaches to automatic text simplification and motivates our choice of ATS systems.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7345505207777023}]}, {"text": "Section 3 describes the chosen ATS systems in more details, presents the datasets and the SMT system used in experiments and describes the evaluation procedure.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9742903709411621}]}, {"text": "Section 4 presents and discusses the results of our experiments, while Section 5 summarises the main findings and presents ideas for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this study, we use two state-of-the-art ATS systems: 1.", "labels": [], "entities": [{"text": "ATS", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.7982100248336792}]}, {"text": "TS-A: A combination of lexical TS system (Glava\u0161 and\u0160tajnerand\u02c7and\u0160tajner, 2015) with the EventSimplify (Glava\u0161 and\u0160tajnerand\u02c7and\u0160tajner, 2013) which performs syntactic simplification with a significant content reduction.", "labels": [], "entities": []}, {"text": "This is the most \"aggressive\" system of all above-mentioned systems which perform content reduction (Section 2), i.e. it is the system which performs the highest level of content reduction and achieves the most readable (simplest) output (due to a high number of sentence splitting operations).", "labels": [], "entities": [{"text": "content reduction", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7420166879892349}, {"text": "content reduction", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.6987846493721008}, {"text": "sentence splitting", "start_pos": 263, "end_pos": 281, "type": "TASK", "confidence": 0.7288330644369125}]}, {"text": "2. TS-C: The lexico-syntactic TS system proposed by which belong to the \"conservative\" ATS systems which do not perform any content reduction and thus, completely preserve the original meaning of the sentence; We used 100 news articles from the EMM NewsBrief 3 for which the output of the EventSimplify (Glava\u0161 and\u0160tajnerand\u02c7and\u0160tajner, 2013) ATS system was freely available.", "labels": [], "entities": [{"text": "EMM NewsBrief 3", "start_pos": 245, "end_pos": 260, "type": "DATASET", "confidence": 0.9415738781293234}, {"text": "EventSimplify (Glava\u0161 and\u0160tajnerand\u02c7and\u0160tajner, 2013) ATS", "start_pos": 289, "end_pos": 346, "type": "DATASET", "confidence": 0.7516869530081749}]}, {"text": "We further focused on the output of the event-wise simplification scheme (which achieved the highest readability of all four provided schemes) and applied the lexical simplification system (Glava\u0161 and\u0160tajnerand\u02c7and\u0160tajner, 2015) on top of it in order to obtained a full simplification system which encompasses lexical simplification, syntactic simplification and content reduction (TS-A).", "labels": [], "entities": [{"text": "syntactic simplification", "start_pos": 334, "end_pos": 358, "type": "TASK", "confidence": 0.691981703042984}, {"text": "content reduction", "start_pos": 363, "end_pos": 380, "type": "TASK", "confidence": 0.6846740245819092}]}, {"text": "Next, we applied the TS-C system on all those 100 original articles.", "labels": [], "entities": [{"text": "TS-C", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.8931353092193604}]}, {"text": "From the initial set of 100 news articles, we randomly selected 65 original sentences and evaluated all translation outputs (from original sentences, and TS-A and TS-C systems, which led to a total of 195 target sentences) with respect to the following aspects: 1.", "labels": [], "entities": []}, {"text": "adequacy, i.e. meaning preservation 2.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.6696098744869232}]}, {"text": "fluency, i.e. grammaticality 3.", "labels": [], "entities": []}, {"text": "technical post-editing effort, i.e. amount of necessary edit operations Each of the tasks has been carried out separately, i.e. the evaluation of adequacy and fluency were carried out in two separate passes, and post-editing was carried out in the third pass.", "labels": [], "entities": []}, {"text": "-The raw edit counts and edit rates (raw counts normalised with the segment length) were calculated using Hjerson (Popovi\u00b4cPopovi\u00b4c, 2011) for: \u2022 five classes of edits/errors \u2022 all edit operations Reference translations were not available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Average scores for adequacy and fluency (first row) and percentage of sentences for each  of the five scores (1-5).", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.975601851940155}]}, {"text": " Table 4. Percentage of changes in adequacy and fluency scores.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9936624765396118}]}, {"text": " Table 6. Raw counts and edit rates (%) normalised with the segment length.", "labels": [], "entities": [{"text": "edit rates", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9625310301780701}]}, {"text": " Table 7. Percentage of sentences with better/worse/same sentences with respect to adequacy (A),  fluency (F), edit rate (ER) and raw edit counts (REC).", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9597349166870117}, {"text": "adequacy (A)", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.7980401664972305}, {"text": "fluency (F)", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.7305500656366348}, {"text": "edit rate (ER)", "start_pos": 111, "end_pos": 125, "type": "METRIC", "confidence": 0.9548390746116638}, {"text": "raw edit counts (REC)", "start_pos": 130, "end_pos": 151, "type": "METRIC", "confidence": 0.8401288688182831}]}]}