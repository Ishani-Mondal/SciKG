{"title": [{"text": "SoMaJo: State-of-the-art tokenization for German web and social media texts", "labels": [], "entities": [{"text": "tokenization", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7587088942527771}]}], "abstractContent": [{"text": "In this paper we describe SoMaJo, a rule-based tokenizer for German web and social media texts that was the best-performing system in the EmpiriST 2015 shared task with an average F 1-score of 99.57.", "labels": [], "entities": [{"text": "EmpiriST 2015 shared task", "start_pos": 138, "end_pos": 163, "type": "DATASET", "confidence": 0.8185348957777023}, {"text": "F 1-score", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9901174902915955}]}, {"text": "We give an overview of the system and the phenomena its rules cover, as well as a detailed error analysis.", "labels": [], "entities": [{"text": "error", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.9565874338150024}]}, {"text": "The tokenizer is available as free software.", "labels": [], "entities": [{"text": "tokenizer", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9555557370185852}]}], "introductionContent": [{"text": "At first sight, tokenization is not only boring but also trivial.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9832181930541992}]}, {"text": "Humans have few problems with this task for at least two reasons: (1) They are experts at pattern-finding (see, for example,.", "labels": [], "entities": []}, {"text": "Thus, whether the form \"your\" in an English Facebook post is to be read as one unit (the possessive determiner) or as two (a common misspelling of \"you're\"), usually causes less problems due to the highly disambiguating grammatical context.", "labels": [], "entities": []}, {"text": "(2) They are happy to accept meaningful units without having to determine the exact number of units.", "labels": [], "entities": []}, {"text": "While most tokenization guidelines force us to treat \"ice cream\" as two tokens and \"ice-cream\" as one token, there often is no difference to native speakers -though it is possible to predict the spelling to some extent based on linguistic context, frequency, etc.", "labels": [], "entities": []}, {"text": "However, given the layered approach typically taken by NLP pipelines, no analysis of the grammatical context is available at the time when tokenization takes place since tokenization is one of the first steps in an NLP text processing pipeline, often only preceded by sentence splitting.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 268, "end_pos": 286, "type": "TASK", "confidence": 0.7268026769161224}]}, {"text": "However, tokenization is not fully independent of sentence splitting due to the ambiguity of some punctuation marks, most notoriously the baseline dot, which can for instance occur as (1) period/full stop to mark the end of a sentence, (2) marker of abbreviated forms, (3) decimal mark separating the integer from the fractional part of a number, (4) separator of host name, subdomain, domain, top-level domain in Internet addresses, (5) part of a so-called horizontal ellipsis (\"...\").", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7568359673023224}]}, {"text": "When all these restrictions are in place, tokenization immediately becomes more challenging as a task, also for humans.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9791702032089233}]}, {"text": "Thus whether the string \"No.\" should be treated as one token or as two is impossible to decide out of context, since it could be a short answer to a question (\"Would you like to join us for lunch?\"", "labels": [], "entities": []}, {"text": "-\"No.\") or it can bean abbreviation for \"number\" (\"No. 6\").", "labels": [], "entities": []}, {"text": "In the former case, tokenization should identify two tokens, in the latter only one.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.9760631918907166}]}, {"text": "Thus the challenge for any tokenizer is to make use of the linguistic context to disambiguate potentially ambiguous forms even though no higher-level grammatical analysis (i. e. PoS-tagging, lemmatization or even syntactic or semantic analysis) is available.", "labels": [], "entities": []}, {"text": "Ina way, some of the work done by these high-level tools is thus duplicated in the tokenizer, e. g. identifying numbers, identifying punctuation, identifying proper names (in English) or nouns in general (in German) based on capitalization, where necessary for the tokenization.", "labels": [], "entities": []}, {"text": "Of course, an extremely large proportion of tokenization is indeed straightforward.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.9851647615432739}]}, {"text": "A simple split on white space and common punctuation marks will result in an average F 1 -score of 96.73 on the test data set used for the present task (cf. Section 4).", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.991985097527504}]}, {"text": "However, the amount of work that is required to get closer to 100% is inversely proportional to the effect size of the improvements that can be achieved, which means that the bulk of this paper is devoted to the remaining 3.27%.", "labels": [], "entities": []}, {"text": "The EmpiriST 2015 shared task on automatic linguistic annotation of computer-mediated communication / social media consists of two subtasks that deal with NLP for web and social media texts: (1) Tokenization and (2) part-of-speech tagging.", "labels": [], "entities": [{"text": "EmpiriST 2015 shared task", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.923956423997879}, {"text": "automatic linguistic annotation of computer-mediated communication / social media", "start_pos": 33, "end_pos": 114, "type": "TASK", "confidence": 0.7022685176796384}, {"text": "part-of-speech tagging", "start_pos": 216, "end_pos": 238, "type": "TASK", "confidence": 0.734523355960846}]}, {"text": "We participated in the first subtask and developed a rule-based tokenizer that implements the EmpiriST 2015 tokenization guidelines.", "labels": [], "entities": [{"text": "EmpiriST 2015 tokenization guidelines", "start_pos": 94, "end_pos": 131, "type": "DATASET", "confidence": 0.8610449135303497}]}, {"text": "Our system, SoMaJo, won the shared task and is freely available from PyPI, the Python Package Index.", "labels": [], "entities": [{"text": "Python Package Index", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.7608643770217896}]}], "datasetContent": [{"text": "The performance of the systems participating in the shared task was evaluated using precision, recall and F 1 -score (.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9997302889823914}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9996386766433716}, {"text": "F 1 -score", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9875263571739197}]}, {"text": "These measures are based on the actual token boundaries (B actual ), i. e. the token boundaries in the gold standard, and the token boundaries identified by the system (B identified ).", "labels": [], "entities": []}], "tableCaptions": []}