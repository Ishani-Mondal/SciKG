{"title": [{"text": "Impact of MWE Resources on Multiword Recognition", "labels": [], "entities": [{"text": "Multiword Recognition", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.860761433839798}]}], "abstractContent": [{"text": "In this paper, we demonstrate the impact of Multiword Expression (MWE) resources in the task of MWE recognition in text.", "labels": [], "entities": [{"text": "MWE recognition", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.9812085032463074}]}, {"text": "We present results based on the Wiki50 corpus for MWE resources, generated using unsupervised methods from raw text and resources that are extracted using manual text markup and lexical resources.", "labels": [], "entities": [{"text": "Wiki50 corpus", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9364582002162933}]}, {"text": "We show that resources acquired from manual annotation yield the best MWE tagging performance.", "labels": [], "entities": [{"text": "MWE tagging", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.945318192243576}]}, {"text": "However, a more fine-grained analysis that differentiates MWEs according to their part of speech (POS) reveals that automatically acquired MWE lists outperform the resources generated from human knowledge for three out of four classes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Identifying MWEs in text is related to the task of Named Entity Recognition (NER).", "labels": [], "entities": [{"text": "Identifying MWEs in text", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.863441213965416}, {"text": "Named Entity Recognition (NER)", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.7830699880917867}]}, {"text": "However, the task of MWE recognition mostly considers the detection of word sequences that form MWEs and are not Named Entities (NEs).", "labels": [], "entities": [{"text": "MWE recognition", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.9934684634208679}]}, {"text": "For both tasks mostly sequence tagging algorithms, e.g. Hidden Markov Model (HMM) or Conditional Random Fields (CRF), are trained and then applied to previously unseen text.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7012483924627304}]}, {"text": "In order to tackle the recognition of MWEs, most approaches (e.g. () use resources containing MWEs.", "labels": [], "entities": [{"text": "recognition of MWEs", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8766387104988098}]}, {"text": "These are mostly extracted from lexical resources (e.g. WordNet) or from markup in text (e.g. Wikipedia, Wiktionary).", "labels": [], "entities": []}, {"text": "While these approaches work well, they require respective resources and markup.", "labels": [], "entities": []}, {"text": "This might not be the case for special domains or under-resourced languages.", "labels": [], "entities": []}, {"text": "On the contrary, methods have been developed that rank word sequences according to their multiwordness automatically using information from corpora, mostly relying on frequencies.", "labels": [], "entities": []}, {"text": "Many of these methods (e.g. C/NC-Value (), GM-MF ()) require previous filters, which are based on Part-ofSpeech (POS) sequences.", "labels": [], "entities": []}, {"text": "Such sequences, (e.g.) need to be defined and mostly do not coverall POS types of MWE.", "labels": [], "entities": []}, {"text": "In this work we do not want to restrict to specific MWE types and thus will use DRUID () and the Student's t-test as multiword ranking methods, which do not require any previous filtering.", "labels": [], "entities": [{"text": "DRUID", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.8839077353477478}]}, {"text": "This paper focuses on the following research question: how do such lists generated from raw text compete against manually generated resources?", "labels": [], "entities": []}, {"text": "Furthermore, we want to examine whether a combination of resources yields better performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation we use the Wikipedia-based Wiki50 The dataset primarily consists of annotations for NEs, especially for the person label.", "labels": [], "entities": []}, {"text": "The annotated MWEs are dominated by noun compounds followed by verb-particle constructions, light-verb constructions and adjective compounds.", "labels": [], "entities": []}, {"text": "Idioms and other MWEs occur only rarely.", "labels": [], "entities": [{"text": "Idioms", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.865382730960846}]}, {"text": "We perform the evaluation, using a 10-fold cross validation and use the crfsuite 3 implementation of CRF as classifier.", "labels": [], "entities": []}, {"text": "For retrieving POS tags, we apply the OpenNLP POS tagger . The lemmatization is performed using the WordNetLemmatizer, contained in nltk ().", "labels": [], "entities": [{"text": "OpenNLP POS tagger", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.8822698990503947}, {"text": "WordNetLemmatizer", "start_pos": 100, "end_pos": 117, "type": "DATASET", "confidence": 0.9522475004196167}]}, {"text": "For the computation of automatically generated MWEs lists, we use the raw text from an English Wikipedia dump, without considering any markup and annotations.", "labels": [], "entities": []}, {"text": "For applying them as resources, we only consider word sequences in the resource that are also contained in the Wiki50 dataset, both training and test data.", "labels": [], "entities": [{"text": "Wiki50 dataset", "start_pos": 111, "end_pos": 125, "type": "DATASET", "confidence": 0.9621069729328156}]}, {"text": "Based on these candidates, we select then highest ranked MWE candidates.", "labels": [], "entities": [{"text": "MWE", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.8159666657447815}]}], "tableCaptions": [{"text": " Table 2: Performance for predicting labels for  MWE and NE without using MWE resources.", "labels": [], "entities": [{"text": "predicting labels", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8919225931167603}]}, {"text": " Table 3: Overall performance on the labels for  different MWE resources applied solely to the  MWEs annotated in the Wiki50 dataset.", "labels": [], "entities": [{"text": "Wiki50 dataset", "start_pos": 118, "end_pos": 132, "type": "DATASET", "confidence": 0.9568249881267548}]}, {"text": " Table 4: Detailed performance in terms of precision (P), recall (R) and F1-measure (F1) for the different  MWE types. The experiments have been performed only on the MWE annotations.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 43, "end_pos": 56, "type": "METRIC", "confidence": 0.941626787185669}, {"text": "recall (R)", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9590438306331635}, {"text": "F1-measure (F1)", "start_pos": 73, "end_pos": 88, "type": "METRIC", "confidence": 0.9429617524147034}]}, {"text": " Table 5: Unlabeled results for MWEs recognition.", "labels": [], "entities": [{"text": "MWEs recognition", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.9778701066970825}]}]}