{"title": [], "abstractContent": [{"text": "This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016.", "labels": [], "entities": [{"text": "AMU (Adam Mickiewicz University) team", "start_pos": 43, "end_pos": 80, "type": "DATASET", "confidence": 0.8812420793942043}, {"text": "Automatic Post-Editing (APE) task of WMT 2016", "start_pos": 88, "end_pos": 133, "type": "TASK", "confidence": 0.5622361765967475}]}, {"text": "We explore the application of neural translation models to the APE problem and achieve good results by treating different models as components in a log-linear model, allowing for multiple inputs (the MT-output and the source) that are decoded to the same target language (post-edited translations).", "labels": [], "entities": [{"text": "neural translation", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7652179300785065}, {"text": "APE problem", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.93289515376091}]}, {"text": "A simple string-matching penalty integrated within the log-linear model is used to control for higher faithfulness with regard to the raw machine translation output.", "labels": [], "entities": []}, {"text": "To overcome the problem of too little training data, we generate large amounts of artificial data.", "labels": [], "entities": []}, {"text": "Our submission improves over the uncor-rected baseline on the unseen test set by-3.2% TER and +5.5% BLEU and outper-forms any other system submitted to the shared-task by a large margin.", "labels": [], "entities": [{"text": "TER", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9989159107208252}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9982725381851196}]}], "introductionContent": [{"text": "This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016.", "labels": [], "entities": [{"text": "AMU (Adam Mickiewicz University) team", "start_pos": 43, "end_pos": 80, "type": "DATASET", "confidence": 0.8812420112746102}, {"text": "Automatic Post-Editing (APE) task of WMT 2016", "start_pos": 88, "end_pos": 133, "type": "TASK", "confidence": 0.562237391869227}]}, {"text": "Following the APE shared task from WMT 2015, the aim is to test methods for correcting errors produced by an unknown machine translation system in a black-box scenario.", "labels": [], "entities": [{"text": "APE shared task from WMT 2015", "start_pos": 14, "end_pos": 43, "type": "DATASET", "confidence": 0.7076898813247681}, {"text": "correcting errors produced by an unknown machine translation", "start_pos": 76, "end_pos": 136, "type": "TASK", "confidence": 0.7684722393751144}]}, {"text": "The organizers provide training data with human postedits, evaluation is carried out part-automatically using TER) and BLEU), and part-manually.", "labels": [], "entities": [{"text": "TER", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9946699738502502}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9896891117095947}]}, {"text": "We explore the application of neural translation models to the APE task and investigate a number of aspects that seem to lead to good results: \u2022 Creation of artificial post-edition data that can be used to train the neural models; \u2022 Log-linear combination of monolingual and bilingual models in an ensemble-like manner; \u2022 Addition of task-specific features in a loglinear model that allow to control for faithfulness of the automatic post-editing output with regard to the input, otherwise a weakness of neural translation models.", "labels": [], "entities": [{"text": "APE task", "start_pos": 63, "end_pos": 71, "type": "TASK", "confidence": 0.9184469282627106}]}, {"text": "According to the automatic evaluation metrics used for the task, our system is ranked first among all submission to the shared task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following the post-editing-by-machine-translation paradigm, we explore the application of softattention neural translation models to post-editing.", "labels": [], "entities": [{"text": "softattention neural translation", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.6751788059870402}]}, {"text": "Analogous to the two dominating approaches de- scribed in Section 2.1, we investigate methods that are purely monolingual as well as a simple method to include source language information in a more natural way than it has been done for phrase-based machine translation.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 236, "end_pos": 268, "type": "TASK", "confidence": 0.6812409659226736}]}, {"text": "The neural machine translation systems explored in this work are attentional encoderdecoder models (, which have been trained with Nematus . We used minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024.", "labels": [], "entities": []}, {"text": "Models were trained with Adadelta, reshuffling the corpus between epochs.", "labels": [], "entities": []}, {"text": "As mentioned before tokens were split into subword units, 40,000 per language.", "labels": [], "entities": []}, {"text": "For decoding, we used AmuNMT: Results on provided development set.", "labels": [], "entities": []}, {"text": "Best-performing models have been chosen based on this development set.", "labels": [], "entities": []}, {"text": "Systems marked with * have weights tuned on the same development set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of full and filtered data sets: number of sentences, average number of words, word  shifts, errors, and TER score.", "labels": [], "entities": [{"text": "errors", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9535276293754578}, {"text": "TER score", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9818489253520966}]}, {"text": " Table 2: Results on provided development set.  Best-performing models have been chosen based  on this development set. Systems marked with  *   have weights tuned on the same development set.", "labels": [], "entities": []}, {"text": " Table 3: Results on unseen test set in comparison  to other shared task submissions as reported by the  task organizers. For submissions by other teams  we include only their best result.", "labels": [], "entities": []}]}