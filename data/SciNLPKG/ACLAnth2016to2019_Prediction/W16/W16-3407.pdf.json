{"title": [{"text": "The Trouble with Machine Translation Coherence", "labels": [], "entities": [{"text": "Machine Translation Coherence", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.853125810623169}]}], "abstractContent": [{"text": "This paper introduces the problem of measuring coherence in Machine Translation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7944897711277008}]}, {"text": "Previously, local coherence has been assessed in a monolingual context using essentially coherent texts.", "labels": [], "entities": []}, {"text": "These are then artificially shuffled to create an incoherent one.", "labels": [], "entities": []}, {"text": "We investigate existing models for the task of measuring the coherence of machine translation output.", "labels": [], "entities": []}, {"text": "This is a much more challenging case where coherent source documents are machine translated into a target language and the task is to distinguish them from their human translated counterparts.", "labels": [], "entities": []}, {"text": "We benchmark state-of-the-art coherence models, and propose anew model which explores syntax following a more principled method to learn the syntactic patterns.", "labels": [], "entities": []}, {"text": "This extension outperforms existing ones in the monolingual shuffling task on news data, and performs well in our new, more challenging task.", "labels": [], "entities": []}, {"text": "Additionally, we show that breaches in coherence in the translation task are much more difficult to capture by any model.", "labels": [], "entities": [{"text": "translation task", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.9168411195278168}]}], "introductionContent": [{"text": "A coherent discourse is said to be one that has meaningful connections between its utterances.", "labels": [], "entities": []}, {"text": "The task of automatically evaluating text coherence has been addressed within applications such as text summarisation and ordering (, where shuffled sentences or inadequate summaries can lead to less coherent documents.", "labels": [], "entities": [{"text": "text summarisation", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.7684288620948792}]}, {"text": "Coherence has then been measured with entity grids, discourse relations and syntax patterns, with experiments run on the original and the artificially modified texts to distinguish coherent from incoherent texts.", "labels": [], "entities": []}, {"text": "We introduce the more challenging problem of evaluating the coherence of documents generated by Machine Translation (MT) systems.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.8389183282852173}]}, {"text": "This is a very different scenario.", "labels": [], "entities": []}, {"text": "Firstly, it is more subtle as the sudden breaks in transitions or shifts of focus which result from shuffling in the traditional monolingual test scenario are absent, as sentences are translated in their original (source) order.", "labels": [], "entities": []}, {"text": "Secondly, the machine translated output may contain other textual issues, such as ungrammatical fragments, which can affect the application of such models in various ways, making it harder to pinpoint coherence-related problems.", "labels": [], "entities": []}, {"text": "Finally, judgements on the coherence of the translations maybe dependent on the source text.", "labels": [], "entities": []}, {"text": "Nevertheless, measuring coherence in MT is important: given the way translations are generated by standard MT systems, on a sentence-by-sentence basis, several phenomena spanning sentence boundaries can lead to incoherent document translations, such as incorrect co-referencing, inadequate discourse markers, and lack of lexical cohesion, as established by previous corpus analyses.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9661362171173096}]}, {"text": "We apply three existing coherence models to original, shuffled and machine translated texts in an attempt to evaluate their ability to discriminate between coherent and incoherent documents: an entity-grid model (, an entity graph similarity metric (, and a model based on syntactic patterns ().", "labels": [], "entities": []}, {"text": "In addition, we propose a fully generative extension of the syntax-based coherence model.", "labels": [], "entities": []}, {"text": "We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.97785884141922}]}, {"text": "The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 134, "end_pos": 136, "type": "TASK", "confidence": 0.970728874206543}]}, {"text": "Section 3 covers the background of the coherence models used in this paper.", "labels": [], "entities": []}, {"text": "Experiments and results are discussed in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "To estimate the parameters of the entity-grid and syntax-based models (e.g. distribution over entity role transitions and syntactic patterns), we use the most recent portion of English LDC Gigaword corpus, excluding 2 sections.", "labels": [], "entities": [{"text": "English LDC Gigaword corpus", "start_pos": 177, "end_pos": 204, "type": "DATASET", "confidence": 0.8814825564622879}]}, {"text": "4 displays information about the size of these datasets.", "labels": [], "entities": []}, {"text": "To test our models on the translation task, we use WMT14 test data as corpus), considering submissions from all participating MT systems (including statistical, rule-based, hybrid) in the translation shared task for three language pairs, namely, 13 German-English (de-en) systems, 9 French-English (fr-en) systems, and 13 Russian-English (ru-en) systems.", "labels": [], "entities": [{"text": "translation task", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.9257316887378693}, {"text": "WMT14 test data", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9518940846125284}]}, {"text": "We assume that the HT (reference) is a coherent text, and that the MT output mayor may not be coherent.", "labels": [], "entities": []}, {"text": "While the former is a fair assumption, we do acknowledge that many outputs from MT systems maybe coherent.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9668362736701965}]}, {"text": "However, we are not aware of any datasets with translated data which have been annotated for coherence.", "labels": [], "entities": []}, {"text": "This is a challenging task in itself, as judging coherence is a complex and subjective task which requires, at the very least, well trained annotators.", "labels": [], "entities": []}, {"text": "Our hypothesis is that a good coherence model should be able to score human translations as having higher coherence than their counterpart machine translations inmost cases.", "labels": [], "entities": []}, {"text": "For the shuffling task we also use the MT data, taking the HT documents as the coherent texts and shuffled versions of them to create incoherent ones.", "labels": [], "entities": [{"text": "MT data", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.8580223917961121}]}], "tableCaptions": [{"text": " Table 1: Number of documents and sentences in the training (Gigaword) and test (WMT14) sets.", "labels": [], "entities": [{"text": "WMT14) sets", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.8220674196879069}]}, {"text": " Table 2: Model comparisons for shuffling experiment on Earthquakes and Accidents corpus, ref1 *   is \"accuracy\" used in previous work with this corpus.", "labels": [], "entities": [{"text": "Earthquakes and Accidents corpus", "start_pos": 56, "end_pos": 88, "type": "DATASET", "confidence": 0.6673158705234528}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.999234676361084}]}, {"text": " Table 3: Model comparisons for shuffling experiment", "labels": [], "entities": []}, {"text": " Table 4: Model comparisons for translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.9188252687454224}]}]}