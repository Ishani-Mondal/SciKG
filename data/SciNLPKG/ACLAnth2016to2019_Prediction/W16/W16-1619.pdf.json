{"title": [{"text": "Parameterized context windows in Random Indexing", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.8022456467151642}]}], "abstractContent": [{"text": "This paper introduces a parameterization for word embeddings produced by the Random Indexing framework.", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.636248990893364}]}, {"text": "The pa-rameterization introduces position specific weights in the context windows, and the approach is shown to improve the performance in both word similarity and sentiment classification tasks.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 144, "end_pos": 159, "type": "TASK", "confidence": 0.7193247973918915}, {"text": "sentiment classification tasks", "start_pos": 164, "end_pos": 194, "type": "TASK", "confidence": 0.8568395574887594}]}, {"text": "We also demonstrate the relation between Random Indexing and Convolutional Neural Networks.", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.6569678783416748}]}], "introductionContent": [{"text": "Quantifying the importance of contextual information for semantic representation is the goal of distributional semantics, in which contextual information is used to quantify semantic similarities between words (.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.6743407249450684}]}, {"text": "However, standard practice in distributional semantics is to weight the importance of context items based on either its frequency (, its distance to the focus word (, or its global co-occurrence statistics.", "labels": [], "entities": []}, {"text": "Thus far, there has not been much work on applying machine learning to this in order to select useful context items for distributional semantics.", "labels": [], "entities": []}, {"text": "The idea with the proposed parameterization is to weight the items in the context window based on their usefulness for accomplishing some specific task, such as sentiment classification or word similarity rating.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 161, "end_pos": 185, "type": "TASK", "confidence": 0.8915955722332001}]}, {"text": "In this paper, we introduce a simple parameterization for the Random Indexing processing model.", "labels": [], "entities": [{"text": "Random Indexing processing", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.5829709867636362}]}, {"text": "We first show that Random Indexing can be formulated in terms of a convolution, in order to situate the framework in the context of neural networks.", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7896251678466797}]}, {"text": "We then introduce a simple parameterization of the positions on the context windows, and we show that it improves the performance of the embeddings in some word similarity and sentiment classification tasks.", "labels": [], "entities": [{"text": "word similarity and sentiment classification tasks", "start_pos": 156, "end_pos": 206, "type": "TASK", "confidence": 0.7181649953126907}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results of the SimLex experiment.  Avg. error Spearman  Initial \u03b8 i s (\u03b8  *  = 1) 0.28  0.21  Optimized \u03b8 i s  0.19  0.62", "labels": [], "entities": [{"text": "Avg. error Spearman  Initial \u03b8 i s (\u03b8  *  = 1) 0.28", "start_pos": 45, "end_pos": 96, "type": "METRIC", "confidence": 0.8921764969825745}]}]}