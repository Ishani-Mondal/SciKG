{"title": [{"text": "On the Correspondence between Compositional Matrix-Space Models of Language and Weighted Automata", "labels": [], "entities": []}], "abstractContent": [{"text": "Compositional matrix-space models of language were recently proposed for the task of meaning representation of complex text structures in natural language processing.", "labels": [], "entities": [{"text": "meaning representation of complex text structures", "start_pos": 85, "end_pos": 134, "type": "TASK", "confidence": 0.8351556857426962}]}, {"text": "These models have been shown to be a theoretically elegant way to model com-positionality in natural language.", "labels": [], "entities": []}, {"text": "However , in practical cases, appropriate methods are required to learn such models by automatically acquiring the necessary token-to-matrix assignments.", "labels": [], "entities": []}, {"text": "In this paper , we introduce graded matrix grammars of natural language, a variant of the matrix grammars proposed by Rudolph and Giesbrecht (2010), and show a close correspondence between this matrix-space model and weighted finite automata.", "labels": [], "entities": []}, {"text": "We conclude that the problem of learning compositional matrix-space models can be mapped to the problem of learning weighted finite automata over the real numbers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quantitative models of language have recently received considerable research attention in the field of Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 103, "end_pos": 136, "type": "TASK", "confidence": 0.7385041614373525}]}, {"text": "In the application of meaning representation of text in NLP, much effort has been spent on semantic Vector Space Models (VSMs).", "labels": [], "entities": [{"text": "meaning representation of text in NLP", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.7631966670354208}]}, {"text": "Such models capture word meanings quantitatively, based on their statistical co-occurrences in the documents.", "labels": [], "entities": []}, {"text": "The basic idea is to represent words as vectors in a highdimensional space, where each dimension corresponds to a separate feature.", "labels": [], "entities": []}, {"text": "In this way, semantic similarities can be computed based on measuring the distance between vectors in the vector * Supported by space.", "labels": [], "entities": []}, {"text": "Vectors which are close together in this space have similar meanings and vectors which are faraway are distant in meaning.", "labels": [], "entities": []}, {"text": "VSMs typically represent each word separately, without considering representations of phrases or sentences.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.7985147833824158}]}, {"text": "So, the compositionality properties of the language is lost in VSMs (.", "labels": [], "entities": []}, {"text": "Recently, some approaches have been developed in the area of compositionality and distributional semantics in NLP.", "labels": [], "entities": []}, {"text": "These approaches introduce different word representations and ways of combining those words.", "labels": [], "entities": []}, {"text": "propose a framework for vector-based semantic composition.", "labels": [], "entities": [{"text": "vector-based semantic composition", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.6439917286237081}]}, {"text": "They define additive or multiplicative function for the composition of two vectors and show that compositional approaches generally outperform non-compositional approaches which treat the phrase as the union of single lexical items.", "labels": [], "entities": []}, {"text": "However, VSMs still have some limitations in the task of modeling complex conceptual text structures.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.8505135178565979}]}, {"text": "For example, in the bag-of-words model, the words order and therefore the structure of the language is lost.", "labels": [], "entities": []}, {"text": "To overcome the limitations of VSMs, proposed Compositional Matrix-Space Models (CMSM) as a recent alternative model to work with distributional approaches.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.9127523899078369}]}, {"text": "These models employ matrices instead of vectors and make use of iterated matrix multiplication as the only composition operation.", "labels": [], "entities": []}, {"text": "They show that these models are powerful enough to subsume many known models, both quantitative (vector-space models with diverse composition operations) and qualitative ones (such as regular languages).", "labels": [], "entities": []}, {"text": "It is also proved theoretically that this framework is an elegant way to model compositional, symbolic and distributional aspects of natural language.", "labels": [], "entities": []}, {"text": "However, in practical cases, methods are needed to automatically acquire the token-to-matrix assignments from available data.", "labels": [], "entities": []}, {"text": "Therefore, methods for training such models should be developed e.g. by leveraging appropriate machine learning methods.", "labels": [], "entities": []}, {"text": "In this paper, we are concerned with Graded Matrix Grammars, a variant of the Matrix Grammars of, where instead of the \"yes or no\" decision, if a sequence is part of a language, a real-valued score is assigned.", "labels": [], "entities": []}, {"text": "This is a popular task in NLP, used, e.g., in sentiment analysis settings (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.9565846025943756}]}, {"text": "Generally, in many tasks of NLP, we need to estimate functions which map arbitrary sequence of words (e.g. sentences) to some semantical space.", "labels": [], "entities": []}, {"text": "Using Weighted Finite Automata (WFA), an extensive class of these functions can be defined, which assign values to these sequences ().", "labels": [], "entities": []}, {"text": "Herein, inspired by the definition of weighted finite automata and their applications in NLP (, we show a tight correspondence between graded matrix grammars and weighted finite automata.", "labels": [], "entities": []}, {"text": "Hence, we argue that the problem of learning CMSMs can be mapped to the problem of learning WFA.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides the basic notions of weighted automata and the matrix-space model.", "labels": [], "entities": []}, {"text": "A detailed description of correspondence between CMSM and WFA is presented in Section 3, followed by related work in Section 4 and conclusion and future work in Section 5.", "labels": [], "entities": [{"text": "CMSM", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8796690702438354}, {"text": "WFA", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.6725154519081116}]}], "datasetContent": [], "tableCaptions": []}