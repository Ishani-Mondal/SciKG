{"title": [{"text": "Comparison of Grapheme-to-Phoneme Conversion Methods on a Myanmar Pronunciation Dictionary", "labels": [], "entities": [{"text": "Myanmar Pronunciation Dictionary", "start_pos": 58, "end_pos": 90, "type": "DATASET", "confidence": 0.9281727274258932}]}], "abstractContent": [{"text": "Grapheme-to-Phoneme (G2P) conversion is the task of predicting the pronunciation of a word given its graphemic or written form.", "labels": [], "entities": [{"text": "Grapheme-to-Phoneme (G2P) conversion", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5652189612388611}, {"text": "predicting the pronunciation of a word", "start_pos": 52, "end_pos": 90, "type": "TASK", "confidence": 0.8713309069474539}]}, {"text": "It is a highly important part of both automatic speech recognition (ASR) and text-to-speech (TTS) systems.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.8029854794343313}]}, {"text": "In this paper, we evaluate seven G2P conversion approaches: Adaptive Regularization of Weight Vectors (AROW) based structured learning (S-AROW), Conditional Random Field (CRF), Joint-sequence models (JSM), phrase-based statistical machine translation (PBSMT), Recurrent Neural Network (RNN), Support Vector Machine (SVM) based point-wise classification, Weighted Finite-state Transducers (WFST) on a manually tagged Myan-mar phoneme dictionary.", "labels": [], "entities": [{"text": "G2P conversion", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.6853486001491547}, {"text": "Adaptive Regularization of Weight Vectors", "start_pos": 60, "end_pos": 101, "type": "TASK", "confidence": 0.7337246894836426}, {"text": "phrase-based statistical machine translation", "start_pos": 206, "end_pos": 250, "type": "TASK", "confidence": 0.6033571735024452}]}, {"text": "The G2P bootstrapping experimental results were measured with both automatic phoneme error rate (PER) calculation and also manual checking in terms of voiced/unvoiced, tones, consonant and vowel errors.", "labels": [], "entities": [{"text": "automatic phoneme error rate (PER) calculation", "start_pos": 67, "end_pos": 113, "type": "METRIC", "confidence": 0.9185275360941887}]}, {"text": "The result shows that CRF, PBSMT and WFST approaches are the best performing methods for G2P conversion on Myanmar language.", "labels": [], "entities": [{"text": "G2P conversion", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.7310527861118317}]}], "introductionContent": [{"text": "Grapheme-to-Phoneme (G2P) conversion models are important for natural language processing (NLP), automatic speech recognition (ASR) and text-to-speech (TTS) developments.", "labels": [], "entities": [{"text": "Grapheme-to-Phoneme (G2P) conversion", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.634727281332016}, {"text": "automatic speech recognition (ASR)", "start_pos": 97, "end_pos": 131, "type": "TASK", "confidence": 0.8216882149378458}]}, {"text": "Although many machine learning approaches are applicable for G2P conversion, most of them are supervised learning approaches and as a prerequisite we have to prepare clean annotated training data and this is costly.", "labels": [], "entities": [{"text": "G2P conversion", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.8219576776027679}]}, {"text": "As a consequence, G2P models are rarely available for under-resourced languages such as South and Southeast Asian languages.", "labels": [], "entities": []}, {"text": "In practice, we need to perform bootstrapping or active learning with a small manually annotated G2P dictionary for efficient development of G2P converters.", "labels": [], "entities": []}, {"text": "In this paper, we examine seven G2P conversion methodologies for incremental training with a small Myanmar language G2P lexicon.", "labels": [], "entities": [{"text": "G2P conversion", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7272581160068512}]}, {"text": "We used automatic evaluation in the form of phoneme error rate (PER) and also manually evaluated Myanmar language specific errors such as inappropriate voiced to unvoiced conversion and tones, on syllable units.", "labels": [], "entities": [{"text": "phoneme error rate (PER)", "start_pos": 44, "end_pos": 68, "type": "METRIC", "confidence": 0.8172472516695658}]}], "datasetContent": [{"text": "To evaluate the quality of the G2P approaches, we used two evaluation criteria.", "labels": [], "entities": []}, {"text": "One is automatic evaluation of phoneme error rate (PER) with SCLITE (score speech recognition system output) program from the NIST scoring toolkit SCTK version 2.4.10 (http://www1.icsi.berkeley.edu/Speech/docs/sctk-1.2/sclite.htm).", "labels": [], "entities": [{"text": "phoneme error rate (PER)", "start_pos": 31, "end_pos": 55, "type": "METRIC", "confidence": 0.7945235321919123}, {"text": "NIST scoring toolkit SCTK version 2.4.10", "start_pos": 126, "end_pos": 166, "type": "DATASET", "confidence": 0.9044396976629893}]}, {"text": "The other evaluation was done manually by counting voiced/unvoiced, tones, consonant and vowel errors on G2P outputs.", "labels": [], "entities": []}, {"text": "The SCLITE scoring method for calculating the erroneous words in Word Error Rate (WER), is as follows: first make an alignment of the G2P hypothesis (the output from the trained model) and the reference (human transcribed) word strings and then perform a global minimization of the Levenshtein distance function which weights the cost of correct words, insertions (I), deletions (D) and substitutions (S).", "labels": [], "entities": [{"text": "SCLITE", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.9624474048614502}, {"text": "Word Error Rate (WER)", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.6142572859923044}]}, {"text": "The formula for WER is as follows: In our case, we trained G2P models with syllable segmented words and thus alignment was done on syllable units and the PER was derived from the Levenshtein distance at the phoneme level rather than the word level.", "labels": [], "entities": [{"text": "WER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.653597891330719}, {"text": "PER", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.9904009699821472}]}, {"text": "For example, phoneme level of syllable alignment, counting I, D and S for Myanmar word \"\u1001\u1001\u1004\u1004 \u1004\u1001\u1001\u1000\u1000 \" (exception in English), left column and \"\u1005\u1005 \u1010\u1010 \u1015\u1015\u1000\u1000 \u101c\u1000\u1000 \u1015\u1015\u1000\u1000 \" (disappointed in English), right column is as follows:  We used PER to evaluate the performance of G2P conversion.", "labels": [], "entities": [{"text": "syllable alignment", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.734930545091629}, {"text": "PER", "start_pos": 228, "end_pos": 231, "type": "METRIC", "confidence": 0.9919009804725647}, {"text": "G2P conversion", "start_pos": 263, "end_pos": 277, "type": "TASK", "confidence": 0.6723660677671432}]}, {"text": "We computed the PER scores using sclite (http://www1.icsi.berkeley.edu/Speech/docs/sctk-1.2/sclite.htm) on the hypotheses of G2P models and references.", "labels": [], "entities": [{"text": "PER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.6907023787498474}]}, {"text": "The results are presented in and lower PER is better in performance as we mentioned in Section 5.3.", "labels": [], "entities": [{"text": "PER", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.999176561832428}]}, {"text": "The experimental results also show the learning curve variations of seven G2P conversion approaches on the training data.", "labels": [], "entities": []}, {"text": "We can clearly see that there is no significant learning improvement for the SVM based point-wise classification from the evaluation results on both the closed and the three open test sets (see, (g)).", "labels": [], "entities": []}, {"text": "Also, the PER results of S-AROW, JSM, PBSMT and RNNA on the closed test data are unstable.", "labels": [], "entities": [{"text": "PER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.993286669254303}, {"text": "JSM", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7678128480911255}, {"text": "RNNA", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.5700274705886841}]}, {"text": "Each of the graphs show the performance of G2P conversion and the best PER scores (i.e. 0) was achieved on the closed test data by the RNN, S-AROW and WFST.", "labels": [], "entities": [{"text": "G2P conversion", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.7325904369354248}, {"text": "PER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9979220032691956}, {"text": "RNN", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.9443684816360474}, {"text": "WFST", "start_pos": 151, "end_pos": 155, "type": "DATASET", "confidence": 0.9609772562980652}]}, {"text": "The best PER scores of the CRF and PBSMT on closed test data were 6.4 and 7.5 respectively.", "labels": [], "entities": [{"text": "PER", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9981239438056946}, {"text": "CRF", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.8026158213615417}]}, {"text": "On the other hand, the final models of the CRF and WFST achieved the lowest PER scores for all three open test data sets (open1, open2 and open3).", "labels": [], "entities": [{"text": "WFST", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8461376428604126}, {"text": "PER", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9988811612129211}]}, {"text": "A PER score 14.7 for open1 was achieved by WFST, 11.4 for open2, and 15.7 for open3 by both CRF and WFST.", "labels": [], "entities": [{"text": "PER", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9991092085838318}, {"text": "WFST", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.9731719493865967}, {"text": "CRF", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.9625012278556824}, {"text": "WFST", "start_pos": 100, "end_pos": 104, "type": "DATASET", "confidence": 0.9755649566650391}]}, {"text": "An interesting point is that the PBSMT approach achieved close to the lowest PERs for the three open test sets (16.1 for open1, 13.1 for open2 and 22.0 for open3)., (e) shows the RNN approach is able to learn to reach zero PER score on the closed test data from epoch two (i.e. with 5,000 words).", "labels": [], "entities": [{"text": "PERs", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9939773082733154}, {"text": "PER score", "start_pos": 223, "end_pos": 232, "type": "METRIC", "confidence": 0.9713074564933777}]}, {"text": "The PER of RNN is lower than RNNA approach for both the closed and the open test data (see, (e) and (f)).", "labels": [], "entities": [{"text": "PER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9992142915725708}]}, {"text": "Manual evaluation was mainly done on the results from the models trained with 25,000 words in terms of errors on voiced/unvoiced pronunciation change, vowel, consonant and tone.", "labels": [], "entities": []}, {"text": "The results show that voiced/unvoiced error is the highest among them.", "labels": [], "entities": [{"text": "error", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.7552456855773926}]}, {"text": "(Ye Kyaw Thu et al., 2015a) discussed the importance of the pronunciation change patterns, and our experimental results also show how these patterns affect the G2P performance.", "labels": [], "entities": []}, {"text": "Pronunciation error rates for PBSMT and WFST are comparable and the PBSMT approach gives the best performance overall.", "labels": [], "entities": [{"text": "PBSMT", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.8205277323722839}, {"text": "WFST", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.8768129348754883}]}, {"text": "The SVM based point-wise classification approach produced the highest phoneme errors on unknown words (i.e. UNK tagging for OOV case by KyTea) among the seven G2P approaches.", "labels": [], "entities": [{"text": "UNK tagging", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.6877482235431671}]}, {"text": "Generally, all methods can handle tone well and we assume that almost all the tonal information of Myanmar graphemes is covered in the training dictionary.", "labels": [], "entities": []}, {"text": "The lowest error rate on tone was achieved by PBSMT.", "labels": [], "entities": [{"text": "error rate", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9884031713008881}, {"text": "PBSMT", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.936954915523529}]}, {"text": "From the overall manual evaluation results from train1 (training number 1: trained with 2,500 words) to train10 (training number 2: trained with 25,000 words), we can see clearly that RNN, PBSMT and WFST approaches gradually improve with increasing training data set size.", "labels": [], "entities": [{"text": "RNN", "start_pos": 184, "end_pos": 187, "type": "DATASET", "confidence": 0.7994512915611267}, {"text": "WFST", "start_pos": 199, "end_pos": 203, "type": "DATASET", "confidence": 0.6627742648124695}]}, {"text": "Some difficult pronunciation changes at the consonant level (such as pronunciation prediction from ljin to jin for the Myanmar word \"kau'jin\", \"\u1000\u1000\u1000\u1000 \u101c\u101c\u1004\u1004 \") can be predicted correctly by the PBSMT approach and the RNN but not by the other approaches.", "labels": [], "entities": [{"text": "pronunciation prediction", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.6722994893789291}, {"text": "RNN", "start_pos": 214, "end_pos": 217, "type": "DATASET", "confidence": 0.9344814419746399}]}, {"text": "Although the training accuracy of RNN is higher than the other techniques, in the automatic evaluation, some OOV predictions are the worst (refer).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9513742327690125}, {"text": "OOV", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9784206748008728}]}], "tableCaptions": [{"text": " Table 2: An example of phoneme prediction errors of G2P conversion methods.", "labels": [], "entities": [{"text": "phoneme prediction", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7242785543203354}, {"text": "G2P conversion", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.6160085499286652}]}]}