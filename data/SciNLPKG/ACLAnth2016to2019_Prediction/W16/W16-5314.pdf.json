{"title": [{"text": "CogALex-V Shared Task: CGSRC -Classifying Semantic Relations using Convolutional Neural Networks", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we describe a system (CGSRC) for classifying four semantic relations: synonym, hypernym, antonym and meronym using convolutional neural networks (CNN).", "labels": [], "entities": []}, {"text": "We have participated in CogALex-V semantic shared task of corpus-based identification of semantic relations.", "labels": [], "entities": [{"text": "corpus-based identification of semantic relations", "start_pos": 58, "end_pos": 107, "type": "TASK", "confidence": 0.711168771982193}]}, {"text": "Proposed approach using CNN-based deep neural networks leveraging pre-compiled word2vec distributional neural embeddings achieved 43.15% weighted-F1 accuracy on subtask-1 (check-ing existence of a relation between two terms) and 25.24% weighted-F1 accuracy on subtask-2 (classifying relation types).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9620764255523682}, {"text": "accuracy", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.9504138231277466}]}], "introductionContent": [{"text": "Discovering semantic relations and the corresponding relation types between word pairs is an important task in Natural Language Processing (NLP) with a wide range of applications, such as automatic Machine Translation, Question Answering Systems, Ontology Learning, Paraphrase Generation, etc.", "labels": [], "entities": [{"text": "Discovering semantic relations", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8924893935521444}, {"text": "Machine Translation", "start_pos": 198, "end_pos": 217, "type": "TASK", "confidence": 0.6751150488853455}, {"text": "Question Answering", "start_pos": 219, "end_pos": 237, "type": "TASK", "confidence": 0.8453960716724396}, {"text": "Ontology Learning", "start_pos": 247, "end_pos": 264, "type": "TASK", "confidence": 0.8200915455818176}, {"text": "Paraphrase Generation", "start_pos": 266, "end_pos": 287, "type": "TASK", "confidence": 0.9653976559638977}]}, {"text": "Corpus-driven automated methods for semantic relation identification have been promising an efficient and scalable solution in the recent past.", "labels": [], "entities": [{"text": "semantic relation identification", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.9043576717376709}]}, {"text": "To discover semantic relations such as synonym, hypernym and antonym, most of the existing methods) employed lexical patterns or distributional hypothesis, and suffer from sparsity and low accuracy problems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9947881698608398}]}, {"text": "Moreover, many of these methodologies model individual semantic relations using external knowledge sources such as thesauri, WordNet, etc.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.9224513173103333}]}, {"text": "Although semantic networks like WordNet 1 define semantic relations such as synonym, hypernym, antonym and part-of between word types, however they are limited in scope and domain.", "labels": [], "entities": []}, {"text": "Recently, few approaches based on distributional word embeddings () reported significant improvements in identifying various lexical semantic relations such as hypernymy, antonymy, synonymy etc.", "labels": [], "entities": []}, {"text": "Distributional representations of words learned from a large corpus capture linguistic regularities and collapse similar words into groups (.", "labels": [], "entities": []}, {"text": "Inspired by these approaches, we propose a lexical semantic relation detection system using CNNbased deep neural networks by leveraging word2vec 2 distributional word embeddings as part of 5th edition of CogALex shared task . The shared task proposed two subtasks namely, relation detection and relation type identification.", "labels": [], "entities": [{"text": "lexical semantic relation detection", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.6676428392529488}, {"text": "relation detection", "start_pos": 272, "end_pos": 290, "type": "TASK", "confidence": 0.8975328505039215}, {"text": "relation type identification", "start_pos": 295, "end_pos": 323, "type": "TASK", "confidence": 0.8162789940834045}]}, {"text": "Subtask-1 aims at detecting a relation between two given terms and subtask-2 aims at identifying semantic relations such as synonym, hypernym, antonym, and part-of between two terms if a relation exists.", "labels": [], "entities": []}, {"text": "This task is particularly challenging as local context for term pairs is not available in the training corpus.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: in section 2, we describe related work and in section 3, we introduce deep learning-based supervised classification technique for identifying semantic relations.", "labels": [], "entities": [{"text": "deep learning-based supervised classification", "start_pos": 117, "end_pos": 162, "type": "TASK", "confidence": 0.5942293107509613}]}, {"text": "We describe datasets and the experimental results in section 4.", "labels": [], "entities": []}, {"text": "In section 5, we analyze various types of errors in relation classification and conclude the paper.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.9156343936920166}]}], "datasetContent": [{"text": "We model the relation classification as a sentence classification task.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.8871507942676544}, {"text": "sentence classification task", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.789198656876882}]}, {"text": "We use the CogALex-V 2016 shared task dataset in our experiments which is described in the next section.", "labels": [], "entities": [{"text": "CogALex-V 2016 shared task dataset", "start_pos": 11, "end_pos": 45, "type": "DATASET", "confidence": 0.9083834648132324}]}, {"text": "This dataset consisting of term pairs is tokenized using white space tokenizer.", "labels": [], "entities": []}, {"text": "We performed both binary and multi-class classification on the given data set containing two binary and five multi-class relations from subtask-1 and subtask-2 respectively.", "labels": [], "entities": []}, {"text": "We used Kim's (2014) Theano implementation of CNN 3 for training the CNN model.", "labels": [], "entities": [{"text": "Kim's (2014) Theano implementation of CNN 3", "start_pos": 8, "end_pos": 51, "type": "DATASET", "confidence": 0.6146406680345535}]}, {"text": "We use word embeddings from word2vec which are learned using the skipgram model of Mikolov et. al (2013a,b) by predicting linear context words surrounding the target words.", "labels": [], "entities": []}, {"text": "These word vectors are trained on about 100 billion words from Google News corpus.", "labels": [], "entities": [{"text": "Google News corpus", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.8262836337089539}]}, {"text": "As word embeddings alone have shown good performance in various classification tasks, we also use them in isolation, with varying dimensions, in our experiment.", "labels": [], "entities": []}, {"text": "We performed 10-fold cross-validation (CV) on the entire training set for both the subtasks in random and word2vec embedding settings.", "labels": [], "entities": [{"text": "cross-validation (CV)", "start_pos": 21, "end_pos": 42, "type": "METRIC", "confidence": 0.7180440723896027}]}, {"text": "We initialized random embeddings in the range of [\u22120.25, 0.25].", "labels": [], "entities": []}, {"text": "We did not use any external corpus for training our model but used precompiled word2vec embeddings trained on about 100 billion words from Google News corpus.", "labels": [], "entities": [{"text": "Google News corpus", "start_pos": 139, "end_pos": 157, "type": "DATASET", "confidence": 0.7707297404607137}]}, {"text": "We used a stochastic gradient descent-based optimization method for minimizing the cross entropy loss during the training with the Rectified Linear Unit (ReLU) non-linear activation function.", "labels": [], "entities": []}, {"text": "The hyper-parameters we varied are the drop-out, batch size, embedding dimension and hidden node sizes for training our models in cross-validation setting for finding    the optimal model using training set.", "labels": [], "entities": []}, {"text": "We performed grid search over these value ranges for the mentioned hyper parameters: drop out{0.1,0.2,0.3,0.4,0.5,0.6}, batch size{12,24,32,48,60}, embedding dimension{50,100,150,200,250,300} and hidden node sizes{100,200,300,400,500}.", "labels": [], "entities": []}, {"text": "Optimal results are obtained using drop out-0.5, batch size-32,embedding size-300 and hidden node size-300 for subtask-1 and dropout-0.5, batchSize-24, embedding size-300 and hidden node size-400 for subtask-2 in cross validation setting as shown in tables 3 and 5.", "labels": [], "entities": []}, {"text": "We used fixed context-window sizes set at as max length of the term pair in given corpus is 2 for both the tasks.", "labels": [], "entities": []}, {"text": "We also used fixed number of 25 iterations with default learning rate (0.95) for training our models.", "labels": [], "entities": []}, {"text": "In this section, we describe CogALex-V 2016 shared task data sets and the experimental results.", "labels": [], "entities": [{"text": "CogALex-V 2016 shared task data sets", "start_pos": 29, "end_pos": 65, "type": "DATASET", "confidence": 0.8703132967154185}]}, {"text": "We used a dataset extracted from EVALution 1.0 (, which was developed from WordNet and ConceptNet, and which was further filtered by native speakers in a CrowdFlower task.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.9416555166244507}]}, {"text": "This data set is split into training and test sets.", "labels": [], "entities": []}, {"text": "The distribution of training and test splits are shown in tables 1 and 2.", "labels": [], "entities": []}, {"text": "The samples in the subtask-1 and subtask-2 test set are unbalanced and majority of relation classes are \"FALSE\" and \"random\" in the given train and test sets.", "labels": [], "entities": [{"text": "FALSE", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9942319989204407}]}, {"text": "We evaluated results on test sets using trained models with the optimal parameters for both the tasks and compared results against random baseline results as shown in tables 4 and 6.", "labels": [], "entities": []}, {"text": "CogALex-V shared task results are evaluated using weighted-F1 measure on both the tasks.", "labels": [], "entities": []}, {"text": "Weighted F-1 values for all the relations except for \"random\" relation are computed and reported on subtask-2.", "labels": [], "entities": [{"text": "F-1", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9877992868423462}]}, {"text": "On subtask-1, i.e. for relation detection, in the cross-validation setting, it is shown that CNN with word2vec embedding setting performed (16%F1) better than the random embeddings.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9266087412834167}, {"text": "F1", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.9977926015853882}]}, {"text": "On test set, CNN with word2vec embeddings outperformed (13%) the random baseline results.", "labels": [], "entities": []}, {"text": "On subtask-2, i.e for relation type detection, in the cross-validation setting, it is shown that CNN with word2vec embedding setting performed (14.63%F1) better than the random embeddings learned from the training set.", "labels": [], "entities": [{"text": "relation type detection", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.877930223941803}, {"text": "F1", "start_pos": 150, "end_pos": 152, "type": "METRIC", "confidence": 0.9981623291969299}]}, {"text": "On the test set, CNN with word2vec embeddings outperformed (14.64%) the random baseline results.", "labels": [], "entities": []}, {"text": "These results suggest that word2vec-based distributional embeddings significantly contributed in improving the relation classification performance.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.909629762172699}]}], "tableCaptions": [{"text": " Table 1: Training and test data sets: Subtask-1  of CogALex-V shared task", "labels": [], "entities": []}, {"text": " Table 2: Training and test data sets: Subtask-2 of  CogALex-V shared task", "labels": [], "entities": []}, {"text": " Table 3: Avg. 10-fold cross-validation results  on subtask-2 with rand. & word2vec embeds.", "labels": [], "entities": [{"text": "Avg", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9260202050209045}]}, {"text": " Table 4: Subtask-1 test set results word2vec embed- ding setting Vs. Random baseline.", "labels": [], "entities": []}, {"text": " Table 5: Avg. 10-fold cross-validation results  on subtask-2 with rand. & word2vec embeds.", "labels": [], "entities": [{"text": "Avg", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.928904116153717}]}, {"text": " Table 6: subtask-2 results in word2vec embedding  setting vs Random baseline.", "labels": [], "entities": []}, {"text": " Table 7: Confusion matrix of subtask-1 test set  results", "labels": [], "entities": []}]}