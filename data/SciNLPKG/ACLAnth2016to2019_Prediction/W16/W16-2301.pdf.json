{"title": [], "abstractContent": [{"text": "This paper presents the results of the WMT16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, run-time estimation of MT quality), and an automatic post-editing task and bilingual document alignment task.", "labels": [], "entities": [{"text": "WMT16 shared tasks", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.5540630022684733}, {"text": "machine translation (MT)", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.8385672926902771}, {"text": "bilingual document alignment", "start_pos": 287, "end_pos": 315, "type": "TASK", "confidence": 0.576502114534378}]}, {"text": "This year, 102 MT systems from 24 institutions (plus 36 anonymized online systems) were submitted to the 12 translation directions in the news translation task.", "labels": [], "entities": [{"text": "news translation task", "start_pos": 138, "end_pos": 159, "type": "TASK", "confidence": 0.7830739418665568}]}, {"text": "The IT-domain task received 31 submissions from 12 institutions in 7 directions and the Biomedical task received 15 submissions systems from 5 institutions.", "labels": [], "entities": []}, {"text": "Evaluation was both automatic and manual (relative ranking and 100-point scale assessments).", "labels": [], "entities": []}, {"text": "The quality estimation task had three sub-tasks, with a total of 14 teams, submitting 39 entries.", "labels": [], "entities": [{"text": "quality estimation task", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7111045221487681}]}, {"text": "The automatic post-editing task had a total of 6 teams, submitting 11 entries .", "labels": [], "entities": []}], "introductionContent": [{"text": "We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT) held at ACL 2016", "start_pos": 70, "end_pos": 124, "type": "TASK", "confidence": 0.7894129931926728}]}, {"text": "This conference builds on nine previous WMT workshops (.", "labels": [], "entities": [{"text": "WMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.8550092577934265}]}, {"text": "This year we conducted several official tasks.", "labels": [], "entities": []}, {"text": "We held 12 translation tasks this year, between English and each of Czech, German, Finnish, Russian, Romanian, and Turkish.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.8866578936576843}]}, {"text": "The Romanian and Turkish translation tasks were new this year, providing a lesser resourced data condition on challenging language pairs.", "labels": [], "entities": [{"text": "Romanian and Turkish translation", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.5727678835391998}]}, {"text": "The system outputs for each task were evaluated both automatically and manually.", "labels": [], "entities": []}, {"text": "The human evaluation ( \u00a73) involves asking human judges to rank sentences output by anonymized systems.", "labels": [], "entities": []}, {"text": "We obtained large numbers of rankings from researchers who contributed evaluations proportional to the number of tasks they entered.", "labels": [], "entities": []}, {"text": "We made data collection more efficient and used TrueSkill as ranking method.", "labels": [], "entities": [{"text": "data collection", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7625780999660492}]}, {"text": "We also explored a novel way of ranking machine translation systems by judgments of adequacy and fluency on a 100-point scale.", "labels": [], "entities": [{"text": "ranking machine translation", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.6907511452833811}]}, {"text": "The IT translation task ( \u00a74) was introduced this year and focused on domain adaptation of MT to the IT (information technology) domain and translation of answers in a cross-lingual help-desk service, where hardware&software troubleshooting answers are translated from English to the users' languages: Bulgarian, Czech, German, Spanish, Basque, Dutch and Portuguese.", "labels": [], "entities": [{"text": "IT translation task", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8268317977587382}, {"text": "domain adaptation", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7495873272418976}, {"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.7672255635261536}]}, {"text": "Similarly as in the News translation task, training and test data were provided and the system outputs were evaluated both automatically and manually.", "labels": [], "entities": [{"text": "News translation task", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7994696299235026}]}, {"text": "Another task newly introduced this year was the biomedical translation task ( \u00a75).", "labels": [], "entities": [{"text": "biomedical translation task", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.8986207445462545}]}, {"text": "Participants were asked to translate the titles and abstracts of scientific articles indexed in the Scielo database.", "labels": [], "entities": [{"text": "Scielo database", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.6982605159282684}]}, {"text": "Training and test data were provided for two subdomains, biological sciences and health sciences, and three language pairs, Portuguese/English, Spanish/English and French/English.", "labels": [], "entities": []}, {"text": "This task therefore provided data fora language not previously covered in WMT, Portuguese.", "labels": [], "entities": [{"text": "WMT", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.821963369846344}]}, {"text": "The system outputs for each language pair were evaluated both automatically and manually.", "labels": [], "entities": []}, {"text": "The quality estimation task ( \u00a76) this year included three subtasks: sentence-level prediction of post-editing effort scores, word and phrase-level prediction of good/bad labels, and document-level prediction of human post-editing scores.", "labels": [], "entities": [{"text": "word and phrase-level prediction of good/bad labels", "start_pos": 126, "end_pos": 177, "type": "TASK", "confidence": 0.736039270957311}, {"text": "document-level prediction of human post-editing", "start_pos": 183, "end_pos": 230, "type": "TASK", "confidence": 0.7517930209636688}]}, {"text": "Datasets were released with English\u2192German IT translations for sentence and word/phrase level, and English\u2194Spanish news translations for document level.", "labels": [], "entities": []}, {"text": "The automatic post-editing task ( \u00a77) examined automatic methods for correcting errors produced by an unknown machine translation system.", "labels": [], "entities": [{"text": "correcting errors produced by an unknown machine translation", "start_pos": 69, "end_pos": 129, "type": "TASK", "confidence": 0.6879982091486454}]}, {"text": "Participants were provided with training triples containing source, target and human post-edits, and were asked to return automatic post-edits for unseen (source, target) pairs.", "labels": [], "entities": []}, {"text": "In this second round, the task focused on correcting English\u2192German translations in the IT domain.", "labels": [], "entities": [{"text": "correcting English\u2192German translations", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.915499210357666}]}, {"text": "The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation and estimation methodologies for machine translation.", "labels": [], "entities": [{"text": "WMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8719257116317749}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8308209776878357}, {"text": "machine translation", "start_pos": 242, "end_pos": 261, "type": "TASK", "confidence": 0.8069436848163605}]}, {"text": "As before, all of the data, translations, and collected human judgments are publicly available.", "labels": [], "entities": []}, {"text": "We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6579025685787201}]}, {"text": "News and IT translations are also available for interactive visualization and comparison of differences between systems at http://wmt.ufal.cz using MT-ComparEval ().", "labels": [], "entities": [{"text": "MT-ComparEval", "start_pos": 148, "end_pos": 161, "type": "DATASET", "confidence": 0.8837057948112488}]}], "datasetContent": [{"text": "Following the trend from previous years, WMT16 ended up being the largest evaluation campaign to date.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.8727909922599792}]}, {"text": "Similar to last year, we collected researcherbased judgments only (as opposed to crowdsourcing annotations from a tool like Mechanical Turk).", "labels": [], "entities": []}, {"text": "For the News translation task, a total of 150 individual annotator accounts were involved.", "labels": [], "entities": [{"text": "News translation task", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.801268478234609}]}, {"text": "Users came from 33 different research groups and contributed judgments on 10,833 HITs.", "labels": [], "entities": []}, {"text": "Each HIT comprises three 5-way ranking tasks fora total of 32,499 such tasks.", "labels": [], "entities": []}, {"text": "Under ordinary circumstances, each of the tasks would correspond to ten individual pairwise system comparisons denoting whether a system A was judged better than, worse than, or equivalent to another system B.", "labels": [], "entities": []}, {"text": "However, since many systems have produced the same outputs fora particular sentence, we are often able to produce more than ten comparisons (Section 3.2), ending up with a total of 569,287 pairwise annotations-a 75.2% increase over the expected baseline of 324,990 pairs.", "labels": [], "entities": []}, {"text": "This is smaller than last year's gain of 87.1% as we have decided to preserve punctuation differences.", "labels": [], "entities": []}, {"text": "Section 3.2 provides more details on our pre-processing.", "labels": [], "entities": []}, {"text": "In total, our human annotators spent nearly 39 days and 3 hours working in Appraise.", "labels": [], "entities": [{"text": "Appraise", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.7116179466247559}]}, {"text": "This gives an average annotation time of 6.4 hours per user.", "labels": [], "entities": []}, {"text": "The average annotation time per HIT amounts to 5 minutes and 12 seconds.", "labels": [], "entities": [{"text": "HIT", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.5800127387046814}]}, {"text": "This is a little slower than last year's average time of 4 minutes and 53 seconds.", "labels": [], "entities": []}, {"text": "Similar to the previous campaign, several of the annotators passed the mark of more than 100 HITs annotated (the maximum number being 684) and, again, some worked for more than 24 hours (the most patient annotator contributing a little over 99 hours of annotation work).", "labels": [], "entities": []}, {"text": "The effort that goes into the manual evaluation campaign each year is impressive, and we are grateful to all participating individuals and teams.", "labels": [], "entities": []}, {"text": "We believe that human annotation provides the best decision basis for evaluation of machine translation output and it is great to see continued contributions on this large scale.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.7979685068130493}]}, {"text": "In addition to the standard relative ranking (RR) manual evaluation, this year anew method of human evaluation was also trialed in the main translation task: monolingual direct assessment (DA) of translation fluency () and adequacy (.", "labels": [], "entities": []}, {"text": "Agreement between human assessors of translation quality is a known problem in evaluation of MT and DA therefore aims to simplify translation assessment, which conventionally takes the form of a bilingual evaluation, by restructuring the task into a monolingual assessment.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9476116299629211}, {"text": "translation assessment", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.9656780362129211}]}, {"text": "provides a screenshot of DA adequacy assessment, where the task is structured as a monolingual similarity of meaning task.", "labels": [], "entities": [{"text": "DA adequacy assessment", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.8955211838086446}]}, {"text": "Human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation on an analogue scale, which corresponds to an underlying absolute 0-100 rating.", "labels": [], "entities": [{"text": "analogue scale", "start_pos": 146, "end_pos": 160, "type": "METRIC", "confidence": 0.9617023169994354}]}, {"text": "DA fluency assessment is similar with two exceptions, firstly no reference translation is displayed and secondly, assessors are asked to rate how much they agree that a given translation is fluent target language text.", "labels": [], "entities": [{"text": "DA fluency assessment", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7872693339983622}]}, {"text": "DA flu-: Official results for the WMT16 translation task.", "labels": [], "entities": [{"text": "DA flu-", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6242341299851736}, {"text": "WMT16 translation task", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8953377803166708}]}, {"text": "Systems are ordered by their inferred system means, though systems within a cluster are considered tied.", "labels": [], "entities": []}, {"text": "Lines between systems indicate clusters according to bootstrap resampling at p-level p \u2264 .05.", "labels": [], "entities": []}, {"text": "Systems with gray background indicate use of resources that fall outside the constraints provided for the shared task.", "labels": [], "entities": []}, {"text": "ency therefore provides a dimension of the assessment that cannot be biased by the presence of a reference translation.", "labels": [], "entities": []}, {"text": "For both fluency and adequacy, the simpler monolingual assessment DA employs also allows the sentence length restriction to be removed.", "labels": [], "entities": []}, {"text": "DA also aims to avoid the possible source of bias identified in, introduced by simultaneous assessment of several translations at once, where systems for which translations were more frequently compared to other low or high quality outputs resulted in either an unfair advantage or disadvantage for that system.", "labels": [], "entities": [{"text": "DA", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.5810328722000122}]}, {"text": "We therefore elicit assessments of individual translations in isolation from the output of other systems, an important criteria when aiming for absolute quality judgments.", "labels": [], "entities": []}, {"text": "To assess the quality of APE systems and produce a ranking based on human judgement, as well as analyze how humans perceive TER/BLEU performance differences between the submitted systems, two runs of human evaluations were conducted.", "labels": [], "entities": [{"text": "APE", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8770669102668762}, {"text": "TER", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.981240451335907}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.7762443423271179}]}, {"text": "The whole evaluation took approximately a month and was performed mainly by student translators who annotated the APE systems' outputs.", "labels": [], "entities": [{"text": "APE systems' outputs", "start_pos": 114, "end_pos": 134, "type": "DATASET", "confidence": 0.8348866105079651}]}, {"text": "This subsection describes the human evaluation pro-cedure, gives details about the annotators' backgrounds and profiles, and finally presents the results of the evaluation.", "labels": [], "entities": []}, {"text": "System performance was evaluated by computing the distance between automatic and human postedits of the machine-translated sentences present in the test set (i.e. for each of the 2, 000 target test sentences).", "labels": [], "entities": []}, {"text": "Differently from the first edition of the task, in which this distance was only measured in terms of Translation Error Rate (TER)), this year the BLEU () score was also used.", "labels": [], "entities": [{"text": "Translation Error Rate (TER))", "start_pos": 101, "end_pos": 130, "type": "METRIC", "confidence": 0.9467875262101492}, {"text": "BLEU () score", "start_pos": 146, "end_pos": 159, "type": "METRIC", "confidence": 0.9577606916427612}]}, {"text": "TER is an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9369082450866699}, {"text": "MT-related tasks", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.9372290372848511}, {"text": "quality estimation)", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7130672931671143}]}, {"text": "32 BLEU is the reference metric for MT evaluation and is based on modified n-gram precision to find how many of the n-grams in the candidate translation are present in the reference translation over the entire test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9363126158714294}, {"text": "MT evaluation", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.959631621837616}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.7907937169075012}]}, {"text": "The main difference between the two metrics is that TER works at word level, while BLEU takes advantage of words and n-grams with n from 2 to 4.", "labels": [], "entities": [{"text": "TER", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9973074197769165}, {"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9983041286468506}]}, {"text": "Systems were ranked based on the average TER calculated on the test set by using the TERcom 33 software: lower average TER scores correspond to higher ranks.", "labels": [], "entities": [{"text": "TER", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9965355396270752}, {"text": "TERcom 33 software", "start_pos": 85, "end_pos": 103, "type": "DATASET", "confidence": 0.809481680393219}, {"text": "TER", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9924905896186829}]}, {"text": "BLEU was computed using the multi-bleu.perl package 34 available in MOSES.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9815442562103271}, {"text": "MOSES", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.9021825790405273}]}, {"text": "Differently from the pilot round, in which TER was computed both in case-sensitive and caseinsensitive mode, this year we opted for only one mode.", "labels": [], "entities": [{"text": "TER", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9682258367538452}]}, {"text": "Working with German, for which case errors are of crucial importance, participants' submissions were evaluated with the more strict casesensitive mode.", "labels": [], "entities": []}, {"text": "Repetition rate measures the repetitiveness inside a text by looking at the rate of non-singleton n-gram types (n=1...4) and combining them using the geometric mean.", "labels": [], "entities": []}, {"text": "Larger value means more repetitions in the text.: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data.", "labels": [], "entities": [{"text": "Repetition Rate (RR)", "start_pos": 50, "end_pos": 70, "type": "METRIC", "confidence": 0.9868371248245239}, {"text": "WMT15", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.948441207408905}, {"text": "WMT16", "start_pos": 143, "end_pos": 148, "type": "DATASET", "confidence": 0.9725702404975891}, {"text": "APE Task data", "start_pos": 203, "end_pos": 216, "type": "DATASET", "confidence": 0.6522360642751058}]}, {"text": "The two runs of human evaluation were conducted using the Appraise 37 opensource annotation platform through the ranking task interface.", "labels": [], "entities": [{"text": "Appraise 37 opensource annotation", "start_pos": 58, "end_pos": 91, "type": "DATASET", "confidence": 0.9042675793170929}]}, {"text": "A ranking task consists of a source segment and the outputs of up to 5 anonymized APE systems randomly selected from the set of participants and displayed in random order to human evaluators.", "labels": [], "entities": []}, {"text": "The main difference between the two evaluation runs is the following: for the first run, the annotators were presented with a translation reference consisting of the manual post-edit of the machine-translated source segment, while for the second run no translation reference was presented to the human evaluator.", "labels": [], "entities": []}, {"text": "For both evaluation runs, the non-post-edited MT output was included among the systems to evaluate.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.951737642288208}]}, {"text": "For the second evaluation run, the human post-edited version of the MT output was included among the systems to evaluate.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9512732625007629}]}, {"text": "A total of 200 randomly extracted source segments taken from the test set presented in with their corresponding systems' outputs were considered for the first evaluation run, while 100 source segments went through the second run.", "labels": [], "entities": []}, {"text": "The decision to consider a larger set of segments for the first evaluation run is based on the previous editions of WMT, where human evaluations conducted for the translation tasks included a translation reference.", "labels": [], "entities": [{"text": "WMT", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.8890781402587891}]}, {"text": "The smaller scale evaluation for the second run can be seen as a pilot study, where no translation reference is given to the annotators and where the human post-edit is presented as part of the anonymized systems.", "labels": [], "entities": []}, {"text": "The latter setup allows us to see if APE systems can reach human postediting in terms of quality while avoiding evaluation bias towards a reference.", "labels": [], "entities": []}, {"text": "We carried out six annotation sessions in a controlled environment of approximately 45 to 60 minutes each, divided in two blocks of equal duration with a small break in between.", "labels": [], "entities": []}, {"text": "Prior to the human evaluation task, we provided annotators with a pilot study in order to be introduced to the ranking task and be familiarized with the annotation interface.", "labels": [], "entities": []}, {"text": "For each source sentence, five systems' outputs were randomly selected among the partic-37 https://github.com/cfedermann/Appraise ipants and the non-post-edited MT output.", "labels": [], "entities": []}, {"text": "For the second evaluation run, the human post-edit was included in the random selection of target sentences to annotate.", "labels": [], "entities": []}, {"text": "The human annotators then ranked the outputs from 1 to 5 (1 being the best) with ties allowed.", "labels": [], "entities": []}, {"text": "All source segments were evaluated by at least 3 annotators.", "labels": [], "entities": []}, {"text": "The annotations were then used with the TrueSkill 38 adaptive ranking system to produce a score for each system based on their inferred means).", "labels": [], "entities": [{"text": "TrueSkill 38 adaptive ranking", "start_pos": 40, "end_pos": 69, "type": "DATASET", "confidence": 0.8460169583559036}]}, {"text": "This score was used to sort and cluster the systems submitted by the participants, as well as the MT output and the human post-edit, and produce the final ranking presented in Section 7.5.3", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.6994017362594604}]}], "tableCaptions": [{"text": " Table 5: \u03ba scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the  human evaluation campaign. Scores are in line with results from WMT14 and WMT15.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 192, "end_pos": 197, "type": "DATASET", "confidence": 0.9629024863243103}, {"text": "WMT15", "start_pos": 202, "end_pos": 207, "type": "DATASET", "confidence": 0.960128903388977}]}, {"text": " Table 6: Official results for the WMT16 translation task. Systems are ordered by their inferred system means, though systems  within a cluster are considered tied. Lines between systems indicate clusters according to bootstrap resampling at p-level  p \u2264 .05. Systems with gray background indicate use of resources that fall outside the constraints provided for the shared task.", "labels": [], "entities": [{"text": "WMT16 translation task", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.8475944797197977}]}, {"text": " Table 7: Numbers of system output translations evaluated on Mechanical Turk for direct assessment (DA) in WMT16, numbers  exclude quality control items.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.8285440802574158}]}, {"text": " Table 8: Number of unique human assessors for DA ade- quacy and fluency on Mechanical Turk in WMT16, (A) those  whose scores for bad reference pairs were significantly dif- ferent and numbers of unique human assessors in (A) whose  scores for exact repeat items also showed no significant dif- ference, paired Wilcoxon signed-rank significance test was  applied in both cases.", "labels": [], "entities": [{"text": "DA ade- quacy", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.9344863295555115}, {"text": "WMT16", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.67095947265625}]}, {"text": " Table 9: DA mean scores for WMT16 translation task participating systems for translation into English.", "labels": [], "entities": [{"text": "DA mean scores", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9524210492769877}, {"text": "WMT16 translation task", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.6463620066642761}, {"text": "translation into English", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.8669509887695312}]}, {"text": " Table 10: DA mean scores for WMT16 translation task par- ticipating systems for translation from English into Russian.", "labels": [], "entities": [{"text": "DA mean scores", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.9436797102292379}, {"text": "WMT16 translation task", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7826758424441019}, {"text": "translation from English into Russian", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.8380658149719238}]}, {"text": " Table 12: Official results for the WMT16 IT translation task. Systems are ordered by their inferred system means, though  systems within a cluster are considered tied. Lines between systems indicate clusters according to bootstrap resampling at p- level p \u2264 .05. Systems with gray background indicate use of resources that fall outside the constraints provided for the shared  task.", "labels": [], "entities": [{"text": "WMT16 IT translation task", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.7285441011190414}]}, {"text": " Table 13: Amount of manual-evaluation pairwise comparisons (after \"de-collapsing\" multi-system outputs) collected and \u03ba  scores measuring inter-and intra-annotator agreement in the IT task. Cf.", "labels": [], "entities": [{"text": "Amount", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9725508689880371}]}, {"text": " Table 14: Statistics on training and test collections for the Biomedical Translation Task. \"T\" corresponds to percentage of  titles and \"A\" to percentage of abstracts, separated by a slash. \"Docs\" to total number of documents, \"Lang\" identifies the  language,\"Sents\" to total number of sentences and \"Tokens\" to total number of tokens.", "labels": [], "entities": [{"text": "Biomedical Translation Task", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.9490802884101868}]}, {"text": " Table 16: Official BLEU scores for the WMT16 Biomedical Translation task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9212309122085571}, {"text": "WMT16 Biomedical Translation task", "start_pos": 40, "end_pos": 73, "type": "TASK", "confidence": 0.7848929241299629}]}, {"text": " Table 17: Results for the manual validation carried out in Appraise for the Biomedical Translation task.", "labels": [], "entities": [{"text": "Biomedical Translation task", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.9435031215349833}]}, {"text": " Table 19: Official results for the scoring ad ranking variants of the WMT16 Quality Estimation Task 1. The systems are  ranked according to the Pearson r metric and significance results are also computed for this metric. The winning submissions  are indicated by a \u2022. These are the top-scoring submission and those that are not significantly worse according to Williams  test with 95% confidence intervals. The systems in the grey area are not different from the baseline system at a statistically  significant level according to the same test.", "labels": [], "entities": [{"text": "WMT16 Quality Estimation Task", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.6497399359941483}, {"text": "Pearson r metric", "start_pos": 145, "end_pos": 161, "type": "METRIC", "confidence": 0.8995255629221598}, {"text": "Williams  test", "start_pos": 362, "end_pos": 376, "type": "DATASET", "confidence": 0.8421155512332916}]}, {"text": " Table 20: Datasets for Task 2.", "labels": [], "entities": []}, {"text": " Table 21: Official results for the WMT16 Quality Estimation Task 2. The winning submissions are indicated by a \u2022. These  are the top-scoring submission and those that are not significantly worse according to approximate randomisation tests with  95% confidence intervals. The grey area indicates the submissions whose results are not statistically different from the baseline  according to the same test.", "labels": [], "entities": [{"text": "WMT16 Quality Estimation Task", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.7191872522234917}]}, {"text": " Table 23: Datasets for Task 2p.", "labels": [], "entities": []}, {"text": " Table 24: Official results for the WMT16 Quality Estimation Task 2p. The winning submissions are indicated by a \u2022. These are  the top-scoring submission and those that are not significantly worse according to approximate randomisation tests with 95%  confidence intervals. The grey area indicates the submissions whose results are not statistically different from the baseline.", "labels": [], "entities": [{"text": "WMT16 Quality Estimation Task", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.695672407746315}]}, {"text": " Table 25: Results for the WMT16 Quality Estimation Task 2p computed in terms of phrase-level F1-scores. The winning  submissions are indicated by a \u2022. These are the top-scoring submission and those that are not significantly worse according to  approximate randomisation tests with 95% confidence intervals. The grey area indicates the submissions whose results are not  statistically different from the baseline.", "labels": [], "entities": [{"text": "WMT16 Quality Estimation Task", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.7299471497535706}, {"text": "F1-scores", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.8061724901199341}]}, {"text": " Table 26: AVG and STDEV of the post-edited data.", "labels": [], "entities": [{"text": "AVG", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9843504428863525}, {"text": "STDEV", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.8030143976211548}]}, {"text": " Table 27: Official results for the scoring ad ranking variants of the WMT16 Quality Estimation Task 3. The systems are  ranked according to the Pearson r metric and significance results are also computed for this metric. The winning submissions  are indicated by a \u2022. These are the top-scoring submission and those that are not significantly worse according to Williams  test with 95% confidence intervals. The systems in the grey area are not different from the baseline system at a statistically  significant level according to the same test.", "labels": [], "entities": [{"text": "WMT16 Quality Estimation Task", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.6650303304195404}, {"text": "Pearson r metric", "start_pos": 145, "end_pos": 161, "type": "METRIC", "confidence": 0.9006407062212626}, {"text": "Williams  test", "start_pos": 362, "end_pos": 376, "type": "DATASET", "confidence": 0.8420795500278473}]}, {"text": " Table 30: Comparison of systems' performance in Task 2 (word-level) and 2p (phrase-level). Performance is evaluated in  terms of word-level F1-mult scores computed on the test set used for the Task 2p. The submissions to the word-level task are  modified in order to comply with the phrase-level task.", "labels": [], "entities": [{"text": "F1-mult", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.7271023988723755}]}, {"text": " Table 32: Repetition Rate (RR) of the WMT15 (English- Spanish, news domain, crowdsourced post-edits) and  WMT16 (English-German, IT domain, professional post- editors) APE Task data.", "labels": [], "entities": [{"text": "Repetition Rate (RR)", "start_pos": 11, "end_pos": 31, "type": "METRIC", "confidence": 0.9756919264793396}, {"text": "WMT15", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9678698182106018}, {"text": "WMT16", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.9687184691429138}, {"text": "APE Task data", "start_pos": 169, "end_pos": 182, "type": "DATASET", "confidence": 0.6826799809932709}]}, {"text": " Table 34: Official results for the WMT16 Automatic Post- editing task -average TER (\u2193), BLEU score (\u2191).", "labels": [], "entities": [{"text": "WMT16 Automatic Post- editing task", "start_pos": 36, "end_pos": 70, "type": "TASK", "confidence": 0.6321617563565572}, {"text": "TER", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.881981611251831}, {"text": "BLEU score", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9828700721263885}]}, {"text": " Table 35: Number of test sentences modified, improved and deteriorated by each submitted run.", "labels": [], "entities": []}, {"text": " Table 36: Results of the first run of human evaluation in- cluding human post-edited MT output as translation refer- ence. Scores and ranges are obtained with TrueSkill (Sak- aguchi et al., 2014). Lines between systems indicate clusters  according to bootstrap resampling at p-level p \u2264 0.05 based  on 1, 000 runs. Systems within a cluster are considered tied.", "labels": [], "entities": [{"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9649468064308167}]}, {"text": " Table 37: Results of the second run of human evaluation  without translation reference provided to annotators. Scores  and ranges are obtained with TrueSkill (Sakaguchi et al.,  2014). Lines between systems indicate clusters according to  bootstrap resampling at p-level p \u2264 0.05 based on 1, 000  runs. Systems within a cluster are considered tied.", "labels": [], "entities": []}, {"text": " Table 40: Head to head comparison, ignoring ties, for Czech-English systems", "labels": [], "entities": []}]}