{"title": [{"text": "Modeling Selectional Preferences of Verbs and Nouns in String-to-Tree Machine Translation", "labels": [], "entities": [{"text": "Modeling Selectional Preferences of Verbs and Nouns in String-to-Tree Machine Translation", "start_pos": 0, "end_pos": 89, "type": "TASK", "confidence": 0.657246624881571}]}], "abstractContent": [{"text": "We address the problem of mistranslated predicate-argument structures in syntax-based machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7563669085502625}]}, {"text": "This paper explores whether knowledge about semantic affinities between the target predicates and their argument fillers is useful for translating ambiguous predicates and arguments.", "labels": [], "entities": [{"text": "translating ambiguous predicates and arguments", "start_pos": 135, "end_pos": 181, "type": "TASK", "confidence": 0.9024038195610047}]}, {"text": "We propose a selectional preference feature based on the selectional association measure of Resnik (1996) and integrate it in a string-to-tree decoder.", "labels": [], "entities": []}, {"text": "The feature models selectional preferences of verbs for their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments.", "labels": [], "entities": []}, {"text": "We compare our features with a variant of the neural relational dependency language model (RDLM) (Sennrich, 2015) and find that neither of the features improves automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "We conclude that mistranslated verbs, errors in the target syntactic trees produced by the de-coder and underspecified syntactic relations are negatively impacting these features .", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntax-based machine translation systems have had some success when applied to language pairs with major structural differences such as GermanEnglish or Chinese-English.", "labels": [], "entities": [{"text": "Syntax-based machine translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6997227470080057}]}, {"text": "Modeling the target side syntactic structure is important in order to produce grammatical, fluent translations and could bean intermediate step on which to build a semantic representation of the target sentence.", "labels": [], "entities": []}, {"text": "However these systems still suffer from errors such as scrambled or mis-translated predicate-argument structures.", "labels": [], "entities": []}, {"text": "We give a few examples of such errors in.", "labels": [], "entities": []}, {"text": "In example a) the baseline system MT1 mistranslates the verb besichtigt as viewed.", "labels": [], "entities": []}, {"text": "The system MT2 which uses information about the semantic affinity between the verb and its argument produces the correct translation visited.", "labels": [], "entities": [{"text": "MT2", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.7651849389076233}]}, {"text": "The semantic affinity score , shown on the right, for the verb viewed and argument trip in the syntactic relation prep on is indicating a stronger affinity than for the baseline translation.", "labels": [], "entities": [{"text": "semantic affinity score", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.6207484205563863}]}, {"text": "In example b) the baseline system MT1 mistranslates the noun Aufnahmen as recordings while the system MT2 produces the correct translation images which is a better fit for the prepositional modifier from the telescope.", "labels": [], "entities": [{"text": "MT1", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.810423731803894}, {"text": "MT2", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.8772600293159485}]}, {"text": "Syntax-based MT systems handle long distance reordering with synchronous translation rules such as: root \u2192 RB \u223c0 V BZ \u223c1 sich nsubj \u223c2 prep \u223c3 , This rule is useful for reordering the verb and its arguments according to the target side word order.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9666104316711426}]}, {"text": "However the rule does not contain the lexical head for the verb, the subject and the prepositional modifier.", "labels": [], "entities": []}, {"text": "Therefore the entire predicate argument structure is translated by subsequent independent rules.", "labels": [], "entities": []}, {"text": "The language model context will capture at most the verb and one main argument.", "labels": [], "entities": []}, {"text": "Due to the lack of a larger source or target context the resulting predicate-argument structures are often not semantically coherent.", "labels": [], "entities": []}, {"text": "This paper explores whether knowledge about semantic affinities between the target predicates and their argument fillers is useful for translating ambiguous predicates and arguments.", "labels": [], "entities": [{"text": "translating ambiguous predicates and arguments", "start_pos": 135, "end_pos": 181, "type": "TASK", "confidence": 0.9024038195610047}]}, {"text": "We propose a selectional preference feature for string-to-tree statistical machine translation based on the information theoretic measure of.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6160937746365865}]}, {"text": "The feature models selectional preferences of verbs for) Affinity a) SRC Bei nur einer Reise k\u00f6nnen nicht alle davon besichtigt werden.", "labels": [], "entities": []}, {"text": "REF You won't be able to visit all of them on one trip . MT1 Not all of them can be viewed on only one trip.", "labels": [], "entities": [{"text": "REF", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8430289626121521}, {"text": "MT1", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8542077541351318}]}, {"text": "(prep on, viewed, trip) -0.154 MT2 Not all of them can be visited on only one trip.", "labels": [], "entities": [{"text": "MT2", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.5646098852157593}]}, {"text": "(prep on, visited, trip) 1.042 b) SRC Eine der sch\u00e4rfsten Aufnahmen des Hubble-Teleskops REF One of the sharpest pictures from the Hubble telescope MT1 One of the strongest recordings of the Hubble telescope (prep of, recordings, telescope) -0.0004 MT2 One of the strongest images from the Hubble telescope (prep from, images, telescope) 0.3917: Examples of errors in the predicate-argument structure produced by a syntax-based MT system.", "labels": [], "entities": [{"text": "SRC", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.6877093315124512}]}, {"text": "a) mistranslated verb b) mistranslated noun.", "labels": [], "entities": []}, {"text": "Semantic affinity scores are shown on the right.", "labels": [], "entities": []}, {"text": "Higher scores indicate a stronger affinity.", "labels": [], "entities": []}, {"text": "Negative scores indicate alack of affinity.", "labels": [], "entities": [{"text": "alack", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9445708394050598}, {"text": "affinity", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.5020835995674133}]}, {"text": "their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments.", "labels": [], "entities": []}, {"text": "Previous work has addressed the selectional preferences of prepositions for noun classes) but not the semantic affinities between a predicate and its argument class.", "labels": [], "entities": []}, {"text": "Another line of research on improving translation of predicate-argument structures includes modeling reordering and deletion of semantic roles ().", "labels": [], "entities": [{"text": "translation of predicate-argument structures", "start_pos": 38, "end_pos": 82, "type": "TASK", "confidence": 0.8637261986732483}]}, {"text": "These models however do not encode information about the lexical semantic affinities between target predicates and their arguments.", "labels": [], "entities": []}, {"text": "proposes a relational dependency language model (RDLM) for string-to-tree machine translation.", "labels": [], "entities": [{"text": "string-to-tree machine translation", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.667748769124349}]}, {"text": "One component of RDLM predicts the headword of a dependent conditioned on a wide syntactic context.", "labels": [], "entities": []}, {"text": "Our feature is different as it quantifies the amount of information that the predicate carries about the argument class filling a particular syntactic function.", "labels": [], "entities": []}, {"text": "For one variant of the proposed feature we found a slight improvement in automatic evaluation metrics when translating short sentences as well as an increase in precision for verb translation.", "labels": [], "entities": [{"text": "precision", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9991206526756287}, {"text": "verb translation", "start_pos": 175, "end_pos": 191, "type": "TASK", "confidence": 0.742255225777626}]}, {"text": "However the features generally did not improve automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "We conclude that mistranslated verbs, errors in the target syntactic trees produced by the decoder and underspecified syntactic relations are negatively impacting these features.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes related work on improving translation of predicate-argument structures.", "labels": [], "entities": [{"text": "translation of predicate-argument structures", "start_pos": 46, "end_pos": 90, "type": "TASK", "confidence": 0.8644445091485977}]}, {"text": "Section 3 introduces the selectional preference feature.", "labels": [], "entities": [{"text": "selectional preference", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.9075053334236145}]}, {"text": "Section 4 describes the experimental setup and Section 5 presents the results of automatic evaluation as well as a qualitative analysis of the machine translated output.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our baseline system for translating German into English is the Moses string-to-tree toolkit implementing GHKM rule extraction ().", "labels": [], "entities": [{"text": "GHKM rule extraction", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.8083108266194662}]}, {"text": "The string-to-tree translation model is based on asynchronous context-free grammar (SCFG) that is extracted from word-aligned parallel data with target-side syntactic annotation.", "labels": [], "entities": [{"text": "string-to-tree translation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.7131247818470001}]}, {"text": "The system was trained on all available data provided at WMT15 2 (.", "labels": [], "entities": [{"text": "WMT15 2", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9221968650817871}]}, {"text": "The number of sentences in the training, tuning and test sets are shown in Table 3.", "labels": [], "entities": []}, {"text": "We use the following rule extraction parameters: Rule Depth = 5, Node Count = 20, Rule Size = 5.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7308259606361389}, {"text": "Rule Depth", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9026268124580383}]}, {"text": "At decoding time we give a high penalty to glue rules and allow non-terminals to span a maximum of 50 words.", "labels": [], "entities": []}, {"text": "We train a 5-gram language model on all available monolingual data 3 using the SRILM toolkit) with modified Kneser-Ney smoothing) for training and KenLM (Heafield, 2011) for language model scoring during decoding.", "labels": [], "entities": [{"text": "KenLM", "start_pos": 147, "end_pos": 152, "type": "METRIC", "confidence": 0.8844666481018066}]}, {"text": "First, we determine the effectiveness of our selectional association features.", "labels": [], "entities": []}, {"text": "We compare the two different selectional association features described in section 3.2: SelAssoc Land SelAssoc C . We report the results of automatic evaluation in.", "labels": [], "entities": []}, {"text": "Neither of the features improved the automatic evaluation scores.", "labels": [], "entities": []}, {"text": "The SelAssoc L suffers from data sparsity while the SelAssoc C feature is overgeneralizing due to noisy clustering.", "labels": [], "entities": []}, {"text": "Adding both features compensates for these issues, however we only see a slight improvement in BLEU scores for shorter sentences: Results for string-to-tree systems with SelAssoc and RDLM-P w features.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9991499185562134}]}, {"text": "The number of clusters used with SelAssoc C is 500.", "labels": [], "entities": []}, {"text": "The triples in parenthesis indicate the context size for ancestors, left siblings and right siblings respectively.", "labels": [], "entities": []}, {"text": "The RDLM-P w configuration (1, 0, 0) captures similar syntactic context as the selectional preference features.", "labels": [], "entities": []}, {"text": "We changed the format of the features in order to experiment with sparse features.", "labels": [], "entities": []}, {"text": "By using sparse features we let the tuning algorithm discriminate between low and high values of the SelAssoc score.", "labels": [], "entities": []}, {"text": "For each of the SelAssoc features we normalized the scores to have zero mean and standard deviation one and mapped them to their corresponding percentile.", "labels": [], "entities": []}, {"text": "A sparse feature was created for each percentile, below and above the mean 6 resulting in a total of 20 sparse features.", "labels": [], "entities": []}, {"text": "However this formulation of the feature also did 2701 sentences with more than 5 words and at most 15 words Up to two standard deviations below the mean and three standard deviations above the mean.", "labels": [], "entities": []}, {"text": "not improve the evaluation scores as shown in the fifth row of.", "labels": [], "entities": []}, {"text": "The lack of variance in automatic evaluation scores can be explained by: a) the feature touches only a few words in the translation and b) the relation between a predicate and its argument is identified at later stages of the bottom-up chart-based decoding when many lexical choices have already been pruned out.", "labels": [], "entities": []}, {"text": "The SelAssoc scores, similar to mutual information scores, are sensitive to outlier events with low frequencies in the training data.", "labels": [], "entities": []}, {"text": "In the next section we investigate whether a more robust model would mitigate some of these issues and experiment with a neural relational dependency language model (RDLM) (Sennrich, 2015).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Example of selectional preference (SelPref) and selectional association (SelAssoc) scores for  different verbs. PRN is the class of pronouns.", "labels": [], "entities": [{"text": "selectional association (SelAssoc)", "start_pos": 58, "end_pos": 92, "type": "METRIC", "confidence": 0.820573353767395}]}, {"text": " Table 5: Number of mistranslated words in 100  sentences manually annotated with error cate- gories.", "labels": [], "entities": []}, {"text": " Table 6.  Neither of the features improved the automatic  evaluation scores. The SelAssoc L suffers from  data sparsity while the SelAssoc C feature is over- generalizing due to noisy clustering. Adding both  features compensates for these issues, however we  only see a slight improvement in BLEU scores  for shorter sentences", "labels": [], "entities": [{"text": "BLEU", "start_pos": 294, "end_pos": 298, "type": "METRIC", "confidence": 0.9992471933364868}]}, {"text": " Table 6: Results for string-to-tree systems with Se- lAssoc and RDLM-P w features. The number of  clusters used with SelAssoc C is 500. The triples  in parenthesis indicate the context size for ances- tors, left siblings and right siblings respectively.  The RDLM-P w configuration (1, 0, 0) captures  similar syntactic context as the selectional prefer- ence features.", "labels": [], "entities": []}, {"text": " Table 7: Average selectional association scores for  the test sets. Scores are aggregated over the main  and prep argument types. main arguments include:  nsubj, nsubjpass, dobj, iobj.", "labels": [], "entities": []}]}