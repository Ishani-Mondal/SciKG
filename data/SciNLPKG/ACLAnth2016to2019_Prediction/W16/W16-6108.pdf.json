{"title": [{"text": "Low-resource OCR error detection and correction in French Clinical Texts", "labels": [], "entities": [{"text": "OCR error detection and correction", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.815045028924942}, {"text": "French Clinical Texts", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.8017788926760355}]}], "abstractContent": [{"text": "In this paper we present a simple yet effective approach to automatic OCR error detection and correction on a corpus of French clinical reports of variable OCR quality within the domain of foetopathology.", "labels": [], "entities": [{"text": "OCR error detection and correction", "start_pos": 70, "end_pos": 104, "type": "TASK", "confidence": 0.8608098030090332}]}, {"text": "While traditional OCR error detection and correction systems rely heavily on external information such as domain-specific lexicons, OCR process information or manually corrected training material , these are not always available given the constraints placed on using medical corpora.", "labels": [], "entities": [{"text": "OCR error detection and correction", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.9122983098030091}]}, {"text": "We therefore propose a novel method that only needs a representative corpus of acceptable OCR quality in order to train models.", "labels": [], "entities": []}, {"text": "Our method uses recurrent neural networks (RNNs) to model sequential information on character level fora given medical text corpus.", "labels": [], "entities": []}, {"text": "By inserting noise during the training process we can simultaneously learn the underlying (character-level) language model and as well as learning to detect and eliminate random noise from the textual input.", "labels": [], "entities": []}, {"text": "The resulting models are robust to the variability of OCR quality but do not require additional, external information such as lexicons.", "labels": [], "entities": [{"text": "OCR", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.916093647480011}]}, {"text": "We compare two different ways of injecting noise into the training process and evaluate our models on a manually corrected data set.", "labels": [], "entities": []}, {"text": "We find that the best performing system achieves a 73% accuracy .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9995912909507751}]}], "introductionContent": [{"text": "While most of the contemporary medical documents are created in electronic form, many of the older patient files are kept in paper version only.", "labels": [], "entities": []}, {"text": "These files represent an invaluable source of information and experience for medical investigations, especially in domains with low-frequency diseases such as foetopathology, the medical domain which specializes in the treatment and diagnosis of illnesses in unborn children.", "labels": [], "entities": []}, {"text": "Over the last two decades, Optical Character Recognition (OCR) technology has improved substantially which has allowed fora massive institutional digitization of textual resources such as books, newspaper articles, ancient handwritten documents, etc.", "labels": [], "entities": [{"text": "Optical Character Recognition (OCR)", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.7686323275168737}]}, {"text": "In recent years, hospitals and medical centers have taken to processing older, paper-based resources into digital form in order to construct knowledge bases and resources that can be consulted by medical staff and students.", "labels": [], "entities": []}, {"text": "When it comes to documents containing patient information, however, the process of digitization or the use of the resulting text corpus are not as straightforward as they may seem on first sight.", "labels": [], "entities": []}, {"text": "Firstly, medical corpora are much less accessible than other general-purpose text corpora since the confidentiality of patients is a first priority.", "labels": [], "entities": []}, {"text": "This results in limited access of researchers to original files which in turns directly limits the quantity of files that can be digitized.", "labels": [], "entities": []}, {"text": "Secondly, text corpora that contain medical information can only be distributed (even internally in hospitals or research centers) when they are de-identified, that is, when all patient-specific information is identified and removed from the OCRed text.", "labels": [], "entities": []}, {"text": "This additional processing step can have a significant impact on the quality of the resulting text corpus when information is incorrectly identified as patient-specific information and consequently trans-formed or removed, e.g. 'Parkinson' in the phrase 'Parkinson's disease'.", "labels": [], "entities": []}, {"text": "A side-effect of the obligation of de-identification is that OCR process information is often not available to the researcher using the text corpus afterwards, since it could potentially be used to reconstruct the original information in the paper version.", "labels": [], "entities": []}, {"text": "Thirdly, medical files in hospitals are generated over many years.", "labels": [], "entities": []}, {"text": "Consequently, the variations in paper, printing techniques or differences in structuring the text (e.g., one-column versus two-column paper formats) can impact the OCR process, and the quality of OCRed files can vary substantially from one year to another.", "labels": [], "entities": [{"text": "OCR", "start_pos": 164, "end_pos": 167, "type": "TASK", "confidence": 0.9393186569213867}]}, {"text": "With the increased use of OCR to digitize paper corpora, the problem of OCR error detection and correction has received considerable attention from the research community, especially as regards to its impact on information retrieval and information extraction tasks.", "labels": [], "entities": [{"text": "OCR error detection and correction", "start_pos": 72, "end_pos": 106, "type": "TASK", "confidence": 0.8432525992393494}, {"text": "information retrieval", "start_pos": 211, "end_pos": 232, "type": "TASK", "confidence": 0.8237995803356171}, {"text": "information extraction tasks", "start_pos": 237, "end_pos": 265, "type": "TASK", "confidence": 0.8511027097702026}]}, {"text": "The majority of the current OCR error correction systems use the same three-step approach: (1) OCR error detection; (2) candidate generation; (3) candidate ranking.", "labels": [], "entities": [{"text": "OCR error correction", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8987458745638529}, {"text": "OCR error detection", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.742599626382192}, {"text": "candidate generation", "start_pos": 120, "end_pos": 140, "type": "TASK", "confidence": 0.804485559463501}]}, {"text": "In the first step, a potential OCR error is detected using either a lookup in a domain-specific lexicon) or unigram language model), and/or by consulting information from the OCR process, i.e., the confidence scores of the recognized characters.", "labels": [], "entities": []}, {"text": "The second step, candidate generation, also heavily depends on external resources, either by generating potential candidate replacements for the erroneous words from a lexicon () or by learning and using a mapping of characters that were often interchanged during the OCR process to generate potential candidates with string distance metrics (.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.8151471018791199}]}, {"text": "Such mappings are known as 'character confusions' but need to be learned over a training corpus of a considerable size before they can become effective).", "labels": [], "entities": [{"text": "character confusions'", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.895826518535614}]}, {"text": "The lack of external information such as OCR process information or domain (and hospital)-specific lexicons and the high variability of OCR quality render these systems useless for OCR error detection in medical text corpora.", "labels": [], "entities": [{"text": "OCR error detection", "start_pos": 181, "end_pos": 200, "type": "TASK", "confidence": 0.9066756169001261}]}, {"text": "Unlike the current state-of-the-art systems, the method proposed in this article requires only a sample of (relatively) clean domain-specific text, and no other external information.", "labels": [], "entities": []}, {"text": "It uses recurrent neural networks (RNNs) to train character-level language models.", "labels": [], "entities": []}, {"text": "By artificially inserting noise into the training data, the system learns to filter out random noise, while learning the domain-specific language model that underlies the documents in the corpus.", "labels": [], "entities": []}, {"text": "Since the models do not depend on external resources the method can also be applied to domain-specific text corpora outside the medical domain, on the condition that the documents in the training corpus are not too heterogeneous.", "labels": [], "entities": []}], "datasetContent": [{"text": "All evaluations in this paper were carried out on a set of 53 files, randomly selected from the Excellent and Good quality subsets, which had been annotated manually by one annotator in two passes.", "labels": [], "entities": [{"text": "Excellent", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9862810373306274}]}, {"text": "These annotations were later verified by a second annotator.", "labels": [], "entities": []}, {"text": "5 The role of the second annotator was to check that the existing annotations were correct and consistent.", "labels": [], "entities": []}, {"text": "Ergo the annotaIn total, the evaluation contains 473 errors.", "labels": [], "entities": []}, {"text": "shows the distribution of the four main types of OCR errors in the evaluation set.", "labels": [], "entities": []}, {"text": "For each error the annotator provided a corrected string.", "labels": [], "entities": []}, {"text": "Consequently, for each document in the evaluation set we had an original version with OCR errors, and a corrected version as the Gold Standard.", "labels": [], "entities": [{"text": "OCR errors", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9689692556858063}, {"text": "Gold Standard", "start_pos": 129, "end_pos": 142, "type": "DATASET", "confidence": 0.9184542894363403}]}, {"text": "as the basic RNN unit since this has shown improved performance on various NLP tasks such as text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.7841334939002991}]}, {"text": "In our model, we stack two LSTM layers on top of each other: the first level is an encoder that reads the source character sequence and the other is a decoder that functions as a language model and generates the output.", "labels": [], "entities": []}, {"text": "We also added a drop-out layer since this has been shown to improve performance ().", "labels": [], "entities": []}, {"text": "The model was implemented in), a python library for deep learning.", "labels": [], "entities": []}, {"text": "tions were not done independently.", "labels": [], "entities": []}, {"text": "Since the annotators did not have access to the original PDF files to check the original text, it was not possible to generate corrected text for some badly corrupted strings.", "labels": [], "entities": []}, {"text": "Since the annotators did not have access to the original PDF files to check the original text, it was not possible to generate corrected text for some badly corrupted strings.", "labels": [], "entities": []}, {"text": "An excellent low-level introduction to RNNs and LSTMs can be found at http://karpathy.github.io/2015/ 05/21/rnn-effectiveness/.", "labels": [], "entities": [{"text": "RNNs and LSTMs", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.6547162036101023}]}, {"text": "In order to learn a robust language model, we fed the neural network with randomly corrupted input strings and provided the original (non-corrupted) strings as output labels.", "labels": [], "entities": []}, {"text": "This way the NN learns both a character level language model that is domainspecific but it also learns to detect and eliminate random noise.", "labels": [], "entities": []}, {"text": "We created corrupted strings by deleting, inserting and substituting one or two characters fora given string.", "labels": [], "entities": []}, {"text": "Since a string could be submitted to multiple corrupting edits this resulted in both monoerror as well as multi-error words in the corrupted string.", "labels": [], "entities": []}, {"text": "We heuristically determined the rate of noise so as to resemble the level of corruption, i.e., number of OCR errors of the actual test data.", "labels": [], "entities": [{"text": "number of OCR errors", "start_pos": 95, "end_pos": 115, "type": "METRIC", "confidence": 0.7097765579819679}]}, {"text": "shows an example of the generated training input with label output.", "labels": [], "entities": []}, {"text": "We used windows of 20 characters from the initial text but since the length of the corrupted text strings varied due insertions and deletions, the network was fed (padded) sequences of 23 characters.", "labels": [], "entities": []}, {"text": "The network was trained on data from the 'Excellent' OCR quality subset.", "labels": [], "entities": []}, {"text": "original text (reference) 'apr\u00e8s l'expulsion de' corrupted text (input) 'arp\u00e8S1'exVlsion e' model output 'apr\u00e8s 1'exulsion de' We experimented with two different string corruption settings: 1.", "labels": [], "entities": []}, {"text": "Random generation (randomNoise) in which we used a random number generator to determine if and which edit options were selected.", "labels": [], "entities": [{"text": "Random generation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7386405616998672}]}, {"text": "Character substitutions were performed at random with characters from the character set; 2.", "labels": [], "entities": [{"text": "Character substitutions", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7241831570863724}]}, {"text": "Insertion of character confusions (confusionPair): In this setting we want to examine if injecting information on possible character confusions in the corpus, i.e. teaching the model that character x is likely to be replaced with character y, leads to a faster convergence of the trained models.", "labels": [], "entities": [{"text": "Insertion of character confusions", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6509610787034035}]}, {"text": "While we do not have annotated training material to learn character confusions, we can exploit the natural redundancy in the corpus: Using a string alignment algorithm we identified near-duplicates in the subset of documents with 'Good' OCR quality.", "labels": [], "entities": [{"text": "character confusions", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.7711953818798065}, {"text": "OCR", "start_pos": 237, "end_pos": 240, "type": "METRIC", "confidence": 0.9472297430038452}]}, {"text": "We then extracted confusion pairs, i.e. 1:1, 2:2, 1:2 and 2:1 character pairs that occurred in the same contexts, and had a relatively high frequency in the corpus.", "labels": [], "entities": []}, {"text": "shows the top 5 of the most frequent confusion pairs extracted from the corpus.", "labels": [], "entities": []}, {"text": "This information was added to the randomization module so that instead of a substitution of a character by a random character, the only substitutions allowed were chosen from this list.", "labels": [], "entities": []}, {"text": "We should stress that, since we do not use annotated training material, the extracted list might not be complete.", "labels": [], "entities": []}, {"text": "The character-based models were trained for 4 iterations with 20 epochs 12 per iteration.", "labels": [], "entities": []}, {"text": "The randomNoise and confusionPair models achieved 73% and 71% accuracy respectively while the baseline model achieved an accuracy of 51%.", "labels": [], "entities": [{"text": "confusionPair", "start_pos": 20, "end_pos": 33, "type": "METRIC", "confidence": 0.6014778017997742}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9995149374008179}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9994140863418579}]}, {"text": "Inspection of the intermediate scores shows that the randomNoise model achieves convergence fairly quickly, while the confusionPair model has a slower learning rate.", "labels": [], "entities": [{"text": "convergence", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9938697814941406}]}, {"text": "This indicates that corrupting the strings in a more 'consistent' manner, i.e. using information on likely confusion pairs extracted from the corpus, leads to more erroneous assumptions during training.", "labels": [], "entities": []}, {"text": "While the randomNoise model is trained to robustly deal with random noise, the confusionPair model's focus on a subset of the possible errors does not train the model well enough to detect other kinds of errors.", "labels": [], "entities": []}, {"text": "A close analysis of the corrections and errors of the randomNoise model on the test set shows that Checks were performed using the same lexicon as for the calculation of the proportion of OOV words in section 3.3.", "labels": [], "entities": []}, {"text": "We extended the lexicon by creating new entries which consisted of two original words of the lexicon glued together, in order to catch whitespace deletion errors.", "labels": [], "entities": []}, {"text": "In our implementation insertion, deletion and substitution steps all had the same cost, i.e. 1. In order to find whitespace insertion errors The number of epochs was empirically determined.", "labels": [], "entities": []}, {"text": "the model is good at detecting 'close' substitutions of characters when they appear in a relatively clean environment, e.g. a substitution between 'e' and '\u00b4 e' in the string 'theorique', or a switch between lowercase and uppercase, such as in 'develoPpement' . We find that when the original input string contains multiple OCR errors close together (and as such is no longer a 'clean' environment fora character substitution), the model cannot adequately decide which characters to replace.", "labels": [], "entities": []}, {"text": "This suggests that either gradually increasing the ratio of noise or slowly extending the context window during training might have a positive impact on performance accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9873594641685486}]}, {"text": "shows the proportions of OCR errors in the manually annotated evaluation set that were corrected by the two character-based approaches and the wordbased baseline model.", "labels": [], "entities": []}, {"text": "We see that most substitution errors are most easily spotted by the models but that the detection of insertion errors proves very difficult.", "labels": [], "entities": []}, {"text": "This is because most of the insertion errors are random insertions of whitespace in words.", "labels": [], "entities": []}, {"text": "Since whitespace is used abundantly in structuring the documents, the model generally predicts this character with a high probability, and thus fails to detect it as an error.", "labels": [], "entities": []}, {"text": "The addition of character confusion information in the creation of corrupted input data (column 2 in) has a slight positive impact on substitution errors but not as much as was expected.", "labels": [], "entities": []}, {"text": "When examining the cases in which the model failed to spot an error or generated corrections where none were needed, we find that text written in uppercase presents a great difficulty for the models.", "labels": [], "entities": []}, {"text": "Only a small part of the documents are written in uppercase, i.e. the headers with de-identified personal information and the titles of the individual sections.", "labels": [], "entities": []}, {"text": "The models clearly do not have enough training data to learn an adequate language model.", "labels": [], "entities": []}, {"text": "Ina followup study, we should either provide the model with more data, or add a lowercasing step to the preprocessing pipeline.", "labels": [], "entities": []}, {"text": "Another interesting but infrequent error are the cases where the language model has clearly learned the character-based language models but uses it incorrectly given the wider context, for example, by changing 'facile' (easy) into 'faciale' (facial) in 'Ponction de trophoblaste facile' (easy puncture of the trophoblasts).", "labels": [], "entities": []}, {"text": "These types of error could be avoided by fitting a larger language model on top of the character-based LSTM model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Distribution of OCR error types in the evaluation set", "labels": [], "entities": []}, {"text": " Table 6: Proportions of corrections for different OCR error", "labels": [], "entities": [{"text": "corrections", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.8907099962234497}]}]}