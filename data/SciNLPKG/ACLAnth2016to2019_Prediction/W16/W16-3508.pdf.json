{"title": [{"text": "Content Selection as Semantic-Based Ontology Exploration", "labels": [], "entities": [{"text": "Content Selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7110006511211395}]}], "abstractContent": [], "introductionContent": [{"text": "Natural Language (NL) based access to information contained in Knowledge Bases (KBs) has been tackled by approaches following different paradigms.", "labels": [], "entities": [{"text": "Natural Language (NL) based access to information contained in Knowledge Bases (KBs)", "start_pos": 0, "end_pos": 84, "type": "TASK", "confidence": 0.7796518262475729}]}, {"text": "One strand of research deals with the task of ontology-based data access and data exploration (.", "labels": [], "entities": [{"text": "ontology-based data access", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6206368803977966}, {"text": "data exploration", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.7237620949745178}]}, {"text": "This type of approach relies on two pillar components.", "labels": [], "entities": []}, {"text": "The first one is an ontology describing the underlying domain with a set of reasoning based query construction operations.", "labels": [], "entities": []}, {"text": "This component guides the lay user in the formulation of a KB query by proposing alternatives for query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.7909259498119354}]}, {"text": "The second is a Natural Language Generation (NLG) system to hide the details of the formal query language to the user.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.799966941277186}]}, {"text": "Our ultimate goal is the automatic creation of a corpus of KB queries for development and evaluation of NLG systems.", "labels": [], "entities": []}, {"text": "The task we address is the following.", "labels": [], "entities": []}, {"text": "Given an ontology K, automatically select from K descriptions q which yield sensible user queries.", "labels": [], "entities": []}, {"text": "The difficulty lies in the fact that ontologies often omit important disjointness axioms and adequate domain or range restrictions).", "labels": [], "entities": []}, {"text": "For instance, the toy ontology shown in licences the meaningless query in (1).", "labels": [], "entities": []}, {"text": "This happens because there is no disjointness axiom between the Song and Rectangular concepts and/or because the domain of the marriedTo relation is not restricted to persons.", "labels": [], "entities": []}, {"text": "In this work, we explore to what extent vector space models can help to improve the coherence of automatically formulated KB queries.", "labels": [], "entities": []}, {"text": "These models are learnt from large corpora and provide general shared common semantic knowledge.", "labels": [], "entities": []}, {"text": "Such models have been proposed for related tasks.", "labels": [], "entities": []}, {"text": "For example, () proposes a distributional semantic approach for the exploration of paths in a knowledge graph and () uses distributional semantics for spotting commonsense inconsistencies in large KBs.", "labels": [], "entities": []}, {"text": "Our approach draws on the fact that natural language is used to name elements, i.e. concepts and relations, in ontologies ().", "labels": [], "entities": []}, {"text": "Hence, the idea is to exploit lexical semantics to detect incoherent query expansions during the automatic query formulation process.", "labels": [], "entities": [{"text": "query formulation", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7034038454294205}]}, {"text": "Following ideas from the work in (; Van de Cruys, 2014), our approach uses word vector representations as lexical semantic resources.", "labels": [], "entities": []}, {"text": "We train two semantic \"compatibility\" models, namely DISCOMP and DRCOMP . The first one will model incompatibility between concepts in a candidate query expansion and the second incompatibility between concepts and candidate properties.", "labels": [], "entities": [{"text": "DRCOMP", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.5887226462364197}]}, {"text": "Following (cf.) a KB query is a labelled tree where edges are labelled with a relation name and nodes are labelled with a variable and a non-empty set of concept names from the ontology.", "labels": [], "entities": []}], "datasetContent": [{"text": "Both models use the best performing word vectors available at http://clic.cimec.unitn.", "labels": [], "entities": []}, {"text": "it/composes/semantic-vectors.html ().", "labels": [], "entities": []}, {"text": "This dataset consists of compatible and incompatible example pairs.", "labels": [], "entities": []}, {"text": "We extract them in the following way.", "labels": [], "entities": []}, {"text": "We combine a set of manually annotated pairs with a set of automatically extracted ones.", "labels": [], "entities": []}, {"text": "As manually annotated examples, we use the dataset of () plus additional examples extracted from the results of dif-ferent runs of the add operation which were annotated manually.", "labels": [], "entities": []}, {"text": "In addition, we automatically extracted compatible and incompatible pairs of concepts from existing ontologies.", "labels": [], "entities": []}, {"text": "For incompatible pairs (5273 examples), we extracted definitions of disjoint axioms from 52 ontologies crawled from the web and from).", "labels": [], "entities": []}, {"text": "The compatible pairs (57968 examples) were extracted from YAGO using the class membership of individuals.", "labels": [], "entities": [{"text": "YAGO", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8215057253837585}]}, {"text": "We assume that if an instance a is defined as a member of the class A and of the class B at the same time then both classes are compatible.", "labels": [], "entities": []}, {"text": "The final dataset contains 71918 instances.", "labels": [], "entities": []}, {"text": "We take 80% for training and the rest for testing.", "labels": [], "entities": []}, {"text": "We automatically extract subject-predicate pairs (s, p) from two different sources, namely nsubj dependencies from parsed sentences and domain restrictions in ontologies.", "labels": [], "entities": []}, {"text": "For the extraction of pairs from text, we use the ukWaCKy corpus (), we call this subset of pairs ukWaCKy.SP, and the Matoll corpus (, call it the WikiDBP.SP subset.", "labels": [], "entities": [{"text": "ukWaCKy corpus", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9671842753887177}, {"text": "ukWaCKy.SP", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.9488822817802429}]}, {"text": "Both corpora contain dependency parsed sentences.", "labels": [], "entities": []}, {"text": "In addition, the Matoll corpus provides annotations linking entities mentioned in the text with DBPedia entities.", "labels": [], "entities": [{"text": "Matoll corpus", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.8549243807792664}]}, {"text": "For the first SP dataset, we take the head and dependent participating in nsubj dependency relations as training pairs (s, p).", "labels": [], "entities": []}, {"text": "For the second SP dataset, we use the DBPedia annotations associated to nsubj dependents.", "labels": [], "entities": [{"text": "SP dataset", "start_pos": 15, "end_pos": 25, "type": "DATASET", "confidence": 0.8521917760372162}]}, {"text": "That is, we create (s, p) pairs where the s component rather than being the head entity mention, it is the DBPedia concept to which this entity belongs to.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.9396516680717468}]}, {"text": "We do this by using the DBPedia entity annotations present in the corpus.", "labels": [], "entities": [{"text": "DBPedia entity annotations present in the corpus", "start_pos": 24, "end_pos": 72, "type": "DATASET", "confidence": 0.8130354114941188}]}, {"text": "For instance, given the dependency nsubj(Stan Kenton, winning), because Stan Kenton is annotated with the DBPedia entity http://dbpedia.", "labels": [], "entities": [{"text": "DBPedia entity http://dbpedia", "start_pos": 106, "end_pos": 135, "type": "DATASET", "confidence": 0.9399138808250427}]}, {"text": "org/resource/Stan_Kenton and this entity is defined to be of type Person and Artist, among others, we can create (s, p) pairs such as (person, winning) and (artist, winning).", "labels": [], "entities": []}, {"text": "For the pairs based on ontology definitions, we use the 52 ontologies crawled from the web.", "labels": [], "entities": []}, {"text": "We call this subset of pairs KB.SP.", "labels": [], "entities": []}, {"text": "For training the model, we generate negative instances by corrupting the extracted data.", "labels": [], "entities": []}, {"text": "For each (s, p) pair in the dataset we generate an (s , p) pair where sis not seen occurring with pin the training corpus.", "labels": [], "entities": []}, {"text": "The final dataset contains 610522 training instances (30796 from ukWaCKy.SP, 571564 from WikiDBP.SP and 8162 from KB.SP ).", "labels": [], "entities": [{"text": "ukWaCKy.SP", "start_pos": 65, "end_pos": 75, "type": "DATASET", "confidence": 0.9808852076530457}]}, {"text": "We takeout 600 cases, 300 from ukWaCKy.SP and 300 from KB.SP, for testing the model on specific text and KB pairs.", "labels": [], "entities": [{"text": "ukWaCKy.SP", "start_pos": 31, "end_pos": 41, "type": "DATASET", "confidence": 0.9846481084823608}]}, {"text": "We separately evaluate the performance of each model in a held out testing set.", "labels": [], "entities": []}, {"text": "shows the results for the DISCOMP model.", "labels": [], "entities": []}, {"text": "We also asses the performance of the models on the task of meaningful query generation.", "labels": [], "entities": [{"text": "meaningful query generation", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6371630529562632}]}, {"text": "We run the random query generation process over 5 ontologies of different domains, namely cars, travel, wines, conferences and human disabilities.", "labels": [], "entities": [{"text": "random query generation process", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.7539684921503067}]}, {"text": "At each query expansion operation, we apply the models to the sets of candidate concepts or relations.", "labels": [], "entities": []}, {"text": "We compare the DISCOMP and DRCOMP models with a baseline cosine similarity (COS ) score . For this score we use GloVe () word embeddings and simple addition for composing multiword concept and relation names.", "labels": [], "entities": [{"text": "cosine similarity (COS ) score", "start_pos": 57, "end_pos": 87, "type": "METRIC", "confidence": 0.846818337837855}]}, {"text": "We use a threshold of 0.3 that was determined empirically . During the query generation process, we registered the candidate sets as well as For the case of add candidate relations, the COS model checks for semantic relatedness between a subject concept and the relation and between the subject concept and the object concept, i.e. (s,p) and (s,o) We compare the COS baseline plus a threshold of 0.3  the predictions of the models.", "labels": [], "entities": []}, {"text": "In total, we collected 67 candidate sets corresponding to the add compatible relation query extension and 39 to the add compatible operation.", "labels": [], "entities": []}, {"text": "The candidate sets were manually annotated with (in)compatibility human judgements.", "labels": [], "entities": []}, {"text": "We use these sets as gold standard to compute precision, recall, f-measure and specificity measures on the task of detecting incompatible candidates as well as the accuracy of the models.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.999313473701477}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9992895126342773}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9991450309753418}]}, {"text": "shows one example for each of the query expansion operations, the annotated candidates and the predictions done by each of the models (only incompatibles are shown).", "labels": [], "entities": []}, {"text": "Unsurprisingly, given the quite strong similarity threshold used for the COS baseline, we observe that it has good precision at spotting incompatible candidates though quite low recall.", "labels": [], "entities": [{"text": "COS baseline", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.642567053437233}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9970609545707703}, {"text": "recall", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9979669451713562}]}, {"text": "In contrast, as shown by the f-measure values the compatibility models seem to achieve a better performance compromise for these measures.", "labels": [], "entities": []}, {"text": "We include the specificity measure as an indicative of the ability of the models to avoid false alarms, that is, to avoid predicting a candidate as incompatible when it was not.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results reported by (Kruszewski and Ba- roni, 2015) and results obtained with the DISCOMP  model.", "labels": [], "entities": []}, {"text": " Table 2: Results after (Emb.+NN) training with  the union of the ukWaCKy.SP, WikiDBP.SP and  KB.SP training sets. Note that if we train only  with the ukWaCKy.SP training set and we evaluate  with the ukWaCKy.SP testing set we get an accu- racy of 0.86 which is similar to the results reported  in (Van de Cruys, 2014).", "labels": [], "entities": [{"text": "ukWaCKy.SP", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.9720350503921509}, {"text": "WikiDBP.SP", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9299705624580383}, {"text": "ukWaCKy.SP training set", "start_pos": 152, "end_pos": 175, "type": "DATASET", "confidence": 0.9249854683876038}, {"text": "ukWaCKy.SP testing set", "start_pos": 202, "end_pos": 224, "type": "DATASET", "confidence": 0.9278103709220886}, {"text": "accu- racy", "start_pos": 235, "end_pos": 245, "type": "METRIC", "confidence": 0.9748897552490234}]}, {"text": " Table 3: Precision (P), recall (R), F-measure (F),  specificity (S) and accuracy (A) results for the", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.930479422211647}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9466079622507095}, {"text": "F-measure (F)", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.9584003239870071}, {"text": "specificity (S)", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.8770158737897873}, {"text": "accuracy (A)", "start_pos": 73, "end_pos": 85, "type": "METRIC", "confidence": 0.9563778191804886}]}]}