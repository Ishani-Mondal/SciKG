{"title": [], "abstractContent": [{"text": "Linguistic annotation underlies many successful approaches in Natural Language Processing (NLP), where the annotated corpora are used for training and evaluating supervised learners.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.7447860638300577}]}, {"text": "The consistency of annotation limits the performance of supervised models, and thus a lot of effort is put into obtaining high-agreement annotated datasets.", "labels": [], "entities": []}, {"text": "Recent research has shown that annotation disagreement is not random noise, but carries a systematic signal that can be used for improving the supervised learner.", "labels": [], "entities": []}, {"text": "However, prior work was limited in scope, focusing only on part-of-speech tagging in a single language.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.6597587764263153}]}, {"text": "In this paper we broaden the experiments to a semantic task (supersense tagging) using multiple languages.", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.6861709654331207}]}, {"text": "In particular, we analyse how systematic disagreement is for sense annotation, and we present a preliminary study of whether patterns of disagreements transfer across languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Consistent annotations are important if we wish to train reliable models and perform conclusive evaluation of NLP.", "labels": [], "entities": []}, {"text": "The standard practice in annotation efforts is to define annotation guidelines that aim to minimize annotator disagreement.", "labels": [], "entities": []}, {"text": "However, in practical annotation projects, perfect agreement is virtually unattainable.", "labels": [], "entities": []}, {"text": "Moreover, not all of disagreement should be considered noise because some of it is systematic.", "labels": [], "entities": []}, {"text": "The work of shows that the regularity of some disagreement in part-of-speech (POS) annotation can be used to obtain more robust POS taggers.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 128, "end_pos": 139, "type": "TASK", "confidence": 0.6984373331069946}]}, {"text": "They adjust the training loss of each example according to its possible variation in agreement, providing smaller losses when a classifier training decision makes a misclassification that matches with human disagreement.", "labels": [], "entities": []}, {"text": "For example, the loss for predicting a particle instead of an adverb is smaller than the loss for predicting a noun instead of an adverb, because the particle/adverb confusion is fairly common among annotators.", "labels": [], "entities": [{"text": "predicting a particle instead of an adverb", "start_pos": 26, "end_pos": 68, "type": "TASK", "confidence": 0.8434711183820452}]}, {"text": "In this article, we apply the method of to a semantic sequence-prediction task, namely supersense tagging (SST).", "labels": [], "entities": [{"text": "supersense tagging (SST)", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.8310876846313476}]}, {"text": "SST is considered a more difficult task than POS tagging, because the semantic classes are more dependent on world knowledge, and the number of supersenses is higher than the number of POS labels.", "labels": [], "entities": [{"text": "SST", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9776760339736938}, {"text": "POS tagging", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.8446872532367706}]}, {"text": "We experiment with different methods to calculate the label-wise agreement (Sec.", "labels": [], "entities": []}, {"text": "3.1), and apply these methods to datasets in two languages, namely English and Danish (Sec. 3.2).", "labels": [], "entities": []}, {"text": "Moreover, we also perform cross-linguistic experiments to assess how much of the annotation variation in one language can be applied to another.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform two kinds of experiments: monolingual and cross-language.", "labels": [], "entities": []}, {"text": "For the monolingual experiments we use each of the four possible factorizations (Sec.", "labels": [], "entities": []}, {"text": "3.1) to train SST models with different costs on a single language.", "labels": [], "entities": [{"text": "SST", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9879865050315857}]}, {"text": "We evaluate each system against the most-frequent sense baseline (MFS), and against a regular structured perceptron without cost-sensitive training (BASELINE).", "labels": [], "entities": [{"text": "sense baseline (MFS)", "start_pos": 50, "end_pos": 70, "type": "METRIC", "confidence": 0.7223803043365479}, {"text": "BASELINE", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9863809943199158}]}, {"text": "The cross-language experiments assess whether some of the disagreement information captured by the factorizations can be used cross-lingually.", "labels": [], "entities": []}, {"text": "To study this hypothesis, we run factorized systems using S DA (Sec. 3.1) on English, and viceversa.", "labels": [], "entities": []}, {"text": "Adapting S DA to English requires projecting back to the canonical supersense inventory, namely removing the adjective supersenses and treating, e.g., all cases of NOUN.VEHICLE as N.ARTIFACT, before calculating factorizations for the different confusion matrices.", "labels": [], "entities": [{"text": "Adapting S DA", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7975014050801595}]}, {"text": "Applying the complementary process-using English disagreement information to train costsensitive models for Danish SST-is more involved.", "labels": [], "entities": [{"text": "Danish SST-is", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.5459995716810226}]}, {"text": "We have converted all the Danish data to the English SST inventory to be able to use the coarser inventory of S EN by projecting the extended senses to their original sense.", "labels": [], "entities": [{"text": "Danish data", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.8614896535873413}, {"text": "English SST inventory", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.6871104637781779}]}, {"text": "an effect on the most frequent sense baseline, because the test data is effectively relabeled.", "labels": [], "entities": []}, {"text": "shows the performance of our system compared to the MFS baseline and the non-regularized baseline that does not use factorizations.", "labels": [], "entities": [{"text": "MFS baseline", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.8485711216926575}]}, {"text": "Note that our baseline structured perceptron already beats the though MFS baseline.", "labels": [], "entities": [{"text": "MFS baseline", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.8023402690887451}]}, {"text": "We mark results in bold when another system beats the BASELINE.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9225588440895081}]}, {"text": "Some factorizations are more favorable for certain datasets.", "labels": [], "entities": []}, {"text": "For instance, all factorizations improve the performance on Ritter-eval, but only WHO-LETAGS aids on Ritter-dev.", "labels": [], "entities": [{"text": "Ritter-eval", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.869953453540802}, {"text": "WHO-LETAGS", "start_pos": 82, "end_pos": 92, "type": "DATASET", "confidence": 0.7972710132598877}, {"text": "Ritter-dev", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.8793599605560303}]}, {"text": "Over all in-language data sets, WHOLETAGS beats the macro-averaged baseline for English.", "labels": [], "entities": [{"text": "WHOLETAGS", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9874803423881531}]}, {"text": "However, the most reliable factorization overall is JUSTSENSE, which beats BASELINE for English and Danish.", "labels": [], "entities": [{"text": "JUSTSENSE", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9446413516998291}, {"text": "BASELINE", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9919330477714539}]}, {"text": "For Danish-JUSTSENSE we observe that the adjective supersenses improve (A.MENTAL goes from 0.00 to 16.53 fora support of 15 instances, and A.SOCIAL goes from 48.87 to 56.75 fora support of 169 instances in the training data), but also other senses with much higher support improve, regardless of POS, like N.PERSON (from 49.72 to 52.66 for 951 instances) or V.COMMUNICATION (from 49.66 to 50.31 for 364 instances).", "labels": [], "entities": [{"text": "Danish-JUSTSENSE", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9341921210289001}]}], "tableCaptions": [{"text": " Table 2: Supersense tagging data sets, the first two  are training data sets.", "labels": [], "entities": [{"text": "Supersense tagging", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8787274360656738}]}, {"text": " Table 4: F 1 scores for English and Danish supersense tagging, with language-wise macro-average.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9868790507316589}, {"text": "English and Danish supersense tagging", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.5010392904281616}]}, {"text": " Table 5: F 1 s for English using cross-lingual costs calculated from S DA", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9694452881813049}]}]}