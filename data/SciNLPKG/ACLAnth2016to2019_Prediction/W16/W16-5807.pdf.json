{"title": [{"text": "A Neural Model for Language Identification in Code-Switched Tweets", "labels": [], "entities": [{"text": "Language Identification in Code-Switched Tweets", "start_pos": 19, "end_pos": 66, "type": "TASK", "confidence": 0.7700932741165161}]}], "abstractContent": [{"text": "Language identification systems suffer when working with short texts or in domains with unconventional spelling, such as Twitter or other social media.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6811896860599518}]}, {"text": "These challenges are explored in a shared task for Language Identification in Code-Switched Data (LICS 2016).", "labels": [], "entities": [{"text": "Language Identification in Code-Switched Data (LICS 2016)", "start_pos": 51, "end_pos": 108, "type": "TASK", "confidence": 0.7624633974499173}]}, {"text": "We apply a hierarchical neural model to this task, learning character and contextualized word-level representations to make word-level language predictions.", "labels": [], "entities": []}, {"text": "This approach performs well on both the 2014 and 2016 versions of the shared task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language identification (language ID) remains a difficult problem, particulary in social media text where informal styles, closely related language pairs, and code-switching are common.", "labels": [], "entities": [{"text": "Language identification (language ID)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.841942310333252}]}, {"text": "Progress on language ID is needed especially since downstream tasks, like translation or semantic parsing, depend on it.", "labels": [], "entities": [{"text": "language ID", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.7339290380477905}, {"text": "translation or semantic parsing", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.6882498115301132}]}, {"text": "Continuous representations for language data, which have produced new states of the art for language modeling (, machine translation (, and other tasks, can be useful for language ID.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7163987308740616}, {"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7770693302154541}, {"text": "language ID", "start_pos": 171, "end_pos": 182, "type": "TASK", "confidence": 0.7490409910678864}]}, {"text": "For the Language Identification in Code-Switched Data shared task (LICS 2016), we submitted a hierarchical characterword model closely following, focusing on word-level language ID.", "labels": [], "entities": [{"text": "Language Identification in Code-Switched Data shared task (LICS 2016)", "start_pos": 8, "end_pos": 77, "type": "TASK", "confidence": 0.8146016679026864}, {"text": "word-level language ID", "start_pos": 158, "end_pos": 180, "type": "TASK", "confidence": 0.6047326326370239}]}, {"text": "Our discussion of the model closely follows that paper.", "labels": [], "entities": []}, {"text": "This model, which we call C2V2L (\"character to vector to language\") is hierarchical in the sense that it explicitly builds a continuous representation for each word from its character sequence, capturing orthographic and morphology-related patterns, and then combines those word-level representations to use context from the full word sequence before making predictions for each word.", "labels": [], "entities": []}, {"text": "The use of character representations is well motivated for codeswitching tasks, since the presence of multiple languages means that one is more likely to encounter a previously unseen word.", "labels": [], "entities": []}, {"text": "Our model does not require special handling of casing or punctuation, nor do we need to remove the URLs, usernames, or hashtags, and it is trained endto-end using standard procedures.", "labels": [], "entities": []}], "datasetContent": [{"text": "Because C2V2L produces language predictions for every word, the architecture is well suited to analysis of code-switched text, in which different words may belong to different languages.", "labels": [], "entities": []}, {"text": "We used the SpanishEnglish dataset from the EMNLP 2014 shared task on Language Identification in Code-Switched Data (, and the Spanish-English and Arabic-MSA datasets from the EMNLP 2016 version of the shared task.", "labels": [], "entities": [{"text": "SpanishEnglish dataset", "start_pos": 12, "end_pos": 34, "type": "DATASET", "confidence": 0.9638097882270813}, {"text": "EMNLP 2014 shared task", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.7887425273656845}, {"text": "Language Identification", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.6560008674860001}]}, {"text": "Each dataset is a collection of monolingual and code-switched tweets in two main languages: English and Spanish, or Modern Standard Arabic (MSA) and Arabic dialects.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1 scores for the LICS 2016 shared task. The best", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9994528889656067}, {"text": "LICS 2016 shared task", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.7782338112592697}]}]}