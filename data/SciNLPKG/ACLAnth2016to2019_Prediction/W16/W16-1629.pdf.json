{"title": [{"text": "Domain Adaptation for Neural Networks by Parameter Augmentation", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6835826486349106}, {"text": "Parameter Augmentation", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7499559819698334}]}], "abstractContent": [{"text": "We propose a simple domain adaptation method for neural networks in a supervised setting.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7081910371780396}]}, {"text": "Supervised domain adaptation is away of improving the generalization performance on the target domain by using the source domain dataset, assuming that both of the datasets are labeled.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7851493060588837}]}, {"text": "Recently, recurrent neural networks have been shown to be successful on a variety of NLP tasks such as caption generation ; however, the existing domain adaptation techniques are limited to (1) tune the model parameters by the target dataset after the training by the source dataset, or (2) design the network to have dual output, one for the source domain and the other for the target domain.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.9548977315425873}, {"text": "domain adaptation", "start_pos": 146, "end_pos": 163, "type": "TASK", "confidence": 0.7655357718467712}]}, {"text": "Reformulating the idea of the domain adaptation technique proposed by Daum\u00e9 (2007), we propose a simple domain adaptation method, which can be applied to neural networks trained with a cross-entropy loss.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7389217913150787}, {"text": "domain adaptation", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7130105048418045}]}, {"text": "On captioning datasets, we show performance improvements over other domain adaptation methods .", "labels": [], "entities": []}], "introductionContent": [{"text": "Domain adaptation is a machine learning paradigm that aims at improving the generalization performance of anew (target) domain by using a dataset from the original (source) domain.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7804352641105652}]}, {"text": "Suppose that, as the source domain dataset, we have a captioning corpus, consisting of images of daily lives and each image has captions.", "labels": [], "entities": []}, {"text": "Suppose also that we would like to generate captions for exotic cuisine, which are rare in the corpus.", "labels": [], "entities": []}, {"text": "It is usually very costly to make anew corpus for the target domain, i.e., taking and captioning those images.", "labels": [], "entities": []}, {"text": "The research question here is how we can leverage the source domain dataset to improve the performance on the target domain.", "labels": [], "entities": []}, {"text": "As described by, there are mainly two settings of domain adaptation: fully supervised and semi-supervised.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7304095029830933}]}, {"text": "Our focus is the supervised setting, where both of the source and target domain datasets are labeled.", "labels": [], "entities": []}, {"text": "We would like to use the label information of the source domain to improve the performance on the target domain.", "labels": [], "entities": []}, {"text": "Recently, Recurrent Neural Networks (RNNs) have been successfully applied to various tasks in the field of natural language processing (NLP), including language modeling (, caption generation () and parsing ().", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 107, "end_pos": 140, "type": "TASK", "confidence": 0.8158997793992361}, {"text": "language modeling", "start_pos": 152, "end_pos": 169, "type": "TASK", "confidence": 0.74259153008461}, {"text": "caption generation", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.8911496996879578}, {"text": "parsing", "start_pos": 199, "end_pos": 206, "type": "TASK", "confidence": 0.9640049338340759}]}, {"text": "For neural networks, there are two standard methods for supervised domain adaptation (.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.7358662883440653}]}, {"text": "The first method is fine tuning: we first train the model with the source dataset and then tune it with the target domain dataset (.", "labels": [], "entities": []}, {"text": "Since the objective function of neural network training is nonconvex, the performance of the trained model can depend on the initialization of the parameters.", "labels": [], "entities": []}, {"text": "This is in contrast with the convex methods such as Support Vector Machines (SVMs).", "labels": [], "entities": []}, {"text": "We expect that the first training gives a good initialization of the parameters, and therefore the latter training gives a good generalization even if the target domain dataset is small.", "labels": [], "entities": []}, {"text": "The downside of this approach is the lack of the optimization objective.", "labels": [], "entities": []}, {"text": "The other method is to design the neural network so that it has two outputs.", "labels": [], "entities": []}, {"text": "The first output is trained with the source dataset and the other output is trained with the target dataset, where the input part is shared among the domains.", "labels": [], "entities": []}, {"text": "We call this method dual outputs.", "labels": [], "entities": []}, {"text": "This type of network architecture has been successfully applied to multitask learning in NLP such as part-of-speech tag-ging and named-entity recognition.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 129, "end_pos": 153, "type": "TASK", "confidence": 0.7455340027809143}]}, {"text": "In the NLP community, there has been a large body of previous work on domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7248192727565765}]}, {"text": "One of the state-of-the-art methods for the supervised domain adaptation is feature augmentation.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.7444982131322225}]}, {"text": "The central idea of this method is to augment the original features/parameters in order to model the source specific, target specific and general behaviors of the data.", "labels": [], "entities": []}, {"text": "However, it is not straight-forward to apply it to neural network models in which the cost function has a form of log probabilities.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew domain adaptation method for neural networks.", "labels": [], "entities": []}, {"text": "We reformulate the method of and derive an objective function using convexity of the loss function.", "labels": [], "entities": []}, {"text": "From a high-level perspective, this method shares the idea of feature augmentation.", "labels": [], "entities": []}, {"text": "We use redundant parameters for the source, target and general domains, where the general parameters are tuned to model the common characteristics of the datasets and the source/target parameters are tuned for domain specific aspects.", "labels": [], "entities": []}, {"text": "In the latter part of this paper, we apply our domain adaptation method to a neural captioning model and show performance improvement over other standard methods on several datasets and metrics.", "labels": [], "entities": [{"text": "neural captioning", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7825018167495728}]}, {"text": "In the datasets, the source and target have different word distributions, and thus adaptation of output parameters is important.", "labels": [], "entities": []}, {"text": "We augment the output parameters to facilitate adaptation.", "labels": [], "entities": []}, {"text": "Although we use captioning models in the experiments, our method can be applied to any neural networks trained with a cross-entropy loss.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have conducted domain adaptation experiments on the following three datasets.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.8166722357273102}]}, {"text": "The first experiment focuses on the situation where the domain adaptation is useful.", "labels": [], "entities": []}, {"text": "The second experiment show the benefit of domain adaptation for both directions: from source to target and target to source.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7148873060941696}]}, {"text": "The third experiment shows an improvement in another metric.", "labels": [], "entities": []}, {"text": "Although our method is applicable to any neural network with across entropy loss, all the experiments use caption generation models because it is one of the most successful neural network applications in NLP.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.8044447004795074}]}], "tableCaptions": [{"text": " Table 3: Results of the domain adaptation to the  food dataset. The evaluation metrics are BLEU,  METOR and CIDEr. The proposed method is the  best in most of the metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9990271329879761}, {"text": "METOR", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.9914085268974304}]}, {"text": " Table 5: Domain adaptation from MSCOCO to  Flickr30K dataset.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8148201406002045}, {"text": "MSCOCO", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.8762614727020264}, {"text": "Flickr30K dataset", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.9689675569534302}]}, {"text": " Table 6: Domain adaptation from Flickr30K to  MSCOCO dataset.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7976880073547363}, {"text": "Flickr30K", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.9452921152114868}, {"text": "MSCOCO dataset", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9500146508216858}]}, {"text": " Table 7: A sample question from TOEIC part 1 test. The correct answer is (C).", "labels": [], "entities": [{"text": "TOEIC part 1 test", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.8385372310876846}]}, {"text": " Table 8: Domain adaptation to TOEIC dataset.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8226060271263123}, {"text": "TOEIC dataset", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9417352378368378}]}]}