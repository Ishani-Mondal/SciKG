{"title": [{"text": "Exploring Different Preposition Sets, Models and Feature Sets in Automatic Generation of Spatial Image Descriptions", "labels": [], "entities": [{"text": "Automatic Generation of Spatial Image Descriptions", "start_pos": 65, "end_pos": 115, "type": "TASK", "confidence": 0.8097394903500875}]}], "abstractContent": [{"text": "In this paper we look at the question of how to create good automatic methods for generating descriptions of spatial relationships between objects in images.", "labels": [], "entities": []}, {"text": "In particular, we investigate the impact of varying different aspects of automatic method development, including using different preposition sets, models and feature sets.", "labels": [], "entities": [{"text": "automatic method development", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.6504022280375162}]}, {"text": "We find that optimising the preposition set improves previous best Accuracy from 46.2 to 50.2.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9875640273094177}]}, {"text": "Feature set optimisa-tion further improves best Accuracy from 50.2 to 53.25.", "labels": [], "entities": [{"text": "best", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9952614903450012}, {"text": "Accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9574502110481262}]}, {"text": "Naive Bayes models out-perform SVMs and decision trees under all conditions tested.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.9633857011795044}]}, {"text": "The utility of individual features depends on the model used, but the most useful features tend to capture a property pertaining to both objects jointly.", "labels": [], "entities": []}], "introductionContent": [{"text": "The research reported here is located in the general area of automatic generation of image descriptions.", "labels": [], "entities": [{"text": "automatic generation of image descriptions", "start_pos": 61, "end_pos": 103, "type": "TASK", "confidence": 0.8384367525577545}]}, {"text": "It can be useful to generate image descriptions, either offline, e.g. to add as alt text to images in websites, or online as one aspect of assistive technology for visually impaired people.", "labels": [], "entities": []}, {"text": "To illustrate the specific task we address, Figure 1 shows an image from the VOC'08 data set) complete with the original annotations, alongside the kind of descriptions we aim to generate: each describes the spatial relationship between two of the objects in the image in simple terms focused around a preposition.", "labels": [], "entities": [{"text": "VOC'08 data set", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.9841786821683248}]}, {"text": "Over the following sections, we describe the data we used, with a particular focus on the set of prepositions used in the annotations (Section 2), outline the learning methods we tested (Section 3), and report the experiments we performed and the results we obtained (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "The training data contains a separate training instance (Obj s , Obj o , p) for each preposition p selected by human annotators for the template 'The Obj sis p the Obj o ' (e.g. the dog is in front of the person) accompanied by an image in which (just) Obj sand Obj o are surrounded by bounding boxes.", "labels": [], "entities": []}, {"text": "All models are trained and tested with leave-oneout cross-validation.", "labels": [], "entities": []}, {"text": "To compare results in this paper, we use the same variants of the basic Accuracy method as in our previous work.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9912880063056946}]}, {"text": "One dimension along which the variants differ is whether or not synonyms are allowed to substitute for each other.", "labels": [], "entities": []}, {"text": "In those variants in which synonyms are allowed to DS-38", "labels": [], "entities": [{"text": "DS-38", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.8042048811912537}]}], "tableCaptions": [{"text": " Table 1: Acc(1) and Acc Syn (1) for the data with  the larger (DS-38) and smaller (DS-16) preposi- tion sets, and for the rule-based model (RB), the  Naive Bayes model (NB), and the two component  models of the NB model (PM and LM).", "labels": [], "entities": [{"text": "Acc", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.995724081993103}, {"text": "Acc", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9688335061073303}]}, {"text": " Table 2: Acc and Acc Syn results for all four models (leaving out component models) described in Sec- tion 3, and the two data sets described in Section 2.", "labels": [], "entities": [{"text": "Acc", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9937723278999329}, {"text": "Acc", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.961462140083313}]}, {"text": " Table 3: Acc(1) for each feature individually  (where possible), for the smaller (DS-16) number  of prepositions, for the Decision Tree and Naive  Bayes models (F 0 and F 1 are the language fea- tures, and F 2..F 8 are the vision features).", "labels": [], "entities": [{"text": "Acc", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9995008707046509}]}]}