{"title": [{"text": "YSDA Participation in the WMT'16 Quality Estimation Shared Task", "labels": [], "entities": [{"text": "WMT'16 Quality Estimation Shared Task", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.8543815970420837}]}], "abstractContent": [{"text": "This paper describes Yandex School of Data Analysis (YSDA) submission for WMT2016 Shared Task on Quality Estimation (QE) / Task 1: Sentence-level prediction of post-editing effort.", "labels": [], "entities": [{"text": "Yandex School of Data Analysis (YSDA) submission", "start_pos": 21, "end_pos": 69, "type": "DATASET", "confidence": 0.8079496357176039}, {"text": "WMT2016 Shared Task on Quality Estimation (QE)", "start_pos": 74, "end_pos": 120, "type": "TASK", "confidence": 0.7304875718222724}, {"text": "Sentence-level prediction", "start_pos": 131, "end_pos": 156, "type": "TASK", "confidence": 0.8670983016490936}]}, {"text": "We solve the problem of quality estimation by using a machine learning approach, where we try to learn a regressor from feature space to HTER score.", "labels": [], "entities": [{"text": "quality estimation", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.6745754778385162}]}, {"text": "By enriching the baseline features with the syntactical features and additional translation system based features , we achieve Pearson correlation of 0.525 on the test set.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 127, "end_pos": 146, "type": "METRIC", "confidence": 0.924991250038147}]}], "introductionContent": [{"text": "The WMT'16 QE has included the sentence level sub-task.", "labels": [], "entities": [{"text": "WMT'16 QE", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.9234028458595276}]}, {"text": "The goal is to predict the amount of effort required to post-edit machine-translated sentences.", "labels": [], "entities": []}, {"text": "For this task the organizers provide a parallel corpus of English-German sentences obtained via some machine translation system, as well as corresponding manually post-edited reference sentences.", "labels": [], "entities": []}, {"text": "The amount of post-editing is measured by edit-distance rate HTER () between the system's translation and the reference translation.", "labels": [], "entities": [{"text": "edit-distance rate HTER", "start_pos": 42, "end_pos": 65, "type": "METRIC", "confidence": 0.8374941349029541}]}, {"text": "HTER scores were computed by TER 1 software.", "labels": [], "entities": [{"text": "TER", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.6401023864746094}]}, {"text": "Our system extracts numerical features from sentences and uses a machine learning approach to predict HTER score.", "labels": [], "entities": [{"text": "HTER score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.5863766819238663}]}, {"text": "In addition to the baseline features we include syntactic features.", "labels": [], "entities": []}, {"text": "We also found that HTER scores have along tailed distribution.", "labels": [], "entities": []}, {"text": "More than 60% of examples have HTER scoreless than 30, at the same time the maximum value (on provided data) is 150, but there are only few sentences getting such high score.", "labels": [], "entities": [{"text": "HTER scoreless", "start_pos": 31, "end_pos": 45, "type": "METRIC", "confidence": 0.9269487261772156}]}, {"text": "This observation led us to an idea first to http://www.cs.umd.edu/ \u02dc snover/tercom/ predict BLEU (which is currently the most popular metric for evaluation in MT ().", "labels": [], "entities": [{"text": "predict", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9527114629745483}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.7711140513420105}, {"text": "MT", "start_pos": 159, "end_pos": 161, "type": "TASK", "confidence": 0.955262303352356}]}, {"text": "The paper is structured as follows: Section 2 describes analysis of provided data, Section 3 contains machine learning setup and features details, Section 4 summarizes and discusses the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are three metrics for this task: Pearson correlation (primary metric), MSE, and RMSE.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 39, "end_pos": 58, "type": "METRIC", "confidence": 0.8874881565570831}, {"text": "MSE", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.8476406335830688}, {"text": "RMSE", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9691452383995056}]}, {"text": "The main disadvantage of using MSE and RSME here is along tail of target values: if the model fails to predict a high score, an absolute error for this prediction will be large as well.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of HTER and BLEU for train  data", "labels": [], "entities": [{"text": "HTER", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.6615650057792664}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9977516531944275}]}, {"text": " Table 2: Results on dev set", "labels": [], "entities": []}, {"text": " Table 3: Results on test set", "labels": [], "entities": []}]}