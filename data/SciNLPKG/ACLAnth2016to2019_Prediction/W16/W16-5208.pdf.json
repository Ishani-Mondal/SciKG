{"title": [{"text": "Evaluating Ensemble Based Pre-annotation on Named Entity Corpus Construction in English and Chinese", "labels": [], "entities": [{"text": "Named Entity Corpus Construction", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.582474485039711}]}], "abstractContent": [{"text": "Annotated corpora are crucial language resources, and pre-annotation is an usual way to reduce the cost of corpus construction.", "labels": [], "entities": []}, {"text": "Ensemble based pre-annotation approach combines multiple existing named entity taggers and categorizes annotations into normal annotations with high confidence and candidate annotations with low confidence, to reduce the human annotation time.", "labels": [], "entities": []}, {"text": "In this paper, we manually annotate three English datasets under various pre-annotation conditions, report the effects of ensemble based pre-annotation, and analyze the experimental results.", "labels": [], "entities": []}, {"text": "In order to verify the effectiveness of ensemble based pre-annotation in other languages, such as Chinese, three Chinese datasets are also tested.", "labels": [], "entities": []}, {"text": "The experimental results show that the ensemble based pre-annotation approach significantly reduces the number of annotations which human annotators have to add, and outperforms the baseline approaches in reduction of human annotation time without loss in annotation performance (in terms of F 1-measure), on both English and Chinese datasets.", "labels": [], "entities": [{"text": "F 1-measure", "start_pos": 292, "end_pos": 303, "type": "METRIC", "confidence": 0.9672999978065491}]}], "introductionContent": [{"text": "The current success and widespread use of machine learning techniques for processing human language make annotated corpora essential language resources.", "labels": [], "entities": []}, {"text": "Many popular natural language processing (NLP) algorithms require large amounts of high-quality training samples, which are time-consuming and costly to build.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.7606773575146993}]}, {"text": "One usual way to improve this situation is to automatically pre-annotate the corpora, so that human annotators need merely to correct errors rather than to annotate from scratch.", "labels": [], "entities": []}, {"text": "Named Entity Recognition (NER), one of the fundamental tasks for building NLP systems, is a task that detects Named Entity (NE) mentions in a given text and classifies these mentions to a predefined list of types.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7668150812387466}]}, {"text": "Resulted from more than two decades of research, many named entity taggers are publicly available now.", "labels": [], "entities": [{"text": "named entity taggers", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7467808524767557}]}, {"text": "Some of the taggers are integrated into NLP workflows based on Service Oriented Architecture (.", "labels": [], "entities": []}, {"text": "And it is well known that multiple taggers can be combined using ensemble techniques to create a system that outperforms the best individual tagger within the system (.", "labels": [], "entities": []}, {"text": "However, only a few studies have been reported on leveraging ensemble to combine multiple existing taggers to assist named entity annotation.", "labels": [], "entities": [{"text": "named entity annotation", "start_pos": 117, "end_pos": 140, "type": "TASK", "confidence": 0.6226934095223745}]}, {"text": "introduced ensemble based pre-annotation approach in named entity corpus construction.", "labels": [], "entities": [{"text": "named entity corpus construction", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.6447528153657913}]}, {"text": "They conducted experiments on an English dataset, and the results showed that the ensemble based pre-annotation approach outperforms the baseline approaches in reduction of human annotation time.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.8824420273303986}]}, {"text": "In this paper, we perform a more thorough evaluation on the ensemble based pre-annotation approach.", "labels": [], "entities": []}, {"text": "1) We manually annotate three English datasets under various pre-annotation conditions, report the effects of ensemble based pre-annotation, and analyze the experimental results.", "labels": [], "entities": []}, {"text": "2) We also manually annotate three Chinese datasets, to verify the effectiveness of ensemble based pre-annotation in Chinese language.", "labels": [], "entities": []}, {"text": "The remaining part of this paper is organized as follows: In Section 2, we mention related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the experimental setup, followed by experimental results and analysis in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude and discuss future directions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "All the datasets used in our experiments are publicly available.", "labels": [], "entities": []}, {"text": "There are three English datasets, and three Chinese datasets.", "labels": [], "entities": [{"text": "English datasets", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.7093084752559662}, {"text": "Chinese datasets", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.6843676865100861}]}, {"text": "From each dataset, 60 articles are selected randomly.", "labels": [], "entities": []}, {"text": "From each of the 60 articles, one sentence is extracted to perform the actual assisted annotation experiments.", "labels": [], "entities": []}, {"text": "Sentences containing more NEs are preferred over ones containing less NEs.", "labels": [], "entities": []}, {"text": "Sentences containing no NE will not be extracted.", "labels": [], "entities": []}, {"text": "The three English datasets have been described in.", "labels": [], "entities": [{"text": "English datasets", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.8422678411006927}]}, {"text": "The three Chinese datasets are People's Daily (), Penn Chinese Treebank 5.1 (CTB5) (), and ITNLP 1 .  Three human annotators (H 1 , H 2 , and H 3 ) participate in our assisted annotation experiments.", "labels": [], "entities": [{"text": "Chinese datasets", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.7389941215515137}, {"text": "People's Daily", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.9679465095202128}, {"text": "Penn Chinese Treebank 5.1 (CTB5)", "start_pos": 50, "end_pos": 82, "type": "DATASET", "confidence": 0.9646118368421283}, {"text": "ITNLP 1", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.8176626861095428}]}, {"text": "They are graduate students in our school, and major in NLP study.", "labels": [], "entities": [{"text": "NLP study", "start_pos": 55, "end_pos": 64, "type": "TASK", "confidence": 0.8316775560379028}]}, {"text": "After they have annotated some sentences in a training dataset to get familiar with the Web based UI, each of them has to annotate all of the sentences in the six datasets.", "labels": [], "entities": []}, {"text": "The human annotators are presented with the sentences in the same order, but for different human annotators, each sentence is pre-annotated by different pre-annotators.", "labels": [], "entities": []}, {"text": "We carefully design the experiments, to ensure that each sentence will be pre-annotated by all the pre-annotators, and will be annotated by all the human annotators.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Assisted annotation experiments. Annotators are assigned to annotate sentences under various  pre-annotation conditions.  Dataset  H 1  H 2  H 3", "labels": [], "entities": []}, {"text": " Table 2: Results of assisted annotation experiments.", "labels": [], "entities": []}, {"text": " Table 3: Estimated time spent on reading a token, adding a new annotation, modifying an existed anno- tation, etc.", "labels": [], "entities": [{"text": "Estimated", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9816465973854065}]}]}