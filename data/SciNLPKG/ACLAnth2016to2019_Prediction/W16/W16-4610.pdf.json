{"title": [{"text": "Lexicons and Minimum Risk Training for Neural Machine Translation: NAIST-CMU at WAT2016", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6599260965983073}, {"text": "NAIST-CMU", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.966428816318512}, {"text": "WAT2016", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.779358446598053}]}], "abstractContent": [{"text": "This year, the Nara Institute of Science and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models.", "labels": [], "entities": [{"text": "Japanese-English translation track of the 2016 Workshop on Asian Translation", "start_pos": 115, "end_pos": 191, "type": "TASK", "confidence": 0.7330602645874024}, {"text": "attentional neural machine translation (NMT)", "start_pos": 205, "end_pos": 249, "type": "TASK", "confidence": 0.7359315540109362}]}, {"text": "In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score.", "labels": [], "entities": [{"text": "MT", "start_pos": 214, "end_pos": 216, "type": "TASK", "confidence": 0.8554421067237854}, {"text": "BLEU score", "start_pos": 228, "end_pos": 238, "type": "METRIC", "confidence": 0.9701846837997437}]}, {"text": "As a result, our system achieved the highest translation evaluation scores for the task.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.8123651742935181}]}], "introductionContent": [{"text": "Neural machine translation (NMT;), creation of translation models using neural networks, has quickly achieved state-of-the-art results on a number of translation tasks.", "labels": [], "entities": [{"text": "Neural machine translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7314682543277741}]}, {"text": "In this paper, we describe NMT systems for the Japanese-English scientific paper translation task of the Workshop on Asian Translation (WAT)).", "labels": [], "entities": [{"text": "Japanese-English scientific paper translation task", "start_pos": 47, "end_pos": 97, "type": "TASK", "confidence": 0.6435056567192078}, {"text": "Asian Translation (WAT))", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.8285250067710876}]}, {"text": "The systems are built using attentional neural networks (, with a number of improvements ( \u00a72).", "labels": [], "entities": []}, {"text": "In particular we focus on two.", "labels": [], "entities": []}, {"text": "First, we follow the recent work of in incorporating discrete translation lexicons to improve the probability estimates of the neural translation model ( \u00a73).", "labels": [], "entities": [{"text": "neural translation", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.748251348733902}]}, {"text": "Second, we incorporate minimum-risk training to optimize the parameters of the model to improve translation accuracy ( \u00a74).", "labels": [], "entities": [{"text": "translation", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.9615446925163269}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.8437135219573975}]}, {"text": "In experiments ( \u00a75), we examine the effect of each of these improvements, and find that they both contribute to overall translation accuracy, leading to state-of-the-art results on the Japanese-English translation task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9307593107223511}, {"text": "Japanese-English translation task", "start_pos": 186, "end_pos": 219, "type": "TASK", "confidence": 0.7979918122291565}]}], "datasetContent": [{"text": "To create data to train the model, we use the top 2M sentences of the ASPEC Japanese-English training corpus (Nakazawa et al., 2016b) provided by the task.", "labels": [], "entities": [{"text": "ASPEC Japanese-English training corpus", "start_pos": 70, "end_pos": 108, "type": "DATASET", "confidence": 0.7726034075021744}]}, {"text": "The Japanese size of the corpus is tokenized using, and the English side is tokenized with the tokenizer provided with the Travatar toolkit.", "labels": [], "entities": []}, {"text": "Japanese is further normalized so all full-width roman characters and digits are normalized to half-width.", "labels": [], "entities": []}, {"text": "The words are further broken into subword units using joint byte pair encoding () with 100,000 merge operations.", "labels": [], "entities": []}, {"text": "In we show results for various settings regarding attention, the use of lexicons, training criterion, and word penalty.", "labels": [], "entities": []}, {"text": "In addition, we calculate the ensemble of 6 models, where the average probability assigned by each of the models is used to determine the probability of the next word attest time.", "labels": [], "entities": []}, {"text": "From the results in the table, we can glean a number of observations.", "labels": [], "entities": []}, {"text": "Use of Lexicons: Comparing (1) with (2-4), we can see that in general, using lexicons tends to provide a benefit, particularly when the \u03f5 parameter is set to a small value.", "labels": [], "entities": []}, {"text": "Type of Attention: Comparing (2-4) with (5-7) we can see that on average, multi-layer perceptron attention was more effective than using the dot product.", "labels": [], "entities": []}, {"text": "Use of Word Penalties: Comparing the first and second columns of results, there is a large increase inaccuracy across the board when using a word penalty, demonstrating that this is an easy way to remedy the length of NMT results.", "labels": [], "entities": []}, {"text": "Minimum Risk Training: Looking at the third column, we can see that there is an additional increase inaccuracy from minimum risk training.", "labels": [], "entities": [{"text": "Minimum Risk Training", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6273241837819418}]}, {"text": "In addition, we can see that after minimum risk, the model produces hypotheses that are more-or-less appropriate length without using a word penalty, an additional benefit.", "labels": [], "entities": []}, {"text": "Ensemble: As widely reported in previous work, ensembling together multiple models greatly improved performance.", "labels": [], "entities": []}, {"text": "The maximum-likelihood trained ensemble system with a word penalty of 0.8 (the bottom middle system in) was submitted for manual evaluation.", "labels": [], "entities": []}, {"text": "The system was evaluated according to the official WAT \"HUMAN\" metric (, which consists of pairwise comparisons with a baseline phrase-based system, where the evaluated system receives +1 for every win, -1 for every tie, 0 for every loss, these values are averaged overall evaluated sentences, then the value is multiplied by 100.", "labels": [], "entities": [{"text": "WAT \"HUMAN\"", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.47861340641975403}]}, {"text": "This system achieved a manual evaluation score of 47.50, which was slightly higher than other systems participating in the task.", "labels": [], "entities": []}, {"text": "In addition, while the full results of the minimum-risk-based ensemble were not ready in time for the manual evaluation stage, a preliminary system ensembling the minimum-risktrained versions of the first four systems (1)-(4) in was also evaluated (its BLEU/RIBES scores were comparable to the fully ensembled ML-trained system), and received a score of 48.25, the best in the task, albeit by a small margin.", "labels": [], "entities": [{"text": "BLEU/RIBES scores", "start_pos": 253, "end_pos": 270, "type": "METRIC", "confidence": 0.8058263212442398}]}], "tableCaptions": [{"text": " Table 1: Overall BLEU, RIBES, and length ratio for systems with various types of attention (dot prod- uct or multi-layer perceptron), lexicon (yes/no and which value of \u03bb), training algorithm (maximum  likelihood or minimum risk), and word penalty value.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9995456337928772}, {"text": "RIBES", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9967855215072632}, {"text": "length ratio", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.9642493724822998}]}]}