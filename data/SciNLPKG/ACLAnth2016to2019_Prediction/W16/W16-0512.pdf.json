{"title": [{"text": "Shallow Semantic Reasoning from an Incomplete Gold Standard for Learner Language", "labels": [], "entities": [{"text": "Semantic Reasoning", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7112736701965332}]}], "abstractContent": [{"text": "We investigate questions of how to reason about learner meaning in cases where the set of correct meanings is never entirely complete , specifically for the case of picture description tasks (PDTs).", "labels": [], "entities": [{"text": "picture description tasks (PDTs)", "start_pos": 165, "end_pos": 197, "type": "TASK", "confidence": 0.8585890630880991}]}, {"text": "To operationalize this, we explore different models of representing and scoring non-native speaker (NNS) responses to a picture, including bags of dependencies , automatically determining the relevant parts of an image from a set of native speaker (NS) responses.", "labels": [], "entities": []}, {"text": "In more exploratory work, we examine the variability in both NS and NNS responses, and how different system parameters correlate with the variability.", "labels": [], "entities": []}, {"text": "In this way, we hope to provide insight for future system development, data collection, and investigations into learner language.", "labels": [], "entities": [{"text": "data collection", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.790674090385437}]}], "introductionContent": [], "datasetContent": [{"text": "We ran 60 response experiments, each with different system settings (section 4.3).", "labels": [], "entities": []}, {"text": "Within each experiment, we rank the 39 scored NNS responses from least to most similar to the GS.", "labels": [], "entities": [{"text": "GS", "start_pos": 94, "end_pos": 96, "type": "DATASET", "confidence": 0.902320921421051}]}, {"text": "For assessing these settings themselves, we rely on past annotation, which counted unacceptable responses as errors (see section 3).", "labels": [], "entities": []}, {"text": "As the lowest rank indicates the greatest distance from the GS, a good system setting should ideally position the unacceptable responses among those with the lowest rankings.", "labels": [], "entities": [{"text": "GS", "start_pos": 60, "end_pos": 62, "type": "DATASET", "confidence": 0.5119361281394958}]}, {"text": "Thus, we assign each error-containing response a score equal to its rank, or, if necessary, the average rank of responses sharing the same score.", "labels": [], "entities": []}, {"text": "In, an excerpt of sentence responses is shown for one item, ranked from lowest to highest.", "labels": [], "entities": []}, {"text": "To take one example, the third-ranked sentence, the man is hurting duck, has a score of 0.996, and it is annotated as an error (1 in the E column).", "labels": [], "entities": []}, {"text": "Thus, the evaluation metric adds a score of 3 to the overall sum.", "labels": [], "entities": []}, {"text": "The sentence ranked 18, by contrast, is not an error, and so nothing is added.", "labels": [], "entities": []}, {"text": "In the case of the top rank, two responses with errors are tied, covering rank 1 and 2, so each adds a score of 1.5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Rankings for Item 10 from the best system setting", "labels": [], "entities": [{"text": "Item 10", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.8544447720050812}]}, {"text": " Table 2: Approaches and parameters ranked by mean average precision for all 10 PDT items.", "labels": [], "entities": [{"text": "Approaches", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9973354935646057}, {"text": "mean average precision", "start_pos": 46, "end_pos": 68, "type": "METRIC", "confidence": 0.7677459915479025}]}, {"text": " Table 3: Based on Mean Average Precision, the five best and", "labels": [], "entities": [{"text": "Mean Average", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.9446132779121399}, {"text": "Precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.5192880630493164}]}, {"text": " Table 4: Based on Average Precision, the five best and five", "labels": [], "entities": [{"text": "Precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.925037682056427}]}, {"text": " Table 5: Based on Average Precision, the five best and five", "labels": [], "entities": [{"text": "Precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9309561252593994}]}]}