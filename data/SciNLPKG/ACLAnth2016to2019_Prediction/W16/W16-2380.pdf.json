{"title": [], "abstractContent": [{"text": "This paper describes our submission UFAL MULTIVEC to the WMT16 Quality Estimation Shared Task, for English-German sentence-level post-editing effort prediction and ranking.", "labels": [], "entities": [{"text": "WMT16 Quality Estimation Shared Task", "start_pos": 57, "end_pos": 93, "type": "TASK", "confidence": 0.5437334418296814}, {"text": "English-German sentence-level post-editing effort prediction", "start_pos": 99, "end_pos": 159, "type": "TASK", "confidence": 0.5254977762699127}]}, {"text": "Our approach exploits the power of bilingual distributed representations, word alignments and also manual post-edits to boost the performance of the baseline QuEst++ set of features.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.7703577876091003}]}, {"text": "Our model outperforms the baseline, as well as the winning system in WMT15, Referential Translation Machines (RTM), in both scoring and ranking sub-tasks.", "labels": [], "entities": [{"text": "WMT15", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.6990554928779602}, {"text": "Referential Translation Machines (RTM)", "start_pos": 76, "end_pos": 114, "type": "TASK", "confidence": 0.8188259899616241}]}], "introductionContent": [{"text": "Recently, the task of quality estimation (QE) for machine translation (MT) output attracted interest among researchers in the machine translation community.", "labels": [], "entities": [{"text": "quality estimation (QE)", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.5484560430049896}, {"text": "machine translation (MT) output", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.8551865120728811}, {"text": "machine translation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7514058947563171}]}, {"text": "QE systems play an important role in improving post-editing efficiency (in terms of the time and effort) in different ways, e.g. by filtering out low quality translations to avoid spending time post-editing them, or by providing end-users with an estimate on how good or bad the translation is.", "labels": [], "entities": []}, {"text": "In 2012, WMT established the first sentencelevel quality estimation shared task).", "labels": [], "entities": [{"text": "WMT", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.7828933000564575}, {"text": "sentencelevel quality estimation shared task", "start_pos": 35, "end_pos": 79, "type": "TASK", "confidence": 0.7577745974063873}]}, {"text": "Since then, new sub-tasks, language pairs and datasets in different domains were introduced every year (.", "labels": [], "entities": []}, {"text": "In contrast to automatic evaluation (the \"metrics task\"), QE task aims to develop systems that provide predictions on the quality of machine translated text without access to reference translations ().", "labels": [], "entities": []}, {"text": "Sentence-level QE is the most popular track in the WMT QE shared task, due to its presence in all editions of the task since the beginning.", "labels": [], "entities": [{"text": "Sentence-level QE", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6321548372507095}, {"text": "WMT QE shared task", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.5204048529267311}]}, {"text": "Many features have been explored by participating systems, including lexical, syntactic, semantic, embeddingbased features, as well as features dependent on any details the particular MT systems may provide; Camargo de).", "labels": [], "entities": []}, {"text": "In our model, we try to exploit the power of bilingual distributed representations combined with word alignment information to boost the performance of translation quality estimation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.7218380868434906}, {"text": "translation quality estimation", "start_pos": 152, "end_pos": 182, "type": "TASK", "confidence": 0.9034698208173116}]}, {"text": "For this purpose, we use the implementation provided by the Multivec tool) for the bilingual distributed representation model, described by and the GIZA++ word alignment model.", "labels": [], "entities": [{"text": "GIZA++ word alignment", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.7767673283815384}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Sections 2 and 3, we give an overview of the bilingual distributional model and word alignment for our purposes.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.7715512812137604}]}, {"text": "Section 4 gives a detailed description of our feature set, including the features derived from manual post-edits of other sentences.", "labels": [], "entities": []}, {"text": "Section 5 describes the datasets and resources we used to build our model.", "labels": [], "entities": []}, {"text": "Section 6 discusses the experiments conducted and the official results.", "labels": [], "entities": []}, {"text": "The final Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our submission, we use the Python wrapper for BiSkip provided in the MultiVec tool.", "labels": [], "entities": []}, {"text": "To train the model, we use the ITcorpus with the default configuration of the tool.", "labels": [], "entities": [{"text": "ITcorpus", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.954806923866272}]}, {"text": "The model was trained using a learning rate \u03b1 set to 0.05 and sample (a threshold on words' frequency) set to 0.001.", "labels": [], "entities": []}, {"text": "As a prediction model, we use the Linear Regression model to predict the post-editing effort need for each translation.", "labels": [], "entities": []}, {"text": "In our experiments, we tried different combinations of the introduced features.", "labels": [], "entities": []}, {"text": "Best results are obtained by training the model using all the features.", "labels": [], "entities": []}, {"text": "list the results of examined feature combinations on the development and test parts of QEcorpus, respectively.", "labels": [], "entities": [{"text": "QEcorpus", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.8637707233428955}]}, {"text": "(The golden truth of the test part was made available only after the outputs submission deadline.)", "labels": [], "entities": []}, {"text": "The models are evaluated in terms of Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Pearson's correlation (Pearson's r) for postediting effort prediction, and Spearman's rank correlation coefficient (Spearman's \u03c1) for the ranking task.", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE)", "start_pos": 37, "end_pos": 62, "type": "METRIC", "confidence": 0.9632249474525452}, {"text": "Root Mean Squared Error (RMSE)", "start_pos": 64, "end_pos": 94, "type": "METRIC", "confidence": 0.8796509759766715}, {"text": "Pearson's correlation (Pearson's r)", "start_pos": 100, "end_pos": 135, "type": "METRIC", "confidence": 0.9522666484117508}, {"text": "postediting effort prediction", "start_pos": 140, "end_pos": 169, "type": "TASK", "confidence": 0.747126559416453}, {"text": "rank correlation coefficient (Spearman's \u03c1)", "start_pos": 186, "end_pos": 229, "type": "METRIC", "confidence": 0.7735966853797436}]}, {"text": "Results show that adding the alignment quality score to the set of baseline features gives the 5 https://github.com/eske/multivec best performance compared to the other introduced features on the test set.", "labels": [], "entities": []}, {"text": "When added alone, features based on POS tags or bilingual embeddings do not help and sometimes even slightly degrade the performance, but apparently, they are useful in the combination.", "labels": [], "entities": []}, {"text": "Our submission to the task corresponds to the line \"All Features\" in.", "labels": [], "entities": []}, {"text": "Additionally, we experimented with replacing the parallel ITcorpus with only the comparable (but larger) ComparableNews when extracting bilingual embeddings.", "labels": [], "entities": [{"text": "ITcorpus", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.9256955981254578}]}, {"text": "As documented in Table 5, the size of the monolingual data is apparently more important for the quality of the alignments.", "labels": [], "entities": []}, {"text": "MultiVec, given two corpora, extracts the word alignments automatically, and obviously, it is going to fail most of the time when given a non-parallel corpus.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.6797983795404434}]}, {"text": "Nevertheless, the few random alignments are probably sufficient to blend the source and target subspaces of the vector representation of words, because the setup with all BE features trained on ComparableNews instead of ITcorpus works better.", "labels": [], "entities": [{"text": "BE", "start_pos": 171, "end_pos": 173, "type": "METRIC", "confidence": 0.9435729384422302}, {"text": "ITcorpus", "start_pos": 220, "end_pos": 228, "type": "DATASET", "confidence": 0.9474500417709351}]}], "tableCaptions": [{"text": " Table 1: Extracted Bigrams Numbers", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of the introduced features using WMT16 Sentence-Level QE Development set", "labels": [], "entities": [{"text": "WMT16 Sentence-Level QE Development set", "start_pos": 54, "end_pos": 93, "type": "DATASET", "confidence": 0.8785362958908081}]}, {"text": " Table 4: Evaluation of the introduced features using WMT16 Sentence-Level QE Test set", "labels": [], "entities": [{"text": "WMT16 Sentence-Level QE Test set", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.8910149216651917}]}, {"text": " Table 5: Results of All Features with bilingual em- beddings trained on ITcorpus or ComparableNews", "labels": [], "entities": [{"text": "ITcorpus", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.9892142415046692}]}, {"text": " Table 6: Official results for WMT16 Sentence-Level QE Scoring sub-task", "labels": [], "entities": [{"text": "WMT16 Sentence-Level QE Scoring", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.6948617249727249}]}, {"text": " Table 7: Official results for WMT16 Sentence-Level QE Ranking sub-task", "labels": [], "entities": [{"text": "WMT16 Sentence-Level QE Ranking sub-task", "start_pos": 31, "end_pos": 71, "type": "DATASET", "confidence": 0.719842004776001}]}]}