{"title": [{"text": "Story Cloze Evaluator: Vector Space Representation Evaluation by Predicting What Happens Next", "labels": [], "entities": [{"text": "Story Cloze Evaluator", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7672043442726135}, {"text": "Vector Space Representation Evaluation", "start_pos": 23, "end_pos": 61, "type": "TASK", "confidence": 0.7818603366613388}]}], "abstractContent": [{"text": "The main intrinsic evaluation for vector space representation has been focused on textual similarity, where the task is to predict how semantically similar two words or sentences are.", "labels": [], "entities": [{"text": "vector space representation", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6419647932052612}]}, {"text": "We propose a novel framework, Story Cloze Evalua-tor, for evaluating vector representations which goes beyond textual similarity and captures the notion of predicting what should happen next given a context.", "labels": [], "entities": []}, {"text": "This evaluation methodology is simple to run, scalable, reproducible by the community, non-subjective, 100% agreeable by human , and challenging to the state-of-the-art models, which makes it a promising new framework for further investment of the representation learning community.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been a surge of work in the vector representation research in the past few years.", "labels": [], "entities": [{"text": "vector representation research", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.8620752692222595}]}, {"text": "While one could evaluate a given vector representation (embedding) on various down-stream applications, it is time-consuming at both implementation and runtime, which gives rise to focusing on an intrinsic evaluation.", "labels": [], "entities": []}, {"text": "The intrinsic evaluation has been mostly focused on textual similarity where the task is to predict how semantically similar two words/sentences are, which is evaluated against the gold human similarity scores.", "labels": [], "entities": []}, {"text": "It has been shown that semantic similarity tasks do not accurately measure the effectiveness of an embedding in the other down-stream tasks.", "labels": [], "entities": []}, {"text": "Furthermore, human annotation of similarity at sentencelevel without any underlying context can be subjective, resulting in lower inter-annotator agreement and hence a less reliable evaluation method.", "labels": [], "entities": []}, {"text": "There has not been any standardized intrinsic evaluation for the quality of sentence and documentlevel vector representations beyond textual similarity . There is therefore a crucial need for new ways of evaluating semantic representations of language which capture other linguistic phenomena.", "labels": [], "entities": []}, {"text": "In this paper we propose anew proxy task, Story Cloze Test, for measuring the quality of vector space representations for generic language understanding and commonsense reasoning.", "labels": [], "entities": [{"text": "generic language understanding", "start_pos": 122, "end_pos": 152, "type": "TASK", "confidence": 0.8526734511057535}, {"text": "commonsense reasoning", "start_pos": 157, "end_pos": 178, "type": "TASK", "confidence": 0.8661643862724304}]}, {"text": "In this task, given a four-sentence story (called the context) and two alternative endings to the story, the system is tasked with choosing the right ending.", "labels": [], "entities": []}, {"text": "We propose the following Story Cloze Evaluator modules: (1) Given an embedding of a foursentence story (the context) and two alternative ending sentences, this module rewards the system if the embedding of the context is closer to the right ending than the wrong ending.", "labels": [], "entities": []}, {"text": "(2) Given the embedding for each of the four sentences and each of the two alternatives, this module uses the trajectory of the four vectors to predict the embedding of the fifth sentence.", "labels": [], "entities": []}, {"text": "Then the system is rewarded if the predicted vector is closer to the right ending than the wrong ending.", "labels": [], "entities": []}, {"text": "A vector representation that achieves a high score according to the Story Cloze Evaluator is demonstrating some level of language and narrative understanding.", "labels": [], "entities": []}, {"text": "We describe the Story Cloze Test in Section 2, where we show that this testis scalable, non-subjective and 100% agreeable by human.", "labels": [], "entities": []}, {"text": "We further describe our evaluation methodology in Section 3.", "labels": [], "entities": []}, {"text": "As with any evaluation framework, we expect the setup to be modified overtime, the updates of which can be followed through http:// cs.rochester.edu/nlp/rocstories/ RepEvalPredictingTheNext/.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}