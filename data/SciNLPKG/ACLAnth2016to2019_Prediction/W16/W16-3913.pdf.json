{"title": [], "abstractContent": [{"text": "Topic modelling techniques such as LDA have recently been applied to speech transcripts and OCR output.", "labels": [], "entities": []}, {"text": "These corpora may contain noisy or erroneous texts which may undermine topic stability.", "labels": [], "entities": []}, {"text": "Therefore, it is important to know how well a topic modelling algorithm will perform when applied to noisy data.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7411575019359589}]}, {"text": "In this paper we show that different types of textual noise can have diverse effects on the stability of topic models.", "labels": [], "entities": []}, {"text": "On the other hand, topic model stability is not consistent with the same type but different levels of noise.", "labels": [], "entities": []}, {"text": "We introduce a dictionary filtering approach to address this challenge, with the result that a topic model with the correct number of topics is always identified across different levels of noise.", "labels": [], "entities": [{"text": "dictionary filtering", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.691874161362648}]}], "introductionContent": [{"text": "Topic modelling techniques are widely applied in text retrieval tasks.", "labels": [], "entities": [{"text": "Topic modelling", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8218990564346313}, {"text": "text retrieval tasks", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.8644770383834839}]}, {"text": "Frequently these techniques have been applied to explore high-quality texts such as news articles) and blog posts () which have low levels of noise (i.e. few missing, misspelled, or incorrect terms and phrases).", "labels": [], "entities": []}, {"text": "However, with the reduction in the cost of automatic speech transcription and optical character recognition (OCR) technologies, the range of sources that topic modelling can now be applied to is growing.", "labels": [], "entities": [{"text": "speech transcription", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.721150279045105}, {"text": "optical character recognition (OCR)", "start_pos": 78, "end_pos": 113, "type": "TASK", "confidence": 0.7909648517767588}, {"text": "topic modelling", "start_pos": 154, "end_pos": 169, "type": "TASK", "confidence": 0.7097281217575073}]}, {"text": "One challenge with such textual sources is dealing with their inherent noise.", "labels": [], "entities": []}, {"text": "In speech to text transcriptions, humans in general manage a WER of 2% to 4% (.", "labels": [], "entities": [{"text": "speech to text transcriptions", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.6552504897117615}, {"text": "WER", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.996856689453125}]}, {"text": "When transcribing with a vocabulary size of 200, 5000 and 100000 terms, the reported corresponding word error rates are 3%, 7% and 45% respectively.", "labels": [], "entities": [{"text": "transcribing", "start_pos": 5, "end_pos": 17, "type": "TASK", "confidence": 0.9603866934776306}, {"text": "word error rates", "start_pos": 99, "end_pos": 115, "type": "METRIC", "confidence": 0.7820384303728739}]}, {"text": "The best accuracy for broadcast news transcription is 13%, but this drops below 25.7% in conference transcription and gets worse in casual conversation (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9993910789489746}, {"text": "broadcast news transcription", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6511480112870535}]}, {"text": "These results indicate that the difficulty of automatic speech recognition increases with vocabulary size, speaker dependency, and level of crosstalk.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.65619628628095}]}, {"text": "Noise aside, many of these newly available sources contain rich and valuable information that can be analysed through topic modelling.", "labels": [], "entities": []}, {"text": "For example, automatic speech transcription applied to call centre audio recordings is able to capture a level of detail that is otherwise unavailable unless the call audio is manually reviewed which is infeasible for large call volumes.", "labels": [], "entities": [{"text": "automatic speech transcription", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.6947848002115885}]}, {"text": "In this case topic modelling can be applied to transcribed text to extract the key issues and emerging topics of discussion.", "labels": [], "entities": []}, {"text": "In this study we propose a method for simulating various types of transcription errors, for the purpose of examining the effect of noise on topic modelling algorithms.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.7466816604137421}]}, {"text": "We then test the robustness of probabilistic topic modelling via Latent Dirichlet Allocation (LDA) using a topic stability measure () over a variety of corpora.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 65, "end_pos": 97, "type": "METRIC", "confidence": 0.8955800056457519}]}, {"text": "The stability of a clustering model refers to its ability to consistently produce similar solutions on data originating from the same source ().", "labels": [], "entities": []}, {"text": "A high level of agreement between the resulting clusterings indicates high stability, in turn suggesting that the current model fits the data well.", "labels": [], "entities": [{"text": "agreement", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9588184952735901}]}, {"text": "Consequently, we measure stability of probabilistic topic models over noise-free dataset and its corresponding noisy dataset.", "labels": [], "entities": []}, {"text": "A high agreement score indicates an appropriate selection of topic model which is robust against textual noise.", "labels": [], "entities": [{"text": "agreement score", "start_pos": 7, "end_pos": 22, "type": "METRIC", "confidence": 0.9528270065784454}]}], "datasetContent": [{"text": "In this paper, we make use of two previously-studied text datasets which differ in terms of content and documents lengths.", "labels": [], "entities": []}, {"text": "The bbc corpus includes 2225 general news articles assigned to five ground truth topics.", "labels": [], "entities": [{"text": "bbc corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9224880337715149}]}, {"text": "The wikilow corpus is a subset of 4986 Wikipedia articles, where the articles are labeled with 10 fine-grained WikiProject sub-categories.", "labels": [], "entities": []}, {"text": "In both datasets the topics consist of distinct vocabularies which we would expect LDA to detect.", "labels": [], "entities": []}, {"text": "For example, the topics in bbc datasets are business, entertainment, politics, sport, and technology.).", "labels": [], "entities": [{"text": "bbc datasets", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.8964317440986633}]}, {"text": "wikipedia-low 4,986 15,441 10 A subset of Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.9509767889976501}]}, {"text": "Articles are labeled with finegrained WikiProject sub-groups ().", "labels": [], "entities": []}, {"text": "In our experiments with LDA, we aim to test topic stability over different levels of noise and different numbers of topics.", "labels": [], "entities": []}, {"text": "In other words, when there is noise added to a text corpus, would the generated topic models be consistent with those from original corpus?", "labels": [], "entities": []}, {"text": "If not, how much discrepancy exists between the two?", "labels": [], "entities": []}, {"text": "Two sets of experiments are designed to introduce word-level noise and image-level noise to the original data.", "labels": [], "entities": []}, {"text": "Our aim is to find guidelines for topic modelling over real noisy text sources in practice.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.791951060295105}]}, {"text": "Since average Jaccard (AJ) score measures the similarity between two ranked lists (topics), we use AJ as an element in evaluating the similarity of two topic models S x and S y . If S x and S y each contains k ranked lists (topics), they are denoted as S x = {R x1 , ..., R xk } and S y = {R y1 , ..., R yk }.", "labels": [], "entities": [{"text": "Jaccard (AJ) score", "start_pos": 14, "end_pos": 32, "type": "METRIC", "confidence": 0.7560954809188842}]}, {"text": "We build a k \u00d7 k matrix M to register each similarity score.", "labels": [], "entities": []}, {"text": "In this matrix entry M ij indicates the AJ score between topic R xi and topic R yj . The best matching between the rows and columns of M can be found using the Hungarian method.", "labels": [], "entities": [{"text": "AJ score", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9751272797584534}]}, {"text": "The agreement score produced from M is named as Hungarian agreement score, and is used as a measure of topic model stability in the following experiments.", "labels": [], "entities": [{"text": "agreement score", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.8873778283596039}, {"text": "Hungarian agreement score", "start_pos": 48, "end_pos": 73, "type": "METRIC", "confidence": 0.6383130451043447}]}], "tableCaptions": []}