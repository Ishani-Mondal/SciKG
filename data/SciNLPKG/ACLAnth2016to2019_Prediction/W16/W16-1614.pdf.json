{"title": [{"text": "Towards cross-lingual distributed representations without parallel text trained with adversarial autoencoders", "labels": [], "entities": []}], "abstractContent": [{"text": "Current approaches to learning vector representations of text that are compatible between different languages usually require some amount of parallel text, aligned at word, sentence or at least document level.", "labels": [], "entities": []}, {"text": "We hypothesize however, that different natural languages share enough semantic structure that it should be possible, in principle , to learn compatible vector representations just by analyzing the monolingual distribution of words.", "labels": [], "entities": []}, {"text": "In order to evaluate this hypothesis, we propose a scheme to map word vectors trained on a source language to vectors semantically compatible with word vectors trained on a target language using an ad-versarial autoencoder.", "labels": [], "entities": []}, {"text": "We present preliminary qualitative results and discuss possible future developments of this technique, such as applications to cross-lingual sentence representations.", "labels": [], "entities": [{"text": "cross-lingual sentence representations", "start_pos": 127, "end_pos": 165, "type": "TASK", "confidence": 0.6934264898300171}]}], "introductionContent": [{"text": "Distributed representations that map words, sentences, paragraphs or documents to vectors real numbers have proven extremely useful fora variety of natural language processing tasks (), as they provide an effective way to inject into machine learning models general prior knowledge about language automatically obtained from inexpensive unannotated corpora.", "labels": [], "entities": []}, {"text": "Based on the assumption that different languages share a similar semantic structure, various approaches succeeded to obtain distributed representations that are compatible across multiple languages, either by learning mappings between different embedding spaces ( or by jointly training cross-lingual representations ().", "labels": [], "entities": []}, {"text": "These approaches all require some amount of parallel text, aligned at word level, sentence level or at least document level, or some other kind of parallel resources such as dictionaries (.", "labels": [], "entities": []}, {"text": "In this work we explore whether the assumption of a shared semantic structure between languages is strong enough that it allows to induce compatible distributed representations without using any parallel resource.", "labels": [], "entities": []}, {"text": "We only require monolingual corpora that are thematically similar between languages in a general sense.", "labels": [], "entities": []}, {"text": "We motivate this hypothesis by observing that humans, especially young children, who acquire multiple languages, can often do so with relatively little exposure to explicitly aligned parallel linguistic information, at best they may have access to distant and noisy alignment information in the form of multisensorial environmental clues.", "labels": [], "entities": []}, {"text": "Nevertheless, multilingual speakers are always au-tomatically able to translate between all the languages that they can speak, which suggests that their brain either uses a shared conceptual representations for the different surface features of each language, or uses distinct but near-isomorphic representations that can be easily transformed into each other.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed some preliminary exploratory experiments on our model.", "labels": [], "entities": []}, {"text": "In this section we report salient results.", "labels": [], "entities": []}, {"text": "The first experiment is qualitative, to assess whether our model is able to learn any semantically sensible transformation at all.", "labels": [], "entities": []}, {"text": "We consider English to Italian embedding mapping.", "labels": [], "entities": []}, {"text": "We train English and Italian word embeddings on randomly subsampled Wikipedia corpora consisting of about 1.5 million sentences per language.", "labels": [], "entities": []}, {"text": "We use word2vec () in skipgram mode to generate embeddings with dimension d = 100.", "labels": [], "entities": []}, {"text": "Our encoder and decoder are linear models with tied matrices (one the transpose of the other), initialized as random orthogonal matrices (we also explored deep non-linear autoencoders but we found that they make the optimization more difficult without providing apparent benefits).", "labels": [], "entities": []}, {"text": "Our discriminator is a Residual Network () without convolutions, one leaky ReLU non-linearity ( per block, no non-linearities on the passthrough path, batch normalization (Ioffe and Szegedy, 2015) and dropout ().", "labels": [], "entities": []}, {"text": "The block (layer) equation is: where W t is a weight matrix and \u03c6 is batch normalization (with its internal parameters) followed by leaky ReLU and ht is a k-dimensional block state (in our experiments k = 40).", "labels": [], "entities": [{"text": "ReLU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.927277147769928}]}, {"text": "The network has T = 10 blocks followed by a 1-dimensional output layer with logistic sigmoid activation.", "labels": [], "entities": []}, {"text": "We found that using a Residual Network as discriminator rather than a standard multi-layer perceptron yields larger gradients being backpropagated to the generator, facilitating training.", "labels": [], "entities": []}, {"text": "We actually train two discriminators per experiment, with identical structure but different random initializations, and use one to train the generator and the other for monitoring in order to help us determine whether overfitting or underfitting occurs.", "labels": [], "entities": []}, {"text": "At each step, word embeddings are sampled according to their frequency in the original corpora, adjusted to subsample frequent words, as in word2vec.", "labels": [], "entities": []}, {"text": "Updates are performed using the Adam optimizer () with learning rate 0.001 for the encoder-decoder and 0.01 for the discriminator.", "labels": [], "entities": []}, {"text": "The code 1 is implemented in Python, Theano (Theano Development Team, 2016) and Lasagne.", "labels": [], "entities": [{"text": "Theano (Theano Development Team, 2016)", "start_pos": 37, "end_pos": 75, "type": "DATASET", "confidence": 0.9082446470856667}]}, {"text": "We qualitatively analyzed the quality of the embeddings by considering the closest Italian embeddings to a sample of transformed English embeddings.", "labels": [], "entities": []}, {"text": "We notice that in some cases the closest or nearly closest embedding is the true translation, for instance 'computer' (en) ->'computer' (it).", "labels": [], "entities": []}, {"text": "Other terms, such as names of places however, tend to be transformed incorrectly, for instance 'France' (en) ->'Radiomobile', 'Cartubi', 'Freniatria', 'UNUCI', 'Cornhole', 'Internazione', 'CSCE', 'Folklorica', 'UECI', 'Rientro' (it 10-best).", "labels": [], "entities": [{"text": "Rientro", "start_pos": 219, "end_pos": 226, "type": "METRIC", "confidence": 0.8962544202804565}]}, {"text": "We further evaluate our model on German to English and English to German embedding transformations, using the same evaluation setup as () with embeddings trained on the concatenation of the Reuters corpora and the News Commentary 2015 corpora, with embedding dimension d = 40 and discriminator depth T = 4.", "labels": [], "entities": [{"text": "Reuters corpora", "start_pos": 190, "end_pos": 205, "type": "DATASET", "confidence": 0.9458280503749847}, {"text": "News Commentary 2015 corpora", "start_pos": 214, "end_pos": 242, "type": "DATASET", "confidence": 0.9342976808547974}, {"text": "discriminator depth T", "start_pos": 280, "end_pos": 301, "type": "METRIC", "confidence": 0.869979202747345}]}, {"text": "On a qualitative analysis notice similar partial semantic similarity patterns.", "labels": [], "entities": []}, {"text": "However the cross-lingual document classification task we were able to improve over the baseline only for the smallest training set size.", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.6830686827500662}]}], "tableCaptions": []}