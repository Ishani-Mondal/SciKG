{"title": [], "abstractContent": [{"text": "The WMT Bilingual Document Alignment Task requires systems to assign source pages to their \"translations\", in a big space of possible pairs.", "labels": [], "entities": [{"text": "WMT Bilingual Document Alignment Task", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.7677863538265228}]}, {"text": "We present four methods: The first one uses the term position similarity between candidate document pairs.", "labels": [], "entities": []}, {"text": "The second method requires automatically translated versions of the target text, and matches them with the candidates.", "labels": [], "entities": []}, {"text": "The third and fourth methods try to overcome some of the challenges presented by the nature of the corpus, by considering the string similarity of source URL and candidate URL, and combining the first two approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parallel data play an essential role in training of statistical machine translation (MT) systems.", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.7848617186148962}]}, {"text": "While big collections have been already created, e.g. the corpus OPUS, the World Wide Web remains a largely underexploited source.", "labels": [], "entities": []}, {"text": "That is the motivation for the shared task \"Bilingual Document Alignment\" of the ACL 2016 workshop First Conference on Machine Translation (WMT16) which requires participants to align web page in one language to their translation counterparts in another language.", "labels": [], "entities": [{"text": "Bilingual Document Alignment", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.8650559782981873}, {"text": "ACL 2016 workshop First Conference on Machine Translation (WMT16)", "start_pos": 81, "end_pos": 146, "type": "TASK", "confidence": 0.6798680153760043}]}, {"text": "Given a large collection of documents, the first step in extracting parallel data is to organize the documents into heaps by the language they are written in.", "labels": [], "entities": []}, {"text": "For two languages of interest, a bruteforce approach would consider all pairs of documents from the two heaps.", "labels": [], "entities": []}, {"text": "Since the number of possible pairings is too high, it is necessary to employ some broad and fast heuristics to filter out the obviously wrong pairs.", "labels": [], "entities": []}, {"text": "Some approaches to the task rely on document metadata (e.g. the similarity of document URLs or language tags within URLs), some emphasize more the actual content of the documents.", "labels": [], "entities": []}, {"text": "Previous work) focused on document alignment by counting word co-occurrences between source and target documents in a fixed-size window.", "labels": [], "entities": [{"text": "document alignment", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7471075654029846}]}, {"text": "More recently, methods from cross-lingual information retrieval (CLIR) have been used, ranking lists of target documents given a source document by a probabilistic model.", "labels": [], "entities": [{"text": "cross-lingual information retrieval (CLIR)", "start_pos": 28, "end_pos": 70, "type": "TASK", "confidence": 0.7589694559574127}]}, {"text": "Locality sensitive hashing has also been applied (.", "labels": [], "entities": []}, {"text": "In this paper, we describe our attempt.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: In Section 2, we describe the methods we used in our four submitted systems.", "labels": [], "entities": []}, {"text": "Section 3 describes our experimental setup and compares the results of the proposed methods.", "labels": [], "entities": []}, {"text": "We conclude the paper and discuss possible future improvements in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We noticed that there were many cases where several documents contained the same (or almost the same) text, which therefore get scored (roughly) the same by each of UFAL-1 and UFAL-2.", "labels": [], "entities": [{"text": "UFAL-1", "start_pos": 165, "end_pos": 171, "type": "DATASET", "confidence": 0.9494773745536804}, {"text": "UFAL-2", "start_pos": 176, "end_pos": 182, "type": "DATASET", "confidence": 0.9197626709938049}]}, {"text": "This issue will create noise that can harm us in the evaluation of the shared task, as can be seen in 2: There is a significant difference between the top 1 and top 2 accuracy of our UFAL-2 system from Section 2.2, see e.g. the webdomains 5, 7, 13 (kusu.com), or 34 (www.eu2007.de).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9900239109992981}]}, {"text": "While both the 1 st best and the 2 nd best top predictions could be assumed correct since the two predicted pages are not distinguishable or only differ in unimportant details (e.g. Google Ads), the offi-cial scoring will be based on a single-best answer.", "labels": [], "entities": []}, {"text": "1 A closer investigation reveals that the URLs that are marked correct in the training data are usually the ones most similar to the source URL.", "labels": [], "entities": []}, {"text": "We therefore look at the top 10 candidates from the UFAL-2, and choose the candidate that is within some threshold of the top result and closest in Levenshtein distance from the source URL.", "labels": [], "entities": [{"text": "UFAL-2", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.888876736164093}, {"text": "Levenshtein distance", "start_pos": 148, "end_pos": 168, "type": "DATASET", "confidence": 0.6579290330410004}]}, {"text": "The threshold value of 85 was obtained experimentally on the training data.", "labels": [], "entities": []}, {"text": "The result after this refinement is submitted for the evaluation under the name UFAL-3.", "labels": [], "entities": [{"text": "UFAL-3", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.9359392523765564}]}, {"text": "We used the data published with the Shared Task on Bilingual Document Alignment (WMT 2016), containing roughly 4200 million pairs, in which 1624 pairs have been labeled as mutual translations to serve as a development set.", "labels": [], "entities": [{"text": "Shared Task on Bilingual Document Alignment (WMT 2016)", "start_pos": 36, "end_pos": 90, "type": "TASK", "confidence": 0.7351440727710724}]}, {"text": "Work on information extraction typically uses precision and recall of the extracted information as an evaluation measure.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.8339863121509552}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9990366697311401}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9977797865867615}]}, {"text": "However, in this task, manually classifying all possible pairs is impossible, so the true recall cannot be established.", "labels": [], "entities": [{"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9771983623504639}]}, {"text": "The organizers thus decided to evaluate the methods on the recall within the fixed set of document pairs, the development set released prior submission deadline and the official test set disclosed only with the final results.", "labels": [], "entities": []}, {"text": "While the official scores are top-1 recall (i.e. the recall taking the single-best prediction for each input sentence), we also evaluate our systems at top 2 and top 5 outputs because, as discussed in Section 2.3, the there are many documents with the same content, but the development set of pairs mentions only one of them.", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9904058575630188}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9569761753082275}]}, {"text": "All documents are tokenized by splitting on white-space and passed to a filter which prunes all pairs having a ratio of the lengths in tokens of two: Recall measures by baseline system, system using fixed-size window method and system using term position similarity documents bigger than 2.", "labels": [], "entities": []}, {"text": "Afterwards, all documents are ranked by the discussed methods.", "labels": [], "entities": []}, {"text": "The first 1, 2 or 5 ranked documents with score higher than a threshold are reported.", "labels": [], "entities": []}, {"text": "In the first experiment, we prepare three systems for comparison.", "labels": [], "entities": []}, {"text": "We use the provided baseline system in the mentioned shared task which simply finds matching URLs by discarding language identifiers, such as en, fr.", "labels": [], "entities": []}, {"text": "We also implement a fixed-size window method as described in.", "labels": [], "entities": []}, {"text": "We compare the fixedsize window method with our term position similarity in 6 tests with increasing size of the underlying bilingual dictionary.", "labels": [], "entities": []}, {"text": "This dictionary is obtained by running IBM Model 2 implemented by on the translations of the data set provided by the organizers.", "labels": [], "entities": []}, {"text": "We extract the 50000 most frequent word alignments fr \u2212 en having P (en | f r) > 0.7 and then randomly draw a subset of this dictionary for each test.", "labels": [], "entities": []}, {"text": "The variant with 5000 entries is our submission called UFAL-1.", "labels": [], "entities": [{"text": "UFAL-1", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.9457879066467285}]}, {"text": "If two documents have an identical score, the one having a shorter URL is preferred.", "labels": [], "entities": []}, {"text": "In the second experiment, we compare the term position similarity method (UFAL-1) with the language model-based approach (UFAL-2 and UFAL-3) and the combination method (UFAL-4).", "labels": [], "entities": []}, {"text": "The term position similarity method uses a bilingual dictionary containing 5000 entries.", "labels": [], "entities": [{"text": "position similarity", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7235682010650635}]}, {"text": "Automatic translations for all target documents were provided by the organizers who used a baseline Moses setup trained on Europarl and the News Commentary corpus.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.9905670285224915}, {"text": "News Commentary corpus", "start_pos": 140, "end_pos": 162, "type": "DATASET", "confidence": 0.9019131064414978}]}, {"text": "The results for first experiment are in.", "labels": [], "entities": []}, {"text": "From these results, we can clearly see that term: Result on the development set position similarity outperforms the fixed-size window method and surpasses the baseline system with around 20% even without a bilingual dictionary.", "labels": [], "entities": []}, {"text": "By increasing size of the bilingual dictionary up to 50000 entries, we can boost up the term position similarity method by 8% to 96.06%.", "labels": [], "entities": []}, {"text": "However, there are still a number of avenues for improvement.", "labels": [], "entities": []}, {"text": "First, as we found that our method encountered many errors on the webdomain www.luontoportti.com that contains extremely specialized words not covered by our dictionary, this makes a domain-based bilingual dictionary one of the most desirable potential improvements.", "labels": [], "entities": []}, {"text": "Secondly, the term position similarity method is very sensitive to the case when a target document contains source language text, because it increases the co-occurrence rate between two documents.", "labels": [], "entities": []}, {"text": "Any errors in language identification can thus adversely affect the final extracted parallel corpus.", "labels": [], "entities": [{"text": "language identification", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.725611999630928}]}, {"text": "We present the results of the second experiment in.", "labels": [], "entities": []}, {"text": "The improved methods UFAL-3 and UFAL-4 show significant gains, achieving 93.7% and 94.7% in recall.", "labels": [], "entities": [{"text": "UFAL-3", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.8307087421417236}, {"text": "UFAL-4", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.6360985040664673}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.998806357383728}]}, {"text": "We also clearly seethe remarkable changes in recall for the top match vs. top two matches caused by the similar documents in the corpus, as discussed in Section 2.3.", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9995567202568054}]}, {"text": "Finally, we report the official scores in.", "labels": [], "entities": []}, {"text": "The official test set consists of 2402 document pairs and methods are evaluated in terms of the percentage of these pairs that they reported (\"Recall\").", "labels": [], "entities": [{"text": "Recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9928339719772339}]}, {"text": "The shared task winner NovaLincs-urlcoverage (denoted \"NovaLics\" in the table for short) reached 94.96%, our best method UFAL-4 ranked about in the middle of the methods with the recall of 84.22%.", "labels": [], "entities": [{"text": "NovaLincs-urlcoverage", "start_pos": 23, "end_pos": 44, "type": "DATASET", "confidence": 0.9658732414245605}, {"text": "NovaLics", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9474436044692993}, {"text": "recall", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.9995869994163513}]}, {"text": "As we see in the remaining columns, UFAL-4 produces by far the highest number of document pairs (more that 1M).", "labels": [], "entities": []}, {"text": "The official scoring script filters this list and keeps only the pairs where neither the source URL nor the target URL was previously reported (\"After 1-1\").: The winner and our methods on the official test set.", "labels": [], "entities": []}, {"text": "After this style of deduplication, the number of pairs reduces to about 268k, slightly higher than the number of pairs reported by the winner.", "labels": [], "entities": []}, {"text": "The official test set results are inline with our observation on the development set: term position similarity (UFAL-1) performs well (although not as well as on the development set) and the two variations of the noisy-channel approach are slightly worse, with UFAL-3 (URL similarity) better than UFAL-2.", "labels": [], "entities": [{"text": "term position similarity (UFAL-1)", "start_pos": 86, "end_pos": 119, "type": "METRIC", "confidence": 0.853194942077001}]}, {"text": "The combination (UFAL-4) is the best of our methods.", "labels": [], "entities": [{"text": "UFAL-4", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.6628746390342712}]}, {"text": "We note that for systems like ours that produce all URL pairs they deem good enough, the 1-1 deduplication maybe too strict.", "labels": [], "entities": []}, {"text": "We thus also report a lenient form of the recall: whenever a pair of URLs from the test set appears (as an unordered pair) among the pairs produced by our method, we give a credit for it.", "labels": [], "entities": [{"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9989824891090393}]}, {"text": "As seen in the last column of, the noisy-channel methods seem better than term position similarity in this measure.", "labels": [], "entities": []}, {"text": "Considering that UFAL-2 and UFAL-3 produced slightly fewer pairs than UFAL-1, it may seem that they are more precise.", "labels": [], "entities": []}, {"text": "This however need not be the case; the set of pairs produced by the systems is again too large for manual validation so the true precision cannot be evaluated.", "labels": [], "entities": [{"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9778874516487122}]}], "tableCaptions": [{"text": " Table 1: Recall measures by baseline system, sys- tem using fixed-size window method and system  using term position similarity", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9309245944023132}]}, {"text": " Table 2: Result on the development set", "labels": [], "entities": []}, {"text": " Table 3: The winner and our methods on the offi- cial test set.", "labels": [], "entities": [{"text": "offi- cial test set", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.733795440196991}]}]}