{"title": [{"text": "Learning Knowledge Base Inference with Neural Theorem Provers", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a proof-of-concept implementation of Neural Theorem Provers (NTPs), end-to-end differentiable counterparts of discrete theorem provers that perform first-order inference on vector representations of symbols using function-free, possibly parameterized, rules.", "labels": [], "entities": []}, {"text": "As such, NTPs follow along tradition of neural-symbolic approaches to automated knowledge base inference , but differ in that they are differentiable with respect to representations of symbols in a knowledge base and can thus learn representations of predicates, constants, as well as rules of predefined structure.", "labels": [], "entities": []}, {"text": "Furthermore, they still allow us to incorporate domain-knowledge provided as rules.", "labels": [], "entities": []}, {"text": "The NTP presented here is realized via a differentiable version of the backward chaining algorithm.", "labels": [], "entities": []}, {"text": "It operates on substitution representations and is able to learn complex logical dependencies from training facts of small knowledge bases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current state-of-the-art methods for automated knowledge base (KB) construction learn distributed representations of fact triples ().", "labels": [], "entities": [{"text": "automated knowledge base (KB) construction", "start_pos": 37, "end_pos": 79, "type": "TASK", "confidence": 0.6638576814106533}]}, {"text": "An open question is how to enable first-order reasoning with commonsense knowledge ().", "labels": [], "entities": []}, {"text": "We believe a promising direction towards this goal is the integration of deep neural networks with the capabilities of theorem provers.", "labels": [], "entities": []}, {"text": "Neural networks can learn to generalize well when observing many input-output examples, but lack interpretability and straightforward ways of incorporating domain-specific knowledge.", "labels": [], "entities": []}, {"text": "Theorem provers on the other hand provide effective ways to reason with logical knowledge.", "labels": [], "entities": []}, {"text": "However, by operating on discrete symbols they do not make use of similarities between predicates or constants in training data (e.g., LECTURERAT \u223c PROFESSORAT, ORANGE \u223c LEMON, etc).", "labels": [], "entities": [{"text": "LECTURERAT", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.9592943787574768}, {"text": "ORANGE \u223c LEMON", "start_pos": 161, "end_pos": 175, "type": "METRIC", "confidence": 0.8405030568440756}]}, {"text": "Recent neural network architectures such as Neural Turing Machines (, Memory Networks (), Neural Stacks/Queues (, Neural Programmer (, Neural Programmer-Interpreters (Reed and de and Hierarchical Attentive Memory ( replace discrete functions and data structures by end-to-end differentiable counterparts.", "labels": [], "entities": []}, {"text": "As such, they can learn complex behaviour from raw input-output examples via gradient-based optimization.", "labels": [], "entities": []}, {"text": "NTMs and their relatives are capable of learning programs and could in principle learn to emulate a theorem prover.", "labels": [], "entities": [{"text": "NTMs", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.81015944480896}]}, {"text": "However, they might not be the most efficient neural architecture for learning firstorder reasoning from input-output examples.", "labels": [], "entities": []}, {"text": "Akin to NTMs, which are end-to-end differentiable counterparts of Turing machines, we investigate Neural Theorem Provers (NTPs): end-to-end differentiable versions of automated theorem provers.", "labels": [], "entities": []}, {"text": "A distinguishing property of NTPs is that they are differentiable with respect to symbol representations in a knowledge base.", "labels": [], "entities": []}, {"text": "This enables us to learn representations of symbols in ground atoms (predicates and constants) and parameters of first-order rules of predefined structure using backpropagation.", "labels": [], "entities": []}, {"text": "Furthermore, NTPs can seamlessly reason with provided domain-specific rules.", "labels": [], "entities": []}, {"text": "As NTPs operate on distributed representations of symbols, a single handcrafted rule can be leveraged for many proofs of queries with similar symbol representations.", "labels": [], "entities": []}, {"text": "Finally, NTPs allow fora high degree of interpretability by providing such proofs.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.9595732092857361}]}, {"text": "Our contributions are threefold: (i) we present the construction of an NTP based on differentiable backward chaining and unification, (ii) we show that when provided with rules this NTP can perform firstorder inference in vector space like a discrete theorem prover would do on symbolic representations, and (iii) we demonstrate that NTPs can learn representations of symbols and first-order rules of predefined structure.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implemented an NTP with differentiable backward chaining in TensorFlow (.", "labels": [], "entities": []}, {"text": "Symbol representations are initialized randomly and constrained to unit-length.", "labels": [], "entities": []}, {"text": "During training we iterate over the set of known facts, and optimize negative log-likelihood of the proof success of every fact based on all other facts (and rules) using Adam (.", "labels": [], "entities": []}, {"text": "Furthermore, for every training fact we sample an unobserved fact for the same predicate (but different entity-pair) and optimize its proof with a target success of zero.", "labels": [], "entities": []}, {"text": "Our NTP implementation is tested on toy KBs for different scenarios shown in the four different subplots in.", "labels": [], "entities": []}, {"text": "Every column (within a subplot) represents a predicate and every rowan entity-pair.", "labels": [], "entities": []}, {"text": "First, we run the NTP with given ground-truth rules without training symbol or rule representations, and test whether it can act as a discrete theorem prover.", "labels": [], "entities": []}, {"text": "As expected, given rules the NTP can infer all test facts (2nd subplot in).", "labels": [], "entities": [{"text": "NTP", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.6380858421325684}]}, {"text": "The third subplot shows predictions when we let the NTP try to reconstruct training facts only with the help of other facts by learning symbol representations (similar to other representation learning approaches for KB inference).", "labels": [], "entities": []}, {"text": "Finally, a core benefit of the NTP is visible once we provide few reasonable rule templates 1 and optimize for rule representations that best explain observed facts (4th subplot).", "labels": [], "entities": []}, {"text": "We found that this can work remarkably well, but also noticed that the quality of trained rules is varying with different random initialization of the rule's parameters.", "labels": [], "entities": []}, {"text": "We need to investigate in future work how the robustness of rule learning in NTPs can be improved.", "labels": [], "entities": [{"text": "rule learning", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.8671749830245972}]}], "tableCaptions": []}