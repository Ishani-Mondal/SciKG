{"title": [], "abstractContent": [{"text": "Code-mixing is a prevalent phenomenon in modern day communication.", "labels": [], "entities": []}, {"text": "Though several systems enjoy success in identifying a single language, identifying languages of words in code-mixed texts is a herculean task, more so in asocial media context.", "labels": [], "entities": [{"text": "identifying languages of words in code-mixed texts", "start_pos": 71, "end_pos": 121, "type": "TASK", "confidence": 0.8144646627562386}]}, {"text": "This paper explores the English-Bengali code-mixing phenomenon and presents algorithms capable of identifying the language of every word to a reasonable accuracy in specific cases and the general case.", "labels": [], "entities": [{"text": "identifying the language of every word", "start_pos": 98, "end_pos": 136, "type": "TASK", "confidence": 0.7951323091983795}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9746471643447876}]}, {"text": "We create and test a predictor-corrector model, develop anew code-mixed corpus from Facebook chat (made available for future research) and test and compare the efficiency of various machine learning algorithms (J48, IBk, Random Forest).", "labels": [], "entities": []}, {"text": "The paper also seeks to remove the ambiguities in the token identification process.", "labels": [], "entities": [{"text": "token identification process", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.9414228399594625}]}], "introductionContent": [{"text": "Code-mixing is a phenomenon in linguistics which is exhibited by multi-lingual people.", "labels": [], "entities": []}, {"text": "Essentially, an utterance in which the speaker makes use of the grammar and lexicon of more than one language is said to have undergone code-mixing or codeswitching ().", "labels": [], "entities": []}, {"text": "Though some linguists draw a distinction between the terms \"code-mixing\" and \"code-switching\", we shall refer to both phenomena as \"code-mixing\" in general and draw distinctions regarding the context of switching when required.", "labels": [], "entities": []}, {"text": "With English as the primary language on the internet, one would intuitively expect English to be the major language of use in social media as well.", "labels": [], "entities": []}, {"text": "However, it comes as a bit of a surprise that around half of the messages on Twitter are in non-English languages.", "labels": [], "entities": []}, {"text": "For multilingual people, we notice a tendency to communicate in all/several of the languages that they know.", "labels": [], "entities": []}, {"text": "This arises from the fact that some multilingual speakers feel a higher level of comfort in their native language than in English.", "labels": [], "entities": []}, {"text": "Apart from this, some conversational topics are more fluid in a particular language and some expressions convey the message properly only in one's native language.", "labels": [], "entities": []}, {"text": "In social media, code-mixing between languages such as English and Spanish (both of which employ a Roman script) is much simpler to analyze (apart from potential spelling mistakes) since the words used in normal Spanish and used in social media are spelled and used in almost entirely the same way and using the same script.", "labels": [], "entities": []}, {"text": "However, when we consider languages that employ different scripts, most people do not have patience to switch between scripts while writing.", "labels": [], "entities": []}, {"text": "Thus, people convert words from their language into the Roman script when mixing with English.", "labels": [], "entities": []}, {"text": "In our analysis, we consider one such language, which is extremely fertile when it comes to code-mixing with English: Bengali.", "labels": [], "entities": []}, {"text": "We wish to explore some procedures to identify languages in social media and internet search contexts in codemixed English-Bengali texts.", "labels": [], "entities": []}, {"text": "The rest of the sections are as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the background of English-Bengali codemixed data, especially in social media.", "labels": [], "entities": []}, {"text": "Section 3 discusses the related work in the field of exploration.", "labels": [], "entities": []}, {"text": "Section 4 speaks of the difficulties and hurdles in this language identification task.", "labels": [], "entities": [{"text": "language identification task", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.802389661471049}]}, {"text": "In Section 5, we talk about the nature of code-switching instances that we have found in our corpora.", "labels": [], "entities": []}, {"text": "Section 6 lists the techniques and tools we use in our experiments and Section 7 shares the details, results and observations from those experiments.", "labels": [], "entities": []}, {"text": "In Section 8, we have a general discussion of our observations and we close in Section 9 with conclusions and future goals.", "labels": [], "entities": []}], "datasetContent": [{"text": "Being a less explored language, there is a dearth of English-Bengali code-mixed tagged datasets.", "labels": [], "entities": []}, {"text": "For this reason, we use one corpus from the FIRE 2013 conference (hosted by the Information Retrieval Society of India), that consists of phrases such as search terms, and one corpus that we have built and tagged from social media messages exchanged on Facebook by college students and adults from West Bengal, India.", "labels": [], "entities": [{"text": "FIRE 2013 conference", "start_pos": 44, "end_pos": 64, "type": "DATASET", "confidence": 0.8995755513509115}, {"text": "Information Retrieval Society of India)", "start_pos": 80, "end_pos": 119, "type": "TASK", "confidence": 0.7387429525454839}]}, {"text": "The two test corpora we used: We also provide the statistics for our test corpora in.", "labels": [], "entities": []}, {"text": "The Facebook corpus is composed from chat messages, because we felt that a chat was more likely to have code-mixing, since one converses in a more informal setting there, while public posts are less likely to be influenced by code-mixing.", "labels": [], "entities": []}, {"text": "In the composition of our Facebook chat corpus (which has been made publicly available for future research 2 ), we wanted to get a variety of styles of texting and mixing.", "labels": [], "entities": []}, {"text": "For that reason, we collected text message conversations between Bengali college students, who are acquaintances, college students who were childhood friends, school friends and middle-aged women who are family friends.", "labels": [], "entities": []}, {"text": "The annotation was done by an author.", "labels": [], "entities": []}, {"text": "The two corpora vary in their content types.", "labels": [], "entities": []}, {"text": "As we see in, the FIRE 2013 corpus has the Bengali words heavily outweigh the number of English words, whereas in the Facebook chat corpus, we see an extremely level mix of words from both languages.", "labels": [], "entities": [{"text": "FIRE 2013 corpus", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.9675130844116211}, {"text": "Facebook chat corpus", "start_pos": 118, "end_pos": 138, "type": "DATASET", "confidence": 0.6811587711175283}]}, {"text": "The \"ambiguous words\" row notes the number of words (already considered in the Bengali and English counts) that could belong to the other language too (based on our dictionary test).", "labels": [], "entities": []}, {"text": "For example, the Bengali word, \"more\" IPA: in the FIRE 2013 corpus could be the English \"more\" IPA: as well.", "labels": [], "entities": [{"text": "FIRE 2013 corpus", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.9836322466532389}]}, {"text": "The ambiguous words makeup 29.68% of the FIRE 2013 corpus and 41.31% of the Facebook chat corpus.", "labels": [], "entities": [{"text": "FIRE 2013 corpus", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.9729851484298706}, {"text": "Facebook chat corpus", "start_pos": 76, "end_pos": 96, "type": "DATASET", "confidence": 0.769447902838389}]}, {"text": "We divide these into two categories: 1.", "labels": [], "entities": []}, {"text": "Bengali words that exist in the English dictionary: Bengali (can be En) in the table.", "labels": [], "entities": []}, {"text": "\"more\" discussed above is an example of this category.", "labels": [], "entities": []}, {"text": "2. English words that find a match our search in the Bengali dictionary: English (can be Bn) in the table.", "labels": [], "entities": []}, {"text": "For example, \"to\" IPA: in our Facebook chat corpus is an English word.", "labels": [], "entities": []}, {"text": "However, a Bengali word, \"to\" IPA: also exists.", "labels": [], "entities": []}, {"text": "We notice there area lot more Bengali (can be En) words than English (can be Bn) words in the FIRE 2013 corpus, while it is the exact opposite in the Facebook corpus.", "labels": [], "entities": [{"text": "FIRE 2013 corpus", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.9821832378705343}, {"text": "Facebook corpus", "start_pos": 150, "end_pos": 165, "type": "DATASET", "confidence": 0.8432728052139282}]}, {"text": "In fact, an interesting statistic is that out of all the English words in the Facebook chat corpus, 66.93% of the words register a match in the Bengali dictionary, and thus have a much higher likelihood of being wrongly classified.", "labels": [], "entities": []}, {"text": "We can attribute this in part to the \"Minimum Edit  Distance\" check in our Bengali search described in section 6.1.1.", "labels": [], "entities": [{"text": "Minimum Edit  Distance\" check", "start_pos": 38, "end_pos": 67, "type": "METRIC", "confidence": 0.8810485363006592}]}, {"text": "The following tests were performed in the first half: In the first part, whose results are mentioned in, we note that the order in which we checked the language of words played a huge role in our accuracy (We measure accuracy as words correctly classified out of all the words).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9989877343177795}, {"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.9923661351203918}]}, {"text": "However, the interesting thing is that this gets reversed for both the corpora.", "labels": [], "entities": []}, {"text": "For the Facebook chat corpus, we get a higher accuracy checking English first, while for the FIRE, of the ambiguous words, we noted 71.88% of those in the FIRE corpus were Bengali and 80.37% of those in the Facebook chat corpus were in English.", "labels": [], "entities": [{"text": "Facebook chat corpus", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.7498632868131002}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9992997646331787}, {"text": "FIRE", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.7705344557762146}, {"text": "FIRE corpus", "start_pos": 155, "end_pos": 166, "type": "DATASET", "confidence": 0.9538730680942535}]}, {"text": "This shows that there is a greater chance of the Bengali words in the FIRE corpus being wrongly classified if we check for them being English first and the reverse for the Facebook chat corpus.", "labels": [], "entities": [{"text": "FIRE corpus", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.9366726279258728}, {"text": "Facebook chat corpus", "start_pos": 172, "end_pos": 192, "type": "DATASET", "confidence": 0.7715843121210734}]}, {"text": "However, if we check for words being Bengali first, the FIRE corpus' ambiguous Bengali words have a much higher chance of being correctly classified.", "labels": [], "entities": [{"text": "FIRE corpus' ambiguous Bengali words", "start_pos": 56, "end_pos": 92, "type": "DATASET", "confidence": 0.9253012895584106}]}, {"text": "This explains the varying effect order of checking has on corpora depending on the composition.", "labels": [], "entities": []}, {"text": "Thus, it is not really a viable method unless we know details of word composition.", "labels": [], "entities": [{"text": "word composition", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7208056449890137}]}, {"text": "Another phenomenon we noted was the effect the inclusion of each feature had on the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9986108541488647}]}, {"text": "Before using the n-gram feature in the predictor for the first half experiments, we observed the following: \u2022 Initially, we had only used regular dictionaries (the Samsada dictionary and Python Enchant) on the FIRE 2013 corpus and achieved an accuracy of 72.54% only (Bengali checks first).", "labels": [], "entities": [{"text": "FIRE 2013 corpus", "start_pos": 210, "end_pos": 226, "type": "DATASET", "confidence": 0.974469522635142}, {"text": "accuracy", "start_pos": 243, "end_pos": 251, "type": "METRIC", "confidence": 0.9991514682769775}]}, {"text": "This was our baseline accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9961192607879639}]}, {"text": "\u2022 After including our Bengali suffix repository, our accuracy increases to 75.51%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9996997117996216}]}, {"text": "\u2022 We include the checks by removing \"a\" from the ends of words in the Bengali dictionary to increment the accuracy to 77.37%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.999519944190979}]}, {"text": "\u2022 Finally, the accuracy increased with the n-gram feature to 86.27%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.999772846698761}]}, {"text": "The corrector method is also more useful with the less accurate predictor.", "labels": [], "entities": [{"text": "corrector", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9316431283950806}]}, {"text": "This is likely because several of the ambiguous words, which were wrongly classified in the predictor, get accurately classified by the corrector and there area lot more such words when we check words with the less ambiguous language first (English for FIRE and Bengali for Facebook; since words from the more ambiguous one get wrongly classified more).", "labels": [], "entities": [{"text": "FIRE", "start_pos": 253, "end_pos": 257, "type": "DATASET", "confidence": 0.7561132907867432}]}, {"text": "Thus, the corrector does a good job in such cases, giving accuracy boosts of +9.28% for the FIRE corpus and +10.62% for the Facebook corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9994497895240784}, {"text": "FIRE corpus", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.9633830785751343}, {"text": "Facebook corpus", "start_pos": 124, "end_pos": 139, "type": "DATASET", "confidence": 0.9431561529636383}]}, {"text": "However, it only marginally improves or reduces the accuracy in the reverse cases (-0.18% for FIRE and +0.39% for Facebook).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9993582367897034}, {"text": "FIRE", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.692374587059021}]}, {"text": "The second half experiments were performed using WEKA and its machine learning algorithms.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.9130193591117859}]}, {"text": "We used the following features of words as vectors: 1.", "labels": [], "entities": []}, {"text": "Presence in Bengali dictionary 2.", "labels": [], "entities": [{"text": "Presence", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8485715389251709}]}, {"text": "Presence in English dictionary 3.", "labels": [], "entities": []}, {"text": "N-gram profile language match 4.", "labels": [], "entities": []}, {"text": "Percentage of surrounding words that are \"predicted\" as Bengali: We predict using presence in one dictionary.", "labels": [], "entities": []}, {"text": "If the word is in both or neither, we use the n-gram match as the predicted language (S in the table).", "labels": [], "entities": []}, {"text": "We also use aversion of this using majority language in the neighborhood, rather than percentage ((s) in the table) Once we had these, we created arff (AttributeRelation File Format) files of the words with these features as vectors and then used various algorithms in WEKA to classify the words.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 269, "end_pos": 273, "type": "DATASET", "confidence": 0.9253510236740112}]}, {"text": "The key results are summarized in We have listed results using three different classifiers and then with no surrounding data (only dictionaries and n-gram), using binary surrounding data, and using only the N-gram language categorizer.", "labels": [], "entities": []}, {"text": "In terms of classifiers, IBk performs the best, giving us accuracies of 91.65% and 90.54% as seen in the  From the sixth row, it is evident that the Ngram categorization is very helpful, but not helpful enough.", "labels": [], "entities": [{"text": "IBk", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.7478229999542236}, {"text": "accuracies", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9973403811454773}]}, {"text": "The fourth row shows us the result from using the dictionaries and breaking ties (word present in both or neither) using the N-gram decision is rather successful, though adding the surrounding data definitely helps the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9985203146934509}]}, {"text": "The results of the majority language in the neighborhood as a feature are in the fifth row.", "labels": [], "entities": []}, {"text": "Interestingly, this technique caused a drop inaccuracy for the Facebook chat corpus, but increased the accuracy for the FIRE 2013 corpus.", "labels": [], "entities": [{"text": "Facebook chat corpus", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.8374204635620117}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9992749094963074}, {"text": "FIRE 2013 corpus", "start_pos": 120, "end_pos": 136, "type": "DATASET", "confidence": 0.9725888768831888}]}, {"text": "We did not use only the dictionaries as sole features, because the classification would have been 2-dimensional and much less useful than the dictionaries with N-gram.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details and statistics on test corpora used.", "labels": [], "entities": []}, {"text": " Table 2: Details and statistics on test corpora used.", "labels": [], "entities": []}, {"text": " Table 3: Results of first part. (BE: Bengali, then English; EB:", "labels": [], "entities": [{"text": "BE", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9970927238464355}, {"text": "EB", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9663053750991821}]}, {"text": " Table 4: Progressive results with features", "labels": [], "entities": []}, {"text": " Table 5: Results of second part (Machine Learning). Key: RF=Random Forest", "labels": [], "entities": [{"text": "RF", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9846973419189453}]}, {"text": " Table 6: Confusion matrix, FIRE(EB) {R:Real, C:Classified}", "labels": [], "entities": [{"text": "FIRE(EB)", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.8923519551753998}]}]}