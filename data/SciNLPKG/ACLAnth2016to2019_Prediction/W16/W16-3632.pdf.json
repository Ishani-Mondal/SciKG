{"title": [{"text": "Toward incremental dialogue act segmentation in fast-paced interactive dialogue systems", "labels": [], "entities": [{"text": "dialogue act segmentation", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.7229981422424316}]}], "abstractContent": [{"text": "In this paper, we present and evaluate an approach to incremental dialogue act (DA) segmentation and classification.", "labels": [], "entities": [{"text": "incremental dialogue act (DA) segmentation and classification", "start_pos": 54, "end_pos": 115, "type": "TASK", "confidence": 0.6758017010158963}]}, {"text": "Our approach utilizes prosodic, lexico-syntactic and contextual features, and achieves an encouraging level of performance in of-fline corpus-based evaluation as well as in simulated human-agent dialogues.", "labels": [], "entities": []}, {"text": "Our approach uses a pipeline of sequential processing steps, and we investigate the contribution of different processing steps to DA segmentation errors.", "labels": [], "entities": [{"text": "DA segmentation", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.926190584897995}]}, {"text": "We present our results using both existing and new metrics for DA segmentation.", "labels": [], "entities": [{"text": "DA segmentation", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.9798648655414581}]}, {"text": "The incremental DA seg-mentation capability described here may help future systems to allow more natural speech from users and enable more natural patterns of interaction.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we explore the feasibility of incorporating an incremental dialogue act segmentation capability into an implemented, high-performance spoken dialogue agent that plays a time-constrained image-matching game with its users (.", "labels": [], "entities": [{"text": "dialogue act segmentation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7309124867121378}]}, {"text": "This work is part of a longer-term research program that aims to use incremental (word-byword) language processing techniques to enable dialogue agents to support efficient, fast-paced interactions with a natural conversational style).", "labels": [], "entities": []}, {"text": "It's important to allow users to speak naturally to spoken dialogue systems.", "labels": [], "entities": []}, {"text": "It has been understood for sometime that this ultimately requires a system to be able to automatically segment a user's speech into meaningful units in real-time while they speak ().", "labels": [], "entities": []}, {"text": "Still, most current systems use relatively simple and limited approaches to this segmentation problem.", "labels": [], "entities": []}, {"text": "For example, in many systems, it's assumed that pauses in the user's speech can be used to determine the segmentation, often by treating each detected pause as indicating a dialogue act (DA) boundary (.", "labels": [], "entities": []}, {"text": "While easily implemented, such a pause-based design has several problems.", "labels": [], "entities": []}, {"text": "First, a substantial number of spoken DAs contain internal pauses (, as in I need a car in...", "labels": [], "entities": []}, {"text": "Using simple pause length thresholds to join certain speech segments together for interpretation is not a very effective remedy for this problem ().", "labels": [], "entities": [{"text": "interpretation", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.9697601795196533}]}, {"text": "More sophisticated approaches train algorithms to join speech across pauses ( or decide which pauses constitute endof-utterances that should trigger interpretation (e.g. ().", "labels": [], "entities": []}, {"text": "This addresses the problem of DA-internal pauses, but it does not address the second problem with pause-based designs, which is that it's also common fora continuous segment of user speech to include multiple DAs without intervening pauses, as in Sure that's fine can you call when you get to the gate?", "labels": [], "entities": []}, {"text": "A third problem is that waiting fora pause to occur before interpreting earlier speech may increase latency and erode the user experience.", "labels": [], "entities": [{"text": "interpreting earlier speech", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.8793571790059408}]}, {"text": "Together, these problems suggest the need for an incremental dialogue act segmentation capability in which a continuous stream of captured user speech, including the intermittent pauses therein, is incrementally segmented into appropriate DA units for interpretation.", "labels": [], "entities": [{"text": "dialogue act segmentation", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6417371332645416}]}, {"text": "In this paper, we present a case study of implementing an incremental DA segmentation capability for an image-matching game called RDGImage, illustrated in.", "labels": [], "entities": [{"text": "DA segmentation", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.8414705693721771}]}, {"text": "In this game, two players converse freely in order to identify a spe- cific target image on the screen (outlined in red).", "labels": [], "entities": []}, {"text": "When played by human players, as in, the game creates a variety of fast-paced interaction patterns, such as question-answer exchanges.", "labels": [], "entities": []}, {"text": "Our motivation is to eventually enable a future version of our automated RDG-Image agent to participate in the most common interaction patterns in human-human gameplay.", "labels": [], "entities": []}, {"text": "For example, in, two fast-paced question-answer exchanges arise as the director Dis describing the target image.", "labels": [], "entities": []}, {"text": "In the first, the matcher M asks brown...brown seat? and receives an almost immediate answer brown seat yup.", "labels": [], "entities": []}, {"text": "A moment later, the director continues the description with and handles got it?, both adding and handles and also asking got it?", "labels": [], "entities": []}, {"text": "We believe that an important step toward automating such fast-paced exchanges is to create an ability for an automated agent to incrementally recognize the various DAs, such as yes-no questions (Q-YN), target descriptions (D-T), and yes answers (A-Y) in real-time as they are happening.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "First, we define a sequential approach to incremental DA segmentation and classification that is straightforward to implement and which achieves a useful level of performance when trained on a small annotated corpus of domain-specific DAs.", "labels": [], "entities": [{"text": "DA segmentation and classification", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.876587837934494}]}, {"text": "Second, we explore the performance of our approach using both existing and new performance metrics for DA segmentation.", "labels": [], "entities": [{"text": "DA segmentation", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.9787904918193817}]}, {"text": "Our new metrics emphasize the importance of precision and recall of specific DA types, independently of DA boundaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9991023540496826}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9983698725700378}]}, {"text": "These metrics are useful for evaluating DA segmenters that operate on noisy ASR output and which are intended for use in systems whose dialogue policies are defined in terms of the presence or absence of specific DA types, independently of their position in user speech.", "labels": [], "entities": [{"text": "DA segmenters", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9220554232597351}]}, {"text": "This is abroad class of systems.", "labels": [], "entities": []}, {"text": "Third, while much of the prior work on DA segmentation has been corpus-based, we report hereon an initial integration of our incremental DA segmenter into an implemented, high-performance agent for the RDG-Image game.", "labels": [], "entities": [{"text": "DA segmentation", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.9650782942771912}]}, {"text": "Our case study suggests that incremental DA segmentation can be performed with sufficient accuracy for us to begin to extend our baseline agent's conversational abilities without significantly degrading its current performance.", "labels": [], "entities": [{"text": "DA segmentation", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.9252575039863586}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9974157810211182}]}], "datasetContent": [{"text": "We report on two experiments.", "labels": [], "entities": []}, {"text": "In the first experiment, we train our DA segmentation pipeline using the annotated corpus of Section 3.1 and report results on the observed DA segment boundaries (Section 5.1) and DA class labels (Section 5.2).", "labels": [], "entities": [{"text": "DA segmentation pipeline", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.9096139073371887}]}, {"text": "In the second experiment, presented in Section 5.3, we report on a policy simulation that investigates the effect of our incremental DA segmentation pipeline on a baseline automated agent's performance.", "labels": [], "entities": [{"text": "DA segmentation", "start_pos": 133, "end_pos": 148, "type": "TASK", "confidence": 0.817434698343277}]}, {"text": "For the first experiment, we use a hold-one-pairout cross-validation setup where, for each fold, the dialogue between one pair of players is held out for testing, while automated models are trained on the other pairs.", "labels": [], "entities": []}, {"text": "To evaluate our pipeline, we use four data conditions, summarized in, that represent increasing amounts of automation in the pipeline.", "labels": [], "entities": []}, {"text": "These conditions allow us to better understand the sources for observed errors in segment boundaries and/or DA labels.", "labels": [], "entities": []}, {"text": "Our notation for these conditions is a compact encoding of the data sources used to create the transcripts of user speech, the segment boundaries, and the DA labels.", "labels": [], "entities": []}, {"text": "Our reference annotation, described in Section 3.1, is notated HT-HS-HD (human transcript, human segment boundaries, human DA labels).", "labels": [], "entities": []}, {"text": "Example segmentations for each condition are in.", "labels": [], "entities": []}, {"text": "In this evaluation, we ignore DA labels and look only at the identification of DA boundaries (notated by in, and encoded using B and I tags in our segmenter).", "labels": [], "entities": []}, {"text": "For this evaluation, we use human   transcripts and compare the boundaries in our reference annotations (HT-HS-HD) to the boundaries inferred by our automated pipeline (HT-AS-AD).", "labels": [], "entities": []}, {"text": "In, we present results for versions of our pipeline that use three different feature sets: only prosody features (I), only lexico-syntactic and contextual features (II), and both (I+II).", "labels": [], "entities": []}, {"text": "We include also a simple 1-DA-per-IPU baseline that assumes each IPU is a single complete DA; it assigns the first word in each IPU a B tag and subsequent words an I tag.", "labels": [], "entities": []}, {"text": "Finally, we also include numbers based on an independent human annotator using the subset of our annotated corpus that was annotated by two human annotators.", "labels": [], "entities": []}, {"text": "For this subset, we use our main annotator as the reference standard and evaluate the other annotator as if their annotation were a system's hypothesis.", "labels": [], "entities": []}, {"text": "The reported numbers include word-level accuracy of the B and I tags, F-score for each of the B and I tags, and the DA segmentation error rate (DSER) metric of.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9188412427902222}, {"text": "F-score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9973588585853577}, {"text": "DA segmentation error rate (DSER) metric", "start_pos": 116, "end_pos": 156, "type": "METRIC", "confidence": 0.8366096094250679}]}, {"text": "DSER measures the fraction of reference DAs whose left and right boundaries are not exactly replicated in the hypothesis.", "labels": [], "entities": [{"text": "DSER", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.47876685857772827}]}, {"text": "For example, in Table 4, the reference (a) contains three DAs, but only the boundaries of the second DA (it's the blue frame) are exactly replicated in hypothesis (c).", "labels": [], "entities": []}, {"text": "This yields a DSER of 2/3 for this example.", "labels": [], "entities": [{"text": "DSER", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9772623777389526}]}, {"text": "We find that our automated pipeline (HT-AS-AD) with all features performs the best among the pipeline methods, with word-level accuracy of 0.91 and DSER of 0.30.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.8823555707931519}, {"text": "DSER", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9960810542106628}]}, {"text": "Its performance how- We evaluate our DA segmentation performance using human transcripts, rather than ASR, as this allows a simple direct comparison of inferred DA boundaries.", "labels": [], "entities": [{"text": "DA segmentation", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.9708152413368225}]}, {"text": "8 For comparison, the chance-corrected kappa value for word-level boundaries is 0.92; see Section 3.1.", "labels": [], "entities": []}, {"text": "In this evaluation, we consider DA labels assigned to recognized DA segments using several types of metrics.", "labels": [], "entities": [{"text": "DA labels assigned to recognized DA segments", "start_pos": 32, "end_pos": 76, "type": "TASK", "confidence": 0.8206402659416199}]}, {"text": "We summarize our results in.", "labels": [], "entities": []}, {"text": "Metrics used for human transcripts We first compare our reference annotations (HT-HS-HD) to the performance of our automated pipeline when provided human transcripts as input.", "labels": [], "entities": []}, {"text": "For this comparison, we use three error rate metrics (Lenient, Strict, and DER) from the DA segmentation literature that are intuitively applied when the token sequence being segmented and labeled is identical (or at least isomorphic) to the annotated token sequence.", "labels": [], "entities": [{"text": "DER", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9977849125862122}, {"text": "DA segmentation", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.8322469890117645}]}, {"text": "Lower is better for these.", "labels": [], "entities": []}, {"text": "The Lenient and Strict metrics () are based on the DA labels assigned to each individual word (by way of the label of the DA segment that contains that word).", "labels": [], "entities": []}, {"text": "Lenient is a per-token DA label error rate that ignores DA segment boundaries.", "labels": [], "entities": []}, {"text": "In Table 6, this error rate is 0.09 when human-annotated boundaries are fed into our DA classifier (HT-HS-AD) and 0.15 when automatically-identified boundaries are used (HT-AS-AD).", "labels": [], "entities": [{"text": "error rate", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9790087342262268}]}, {"text": "Strict and DER are boundary-sensitive metrics.", "labels": [], "entities": [{"text": "DER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9822555780410767}]}, {"text": "Strict is a per-token error rate that requires each token to receive the correct DA label and also to be part of a DA segment whose exact boundaries appear in the reference annotation.", "labels": [], "entities": []}, {"text": "This is a much higher standard.", "labels": [], "entities": []}, {"text": "Dialogue Act Error Rate (DER) () is the fraction of reference DAs whose left and right boundaries and label are perfectly replicated in the hypothesis.", "labels": [], "entities": [{"text": "Dialogue Act Error Rate (DER)", "start_pos": 0, "end_pos": 29, "type": "METRIC", "confidence": 0.6869168324129922}]}, {"text": "While the reported boundary-sensitive error rate numbers (0.38 and 0.72) may appear to be high, many of these boundary errors maybe relatively innocuous from a system standpoint.", "labels": [], "entities": [{"text": "boundary-sensitive error rate", "start_pos": 19, "end_pos": 48, "type": "METRIC", "confidence": 0.671169638633728}]}, {"text": "We return to this below.", "labels": [], "entities": []}, {"text": "Alignment-based metrics We also report two additional metrics that are intuitively applied even when the word sequence being segmented and classified is only a noisy approximation to the word sequence that was annotated, i.e. under an ASR condition such as AT-AS-AD.", "labels": [], "entities": []}, {"text": "The Concept Error Rate (CER) is a word error rate (WER) calculation) based on a minimum edit distance alignment of the DA tags (using one DA tag per DA segment).", "labels": [], "entities": [{"text": "Concept Error Rate (CER)", "start_pos": 4, "end_pos": 28, "type": "METRIC", "confidence": 0.7878330498933792}, {"text": "word error rate (WER)", "start_pos": 34, "end_pos": 55, "type": "METRIC", "confidence": 0.8377739836772283}]}, {"text": "Our fully automated pipeline (AT-AS-AD) has a CER of 0.52.", "labels": [], "entities": [{"text": "CER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9986503720283508}]}, {"text": "We also report an analogous word-level metric which we call 'Levenshtein-Lenient'.", "labels": [], "entities": []}, {"text": "To our knowledge this metric has not previously been used in the literature.", "labels": [], "entities": []}, {"text": "It replaces each word in the reference and hypothesis with the DA tag that applies to it, and then computes a WER on the DA tag sequence.", "labels": [], "entities": [{"text": "WER", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9971233010292053}]}, {"text": "It is thus a Lenient-like metric that can be applied to DA segmentation based on ASR results.", "labels": [], "entities": [{"text": "DA segmentation", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.9437715411186218}, {"text": "ASR", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9351263642311096}]}, {"text": "Our automated pipeline (AT-AS-AD) scores 0.39.", "labels": [], "entities": [{"text": "AT-AS-AD)", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.8351957201957703}]}, {"text": "DA multiset precision and recall metrics When ASR is used, the CER and LevenshteinLenient metrics give an indication of how well you are doing at replicating the ordered sequence of DA tags.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9503808617591858}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9972254037857056}, {"text": "ASR", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.8241653442382812}, {"text": "CER", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9355862736701965}]}, {"text": "But in building a system, sometimes the sequence is less of a concern, and what is desired is a breakdown in terms of precision and recall per DA tag.", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9993539452552795}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9989392161369324}]}, {"text": "Many dialogue systems use policies that are triggered when a certain DA type has occurred in the user's speech (such as an agent that processes yes (A-Y) or no (A-N) answers differently, or a di-9 E.g. in (c), the only Lenient error is at word um.", "labels": [], "entities": []}, {"text": "10 E.g. in (c), only the four words it's the blue frame would count as non-errors on the Strict standard.: DA multiset precision and recall metrics fora sample of higher-frequency DA tags.", "labels": [], "entities": [{"text": "Strict standard.", "start_pos": 89, "end_pos": 105, "type": "DATASET", "confidence": 0.7101451456546783}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.749401867389679}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9974308609962463}]}, {"text": "rector agent for the RDG-Image game that moves on when the matcher performs As-I (\"got it\")).", "labels": [], "entities": []}, {"text": "For such systems, exact DA boundaries and even the order of DAs is not of paramount importance so long as a correct DA label is produced around the time the user performs the DA.", "labels": [], "entities": []}, {"text": "We therefore define a more permissive measure that looks only at precision and recall of DA labels within a sample of user speech.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9989014863967896}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9981274008750916}]}, {"text": "As an example, in (a) in, there is one A-N label and two D-T labels.", "labels": [], "entities": []}, {"text": "In (d), there are two A-N labels and 3 D-T labels.", "labels": [], "entities": []}, {"text": "Ignoring boundaries, we can represent as a multiset the collection of DA labels in a reference A or hypothesis H, and compute standard multiset versions of precision and recall for each DA type.", "labels": [], "entities": [{"text": "precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.998426079750061}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9970398545265198}]}, {"text": "For reference, a formal definition of multiset precision P (DA i ) and recall R(DA i ) for DA type DA i is provided in the appendix.", "labels": [], "entities": [{"text": "recall R(DA i )", "start_pos": 71, "end_pos": 86, "type": "METRIC", "confidence": 0.9486127297083536}]}, {"text": "We report these numbers for our most common DA types in.", "labels": [], "entities": []}, {"text": "Here, we continue to use the speech of one speaker during a target image subdialogue as the unit of analysis.", "labels": [], "entities": []}, {"text": "The data show that precision and recall generally decline for all DA types as automation increases in the conditions from left to right.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9996594190597534}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9996402263641357}, {"text": "automation", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9951893091201782}]}, {"text": "We do relatively well with the most frequent DA types, which are D-T and As-I.", "labels": [], "entities": []}, {"text": "A particular challenge, even inhuman transcript+segment condition HT-HS-AD, is the DA tag PFB.", "labels": [], "entities": [{"text": "DA tag PFB", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.6265018880367279}]}, {"text": "Ina manual analysis of common error types, we found that the different DA labels used for very short utterances like 'okay' (D-M, PFB, As-I) and 'yeah' (A-Y, PFB, As-I) are often confused.", "labels": [], "entities": []}, {"text": "We believe this type of error could be reduced through a combination of improved features, collapsed DA categories, and more detailed annotation guidelines.", "labels": [], "entities": [{"text": "error", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.8411017656326294}]}, {"text": "ASR errors also often cause DA errors; see e.g.: Overall performance of the eavesdropper simulation on the unsegmented data (All DAs) and the automatically segmented data (Only D-T) identified with our pipeline (AT-AS-AD).", "labels": [], "entities": [{"text": "ASR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9591466188430786}]}, {"text": "In prior work (, we developed an automated agent called Eve which plays the matcher role in the RDG-Image game and has been evaluated in a live interactive study with 125 human users.", "labels": [], "entities": []}, {"text": "Our prior work underscored the critical importance of pervasive incremental processing in order for Eve to achieve her highest performance in terms of points scored and also the best subjective user impressions.", "labels": [], "entities": []}, {"text": "In this second experiment, we perform an offline investigation into the potential impact on our agent's image-matching performance if we integrate the incremental DA segmentation pipeline from this paper.", "labels": [], "entities": [{"text": "DA segmentation", "start_pos": 163, "end_pos": 178, "type": "TASK", "confidence": 0.8466879725456238}]}, {"text": "We take the \"fully-incremental\" version of Eve from as our baseline agent in this experiment.", "labels": [], "entities": []}, {"text": "Briefly, this version of Eve includes the same incremental ASR used in our new DA segmentation pipeline, incremental language understanding to identify the target image (Naive Bayes classification), and an incremental dialogue policy that uses parameterized rules.", "labels": [], "entities": [{"text": "ASR", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.7662361860275269}, {"text": "DA segmentation pipeline", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.9300191005071005}]}, {"text": "The baseline agent's design focuses on the most common DA types in our RDG-Image corpora: D-T for the director (constituting 60% of director DAs), and As-I for the matcher (constituting 46% of matcher DAs).", "labels": [], "entities": []}, {"text": "Effectively, the baseline agent assumes every word the user says is describing the target, and uses an optimized policy to decide the right moment to commit to a selection (As-I) or ask the user to skip the image (As-S).", "labels": [], "entities": []}, {"text": "Eve's typical interaction pattern is illustrated in.", "labels": [], "entities": []}, {"text": "This experiment is narrowly focused on the impact of using the pipeline to segment out only the D-T DAs and to use only the words from detected D-Ts in the target image classifier and the agent's policy decisions.", "labels": [], "entities": []}, {"text": "Changing the agent pipeline from using the director's full utterance towards only taking the D-T tagged words into account could po- tentially have a negative impact on the baseline agent's performance.", "labels": [], "entities": []}, {"text": "For example, for the fully automated condition AT-AS-AD in, D-T has precision 0.79 and recall 0.88.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9989303946495056}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9992291927337646}]}, {"text": "The 0.88 recall suggests that some D-T words will be lost (in false negative D-Ts) by integrating the new DA segmenter.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9792910218238831}]}, {"text": "Additionally, as shown in, the recognized words and whether they are tagged as D-T can change dynamically as new incremental ASR results arrive, and this instability could undermine some of the advantage of segmentation.", "labels": [], "entities": [{"text": "ASR", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9778419733047485}]}, {"text": "On the other hand, by excluding non-D-T text from consideration, there is a potential to decrease noise in the agent's understanding and improve the agent's accuracy or speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9985299110412598}, {"text": "speed", "start_pos": 169, "end_pos": 174, "type": "METRIC", "confidence": 0.8557736277580261}]}, {"text": "As an initial investigation into the issues described above, we adopt the \"Eavesdropper\" framework for policy simulation and training detailed in.", "labels": [], "entities": []}, {"text": "In an Eavesdropper simulation, the director's speech from pre-recorded target image dialogues is provided to the agent, and the agent simulates alternative policy decisions as if it were in the matcher role.", "labels": [], "entities": []}, {"text": "We have found that higher cross-validation performance in these offline simulations has translated to higher performance in live interactive human-agent studies.", "labels": [], "entities": []}, {"text": "We created a modified version of our agent that uses the fully automated pipeline (AT-AS-AD) to pass only word sequences tagged as D-T to the agent's language understanding component (a target image classifier), effectively ignoring other DA types.", "labels": [], "entities": [{"text": "AT-AS-AD", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9156978130340576}]}, {"text": "Tagging is performed every 100 ms on each new incremental output segment published by the ASR.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9710038304328918}]}, {"text": "We then compare the performance of our baseline and modified agent in a cross-validation setup, using an Eavesdropper simulation to train and test the agents.", "labels": [], "entities": []}, {"text": "summarizes the observed performance in these simulations for four sample image sets in the two agent conditions.", "labels": [], "entities": []}, {"text": "All results are calculated based on leave-one-user-out training and a policy optimized on points per second.", "labels": [], "entities": []}, {"text": "A WilcoxonMann-Whitney Test on all 18 image sets indicated that, between the two conditions, there is no signif- These encouraging results suggest that our incremental DA segmenter achieves a performance level that is sufficient for it to be integrated into our agent, enabling the incremental segmentation of other DA types without significantly compromising (or improving) the agent's current performance level.", "labels": [], "entities": [{"text": "DA segmenter", "start_pos": 168, "end_pos": 180, "type": "TASK", "confidence": 0.7570318877696991}]}, {"text": "These results provide a complementary perspective on the various DA classification metrics reported in Section 5.2.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.9226237833499908}]}, {"text": "The current baseline agent ( can only generate As-I and As-S dialogue acts.", "labels": [], "entities": []}, {"text": "In future work, the fully automated pipeline presented here will enable us to expand the agent's dialogue policies to support additional patterns of interaction beyond its current skillset.", "labels": [], "entities": []}, {"text": "For example, the agent would be better able to understand and react to a multi-DA user utterance like and handles got it?", "labels": [], "entities": []}, {"text": "By segmenting out and understanding the Q-YN got it?, the agent would be able to detect the question and answer with an A-Y like yeah.", "labels": [], "entities": []}, {"text": "Overall, we believe the ability to understand the natural range of director's utterances will help the agent to create more natural interaction patterns, which might receive a better subjective rating by the human dialogue partner and in the end might even achieve a better overall game performance, as ambiguities can be resolved quicker and the flow of communication can be more efficient.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of annotated DA types, DA boundaries (), and IPU boundaries (...). The number of  IPUs and DAs in each example are indicated.", "labels": [], "entities": []}, {"text": " Table 2: The distribution in the number of DAs  whose first word is within a speech segment.", "labels": [], "entities": []}, {"text": " Table 4: Examples of DA boundaries () and DA labels in each condition.", "labels": [], "entities": []}, {"text": " Table 6: Observed DA classification and joint seg- mentation+classification performance.", "labels": [], "entities": [{"text": "Observed", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9910647869110107}, {"text": "DA classification", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7463873028755188}]}, {"text": " Table 7: DA multiset precision and recall metrics  for a sample of higher-frequency DA tags.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9379196763038635}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9972712397575378}]}, {"text": " Table 8: Overall performance of the eavesdropper simulation on the unsegmented data (All DAs) and the  automatically segmented data (Only D-T) identified with our pipeline (AT-AS-AD).", "labels": [], "entities": []}]}