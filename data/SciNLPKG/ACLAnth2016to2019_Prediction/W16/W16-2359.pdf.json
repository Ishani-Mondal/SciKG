{"title": [], "abstractContent": [{"text": "We present a doubly-attentive multimodal machine translation model.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.6456156770388285}]}, {"text": "Our model learns to attend to source language and spatial-preserving CONV 5,4 visual features as separate attention mechanisms in a neural translation model.", "labels": [], "entities": []}, {"text": "In image description translation experiments (Task 1), we find an improvement of 2.3 Meteor points compared to initialising the hidden state of the decoder with only the FC 7 features and 2.9 Meteor points compared to a text-only neural machine translation base-line, confirming the useful nature of attending to the CONV 5,4 features.", "labels": [], "entities": [{"text": "image description translation", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.7764608065287272}, {"text": "CONV 5,4", "start_pos": 317, "end_pos": 325, "type": "DATASET", "confidence": 0.9043154716491699}]}], "introductionContent": [{"text": "Our system learns to translate image descriptions using both the source language descriptions and the images.", "labels": [], "entities": []}, {"text": "We integrate an attention-based neural network for machine translation and image description in a unified model, in which two separate attention mechanisms operate over the language and visual modalities.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7520301342010498}, {"text": "image description", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7100415676832199}]}, {"text": "We believe that this is a principled approach to learning which source words and which areas of the image to attend to when generating words in the target description.", "labels": [], "entities": []}, {"text": "We are inspired by recent successes in using attentive models in both neural machine translation (NMT) and neural image description.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 70, "end_pos": 102, "type": "TASK", "confidence": 0.8361476560433706}, {"text": "neural image description", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.6288303037484487}]}, {"text": "Originally, in non-attentive NMT models, the entire source sentence is encoded into a single vector which is in turn used by the decoder to generate a translation ().", "labels": [], "entities": []}, {"text": "Ina similar vein, image description models can use a vector encoding the image as input for the description generation process (.", "labels": [], "entities": [{"text": "image description", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7300442159175873}]}, {"text": "first proposed a NMT model with an attention mechanism over the source sentence.", "labels": [], "entities": []}, {"text": "Their model is trained so that the decoder learns to attend to words in the source sentence when translating each token in the target sentence.", "labels": [], "entities": []}, {"text": "introduced a similar attention-based neural image description model.", "labels": [], "entities": [{"text": "attention-based neural image description", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.6295765340328217}]}, {"text": "In this case, the attention mechanism learns which parts of the image to attend to while generating words in the description.", "labels": [], "entities": []}, {"text": "When translating image descriptions, given both the source description and the source image (i.e., the setting for Task 1), we believe that both modalities can provide cues for generating the target language description.", "labels": [], "entities": []}, {"text": "The source description provides the content for translation, but in cases where this maybe ambiguous, the image features can provide contextual disambiguation.", "labels": [], "entities": [{"text": "translation", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.981813907623291}]}, {"text": "The system we propose is a first step towards integrating both modalities using attention mechanisms.", "labels": [], "entities": []}, {"text": "Previous work has demonstrated the plausibility of multilingual multimodal natural language processing.", "labels": [], "entities": [{"text": "multilingual multimodal natural language processing", "start_pos": 51, "end_pos": 102, "type": "TASK", "confidence": 0.6068760275840759}]}, {"text": "showed how to generate descriptions of images in English and German by learning and transferring features between independent neural image description models.", "labels": [], "entities": []}, {"text": "In comparison, our approach is a single end-to-end model over the source and target languages with attention mechanisms over both the source language and the visual features.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for our models on Task 1. We  find that attending over the source language and  CONV 5,4 visual features is better than not using  image features (text-only, attentive NMT model)  and also just initialising an attention-based de- coder with FC 7 features.", "labels": [], "entities": [{"text": "CONV 5,4 visual", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.8668503761291504}]}]}