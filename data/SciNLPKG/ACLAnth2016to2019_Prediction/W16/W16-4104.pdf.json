{"title": [{"text": "Addressing surprisal deficiencies in reading time models", "labels": [], "entities": []}], "abstractContent": [{"text": "This study demonstrates a weakness in how n-gram and PCFG surprisal are used to predict reading times in eye-tracking data.", "labels": [], "entities": []}, {"text": "In particular, the information conveyed by words skipped during saccades is not usually included in the surprisal measures.", "labels": [], "entities": []}, {"text": "This study shows that correcting the surprisal calculation improves n-gram surprisal and that upcoming n-grams affect reading times, replicating previous findings of how lexical frequencies affect reading times.", "labels": [], "entities": []}, {"text": "In contrast, the predictivity of PCFG surprisal does not benefit from the surprisal correction despite the fact that lexical sequences skipped by saccades are processed by readers, as demonstrated by the corrected n-gram measure.", "labels": [], "entities": []}, {"text": "These results raise questions about the formulation of information-theoretic measures of syntactic processing such as PCFG surprisal and entropy reduction when applied to reading times.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rare words and constructions produce longer reading times than their more frequent counterparts.", "labels": [], "entities": []}, {"text": "Such effects can be captured by n-grams and by probabilistic context-free grammar (PCFG) surprisal.", "labels": [], "entities": []}, {"text": "Surprisal theory predicts reading times will be directly proportional to the amount of information which must be processed, as calculated by a generative model, but the surprisal measures commonly used in eye-tracking studies omit probability estimates for words skipped in saccades.", "labels": [], "entities": []}, {"text": "Therefore, the generative model assumed by those studies does not account for the information contributed by the skipped words even though those words must be processed by readers.", "labels": [], "entities": []}, {"text": "1 This deficiency can be addressed by summing surprisal measures over the saccade region (see), and the resulting cumulative n-grams have been shown to be more predictive of reading times than the usual non-cumulative n-grams (van.", "labels": [], "entities": []}, {"text": "However PCFG surprisal, which has a similar deficiency when non-cumulatively modeling reading times, has not previously been found to be predictive when accumulated over saccade regions.", "labels": [], "entities": []}, {"text": "This paper uses a reading time corpus to investigate two accumulation techniques (pre-and post-saccade) and finds that both forms of accumulation improve the fit of n-gram surprisal to reading times.", "labels": [], "entities": []}, {"text": "However, even though accumulated n-grams demonstrate that the lexical sequence of the saccade region is processed, PCFG surprisal does not seem to be improved by either accumulation technique.", "labels": [], "entities": []}, {"text": "The results of this work call into question the usual formulation of PCFG surprisal as a reading time predictor and suggest future directions for investigation of the influence of upcoming material on reading times.", "labels": [], "entities": []}, {"text": "The present work merely accounts for the processing load introduced by the words initially skipped by a progressive saccade.", "labels": [], "entities": []}, {"text": "This correction is consistent with any process by which those words could be processed: predictive processing, parafoveal processing, or subsequent regression.", "labels": [], "entities": [{"text": "predictive processing", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.9173383712768555}]}, {"text": "Since all of those methods would contribute load during the associated duration (e.g,.", "labels": [], "entities": []}, {"text": "first pass time), reading time predictivity should improve if the complexity metrics account for the additional load., while traditional n-gram measures are conditioned on the preceding adjacent context, which is never generated by the typical surprisal models used in eye-tracking studies.", "labels": [], "entities": [{"text": "reading time predictivity", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.5994386076927185}]}, {"text": "Cumulative n-grams sum the n-gram measures over the entire skipped region in order to better capture the information that readers need to process.", "labels": [], "entities": []}], "datasetContent": [{"text": "3.1 Cumulative n-gram surprisal N -gram surprisal is conditioned on the preceding context (see Equation 1).", "labels": [], "entities": []}, {"text": "As stated in the introduction, however, direct use of this factor in a reading time model ignores the fact that some or all of the preceding context may not be generated if the associated lexical targets were not previously fixated by readers (see).", "labels": [], "entities": []}, {"text": "The lack of a generated condition results in a probability model that does not reflect the influence of words skipped during saccades.", "labels": [], "entities": []}, {"text": "This deficiency can be corrected by accumulating n-gram surprisal over the entire saccade region (see Equation 2).", "labels": [], "entities": []}, {"text": "n-gram(w, where w is a vector of input tokens, f t\u22121 is the index of the previous fixation, ft is the index of the current fixation.", "labels": [], "entities": []}, {"text": "The linear mixed model 3 that was used in this experiment included item, subject, and sentence ID-crossed-with-subject random intercepts 4 as well as by-subject random slopes and fixed effects for the following predictors: sentence position (sentpos), word length (wlen), region length (rlen), 5 whether the previous word was fixated (prevfix), 5-grams and cumulative 5-grams.", "labels": [], "entities": [{"text": "region length (rlen)", "start_pos": 272, "end_pos": 292, "type": "METRIC", "confidence": 0.8498865842819214}]}, {"text": "Likelihood ratio tests were used to compare the mixed model with and without fixed effects for the 5-gram measures (see).", "labels": [], "entities": [{"text": "Likelihood ratio", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.8523848354816437}]}, {"text": "In line with previous findings on the Dundee corpus (van, cumulative 5-grams provide a significant improvement over basic n-grams (p < 0.001), but unlike previous work, basic n-grams do not improve over cumulative n-grams on this corpus (p > 0.05).", "labels": [], "entities": [{"text": "Dundee corpus", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9943113625049591}]}, {"text": "The benefit of cumulative n-grams suggests that the lexical processing of words skipped during a saccade has a time cost similar to directly fixated words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Goodness of fit of n-gram models to reading times in the UCL corpus. Significance  testing was performed between each model and the models in the section above it. Significance  for the Base+Both model applies to improvement over its Base+Basic model.  *  p < .001", "labels": [], "entities": [{"text": "UCL corpus", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.9909069538116455}]}, {"text": " Table 2: Goodness of fit of future n-grams and future surprisal to reading times. Significance  testing was performed between each model and the models in the section above it. Signifi- cance for the Base+Both model applies to improvement over the Base+Future-PCFG model.   *  p < 0.001", "labels": [], "entities": [{"text": "Significance", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.9489060044288635}]}]}