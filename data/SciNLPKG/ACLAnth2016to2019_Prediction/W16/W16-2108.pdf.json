{"title": [{"text": "Deriving Players & Themes in the Regesta Imperii using SVMs and Neural Networks", "labels": [], "entities": [{"text": "Regesta Imperii", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.9328559637069702}]}], "abstractContent": [{"text": "The Regesta Imperii (RI) are an important source for research in European-medieval history.", "labels": [], "entities": [{"text": "Regesta Imperii (RI)", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.772721779346466}]}, {"text": "Sources spread over many centuries of medieval history-mainly charters of German-Roman Emperors-are summarized as \"Regests\" and pooled in the RI.", "labels": [], "entities": [{"text": "RI", "start_pos": 142, "end_pos": 144, "type": "DATASET", "confidence": 0.9470429420471191}]}, {"text": "Interesting medieval demographic groups and players are i.a. cities, citizens or spiritual institutions (e.g. bishops or monasteries).", "labels": [], "entities": []}, {"text": "Themes of historical interest are i.a. peace and war or the endowment of new privileges.", "labels": [], "entities": []}, {"text": "We investigate the RI for important players and themes, applying state-of-the-art text classification methods from computational linguistics.", "labels": [], "entities": [{"text": "RI", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9577776193618774}, {"text": "text classification", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7506217062473297}]}, {"text": "We examine the performance of different classification methods in view of the linguistically very heterogeneous RI, including a Neural Network approach that is designed to capture complex interactions between players and themes.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Regesta Imperii (RI) 1 are considered a fundamental, autonomous source for German and European history.", "labels": [], "entities": [{"text": "Regesta Imperii (RI) 1", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.7780426939328512}]}, {"text": "It extends over many centuries, from the Karolinger dynasty to Maximilian I, from around 800 to 1500 AD.", "labels": [], "entities": []}, {"text": "The RI have their roots in the 19th century, when the German librarian Johann Friedrich B\u00f6hmer started to collect and document the charters (including known and possibly unknown fakes) of the German-Roman emperors, in terms of so-called Regests.", "labels": [], "entities": [{"text": "RI", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.6106555461883545}]}, {"text": "The Regests contain relevant judicial content of the referenced charters (cf.,,  Regests across time.", "labels": [], "entities": []}, {"text": "Others: ratios of Regests, in which the terms \"Friedrich II.\"", "labels": [], "entities": [{"text": "ratios", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9349275827407837}, {"text": "Regests", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.6285381317138672}]}, {"text": "(triangles) and \"Friedrich III.\"", "labels": [], "entities": [{"text": "Friedrich III.\"", "start_pos": 17, "end_pos": 32, "type": "DATASET", "confidence": 0.9751185327768326}]}, {"text": "The names of these German-Roman kings are examples for concepts which are rather confined in time in the RI. was created, for example, when an emperor decided to give a land grant, or privileges such as new rights to one of his landlords or cities.", "labels": [], "entities": [{"text": "RI.", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.5510168075561523}]}, {"text": "Covering about 13 million tokens, the RI constitutes a large-scale resource that is still growing today 2 . The 129,504 Regests we have access to can be treated as a collection of corpora (e.g., one corpus for each Roman-German emperor dynasty), or as a single corpus covering all collected materials.", "labels": [], "entities": [{"text": "RI", "start_pos": 38, "end_pos": 40, "type": "DATASET", "confidence": 0.7146751880645752}]}, {"text": "Our work treats the RI as a single corpus.", "labels": [], "entities": []}, {"text": "The RI comprises texts written in different German varieties, as well as Latin.", "labels": [], "entities": []}, {"text": "Often we find up to three different languages or varieties within a single Regest.", "labels": [], "entities": []}, {"text": "As seen in, the Regests are not evenly distributed overtime but have the greatest mass from about 1200 to 1500 AD.", "labels": [], "entities": [{"text": "Regests", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.8810694217681885}]}, {"text": "Many terms and concepts only occur in certain times.", "labels": [], "entities": []}, {"text": "An overview # Regests 129,504 # types \u2248 407,000 # tokens \u2248 13,000,000 mean length (in tokens) \u2248 85 median length (in tokens) \u2248 52 ttr log \u2248 0.79 ttr log SDeWaC \u2248 0.68: Corpus statistics for the RI at the time we used it. ttr log = log(#types) log(#tokens) is the logarithmic type-token ratio.", "labels": [], "entities": []}, {"text": "Taking the logarithm allows better comparison with corpora of different sizes.", "labels": [], "entities": []}, {"text": "SDeWaC is a German Corpus comprising 44 million sentences crawled from the internet. of corpus statistics is given in.", "labels": [], "entities": [{"text": "SDeWaC is a German Corpus", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.7205021023750305}]}, {"text": "The high logarithmic type-token ratio (ttr log ) supports the observation that the language of the RI is highly heterogeneous: although the domain of the RI is rather focused (abstracts of medieval charters), it is notably higher than what we find in the contemporary German SDeWaC corpus . A Regest itself is a very unique form of a document, and some of them are not easy to comprehend even for humans.", "labels": [], "entities": []}, {"text": "Consider The Regest describes an action of King Karl IV.", "labels": [], "entities": []}, {"text": "in 1332, in Parma, Italy.", "labels": [], "entities": []}, {"text": "acknowledges that he owes \"Johann de Landulphis\", \"achtzig goldgulden\" (eighty gold coins) for wages and \"sechzig goldgulden\" (sixty gold coins) for reasons which are rather difficult to interpret: \"(...) wegen versendungen desselben schuldig zu sein\" (interpretable as wages and travel expenses).", "labels": [], "entities": [{"text": "Johann de Landulphis\"", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.8035280555486679}]}, {"text": "Beyond that, the Regest contains information in Latin (\"iudici et auditori curie paterne et sue\"), plus references and meta information (last sentence).", "labels": [], "entities": []}, {"text": "It is easy for humans to infer that the theme of the above Regest is about finances (indicated by mentions of \"goldgulden\" (gold coins) and \"besoldung\" (wages)).", "labels": [], "entities": []}, {"text": "Further, a specific group of persons plays a role, namely nobles.", "labels": [], "entities": []}, {"text": "This is indicated 3 http://www.ims.uni-stuttgart.de/ forschung/ressourcen/korpora/sdewac.html 4 cf.: RI VIII n.", "labels": [], "entities": [{"text": "RI VIII n", "start_pos": 101, "end_pos": 110, "type": "DATASET", "confidence": 0.9257789055506388}]}, {"text": "1, in: Regesta Imperii Online, URI: http://www.regesta-imperii.de/id/ 1332-09-22_1_0_8_0_0_7_1.", "labels": [], "entities": [{"text": "Regesta Imperii Online", "start_pos": 7, "end_pos": 29, "type": "DATASET", "confidence": 0.9529395898183187}]}, {"text": "abbreviation groups and themes traced in war and peace: Traced demographic groups and themes. by \"de\" in the name of \"Johann de Landulphi\", who is promised money by the king.", "labels": [], "entities": []}, {"text": "The Latin \"de\" in the middle of a name generally suggests that the person belongs to the class of nobles, as in \"Elizabeth of (=de) England\".", "labels": [], "entities": []}, {"text": "So, one may conclude that in the above Regest, the players are nobles, acting under the theme finances.", "labels": [], "entities": []}, {"text": "Our aim is to trace within the RI interesting demographic groups joint with the themes of their interactions.", "labels": [], "entities": [{"text": "RI", "start_pos": 31, "end_pos": 33, "type": "DATASET", "confidence": 0.7047674059867859}]}, {"text": "We aim to identify which Regest is about which theme(s) and group(s), to perform interesting data analysis, e.g. visualizing the importance of different groups and themes not only in relation to time but also in relation to other factors such as issuer, location, and possibly more.", "labels": [], "entities": []}, {"text": "With the support of a domain expert we determined interesting demographic groups (players) and themes which play a role in the Regests.", "labels": [], "entities": [{"text": "Regests", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.7715514898300171}]}, {"text": "All players and themes can be treated as individual binary classification problems.", "labels": [], "entities": []}, {"text": "An overview is given in.", "labels": [], "entities": []}, {"text": "It can be interesting, e.g., to relate the occurrence of city or citizens with occurrences of privileges with respect to time, thus approximately tracing the development of privileges for cities.", "labels": [], "entities": []}, {"text": "A Regest can be labeled with zero to all of the 12 selected labels.", "labels": [], "entities": []}, {"text": "Thus, there exist many possible combinations.", "labels": [], "entities": []}, {"text": "We cast the labeling problem as a multi-label document classification task, allowing several labels (i.e. groups and themes) to be assigned fora single document (i.e. Regest).", "labels": [], "entities": [{"text": "multi-label document classification task", "start_pos": 34, "end_pos": 74, "type": "TASK", "confidence": 0.7728084772825241}]}, {"text": "For automatic pattern recognition on this historic data, we deploy four state-of-the text classification methods, (i.)", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.8020400702953339}, {"text": "state-of-the text classification", "start_pos": 72, "end_pos": 104, "type": "TASK", "confidence": 0.7140935659408569}]}, {"text": "Support Vector Machines (SVM) (binary classification); (ii.)", "labels": [], "entities": []}, {"text": "Semi-Supervised SVMs (S 3 VMs), to exploit the large amount of unlabeled data; (iii.) a Neural Network as a meta-learner applied to the SVM outputs (do the groups and themes influence each other?) and (iv.).", "labels": [], "entities": []}, {"text": "a Convolutional Neural Network (CNN) classifier with pre-trained word vectors as input, which operates directly on the input documents.", "labels": [], "entities": []}, {"text": "We evaluate all methods on a manually labeled test set and perform data analyses on the full RI to illustrate its usage in Digital Humanities research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We manually labeled 500 Regests, randomly drawn from the corpus to prevent bias.", "labels": [], "entities": []}, {"text": "The data was split into a training and test section of 400 and 100 Regests.", "labels": [], "entities": []}, {"text": "The first two lines in display the distribution of players and themes in the annotated data.", "labels": [], "entities": []}, {"text": "Some of them occur rarely in both training and test data (e.g. Jewish people (b4) with only 3% and 2% of the respective data sets).", "labels": [], "entities": []}, {"text": "On the other hand, nobles play a role in over 70% of annotated Regests.", "labels": [], "entities": []}, {"text": "For estimation of model parameters we apply cross validation (CV) on the training set.", "labels": [], "entities": []}, {"text": "We proceed as follows: (i) Parameter tuning of SVMs.", "labels": [], "entities": [{"text": "Parameter tuning", "start_pos": 27, "end_pos": 43, "type": "METRIC", "confidence": 0.9166157245635986}]}, {"text": "For each different vector size and representation scheme, we tune the inner parameters of an SVM with CV on the training data.", "labels": [], "entities": []}, {"text": "(ii) Testing of SVMs.", "labels": [], "entities": []}, {"text": "We retrain each SVM on the full training data using the chosen hyperparameters, and evaluate the model on the test data set.", "labels": [], "entities": []}, {"text": "(iii) Determining an independent multi-label system.", "labels": [], "entities": []}, {"text": "As input to the NN models as meta learners over SVM outputs, we determine an IMC (\"Independent Margin Classifiers\"), a set of independent margin classifiers, consisting of the 12 SVMs that achieve maximum training CV score for each problem.", "labels": [], "entities": [{"text": "training CV score", "start_pos": 205, "end_pos": 222, "type": "METRIC", "confidence": 0.7754053274790446}]}, {"text": "(iv) Training NN models.", "labels": [], "entities": []}, {"text": "For different NN models we again determine hyperparameters with CV on the IMC-outputs for the training section, and retrain the final NN models on the full training data, before (v), Testing of the NNs is again done on the final test set.", "labels": [], "entities": [{"text": "IMC-outputs", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.8806180357933044}]}, {"text": "Our evaluation needs to take into account that many labels underlie a skewed distribution (cf.).", "labels": [], "entities": []}, {"text": "For example, consider that one label only is positive among 100 test samples.", "labels": [], "entities": []}, {"text": "A classifier that labels all instances as negative yields a deceivingly high score of 0.99 accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9970020651817322}]}, {"text": "Hence we employ Balanced Accuracy, the mean of Recall (Sensitivity) and inverse Recall (Specificity 10 ), defined as Acc bal = Sensitivity+Specif icity 2 . In the above example, where Accuracy yields a biased score of almost one, balanced Accuracy yields a more realistic value of 0.5.", "labels": [], "entities": [{"text": "Balanced Accuracy", "start_pos": 16, "end_pos": 33, "type": "METRIC", "confidence": 0.8807598948478699}, {"text": "Recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9474275708198547}, {"text": "Recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.8283661603927612}, {"text": "Acc bal", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.9845521152019501}, {"text": "Accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9519439339637756}, {"text": "Accuracy", "start_pos": 239, "end_pos": 247, "type": "METRIC", "confidence": 0.9759418964385986}]}, {"text": "Given the unbalanced distribution of our test data set, we report balanced accuracy for each of the 12 binary problems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9995074272155762}]}, {"text": "We also report their arithmetic mean Acc bal to provide a global measure of performance.", "labels": [], "entities": [{"text": "arithmetic mean Acc bal", "start_pos": 21, "end_pos": 44, "type": "METRIC", "confidence": 0.8731396496295929}]}, {"text": "As Baselines we choose, besides a simple majority voter, a Multinomial Naive Bayes algorithm, which is commonly used in text classification tasks (both in an independent binary manner for each label).", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.8227619528770447}]}, {"text": "shows that Naive Bayes improves over the majority baseline for all problems and yields a solid 0.67 Acc bal , 0.17 pp. above the majority voter.", "labels": [], "entities": [{"text": "Acc bal", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9846081137657166}]}, {"text": "IMC achieves 0.795 Acc bal and significantly outperforms both the majority baseline (+0.3 Acc bal ) and Naive Bayes (+0.13).", "labels": [], "entities": [{"text": "IMC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9477870464324951}, {"text": "Acc bal", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9113434553146362}, {"text": "Acc bal", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9767612814903259}]}, {"text": "For each problem the score is better with up to +0.47 Acc bal for recognizing women (b5) in a Regest.", "labels": [], "entities": [{"text": "Acc bal", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9740211665630341}, {"text": "Regest", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.6507149934768677}]}, {"text": "For lesser nobles (b2) and war and peace (b11), the independent classifiers combination baseline yields the overall best results (0.62 and 0.79 Acc bal ).", "labels": [], "entities": [{"text": "Acc bal", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.9854740798473358}]}, {"text": "SVMs/S 3 VMs combined into the multi-labeler (\"IMC\",) achieve good performance (0.795 Acc bal ).", "labels": [], "entities": [{"text": "Acc bal", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9811464250087738}]}, {"text": "Based on the training CV scores, IMC consists of six supervised SVMs and four S 3 VMs.", "labels": [], "entities": [{"text": "IMC", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.5471989512443542}]}, {"text": "S 3 VMs in the IMC were chosen for problems b0, b1, b8 and b10.", "labels": [], "entities": [{"text": "IMC", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.9346848726272583}]}, {"text": "With respect to b2 and b11, IMC outperforms all NN approaches (b2: +0.04, b11: +0.01).", "labels": [], "entities": [{"text": "IMC", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.760026752948761}]}, {"text": "The Naive Bayes Baseline is outperformed with +0.128 Acc bal . This strong improvement could be due to the generalization capacity of the maximum margin, which might be especially useful with small training set sizes.", "labels": [], "entities": [{"text": "Naive Bayes Baseline", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.5801535745461782}, {"text": "Acc bal", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.979672372341156}]}, {"text": "With regard to representation schemes such as boolean or tfidf and 2,000 words or 10,000 words, we observe no clear patterns whether one works generally better than the other on the RI.", "labels": [], "entities": []}, {"text": "5 classifiers of IMC are trained on 10,000 words and 10 classifiers use boolean word-features.", "labels": [], "entities": [{"text": "IMC", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.817833423614502}]}, {"text": "Specif icity = TN TN +F P CNNs fed with 128 dimensional embeddings outperform majority vote (+0.06 Acc bal ) but not Naive Bayes (-0.11), most likely due to the low amount of training data.", "labels": [], "entities": [{"text": "Acc bal", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9831815958023071}]}, {"text": "Another explanation is that the 129,504 Regests were not sufficient to pretrain useful word-vectors (possibly also negatively influenced by the word variety).", "labels": [], "entities": []}, {"text": "As the vector size increases (512 dimensions), the performance drops further (+0.01 over the majority voter).", "labels": [], "entities": []}, {"text": "The remaining classifier models are intended to detect dependencies between players and themes and had access to the outputs of IMC.", "labels": [], "entities": [{"text": "IMC", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.8813867568969727}]}, {"text": "Specifically, the question is whether NNs are suitable for detecting such dependencies.", "labels": [], "entities": []}, {"text": "As baselines we considered SVM and Decision Tree models, trained on the outputs of the independent learners (in: +Decision Tree, +SVMs).", "labels": [], "entities": []}, {"text": "Neither copes specifically well with this input information (-0.045 Acc bal for +Decision Tree and -0.007 for +SVMs).", "labels": [], "entities": [{"text": "Acc bal", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.9717930853366852}]}, {"text": "Even when supplied with more information using various sizes of Paragraph Vectors (omitted in), both systems do not improve their previous scores.", "labels": [], "entities": []}, {"text": "Neural Networks employed as meta learners, by contrast, are able to improve results for specific problems, especially when supplied with Paragraph Vectors, resulting in the overall best system on test, a NN with 2048 hidden nodes and Paragraph Vectors of dimension 512 (+NN 2048 +PV 512 ,).", "labels": [], "entities": []}, {"text": "Still, the overall performance gain is small with only +0.004 Acc bal . When omitting b3 (lesser nobles) from the result calculation (it was the most controversial class in the annotation), the gain over IMC increases to +0.006.", "labels": [], "entities": [{"text": "Acc bal", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9801381826400757}, {"text": "IMC", "start_pos": 204, "end_pos": 207, "type": "DATASET", "confidence": 0.8751340508460999}]}, {"text": "Notable individual performance gains are achieved for b0 (nobles, +0.02), b6 (new privileges, +0.05) and bestowal of land (+0.02).", "labels": [], "entities": []}, {"text": "We conclude that there are dependencies between nobles, bestowal of land and privileges which cannot be captured by considering these classes independently.", "labels": [], "entities": []}, {"text": "To analyze on which groups and themes the neural network meta-learner offers significantly differing predictions (\"it disagrees with its input\"), we calculate mid-p-values with McNemars test) between different systems outputs (cf.).", "labels": [], "entities": []}, {"text": "Comparing the best three NNs among each other, the 1,200 single predictions each system made do not differ significantly (min.", "labels": [], "entities": []}, {"text": "p = 0.065), however the opposite is true when comparing the best three NNs to IMC (all p-values < 0.05).", "labels": [], "entities": [{"text": "IMC", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.873867392539978}]}, {"text": "This indicates that there is more   consensus about the label-predictions between different NN architectures than between the NNs and IMC.", "labels": [], "entities": []}, {"text": "On the binary problem level, the p-value for theme 6, new privileges, lies below 0.05 for all NN architectures with more than 512 hidden units.", "labels": [], "entities": []}, {"text": "For b8, land grants, all p-values are < 0.05 for architectures with more than 128 hidden units.", "labels": [], "entities": []}, {"text": "Observation of the predictions further suggests that the NNs feel the most need to correct the SVMs with b6 and b8 (with these the correction ends up in better predictions) and b0, nobles.", "labels": [], "entities": []}, {"text": "However, in predicting nobles the difference is never significant.", "labels": [], "entities": []}, {"text": "For example, NN2048+PV512, the best NN on the test set disagrees with the SVM on the nobles-label in 11 of 100 cases.", "labels": [], "entities": []}, {"text": "Here the NN is correct only in 6 cases, making the difference nonsignificant with p=0.77.", "labels": [], "entities": []}, {"text": "With b8 on the other hand there are 14 disagreements and 13 accurate corrections, resulting in a p-value of 0.001 (b6: 6 corrections, 6 accurate, p=0.016).", "labels": [], "entities": [{"text": "accurate corrections", "start_pos": 60, "end_pos": 80, "type": "METRIC", "confidence": 0.9016084969043732}, {"text": "accurate", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9814027547836304}]}, {"text": "Taking predictions overall groups again, this NN differs in 46 cases from the IMC choice and is correct 39 times (p < 0.0005).", "labels": [], "entities": []}, {"text": "Why is the resulting perfomance increase in Acc bal only 0.2%?", "labels": [], "entities": [{"text": "Acc bal", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.8899242579936981}]}, {"text": "This is due to the fact that the NN is more restrictive in assigning labels than the independent learner model: in all 129,504 Regests, it predicts 50,968 less positive labels than IMC.", "labels": [], "entities": []}, {"text": "As positive labels are strongly under-represented in the manually labeled data, the (non-weighted) Acc bal measure is much more influenced by an additional True Positive than a True Negative fora rare group or theme.", "labels": [], "entities": [{"text": "Acc bal measure", "start_pos": 99, "end_pos": 114, "type": "METRIC", "confidence": 0.9791052142779032}]}, {"text": "Paragraph Vectors (PV) used as input to the NNs apparently contain more information than standard (boolean) bag-of-word (BoW) vectors.", "labels": [], "entities": []}, {"text": "When the best NN is fed with BoW vectors instead of PVs it achieves lower performance (-0.07 Acc bal ).", "labels": [], "entities": [{"text": "Acc bal", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9818210899829865}]}, {"text": "To test whether Paragraph Vectors work better simply in general, we trained 12 independent SVM classifiers on PVs only, to predict players and themes.", "labels": [], "entities": []}, {"text": "The result, for several dimensions of Paragraph Vectors (between 64 and 2048) fed into an SVM (best result: SVMs+PV 128 in), did not exceed the Naive Bayes baseline, indicating strongly that PVs alone are inferior to BoW vectors for standard textual classification of the RI.", "labels": [], "entities": []}, {"text": "Our explanation is as follows: While Quoc Le (2014) achieved good results in classifying sentiment of movie reviews with Paragraph Vectors, he hypotheses that movie reviews are tailor-cut for learning the vectors for this problem, because compositionality plays an important role in deciding whether the review is positive or negative.", "labels": [], "entities": []}, {"text": "The RI area more complex source and it is debatable whether compositionality plays a role with regard to co-occurring groups and themes.", "labels": [], "entities": [{"text": "RI", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.5937142968177795}]}, {"text": "Also, while movie reviews often contain similar (sentiment) vocabulary, each Regest presents its content in rather unique ways.", "labels": [], "entities": []}, {"text": "The NN that learns Paragraph Vectors is thus presented with very diverse information, most likely generating vectors containing every and thus little information.", "labels": [], "entities": []}, {"text": "We conclude that using standard BoW vectors as firstorder information was the correct choice, while PVs prove more suitable as higher-order information for the NN acting as a meta-classifier (as they add little but additional information).", "labels": [], "entities": [{"text": "BoW vectors", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.8967010676860809}]}, {"text": "Players and themes that can be predicted with great success by many systems on the test set are confirmation of privileges (b7: 0.94), Jews (b4: 1.00) and women (b5: 0.98).", "labels": [], "entities": []}, {"text": "By contrast, all systems fail to reliably predict class b2 (lesser nobles), which yields a maximum of 0.12 points beyond majority and no gains beyond Naive Bayes.", "labels": [], "entities": []}, {"text": "One explanation for this low performance is that it was really hard (if not sometimes impossible) to distinguish between non-nobles and nobles in the annotation process.", "labels": [], "entities": []}, {"text": "All other groups and themes can be predicted with solid accuracy scores (\u2265 0.20 above majority, \u2265 0.02 above Naive Bayes, and \u2265 0.62 Acc bal per category in general).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9987875819206238}, {"text": "Acc bal", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9812205135822296}]}, {"text": "The system +NN 2048 +PV 512 perfoms best in Acc bal . We also analyze two additional criteria of performance: (i) the Kullback-Leibler (KL) divergence between distributions of labels in the manually annotated data to the distributions of labels automatically assigned to the full RI and (ii), the KL divergence between the distributions of amounts of labels (0-12 labels can be assigned to a Regest).", "labels": [], "entities": [{"text": "Acc bal", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.8081475496292114}]}, {"text": "For (i), the KL divergences are KL train,test = 0.033 and KL train,RI N N = 0.036, KL train,RI IM C = 0.058 indicating only a small divergence between human and automatic labeling by the NN w.r.t. the distributions of the twelve groups and themes (cf.).", "labels": [], "entities": [{"text": "KL train,RI N N", "start_pos": 58, "end_pos": 73, "type": "METRIC", "confidence": 0.8161740899085999}, {"text": "KL train,RI IM C", "start_pos": 83, "end_pos": 99, "type": "METRIC", "confidence": 0.7530919164419174}]}, {"text": "In fact, all of the best three NNs appear to have smaller KL-divergencies than IMC.", "labels": [], "entities": [{"text": "IMC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.908370316028595}]}, {"text": "Also (ii), number of group and theme labels that are assigned by human vs. automatic labeling, shows similar tendencies: KL train,test = 0.02, KL train,RI N N = 0.01, Note that this applies only to NNs as meta-learners: the SVM-based meta-learner baseline performed below majority baseline when supplied also with Paragraph Vectors (acc bal with additional Paragraph Vectors: 0.786, without: 0.788).", "labels": [], "entities": [{"text": "KL train,RI N N", "start_pos": 143, "end_pos": 158, "type": "METRIC", "confidence": 0.7783858329057693}]}, {"text": "KL train,RI IM C = 0.07.", "labels": [], "entities": [{"text": "KL train,RI", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8201588690280914}, {"text": "IM C", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.8901078701019287}]}, {"text": "On average, two labels were assigned to a Regest by all labeling systems.", "labels": [], "entities": []}, {"text": "The human assigned 43% of the Regests two labels, IMC 27% and the NN 34%.", "labels": [], "entities": []}, {"text": "In sum, our results indicate that NNs can learn dependencies of labels from independent classifier predictions.", "labels": [], "entities": []}, {"text": "NNs are thus suitable to detect structures in the data that are intuitive for humans.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Prevalence of groups and themes: humanly labelled data vs. full automatically labelled RI.", "labels": [], "entities": []}]}