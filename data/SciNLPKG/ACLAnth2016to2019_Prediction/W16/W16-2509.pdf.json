{"title": [{"text": "Capturing Discriminative Attributes in a Distributional Space: Task Proposal", "labels": [], "entities": [{"text": "Capturing Discriminative Attributes in a Distributional Space", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.8822261265345982}]}], "abstractContent": [{"text": "If lexical similarity is not enough to reliably assess how word vectors would perform on various specific tasks, we need other ways of evaluating semantic representations.", "labels": [], "entities": []}, {"text": "We propose anew task, which consists in extracting semantic differences using distributional models: given two words, what is the difference between their meanings?", "labels": [], "entities": []}, {"text": "We present two proof of concept datasets for this task and outline how it maybe performed.", "labels": [], "entities": []}], "introductionContent": [{"text": "All similar pairs of words are similar in the same way: they share a substantial number of semantic properties (although properties themselves may belong to different groups, i.e. visual, functional, etc.).", "labels": [], "entities": []}, {"text": "Cosine of two feature vectors in a distributional semantic space is a formalization of this idea, standardly used as a measure of semantic similarity for the evaluation of distributional models (.", "labels": [], "entities": []}, {"text": "While similarity tasks have become the standard in the evaluation of distributional models, the validity of those tasks has been put into question: inter-annotator agreement tends to below, the small size of some of the most popular datasets is a concern, and subjective similarity scores have limitations when it comes to task-specific applications (.", "labels": [], "entities": [{"text": "agreement", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.8122191429138184}]}, {"text": "In contrast to similarity, the nature of semantic difference between two (related) words can vary greatly.", "labels": [], "entities": []}, {"text": "Modeling difference can help capture individual aspects of meaning; similarity alone maybe too simple a task to assess semantic representations in all their complexity, and therefore insufficient for driving the progress of computational models.", "labels": [], "entities": []}, {"text": "Our project is related to previous work that attempts to predict the discriminative features of referents, using natural images to represent the input objects (.", "labels": [], "entities": []}, {"text": "Attributes have also been used to simulate similarity judgements and concept categorization).", "labels": [], "entities": []}, {"text": "On a more abstract level, our work is related to previous attempts at using offset vectors to capture lexical relations without explicit supervision (, which have been shown to be able to generalise well to a range of relations (.", "labels": [], "entities": []}, {"text": "We created two proof of concept datasets for the difference task: a small dataset of differences as feature oppositions and a bigger one with differences as presence vs. absence of a feature.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a random sample of seed words from the BLESS dataset () along with their semantic neighbors to create word pairs that were in some ways similar and denoted concrete objects.", "labels": [], "entities": [{"text": "BLESS dataset", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9368859231472015}]}, {"text": "For each word pair, one or more pair(s) of discriminating attributes were assigned manually.", "labels": [], "entities": []}, {"text": "For example, the word pair [scooter, moped] received two pairs of attributes: and.", "labels": [], "entities": []}, {"text": "Some word pairs were also added manually to further exemplify specific differences, such as [horse, foal] for the age properties.", "labels": [], "entities": []}, {"text": "The resulting dataset contains 91 items.", "labels": [], "entities": []}, {"text": "To get a simple unsupervised baseline on the detection of difference direction, we calculated a similarity score for each item, using the cooccurrence counts of the best count-based configuration presented in, which were extracted from the concatenation of the web-crawled ukWack corpus (, Wikipedia, and the BNC, fora total of 2.8 billion tokens.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 96, "end_pos": 112, "type": "METRIC", "confidence": 0.9518829584121704}, {"text": "ukWack corpus", "start_pos": 273, "end_pos": 286, "type": "DATASET", "confidence": 0.968929260969162}, {"text": "BNC", "start_pos": 309, "end_pos": 312, "type": "DATASET", "confidence": 0.9376884698867798}]}, {"text": "This similarity score calculates whether the attribute is closer to the first or second word.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 5, "end_pos": 21, "type": "METRIC", "confidence": 0.962849497795105}]}, {"text": "We found that 67% of items had positive scores.", "labels": [], "entities": []}, {"text": "The most successful types of attributes were color (34 out 51), age (9 out of 9) and diet (4 out of 5).", "labels": [], "entities": [{"text": "diet", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9592655897140503}]}, {"text": "The dataset is too small for training supervised models; our attempts (logistic regression on pairwise cosines with cross-validation) showed negligibly low results.", "labels": [], "entities": []}, {"text": "Only some differences can be expressed in the format assumed above, i.e. as the opposition of two attributes, such as yellow vs. red being the difference between bananas and apples.", "labels": [], "entities": []}, {"text": "Other differences are better expressed as the presence or absence of a feature.", "labels": [], "entities": []}, {"text": "For instance, the difference between a narwhal and a dolphin is the presence of a horn.", "labels": [], "entities": []}, {"text": "For natural salient features of word concepts, we turned to property norms.", "labels": [], "entities": []}, {"text": "We used the set of feature norms collected by, which includes features for 541 concepts (living and non-living entities), collected by asking 725 participants to produce features they found important for each concept.", "labels": [], "entities": []}, {"text": "Production frequencies of these features indicate how salient they are.", "labels": [], "entities": []}, {"text": "Feature norms of concepts are able to encode semantic knowledge because they tap into the representations that the participants have acquired through repeated exposure to those concepts.", "labels": [], "entities": []}, {"text": "McRae et al. divided disjunctive features, so that if a participant produced the feature is green or red the concept will be associated with both the feature is green and the feature is red.", "labels": [], "entities": []}, {"text": "Concepts that have different meanings had been disambiguated before being shown to participants.", "labels": [], "entities": []}, {"text": "For example, there are two entries for bow, bow (weapon) and bow (ribbon).", "labels": [], "entities": []}, {"text": "Because the word vector for bow encodes the properties of both senses, we did not differentiate between entries that have multiple senses.", "labels": [], "entities": []}, {"text": "In our dataset, the concept bow has the features of both the weapon and the ribbon.", "labels": [], "entities": []}, {"text": "The McRae dataset uses the brain region taxonomy ( to classify features into different types, such as function, sound or taxonomic.", "labels": [], "entities": [{"text": "McRae dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.950911670923233}]}, {"text": "We decided to only work with visual features, which exist for all concrete concepts, while features such as sound or taste are only relevant for some concepts.", "labels": [], "entities": []}, {"text": "This classification distinguishes between three types of visual features: motion, color and form and surface.", "labels": [], "entities": []}, {"text": "We first selected words that had at least one visual feature of any type.", "labels": [], "entities": []}, {"text": "We then created word pairs by selecting the 50 closest neighbours of every word in the dataset.", "labels": [], "entities": []}, {"text": "For each word pair, if there was a feature that the first word had but the second didn't, that word pair and feature item was added to our dataset.", "labels": [], "entities": []}, {"text": "The set was builtin such away that the feature of each item always refers to an attribute of the first word.", "labels": [], "entities": []}, {"text": "For example, in, wings is an attribute of airplane.", "labels": [], "entities": []}, {"text": "The word pair will only be included in the order if helicopter has a feature that airplane doesn't have.", "labels": [], "entities": []}, {"text": "The relations are thus asymmetric and have fixed directionality.", "labels": [], "entities": []}, {"text": "For simplicity, multi-word features were processed so that only the final word is taken into account (e.g. has wings becomes wings).", "labels": [], "entities": []}, {"text": "In total, our dataset contains 528 concepts,   We computed a simple unsupervised baseline for direction of difference (e.g. is subway or train dirty?), choosing the first word iff cos(w 1 w f ) > cos(w 2 , w f ), and achieved 69% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 230, "end_pos": 238, "type": "METRIC", "confidence": 0.998734176158905}]}, {"text": "Ultimately, this dataset could be used to build a model that can predict an exhaustive list of distinctive attributes for any pair of words.", "labels": [], "entities": []}, {"text": "This could be done in a binary set-up where the dataset has been supplemented with negative examples: fora given triple, predict whether the attribute is a difference between word 1 and word 2 .", "labels": [], "entities": []}], "tableCaptions": []}