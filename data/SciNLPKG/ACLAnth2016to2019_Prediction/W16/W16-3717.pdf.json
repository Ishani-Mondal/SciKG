{"title": [{"text": "A study of attention-based Neural Machine Translation models on Indian Languages", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.623986005783081}]}], "abstractContent": [{"text": "Neural machine translation (NMT) models have recently been shown to be very successful in machine translation (MT).", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7494319727023443}, {"text": "machine translation (MT)", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.8695509910583497}]}, {"text": "The use of LSTMs in machine translation has significantly improved the translation performance for longer sentences by being able to capture the context and long range correlations of the sentences in their hidden layers.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7594046592712402}]}, {"text": "The attention model based NMT system has become state-of-the-art, performing equal or better than other statistical MT approaches.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9112067222595215}]}, {"text": "In this paper, we studied the performance of the attention-model based NMT system on the Indian language pair, Hindi and Bengali.", "labels": [], "entities": []}, {"text": "We analysed the types of errors that occur in morphologically rich languages when there is a scarcity of large parallel training corpus.", "labels": [], "entities": []}, {"text": "We then carried out certain post-processing heuristic steps to improve the quality of the translated statements and suggest further measures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep Neural Network has been successfully applied to machine translation.The work of () have shown that it is possible to build an end-toend machine translation system using neural networks by introducing the encoder-decoder model.", "labels": [], "entities": [{"text": "machine translation.The", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7900595963001251}, {"text": "machine translation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.7659894227981567}]}, {"text": "NMT systems have several advantages over the existing phrase-based statistical machine translation (SMT) systems ().", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 54, "end_pos": 104, "type": "TASK", "confidence": 0.7279369022165026}]}, {"text": "The NMT systems do not assume any domain knowledge or linguistic features in source and target language sentences.", "labels": [], "entities": []}, {"text": "Secondly, the entire encoder-decoder models are jointly trained to maximize the translation quality as opposed to the phrase-based SMT systems in which the individual components needs to be trained and tuned separately for optimal performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.8046319484710693}]}, {"text": "Although the NMT systems have several advantages, their performance is restricted in case of lowresource language pairs for which sufficiently large parallel corpora is not available and the language pairs whose syntaxes differ significantly.", "labels": [], "entities": []}, {"text": "Morphological richness of language pairs poses another challenge for NMT systems that do not have any prior knowledge of the languages as it tends to increase the number of surface forms of the words due to inflectional attachments resulting in an increased vocabulary of the languages.", "labels": [], "entities": []}, {"text": "Moreover, the inflectional forms have their semantic roles that have to be interpreted for proper translation.", "labels": [], "entities": []}, {"text": "In order to enable the NMT systems to learn the roles of the inflectional forms automatically we need sufficiently large data.", "labels": [], "entities": []}, {"text": "However, sufficiently large parallel data may not be available for low-resource morphologically rich language pairs.", "labels": [], "entities": []}, {"text": "Most of the Indian languages are morphologically rich and there is lack of sufficiently large parallel corpus for Indian language pairs.", "labels": [], "entities": []}, {"text": "Given our familiarity with Bengali and Hindi, we took up this task as a case-study and evaluated the performance of NMT models on Indian language pair-Hindi and Bengali.", "labels": [], "entities": []}, {"text": "We then analyzed the resulting translated sentences and suggested post-processing heuristics to improve the quality of the translated sentences.", "labels": [], "entities": []}, {"text": "We have proposed heuristics to rectify the incorrect translations of the named entities.", "labels": [], "entities": []}, {"text": "We have also proposed a heuristic to translate and predict the position of untranslated source words.", "labels": [], "entities": [{"text": "translate and predict the position of untranslated source words", "start_pos": 37, "end_pos": 100, "type": "TASK", "confidence": 0.6024306681421068}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of 1) attention-based NMT model and 2) MOSES phrase-based SMT system.  Translation model  BLEU score Iterations  MOSES  14.35  - Attention-based translation model  20.41  25", "labels": [], "entities": [{"text": "MOSES phrase-based SMT", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.4026245176792145}, {"text": "BLEU score Iterations  MOSES", "start_pos": 111, "end_pos": 139, "type": "METRIC", "confidence": 0.822575032711029}]}]}