{"title": [{"text": "Improving Temporal Relation Extraction with Training Instance Augmentation", "labels": [], "entities": [{"text": "Improving Temporal Relation Extraction", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.9075217992067337}, {"text": "Training Instance Augmentation", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.6003677546977997}]}], "abstractContent": [{"text": "Temporal relation extraction is important for understanding the ordering of events in narrative text.", "labels": [], "entities": [{"text": "Temporal relation extraction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9321852922439575}]}, {"text": "We describe a method for increasing the number of high-quality training instances available to a temporal relation extraction task, with an adaptation to different annotation styles in the clinical domain by taking advantage of the Unified Medical Language System (UMLS).", "labels": [], "entities": [{"text": "temporal relation extraction task", "start_pos": 97, "end_pos": 130, "type": "TASK", "confidence": 0.7102718725800514}]}, {"text": "This method notably improves clinical temporal relation extraction, works beyond fea-turizing or duplicating the same information , can generalize between-argument signals in a more effective and robust fashion.", "labels": [], "entities": [{"text": "clinical temporal relation extraction", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.6612045913934708}]}, {"text": "We also report anew state-of-the-art result, which is a two point improvement over the best Clinical TempEval 2016 system.", "labels": [], "entities": [{"text": "Clinical TempEval 2016 system", "start_pos": 92, "end_pos": 121, "type": "DATASET", "confidence": 0.6600402519106865}]}], "introductionContent": [{"text": "Temporal relation extraction is important for understanding ordering of events from a narrative text.", "labels": [], "entities": [{"text": "Temporal relation extraction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.935359001159668}, {"text": "ordering of events from a narrative text", "start_pos": 60, "end_pos": 100, "type": "TASK", "confidence": 0.8054887567247663}]}, {"text": "Recent years have seen annotated corpora created for temporal information extraction, from newspaper text (, to clinical narratives (), all with the aim of developing systems for building event timelines from textual descriptions of events.", "labels": [], "entities": [{"text": "temporal information extraction", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.6855721275011698}]}, {"text": "Such narrative timelines are important for information extraction tasks such as question answering (, clinical outcomes prediction (, and the identification of temporal patterns () among many.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7837715744972229}, {"text": "question answering", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.9174934923648834}, {"text": "clinical outcomes prediction", "start_pos": 102, "end_pos": 130, "type": "TASK", "confidence": 0.6577861507733663}, {"text": "identification of temporal patterns", "start_pos": 142, "end_pos": 177, "type": "TASK", "confidence": 0.8274577707052231}]}, {"text": "Ina typical supervised approach to the temporal relation extraction task, argument pairs consist of pairs of events or temporal expressions.", "labels": [], "entities": [{"text": "temporal relation extraction task", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.699393592774868}]}, {"text": "Corpora differ in their syntactic annotation of such expressions.", "labels": [], "entities": []}, {"text": "For example, the THYME corpus, consisting of oncology, pathology and radiology notes, annotated only event headwords), while the i2b2 corpus, consisting of discharge summaries, annotated entire noun phrases as events (.", "labels": [], "entities": [{"text": "THYME corpus", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.8761119544506073}]}, {"text": "As a result, it is necessary to account for these differences when implementing a generalizable relation extraction system.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7421853840351105}]}, {"text": "However, the annotations of the temporal relations between the events remain unaffected by the choice of headwords or phrases for the event annotation.", "labels": [], "entities": []}, {"text": "For example, in a relation between the temporal expression yesterday and the event severe lower abdominal pain, if the argument had been the headword pain it still would have been an instance of the same temporal relation.", "labels": [], "entities": []}, {"text": "Thus, we can automatically create additional training examples by varying the extent of headword expansion.", "labels": [], "entities": []}, {"text": "For example, the relation between yesterday and severe lower abdominal pain can automatically generate four valid relations of the same type where the second arguments are pain, abdominal pain, and lower abdominal pain.", "labels": [], "entities": []}, {"text": "In this paper, we describe an automatic method that generates more temporal training instances by semantically expanding gold medical events based on a clinical ontology, the Unified Medical Language System (UMLS) (.", "labels": [], "entities": []}, {"text": "It bridges the gap between different syntactic annotations of events in clinical corpora.", "labels": [], "entities": []}, {"text": "We show that this method is superior to representing the same information as additional features, that it differs from plain upsampling, and that the primary mechanism of improvement is in the better representation of between-argument features.", "labels": [], "entities": []}, {"text": "Our method can be viewed as anew form of data augmentation, akin to the generation of image variants for vision recognition () or the generation of word substitutions for information extraction.", "labels": [], "entities": [{"text": "vision recognition", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7495156824588776}, {"text": "information extraction", "start_pos": 171, "end_pos": 193, "type": "TASK", "confidence": 0.7155086249113083}]}], "datasetContent": [{"text": "We tested our event expansion technique on a publicly available clinical corpus: the colon cancer set of the THYME corpus () used in and SemEval 2016 Task 12: Clinical TempEval (.", "labels": [], "entities": [{"text": "event expansion", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.7539898753166199}, {"text": "THYME corpus", "start_pos": 109, "end_pos": 121, "type": "DATASET", "confidence": 0.865310400724411}, {"text": "SemEval 2016 Task 12", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.8037112802267075}]}, {"text": "It contains 600 documents (400 oncology notes and 200 pathology notes) of 200 colon cancer patients.", "labels": [], "entities": []}, {"text": "The gold standard annotations contain events (including both medical and general events, all annotated by head words), temporal expressions (e.g. tomorrow, postoperative, and March-11-2009), and temporal relations.", "labels": [], "entities": []}, {"text": "We used the same training/development/test split as Clinical TempEval.", "labels": [], "entities": []}, {"text": "The development set was used for testing research questions and building final models.", "labels": [], "entities": []}, {"text": "Once the models were deemed finalized, they were rebuilt on the combined training and development sets and tested on the test set.", "labels": [], "entities": []}, {"text": "For results on the development set, we calculate closure-enhanced precision, recall and F1-score (UzZaman and Allen, 2011) on just the withinsentence relations (since that's what our models are able to predict).", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9504329562187195}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9995725750923157}, {"text": "F1-score", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9996886253356934}]}, {"text": "Precision is the percentage of system-generated relations that can be verified in the transitive closure of the gold standard relations.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9916212558746338}]}, {"text": "Recall is the percentage of gold standard relations that can be found in the transitive closure of the system-generated relations.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9717782139778137}]}, {"text": "The final F1-score is the harmonic mean of the transitiveclosure-processed precision and recall.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985595345497131}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9918310046195984}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.996293842792511}]}, {"text": "For results on the test set, we used the official Clinical TempEval evaluation scripts so that our results are directly comparable with the outcomes of Clinical TempEval 2016 ().", "labels": [], "entities": [{"text": "Clinical TempEval evaluation scripts", "start_pos": 50, "end_pos": 86, "type": "DATASET", "confidence": 0.8994085937738419}, {"text": "Clinical TempEval 2016", "start_pos": 152, "end_pos": 174, "type": "DATASET", "confidence": 0.788927952448527}]}, {"text": "These scripts use similar definitions of closureenhanced precision, recall and F1-score, but evaluate only CONTAINS relations in oncology notes.: Results on the development set.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.8619877696037292}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9994128942489624}, {"text": "F1-score", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9987661838531494}, {"text": "CONTAINS", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9301389455795288}]}, {"text": "Comparison of improvement for feature groups: (A) words covered by the arguments; (B) words in between the arguments; (C) words around the arguments.", "labels": [], "entities": []}, {"text": "mance as UMLS expansion, and in fact decreases performance.", "labels": [], "entities": []}, {"text": "Question 2 is addressed in the last three rows: expanding to all possible UMLS spans works better than expanding only to the longest span or to the immediate enclosing NP.", "labels": [], "entities": []}, {"text": "Expanding to NPs achieved the second best result, suggesting that when a domain-specific ontology is unavailable, expansion via syntax might provide a viable alternative.", "labels": [], "entities": []}, {"text": "Question 3 is answered by rows 1, 3 and 4: when the cost parameter is properly tuned, doubling or tripling instances (rows 3 and 4) does not improve performance over no expansion (row 1).", "labels": [], "entities": []}, {"text": "Question 4 is addressed by: features extracted between the two arguments achieve the biggest gain from our expansion method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on the development set. No expan- sion vs. encoding UMLS as features; duplicating  and triplicating training instances; expand to the  longest UMLS span, expand to the immediate en- closing NP vs. expand to all UMLS spans.", "labels": [], "entities": []}, {"text": " Table 3: Results on the development set. Compari- son of improvement for feature groups: (A) words  covered by the arguments; (B) words in between  the arguments; (C) words around the arguments.", "labels": [], "entities": [{"text": "Compari- son", "start_pos": 42, "end_pos": 54, "type": "METRIC", "confidence": 0.9313483635584513}]}, {"text": " Table 4: Results on the test set with all features. (1)  Evaluate both Event-Time and Event-Event mod- els; (2) Evaluate Event-Time model only; (3) Eval- uate Event-Event model only. See Section 3.4 for  explanation for why shaded scores are different  from their counterparts in Table 2.", "labels": [], "entities": []}]}