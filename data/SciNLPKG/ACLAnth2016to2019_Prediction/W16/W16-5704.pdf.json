{"title": [{"text": "Nonparametric Bayesian Storyline Detection from Microtexts", "labels": [], "entities": [{"text": "Bayesian Storyline Detection from Microtexts", "start_pos": 14, "end_pos": 58, "type": "TASK", "confidence": 0.6777577936649323}]}], "abstractContent": [{"text": "News events and social media are composed of evolving storylines, which capture public attention fora limited period of time.", "labels": [], "entities": []}, {"text": "Identifying storylines requires integrating temporal and linguistic information, and prior work takes a largely heuristic approach.", "labels": [], "entities": [{"text": "Identifying storylines", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9207873046398163}]}, {"text": "We present a novel online non-parametric Bayesian framework for storyline detection, using the distance-dependent Chinese Restaurant Process (dd-CRP).", "labels": [], "entities": [{"text": "storyline detection", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.919299989938736}]}, {"text": "To ensure efficient linear-time inference, we employ a fixed-lag Gibbs sampling procedure, which is novel for the dd-CRP.", "labels": [], "entities": []}, {"text": "We evaluate on the TREC Twit-ter Timeline Generation (TTG), obtaining encouraging results: despite using a weak base-line retrieval model, the dd-CRP story clustering method is competitive with the best entries in the 2014 TTG task.", "labels": [], "entities": [{"text": "TREC Twit-ter Timeline Generation (TTG)", "start_pos": 19, "end_pos": 58, "type": "TASK", "confidence": 0.4924845950944083}, {"text": "dd-CRP story clustering", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.5859977205594381}]}], "introductionContent": [{"text": "A long-standing goal for information retrieval and extraction is to identify and group textual references to ongoing events in the world).", "labels": [], "entities": [{"text": "information retrieval and extraction", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.7564229741692543}]}, {"text": "Success on this task would have applications in personalized news portals (), intelligence analysis, disaster relief (, and in understanding the properties of the news cycle (.", "labels": [], "entities": [{"text": "intelligence analysis", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.8703031837940216}, {"text": "disaster relief", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.7140699028968811}]}, {"text": "This task attains anew importance in the era of social media, where citizen journalists can document events as they unfold), but where repetition and untrustworthy information can make the reader's task especially challenging.", "labels": [], "entities": []}, {"text": "A major technical challenge is in fusing information from two heterogeneous data sources: textual content and time.", "labels": [], "entities": []}, {"text": "Two different documents about a single event might use very different vocabulary, particularly in sparse social media data such as microblogs; conversely, two different sporting events might be described in nearly identical language, with differences only in the numerical outcome.", "labels": [], "entities": []}, {"text": "Temporal information is therefore critical: in the first case, to find the commonalities across disparate writing styles, and in the second case, to identify the differences.", "labels": [], "entities": []}, {"text": "A further challenge is that unlike in standard document clustering tasks, the number of events in a data stream is typically unknown in advance.", "labels": [], "entities": [{"text": "document clustering tasks", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.7731588880221049}]}, {"text": "Finally, there is a high premium on scalability, since online text is produced at a high rate.", "labels": [], "entities": []}, {"text": "Due to these challenges, existing approaches for combining these modalities have been somewhat heuristic, relying on tunable parameters to control the tradeoff between textual and temporal similarity.", "labels": [], "entities": []}, {"text": "In contrast, the Bayesian setting provides elegant formalisms for reasoning about latent structures (e.g., events) and their stochastically-generated realizations across text and time.", "labels": [], "entities": []}, {"text": "In this paper, we describe one such model, based on the distancedependent Chinese Restaurant Process (dd-CRP;.", "labels": [], "entities": []}, {"text": "This model is distinguished by the neat separation that it draws between textual content, which is treated as a stochastic emission from an unknown Multinomial distribution, and time, which is modeled as a prior on graphs over documents, through an arbitrary distance function.", "labels": [], "entities": []}, {"text": "However, straightforward implementations of the dd-CRP are insufficiently scalable, and so the model has been relatively underutilized in the NLP literature ().", "labels": [], "entities": []}, {"text": "We describe improvements to Bayesian inference that make the application of this model feasible, and present encouraging empirical results on the Tweet Timeline Generation task from TREC 2014 ().", "labels": [], "entities": [{"text": "Tweet Timeline Generation task", "start_pos": 146, "end_pos": 176, "type": "TASK", "confidence": 0.8045085370540619}, {"text": "TREC 2014", "start_pos": 182, "end_pos": 191, "type": "DATASET", "confidence": 0.7419978976249695}]}], "datasetContent": [{"text": "To test the efficacy of this approach, we evaluate on the Twitter Timeline Generation (TTG) task in the Microblog track of TREC 2014.", "labels": [], "entities": [{"text": "Microblog track of TREC 2014", "start_pos": 104, "end_pos": 132, "type": "DATASET", "confidence": 0.9523264527320862}]}, {"text": "It involves taking tweets based on a query Q at time T and returning a summary that captures relevant information.", "labels": [], "entities": []}, {"text": "We perform the task on 55 queries with different timestamps and compare our results with 13 groups that submitted 50 runs for this task in 2014.", "labels": [], "entities": []}, {"text": "We consider the following systems: Baseline We replace the distance-dependent prior with a standard Dirichlet prior.", "labels": [], "entities": []}, {"text": "The number of clusters is heuristically set to 20.", "labels": [], "entities": []}, {"text": "Annealed Gibbs sampling is employed for inference.", "labels": [], "entities": []}, {"text": "Offline inference The dd-CRP model with offline inference procedure (described in \u00a7 3).", "labels": [], "entities": []}, {"text": "Online inference The dd-CRP model with online inference procedure (described in \u00a7 3.1).", "labels": [], "entities": []}, {"text": "For the online inference implementation, we set the size of window and number of iterations to five days and 500 respectively.", "labels": [], "entities": []}, {"text": "For the baseline, the parameter of the Dirichlet prior was set to a vector of 0.5 for each cluster.", "labels": [], "entities": []}, {"text": "These values were chosen through 10-fold cross validation.", "labels": [], "entities": []}, {"text": "To measure the quality of the clusterings obtained by these models, we compare the average weighted and unweighted F-measures for 55 TREC topics, using the evaluation scripts from the TREC TTG task.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.920678973197937}, {"text": "TREC TTG task", "start_pos": 184, "end_pos": 197, "type": "DATASET", "confidence": 0.7953870495160421}]}, {"text": "Overall results are shown in.", "labels": [], "entities": []}, {"text": "The ON-LINE MODEL has the best weighted F1 score, outperforming the offline version of the same model, even though its inference procedure is an approximation to the OFFLINE MODEL.", "labels": [], "entities": [{"text": "ON-LINE MODEL", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.8109334707260132}, {"text": "F1 score", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9813216328620911}, {"text": "OFFLINE", "start_pos": 166, "end_pos": 173, "type": "METRIC", "confidence": 0.8893624544143677}]}, {"text": "It maybe that its approximate inference procedure discourages longrange linkages, thus placing a greater emphasis on the temporal dimension.", "labels": [], "entities": []}, {"text": "Both models were trained over 500 iterations, and the ONLINE MODEL was 30% faster to train than the offline model.", "labels": [], "entities": [{"text": "ONLINE", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.99346923828125}, {"text": "MODEL", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.6684829592704773}]}, {"text": "Compared to the other 2014 TREC TTG systems, our dd-CRP models are competitive.", "labels": [], "entities": [{"text": "TREC TTG", "start_pos": 27, "end_pos": 35, "type": "TASK", "confidence": 0.5193315148353577}]}, {"text": "Both models outperform all but one of the fourteen submissions on the unweighted F 1 metric, and would have placed fourth on the weighted F w 1 metric.", "labels": [], "entities": [{"text": "F 1 metric", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.8746272722880045}]}, {"text": "Note that the TREC evaluation scores both clustering quality and retrieval.", "labels": [], "entities": [{"text": "TREC", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.732576847076416}]}, {"text": "We use only the baseline retrieval model, which achieved a mean average precision of 0.31.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.711614727973938}]}, {"text": "The competing systems shown in all use retrieval models that are far superior: the retrieval model for top-ranked PKUICST team (line 4) achieved a mean average precision (MAP) of 0.59 (, and the QCRI () and and hltcoe () teams (lines 5 and 6) used retrieval models with MAP scores of at least 0.5.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 147, "end_pos": 175, "type": "METRIC", "confidence": 0.9056565264860789}, {"text": "QCRI", "start_pos": 195, "end_pos": 199, "type": "DATASET", "confidence": 0.9479730725288391}]}, {"text": "Bayesian dd-CRP storyline clustering was competitive with these timeline generation systems despite employing afar worse retrieval model, so improving the retrieval model to achieve parity with these alternative systems seems the most straightforward path towards better overall performance.", "labels": [], "entities": [{"text": "dd-CRP storyline clustering", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.5956131120522817}]}], "tableCaptions": [{"text": " Table 1: Performance of Models in the TREC 2014 TTG Task. Weighted recall and F1 are indicated as Rec. w and F w  1 .", "labels": [], "entities": [{"text": "TREC 2014 TTG Task", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.7781806737184525}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9494258165359497}, {"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9978300929069519}, {"text": "Rec. w", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.8681972821553549}, {"text": "F", "start_pos": 110, "end_pos": 111, "type": "METRIC", "confidence": 0.8048574924468994}]}]}