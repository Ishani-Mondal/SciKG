{"title": [{"text": "Citation Analysis with Neural Attention Models", "labels": [], "entities": [{"text": "Citation Analysis", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7114574015140533}]}], "abstractContent": [{"text": "Automated citation analysis (ACA) can be important for many applications including author ranking and literature based information retrieval, extraction, summarization and question answering.", "labels": [], "entities": [{"text": "Automated citation analysis (ACA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8244553407033285}, {"text": "author ranking", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.7116756439208984}, {"text": "information retrieval", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.7046724855899811}, {"text": "summarization", "start_pos": 154, "end_pos": 167, "type": "TASK", "confidence": 0.9880138635635376}, {"text": "question answering", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.8949422240257263}]}, {"text": "In this study, we developed anew compositional attention network (CAN) model to integrate local and global attention representations with a hierarchical attention mechanism.", "labels": [], "entities": []}, {"text": "Training on anew benchmark corpus we built, our evaluation shows that the CAN model performs consistently well on both citation classification and sentiment analysis tasks.", "labels": [], "entities": [{"text": "citation classification and sentiment analysis", "start_pos": 119, "end_pos": 165, "type": "TASK", "confidence": 0.7588509798049927}]}], "introductionContent": [{"text": "Citations are relations between the cited and citing articles and are important content in literature.", "labels": [], "entities": []}, {"text": "There are different reasons that authors choose to cite an article.", "labels": [], "entities": []}, {"text": "Identifying the purpose of the citations has important applications including faceted navigation, citation based information retrieval, impact factor assessment and summarization of scientific papers.", "labels": [], "entities": [{"text": "Identifying the purpose of the citations", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7830637594064077}, {"text": "faceted navigation", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7529405355453491}, {"text": "citation based information retrieval", "start_pos": 98, "end_pos": 134, "type": "TASK", "confidence": 0.5923823863267899}, {"text": "impact factor assessment", "start_pos": 136, "end_pos": 160, "type": "TASK", "confidence": 0.5644466181596121}, {"text": "summarization of scientific papers", "start_pos": 165, "end_pos": 199, "type": "TASK", "confidence": 0.8924032598733902}]}, {"text": "ACA refers to the tasks of citation function classification and citation sentiment analysis.", "labels": [], "entities": [{"text": "citation function classification", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.8176574110984802}, {"text": "citation sentiment analysis", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.9015705386797587}]}, {"text": "Pioneered by, a large body of citation-related studies have been carried out to develop categorization schemes for citation function analysis.", "labels": [], "entities": [{"text": "citation function analysis", "start_pos": 115, "end_pos": 141, "type": "TASK", "confidence": 0.769781490166982}]}, {"text": "However, most of the studies are limited to to specific domain.", "labels": [], "entities": []}, {"text": "The classification schemes are typically complex, containing multiple overlapping categories ranging from three to.", "labels": [], "entities": []}, {"text": "In contrast, the success of ACA depends on a small but well-defined set of citation categories.", "labels": [], "entities": [{"text": "ACA", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9435409307479858}]}, {"text": "developed a semi-ACA based on a 3-category scheme derived from's 15 categories.", "labels": [], "entities": []}, {"text": "Similarly, developed rule-based approaches (cue phrases) to classify citations into one of the four classes (basis, support, limitation and comparison).", "labels": [], "entities": []}, {"text": "addressed citation function classification and sentiment analysis jointly by a hierarchical scheme with the top nodes for sentiment and the leaf nodes for function classes.", "labels": [], "entities": [{"text": "citation function classification", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.811235229174296}, {"text": "sentiment analysis", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.9131276905536652}]}, {"text": "developed a scheme of eight non-overlapping categories for citation function classification in biomedical literatures.", "labels": [], "entities": [{"text": "citation function classification", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.7192531625429789}]}, {"text": "This scheme simplifies's hierarchical overlapping categories.", "labels": [], "entities": []}, {"text": "Recently, a decision-tree based scheme was introduced to facilitate citation context based intelligent systems.", "labels": [], "entities": []}, {"text": "The citation function classes, organic and perfunctory proposed by was adapted fora facet-based classification scheme.", "labels": [], "entities": []}, {"text": "Machine learning (ML) approaches to ACA mainly adapted statistical classifiers including support vector machines (SVM), logistic regression and Nave-Bayes classifier).", "labels": [], "entities": [{"text": "Machine learning (ML)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7501440286636353}, {"text": "ACA", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9459447860717773}]}, {"text": "The feature set extracted includes n-grams, part-of-speech tags, word stems, cue phrases, sentence dependency components, named entity mentions and word and sentence location based features.", "labels": [], "entities": []}, {"text": "Despite the rich linguistically motivated feature sets, ACA remains a challenge, performing significantly worse than human.", "labels": [], "entities": [{"text": "ACA", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9138778448104858}]}, {"text": "One of the reasons for this could be the lack of", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to increase the generalization of data, we maximizes the total number of selected articles.", "labels": [], "entities": []}, {"text": "Specifically, we selected a total of 5,000 citation sentences from 2,500 randomly selected PubMed Central articles (we randomly selected two citation  sentences from each article).", "labels": [], "entities": [{"text": "PubMed Central articles", "start_pos": 91, "end_pos": 114, "type": "DATASET", "confidence": 0.9301401972770691}]}, {"text": "We then developed guidelines and deployed an annotation task in a crowdsourcing platform, Amazon Mechanical Turk (AMT).", "labels": [], "entities": []}, {"text": "Each citation was labeled by five annotators.", "labels": [], "entities": []}, {"text": "We provide the AMT annotators the previous and the next sentences of the citation sentence to enrich the context.", "labels": [], "entities": [{"text": "AMT annotators", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.5701421797275543}]}, {"text": "We designed a quality control (attention check questions) and ended the AMT session if the AMT workers failed to answer correctly the attention check questions.", "labels": [], "entities": [{"text": "AMT session", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.7122212648391724}]}, {"text": "To evaluate the quality of annotation, we asked a domain expert (a MD) to independently annotate 100 citation sentences randomly selected from our corpus and used it as the gold standard to evaluate inter-annotator agreement with the AMT workers.", "labels": [], "entities": []}, {"text": "We built two gold standard datasets to use for training and for evaluation.", "labels": [], "entities": []}, {"text": "The first dataset is composed of labels agreed by at least three of the five annotators (three label matching).", "labels": [], "entities": []}, {"text": "This resulted in 3,422 citations for the function analysis and 3,624 citations for the sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8823712766170502}]}, {"text": "The second dataset is more relaxed in which we selected a label given by the majority of the five annotators.", "labels": [], "entities": []}, {"text": "In this setting, we included a label that may fail inclusion by the first approach.", "labels": [], "entities": []}, {"text": "For example, even if only two annotators agreed on a label, we will include it in our gold standard dataset because it represents a clear majority vote (the rest of three labels all differ).", "labels": [], "entities": []}, {"text": "As a result, this dataset included 4,426 citations for the function classification and 4,423 citations for the sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.8824490308761597}]}, {"text": "During the experiment, citations labeled with don't know were removed from the training data.", "labels": [], "entities": []}, {"text": "Each dataset was split into 200/200/rest for dev/test/train sets with a stratified sampling.", "labels": [], "entities": []}, {"text": "A stratified sampling is performed to preserve percentage of the citations for each class in each set.", "labels": [], "entities": []}, {"text": "We experimented with using only the citation sentence as input example and the expansion with both the previous and the next sentences.", "labels": [], "entities": []}, {"text": "We used ADAM () for optimization of the neural models.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.7685845494270325}]}, {"text": "The size of the LSTM hidden units was set to 200.", "labels": [], "entities": [{"text": "LSTM hidden", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.4998660683631897}]}, {"text": "All neural models were regularized by using 20% input and 30% output dropouts and an l 2 regularizer with strength value 1e-3.", "labels": [], "entities": []}, {"text": "A word2vec (   model trained on a collection of PubMed Central documents transformed citation context to word vectors with size of 200 ().", "labels": [], "entities": [{"text": "PubMed Central documents", "start_pos": 48, "end_pos": 72, "type": "DATASET", "confidence": 0.8857778708140055}]}, {"text": "The parameters of CAN are tied and equal to that of the global attention.", "labels": [], "entities": [{"text": "CAN", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.8807412981987}]}, {"text": "The neural models were trained only on the training set while SVM model was built on both training and development sets.", "labels": [], "entities": []}, {"text": "We use the development set to evaluate the neural models for each epoch to choose the best model.", "labels": [], "entities": []}, {"text": "Each model was given 30 epochs, which was empirically found to be enough time for the models to converge to an optima.", "labels": [], "entities": []}, {"text": "The final performances of the methods were reported on the test set.", "labels": [], "entities": []}, {"text": "The average training time for the neural network models was approximately three hours on a single GPU (GeForce GTX 980).", "labels": [], "entities": [{"text": "GeForce GTX 980", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.8960223197937012}]}, {"text": "lists the detailed statistics of our AMT annotated corpus.", "labels": [], "entities": [{"text": "AMT annotated corpus", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.770423690478007}]}, {"text": "The overall agreement between the expert's annotation and the AMT annotation was 63.1% and 64.7% for function and sentiment analysis tasks.", "labels": [], "entities": [{"text": "AMT", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.5292618870735168}, {"text": "sentiment analysis tasks", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.8323851426442465}]}, {"text": "For the function classification, a majority of citations were annotated as results and findings.", "labels": [], "entities": [{"text": "function classification", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.8729254305362701}]}, {"text": "As shown in, for the sentiment classification, 4.8% was labeled as Negational while 75% and 19.8% were Confirmative and Neutral.", "labels": [], "entities": []}, {"text": "This shows that the citations bias towards a positive statement, resulting a highly unbalanced class distribution.", "labels": [], "entities": []}, {"text": "lists the results of the function classification by using only citation sentences as input to the models.", "labels": [], "entities": [{"text": "function classification", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7218094319105148}]}, {"text": "The SVM baseline obtains the lowest training error.", "labels": [], "entities": [{"text": "training error", "start_pos": 36, "end_pos": 50, "type": "METRIC", "confidence": 0.9733392894268036}]}, {"text": "As the models become complex the performance increases.", "labels": [], "entities": []}, {"text": "However, some cases like the BiLSTMs based global attention model tend to overfit the training data.", "labels": [], "entities": []}, {"text": "The unidirectional LSTMs with global attention achieves the best F1-score in both settings when only the citation sentence is input.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.998971700668335}]}, {"text": "shows the performance where the inputs are represented by a larger context of the previous, citation and next sentences.", "labels": [], "entities": []}, {"text": "We treated the each sentence related to a citation as a subsequence and applied our CAN.", "labels": [], "entities": [{"text": "CAN", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.8814145922660828}]}, {"text": "Here the bi-directional LSTMs with CAN is the clear winner in terms of the test performance.", "labels": [], "entities": [{"text": "CAN", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9073318839073181}]}, {"text": "This model achieves 75.86% F1-score improving the results of the previous model by nearly 7% in the three label matching setup.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9997606873512268}]}, {"text": "Unlike the compositional models, the performance of the global attention models decreased in response to additional context given in the input.", "labels": [], "entities": []}, {"text": "Furthermore, the models tend to get a higher F1-score in the three label matching setup because this setting has an extra annotation noise filter in selecting the gold labels.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9992386102676392}, {"text": "three label matching", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.6945540507634481}]}], "tableCaptions": [{"text": " Table 2: Statistics for the document-level sentiment datasets.", "labels": [], "entities": [{"text": "document-level sentiment datasets", "start_pos": 29, "end_pos": 62, "type": "DATASET", "confidence": 0.649662176767985}]}, {"text": " Table 3: Statistics for our automated citation analysis corpus.", "labels": [], "entities": [{"text": "automated citation analysis", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.637835959593455}]}, {"text": " Table 4: Citation function classification results. Single citation sentence is presented as input.", "labels": [], "entities": [{"text": "Citation function classification", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7550321022669474}]}, {"text": " Table 5: Citation function classification results. Citation sentence + its left and right sentences are used as input.", "labels": [], "entities": [{"text": "Citation function classification", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7476415435473124}]}, {"text": " Table 6: Citation sentiment classification results. Single citation sentence is presented as input.", "labels": [], "entities": [{"text": "Citation sentiment classification", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8835257689158121}]}, {"text": " Table 7: Citation sentiment classification results. Citation sentence + its left and right sentences are used as input.", "labels": [], "entities": [{"text": "Citation sentiment classification", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8908804853757223}]}, {"text": " Table 8: Results of document-level sentiment classification. MSE: mean squared error (lower is better).", "labels": [], "entities": [{"text": "document-level sentiment classification", "start_pos": 21, "end_pos": 60, "type": "TASK", "confidence": 0.8153332471847534}, {"text": "MSE", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.8786135911941528}, {"text": "mean squared error", "start_pos": 67, "end_pos": 85, "type": "METRIC", "confidence": 0.9749504923820496}]}]}