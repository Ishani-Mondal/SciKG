{"title": [{"text": "Towards a Distributional Model of Semantic Complexity", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we introduce for the first time a Distributional Model for computing semantic complexity , inspired by the general principles of the Memory, Unification and Control framework (Hagoort, 2013; Hagoort, 2016).", "labels": [], "entities": []}, {"text": "We argue that sentence comprehension is an incremental process driven by the goal of constructing a coherent representation of the event represented by the sentence.", "labels": [], "entities": []}, {"text": "The composition cost of a sentence depends on the semantic coherence of the event being constructed and on the activation degree of the linguistic constructions.", "labels": [], "entities": []}, {"text": "We also report the results of a first evaluation of the model on the Bicknell dataset (Bicknell et al., 2010).", "labels": [], "entities": [{"text": "Bicknell dataset", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.9851087927818298}]}], "introductionContent": [{"text": "The differences in semantic processing between typical and atypical sentences have recently attracted a lot of attention in experimental linguistics.", "labels": [], "entities": []}, {"text": "Consider the following examples: (1) a.", "labels": [], "entities": []}, {"text": "The musician plays the flute in the theater. b. The gardener plays the castanets in the cave. c. * The nominative plays the global map in the pot.", "labels": [], "entities": []}, {"text": "Since the early work of and the introduction of the notion of acceptability, linguistic theory has mostly focused on the contrast between (1c) and the former two.", "labels": [], "entities": [{"text": "linguistic theory", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7041807174682617}]}, {"text": "The last sentence violates the combinatorial constraints of the lexical items, and that is the reason why, although (1c) is syntactically well-formed, we are notable to build any coherent representation for the situation it expresses.", "labels": [], "entities": []}, {"text": "Investigations on event-related potentials (ERP) 1 brought extensive evidence that sentences like (1a) and (1b), despite being both semantically acceptable, have a different cognitive status: sentences such as (1b), including possible but unexpected combinations of lexemes, evoke stronger N400 components 2 in the ERP waveform than sentences with non-novel combinations, like (1a)(.", "labels": [], "entities": [{"text": "event-related potentials (ERP) 1", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.7052721182505289}]}, {"text": "Although there are different interpretations of the N400 effect, 3 there is general agreement among researchers that it is a brain signature of semantic complexity, that can be reinforced at the syntactic level (the syntactic boost effect; see): novel and unexpected combinations are more complex and require larger cognitive efforts for processing.", "labels": [], "entities": []}, {"text": "An open question is what are the factors determining the semantic complexity of sentence comprehension.", "labels": [], "entities": []}, {"text": "claim that the real issue about compositionality and open-ended productivity is the balance between storage and computation.", "labels": [], "entities": []}, {"text": "Productivity entails that not everything can be stored.", "labels": [], "entities": []}, {"text": "However, the N400 effect suggests that there is a large amount of stored knowledge in semantic memory about event contingencies and concept combinations.", "labels": [], "entities": [{"text": "semantic memory about event contingencies and concept combinations", "start_pos": 86, "end_pos": 152, "type": "TASK", "confidence": 0.6617869585752487}]}, {"text": "This knowledge is triggered by words during processing and affects the expectations on the upcoming input.", "labels": [], "entities": []}, {"text": "Consequently, combinations that are new with respect to the already-stored knowledge require more cognitive efforts to be unified in the semantic memory.", "labels": [], "entities": []}, {"text": "Such effect has been shown at the discourse level by the Dependency Locality Theory, proving that the introduction of new discourse referents is a complexity parameter.", "labels": [], "entities": []}, {"text": "Hagoort has proposed Memory, Unification and Control (MUC) as a general model for sentence comprehension that aims at accounting for such balance between storage and computation.", "labels": [], "entities": [{"text": "Memory, Unification and Control (MUC)", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.6134767681360245}]}, {"text": "The Memory component of the model refers to the linguistic knowledge that is stored in long-term memory.", "labels": [], "entities": []}, {"text": "This includes unification-ready structures corresponding to constructions) represented by sets of constraints pertaining to the various levels of linguistic representation (phonology, syntax, and semantics) for that construction.", "labels": [], "entities": []}, {"text": "Each constraint specifies how a given construction can combine with other constructions at a particular level of linguistic representation, as well as the result of such unification.", "labels": [], "entities": []}, {"text": "The Unification component refers to the assembly of pieces stored in memory into larger structures, with contributions from context.", "labels": [], "entities": []}, {"text": "Unification is a constraint-based process, which attempts to solve the constraints defining the constructions.", "labels": [], "entities": []}, {"text": "Unification operations take place in parallel at all the representation levels.", "labels": [], "entities": []}, {"text": "Therefore, syntax is not the only combinatorial component (cf. also Jackendoff (2002)): constructions are combined into larger structures also at the semantic and phonological levels.", "labels": [], "entities": []}, {"text": "In this paper, we present a computational model of semantic complexity in sentence processing, which is strongly inspired by MUC.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7366279065608978}]}, {"text": "Our model integrates various insights from current research in distributional semantics and recent psycholinguistic findings, which highlight the key role of knowledge about event structure and participants stored in semantic memory and activated during language processing).", "labels": [], "entities": []}, {"text": "Moreover, recent experiments in EEG showed that the activation of the so-called literal' word meanings is only carried out when necessary (.", "labels": [], "entities": []}, {"text": "Following such findings, some recent theoretical proposals argued that words do not really have meaning, they are rather cues to meaning: sentence comprehenders use them to make inferences about the event or the situation that the speaker wants to convey.", "labels": [], "entities": []}, {"text": "In particular, our model relies on the following assumptions: \u2022 long-term semantic memory stores Generalized Event Knowledge (GEK).", "labels": [], "entities": []}, {"text": "GEK includes people's knowledge of typical participants and settings for events); \u2022 at least a (substantial) part of GEK derives from our linguistic experience and can be modeled with distributional information extracted from large parsed corpora.", "labels": [], "entities": []}, {"text": "In this paper, we only focus on this distributional subset of GEK, which we refer to as GEK D ; \u2022 during sentence processing, lexical items (and constructions in general) activate portions of GEK D , which are then unified to form a coherent representation of the event expressed by the sentence.", "labels": [], "entities": []}, {"text": "The aim of this research is to propose a novel distributional semantic framework to model online sentence comprehension.", "labels": [], "entities": []}, {"text": "Our two-fold goal is i.) to build an incremental distributional representation of a sentence, and ii.) to associate a compositional cost to such a representation to model the complexity of semantic processing.", "labels": [], "entities": []}, {"text": "In particular, we argue that semantic complexity depends on two factors: a.) the availability and salience of \"ready-to-use\" event information already stored in GEK D and cued by lexical items, and b.) the cost of unifying activated GEK D into a coherent semantic representation, with the latter depending on the mutual semantic congruence of the events participants.", "labels": [], "entities": []}, {"text": "We thus predict that sentences containing highly familiar lexical combinations like (1a) (musician is in fact a familiar subject of play) are easier to process than sentences expressing novel ones like (1b).", "labels": [], "entities": []}, {"text": "Moreover, the complexity of novel combinations depends on how easily they fit with stored event knowledge.", "labels": [], "entities": []}, {"text": "In the following sections, we will present a global distributional semantic complexity score combining event activation and unification costs.", "labels": [], "entities": []}, {"text": "As a first evaluation of our framework, we will use the semantic complexity score in a difficulty estimation task on the Bicknell dataset ().", "labels": [], "entities": [{"text": "Bicknell dataset", "start_pos": 121, "end_pos": 137, "type": "DATASET", "confidence": 0.9867280721664429}]}], "datasetContent": [{"text": "As a first test for our framework, we measure the semantic complexity of the sentences in the Bicknell dataset ().", "labels": [], "entities": [{"text": "Bicknell dataset", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.9763482213020325}]}, {"text": "The Bicknell dataset was prepared to verify the hypothesis that the typicality of a verb direct object depends on the subject argument.", "labels": [], "entities": [{"text": "Bicknell dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9645924866199493}]}, {"text": "For this purpose, the authors selected 50 verbs, each paired with two agent nouns that altered the scenario evoked by the subject-verb combination.", "labels": [], "entities": []}, {"text": "Plausible patients for each agent-verb pair were obtained by means of production norms, in order to generate triples where the patient was congruent with the agent and with the verb.", "labels": [], "entities": []}, {"text": "For each congruent triple, they also generated an incongruent triple, by combining each verb-congruent patient pair with the other agent noun, in order to have items describing atypical situations.", "labels": [], "entities": []}, {"text": "The final dataset included 100 pairs subject-verb-object triples, that were used to build the sentences fora self-paced reading and for an ERP experiment.", "labels": [], "entities": []}, {"text": "To give an example, experimental subjects were presented with sentence pairs such as: The sentences of each pair contain the same verb and the same object, differing for the subject.", "labels": [], "entities": []}, {"text": "Given the subject, the object is a preferred argument of the verb in the congruent condition, whereas it is an implausible filler in the incongruent condition.", "labels": [], "entities": []}, {"text": "reported that the congruent condition produced shorter reading times and smaller N400 signatures.", "labels": [], "entities": []}, {"text": "Their conclusion was that verb argument expectations are dynamically updated during sentence processing, by integrating some kind of general knowledge about events and their typical participants.", "labels": [], "entities": []}, {"text": "evaluated his model on the ability to assign a higher thematic fit score to the congruent triples than to the incongruent ones.", "labels": [], "entities": []}, {"text": "We interpret Bicknell's experimental data as suggesting that congruent sentences are less semantically complex than incongruent sentences.", "labels": [], "entities": []}, {"text": "Consistently, we predict that our model will assign a higher semantic complexity score to incongruent sentences than to congruent ones.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of hits and accuracy with or without \u03c3 scores. p-values computed with the \u03c7 2 test.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9519654512405396}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9995261430740356}]}]}