{"title": [{"text": "An incremental model of syntactic bootstrapping", "labels": [], "entities": []}], "abstractContent": [{"text": "Syntactic bootstrapping is the hypothesis that learners can use the preliminary syntactic structure of a sentence to identify and characterise the meanings of novel verbs.", "labels": [], "entities": []}, {"text": "Previous work has shown that syntactic bootstrapping can begin using only a few seed nouns (Connor et al., 2010; Connor et al., 2012).", "labels": [], "entities": []}, {"text": "Here, we relax their key assumption: rather than training the model over the entire corpus at once (batch mode), we train the model incre-mentally, thus more realistically simulating a human learner.", "labels": [], "entities": []}, {"text": "We also improve on the verb prediction method by incorporating the assumption that verb assignments are stable overtime.", "labels": [], "entities": [{"text": "verb prediction", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7939122021198273}]}, {"text": "We show that, given a high enough number of seed nouns (around 30), an incremental model achieves similar performance to the batch model.", "labels": [], "entities": []}, {"text": "We also find that the number of seed nouns shown to be sufficient in the previous work is not sufficient under the more realistic incremental model.", "labels": [], "entities": []}, {"text": "The results demonstrate that adopting more realistic assumptions about the early stages of language acquisition can provide new insights without undermining performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important aspect of how children acquire language is how they map lexical units and their combinations to underlying semantic representations.", "labels": [], "entities": []}, {"text": "Syntactic bootstrapping is an account of this aspect of language learning.", "labels": [], "entities": [{"text": "Syntactic bootstrapping", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8306387364864349}]}, {"text": "It is the hypothesis that learners can use the syntactic structure of a sentence to characterise the meanings of novel verbs.", "labels": [], "entities": []}, {"text": "However, the problem remains of how learners first identify verbs, and characterise the syntactic structure of sentences.", "labels": [], "entities": []}, {"text": "One mechanism for resolving this issue is Structure Mapping ( , which hypothesises that, assuming an innate one-to-one mapping between nouns and semantic arguments in an utterance, children are able to use this information to first identify verbs and their arguments, and then assign semantic roles to those arguments.", "labels": [], "entities": [{"text": "Structure Mapping", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7153082340955734}]}, {"text": "In this paper we provide a computational model for this account of syntactic bootstrapping.", "labels": [], "entities": []}, {"text": "We use a system called) that assigns semantic roles to arguments in an utterance -a simplified version of the Semantic Role Labeling Task (SRL;).", "labels": [], "entities": [{"text": "Semantic Role Labeling Task (SRL", "start_pos": 110, "end_pos": 142, "type": "TASK", "confidence": 0.7612242549657822}]}, {"text": "Here, we focus on the preliminary task of identifying nouns and verbs from sentences in a corpus of child-directed speech (the Brown corpus, a subset of the CHILDES database).", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 127, "end_pos": 139, "type": "DATASET", "confidence": 0.8795381188392639}, {"text": "CHILDES database", "start_pos": 157, "end_pos": 173, "type": "DATASET", "confidence": 0.8850947916507721}]}, {"text": "Previous work) presented a model which could identifying noun and verb clusters with minimal supervision (a few seed nouns).", "labels": [], "entities": [{"text": "identifying noun and verb clusters", "start_pos": 45, "end_pos": 79, "type": "TASK", "confidence": 0.7881595373153687}]}, {"text": "However, this model had two substantial limitations: the first was training was done in a batch mode, where the entire dataset was made available to the learner before any predictions were made; the second was that while the noun prediction was aggregated (previously identified known clusters persisted throughout the run through the data), the verb prediction was not (previously identified verb clusters had no effect on future predictions).", "labels": [], "entities": [{"text": "verb prediction", "start_pos": 346, "end_pos": 361, "type": "TASK", "confidence": 0.6925911009311676}]}, {"text": "The current work makes two main advances on the previous work.", "labels": [], "entities": []}, {"text": "Firstly, it addresses the batch mode limitation, adopting a more cognitively plausible approach where all sentences are given to the learner incrementally, more accurately modelling ongoing learning from child-directed speech.", "labels": [], "entities": []}, {"text": "Secondly, it adopts an aggregated approach to verb prediction, as described in section 2.2, which capitalises on the fundamental assump-: Illustration of the noun and verb prediction heuristics.", "labels": [], "entities": [{"text": "verb prediction", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7382620275020599}, {"text": "noun and verb prediction heuristics", "start_pos": 158, "end_pos": 193, "type": "TASK", "confidence": 0.772884464263916}]}, {"text": "The noun heuristic stage receives words assigned to HMM states and a list of seed nouns, and assigns the noun label to states that contain 4 or more seed nouns, assumed to be learned without syntactic help (right-hand side columns show the number of identified seeds and assignment).", "labels": [], "entities": []}, {"text": "The verb heuristic receives a list of noun states per sentence and accumulates counts of co-occurring nouns for each of the non-noun states (right-hand side histograms).", "labels": [], "entities": []}, {"text": "It assigns the verb label to the state with the highest probability of occurring with the number of nouns that appear in the sentence.", "labels": [], "entities": []}, {"text": "tion that distributional clusters will behave in a grammatically consistent fashion (\"once a verb, always a verb\").", "labels": [], "entities": []}, {"text": "describes the heuristics for noun and verb prediction.", "labels": [], "entities": [{"text": "noun and verb prediction", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.7208521217107773}]}, {"text": "Firstly, we model the distributionalbased word categorization with a hidden Markov model (HMM) using 80 states.", "labels": [], "entities": []}, {"text": "We used a Variational Bayes HMM model, trained off-line over a very large corpus of child-directed speech (2.1M tokens).", "labels": [], "entities": []}, {"text": "We then use the method described in to identify which of these HMM states act as arguments (nouns) and predicates (verbs).", "labels": [], "entities": []}, {"text": "As in the original work, we also give the HMM a number of function words as identified by their part-of-speech tags in order to be clustered into separate reserved states.", "labels": [], "entities": []}, {"text": "This represents (but does not model explicitly) the assumption that infant learners can identify function words based on a variety of cues, including linguistic context, prosody, and frequency.", "labels": [], "entities": []}, {"text": "Note also, that the list of function words was given to the HMM during training and not during the tagging of the BabySRL corpus.", "labels": [], "entities": [{"text": "BabySRL corpus", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.9681066870689392}]}, {"text": "This means that for this corpus, the HMM is using the same distributional statistics as for the content words to decide on the function-word state membership.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}