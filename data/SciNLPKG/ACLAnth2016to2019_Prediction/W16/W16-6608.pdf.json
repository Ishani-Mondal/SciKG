{"title": [{"text": "Abstractive Compression of Captions with Attentive Recurrent Neural Networks", "labels": [], "entities": [{"text": "Abstractive Compression of Captions with Attentive Recurrent Neural Networks", "start_pos": 0, "end_pos": 76, "type": "TASK", "confidence": 0.6218029492431216}]}], "abstractContent": [{"text": "In this paper we introduce the task of abstrac-tive caption or scene description compression.", "labels": [], "entities": [{"text": "abstrac-tive caption", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.627387210726738}, {"text": "scene description compression", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.7098690668741862}]}, {"text": "We describe a parallel dataset derived from the FLICKR30K and MSCOCO datasets.", "labels": [], "entities": [{"text": "FLICKR30K", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.911702036857605}, {"text": "MSCOCO datasets", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.9276042282581329}]}, {"text": "With this data we train an attention-based bidirec-tional LSTM recurrent neural network and compare the quality of its output to a Phrase-based Machine Translation (PBMT) model and a human generated short description.", "labels": [], "entities": [{"text": "Phrase-based Machine Translation (PBMT)", "start_pos": 131, "end_pos": 170, "type": "TASK", "confidence": 0.7731348474820455}]}, {"text": "An extensive evaluation is done using automatic measures and human judgements.", "labels": [], "entities": []}, {"text": "We show that the neural model outperforms the PBMT model.", "labels": [], "entities": []}, {"text": "Additionally, we show that automatic measures are not very well suited for evaluating this text-to-text generation task.", "labels": [], "entities": [{"text": "text-to-text generation task", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.7966813941796621}]}], "introductionContent": [{"text": "Text summarization is an important, yet challenging subfield of Natural Language Processing.", "labels": [], "entities": [{"text": "Text summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8160265684127808}, {"text": "Natural Language Processing", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.6380459666252136}]}, {"text": "Summarization can be defined as the process of finding the important items in a text and presenting them in a condensed form).", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9845536947250366}]}, {"text": "Summarization on the sentence level is called sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7428607493638992}]}, {"text": "Sentence compression approaches can be classified into two categories: extractive and abstractive sentence compression.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9197137951850891}, {"text": "abstractive sentence compression", "start_pos": 86, "end_pos": 118, "type": "TASK", "confidence": 0.6667760908603668}]}, {"text": "Most successful sentence compression models consist of extractive approaches that select the most relevant fragments from the source document and generate a shorter representation of this document by stitching the selected fragments together.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7564108669757843}]}, {"text": "In contrast, abstractive sentence compression is the process of producing a representation of the original sentence in a bottom-up manner.", "labels": [], "entities": [{"text": "abstractive sentence compression", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.61385511358579}]}, {"text": "This results in a summary that may contain fragments that do not appear as part of the source sentence.", "labels": [], "entities": []}, {"text": "While extractive sentence compression is an easier task, the challenges in abstractive sentence compression have gained more and more attention in recent years).", "labels": [], "entities": [{"text": "extractive sentence compression", "start_pos": 6, "end_pos": 37, "type": "TASK", "confidence": 0.6876738568147024}, {"text": "abstractive sentence compression", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.671543171008428}]}, {"text": "Extractive sentence compression entails finding a subset of words in the source sentence that can be dropped to create anew, shorter sentence that is still grammatical and contains the most important information.", "labels": [], "entities": [{"text": "Extractive sentence compression", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8057469129562378}]}, {"text": "More formally, the aim is to shorten a sentence x = x 1 , x 2 , ..., x n into a substring y = y 1 , y 2 , ..., y m where all words in y also occur in x in the same order and m < n.", "labels": [], "entities": []}, {"text": "A number of techniques have been used for extractive sentence compression, ranging from the noisy-channel model), large-margin learning to Integer Linear Programming.) characterize these approaches in terms of two assumptions: (1) only word deletions are allowed and (2) the word order is fixed.", "labels": [], "entities": [{"text": "extractive sentence compression", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6872654259204865}]}, {"text": "They argue that these constraints rule out more complicated operations such as reordering, substitution and insertion, and reduce the sentence compression task to a word deletion task.", "labels": [], "entities": [{"text": "substitution and insertion", "start_pos": 91, "end_pos": 117, "type": "TASK", "confidence": 0.7308328847090403}, {"text": "sentence compression", "start_pos": 134, "end_pos": 154, "type": "TASK", "confidence": 0.734358087182045}]}, {"text": "This does not model human sentence compression accurately, as humans tend to paraphrase when summarizing), resulting in an abstractive compression of the source sentence. with RNNs.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7213705480098724}]}, {"text": "In order to be applied to sentence compression, RNNs typically need to be trained on large data sets of aligned sequences.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7693527042865753}]}, {"text": "In the domain of abstractive sentence compression, not many of such data sets are available.", "labels": [], "entities": [{"text": "abstractive sentence compression", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.6341160237789154}]}, {"text": "For the related task of sentence simplification, data sets are available of aligned sentences from Wikipedia and Simple Wikipedia (.", "labels": [], "entities": [{"text": "sentence simplification", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.7153798192739487}, {"text": "Simple Wikipedia", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.8101981282234192}]}, {"text": "Recently, () used the Gigaword corpus to construct a large corpus containing headlines paired with the article's first sentence.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.961643397808075}]}, {"text": "Here, we present a data set compiled from scene descriptions taken from the MSCOCO dataset ().", "labels": [], "entities": [{"text": "MSCOCO dataset", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.9744091331958771}]}, {"text": "These descriptions are generally only one sentence long, and humans tend to describe photos in different ways, which makes this task suitable for abstractive sentence compression.", "labels": [], "entities": [{"text": "abstractive sentence compression", "start_pos": 146, "end_pos": 178, "type": "TASK", "confidence": 0.6195056339104971}]}, {"text": "For each image, we align long descriptions with shorter descriptions to construct a corpus of abstractive compressions . We employ an Attentive Recurrent Neural Network (aRNN) to the task of sentence compression and compare its output with a Phrase-based Machine Translation (PBMT) system (Moses) and a human compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 191, "end_pos": 211, "type": "TASK", "confidence": 0.7413960248231888}, {"text": "Phrase-based Machine Translation (PBMT)", "start_pos": 242, "end_pos": 281, "type": "TASK", "confidence": 0.7962498267491659}]}, {"text": "We show through extensive automatic and human evaluation that the aRNN outperforms the Moses system and even performs on par with the human generated description.", "labels": [], "entities": [{"text": "aRNN", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9030596613883972}]}, {"text": "We also show that automatic measures such as ROUGE that are used generally to evaluate compression tasks do not correlate with human judgements.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9859819412231445}]}], "datasetContent": [{"text": "Here we describe the experiment we performed in order to evaluate our models.", "labels": [], "entities": []}, {"text": "To  First, we perform automatic evaluation using regular summarization and text generation evaluation metrics, such as BLEU (), which is generally used for Machine Translation and variants of ROUGE, which is generally used for summarization evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9988861680030823}, {"text": "Machine Translation", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.8295577764511108}, {"text": "ROUGE", "start_pos": 192, "end_pos": 197, "type": "METRIC", "confidence": 0.912412166595459}, {"text": "summarization evaluation", "start_pos": 227, "end_pos": 251, "type": "TASK", "confidence": 0.9652583003044128}]}, {"text": "Both take into account reference sentences and calculate overlap on the n-gram level.", "labels": [], "entities": [{"text": "overlap", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9806616306304932}]}, {"text": "ROUGE also accounts for compression.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9249446392059326}]}, {"text": "ROUGE 1-4 take into account unigrams up to four-grams and ROUGE SU4 also takes into account skipgrams.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7346229553222656}, {"text": "ROUGE", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.7434865236282349}]}, {"text": "For BLEU we use multi-bleu.pl, and for ROUGE we used pyrouge.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9931076169013977}, {"text": "ROUGE", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9119052290916443}]}, {"text": "We also compute compression rate on the character level, as this tells us how much the source sentence has been compressed.", "labels": [], "entities": [{"text": "compression rate", "start_pos": 16, "end_pos": 32, "type": "METRIC", "confidence": 0.9395785927772522}]}, {"text": "We simply compute this by dividing the number of characters in the target sentence by the number of characters in the source sentence.", "labels": [], "entities": []}, {"text": "We call this measure Character Compression Rate (CCR).", "labels": [], "entities": [{"text": "Character Compression Rate (CCR)", "start_pos": 21, "end_pos": 53, "type": "METRIC", "confidence": 0.8485616942246755}]}, {"text": "Besides those measures, we additionally compute Source BLEU, which is the BLEU score of the output sentence if we take the source sentence as reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.6383844017982483}, {"text": "BLEU score", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9813933670520782}]}, {"text": "This tells us something about how similar the sentence is compared to the source, or in other words, how aggressively the system had transformed the sentence.", "labels": [], "entities": []}, {"text": "In order to gain more insight in the quality of the generated compressions we let human subjects rate the generated compressions.", "labels": [], "entities": []}, {"text": "Because we can only compare compressions in a meaningful way if the compression rates are similar (Napoles et al., 2011), we selected only those cases with rougly equal character compression rate (we limited this by selecting within a 0.1 CCR resolution).", "labels": [], "entities": []}, {"text": "From this selection, we randomly selected 30 source sentences with their corresponding system outputs and one short human description which served as the human compression.", "labels": [], "entities": []}, {"text": "We used Crowdflower 6 to perform the evaluation study.", "labels": [], "entities": [{"text": "Crowdflower 6", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.8783765733242035}]}, {"text": "CrowdFlower is a platform for data annotation by the crowd.", "labels": [], "entities": [{"text": "data annotation", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.693851575255394}]}, {"text": "We allowed only native English speakers with a trust level of minimally 90 percent to partcipate.", "labels": [], "entities": []}, {"text": "Following earlier evaluation studies) we asked 25 participants to evaluate Fluency and Importance of the target compressions on a seven point Likert scale.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9993521571159363}, {"text": "Importance", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9945665597915649}]}, {"text": "Fluency was defined in the instructions as the extent to which a sentence is in proper, grammatical English.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8817778825759888}]}, {"text": "Importance was defined as the extent to which the sentence has retained the important information from the source sentence.", "labels": [], "entities": [{"text": "Importance", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8706300258636475}]}, {"text": "The order of the output of the various systems was randomized.", "labels": [], "entities": []}, {"text": "The participants saw 30 source descriptions and for each source description they evaluated all three compressions: the aRNN, Moses and Human compression.", "labels": [], "entities": []}, {"text": "They were asked to rate the Importance and Fluency of each compression on a seven point scale with 1 being very bad and 7 very good.", "labels": [], "entities": [{"text": "Importance", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9992780089378357}, {"text": "Fluency", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9940430521965027}]}], "tableCaptions": [{"text": " Table 1: Parameters used in the aRNN model", "labels": [], "entities": [{"text": "aRNN", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.7147236466407776}]}, {"text": " Table 3: Character compression rates and similarity to the", "labels": [], "entities": [{"text": "Character compression", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7963981926441193}, {"text": "similarity", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9835218191146851}]}, {"text": " Table 4: BLEU and ROUGE scores", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992436170578003}, {"text": "ROUGE", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9948242902755737}]}]}