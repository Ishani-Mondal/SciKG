{"title": [{"text": "Content Selection through Paraphrase Detection: Capturing different Semantic Realisations of the Same Idea", "labels": [], "entities": [{"text": "Content Selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7148194015026093}, {"text": "Paraphrase Detection", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8791666924953461}]}], "abstractContent": [], "introductionContent": [{"text": "Summarisation can be seen as an instance of Natural Language Generation (NLG), where \"what to say\" corresponds to the identification of relevant information, and \"how to say it\" would be associated to the final creation of the summary.", "labels": [], "entities": [{"text": "Summarisation", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9802954792976379}, {"text": "Natural Language Generation (NLG)", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.8152335286140442}]}, {"text": "When dealing with data coming from the Semantic Web (e.g., RDF triples), the challenge of how a good summary can be produced arises.", "labels": [], "entities": []}, {"text": "For instance, having the RDF properties from an infobox of a Wikipedia page, how could a summary expressed in natural language text be generated? and how could this summary sound as natural as possible (i.e., bean abstractive summary) far from only being a bunch of selected sentences output together (i.e., extractive summary)?", "labels": [], "entities": []}, {"text": "This would imply to be able to successfully map the RDF information to a semantic representation of natural language sentences (e.g., predicate-argument (pred-arg) structures).", "labels": [], "entities": []}, {"text": "Towards the long-term objective of generating abstractive summaries from Semantic Web data, the specific goal of this paper is to propose and validate an approach to map linguistic structures that can encode the same meaning but with different words (e.g., sentence-to-sentence, predarg-to-pred-arg, RDF-to-TEXT) using continuous semantic representation of text.", "labels": [], "entities": []}, {"text": "The idea is to decide the level of document representation to work with; convert the text into that representation; and perform a pairwise comparison to decide to what extent two pairs can be mapped or not.", "labels": [], "entities": []}, {"text": "For achieving this, different methods were analysed, including traditional Wordnet-based ones, as well as more recent ones based on word embeddings.", "labels": [], "entities": [{"text": "Wordnet-based", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.9630630016326904}]}, {"text": "Our approach was tested and validated in the context of document-abstract sentence mapping to check whether it was appropriate for identifying important information.", "labels": [], "entities": [{"text": "document-abstract sentence mapping", "start_pos": 56, "end_pos": 90, "type": "TASK", "confidence": 0.6803630391756693}]}, {"text": "The results obtained good performance, thus indicating that we can rely on the approach and apply it to further contexts (e.g., mapping RDFs into natural language).", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows: Section 2 outlines related work.", "labels": [], "entities": []}, {"text": "Section 3 explains the proposed approach for mapping linguistic units.", "labels": [], "entities": []}, {"text": "Section 4 describes our dataset and experiments.", "labels": [], "entities": []}, {"text": "Section 5 provides the results and discussion.", "labels": [], "entities": []}, {"text": "Finally, Section 6 draws the main conclusions and highlights possible futures directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The English training collection of documents and abstracts from the Single document Summarization task (MSS) 4 of the MultiLing2015 was used as corpus.", "labels": [], "entities": [{"text": "English training collection of documents", "start_pos": 4, "end_pos": 44, "type": "DATASET", "confidence": 0.7442408323287963}, {"text": "Single document Summarization task (MSS)", "start_pos": 68, "end_pos": 108, "type": "TASK", "confidence": 0.7460504599979946}]}, {"text": "It consisted of 30 Wikipedia documents from heterogeneous topics (e.g., history of Texas University, fauna of Australia, or Magic Johnson) and their abstracts, which corresponded to the introductory paragraphs of the Wikipedia page.", "labels": [], "entities": []}, {"text": "Documents were rather long, having 3,972 words on average (the longest document had 8,348 words and the shortest 2,091), whereas abstracts were 274 words on average (the maximum value was 305 words and the minimum 243), thus resulting in a very low compression ratio 5 -around 7%.", "labels": [], "entities": []}, {"text": "For carrying out the experiments, our approach receives document-abstract pairs as input.", "labels": [], "entities": []}, {"text": "These correspond to the source documents, as well as the abstracts associated to those documents.", "labels": [], "entities": []}, {"text": "Following the stages defined in Section 3, both were segmented in sentences, and the pred-arg structures were automatically identified using SENNA semantic role labeller . Different configurations were tested as far as the WE and the similarity metrics were concerned for the second and third stages.", "labels": [], "entities": [{"text": "WE", "start_pos": 223, "end_pos": 225, "type": "DATASET", "confidence": 0.43957483768463135}]}, {"text": "For representing either sentences or pred-arg structures, GLoVe pre-trained WE vectors () were used, specifically the ones derived from Wikipedia 2014 + Gigaword 5 corpora, containing around 6 billion tokens; and the ones derived from a Common Crawl, with 840 billion tokens.", "labels": [], "entities": [{"text": "Wikipedia 2014 + Gigaword 5 corpora", "start_pos": 136, "end_pos": 171, "type": "DATASET", "confidence": 0.8607131640116373}]}, {"text": "Regarding the similarity metrics, Wordnet-based metrics included the shortest path between synsets, Leacock-Chodorow similarity, Wu-Palmer similarity, Resnik similarity, Jiang-Conrath similarity, and Lin similarity, all of them implemented in NLTK 7 . For the WE settings, the similarity metrics were computed on the basis of the cosine similarity and the Euclidean distance.", "labels": [], "entities": [{"text": "Lin similarity", "start_pos": 200, "end_pos": 214, "type": "METRIC", "confidence": 0.8908693790435791}, {"text": "WE", "start_pos": 260, "end_pos": 262, "type": "TASK", "confidence": 0.8210926651954651}]}, {"text": "These latter metrics were applied upon the two composition methods for sentence embedding representations: addition and product, as described in).", "labels": [], "entities": []}, {"text": "In the end, a total of 38 distinct configurations were obtained.", "labels": [], "entities": []}, {"text": "We addressed the validation of the source document-abstract pairs mapping as an extrinsic task using ROUGE).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.8838358521461487}]}, {"text": "ROUGE is a wellknown tool employed for summarisation evaluation, which computes the n-gram overlapping between an automatic and a reference summary in terms of n-grams (unigrams -ROUGE 1; bigrams -ROUGE 2, etc.).", "labels": [], "entities": [{"text": "summarisation evaluation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.9838397204875946}]}, {"text": "Our assumption behind this type of evaluation was that considering the: Results (in percentages) for the extrinsic validation of the mapping.", "labels": [], "entities": []}, {"text": "source document snippets of the top-ranked mapping pairs, and directly building a summary with them (i.e., an extractive summary), good ROUGE results should be obtained if the mapping was good enough.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 136, "end_pos": 141, "type": "METRIC", "confidence": 0.9618045091629028}]}, {"text": "reports the most relevant results obtained.", "labels": [], "entities": []}, {"text": "As baselines, we considered the ROUGE direct comparison between the sentences (or predarg structures) of the source document and the ones in the abstract (TEXT baseline, and PRED-ARG baseline, respectively).", "labels": [], "entities": []}, {"text": "We report the results for ROUGE-1, ROUGE-2 and ROUGE-SU4 . The results obtained show that representing the semantics of a sentence or pred-arg structure using WE leads to the best results, improving those from traditional WordNet-based similarity metrics.", "labels": [], "entities": []}, {"text": "The best approach for the WE configuration corresponds to the addition composition method with cosine similarity, and using the pretrained WE derived from Wikipedia+GigaWord.", "labels": [], "entities": [{"text": "WE", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9015578627586365}]}, {"text": "Compared to the state of the art in summarisation, the results with WE are also encouraging, since previous published results with the same corpus) are close to 44% (Fmeasure for ROUGE-1).", "labels": [], "entities": [{"text": "summarisation", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9864330887794495}, {"text": "WE", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.4263046085834503}, {"text": "Fmeasure", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.947568953037262}, {"text": "ROUGE-1", "start_pos": 179, "end_pos": 186, "type": "METRIC", "confidence": 0.8618946075439453}]}, {"text": "Concerning the comparison between whether using the whole text with respect to only using the pred-arg structures, the former gets better results.", "labels": [], "entities": []}, {"text": "This is logical since the more text to compare, the higher chances to obtain similar ngrams when evaluating with ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9613202214241028}]}, {"text": "However, this also limits the capability of abstractive summarisation systems, since we would end up with selecting the sentences as they are, thus restricting the method to purely extractive.", "labels": [], "entities": []}, {"text": "Nevertheless, the results obtained by the use of pred-arg structures are still reasonably acceptable, and this type of structure would allow to generalise the key content to be selected that should be later rephrased in a proper sentence, producing an abstractive sum-mary.", "labels": [], "entities": []}, {"text": "Next, we provide the top 3 best pair alignments (source document-abstract) of the highest performing configuration using pred-arg structure as examples.", "labels": [], "entities": []}, {"text": "The value in brackets mean the similarity percentage obtained by our approach.", "labels": [], "entities": [{"text": "similarity percentage", "start_pos": 31, "end_pos": 52, "type": "METRIC", "confidence": 0.979517787694931}]}, {"text": "protected areas -protected areas (100%) the insects comprising 75% of Australia 's known species of animals -The fauna of Australia consists of a huge variety of strange and unique animals ; some 83% of mammals, 89% of reptiles, 90% offish and insects (99.94%) European settlement , direct exploitation of native faun , habitat destruction and the introduction of exotic predators and competitive herbivores led to the extinction of some 27 mammal, 23 bird and 4 frog species.", "labels": [], "entities": []}, {"text": "-Hunting, the introduction of non-native species, and land -management practices involving the modification or destruction of habitats led to numerous extinctions (99.93%) Finally, our intuition behind the results obtained (maximum values of 50%) is that not all the information in the abstract can be mapped with the information of the source document, indicating that a proper abstract may contain extra information that provides from the world knowledge of its author.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results (in percentages) for the extrinsic validation of the mapping.", "labels": [], "entities": []}]}