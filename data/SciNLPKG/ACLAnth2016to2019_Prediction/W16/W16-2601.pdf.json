{"title": [{"text": "Automatic Classification by Topic Domain for Meta Data Generation, Web Corpus Evaluation, and Corpus Comparison", "labels": [], "entities": [{"text": "Meta Data Generation", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.6845911145210266}, {"text": "Web Corpus Evaluation", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.5955834686756134}, {"text": "Corpus Comparison", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.6597591787576675}]}], "abstractContent": [{"text": "In this paper, we describe preliminary results from an ongoing experiment wherein we classify two large unstructured text corpora-a web corpus and a newspaper corpus-by topic domain (or subject area).", "labels": [], "entities": []}, {"text": "Our primary goal is to develop a method that allows for the reliable annotation of large crawled web corpora with metadata required by many corpus linguists.", "labels": [], "entities": []}, {"text": "We are especially interested in designing an annotation scheme whose categories are both intuitively interpretable by linguists and firmly rooted in the distribution of lexical material in the documents.", "labels": [], "entities": []}, {"text": "Since we use data from a web corpus and a more traditional corpus, we also contribute to the important field of corpus comparison and corpus evaluation.", "labels": [], "entities": [{"text": "corpus comparison", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7518952190876007}, {"text": "corpus evaluation", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.7724149823188782}]}, {"text": "Technically, we use (unsupervised) topic modeling to automatically induce topic distributions over gold standard corpora that were manually annotated for 13 coarse-grained topic domains.", "labels": [], "entities": []}, {"text": "Ina second step, we apply supervised machine learning to learn the manually annotated topic domains using the previously induced topics as features.", "labels": [], "entities": []}, {"text": "We achieve around 70% accuracy in 10-fold cross validations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9995260238647461}]}, {"text": "An analysis of the errors clearly indicates, however, that a revised classification scheme and larger gold standard corpora will likely lead to a substantial increase inaccuracy.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the experiment reported here, we classified large unstructured text corpora by topic domain.", "labels": [], "entities": []}, {"text": "The topic domain of a document-along with other high-level classifications such as genre or register-is among the types of metadata most essential to many corpus linguists.", "labels": [], "entities": []}, {"text": "Therefore, the lack of reliable metadata in general is often mentioned as a major drawback of large, crawled web corpora, and the automatic generation of such metadata is an active field of research.", "labels": [], "entities": []}, {"text": "It must be noted, however, that such high-level annotations are not reliably available for many very large traditional corpora (such as newspaper corpora), either.", "labels": [], "entities": []}, {"text": "When it comes to the automatic identification of high-level categories like register (such as Opinion, Narrative, Informational Persuation; Biber and Egbert 2016), even very recent approaches based on very large amounts of training data cannot deliver satisfying (arguably not even encouraging) results.", "labels": [], "entities": [{"text": "automatic identification of high-level categories", "start_pos": 21, "end_pos": 70, "type": "TASK", "confidence": 0.7189368486404419}]}, {"text": "For instance,) report accuracy=0.421, precision=0.268, recall=0.3.", "labels": [], "entities": [{"text": "report", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9848734140396118}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8705540895462036}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9996200799942017}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9997023940086365}]}, {"text": "It is not even clear whether categories such as register and genre can be operationalized such that a reliable annotation is possible for humans.", "labels": [], "entities": []}, {"text": "By contrast, automatic text categorization by content yielded much more promising results years ago already.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.6709234416484833}]}, {"text": "Furthermore, data-driven induction of topics (topic modeling) has proven quite successful, and it is in many respects a very objective way of organizing a collection of documents by content.", "labels": [], "entities": [{"text": "data-driven induction of topics (topic modeling", "start_pos": 13, "end_pos": 60, "type": "TASK", "confidence": 0.6696810935224805}]}, {"text": "Deriving topic classifications from text-internal criteria is also advocated in the EAGLES (1996) guidelines, among others.", "labels": [], "entities": [{"text": "topic classifications", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.6597721576690674}, {"text": "EAGLES (1996)", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.8131106346845627}]}, {"text": "However, topic modeling usually does not come with category labels that are useful for linguistic corpus users.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.885146975517273}]}, {"text": "In our project, we explore the possibility of inferring a small, more traditional set of topic domains (or subject areas) from the topics induced in an unsupervised manner by Latent Semantic Indexing ().", "labels": [], "entities": []}, {"text": "Since we classify and compare one large German web corpus and one large German newspaper corpus with respect to their distribution of topic domains, our paper also contributes to the area of corpus comparison, another important issue in corpus linguistics.", "labels": [], "entities": [{"text": "corpus comparison", "start_pos": 191, "end_pos": 208, "type": "TASK", "confidence": 0.6950012743473053}, {"text": "corpus linguistics", "start_pos": 237, "end_pos": 255, "type": "TASK", "confidence": 0.7998224794864655}]}, {"text": "For the construction of crawled web corpora, such comparisons are vital because next to nothing is known about their composition.", "labels": [], "entities": []}, {"text": "The computational tools used in our method (unsupervised topic induction and supervised classifiers) are by now well-established and highly developed.", "labels": [], "entities": []}, {"text": "This paper contributes to the field of applying such methods and making them usable for real-life problems of data processing and the development of suitable annotation schemes rather than to the development of the underlying mathematics and algorithms.", "labels": [], "entities": [{"text": "data processing", "start_pos": 110, "end_pos": 125, "type": "TASK", "confidence": 0.7517163753509521}]}], "datasetContent": [{"text": "Our general approach was to infer a topic distribution over a corpus using unsupervised topic modeling algorithms as a first step.", "labels": [], "entities": []}, {"text": "In the second step, rather than examining and interpreting the inferred topical structure, we used the resulting documenttopic matrix to learn topic domain distinctions for the documents from their assignment to the topics in a supervised manner.", "labels": [], "entities": []}, {"text": "To achieve this, supervised classifiers were used.", "labels": [], "entities": []}, {"text": "Through permutation of virtually all available classifiers (with the appropriate capabilities) available in the Weka toolkit (), LM Trees () and SVMs with a Pearson VII kernel) were found to be most accurate.", "labels": [], "entities": [{"text": "Weka toolkit", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.9633193612098694}, {"text": "Pearson VII kernel", "start_pos": 157, "end_pos": 175, "type": "DATASET", "confidence": 0.8330316742261251}]}, {"text": "Due to minimally higher accuracy, SVMs were used in all subsequent experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9988511800765991}, {"text": "SVMs", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.8731882572174072}]}, {"text": "Some topic domains occurred only rarely in the gold standard, and we did not expect the classifier to be able to generalize well from just a few instances.", "labels": [], "entities": []}, {"text": "Therefore, we evaluated the results on the full data set and a reduced data set with rare categories removed.", "labels": [], "entities": []}, {"text": "For the first step (unsupervised topic induction), we used LSI and LDA (Latent Dirichlet Allocation,) as implemented in the Gensim toolkit.", "labels": [], "entities": [{"text": "topic induction", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.6992229521274567}, {"text": "LSI", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.966494083404541}, {"text": "LDA", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9186198711395264}]}, {"text": "In our first experiments, the LDA topic distribution was unstable, and results were generally unusable, possibly due to the comparatively small gold standard corpora used.", "labels": [], "entities": []}, {"text": "We consequently only report LSI results here and will return to LDA in further experiments (cf. Section 5).", "labels": [], "entities": [{"text": "LSI", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.7449541687965393}, {"text": "LDA", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.4296320676803589}]}, {"text": "However, for any topic modeling algorithm, our corpora can be considered small.", "labels": [], "entities": []}, {"text": "Therefore, we inferred topics not just based on the annotated gold standard data sets, but also on larger datasets which consisted of the gold standard mixed with additional documents from the source corpora.", "labels": [], "entities": [{"text": "gold standard data sets", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.8297872468829155}]}, {"text": "For the training of the SVM classifiers, the documents that had been mixed in were removed again because no gold standard annotation was available for them.", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.5647439360618591}]}, {"text": "We systematically increased the number of mixed-in document in increments of roughly half as many documents as contained in the gold standard corpora.", "labels": [], "entities": []}, {"text": "We pre-processed both corpora in exactly the same way (tokenization, lemmatization, POStagging, named entity recognition).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.6574164927005768}]}, {"text": "Using the lemma and the simplified POS tags (such as kindergarten nn) as terms in combination with some filters (use only lower-cased purely alphabetic common and proper noun lemmas between 4 and 30 characters long) usually gave the best results.", "labels": [], "entities": []}, {"text": "shows the classification accuracy using 20 to 90 LSI topics.", "labels": [], "entities": [{"text": "classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9428097009658813}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9417529702186584}]}, {"text": "Each line corresponds to one sub-experiment (with slightly different preprocessing options for lines of the same color and style), and the lines form well distinguishable bands.", "labels": [], "entities": []}, {"text": "The highest accuracy is achieved with the reduced set of topic domains (minor categories removed) when the evaluation is performed on the training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9992836117744446}]}, {"text": "The full set of topic domains leads to a drop inaccuracy of about 5%.", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.8000661730766296}]}, {"text": "The two lower bands show the classification accuracy in a 10-fold cross-validation (10CV), again with the reduced set of topic domains performing roughly 5% better.", "labels": [], "entities": [{"text": "classification", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.9106713533401489}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9819889068603516}]}, {"text": "While a higher number of topics improves results on the training data, the accuracy in the crossvalidation drops.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.99960857629776}]}, {"text": "Too large numbers of topics obviously allow the method to pickup idiosyncratic features of single documents or very small clusters of documents, leading to extreme overfitting.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation at best achievable accuracy with the reduced set of topic domains in 10-fold cross- validation (  *  weighted average across all categories)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9454061985015869}]}]}