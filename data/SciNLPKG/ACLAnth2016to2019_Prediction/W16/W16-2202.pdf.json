{"title": [{"text": "Improving Pronoun Translation by Modeling Coreference Uncertainty", "labels": [], "entities": [{"text": "Improving Pronoun Translation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8857450683911642}, {"text": "Modeling Coreference Uncertainty", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.7403680682182312}]}], "abstractContent": [{"text": "Information about the antecedents of pronouns is considered essential to solve certain translation divergencies, such as those concerning the English pronoun it when translated into gendered languages, e.g. for French into il, elle, or several other options.", "labels": [], "entities": []}, {"text": "However, no machine translation system using anaphora resolution has so far been able to outperform a phrase-based statistical MT baseline.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7550486624240875}, {"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.7261756658554077}]}, {"text": "We address here one of the reasons for this failure: the imperfection of automatic anaphora resolution algorithms.", "labels": [], "entities": [{"text": "automatic anaphora resolution", "start_pos": 73, "end_pos": 102, "type": "TASK", "confidence": 0.6619357268015543}]}, {"text": "Using parallel data, we learn probabilistic correlations between target-side pronouns and the gender and number features of their (uncertain) antecedents , as hypothesized by the Stan-ford Coreference Resolution system on the source side.", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 189, "end_pos": 211, "type": "TASK", "confidence": 0.6095606982707977}]}, {"text": "We embody these correlations into a secondary translation model, which we invoke upon decoding with the Moses statistical phrase-based MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 135, "end_pos": 137, "type": "TASK", "confidence": 0.8419162631034851}]}, {"text": "This solution outperforms a deterministic pronoun post-editing system, as well as a statistical MT baseline, on automatic and human evaluation metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9456766843795776}]}], "introductionContent": [{"text": "Pronoun translation remains a challenge for machine translation (MT), likely because solving certain translation divergencies between source and target pronouns requires non-local information, possibly from one or more sentences before the one that is being translated.", "labels": [], "entities": [{"text": "Pronoun translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9104092419147491}, {"text": "machine translation (MT)", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.8516529381275177}]}, {"text": "In this paper, we focus on the divergencies that occur when translating the English neutral pronouns it and they into French.", "labels": [], "entities": []}, {"text": "Depending on their functions (referential or pleonastic) and on their actual antecedents, Source: My cat brought home a mouse that he hunted, and it 1 was not dead but it 2 was mortally wounded.", "labels": [], "entities": []}, {"text": "What is the best way to kill it 3 humanely?", "labels": [], "entities": []}, {"text": "MT: Mon chat a ramen\u00e9ramen\u00e9`ramen\u00e9\u00e0 la maison une souris qui il a chass\u00e9, et il 1 \u00b4 etait pas mort, mais il 2 a \u00b4 et\u00e9 mortellement bless\u00e9.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.49029091000556946}]}, {"text": "Quelle est la meilleure fa\u00e7on de le 3 tuer humainement?", "labels": [], "entities": []}, {"text": "there are almost twenty different lexical items that can serve as translations into French, e.g. for it: il, elle, ce/c', cela, c \u00b8a, on, le, and others.", "labels": [], "entities": []}, {"text": "For instance, in an example from an online discussion forum shown in, two referents are mentioned, a cat and a mouse, which are translated in French by nouns with different genders: masculine for cat (le chat) vs. feminine for mouse (la souris).", "labels": [], "entities": []}, {"text": "The three instances of it, referring to the mouse, should be translated into feminine French pronouns: respectively elle, elle and la (the latter is an object pronoun).", "labels": [], "entities": []}, {"text": "However, the online MT system to which we submitted this example translated all of them with the masculine forms, making the readers think that the author intends to kill his/her cat.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9688742160797119}]}, {"text": "The designers of MT systems have been aware of this problem and sometimes tried to address it, starting already from rule-based systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9821218252182007}]}, {"text": "However, it is only recently that specific strategies for translating pronouns have been proposed and evaluated (see Hardmeier (2014), Section 2.3.1).", "labels": [], "entities": []}, {"text": "Most of the strategies have attempted to convey information from anaphora resolution systems to statistical MT ones, by constraining target pronouns based on features of their antecedents in the target language.", "labels": [], "entities": [{"text": "convey information from anaphora resolution", "start_pos": 41, "end_pos": 84, "type": "TASK", "confidence": 0.6612848818302155}, {"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.907476544380188}]}, {"text": "Still, at the DiscoMT 2015 shared task on pronoun-focused EN/FR translation , none of the submitted systems was able to outperform a well-trained phrase-based statistical MT baseline.", "labels": [], "entities": [{"text": "DiscoMT 2015 shared task", "start_pos": 14, "end_pos": 38, "type": "DATASET", "confidence": 0.8407412171363831}, {"text": "EN/FR translation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.5587306767702103}]}, {"text": "Apart from the need for considering first the functions of pronouns and then their antecedents, if any, one of the reasons that limit performance is the large number of errors made by co-reference or anaphora resolution systems.", "labels": [], "entities": [{"text": "co-reference or anaphora resolution", "start_pos": 184, "end_pos": 219, "type": "TASK", "confidence": 0.7277381718158722}]}, {"text": "In this paper, we attempt to model the uncertainty of an off-the-shelf coreference resolution system (Lee et al.'s (2011) Stanford system) with respect to its impact on MT.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.9297990798950195}, {"text": "MT", "start_pos": 169, "end_pos": 171, "type": "TASK", "confidence": 0.9893823862075806}]}, {"text": "We propose to learn from parallel data the correlations between target side pronouns and the gender/number of their (uncertain) antecedents, as hypothesized by the coreference resolution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 164, "end_pos": 186, "type": "TASK", "confidence": 0.943829208612442}]}, {"text": "These correlations are represented as an additional translation model, which we baptize 'coreference model' or CM.", "labels": [], "entities": []}, {"text": "We use this model as an additional translation table in the Moses phrase-based statistical MT system () along with a standard phrasebased translation table.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.7750999331474304}]}, {"text": "While decoding, the antecedents are obtained from the Stanford system as well, and their target-side features are obtained through alignment and POS analysis.", "labels": [], "entities": [{"text": "Stanford system", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9442208111286163}]}, {"text": "Through experiments based on the DiscoMT 2015 data (transcripts of TED talks), and automatic and human evaluation metrics, we show that our solution outperforms a deterministic pronoun post-editing system, as well as the DiscoMT 2015 statistical MT baseline.", "labels": [], "entities": [{"text": "DiscoMT 2015 data", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.9276147882143656}, {"text": "DiscoMT 2015 statistical MT baseline", "start_pos": 221, "end_pos": 257, "type": "DATASET", "confidence": 0.8498406529426574}]}, {"text": "Below, we first review previous work (Section 2) before explaining how the coreference model is constructed (Section 3).", "labels": [], "entities": []}, {"text": "The integration of the model into the Moses SMT decoder is presented in Section 4.", "labels": [], "entities": [{"text": "Moses SMT decoder", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.760532389084498}]}, {"text": "We report and discuss the results of our experiments in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.8743570446968079}]}], "datasetContent": [{"text": "We built the phrase table on the following parallel datasets: aligned TED talks from the WIT 3 corpus (), Europarl v.", "labels": [], "entities": [{"text": "WIT 3 corpus", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.90973299741745}, {"text": "Europarl", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9715756177902222}]}, {"text": "7), News Commentary v.", "labels": [], "entities": []}, {"text": "9 and other news data from).", "labels": [], "entities": []}, {"text": "The language model was trained on the target side (French) of all above datasets.", "labels": [], "entities": []}, {"text": "Then, the system was tuned on a development set of 887 sentences from IWSLT 2010 provided for the shared task on pronoun translation of the DiscoMT 2015 workshop ( ).", "labels": [], "entities": [{"text": "IWSLT 2010", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.9013531506061554}, {"text": "pronoun translation of the DiscoMT 2015 workshop", "start_pos": 113, "end_pos": 161, "type": "TASK", "confidence": 0.6494843448911395}]}, {"text": "The test set was also the one from the DiscoMT 2015 shared task, with 2,093 English sentences along with French gold-standard translations, extracted from 12 recent TED talks.", "labels": [], "entities": [{"text": "DiscoMT 2015 shared task", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.8963452279567719}]}, {"text": "The test set contains 809 occurrences of it and 307 of they.", "labels": [], "entities": []}, {"text": "We processed each talk separately, translating its sentences in order.", "labels": [], "entities": []}, {"text": "As explained above, after translating each sentence, the G/N values of any target antecedents, if any, are passed to the current or following sentence containing the anaphoric pronoun.", "labels": [], "entities": []}, {"text": "If the antecedent is unidentified or not nominal (due to errors of anaphora resolution or alignment), we let these pronouns be translated by the default phrase table.", "labels": [], "entities": []}, {"text": "As a result, only 367 occurrences of it and 196 of they (i.e. 563 instances or about 50% of the total) are processed by the Coreference-Aware Decoder, and have the potential to improve over the SMT baseline.", "labels": [], "entities": [{"text": "SMT", "start_pos": 194, "end_pos": 197, "type": "TASK", "confidence": 0.9625433087348938}]}, {"text": "The accuracy of the new decoder will be therefore evaluated only over the pronouns that have actually been processed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9991939663887024}]}, {"text": "The automatic metrics have demonstrated that the system using the Coreference Model is closer to the reference, in terms of pronouns, than the Baseline and the Post-editing systems.", "labels": [], "entities": []}, {"text": "Our automatic metric is particularly strict in requiring identity to the reference, with only minimal variation accepted on the forms of \"ce\" and \"c \u00b8a\".", "labels": [], "entities": []}, {"text": "However, in French, some variations of pronouns are acceptable.", "labels": [], "entities": []}, {"text": "For instance, the indefinite pronoun \"on\" may replace the third person plural pronouns \"ils\" or \"elles\"; the pronouns \"il\" and \"ce\" maybe substituted in some cases (e.g. as in il est important \u2248 c'est important); and idiomatic translations are frequent (e.g. on discute de c \u00b8a \u2248 on en discute).", "labels": [], "entities": []}, {"text": "Therefore, in addition to automatic metrics, we performed a human evaluation of the translated pronouns.", "labels": [], "entities": []}, {"text": "Two annotators with good knowledge of French and English evaluated the 329 sentences of the test set, containing 563 instances of it and they.", "labels": [], "entities": []}, {"text": "For each sentence, the annotators were shown the English source sentence and the preceding one, followed by the outputs of the three systems for the source sentence, as well as the reference translation of this sentence and the preceding one, as exemplified in: Number of correctly vs. incorrectly translated pronouns by the three systems BL, PE and CM.", "labels": [], "entities": [{"text": "BL", "start_pos": 339, "end_pos": 341, "type": "METRIC", "confidence": 0.8312011957168579}]}, {"text": "In Evaluation 1, they are rated on 40 blocks by two human annotators after deliberation.", "labels": [], "entities": []}, {"text": "In Evaluation 2, they are rated on the full set (329 blocks) by one annotator.", "labels": [], "entities": []}, {"text": "nouns to be evaluated were specified.", "labels": [], "entities": []}, {"text": "The order of the three systems was randomly assigned in each such evaluation block and was hence unknown to annotators.", "labels": [], "entities": []}, {"text": "The annotators were instructed to judge pronouns according to their subjective impression of correction, based mainly on compatibility with the antecedent, and not on the identity to the reference translation, which was shown only to make sure that the source was correctly understood.", "labels": [], "entities": []}, {"text": "The score of an evaluated pronoun is 1 if correct and 0 if not, and the system's score is the sum of the scores overall source pronouns.", "labels": [], "entities": []}, {"text": "Due to time limitations, one annotator completed the entire evaluation (329 blocks with 563 pronouns), whereas the other one completed 40 blocks which contained 73 occurrences of it and they in the source.", "labels": [], "entities": []}, {"text": "Of the total of 73 \u00d7 3 = 219 instances of the 40 blocks rated by the two annotators, the annotators agreed on the rating (correct or incorrect) of 188 instances and disagreed on 31, corresponding to a Kappa score of 0.645, i.e. a moderate agreement.", "labels": [], "entities": []}, {"text": "The annotators deliberated to analyze their differences and reached consensus over 26 additional instances, leading to an adjudicated Kappa score of 0.939.", "labels": [], "entities": [{"text": "Kappa score", "start_pos": 134, "end_pos": 145, "type": "METRIC", "confidence": 0.9700181782245636}]}, {"text": "The accuracy of the three systems computed against the adjudicated annotations of 73 source pronouns is shown in, as Evaluation 1, while accuracy over the full set of 563 source pronouns rated by only one annotator (hence with a smaller confidence) is shown as Evaluation 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993059635162354}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9986023306846619}]}, {"text": ".]: Example of a block for human evaluation: source sentence SRC (and the preceding one SRC-1) followed by the three system translations in random order, the reference translation REF and the preceding sentence.", "labels": [], "entities": [{"text": "REF", "start_pos": 180, "end_pos": 183, "type": "METRIC", "confidence": 0.7758461833000183}]}, {"text": "ative improvements of 5.5% and 6.9% over BL and PE respectively.", "labels": [], "entities": [{"text": "BL", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9951760768890381}, {"text": "PE", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.839041531085968}]}, {"text": "Although less reliable, results from Evaluation 2 show that CM outperforms BL by 10 correct translations (ca. 1.8%), and PE by 26 correct translations (ca. 4.6%).", "labels": [], "entities": [{"text": "BL", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9654902815818787}, {"text": "PE", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.7699397206306458}]}, {"text": "These proportions are in the same order as those from Evaluation 1.", "labels": [], "entities": []}, {"text": "The results of Evaluation 2 show a considerable increase of the accuracy of all systems compared to the scores from the automatic metrics, with relative gains slightly above 20%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.999568521976471}]}, {"text": "As expected, in all three systems, a large number of pronouns judged as incorrect by the automated metric because they differed from the reference (C 3 ) have been judged as correct by the human evaluators.", "labels": [], "entities": []}, {"text": "However, although they are higher, human scores are strongly correlated with automatic ones: Pearson's correlation coefficient between C 1 + C 2 and scores from Evaluation 1 is 0.994, while for Evaluation 2 it is 0.936.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 93, "end_pos": 126, "type": "METRIC", "confidence": 0.9303671568632126}]}, {"text": "shows two examples in which candidate pronouns were judged as correct by both annotators, although they differ from the reference.", "labels": [], "entities": []}, {"text": "In Example 1, the second it in the source sentence was translated into c \u00b8a by CM, but was not translated in the reference, as the human translator combined two identical source pronouns into a unique target one.", "labels": [], "entities": []}, {"text": "Similarly, in Example 2, CM translated the first it into a French subject pronoun (c'), while the reference used a third person object pronoun (la).", "labels": [], "entities": []}, {"text": "A more flexible assessment than the strict automatic one thus increases the scores of the systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Detailed scores of the three systems: BL,  PE and CM. The accuracy is the proportion of  good translations (C 1 + C 2 ) over the total num- ber of pronouns (563). CM outperforms both PE  and BL on all scores.", "labels": [], "entities": [{"text": "BL", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.926855206489563}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9995773434638977}]}, {"text": " Table 2: Overall precision, recall, F-score and  BLEU score of BL, PE and CM.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9984126091003418}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.999822199344635}, {"text": "F-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9985747337341309}, {"text": "BLEU score", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9844208657741547}, {"text": "BL", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9948675632476807}, {"text": "PE", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9490847587585449}]}, {"text": " Table 4: Number of correctly vs. incorrectly trans- lated pronouns by the three systems BL, PE and  CM. In Evaluation 1, they are rated on 40 blocks  by two human annotators after deliberation. In  Evaluation 2, they are rated on the full set (329  blocks) by one annotator.", "labels": [], "entities": [{"text": "BL", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9926238059997559}]}]}