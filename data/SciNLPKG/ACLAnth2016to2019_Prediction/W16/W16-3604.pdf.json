{"title": [{"text": "Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue", "labels": [], "entities": []}], "abstractContent": [{"text": "The use of irony and sarcasm in social media allows us to study them at scale for the first time.", "labels": [], "entities": []}, {"text": "However, their diversity has made it difficult to construct a high-quality corpus of sarcasm in dialogue.", "labels": [], "entities": []}, {"text": "Here, we describe the process of creating a large-scale, highly-diverse corpus of online debate forums dialogue, and our novel methods for operationalizing classes of sarcasm in the form of rhetorical questions and hyperbole.", "labels": [], "entities": []}, {"text": "We show that we can use lexico-syntactic cues to reliably retrieve sarcastic utterances with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9821922183036804}]}, {"text": "To demonstrate the properties and quality of our corpus, we conduct supervised learning experiments with simple features, and show that we achieve both higher precision and F than previous work on sarcasm in debate forums dialogue.", "labels": [], "entities": [{"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9991637468338013}, {"text": "F", "start_pos": 173, "end_pos": 174, "type": "METRIC", "confidence": 0.9995030164718628}]}, {"text": "We apply a weakly-supervised linguistic pattern learner and qualitatively analyze the linguistic differences in each class.", "labels": [], "entities": []}], "introductionContent": [{"text": "Irony and sarcasm in dialogue constitute a highly creative use of language signaled by a large range of situational, semantic, pragmatic and lexical cues.", "labels": [], "entities": []}, {"text": "Previous work draws attention to the use of both hyperbole and rhetorical questions in conversation as distinct types of lexico-syntactic cues defining diverse classes of sarcasm).", "labels": [], "entities": []}, {"text": "Theoretical models posit that a single semantic basis underlies sarcasm's diversity of form, namely \"a contrast\" between expected and experienced events, giving rise to a contrast between what is said and a literal description of the actual situation.", "labels": [], "entities": []}, {"text": "This semantic characterization has not been straightforward to operationalize computationally for sarcasm in dialogue.", "labels": [], "entities": []}, {"text": "operationalize this notion for sarcasm in tweets, achieving good results.", "labels": [], "entities": []}, {"text": "develop several incongruity features to capture it, but although they improve performance on tweets, their features do not yield improvements for dialogue.", "labels": [], "entities": []}, {"text": "Previous work on the Internet Argument Corpus (IAC) 1.0 dataset aimed to develop a highprecision classifier for sarcasm in order to bootstrap a much larger corpus, but was only able to obtain a precision of just 0.62, with a best F of 0.57, not high enough for bootstrapping ().", "labels": [], "entities": [{"text": "Internet Argument Corpus (IAC) 1.0 dataset", "start_pos": 21, "end_pos": 63, "type": "DATASET", "confidence": 0.707223154604435}, {"text": "precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.9829848408699036}, {"text": "F", "start_pos": 230, "end_pos": 231, "type": "METRIC", "confidence": 0.9964730143547058}]}, {"text": "experimented with the same corpus, using supervised learning, and achieved a best precision of 0.66 and a best F of 0.70.'s explicit congruity features achieve precision around 0.70 and best F of 0.64 on a subset of IAC 1.0.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9932410717010498}, {"text": "F", "start_pos": 111, "end_pos": 112, "type": "METRIC", "confidence": 0.9984207153320312}, {"text": "precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.989376425743103}, {"text": "F", "start_pos": 191, "end_pos": 192, "type": "METRIC", "confidence": 0.9985633492469788}, {"text": "IAC 1.0", "start_pos": 216, "end_pos": 223, "type": "DATASET", "confidence": 0.9226896464824677}]}, {"text": "We decided that we need a larger and more diverse corpus of sarcasm in dialogue.", "labels": [], "entities": []}, {"text": "It is difficult to efficiently gather sarcastic data, because only about 12% of the utterances in written online debate forums dialogue are sarcastic (, and it is difficult to achieve high reliability for sarcasm annotation).", "labels": [], "entities": []}, {"text": "Thus, our contributions are: \u2022 We develop anew larger corpus, using several methods that filter non-sarcastic utterances to skew the distribution toward/in favor of sarcastic utterances.", "labels": [], "entities": []}, {"text": "We put filtered data out for annotation, and are able to achieve high annotation reliability.", "labels": [], "entities": []}, {"text": "\u2022 We present a novel operationalization of both rhetorical questions and hyperbole to develop subcorpora to explore the differences between them and general sarcasm.", "labels": [], "entities": []}, {"text": "\u2022 We show that our new corpus is of high quality by applying supervised machine learning with simple features to explore how different corpus properties affect classification results.", "labels": [], "entities": []}, {"text": "We achieve a highest precision of 0.73 and a highest F of 0.74 on the new corpus with basic n-gram and Word2Vec features, showcasing the quality of the corpus, and improving on previous work.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9979756474494934}, {"text": "F", "start_pos": 53, "end_pos": 54, "type": "METRIC", "confidence": 0.9996546506881714}, {"text": "Word2Vec", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.9348296523094177}]}, {"text": "\u2022 We apply a weakly-supervised learner to characterize linguistic patterns in each corpus, and describe the differences across generic sarcasm, rhetorical questions and hyperbole in terms of the patterns learned.", "labels": [], "entities": []}, {"text": "\u2022 We show for the first time that it is straightforward to develop very high precision classifiers for NOT-SARCASTIC utterances across our rhetorical questions and hyperbole subtypes, due to the nature of these utterances in debate forum dialogue.", "labels": [], "entities": [{"text": "NOT-SARCASTIC utterances", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.6887980699539185}]}], "datasetContent": [{"text": "Our primary goal is not to optimize classification results, but to explore how results vary across different subcorpora and corpus properties.", "labels": [], "entities": []}, {"text": "We also aim to demonstrate that the quality of our corpus makes it more straightforward to achieve high classification performance.", "labels": [], "entities": []}, {"text": "We apply both supervised learning using SVM (from Scikit-Learn (Pedregosa et al., 2011)) and weakly-supervised linguistic pattern learning using AutoSlog-TS (.", "labels": [], "entities": []}, {"text": "These reveal different aspects of the corpus.", "labels": [], "entities": []}, {"text": "We restrict our supervised experiments to a default linear SVM learner with Stochastic Gradient Descent (SGD) training and L2 regularization, available in the SciKit-Learn toolkit).", "labels": [], "entities": []}, {"text": "We use 10-fold cross-validation, and only two types of features: n-grams and Word2Vec word embeddings.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9320337772369385}]}, {"text": "We expect Word2Vec to be able to capture semantic generalizations that n-grams do not).", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.9374322891235352}]}, {"text": "The n-gram features include unigrams, bigrams, and trigrams, including sequences of punctuation (for example, ellipses or \"!!!\"), and emoticons.", "labels": [], "entities": []}, {"text": "We use GoogleNews Word2Vec features ().", "labels": [], "entities": []}, {"text": "4 summarizes the results of our supervised learning experiments on our datasets using 10-fold cross validation.", "labels": [], "entities": []}, {"text": "The data is balanced evenly between the SARCASTIC and NOT-SARCASTIC classes, and the best F-Measures for each class are shown in bold.", "labels": [], "entities": [{"text": "SARCASTIC", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.8022766709327698}, {"text": "NOT-SARCASTIC", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.7032066583633423}, {"text": "F-Measures", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9694042801856995}]}, {"text": "The default W2V model, (trained on Google News), gives the best overall F-measure of 0.74 on the Gen corpus for the SARCASTIC class, while n-grams give the best NOT-SARCASTIC F-measure of 0.73.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9974428415298462}, {"text": "Gen corpus", "start_pos": 97, "end_pos": 107, "type": "DATASET", "confidence": 0.9701285660266876}, {"text": "NOT-SARCASTIC F-measure", "start_pos": 161, "end_pos": 184, "type": "METRIC", "confidence": 0.5299289524555206}]}, {"text": "Both of these results are higher F than previously reported for classifying sarcasm in dialogue, and we might expect that feature engineering could yield even greater performance.", "labels": [], "entities": [{"text": "F", "start_pos": 33, "end_pos": 34, "type": "METRIC", "confidence": 0.9995189905166626}, {"text": "classifying sarcasm in dialogue", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.9111127704381943}]}, {"text": "We test our own custom 300-dimensional embeddings created for the dialogic domain using the Gensim library, and a very large corpus of user-generated dialogue.", "labels": [], "entities": []}, {"text": "While this custom model works well for other tasks on IAC 2.0, it did notwork well for sarcasm classification, so we do not discuss it further.", "labels": [], "entities": [{"text": "sarcasm classification", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.9369747340679169}]}, {"text": "On the RQ corpus, n-grams provide the best F-measure for SARCASTIC at 0.70 and NOT-SARCASTIC at 0.71.", "labels": [], "entities": [{"text": "RQ corpus", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.9253155589103699}, {"text": "F-measure", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9981604218482971}, {"text": "NOT-SARCASTIC", "start_pos": 79, "end_pos": 92, "type": "METRIC", "confidence": 0.8901230692863464}]}, {"text": "Although W2V performs well, the n-gram model includes features involving repeated punctuation and emoticons, which the W2V model excludes.", "labels": [], "entities": [{"text": "W2V", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.8991091251373291}]}, {"text": "Punctuation and emoticons are often used as distinctive feature of sarcasm (i.e. \"Oh, really?!?!\",).", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8835949301719666}]}, {"text": "For the Hyp corpus, the best F-measure for both the SARCASTIC and NOT-SARCASTIC classes again comes from n-grams, with F-measures of 0.65 and 0.68 respectively.", "labels": [], "entities": [{"text": "Hyp corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.9604796171188354}, {"text": "F-measure", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9909319877624512}, {"text": "SARCASTIC", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.7819119691848755}, {"text": "F-measures", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.9731820225715637}]}, {"text": "It is interesting to note that the overall results of the Hyp data are lower than those for Gen and RQs, likely due to the smaller size of the Hyp dataset.", "labels": [], "entities": [{"text": "Hyp data", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.858078807592392}, {"text": "Hyp dataset", "start_pos": 143, "end_pos": 154, "type": "DATASET", "confidence": 0.9030996561050415}]}, {"text": "To examine the effect of dataset size, we com-pare F-measure (using the same 10-fold crossvalidation setup) for each dataset while holding the number of posts per class constant.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9900020360946655}]}, {"text": "shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ).", "labels": [], "entities": [{"text": "Hyp datasets", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.8560514450073242}]}, {"text": "From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes.", "labels": [], "entities": []}, {"text": "Interestingly, the results for the RQ dataset are very comparable to those of Gen. The Gen dataset eventually gets the highest sarcastic F-measure (0.74) at its full dataset size of 3,260 posts per class.", "labels": [], "entities": [{"text": "RQ dataset", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.7959879636764526}, {"text": "Gen dataset", "start_pos": 87, "end_pos": 98, "type": "DATASET", "confidence": 0.8884586095809937}, {"text": "sarcastic", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9582380056381226}, {"text": "F-measure", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.7436193227767944}]}, {"text": "AutoSlog-TS is a weakly supervised pattern learner that only requires training documents labeled broadly as SAR-CASTIC or NOT-SARCASTIC.", "labels": [], "entities": []}, {"text": "AutoSlog-TS uses a set of syntactic templates to define different types of linguistic expressions.", "labels": [], "entities": []}, {"text": "The left-hand side of lists each pattern template and the right-hand side illustrates a specific lexicosyntactic pattern (in bold) that represents an instantiation of each general pattern template for learning sarcastic patterns in our data.", "labels": [], "entities": []}, {"text": "In addition to these 17 templates, we added patterns to AutoSlog for adjective-noun, adverb-adjective and adjective-adjective, because these patterns are frequent in hyperbolic sarcastic utterances.", "labels": [], "entities": []}, {"text": "The examples in show that Colston's notion of contrast shows up in many learned patterns, and that the source of the contrast is highly variable.", "labels": [], "entities": [{"text": "Colston's notion of contrast", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.6885293245315551}]}, {"text": "For example, Row 1 implies a contrast with a set of people who are not your mother.", "labels": [], "entities": []}, {"text": "Row 5 contrasts what you were asked with what you've (just) done.", "labels": [], "entities": []}, {"text": "Row 10 contrasts chapter 12 and chapter 13.", "labels": [], "entities": []}, {"text": "Row 11 contrasts what I am allowed vs. what you have to do.", "labels": [], "entities": []}, {"text": "AutoSlog-TS computes statistics on the strength of association of each pattern with each class, i.e. P(SARCASTIC | p) and P(NOT-SARCASTIC | p), along with the pattern's overall frequency.", "labels": [], "entities": []}, {"text": "We define two tuning parameters for each class: \u03b8 f , the frequency with which a pattern occurs, \u03b8 p , the probability with which a pattern is associated with the given class.", "labels": [], "entities": []}, {"text": "We do a grid-search, testing the performance of our patterns thresholds from \u03b8 f = {2-6} in intervals of 1, \u03b8 p ={0.60-0.85} in intervals of 0.05.", "labels": [], "entities": []}, {"text": "Once we extract the subset of patterns passing our thresholds, we search for these patterns in the posts in our development set, classifying a post as a given class if it contains \u03b8 n ={1,: Examples of Characteristic Patterns for Gen using AutoSlog-TS Templates: Plot of Precision (x-axis) vs Recall (yaxis) for three subcorpora with AutoSlog-TS parameters, aimed at optimizing precision", "labels": [], "entities": [{"text": "Recall", "start_pos": 293, "end_pos": 299, "type": "METRIC", "confidence": 0.9547868967056274}]}], "tableCaptions": [{"text": " Table 1: Examples of different types of SARCAS- TIC (S) and NOT-SARCASTIC (N S) Posts", "labels": [], "entities": [{"text": "NOT-SARCASTIC (N S) Posts", "start_pos": 61, "end_pos": 86, "type": "DATASET", "confidence": 0.6469204227129618}]}, {"text": " Table 2: Total number of posts in each subcorpus  (each with a 50% split of SARCASTIC and NOT- SARCASTIC posts)", "labels": [], "entities": [{"text": "SARCASTIC", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.7255181670188904}]}, {"text": " Table 3: Annotation Counts for a Subset of Cues", "labels": [], "entities": []}, {"text": " Table 7: Supervised Learning Results for Generic  (Gen: 3,260 posts per class), Rhetorical Questions  (RQ: 851 posts per class) and Hyperbole (Hyp:  582 posts per class)", "labels": [], "entities": []}, {"text": " Table 9: Examples of Characteristic Patterns for Gen using AutoSlog-TS Templates", "labels": [], "entities": []}, {"text": " Table 10: Total number of patterns passing  threshold of Freq \u2265 2, Prob \u2265 0.75", "labels": [], "entities": [{"text": "Total number of patterns passing  threshold", "start_pos": 11, "end_pos": 54, "type": "METRIC", "confidence": 0.7283370743195215}, {"text": "Freq", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9834831953048706}, {"text": "Prob", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9719854593276978}]}]}