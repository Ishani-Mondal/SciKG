{"title": [{"text": "Crowd-sourcing NLG Data: Pictures Elicit Better Data", "labels": [], "entities": [{"text": "NLG Data", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.8961041867733002}, {"text": "Pictures Elicit", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.698842316865921}]}], "abstractContent": [{"text": "Recent advances in corpus-based Natural Language Generation (NLG) hold the promise of being easily portable across domains, but require costly training data, consisting of meaning representations (MRs) paired with Natural Language (NL) utterances.", "labels": [], "entities": [{"text": "corpus-based Natural Language Generation (NLG)", "start_pos": 19, "end_pos": 65, "type": "TASK", "confidence": 0.8285622852189201}]}, {"text": "In this work, we propose a novel framework for crowd-sourcing high quality NLG training data, using automatic quality control measures and evaluating different MRs with which to elicit data.", "labels": [], "entities": []}, {"text": "We show that pictorial MRs result in better NL data being collected than logic-based MRs: utterances elicited by pictorial MRs are judged as significantly more natural, more informative, and better phrased, with a significant increase in average quality ratings (around 0.5 points on a 6-point scale), compared to using the logical MRs.", "labels": [], "entities": []}, {"text": "As the MR becomes more complex, the benefits of pictorial stimuli increase.", "labels": [], "entities": [{"text": "MR", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9349192380905151}]}, {"text": "The collected data will be released as part of this submission.", "labels": [], "entities": []}], "introductionContent": [{"text": "The overall aim of this research is to develop methods that will allow the full automation of the creation of NLG systems for new applications and domains.", "labels": [], "entities": []}, {"text": "Currently deployed technologies for NLG utilise domain-dependent methods including hand-written grammars or domain-specific language templates for surface realisation, both of which are costly to develop and maintain.", "labels": [], "entities": []}, {"text": "Recent corpus-based methods hold the promise of being easily portable across domains, e.g. (), but require high quality training data consisting of meaning representations (MR) paired with Natural Language (NL) utterances, augmented by alignments between MR elements and NL words.", "labels": [], "entities": []}, {"text": "Recent work) removes the need for alignment, but the question of whereto get indomain training data of sufficient quality remains.", "labels": [], "entities": [{"text": "alignment", "start_pos": 34, "end_pos": 43, "type": "TASK", "confidence": 0.9693127274513245}]}, {"text": "In this work, we propose a novel framework for crowd-sourcing high quality NLG training data, using automatic quality control measures and evaluating different meaning representations.", "labels": [], "entities": []}, {"text": "So far, we collected 1410 utterances using this framework.", "labels": [], "entities": []}, {"text": "The data will be released as part of this submission.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiment was designed to investigate whether we can elicit high-quality Natural Language via crowdsourcing, using different modalities of meaning representation: textual/logical and pictorial MR.", "labels": [], "entities": []}, {"text": "We use the CrowdFlower platform to setup our experiments and to access an online workforce.", "labels": [], "entities": []}, {"text": "While automatic validators help reject some invalid cases, human feedback is needed to assess the quality of the collected data.", "labels": [], "entities": []}, {"text": "Ina 2nd phase we evaluated the collected data through a large-scale subjective rating experiment using the CrowdFlower system.", "labels": [], "entities": []}, {"text": "6-point Likert scales were used to collect judgements on the data, via the following criteria: 1.", "labels": [], "entities": []}, {"text": "Q1: \"Is this utterance informative?", "labels": [], "entities": []}, {"text": "(i.e. do you think it provides enough useful information about the venue?)\" 2.", "labels": [], "entities": []}, {"text": "Q2: \"Is this utterance natural?", "labels": [], "entities": []}, {"text": "(e.g. could it have been produced by a native speaker?)\" 3.", "labels": [], "entities": []}, {"text": "Q3: \"Is this utterance well phrased?", "labels": [], "entities": []}, {"text": "(i.e. do you like how it is expressed?)\" Finally, crowd workers were asked to judge whether the utterance is grammatically correct.", "labels": [], "entities": []}, {"text": "While automated or semi-automated metrics provide some useful information about the collected utterances, human feedback is necessary to properly assess their quality.", "labels": [], "entities": []}, {"text": "In this section, we first compare the data collected using self-evaluation and crowd evaluation methods, and later we analyse Informativeness, Naturalness, and Phrasing of the collected utterances.", "labels": [], "entities": [{"text": "Informativeness", "start_pos": 126, "end_pos": 141, "type": "METRIC", "confidence": 0.9013416171073914}]}, {"text": "We mostly use parametric statistical methods in our analysis.", "labels": [], "entities": []}, {"text": "It has been debated for over 50 years whether Likert-type measurement scales should be analysed using parametric http://swoogle.umbc.edu/SimService/api.html or non-parametric statistical methods.", "labels": [], "entities": []}, {"text": "The use of parametric statistics, however, was justified repeatedly by, and more recently by) as a \"perfectly appropriate\") statistical method for Likert scales that maybe used by researchers \"with no fear of coming to the wrong conclusion\".", "labels": [], "entities": []}, {"text": "We therefore present and analyse mean averages (rather than the mode) for the collected judgements.", "labels": [], "entities": []}, {"text": "In our experiment we used two methods to evaluate the quality of collected utterances: self-evaluation and an independent crowd-based evaluation.", "labels": [], "entities": []}, {"text": "During the self-evaluation, crowd workers were asked to rank their own utterances.", "labels": [], "entities": []}, {"text": "Note that data collected using the self-evaluation method was not intended to allow us to compare the quality of utterances elicited via pictorial and textual MRs.", "labels": [], "entities": []}, {"text": "Rather, this data was collected in order to understand whether selfevaluation maybe a reliable technique to evaluate the quality of created utterances in future studies.", "labels": [], "entities": []}, {"text": "In the self-evaluation, for each of their own NL utterances, crowd workers could select either higher than average, average, or lower than average values for Informativeness, Naturalness, and Phrasing.", "labels": [], "entities": [{"text": "Phrasing", "start_pos": 192, "end_pos": 200, "type": "TASK", "confidence": 0.8902817964553833}]}, {"text": "For the independent crowd evaluation, anew CrowdFlower task was created.", "labels": [], "entities": []}, {"text": "In this task, crowd workers were asked to look atone utterance at a time and to rate each utterance using the same procedure.", "labels": [], "entities": []}, {"text": "In order to compare the results of self-evaluation with the results of the independent crowd evaluation, we labelled the results of perceived Informativeness, Naturalness and Phrasing as higher than average, average and lower than average in both modes.", "labels": [], "entities": []}, {"text": "Cohen's kappa coefficient was used to measure inter-rater agreement between the two groups of evaluators, i.e. self-evaluators and independent crowd evaluators.", "labels": [], "entities": []}, {"text": "The statistics did not reveal a significant level of agreement between the two groups of evaluators neither for the scores of Informativeness (\u03ba = 0.014, p = 0.36), nor Phrasing (\u03ba = 0.007, p = 0.64), nor Naturalness (\u03ba = -0.007, p = 0.62).", "labels": [], "entities": [{"text": "Informativeness", "start_pos": 126, "end_pos": 141, "type": "METRIC", "confidence": 0.9077463150024414}, {"text": "Phrasing", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.5643871426582336}]}, {"text": "The lack of agreement with the independent evaluation already indicates a potential problem with the self-evaluation method.", "labels": [], "entities": []}, {"text": "However, in order to further assess which group was more reliable in eval-270 uating utterances, we compared their Informativeness scores with the Semantic Similarity score of the corresponding utterances.", "labels": [], "entities": [{"text": "Informativeness", "start_pos": 115, "end_pos": 130, "type": "METRIC", "confidence": 0.913236141204834}]}, {"text": "As discussed before, the concepts of Informativeness and Semantic Similarity are similar to each other, so better agreement between these scores indicates higher reliability of evaluation results.", "labels": [], "entities": [{"text": "reliability", "start_pos": 162, "end_pos": 173, "type": "METRIC", "confidence": 0.9709861874580383}]}, {"text": "In particular, utterances with high Semantic Similarity would be expected to have high ratings for Informativeness, as they express more of the concepts from the original MR.", "labels": [], "entities": [{"text": "Informativeness", "start_pos": 99, "end_pos": 114, "type": "METRIC", "confidence": 0.9481015205383301}]}, {"text": "The percentage agreement between the Informativeness and Semantic Similarity was 31.1%, while for the utterances evaluated independently by the crowd it was 60.3%.", "labels": [], "entities": []}, {"text": "The differences in percentage agreements for the utterances with good semantic similarity was even higher: 32.1% for selfevaluators vs. 75.1% for crowd evaluators.", "labels": [], "entities": []}, {"text": "This strongly suggests that the evaluation quality of selfevaluators is less reliable than that of the crowd.", "labels": [], "entities": []}, {"text": "Therefore, we focus on the data collected from crowd evaluation for the analysis presented in the following sections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Nature of the data collected with each  MR. Italics denote averages across all numbers of  attributes.", "labels": [], "entities": []}, {"text": " Table 2. A two- way ANOVA was conducted to examine the effect  of MR modality and the number of attributes on the  length of utterance. There was a statistically sig- nificant interaction between the effects of modality  and the number of attributes in the MR, F(2,1236) =  23.74, p < 0.001. A main effects analysis showed  that the average length of utterance was significantly  larger not only for a larger number of attributes, with  p < 0.001, but also for the utterances created based", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.604657769203186}, {"text": "F", "start_pos": 262, "end_pos": 263, "type": "METRIC", "confidence": 0.9959558844566345}]}, {"text": " Table 3: Human evaluation of the data collected  with each MR (** = p < 0.01 and *** = p < 0.001  for Pictorial versus Textual conditions). Italics de- note averages across all numbers of attributes.", "labels": [], "entities": []}]}