{"title": [{"text": "Political Issue Extraction Model: A Novel Hierarchical Topic Model That Uses Tweets By Political And Non-Political Authors", "labels": [], "entities": [{"text": "Political Issue Extraction Model", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7500643730163574}]}], "abstractContent": [{"text": "People often use social media to discuss opinions, including political ones.", "labels": [], "entities": []}, {"text": "We refer to relevant topics in these discussions as political issues, and the alternate stands towards these topics as political positions.", "labels": [], "entities": []}, {"text": "We present a Political Issue Extraction (PIE) model that is capable of discovering political issues and positions from an unlabeled dataset of tweets.", "labels": [], "entities": [{"text": "Political Issue Extraction (PIE)", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.7883797188599905}]}, {"text": "A strength of this model is that it uses twitter timelines of political and non-political authors, and affiliation information of only political authors.", "labels": [], "entities": []}, {"text": "The model estimates word-specific distributions (that denote political issues and positions) and hierarchical author/group-specific distributions (that show how these issues divide people).", "labels": [], "entities": []}, {"text": "Our experiments using a dataset of 2.4 million tweets from the US show that this model effectively captures the desired properties (with respect to words and groups) of political discussions.", "labels": [], "entities": []}, {"text": "We also evaluate the two components of the model by experimenting with: (a) Use to alternate strategies to classify words, and (b) Value addition due to incorporation of group membership information.", "labels": [], "entities": [{"text": "Value addition", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.7280854284763336}]}, {"text": "Estimated distributions are then used to predict political affiliation with 68% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9986580610275269}]}], "introductionContent": [{"text": "Political discussions in social media contain contentious topics (called 'political issues'), and alternate stands with respect to these issues (called 'positions').", "labels": [], "entities": []}, {"text": "We present a topic model that discovers political issues and positions in tweets.", "labels": [], "entities": []}, {"text": "Our model is called Political Issue Extraction (PIE) model.", "labels": [], "entities": [{"text": "Political Issue Extraction (PIE)", "start_pos": 20, "end_pos": 52, "type": "TASK", "confidence": 0.7597008347511292}]}, {"text": "The input is the twitter timelines of authors (i.e., the user who created the tweet), and political affiliation information fora subset of authors (i.e., political authors. Antonym: Non-political authors).", "labels": [], "entities": []}, {"text": "Political and non-political authors contribute to formation of topics, whereas only political authors contribute to position that a group is likely to take.", "labels": [], "entities": []}, {"text": "Since our dataset consists of tweets from the US, political affiliation can be one of the three groups: 'Democrats', 'Republicans' or 'Unknown'.", "labels": [], "entities": []}, {"text": "For every tweet, we estimate two latent variables: issue and position.", "labels": [], "entities": [{"text": "issue", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.955670177936554}]}, {"text": "To discover topics related to issues and positions, we classify words in a tweet in three categories: issue words, position words and emoticons.", "labels": [], "entities": []}, {"text": "Instead of document-specific distributions as in LDA, we include a hierarchy of authorspecific and group-specific position distributions in our model.", "labels": [], "entities": []}, {"text": "This hierarchy estimates three distributions for each topic: global position, position of a given political group and position of a specific author.", "labels": [], "entities": []}, {"text": "We evaluate our model by (a) validating our topics against standard topic lists, (b) considering different strategies of splitting words into the three categories, and (c) validating how the model benefits from the group information.", "labels": [], "entities": []}, {"text": "Finally, we use our model to predict political affiliation of authors.", "labels": [], "entities": []}, {"text": "Models based on LDA by,; extract sentimentcoherent topics.", "labels": [], "entities": []}, {"text": "Past work related to political opinion has been reported by; O';;;; 82, and more recently by.", "labels": [], "entities": [{"text": "O';;", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9391506761312485}]}, {"text": "Two of them close to our work are by;.", "labels": [], "entities": []}, {"text": "Our model improves upon them in three ways: 1.", "labels": [], "entities": []}, {"text": "In PIE model, position words depend on both issue and position latent variables (as opposed to only the latter in prior work), 2.", "labels": [], "entities": []}, {"text": "In PIE model, a novel hierarchical author/group-wise distribution is considered instead of document-wise distribution.", "labels": [], "entities": []}, {"text": "3. To the best of our knowledge, PIE model is the first that operates at the author level by using complete author timelines of both political and non-political authors, and affiliation information of a subset of authors (only political authors).", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We present the structure and estimation procedure of PIE model in Section 3 and discuss our experiment setup in Section 4.", "labels": [], "entities": []}, {"text": "The evaluation is in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we conclude and point to future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We create a dataset of tweets using Twitter API (https://dev.twitter.com/).", "labels": [], "entities": []}, {"text": "The authors whose timelines will be downloaded are obtained as follows.", "labels": [], "entities": []}, {"text": "We first obtain a list of famous Democrats and Republicans using sources like about.com, The Guardian and Fanpagelist.", "labels": [], "entities": [{"text": "about.com", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.9613516330718994}, {"text": "The Guardian", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.9225949645042419}, {"text": "Fanpagelist", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.8651048541069031}]}, {"text": "This results in a list of 32 Republicans and 46 Democrats.", "labels": [], "entities": []}, {"text": "We expand this list by adding randomly selected friends of these twitter handles.", "labels": [], "entities": []}, {"text": "(The choice of \"friends\" as opposed to \"followers\" is intentional.)", "labels": [], "entities": []}, {"text": "We then download complete twitter timelines of all authors (Twitter sets the upper limit to 3200 tweets).", "labels": [], "entities": []}, {"text": "The resultant dataset consists of 2441058 tweets.", "labels": [], "entities": []}, {"text": "Dirichlet hyperparameters and values of I=35 and P =2 are experimentally determined.", "labels": [], "entities": [{"text": "I", "start_pos": 40, "end_pos": 41, "type": "METRIC", "confidence": 0.9859366416931152}]}, {"text": "We set priors on position words using a word list of 6789 words given by.", "labels": [], "entities": []}, {"text": "Function words and 25 most frequent words are removed.", "labels": [], "entities": []}, {"text": "To validate the efficacy of our model, our evaluation addresses the following questions: \u2022 What impact do components of the model have, on its ability to discover these issues and positions?", "labels": [], "entities": []}, {"text": "(Section 5.1) \u2022 What political issues and positions does the model discover?", "labels": [], "entities": []}, {"text": "(Section 5.2) \u2022 Once we discovered political issues, positions and group-wise distribution, can the model be used to predict political affiliation?", "labels": [], "entities": []}, {"text": "(Section 5.3)  The issues extracted from our model are represented by the topics formed using topic words.", "labels": [], "entities": []}, {"text": "We list some of the topics extracted using PIE model in.", "labels": [], "entities": []}, {"text": "Each cell contains top 5 words of each topic with a manually assigned description in boldface.", "labels": [], "entities": []}, {"text": "These topics are the political issues underlying our dataset.", "labels": [], "entities": []}, {"text": "The issues discovered are \"health in-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average topic coherence per topic for different strate-", "labels": [], "entities": []}, {"text": " Table 3: Average cosine similarity between author-position dis-", "labels": [], "entities": [{"text": "Average cosine similarity", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8240689436594645}]}, {"text": " Table 5: Comparison of Our Political Issues with three Online Lists of Political Issues from About.com (A), Gallup (G) and", "labels": [], "entities": [{"text": "About.com", "start_pos": 94, "end_pos": 103, "type": "DATASET", "confidence": 0.977562665939331}, {"text": "Gallup (G)", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.8031418770551682}]}, {"text": " Table 7: Comparison of PIE model with past approaches for", "labels": [], "entities": []}]}