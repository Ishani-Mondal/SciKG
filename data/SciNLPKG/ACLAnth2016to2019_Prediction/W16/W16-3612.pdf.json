{"title": [{"text": "Character Identification on Multiparty Conversation: Identifying Mentions of Characters in TV Shows", "labels": [], "entities": [{"text": "Character Identification on Multiparty Conversation", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.7928780913352966}, {"text": "Identifying Mentions of Characters in TV Shows", "start_pos": 53, "end_pos": 99, "type": "TASK", "confidence": 0.8583769457680839}]}], "abstractContent": [{"text": "This paper introduces a subtask of entity linking, called character identification, that maps mentions in multiparty conversation to their referent characters.", "labels": [], "entities": [{"text": "character identification", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.7750053107738495}]}, {"text": "Transcripts of TV shows are collected as the sources of our corpus and automatically annotated with mentions by linguistically-motivated rules.", "labels": [], "entities": []}, {"text": "These mentions are manually linked to their referents through crowdsourcing.", "labels": [], "entities": []}, {"text": "Our corpus comprises 543 scenes from two TV shows, and shows the inter-annotator agreement of \u03ba = 79.96.", "labels": [], "entities": []}, {"text": "For statistical mod-eling, this task is reformulated as corefer-ence resolution, and experimented with a state-of-the-art system on our corpus.", "labels": [], "entities": [{"text": "corefer-ence resolution", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.7529357969760895}]}, {"text": "Our best model gives a purity score of 69.21 on average, which is promising given the challenging nature of this task and our corpus.", "labels": [], "entities": [{"text": "purity", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.996931791305542}]}], "introductionContent": [{"text": "Machine comprehension has recently become one of the main targeted challenges in natural language processing (.", "labels": [], "entities": [{"text": "Machine comprehension", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7927277088165283}, {"text": "natural language processing", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.6762888630231222}]}, {"text": "The latest approaches to machine comprehension show lots of promises; however, most of these approaches face difficulties in understanding information scattered across different parts of documents.", "labels": [], "entities": []}, {"text": "Reading comprehension in dialogues is particularly hard because speakers take turns to form a conversation such that it often requires connecting mentions from multiple utterances together to derive meaningful inferences.", "labels": [], "entities": [{"text": "Reading comprehension in dialogues", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.898203432559967}]}, {"text": "Coreference resolution is a common choice for making connections between these mentions.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9073134660720825}]}, {"text": "However, most of the state-of-the-art coreference resolution systems are not accustomed to handle dialogues well, especially when multiple participants are involved).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.9084465801715851}]}, {"text": "Furthermore, linking mentions to one another may not be good enough for certain tasks such as question answering, which requires to know what specific entities that mentions refer to.", "labels": [], "entities": [{"text": "question answering", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8721704483032227}]}, {"text": "This implies that the task needs to be approached from the side of entity linking, which maps each mention to one or more pre-determined entities.", "labels": [], "entities": []}, {"text": "In this paper, we introduce an entity linking task, called character identification, that maps each mention in multiparty conversation to its referent character(s).", "labels": [], "entities": [{"text": "character identification", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.7671533823013306}]}, {"text": "Mentions can be any nominals referring to humans.", "labels": [], "entities": []}, {"text": "At the moment, there is no dialogue corpus available to train statistical models for entity linking using such mentions.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.709030345082283}]}, {"text": "Thus, anew corpus is created by collecting transcripts of TV shows and annotating mentions with their referent characters.", "labels": [], "entities": []}, {"text": "Our corpus is experimented with a coreference resolution system to show the feasibility of this task by utilizing an existing technology.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8787786364555359}]}, {"text": "The contributions of this work include: \u2022 Introducing a subtask of entity linking, called character identification (Section 2).", "labels": [], "entities": [{"text": "entity linking", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.717473715543747}, {"text": "character identification", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.8851612210273743}]}, {"text": "\u2022 Creating anew corpus for character identification with thorough analysis (Section 3).", "labels": [], "entities": [{"text": "character identification", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.9455200135707855}]}, {"text": "\u2022 Reformulating character identification into a coreference resolution task (Section 4).", "labels": [], "entities": [{"text": "Reformulating character identification", "start_pos": 2, "end_pos": 40, "type": "TASK", "confidence": 0.8847905794779459}, {"text": "coreference resolution task", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.942315141359965}]}, {"text": "\u2022 Evaluating our approach to character identification on our corpus (Section 5).", "labels": [], "entities": [{"text": "character identification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.9406976997852325}]}, {"text": "To the best of our knowledge, it is the first time that character identification is experimented on such a large corpus.", "labels": [], "entities": [{"text": "character identification", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.9585202038288116}]}, {"text": "It is worth pointing out that character identification is just the first step to a bigger task called character mining.", "labels": [], "entities": [{"text": "character identification", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.9357281029224396}, {"text": "character mining", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.9018310606479645}]}, {"text": "Character mining is a task that focuses on extracting information and constructing knowledge bases associated with particular characters in contexts.", "labels": [], "entities": [{"text": "Character mining", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8401345610618591}]}, {"text": "The target entities are primarily participants, either spoken or mentioned, in dialogues.", "labels": [], "entities": []}, {"text": "The task can be subdivided into three sequential tasks, character identification, attribute extraction, and knowledge base construction.", "labels": [], "entities": [{"text": "character identification", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.9181438684463501}, {"text": "attribute extraction", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.7551983296871185}, {"text": "knowledge base construction", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.6471596360206604}]}, {"text": "Character mining is expected to facilitate and provide entity-specific knowledge for systems like question answering and dialogue generation.", "labels": [], "entities": [{"text": "Character mining", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8670472204685211}, {"text": "question answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.8889462351799011}, {"text": "dialogue generation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7201463580131531}]}, {"text": "We believe that these tasks altogether are beneficial for machine comprehension on multiparty conversation.", "labels": [], "entities": []}], "datasetContent": [{"text": "All systems are evaluated with the official CoNLL scorer on three metrics concerning coreference resolution: MUC, B 3 , and CEAF e . MUC MUC ( concerns the number of pairwise links needed to be inserted or removed to map system responses to gold keys.", "labels": [], "entities": [{"text": "CoNLL scorer", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.7830047011375427}, {"text": "coreference resolution", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.9101797938346863}, {"text": "B 3", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9508329033851624}, {"text": "CEAF", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.8936957120895386}]}, {"text": "The number of links the system and gold shared and minimum numbers of links needed to describe coreference chains of the system and gold are computed.", "labels": [], "entities": []}, {"text": "Precision is calculated by dividing the former with the latter that describes the system chains, and recall is calculated by dividing the former with the later that describes the gold chains.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9909473061561584}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9993895292282104}]}, {"text": "In stead of evaluating the coreference chains solely on their links, the B 3 (Bagga and Baldwin, 1998) metric computes precision and recall on a mention level.", "labels": [], "entities": [{"text": "B 3", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.96680948138237}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9956023693084717}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9993174076080322}]}, {"text": "System performance is evaluated by the average of all mention scores.", "labels": [], "entities": []}, {"text": "Given a set M that contains mentions denoted as mi . Coreference chains Sm i and G mi represent the chains containing mention mi in system and gold responses.", "labels": [], "entities": []}, {"text": "Precision(P) and recall(R) are calculated as below:) metric further points out the drawback of B 3 , in which entities can be used more than once during evaluation.", "labels": [], "entities": [{"text": "Precision(P)", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9431921392679214}, {"text": "recall(R)", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.966403067111969}, {"text": "B 3", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9837846755981445}]}, {"text": "As result, both multiple coreference chains of the same entity and chains with mentions of multiple entities are not penalized.", "labels": [], "entities": []}, {"text": "To cope with this problem, CEAF evaluates only on the best one-to-one mapping between the system's and gold's entities.", "labels": [], "entities": []}, {"text": "Given a system entity Si and gold entity G j . An entity-based similarity metric \u03c6(S i , G j ) gives the count of common mentions that refer to both Si and G j . The alignment with the best total similarity is denoted as \u03a6(g * ).", "labels": [], "entities": []}, {"text": "Thus precision(P) and recall(R) are measured as below.", "labels": [], "entities": [{"text": "precision(P)", "start_pos": 5, "end_pos": 17, "type": "METRIC", "confidence": 0.9462921023368835}, {"text": "recall(R)", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9692166447639465}]}, {"text": "Both the sieve system and the entity-centric system with its pre-trained model are first evaluated on our corpus.", "labels": [], "entities": []}, {"text": "The entity-centric system is further evaluated with new models trained on our corpus.", "labels": [], "entities": []}, {"text": "The gold mentions are used for these experiments because we want to focus solely on the performance analysis of these existing systems on our task.", "labels": [], "entities": []}, {"text": "Before looking at the results of the models trained on F1 and F1+F2, we anticipated that these models would give undesirable performance when evaluated on B1.", "labels": [], "entities": [{"text": "F1", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.89128577709198}]}, {"text": "Those models give the average scores: Character identification results on our corpus using cluster remapping on the coreference resolution system results.", "labels": [], "entities": [{"text": "Character identification", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8947862386703491}, {"text": "coreference resolution", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.8626792132854462}]}, {"text": "FC: found clusters after remapping.", "labels": [], "entities": []}, {"text": "EC: expected clusters from gold.", "labels": [], "entities": [{"text": "EC", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8084590435028076}]}, {"text": "UC: percentage of unknown clusters after remapping.", "labels": [], "entities": [{"text": "UC", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.5658132433891296}]}, {"text": "UM: percentage of unknown mentions in the unknown clusters to all the mentions. of 76.69 and 72.27 for B1 on the episode-level, and 79.62 and 79.01 for B1 on the scene-level, respectively.", "labels": [], "entities": [{"text": "UM", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.80262690782547}]}, {"text": "Surprisingly, the models trained on B1 do not yield a better accuracy on the episode-level (76.15), and show an improvement of 1.69 on the scene-level, which is smaller than expected.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9994531273841858}]}, {"text": "Thus, it is plausible to take models trained on one show and apply it to another for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.967292308807373}]}], "tableCaptions": [{"text": " Table 1: Composition of our corpus. Epi/Sce/Spk:  # of episodes/scenes/speakers. UC/SC/WC: # of  utterances/statements/words. Redundant speakers  between F1 & F2 are counted only once.", "labels": [], "entities": []}, {"text": " Table 2: Composition of the detected mentions.  NE: named entities, PRP: pronouns, PNN(%): sin- gular personal nouns and its ratio to all nouns.", "labels": [], "entities": [{"text": "PNN", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.8678730130195618}]}, {"text": " Table 3: Evaluation of our mention detection. P:  precision, R: recall, F: F1 score (in %).", "labels": [], "entities": [{"text": "mention detection", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.6175600588321686}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9741677045822144}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9006224274635315}, {"text": "F1 score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9641986191272736}]}, {"text": " Table 4: Proportions of the misses and errors of our  mention detection.", "labels": [], "entities": [{"text": "Proportions", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9653661251068115}, {"text": "misses", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.984919548034668}, {"text": "errors", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.7414764165878296}, {"text": "mention detection", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.6350045949220657}]}, {"text": " Table 5: An example of our annotation task conducted. Main character 1..n displays the names of all main  characters of the show. Extra character 1..m displays the names of high frequent, but not main, characters.", "labels": [], "entities": []}, {"text": " Table 6: Annotation analysis. Match and Kappa  show the absolute matching and Cohen's Kappa  scores between two annotators (in %). Col/Unk/Err  shows the percentage of mentions annotated as col- lective, unknown, and error, respectively.", "labels": [], "entities": [{"text": "Match", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9971518516540527}, {"text": "Cohen's Kappa  scores", "start_pos": 79, "end_pos": 100, "type": "METRIC", "confidence": 0.6315841674804688}, {"text": "error", "start_pos": 218, "end_pos": 223, "type": "METRIC", "confidence": 0.9586895108222961}]}, {"text": " Table 7: Coreference resolution results on our corpus. Stanford multi-pass sieve is a rule-based system.  Stanford entity-centric uses its pre-trained model. Every other row shows results achieved by the entity- centric system using models trained on the indicated training sets.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.9165315926074982}]}, {"text": " Table 8: Data splits. TRN/DEV/TST: training, de- velopment, and evaluation sets. See", "labels": [], "entities": [{"text": "TRN/DEV/TST", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.6603487491607666}]}, {"text": " Table 9: Character identification results on our corpus using cluster remapping on the coreference  resolution system results. FC: found clusters after remapping. EC: expected clusters from gold. UC:  percentage of unknown clusters after remapping. UM: percentage of unknown mentions in the unknown  clusters to all the mentions.", "labels": [], "entities": [{"text": "Character identification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8201505541801453}, {"text": "coreference  resolution", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.8249254822731018}]}]}