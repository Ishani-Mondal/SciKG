{"title": [{"text": "Creating a Novel Geolocation Corpus from Historical Texts", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the process of annotating a historical US civil war corpus with geographic reference.", "labels": [], "entities": []}, {"text": "Reference annotations are given at two different textual scales: individual place names and documents.", "labels": [], "entities": []}, {"text": "This is the first published corpus of its kind in document-level geolocation, and it has over 10,000 disambiguated to-ponyms, double the amount of any prior toponym corpus.", "labels": [], "entities": []}, {"text": "We outline many challenges and considerations in creating such a corpus, and we evaluate baseline and benchmark toponym resolution and document geolocation systems on it.", "labels": [], "entities": []}, {"text": "Aspects of the corpus suggest several recommendations for proper annotation procedure for the tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Geographic information is an important component of a number of areas including information retrieval, social media analysis, and historical research).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.7886601388454437}, {"text": "social media analysis", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.7524355252583822}]}, {"text": "To date however, very few corpora exist for text geolocation tasks, and those which do exist have flaws or are very small in size.", "labels": [], "entities": [{"text": "text geolocation tasks", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.8451367020606995}]}, {"text": "This is particularly true for tasks seeking to do geolocation work with historical texts.", "labels": [], "entities": []}, {"text": "In the realm of document geolocation, there exist no historical corpora whatsoever; in the realm of toponym resolution historical corpora exist, but are flawed in important respects (.", "labels": [], "entities": [{"text": "toponym resolution historical", "start_pos": 100, "end_pos": 129, "type": "TASK", "confidence": 0.7659700810909271}]}, {"text": "This paper describes the process of annotating a set of American Civil War archives commonly known as the Official Records of the War of the Rebellion (officially titled The War of the Rebel-: Statistics on WOTR, annotated subset and full data (using documents predicted based on a sequence model derived from the annotated data, as described in \u00a73).", "labels": [], "entities": [{"text": "Official Records of the War of the Rebellion", "start_pos": 106, "end_pos": 150, "type": "DATASET", "confidence": 0.9007432386279106}, {"text": "WOTR", "start_pos": 207, "end_pos": 211, "type": "DATASET", "confidence": 0.8389410972595215}]}, {"text": "lion: a Compilation of the Official Records of the Union and Confederate Armies and henceforth abbreviated as WOTR), arguably the most important and widely used corpus in this area of historical study . Document geolocation and toponym resolution enable work on the specific content of individual documents and themes contained within this corpus, revealing the ways in which content is distributed in the corpus overtime and space).", "labels": [], "entities": [{"text": "WOTR", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.759675920009613}, {"text": "toponym resolution", "start_pos": 228, "end_pos": 246, "type": "TASK", "confidence": 0.7480577528476715}]}, {"text": "Themes in this corpus pertinent to the study of Civil War literature include the rise of irregular warfare, the end of slavery in Confederate and Union states, the use of railroads by United States and Confederate armies in the war, and the destruction of the warmaking capacity of the Confederate states.", "labels": [], "entities": []}, {"text": "The annotation process and geolocation tools also enable historians to reexamine the process by which the archive was produced, an area which has recently seen growing interest.", "labels": [], "entities": []}, {"text": "We develop geolocation corpora for two related but separate tasks: document geolocation (docgeo) and toponym resolution (TR).", "labels": [], "entities": [{"text": "toponym resolution (TR)", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.8051090598106384}]}, {"text": "Statistics on the full WOTR corpus and the annotated document geolocation and toponym subsets are shown in.", "labels": [], "entities": [{"text": "WOTR corpus", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.9135207831859589}]}, {"text": "Geographic summaries of the annotations are given in (documents) and (toponyms).", "labels": [], "entities": []}, {"text": "The docgeo annotations are concen-: Statistics on WOTR, annotated subset (using documents predicted based on a sequence model derived from the annotated data, as described in \u00a73).", "labels": [], "entities": [{"text": "WOTR", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9054962992668152}]}, {"text": "trated in a number of areas that saw heavy fighting, such as in Virginia, South Carolina and Northern Georgia.", "labels": [], "entities": []}, {"text": "The toponym annotations are more concentrated around the western theater of the Civil War.", "labels": [], "entities": [{"text": "toponym annotations", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8479910492897034}]}, {"text": "In both corpora, almost all US states are represented by at least some references.", "labels": [], "entities": []}, {"text": "The toponym annotations contain more full-state polygons, while the docgeo annotations are primarily points, leading to the differing appearances of the two maps.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to gain an understanding of the difficulties of the corpus and encourage its adoption, we evaluate the performance of a number of baseline and benchmark systems on the dataset.", "labels": [], "entities": []}, {"text": "For docgeo, two methods are used for constructing grid cells: Uniform and adaptive (KD), which adjusts cell sizes to equalize the number of documents in each cell.", "labels": [], "entities": []}, {"text": "LR uses flat logistic regression while Hier constructs a coarse-to-fine hierarchy of grids with abeam search ( . For TR, Population selects a matching gazetteer referent with the highest population.", "labels": [], "entities": []}, {"text": "WISTR is a bag of words multinomial logistic regression model trained on Wikipedia (.", "labels": [], "entities": [{"text": "WISTR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9549574851989746}]}, {"text": "SPIDER is a weighted distance minimization approach that prefers selecting gazetteer referents that occupy minimal area (.", "labels": [], "entities": [{"text": "weighted distance minimization", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.7115533749262491}]}, {"text": "TopoCluster uses a geographic density estimation of the toponym and context words; TopoClusterGaz 5 additionally 'snaps' to the nearest gazetteer referent.", "labels": [], "entities": []}, {"text": "All TR systems were  trained using out of domain resources, but some weights and parameters (e.g. context window size) were optimized using the WOTR dev set.", "labels": [], "entities": [{"text": "WOTR dev set", "start_pos": 144, "end_pos": 156, "type": "DATASET", "confidence": 0.8592389027277628}]}, {"text": "shows the results of a number of current text-only document geolocation systems on WOTR.", "labels": [], "entities": [{"text": "WOTR", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.9759239554405212}]}, {"text": "Compared with Naive Bayes, both flat (LR) and hierarchical logistic regression (Hier) produce additional benefits.", "labels": [], "entities": [{"text": "hierarchical logistic regression (Hier)", "start_pos": 46, "end_pos": 85, "type": "METRIC", "confidence": 0.7077174683411916}]}, {"text": "Hier produces the best mean and median despite the fact that it is designed primarily for larger corpora than WOTR.", "labels": [], "entities": [{"text": "mean", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9859399795532227}, {"text": "median", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9218823909759521}, {"text": "WOTR", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.871135413646698}]}, {"text": "Uniform grids do slightly better overall, a result we have seen before in similar-sized corpora, but adaptive (KD) grids do better with Hier, which is able to compensate somewhat for the larger adaptive grid cells found in low-density areas through its use of multiple grid levels.", "labels": [], "entities": []}, {"text": "shows the resolution results of many state-of-the-art toponym resolution systems on the test split of WOTR.", "labels": [], "entities": [{"text": "WOTR", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.8722965121269226}]}, {"text": "As can be seen, TopoClusterGaz outperforms all resolvers on all metrics when oracle NER is used, and outperforms others on Recall and F-1 Score when predictive NER is included in the evaluation.", "labels": [], "entities": [{"text": "Recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.6782849431037903}, {"text": "F-1 Score", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9140917062759399}]}, {"text": "Key to the TopoClusterGaz's success is the ability to predict on both non-gazetteer and gazetteer matched entities, directly boosting Recall and F-1 Score by large margins.", "labels": [], "entities": [{"text": "Recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.981894314289093}, {"text": "F-1 Score", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9509169459342957}]}, {"text": "When evaluating on a development set of the data, we observed that most differences in system performance could be sourced to how the respective systems dealt with place names that do not have specific GeoNames entries, or are spelled differently than their GeoNames entry (e.g. Camp Lapwai, Colo. Terr.).", "labels": [], "entities": [{"text": "Camp Lapwai, Colo. Terr.", "start_pos": 279, "end_pos": 303, "type": "DATASET", "confidence": 0.8096038043498993}]}, {"text": "TopoCluster often produced correct predictions on these entities, while the gazetteer dependent systems like Population, WISTR, and SPIDER were unable to make predictions.", "labels": [], "entities": [{"text": "WISTR", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.7827391028404236}]}, {"text": "NER inclusive scores (P, R, F-1) are generally much lower for WoTRTopo than other datasets because the NER systems utilized (Stanford-NER and openNLP-NER) are trained on very different domains.", "labels": [], "entities": [{"text": "WoTRTopo", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.7888513207435608}, {"text": "openNLP-NER", "start_pos": 142, "end_pos": 153, "type": "DATASET", "confidence": 0.9005781412124634}]}, {"text": "Nevertheless, strongly superior recall on the gazetteerindependent TopoCluster systems leads to higher F-1 scores on the dataset.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9995017051696777}, {"text": "F-1", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9970754384994507}]}], "tableCaptions": [{"text": " Table 1: Statistics on WOTR, annotated subset  and full data (using documents predicted based on  a sequence model derived from the annotated data,  as described in  \u00a73).", "labels": [], "entities": [{"text": "WOTR", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.7958995699882507}]}, {"text": " Table 4: WoTR Toponym Resolution Results", "labels": [], "entities": [{"text": "WoTR Toponym Resolution", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7786262035369873}]}, {"text": " Table 5: Doc Geolocation Results", "labels": [], "entities": [{"text": "Doc Geolocation", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6792299151420593}]}]}