{"title": [{"text": "A Joint Model for Word Embedding and Word Morphology", "labels": [], "entities": [{"text": "Word Morphology", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.602423682808876}]}], "abstractContent": [{"text": "This paper presents a joint model for performing unsupervised morphological analysis on words, and learning a character-level composition function from morphemes to word embeddings.", "labels": [], "entities": []}, {"text": "Our model splits individual words into segments, and weights each segment according to its ability to predict context words.", "labels": [], "entities": []}, {"text": "Our morphological analysis is comparable to dedicated morphological analyzers at the task of morpheme boundary recovery, and also performs better than word-based embedding models at the task of syntactic analogy answering.", "labels": [], "entities": [{"text": "morpheme boundary recovery", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.6522407829761505}, {"text": "syntactic analogy answering", "start_pos": 194, "end_pos": 221, "type": "TASK", "confidence": 0.8094758788744608}]}, {"text": "Finally, we show that incorporating morphology explicitly into character-level models helps them produce embeddings for unseen words which correlate better with human judgments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embedding models associate each word in a corpus with a vector in a semantic space.", "labels": [], "entities": []}, {"text": "These vectors can either be learnt to optimize performance in a downstream task () or learnt via the distributional hypothesis: words with similar contexts have similar meanings).", "labels": [], "entities": []}, {"text": "Current word embedding models treat words as atomic.", "labels": [], "entities": []}, {"text": "However, words follow a power law distribution, and word embedding models suffer from the problem of sparsity: a word like 'unbelievableness' does not appear at all in the first 17 million words of Wikipedia, even though it is derived from common morphemes.", "labels": [], "entities": []}, {"text": "This leads to three problems: 1.", "labels": [], "entities": []}, {"text": "word representations decline in quality for rarely observed words).", "labels": [], "entities": []}, {"text": "2. word embedding models handle out-ofvocabulary words badly, typically as a single 'OOV' token.", "labels": [], "entities": []}, {"text": "3. the word distribution has along tail, and many parameters are needed to capture all of the words in a corpus (for an embedding size of 300 with a vocabulary of 10k words, 3 million parameters are needed) One approach to smooth word distributions is to operate on the smallest meaningful semantic unit, the morpheme ().", "labels": [], "entities": []}, {"text": "However, previous work on the morpheme level has all used external morphological analyzers.", "labels": [], "entities": []}, {"text": "These require a separate preprocessing step, and cannot be adapted to suit the problem at hand.", "labels": [], "entities": []}, {"text": "Another is to operate on the smallest orthographic unit, the character (.", "labels": [], "entities": []}, {"text": "However, the link between shape and meaning is often complicated (, as alphabetic characters carry no inherent semantic meaning.", "labels": [], "entities": []}, {"text": "To account for this, the model has to learn complicated dependencies between strings of characters to accurately capture word meaning.", "labels": [], "entities": []}, {"text": "We hypothesize that explicitly introducing morphology into character-level models can help them learn morphological features, and hence word meaning.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a word embedding model that jointly learns word morphology and word embeddings.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first word embedding model that learns morphology as part of the model.", "labels": [], "entities": []}, {"text": "Our guiding intuition is that the words with the same stem have similar contexts.", "labels": [], "entities": []}, {"text": "Thus, when considering word segments in terms of context-predictive power, the segment corresponding to the stem will have the most weight.", "labels": [], "entities": []}, {"text": "Our model 'reads' the word and outputs a sequence of word segments.", "labels": [], "entities": []}, {"text": "We weight each segment, and then combine the segments to obtain the final word representation.", "labels": [], "entities": []}, {"text": "These representations are trained to predict context words, as this has been shown to give word representations which capture word semantics well).", "labels": [], "entities": []}, {"text": "As the root morpheme has the most context-predictive power, we expect our model to assign high weight to this segment, thereby learning to separate root+affix structures.", "labels": [], "entities": []}, {"text": "One exciting feature of character-level models is their ability to represent open-vocabulary words.", "labels": [], "entities": []}, {"text": "After training, they can predict a vector for any word, not just words that they have seen before.", "labels": [], "entities": []}, {"text": "Our model has an advantage in that it can split unknown words into known and unknown components.", "labels": [], "entities": []}, {"text": "Hence, it can potentially generalise better overseen morphemes and words and apply existing knowledge to new cases.", "labels": [], "entities": []}, {"text": "To evaluate our model, we evaluate its use as a morphological analyzer ( \u00a74.1), test how well it learns word semantics, including for unseen words ( \u00a74.2), and examine the structure of the embedding space ( \u00a74.3).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on three tasks: morphological analysis ( \u00a74.1), semantic similarity ( \u00a74.2), and analogy retrieval ( \u00a74.3).", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.750347912311554}, {"text": "analogy retrieval", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.8484621644020081}]}, {"text": "We trained all of the models once, and then use the same trained model for all three tasks -we do not perform hyperparameter tuning to optimize performance on each task.", "labels": [], "entities": []}, {"text": "We trained our Char2Vec model on the Text8 corpus, consisting of the first 100MB of a 2006 cleaned-up dump of Wikipedia 1 . We only trained on words which appeared more than 5 times in our corpus.", "labels": [], "entities": [{"text": "Text8 corpus", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.9700976610183716}]}, {"text": "We used a context window size of 3 words either side of the target word, and took 11 negative samples per positive sample, using the same smoothed unigram distribution as word2vec.", "labels": [], "entities": []}, {"text": "The model was trained for 3 epochs using the Adam optimizer (.", "labels": [], "entities": []}, {"text": "All experiments were carried out using Keras As baselines, we trained a SGNS model on the same dataset with the same parameters.", "labels": [], "entities": []}, {"text": "To test how much the attention model helps the characterlevel model to generalize, we also trained the Char2Vec model without the attention layer, but with the same parameters.", "labels": [], "entities": []}, {"text": "In this model, the word embeddings are just the concatenation of the final forward and backward states, passed through a feedforward layer.", "labels": [], "entities": []}, {"text": "We refer to this model as C2V-NO-ATT.", "labels": [], "entities": []}, {"text": "We also constructed count-based vectors using SVD on PPMI-weighted co-occurence counts, with a window size of 3.", "labels": [], "entities": []}, {"text": "We kept the top 256 principal components in the SVD decomposition, to obtain embeddings with the same size as our other models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results at retrieving intra-word mor- pheme boundaries.", "labels": [], "entities": []}, {"text": " Table 3: Similarity correlations of in-vocabulary  word pairs between the models and human anno- tators.", "labels": [], "entities": []}, {"text": " Table 4: Similarity correlations of all word pairs  between the character-level models and human an- notators. RW OOV indicates results specifically  on pairs in the RW dataset with at least one word  not seen in the training corpus.", "labels": [], "entities": [{"text": "RW", "start_pos": 112, "end_pos": 114, "type": "DATASET", "confidence": 0.4146123230457306}, {"text": "OOV", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.5054954886436462}, {"text": "RW dataset", "start_pos": 167, "end_pos": 177, "type": "DATASET", "confidence": 0.8872611224651337}]}, {"text": " Table 5: Filtered and unfiltered model nearest neighbours for some in-vocabulary and out-of-vocabulary  words", "labels": [], "entities": []}, {"text": " Table 6: Results on the Google analogy task", "labels": [], "entities": [{"text": "Google analogy task", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.7667336265246073}]}]}