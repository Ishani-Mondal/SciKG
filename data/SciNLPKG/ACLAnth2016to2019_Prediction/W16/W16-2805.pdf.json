{"title": [{"text": "The CASS Technique for Evaluating the Performance of Argument Mining", "labels": [], "entities": []}], "abstractContent": [{"text": "Argument mining integrates many distinct computational linguistics tasks, and as a result, reporting agreement between anno-tators or between automated output and gold standard is particularly challenging.", "labels": [], "entities": [{"text": "Argument mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.825928121805191}]}, {"text": "More worrying for the field, agreement and performance are also reported in a wide variety of different ways, making comparison between approaches difficult.", "labels": [], "entities": [{"text": "agreement", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9816983938217163}]}, {"text": "To solve this problem, we propose the CASS technique for combining metrics covering different parts of the argument mining task.", "labels": [], "entities": [{"text": "CASS", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.814744234085083}, {"text": "argument mining task", "start_pos": 107, "end_pos": 127, "type": "TASK", "confidence": 0.8100906014442444}]}, {"text": "CASS delivers a justified method of integrating results yielding confusion matrices from which CASS-\u03ba and CASS-F1 scores can be calculated.", "labels": [], "entities": [{"text": "CASS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8859618306159973}]}], "introductionContent": [{"text": "To calculate the agreement, or similarity, between two different argumentative structures is an important and commonly occurring task in argument mining.", "labels": [], "entities": [{"text": "similarity", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.8112573027610779}, {"text": "argument mining", "start_pos": 137, "end_pos": 152, "type": "TASK", "confidence": 0.8454784750938416}]}, {"text": "For example, measures of similarity are required to determine the efficacy of annotation guidelines via inter-annotator agreement, and to compare test analyses against a gold standard, whether these test analyses are produced by students, or automated argument mining techniques (cf.).", "labels": [], "entities": [{"text": "argument mining", "start_pos": 252, "end_pos": 267, "type": "TASK", "confidence": 0.700186237692833}]}, {"text": "To find the the similarity of automatic and manually segmented texts and what impact these segments have on agreement between annotations for an overall argument structure, is a complex task.", "labels": [], "entities": []}, {"text": "Similar to these problems is the task of evaluating the argumentative structure of annotations using pre-segmented text.", "labels": [], "entities": []}, {"text": "Despite the relative ease of manually analysing these situations, arguments with long relations can easily make this task complex.", "labels": [], "entities": []}, {"text": "Commonly to find the agreement of manual annotators or the effectiveness of an automatic solution, two scores are given, Cohen's kappa, which takes into account the observed agreement between two annotators and the chance agreement, giving an overall kappa value for agreement, and F1 score (, which is the harmonic mean of the precision and recall of an algorithm.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 282, "end_pos": 290, "type": "METRIC", "confidence": 0.993184894323349}, {"text": "precision", "start_pos": 328, "end_pos": 337, "type": "METRIC", "confidence": 0.9984928369522095}, {"text": "recall", "start_pos": 342, "end_pos": 348, "type": "METRIC", "confidence": 0.9831013083457947}]}, {"text": "The way in which these scores are utilised can over penalise differences in argumentative structures.", "labels": [], "entities": []}, {"text": "In particular, if used incorrectly, Cohen's kappa can penalise doubly (penalise for segmentation and penalise segmentation in argumentative structures) if not split into separate tasks or penalise too harshly when annotations have only slight differences, again if the calculation is not split by argumentative structure.", "labels": [], "entities": []}, {"text": "When using the F1 score the same problems arise without split calculations.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9986457228660583}]}], "datasetContent": [], "tableCaptions": []}