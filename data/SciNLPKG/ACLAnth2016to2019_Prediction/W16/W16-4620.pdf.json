{"title": [{"text": "Controlling the Voice of a Sentence in Japanese-to-English Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.6456619501113892}]}], "abstractContent": [{"text": "In machine translation, we must consider the difference in expression between languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7683813869953156}]}, {"text": "For example, the active/passive voice may change in Japanese-English translation.", "labels": [], "entities": []}, {"text": "The same verb in Japanese maybe translated into different voices at each translation because the voice of a generated sentence cannot be determined using only the information of the Japanese sentence.", "labels": [], "entities": []}, {"text": "Machine translation systems should consider the information structure to improve the coherence of the output by using several topicalization techniques such as passivization.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7975022792816162}]}, {"text": "Therefore, this paper reports on our attempt to control the voice of the sentence generated by an encoder-decoder model.", "labels": [], "entities": []}, {"text": "To control the voice of the generated sentence, we added the voice information of the target sentence to the source sentence during the training.", "labels": [], "entities": []}, {"text": "We then generated sentences with a specified voice by appending the voice information to the source sentence.", "labels": [], "entities": []}, {"text": "We observed experimentally whether the voice could be controlled.", "labels": [], "entities": []}, {"text": "The results showed that, we could control the voice of the generated sentence with 85.0% accuracy on average.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9992392063140869}]}, {"text": "In the evaluation of Japanese-English translation, we obtained a 0.73-point improvement in BLEU score by using gold voice labels.", "labels": [], "entities": [{"text": "Japanese-English translation", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.5780924260616302}, {"text": "BLEU score", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9800330698490143}]}], "introductionContent": [{"text": "Ina distant language pair such as Japanese-English, verbs between the source language and the target language are often used differently.", "labels": [], "entities": []}, {"text": "In particular, the voices of the source and target sentences are sometimes different in a fluent translation when considering the discourse structure of the target side because Japanese is a pro-drop language and does not the use passive voice for object topicalization.", "labels": [], "entities": [{"text": "object topicalization", "start_pos": 248, "end_pos": 269, "type": "TASK", "confidence": 0.7235966771841049}]}, {"text": "In, we show the number of occurrences of each voice in high-frequency verbs in Asian Scientific Paper Expert Corpus (ASPEC;).", "labels": [], "entities": [{"text": "Asian Scientific Paper Expert Corpus (ASPEC", "start_pos": 79, "end_pos": 122, "type": "DATASET", "confidence": 0.909681669303349}]}, {"text": "In the top seven high frequency verbs, \"show\" tended to be used in active voice, whereas \"examine,\" \"find,\" and \"observe\" tended to be used in the passive voice.", "labels": [], "entities": []}, {"text": "However, \"describe,\" \"explain,\" and \"introduce\" tended not to be used in any particular voice.", "labels": [], "entities": []}, {"text": "For example, the voice of the verb \"introduce\" could not be determined uniquely, because it was sometimes used in phrases like \"This paper introduces ...\" and, sometimes, \"... are introduced.\"", "labels": [], "entities": []}, {"text": "Therefore, it is possible that the translation model failed to learn the correspondence between Japanese and English.", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9648136496543884}]}, {"text": "Recently, recurrent neural networks (RNNs) such as encoder-decoder models have gained considerable attention in machine translation because of their ability to generate fluent sentences.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7356700599193573}]}, {"text": "However, compared to traditional statistical machine translation, it is not straightforward to interpret and control the output of the encoder-decoder models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.6437588135401408}]}, {"text": "Several attempts have been made to control the output of the encoderdecoder models.", "labels": [], "entities": []}, {"text": "First, proposed anew Long Short-Term Memory (LSTM) network to control the length of the sentence generated by an encoder-decoder model in a text summarization task.", "labels": [], "entities": [{"text": "text summarization task", "start_pos": 140, "end_pos": 163, "type": "TASK", "confidence": 0.7912797133127848}]}, {"text": "In their experiment, they controlled the sentence length while maintaining the performance compared to the results of previous works.", "labels": [], "entities": []}, {"text": "Second, attempted to control the honorific in English-German neural machine translation (NMT).", "labels": [], "entities": [{"text": "English-German neural machine translation (NMT)", "start_pos": 46, "end_pos": 93, "type": "TASK", "confidence": 0.696186751127243}]}, {"text": "They trained an attentional encoder-decoder model using English (source) data to which the honorific information of a German (target) sentence was added.", "labels": [], "entities": []}, {"text": "They restricted the honorific on the German side at the test phase.", "labels": [], "entities": []}, {"text": "Similar to, this paper reports on our attempt to control the voice of a sentence generated by an encoder-decoder model.", "labels": [], "entities": []}, {"text": "At the preprocessing phase, we determined the voice of the root phrase in the target side by parsing and added it to the end of the source sentence as a voice label.", "labels": [], "entities": []}, {"text": "At the training phase, we trained an attentional encoder-decoder model by using the preprocessed source data.", "labels": [], "entities": []}, {"text": "Lastly, we controlled the voice of the generated sentence by adding a voice label to the source sentence at the test phase.", "labels": [], "entities": []}, {"text": "We tested several configurations: (1) controlling all sentences to active/passive voices, (2) controlling each sentence to the same voice as the reference sentence, and (3) predicting the voice using only the source sentence.", "labels": [], "entities": [{"text": "predicting", "start_pos": 173, "end_pos": 183, "type": "TASK", "confidence": 0.9599701762199402}]}, {"text": "The result showed that we were able to control the voice of the generated sentence with 85.0% accuracy on average.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9992105960845947}]}, {"text": "In the evaluation of the Japanese-English translation, we obtained a 0.73-point improvement in BLEU score compared to the NMT baseline, in the case of using the voice information of the references.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9796596765518188}, {"text": "NMT baseline", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.8622355759143829}]}], "datasetContent": [{"text": "We conducted two types of evaluations: evaluation of the controlling accuracy and evaluation of the machine translation quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9702142477035522}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7261946201324463}]}, {"text": "We tested the following four patterns of labeling the voice features to evaluate  the extent to which the voice of the generated sentence was controlled correctly.", "labels": [], "entities": []}, {"text": "Controlling all target sentences to the active voice.", "labels": [], "entities": []}, {"text": "Controlling all target sentences to the passive voice.", "labels": [], "entities": []}, {"text": "Controlling each target sentence to the same voice as that of the reference sentence.", "labels": [], "entities": []}, {"text": "Controlling each target sentence to the predicted voice.", "labels": [], "entities": []}, {"text": "There were two reasons for testing ALL_ACTIVE and ALL_PASSIVE: to evaluate how correctly we counld control the voice, and to discuss the source of errors.", "labels": [], "entities": [{"text": "ALL_PASSIVE", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.7070764104525248}]}, {"text": "In REFERENCE, the generated sentences tended to be natural.", "labels": [], "entities": []}, {"text": "However, in ALL_ACTIVE and ALL_PASSIVE, the generated sentences were sometimes unnatural in terms of the voice.", "labels": [], "entities": [{"text": "PASSIVE", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.704917311668396}]}, {"text": "We identified these sentences to investigate the reasons why these errors occurred.", "labels": [], "entities": []}, {"text": "We checked the voice of the generated sentence and calculated the accuracy manually because the performance of voice labeling depends on the performance of the parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9997195601463318}]}, {"text": "We used the Stanford Parser (ver. 3.5.2) to parse the English sentence.", "labels": [], "entities": [{"text": "Stanford Parser (ver. 3.5.2)", "start_pos": 12, "end_pos": 40, "type": "DATASET", "confidence": 0.8970501820246378}]}, {"text": "The labelling performance was 95% in this experiment.", "labels": [], "entities": []}, {"text": "We used CaboCha (ver. 0.68;) to obtain the root phrase of the Japanese sentence in PREDICT.", "labels": [], "entities": [{"text": "CaboCha (ver. 0.68", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.7762422412633896}, {"text": "PREDICT", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.8215444684028625}]}, {"text": "If the sentence was a complex sentence, we checked the voice of the root verb 2 . The test data of ASPEC consisted of 1,812 sentences in total.", "labels": [], "entities": [{"text": "ASPEC", "start_pos": 99, "end_pos": 104, "type": "DATASET", "confidence": 0.6079450249671936}]}, {"text": "The evaluation data for the voice controlling consisted of 100 passive sentences and 100 active sentences chosen from the top of the test data.", "labels": [], "entities": []}, {"text": "We did not consider subject and object alternation because this evaluation only focused on the voice of the sentence.", "labels": [], "entities": [{"text": "subject and object alternation", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.7382783442735672}]}, {"text": "Only one evaluator performed an annotation.", "labels": [], "entities": []}, {"text": "In this experiment, the accuracy was calculated as the agreement between the label and the voice of the generated sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9996333122253418}]}, {"text": "\"Error sentence\" means the root verb of the generated sentence could not be distinguished manually, or it did not include a verb, and soon.", "labels": [], "entities": [{"text": "Error", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9865744709968567}]}, {"text": "The baseline was an attentional encoder-decoder by, which does not control the voice.", "labels": [], "entities": []}, {"text": "In the evaluation of the Japanese-English translation, we calculated the BLEU () score with the test data of all 1,812 sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.999553382396698}]}, {"text": "At the training phase, we used 827,503 sentences, obtained by eliminating sentences with more than 40 words in the first 1 million sentences of the ASPEC.", "labels": [], "entities": [{"text": "ASPEC", "start_pos": 148, "end_pos": 153, "type": "DATASET", "confidence": 0.6408738493919373}]}, {"text": "Word2Vec 3 (Mikolov et al., 2013) was trained with all 3 million sentences of ASPEC.", "labels": [], "entities": [{"text": "Word2Vec 3", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9290138483047485}, {"text": "ASPEC", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.7473934292793274}]}, {"text": "The vocabulary size was 30,000 . The dimension of the embeddings and hidden units was 512.", "labels": [], "entities": []}, {"text": "The batch size was 128.", "labels": [], "entities": []}, {"text": "The optimizer was Adagrad, and the learning rate was 0.01.", "labels": [], "entities": [{"text": "Adagrad", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.8374751806259155}, {"text": "learning rate", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.9789402186870575}]}, {"text": "We used Chainer 1.12 () to implementing the neural network.", "labels": [], "entities": []}, {"text": "shows the accuracy of the voice control and the BLEU score of the translation 5 . In the baseline, our system tended to generate a passive sentence compared to the voice distribution of the reference because the number of passive sentences was greater than that of the active sentences in the training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997707009315491}, {"text": "BLEU score", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9785318672657013}]}, {"text": "The accuracy of the baseline was calculated as the agreement between the voice of the generated sentence and that of the reference.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995125532150269}]}, {"text": "ALL_ACTIVE and ALL_PASSIVE demonstrated that the voice could be controlled with high performance.", "labels": [], "entities": [{"text": "ALL_PASSIVE", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.5819082657496134}]}, {"text": "The BLEU score became lower than the baseline because some sentences were transformed into different voices regardless of the contexts and voice distribution.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.983294278383255}]}, {"text": "In other words, active sentences in the test data included sentences whose root verb of the reference was an intransitive verb.", "labels": [], "entities": []}, {"text": "Even in that case, we forced the voice of the generated sentence to become passive in ALL_PASSIVE.", "labels": [], "entities": [{"text": "ALL_PASSIVE", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.8498795827229818}]}, {"text": "As a result, the voice of some sentences did not become passive, compared to other sentences that were controlled to become passive sentences if not natural.", "labels": [], "entities": []}, {"text": "REFERENCE achieved the highest accuracy, and its voice distribution was close to that of the references.", "labels": [], "entities": [{"text": "REFERENCE", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.49038779735565186}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9995266199111938}]}, {"text": "As mentioned earlier, the voice of REFERENCE was more natural than that of ALL_ACTIVE or ALL_PASSIVE.", "labels": [], "entities": [{"text": "REFERENCE", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.908409059047699}]}, {"text": "We obtained a 0.73-point improvement in the BLEU score compared to the baseline . Therefore, we found that there is room for improvement if we can correctly predict the voice of the reference.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9704561531543732}]}, {"text": "PREDICT used the labels predicted from the voice distribution.", "labels": [], "entities": [{"text": "PREDICT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9248454570770264}]}, {"text": "It tended to generate a passive sentence compared to the baseline.", "labels": [], "entities": []}, {"text": "The controlling accuracy was 87.5% because the voice distributions were skewed in many verbs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.862557590007782}]}, {"text": "However, the agreement rate between the predicted and the reference voices was 63.7%.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 13, "end_pos": 27, "type": "METRIC", "confidence": 0.990730345249176}]}, {"text": "Therefore, PREDICT failed to predict the voice of the reference, especially with high-frequency verbs, resulting in decrease in the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 132, "end_pos": 142, "type": "METRIC", "confidence": 0.9847701787948608}]}, {"text": "We leave the prediction of the voice of references as a future work.", "labels": [], "entities": [{"text": "prediction of the voice of references", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.8429435888926188}]}, {"text": "We show the output examples in.", "labels": [], "entities": []}, {"text": "Examples 1, 2, and 3 are the success cases, whereas Examples 4 and 5 are the failure cases.", "labels": [], "entities": []}, {"text": "Examples 1 and 2 showed that the voice of the generated sentence was correctly controlled.", "labels": [], "entities": []}, {"text": "When a passive sentence was changed into an active sentence, a subject was needed.", "labels": [], "entities": []}, {"text": "Both examples generated adequate subjects depending on the context.", "labels": [], "entities": []}, {"text": "In Example 3, although the voice was controlled, the subject and object were not exchanged.", "labels": [], "entities": []}, {"text": "Besides this example, there were many sentences that persisted the \"beverb + verb in past participle form\" structure when adding the <Passive> label was added.", "labels": [], "entities": []}, {"text": "For example, the \"...", "labels": [], "entities": []}, {"text": "can be done ...\" structure was changed into the \"... is able to be done ...\" structure.", "labels": [], "entities": []}, {"text": "In this experiment, we did not evaluate whether the subject and object were exchanged, but it maybe necessary to distinguish these patterns for the purpose of improving the coherence of the discourse structure.", "labels": [], "entities": []}, {"text": "In Example 4, it was impossible to make a passive sentence because the root verb in the target sentence should bean intransitive verb.", "labels": [], "entities": []}, {"text": "Most of the active sentences in ALL_PASSIVE should stay active sentences that used intransitive verbs.", "labels": [], "entities": [{"text": "ALL_PASSIVE", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.5696455240249634}]}, {"text": "Like Example 3, there were many sentences that were successfully controlled by using the \"be found to be ...\" structure when an intransitive verb was included as a root verb.", "labels": [], "entities": []}, {"text": "Example 5 showed the case wherein the voice could not be controlled despite the attempt to control it to the active voice.", "labels": [], "entities": []}, {"text": "The frequency of the voice of the verb \"detect\" in the training data consisted of 468 active-voice sentences and 2,858 passive sentences.", "labels": [], "entities": []}, {"text": "When we forced the voice of the generated sentence to become active, the result of generation tended to fail sometimes if we input the verb that had few examples of active sentences in the training data.", "labels": [], "entities": []}, {"text": "The subject should be generated if we forced the voice of the generated sentence to become active.", "labels": [], "entities": []}, {"text": "However, the encoder-decoder model did not know what to generate as a subject if the training data had only a few examples of an active sentence for that verb.", "labels": [], "entities": []}, {"text": "On the other hand, when we forced the voice of the generated sentence to become passive, we failed to find any tendencies of this type of the failure.", "labels": [], "entities": []}, {"text": "We would like to do some additional investigation on the tendency of this result.", "labels": [], "entities": []}, {"text": "shows the results of two methods submitted for the shared task at WAT 2016 ().", "labels": [], "entities": [{"text": "WAT 2016", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.6892637014389038}]}, {"text": "The BLEU, RIBES, and AMFM () were calculated  automatically, and HUMAN was evaluated by the pairwise crowdsourcing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9990978240966797}, {"text": "RIBES", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9881228804588318}, {"text": "AMFM", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.94619220495224}, {"text": "HUMAN", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9948211908340454}]}, {"text": "Note that the NMT baseline is different from the baseline of the voice controlling experiment reported in the previous section.", "labels": [], "entities": []}, {"text": "6 ensemble: We performed an ensemble learning of the NMT baseline.", "labels": [], "entities": [{"text": "NMT baseline", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.8982127904891968}]}, {"text": "Because of the lack of time, we trained the baseline NMT only twice.", "labels": [], "entities": []}, {"text": "Thus, we chose three models that showed the three highest BLEU scores from all epochs of the development set for each NMT baseline, resulting in 6 ensemble.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9987420439720154}]}, {"text": "As a result, BLEU score achieves 18.45.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.8979891836643219}]}, {"text": "It improves 1.56 point compared with the result of the single NMT Baseline.", "labels": [], "entities": [{"text": "1.56", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9756041169166565}]}, {"text": "PREDICT (2016 our proposed method to control output voice): We submitted our system in the configuration of PREDICT for pairwise crowdsourcing evaluation.", "labels": [], "entities": [{"text": "PREDICT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.909359335899353}]}, {"text": "It improved by 1.40 points in the BLEU score compared to the NMT baseline.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9684608578681946}, {"text": "NMT baseline", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.9392337799072266}]}, {"text": "Since we did not perform an ensemble learning for PRE-DICT, we expected a similar improvement in the BLEU score if we combined multiple models of PRE-DICT using an ensemble technique.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9746070504188538}]}], "tableCaptions": [{"text": " Table 1: Number of occurrences of each voice in high-frequency verbs.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy of voice controlling and BLEU score of the translation.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991633892059326}, {"text": "BLEU score", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.979049414396286}]}, {"text": " Table 3: Examples of the generated sentences", "labels": [], "entities": []}, {"text": " Table 4: Evaluation scores of WAT 2016.", "labels": [], "entities": [{"text": "WAT 2016", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.6869309842586517}]}]}