{"title": [{"text": "EM-Training for Weighted Aligned Hypergraph Bimorphisms", "labels": [], "entities": []}], "abstractContent": [{"text": "We develop the concept of weighted aligned hypergraph bimorphism where the weights may, in particular, represent probabilities.", "labels": [], "entities": []}, {"text": "Such a bimorphism consists of an R \u22650-weighted regular tree grammar, two hypergraph algebras that interpret the generated trees, and a family of alignments between the two interpretations.", "labels": [], "entities": []}, {"text": "Semantically , this yields a set of bihypergraphs each consisting of two hypergraphs and an explicit alignment between them; e.g., discontinuous phrase structures and non-projective dependency structures are bihy-pergraphs.", "labels": [], "entities": []}, {"text": "We present an EM-training algorithm which takes a corpus of bihyper-graphs and an aligned hypergraph bimor-phism as input and generates a sequence of weight assignments which converges to a local maximum or saddle point of the likelihood function of the corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "In natural language processing alignments play an important role.", "labels": [], "entities": [{"text": "natural language processing alignments", "start_pos": 3, "end_pos": 41, "type": "TASK", "confidence": 0.6978420689702034}]}, {"text": "For instance, in machine translation they show up as hidden information when training probabilities of dictionaries ( or when considering pairs of input/output sentences derived by asynchronous grammar (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7879927456378937}]}, {"text": "As another example, in language models for discontinuous phrase structures and non-projective dependency structures they can be used to capture the connection between the words in a natural language sentence and the corresponding nodes of the parse tree or dependency structure of that sentence.", "labels": [], "entities": []}, {"text": "In) the generation of discontinuous phrase structures has been formalized by the new concept of hybrid grammar.", "labels": [], "entities": []}, {"text": "Much as in the mentioned synchronous grammars, a hybrid grammar synchronizes the derivations of nonterminals of a string grammar, e.g., a linear context-free rewriting system (LCFRS), and of nonterminals of a tree grammar, e.g., regular tree grammar or simple definite-clause programs (sDCP).", "labels": [], "entities": []}, {"text": "Additionally it synchronizes terminal symbols, thereby establishing an explicit alignment between the positions of the string and the nodes of the tree.", "labels": [], "entities": []}, {"text": "We note that LCFRS/sDCP hybrid grammars can also generate non-projective dependency structures.", "labels": [], "entities": []}, {"text": "In this paper we focus on the task of training an LCFRS/sDCP hybrid grammar, that is, assigning probabilities to its rules given a corpus of discontinuous phrase structures or non-projective dependency structures.", "labels": [], "entities": []}, {"text": "Since the alignments are first class citizens, we develop our approach in the general framework of hypergraphs and hyperedge replacement (HR).", "labels": [], "entities": [{"text": "hyperedge replacement (HR)", "start_pos": 115, "end_pos": 141, "type": "TASK", "confidence": 0.8205356597900391}]}, {"text": "We define the concepts of bihypergraph (for short: bigraph) and aligned HR bimorphism.", "labels": [], "entities": []}, {"text": "The semantics of B is a set of bigraphs.", "labels": [], "entities": []}, {"text": "For instance, each discontinuous phrase structure or non-projective dependency structure can be represented as a bigraph (H 1 , \u03bb, H 2 ) where H 1 and H 2 correspond to the string component and the tree component, respectively.: (a) A sentence with non-projective dependencies is represented in (b) by a bigraph (H 1 , \u03bb, H 2 ).", "labels": [], "entities": []}, {"text": "Both hypergraphs H 1 and H 2 contain a distinct hyperedge (box) for each word of the sentence.", "labels": [], "entities": []}, {"text": "H 1 specifies the linear order on the words.", "labels": [], "entities": []}, {"text": "H 2 describes parent-child relationships between the words, where children form a list to whose start and end the parent has a tentacle.", "labels": [], "entities": []}, {"text": "The alignment \u03bb establishes a one-to-one correspondence between the (input vertices of the) hyperedges in H 1 and H 2 . dency structure.", "labels": [], "entities": []}, {"text": "We present each LCFRS/sDCP hybrid grammar as a particular aligned HR bimorphism; this establishes an initial algebra semantics ( for hybrid grammars.", "labels": [], "entities": []}, {"text": "The flexibility of aligned HR bimorphisms goes well beyond hybrid grammars as they generalize the synchronous HR grammars of (Jones et al., 2012), making it possible to synchronously generate two graphs connected by explicit alignment structures.", "labels": [], "entities": []}, {"text": "Thus, they can for instance model alignments involving directed acyclic graphs like Abstract Meaning Representations ( or Millstream systems (.", "labels": [], "entities": [{"text": "Abstract Meaning Representations", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.5995537141958872}]}, {"text": "Our training algorithm takes as input an aligned HR bimorphism B = (g, A 1 , \u039b, A 2 ) and a corpus c of bigraphs.", "labels": [], "entities": []}, {"text": "It is based on the dynamic programming variant) of the EM-algorithm and thus approximates a local maximum or saddle point of the likelihood function of c.", "labels": [], "entities": []}, {"text": "In order to calculate the significance of each rule of g for the generation of a single bigraph (H 1 , \u03bb, H 2 ) occurring inc, we proceed as usual, constructing the reduct B \u00a3 (H 1 , \u03bb, H 2 ) which generates the singleton (H 1 , \u03bb, H 2 ) via the same derivation trees as B and preserves the probabilities.", "labels": [], "entities": []}, {"text": "We show that the complexity of constructing the reduct is polynomial in the size of g and (H 1 , \u03bb, H 2 ) if B is an LCFRS/sDCP hybrid grammar.", "labels": [], "entities": []}, {"text": "However, as the algorithm itself is not limited to this situation, we expect it to be useful in other cases as well.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}