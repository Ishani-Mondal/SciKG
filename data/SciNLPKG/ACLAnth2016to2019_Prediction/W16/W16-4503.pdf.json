{"title": [{"text": "Image-Image Search for Comparable Corpora Construction", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel method of comparable corpora construction.", "labels": [], "entities": [{"text": "comparable corpora construction", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.7099712689717611}]}, {"text": "Unlike the traditional methods which heavily rely on linguistic features, our method only takes image similarity into consideration.", "labels": [], "entities": []}, {"text": "We use an image-image search engine to obtain similar images, together with the captions in source language and target language.", "labels": [], "entities": []}, {"text": "On the basis, we utilize captions of similar images to construct sentence-level bilingual corpora.", "labels": [], "entities": []}, {"text": "Experiments on 10,371 target captions show that our method achieves a precision of 0.85 in the top search results.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9986357092857361}]}], "introductionContent": [{"text": "We limit our discussion to the sentence-level comparable corpora.", "labels": [], "entities": []}, {"text": "Each sample in the dataset is a pair of bilingual sentences whose constituents are translations of each other, mostly or in whole.", "labels": [], "entities": []}, {"text": "Briefly, they contain semantically similar contents, although they are expressed in different languages.", "labels": [], "entities": []}, {"text": "In order to make it easier to read, we name such a sample as a bilingual sentence pair.", "labels": [], "entities": []}, {"text": "See an English-Chinese case as below (English translations are attached behind).", "labels": [], "entities": []}, {"text": "Different from the previous work, we employ similar images as the bridge to retrieve the bilingual sentence pairs.", "labels": [], "entities": []}, {"text": "We suppose that captions generally represent the semantic contents of images, so that if two images are visually similar, their captions are very likely to be semantically comparable.", "labels": [], "entities": []}, {"text": "shows two real images which are respectively crawled from the English and Chinese news websites.", "labels": [], "entities": []}, {"text": "Listed below the images are the captions which, as usual in the websites, are written in the native languages (Note that the captions have been exhibited in and).", "labels": [], "entities": []}, {"text": "Not just the similar images, it can be found that the captions are comparable as well.", "labels": [], "entities": []}, {"text": "Accordingly, we collect the captions of similar images from websites, and specify them as the candidates of bilingual sentence pairs.", "labels": [], "entities": []}, {"text": "We rank the candidates in the order of image similarity, and determine the most highly ranked ones as the reliable bilingual sentence pairs.", "labels": [], "entities": []}, {"text": "In practice, we build an image-image search engine.", "labels": [], "entities": []}, {"text": "The engine uses images as queries, and retrieves similar images based on consistency of image features of scale-invariant keypoints.", "labels": [], "entities": []}, {"text": "In this paper, we aim to independently evaluate the proposed method rather than a well-structured sophisticated system, and answer the question of whether it is possible to capture the bilingual caption pairs (sentence pairs) by image-image search, if they really exist.", "labels": [], "entities": []}, {"text": "In reality, the system should additionally consist of the modules like crawling, webpage structure analysis and image indexing.", "labels": [], "entities": [{"text": "webpage structure analysis", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.6105219523111979}, {"text": "image indexing", "start_pos": 112, "end_pos": 126, "type": "TASK", "confidence": 0.7322332710027695}]}, {"text": "For these modules, we only provide a brief introduction.", "labels": [], "entities": []}, {"text": "Nevertheless, it is noteworthy that these techniques are undoubtedly important for mining large-scale comparable data from websites.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 overviews the related work.", "labels": [], "entities": []}, {"text": "We present the methodology and detail the image-image search engine in section 3.", "labels": [], "entities": []}, {"text": "Section 4 shows the experimental settings and results.", "labels": [], "entities": []}, {"text": "We conclude the paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct a pilot study for CMIR towards comparable corpora construction.", "labels": [], "entities": []}, {"text": "The goal of this study is to verify whether image-image search is useful for the discovery of textual equivalents in TDB.", "labels": [], "entities": []}, {"text": "There is an important problem need to be solved firstly: Reliability.", "labels": [], "entities": []}, {"text": "As mentioned in section 3.1, TDB is a data bank which contains a great number of images, along with the captions in the target language (named target captions for short).", "labels": [], "entities": []}, {"text": "However, if we randomly select the captions in the source language (source captions) as the test samples for mining the bilingual captions, it is easy for us to encounter the problem that there is not areal equivalent in TDB.", "labels": [], "entities": []}, {"text": "In the case, the experimental results are definitely unreliable.", "labels": [], "entities": []}, {"text": "For example, the precision rate in the 5 highly-ranked target captions will always be 0.", "labels": [], "entities": [{"text": "precision rate", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.9885536134243011}]}, {"text": "On the contrary however, if we added some ground-truth equivalents of the test samples to TDB, the experimental settings will be far from the real condition.", "labels": [], "entities": [{"text": "TDB", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.925102174282074}]}, {"text": "To solve the problem, we propose an automatic method of measuring comparability between source captions and target captions.", "labels": [], "entities": []}, {"text": "Based on the measurement results, we collect pseudo ground-truth equivalents, and use them to enrich the test data.", "labels": [], "entities": []}, {"text": "By this way, we can build an experimental environment similar to the real condition.", "labels": [], "entities": []}, {"text": "We detail the method in section 4.4.", "labels": [], "entities": []}, {"text": "Besides, as usual, we show the corpus, traditional evaluation metrics and main experimental results one-by-one, which can be found in sections 4.1, 4.2 and 4.3.", "labels": [], "entities": []}, {"text": "For the part of main result, we report the precision in top 5 highly-ranked target captions, as well as the ranking results, at four levels of comparability (parallel-level (abbr., Par.), comparable (Com.), pseudo-comparable (Pse.), and incomparable (Inc.)).", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9993425011634827}]}, {"text": "In addition, we compare our method with the state-of-the-art CLIR method and the other image-image search engine.", "labels": [], "entities": []}, {"text": "We conduct our CMIR process in TDB, with the aim to verify whether CMIR is able to seek out the comparable target captions in a large-scale data set.", "labels": [], "entities": [{"text": "TDB", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.9590216279029846}]}, {"text": "This is the kernel of the proposed corpora construction method.", "labels": [], "entities": [{"text": "corpora construction", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.6939603984355927}]}, {"text": "If it is promising, we can accomplish the corpora construction by continuous CMIR using a massive number of source captions as the queries.", "labels": [], "entities": []}, {"text": "Therefore we focus on evaluating the CMIR in this paper, using the samples in the mini-sized SDB as the queries.", "labels": [], "entities": []}, {"text": "The basic evaluation metric is the Precision rate (P).", "labels": [], "entities": [{"text": "Precision rate (P)", "start_pos": 35, "end_pos": 53, "type": "METRIC", "confidence": 0.9871469020843506}]}, {"text": "We didn't consider the Recall (R) rate.", "labels": [], "entities": [{"text": "Recall (R) rate", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.9636128306388855}]}, {"text": "It is because that the genuine requirements of comparable corpora construction are the noise-free high-quality equivalents, but not all.", "labels": [], "entities": []}, {"text": "Not just the acquisition of qualified equivalents, P@N also reflects the ability of a CMIR (or CLIR) system to filter incorrect equivalents out of the top n search results.", "labels": [], "entities": []}, {"text": "We propose an automatic method to measure the comparability between a source caption and the target (a candidate equivalent).", "labels": [], "entities": []}, {"text": "The method can be used to evaluate the results of CMIR without knowing the ground truth.", "labels": [], "entities": []}, {"text": "It measures the comparability by using the following features: \uf09f Content similarity (fc) is calculated by the Cosine measure between TFIDF based VSM models of source caption and target caption (Only content words are considered in the calculation).", "labels": [], "entities": [{"text": "\uf09f Content similarity (fc)", "start_pos": 63, "end_pos": 88, "type": "METRIC", "confidence": 0.7291940997044245}, {"text": "Cosine", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9783170819282532}]}, {"text": "\uf09f Co-occurrence of entities (fe) is calculated by the joint co-occurrence rates of entity mentions in source caption and target caption.", "labels": [], "entities": [{"text": "Co-occurrence of entities (fe)", "start_pos": 2, "end_pos": 32, "type": "METRIC", "confidence": 0.5328330447276434}]}, {"text": "\uf09f Length ratio (fl) is the difference of the length of the captions.", "labels": [], "entities": [{"text": "\uf09f", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9873762130737305}, {"text": "Length ratio (fl)", "start_pos": 2, "end_pos": 19, "type": "METRIC", "confidence": 0.9183323740959167}]}, {"text": "If they have the same length, fl is equal to 1, otherwise a smaller value (divide the length of the shorter by that of the longer) On the basis, we measure the comparability by combing the features by the linear weighted sum method: C=\u03b1\uf09efc+\u03b2\uf09efe+\u03b3\uf09efl, where the parameters \u03b1, \u03b2 and \u03b3 are empirically set as 0.8, 0.15 and 0.05.", "labels": [], "entities": []}, {"text": "Further, we divide the ground-truth samples (i.e., bilingual caption pairs) into three classes in terms of the prior level of comparability, i.e., Par, Com and Pse.", "labels": [], "entities": [{"text": "Par", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.9524944424629211}]}, {"text": "For each class, we calculate the average C. shows the calculation results.", "labels": [], "entities": []}, {"text": "It can be found that the average C-measure of the classes of the ground truth closely fit the manual ratings of the classes (the rating score 3 corresponds to the Parlevel, 2 to Com and 1 to Pse).", "labels": [], "entities": [{"text": "Parlevel", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9948800802230835}, {"text": "Pse", "start_pos": 191, "end_pos": 194, "type": "METRIC", "confidence": 0.9676452875137329}]}, {"text": "The Pearson factor between the ratings and the C scores is high up to 0.99.", "labels": [], "entities": [{"text": "Pearson factor", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9825364649295807}, {"text": "C scores", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9386168718338013}]}, {"text": "It illustrates that the trend of gradient descent of C is similar to that of manual ratings.", "labels": [], "entities": []}, {"text": "Pearson  Accordingly, we use C-measure to ensure the reliability of the evaluation process when there is lack of known ground-truth equivalents.", "labels": [], "entities": [{"text": "reliability", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9707839488983154}]}, {"text": "We set \u03d1(C, \u025b) as a linear function of the deviation \u025b from the average C-measure of certain level of equivalents: \u03d1(C, \u025b)=C-a\uf09e\u025b.", "labels": [], "entities": []}, {"text": "We estimate the optimal factor a in the training data by maximizing the precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9995381832122803}]}, {"text": "We use \u03d1(C, \u025b) as the criteria to determine whether a target caption is a qualified equivalent fora certain level of comparability.", "labels": [], "entities": []}, {"text": "For example, if C(x, y)> \u03d1(C, \u025b), x is comparable toy (at Par-level, Com or Pse).", "labels": [], "entities": [{"text": "Pse", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9339665174484253}]}, {"text": "The qualified equivalents will be used as the groundtruth data to evaluate the performance of the CMIR systems.", "labels": [], "entities": []}, {"text": "An instantiated \u03d1(C, \u025b) enables an experiment on large-scale test data (source captions as queries) and rich ground-truth data.", "labels": [], "entities": []}, {"text": "The test result, therefore, will be more reliable than the current case.", "labels": [], "entities": []}, {"text": "Active learning can be applied for enhancing the evaluation process.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Main test results (Precision rates in top-n equivalents) for both CMIR and CLIR", "labels": [], "entities": [{"text": "Main", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9685347080230713}, {"text": "Precision rates", "start_pos": 29, "end_pos": 44, "type": "METRIC", "confidence": 0.9822793900966644}, {"text": "CMIR", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.9164468050003052}, {"text": "CLIR", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.7669124603271484}]}, {"text": " Table 3: Comparability C-measure and Pearson parameter", "labels": [], "entities": [{"text": "Pearson", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9885027408599854}]}]}