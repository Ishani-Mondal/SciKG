{"title": [{"text": "Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure", "labels": [], "entities": [{"text": "Improving Reliability of Word Similarity Evaluation", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.886195162932078}]}], "abstractContent": [{"text": "We suggest anew method for creating and using gold-standard datasets for word similarity evaluation.", "labels": [], "entities": [{"text": "word similarity evaluation", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.8216560085614523}]}, {"text": "Our goal is to improve the reliability of the evaluation, and we do this by redesigning the annotation task to achieve higher inter-rater agreement, and by defining a performance measure which takes the reliability of each annotation decision in the dataset into account.", "labels": [], "entities": [{"text": "reliability", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9578385353088379}]}], "introductionContent": [{"text": "Computing similarity between words is a fundamental challenge in natural language processing.", "labels": [], "entities": [{"text": "Computing similarity between words", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8452862501144409}, {"text": "natural language processing", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.6455770333607992}]}, {"text": "Given a pair of words, a similarity model sim(w 1 , w 2 ) should assign a score that reflects the level of similarity between them, e.g.: sim(singer, musician) = 0.83.", "labels": [], "entities": []}, {"text": "While many methods for computing sim exist (e.g., taking the cosine between vector embeddings derived by), there are currently no reliable measures of quality for such models.", "labels": [], "entities": []}, {"text": "In the past few years, word similarity models show a consistent improvement in performance when evaluated using the conventional evaluation methods and datasets.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7834717333316803}]}, {"text": "But are these evaluation measures really reliable indicators of the model quality?", "labels": [], "entities": []}, {"text": "Lately, claimed that the answer is no.", "labels": [], "entities": []}, {"text": "They identified several problems with the existing datasets, and created anew dataset -SimLex-999 -which does not suffer from them.", "labels": [], "entities": []}, {"text": "However, we argue that there are inherent problems with conventional datasets and the method of using them that were not addressed in SimLex-999.", "labels": [], "entities": []}, {"text": "We list these problems, and suggest anew and more reliable way of evaluating similarity models.", "labels": [], "entities": []}, {"text": "We then report initial experiments on a dataset of Hebrew nouns similarity that we created according to our proposed method.", "labels": [], "entities": []}], "datasetContent": [{"text": "Over the years, several datasets have been used for evaluating word similarity models.", "labels": [], "entities": [{"text": "word similarity models", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.8220507105191549}]}, {"text": "Popular ones include RG (,), WS-Sim () and MEN ().", "labels": [], "entities": [{"text": "RG", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.4748062491416931}, {"text": "WS-Sim", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.800545871257782}, {"text": "MEN", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.8151390552520752}]}, {"text": "Each of these datasets is a collection of word pairs together with their similarity scores as assigned by human annotators.", "labels": [], "entities": []}, {"text": "A model is evaluated by assigning a similarity score to each pair, sorting the pairs according to their similarity, and calculating the correlation (Spearman's \u03c1) with the human ranking.", "labels": [], "entities": [{"text": "Spearman's \u03c1)", "start_pos": 149, "end_pos": 162, "type": "METRIC", "confidence": 0.7628526240587234}]}, {"text": "had made a comprehensive review of these datasets, and pointed out some common shortcomings they have.", "labels": [], "entities": []}, {"text": "The main shortcoming discussed by Hill et al is the handling of associated but dissimilar words, e.g. (singer, microphone): in datasets which contain such pairs (WordSim and MEN) they are usually ranked high, sometimes even above pairs of similar words.", "labels": [], "entities": [{"text": "WordSim", "start_pos": 162, "end_pos": 169, "type": "DATASET", "confidence": 0.9381450414657593}]}, {"text": "This causes an undesirable penalization of models that apply the correct behavior (i.e., always prefer similar pairs over associated dissimilar ones).", "labels": [], "entities": []}, {"text": "Other datasets (WS-Sim and RG) do not contain pairs of associated words pairs at all.", "labels": [], "entities": [{"text": "WS-Sim", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.8431206941604614}]}, {"text": "Their absence makes these datasets unable to evaluate the models' ability to distinct between associated and similar words.", "labels": [], "entities": [{"text": "distinct between associated and similar words", "start_pos": 77, "end_pos": 122, "type": "TASK", "confidence": 0.7655329803625742}]}, {"text": "Another shortcoming mentioned by is low interrater agreement over the human assigned similarity scores, which might have been caused by unclear instructions for the annotation task.", "labels": [], "entities": [{"text": "interrater agreement", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.8015022873878479}, {"text": "similarity scores", "start_pos": 85, "end_pos": 102, "type": "METRIC", "confidence": 0.8864801824092865}]}, {"text": "As a result, state-of-the-art models reach the agreement ceiling for most of the datasets, while a simple manual evaluation will suggest that these models are still inferior to humans.", "labels": [], "entities": []}, {"text": "In order to solve these shortcomings, Hill et al (2015) developed anew dataset -Simlex-999 -in which the instructions presented to the annotators emphasized the difference between the terms associated and similar, and managed to solve the discussed problems.", "labels": [], "entities": []}, {"text": "While SimLex-999 was definitely a step in the right direction, we argue that there are more fundamental problems which all conventional methods, including SimLex-999, suffer from.", "labels": [], "entities": [{"text": "SimLex-999", "start_pos": 6, "end_pos": 16, "type": "DATASET", "confidence": 0.8172820210456848}]}, {"text": "In what follows, we describe each one of these problems.", "labels": [], "entities": []}, {"text": "Before diving in, we define some terms we are about to use.", "labels": [], "entities": []}, {"text": "used the terms similar and associated but dissimilar, which they didn't formally connected to fine-grained semantic relations.", "labels": [], "entities": []}, {"text": "However, by inspecting the average score per relation, they found a clear preference for hyponym-hypernym pairs (e.g. the scores of the pairs (cat, pet) and (winter, season) are much higher than those of the cohyponyms pair (cat, dog) and the antonyms pair (winter, summer)).", "labels": [], "entities": []}, {"text": "Referring hyponym-hypernym pairs as similar may imply that a good similarity model should prefer hyponym-hypernym pairs over pairs of other relations, which is not always true since the desirable behavior is task-dependent.", "labels": [], "entities": []}, {"text": "Therefore, we will use a different terminology: we use the term preferred-relation to denote the relation which the model should prefer, and unpreferred-relation to denote any other relation.", "labels": [], "entities": []}, {"text": "The first problem is the use of rating scales.", "labels": [], "entities": []}, {"text": "Since the level of similarity is a relative measure, we would expect the annotation task to ask the annotator fora ranking.", "labels": [], "entities": []}, {"text": "But inmost of the existing datasets, the annotators were asked to assign a numeric score to each pair (e.g. 0-7 in SimLex-999), and a ranking was derived based on these scores.", "labels": [], "entities": []}, {"text": "This choice is probably due to the fact that a ranking of hundreds of pairs is an exhausting task for humans.", "labels": [], "entities": []}, {"text": "However, using rating scales makes the annotations vulnerable to a variety of biases (Friedman and Amoo, 1999).", "labels": [], "entities": []}, {"text": "addressed this problem by asking the annotators to rank each pair in comparison to 50 randomly selected pairs.", "labels": [], "entities": []}, {"text": "This is a reasonable compromise, but it still results in a daunting annotation task, and makes the quality of the dataset depend on a random selection of comparisons.", "labels": [], "entities": []}, {"text": "The second problem is rating different relations on the same scale.", "labels": [], "entities": []}, {"text": "In Simlex-999, the annotators were instructed to assign low scores to unpreferred-relation pairs, but the decision of how low was still up to the annotator.", "labels": [], "entities": []}, {"text": "While some of these pairs were assigned very low scores (e.g. sim(smart, dumb) = 0.55), others got significantly higher ones (e.g. sim(winter, summer) = 2.38).", "labels": [], "entities": []}, {"text": "A difference of 1.8 similarity scores should not be underestimated -in other cases it testifies to a true superiority of one pair over another, e.g.: sim(cab, taxi) = 9.2, sim(cab, car) = 7.42.", "labels": [], "entities": [{"text": "similarity", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9505640268325806}]}, {"text": "The situation where an arbitrary decision of the annotators affects the model score, impairs the reliability of the evaluation: a model shouldn't be punished for preferring (smart, dumb) over (winter, summer) or vice versa, since this comparison is just ill-defined.", "labels": [], "entities": []}, {"text": "The third problem is rating different targetwords on the same scale.", "labels": [], "entities": []}, {"text": "Even within preferredrelation pairs, there are ill-defined comparisons, e.g.: (cat, pet) vs. (winter, season).", "labels": [], "entities": []}, {"text": "It's quite unnatural to compare between pairs that have different target-words, in contrast to pairs which share the target word, like (cat, pet) vs. cat, animal).", "labels": [], "entities": []}, {"text": "Penalizing a model for preferring (cat, pet) over (winter, season) or vice versa impairs the evaluation reliability.", "labels": [], "entities": []}, {"text": "The fourth problem is that the evaluation measure does not consider annotation decisions reliability.", "labels": [], "entities": []}, {"text": "The conventional method measures the model score by calculating Spearman correlation between the model ranking and the annotators average ranking.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 64, "end_pos": 84, "type": "METRIC", "confidence": 0.8022665977478027}]}, {"text": "This method ignores an important information source: the reliability of each annotation decision, which can be determined by the agreement of the annotators on this decision.", "labels": [], "entities": [{"text": "reliability", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.9867396354675293}]}, {"text": "For example, consider a dataset containing the pairs (singer, person), (singer, performer) and (singer, musician).", "labels": [], "entities": []}, {"text": "Now let's assume that in the average annotator ranking, (singer, performer) is ranked above (singer, person) after 90% of the annotators assigned it with a higher score, and (singer, musician) is ranked above (singer, performer) after 51% percent of the annotators assigned it with a higher score.", "labels": [], "entities": []}, {"text": "Considering this, we would like the evaluation measure to severely punish a model which prefers (singer, person) over (singer, performer), but be almost indifferent to the model's decision over (singer, performer) vs. (singer, musician) because it seems that even humans cannot reliably tell which one is more similar.", "labels": [], "entities": []}, {"text": "In the conventional datasets, no information on reliability of ratings is supplied except for the overall agreement, and each average rank has the same weight in the evaluation measure.", "labels": [], "entities": [{"text": "reliability", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9652683734893799}]}, {"text": "The problem of relia-bility is addressed by which included many rare words in their dataset, and thus allowed an annotator to indicate \"Don't know\" fora pair if they does not know one of the words.", "labels": [], "entities": []}, {"text": "The problem with applying this approach as a more general reliability indicator is that the annotator confidence level is subjective and not absolute.", "labels": [], "entities": []}, {"text": "In this section we describe the structure of a dataset which applies the above improvements.", "labels": [], "entities": []}, {"text": "First, we need to define the preferred-relation (to apply improvement based on target words.", "labels": [], "entities": []}, {"text": "For each target word we create a group of complement words, which we refer to as the target-group.", "labels": [], "entities": []}, {"text": "Each complement word belongs to one of three categories: positives (related to the target, and the type of the relation is the preferred one), distractors (related to the target, but the type of the relation is not the preferred one), and randoms (not related to the target at all).", "labels": [], "entities": []}, {"text": "For example, for the target word singer, the target group may include musician, performer, person and artist as positives, dancer and song as distractors, and laptop as random.", "labels": [], "entities": []}, {"text": "For each target word, the human annotators will be asked to rank the positive complements by their similarity to the target word (improvements &).", "labels": [], "entities": []}, {"text": "For example, a possible ranking may be: musician > performer > artist > person.", "labels": [], "entities": []}, {"text": "The annotators responses allow us to create the actual dataset, which consists of a collection of binary comparisons.", "labels": [], "entities": []}, {"text": "A binary comparison is a value R > (w 1 , w 2 ; wt ) indicating how likely it is to rank the pair (w t , w 1 ) higher than (w t , w 2 ), where wt is a target word and w 1 , w 2 are two complement words.", "labels": [], "entities": []}, {"text": "By definition, R > (w 1 , w 2 ; wt ) = 1 -R > (w 2 , w 1 ; wt ).", "labels": [], "entities": []}, {"text": "For each target-group, the dataset will contain a binary comparison for any possible combination of two positive complements w p1 and w p2 , as well as for positive complements w p and negative ones (either distractor or random) w n . When comparing positive complements, R > (w 1 , w 2 ; wt ) is the portion of annotators who ranked (w t , w 1 ) over (w t , w 2 ).", "labels": [], "entities": []}, {"text": "When comparing to negative complements, the value of R > (w p , w n ; wt ) is 1.", "labels": [], "entities": []}, {"text": "This reflects the intuition that a good model should always rank preferred-relation pairs above other pairs.", "labels": [], "entities": []}, {"text": "Notice that R > (w 1 , w 2 ; wt ) is the reliability indicator for each of the dataset key answers, which will be used to apply improvement (4).", "labels": [], "entities": [{"text": "R", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.9700648784637451}, {"text": "reliability indicator", "start_pos": 41, "end_pos": 62, "type": "METRIC", "confidence": 0.9732470512390137}]}, {"text": "For some example comparisons, see.", "labels": [], "entities": []}, {"text": "We created two datasets following the proposal discussed above: one preferring the hyponymhypernym relation, and the other the cohyponym relation.", "labels": [], "entities": []}, {"text": "The datasets contain Hebrew nouns, but such datasets can be created for different languages and parts of speech -providing that the language has basic lexical resources.", "labels": [], "entities": []}, {"text": "For our dataset, we used a dictionary, an encyclopedia and a thesaurus to create the hyponym-hypernym pairs, and databases of word association norms () and categories norms to create the distractors pairs and the cohyponyms pairs, respectively.", "labels": [], "entities": []}, {"text": "The hyponym-hypernym dataset is based on 75 targetgroups, each contains 3-6 positive pairs, 2 distractor pairs and one random pair, which sums up to 476 pairs.", "labels": [], "entities": []}, {"text": "The cohyponym dataset is based on 30 target-groups, each contains 4 positive pairs, 1-2 distractor pairs and one random pair, which sums up to 207 pairs.", "labels": [], "entities": []}, {"text": "We used the target groups to create 4 questionnaires: 3 for the hyponym-hypernym relation (each contains 25 target-groups), and one for the cohyponyms relation.", "labels": [], "entities": []}, {"text": "We asked human annotators to order the positive pairs of each targetgroup by the similarity between their words.", "labels": [], "entities": []}, {"text": "In order to prevent the annotators from confusing between the different aspects of similarity, each annotator was requested to answer only one of the questionnaires, and the instructions for each questionnaire included an example question which demonstrates what the term \"similarity\" means in that questionnaire (as shown in.", "labels": [], "entities": []}, {"text": "Each target-group was ranked by 18-20 annotators.", "labels": [], "entities": []}, {"text": "We measured the average pairwise inter-rater agreement, and as done in () -we excluded any annotator which its agreement with the other was more than one standard deviation below that average (17.8 percent of the annotators were excluded).", "labels": [], "entities": []}, {"text": "The agreement was quite high (0.646 and 0.659 for hyponym-hypernym and cohyponyms target-groups, respectively), especially considering that in contrast to other datasetsour annotation task did not include pairs that are \"trivial\" to rank (e.g. random pairs).", "labels": [], "entities": [{"text": "agreement", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.994326651096344}]}, {"text": "Finally, we used the remaining annotators responses to create the binary comparisons collection.", "labels": [], "entities": []}, {"text": "The hyponym-hypernym dataset includes 1063 comparisons, while the cohyponym dataset includes 538 comparisons.", "labels": [], "entities": []}, {"text": "To measure the gap between a human and a model performance on the dataset, we trained a word2vec () model 1 on the Hebrew Wikipedia.", "labels": [], "entities": [{"text": "Hebrew Wikipedia", "start_pos": 115, "end_pos": 131, "type": "DATASET", "confidence": 0.784972220659256}]}, {"text": "We used two methods of measuring: the first is the conventional way (Spearman correlation), and the second is the scoring method we described in the previous section, which we used to measure general and percomparison-type scores.", "labels": [], "entities": [{"text": "Spearman correlation)", "start_pos": 69, "end_pos": 90, "type": "METRIC", "confidence": 0.760654220978419}]}, {"text": "The results are presented in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Binary Comparisons for the target word singer. P:", "labels": [], "entities": [{"text": "P", "start_pos": 57, "end_pos": 58, "type": "METRIC", "confidence": 0.9515381455421448}]}, {"text": " Table 2: The hyponym-hypernym dataset agreement", "labels": [], "entities": []}]}