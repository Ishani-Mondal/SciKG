{"title": [], "abstractContent": [{"text": "In this paper we present our winning system in the WMT16 Shared Task on Cross-Lingual Pronoun Prediction, where the objective is to predict a missing target language pronoun based on the target and source sentences.", "labels": [], "entities": [{"text": "WMT16 Shared Task", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.5465655028820038}, {"text": "Cross-Lingual Pronoun Prediction", "start_pos": 72, "end_pos": 104, "type": "TASK", "confidence": 0.579720695813497}]}, {"text": "Our system is a deep recurrent neural network, which reads both the source language and target language context with a softmax layer making the final prediction.", "labels": [], "entities": []}, {"text": "Our system achieves the best macro recall on all four language pairs.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9814504384994507}]}, {"text": "The margin to the next best system ranges between less than 1pp and almost 12pp depending on the language pair.", "labels": [], "entities": [{"text": "margin", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9884636998176575}]}], "introductionContent": [{"text": "Automatic translation of pronouns across languages can be seen as an subtask of the full machine translation.", "labels": [], "entities": []}, {"text": "In the pronoun translation task the special challenge is posed by anaphora resolution as well as differing gender marking in different languages.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.745240330696106}, {"text": "anaphora resolution", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7308401465415955}]}, {"text": "The WMT16 Shared Task on Cross-Language Pronoun Prediction strives to seek for methods to address this particular problem (.", "labels": [], "entities": [{"text": "WMT16 Shared Task on Cross-Language Pronoun Prediction", "start_pos": 4, "end_pos": 58, "type": "TASK", "confidence": 0.5452947403703418}]}, {"text": "This shared task includes two language pairs, English-French and English-German, and both translation directions, so in total four different source-target pairs must be considered.", "labels": [], "entities": []}, {"text": "In the target language side selected set of pronouns are substituted with replace, and the task is then to predict the missing pronoun.", "labels": [], "entities": []}, {"text": "Furthermore, the target side language is not given as running text, but instead in lemma plus part-of-speech tag format.", "labels": [], "entities": []}, {"text": "This is to mimic the representation which many standard machine translation systems produce and to complicate the matter of standard Both authors contributed equally to this work.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7605755925178528}, {"text": "standard", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9576261639595032}]}, {"text": "Source: That 's how they like to live . Target: ce|PRON\u00eatre|VERce|PRON\u02c6ce|PRON\u00eatre|VER comme|ADV cela|PRON que|PRON REPLACE 3 aimer|VER vivre|VER .|.", "labels": [], "entities": [{"text": "VERce", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.8793784379959106}, {"text": "REPLACE 3 aimer", "start_pos": 116, "end_pos": 131, "type": "METRIC", "confidence": 0.8924689690272013}]}, {"text": "An example of an EnglishFrench sentence pair is given in.", "labels": [], "entities": []}, {"text": "Furthermore, the training data as provided by the organizers of the the task includes automatically produced word-level alignments between the source and the target language.", "labels": [], "entities": []}, {"text": "In this paper we describe the pronoun prediction system of the Turku NLP Group.", "labels": [], "entities": [{"text": "pronoun prediction", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7181691080331802}, {"text": "Turku NLP Group", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9361984332402548}]}, {"text": "Our system is a deep recurrent neural network with word-level embeddings, two layers of Gated Recurrent Units (GRUs) and a softmax layer on top of it to make the final prediction.", "labels": [], "entities": []}, {"text": "The network uses both source and target contexts to make the prediction, and no additional data or tools are used beside the data provided by the organizers.", "labels": [], "entities": []}, {"text": "The system has the best macro recall score in the official evaluation on all four language pairs.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9832707047462463}]}], "datasetContent": [{"text": "We ran a small study of different system settings to evaluate our design choices.", "labels": [], "entities": []}, {"text": "Results are shown in, where the performance is evaluated on the official test set.", "labels": [], "entities": [{"text": "official test set", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.7659045656522115}]}, {"text": "In the test set evaluation our primary system gives the highest score on two language pairs, but loses to another system setting in other two language pairs.", "labels": [], "entities": []}, {"text": "Overall, the primary system still performs best on average when measured on macro recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9510563611984253}]}, {"text": "As stated in Section 3.1, both our submissions are based on aversion of the network with stacked GRU units.", "labels": [], "entities": []}, {"text": "In preliminary studies, the stacked approach increased the prediction performance and this holds on the test set for all language pairs except English-French.", "labels": [], "entities": []}, {"text": "While on average the stacked system performs 2.4pp better on macro recall, on the English-French pair the non-stacked model performs 4.3pp better.", "labels": [], "entities": [{"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.969645619392395}]}, {"text": "Another important feature is the size of the context window.", "labels": [], "entities": []}, {"text": "In previous work a rather small context was noted to work relatively well (Tiedemann,: Scores for all primary systems and our contrastive system on the official test set evaluation sorted by the average score across language pairs.", "labels": [], "entities": []}, {"text": "For each language pair the best score is bolded and the second best is marked with a star (our contrastive submission is not taken into account).", "labels": [], "entities": []}, {"text": "However, in our submission systems the maximum size of the context was set to 50, and in our development experiments radically shorter context sizes hurt the prediction performance of our system.", "labels": [], "entities": []}, {"text": "However, in test set evaluation both language pairs with English as the source language seem to benefit from shorter context, especially English-French pair which scores 3.6pp higher in macro recall than our primary system, but also loses to the version with longer context without stacking by 0.73pp in macro recall.", "labels": [], "entities": []}, {"text": "Other language pairs benefit from larger context (see short context in).", "labels": [], "entities": []}, {"text": "In addition, we evaluate allowing the context window to extend beyond the current sentence boundary.", "labels": [], "entities": []}, {"text": "The maximum context size is always 50, although when restricted to within one sentence, it naturally rarely reaches it.", "labels": [], "entities": []}, {"text": "In our primary and contrastive submissions, the context was limited to include only the current sentence, and the results using the context beyond the sentence are in the row cross-sentence in.", "labels": [], "entities": []}, {"text": "We can observe that no language pair seems to benefit from a larger context on the test set.", "labels": [], "entities": []}, {"text": "As mentioned earlier, the Europarl dataset can be considered as out-of-the-domain data.", "labels": [], "entities": [{"text": "Europarl dataset", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.994634598493576}]}, {"text": "The indomain row in refers to an experiment where Europarl was discarded from the training data and thus the system was trained only on indomain data.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.8928418159484863}]}, {"text": "Naturally, the amount of training data is then much smaller, the data size drops from 2.4M sentences to approx. 400K sentences.", "labels": [], "entities": []}, {"text": "This hurts the performance on all language pairs, indicating that our method benefits from a lot of training data and might be indicative of its ability to generalize to other domains.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The vocabulary sizes of the models", "labels": [], "entities": []}, {"text": " Table 3: Scores for all primary systems and our contrastive system on the official test set evaluation  sorted by the average score across language pairs. For each language pair the best score is bolded and  the second best is marked with a star (our contrastive submission is not taken into account).", "labels": [], "entities": [{"text": "official test set", "start_pos": 75, "end_pos": 92, "type": "DATASET", "confidence": 0.7823732197284698}]}]}