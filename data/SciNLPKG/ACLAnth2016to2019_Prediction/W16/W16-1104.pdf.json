{"title": [{"text": "Token-Level Metaphor Detection using Neural Networks", "labels": [], "entities": [{"text": "Token-Level Metaphor Detection", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.75702432791392}]}], "abstractContent": [{"text": "Automatic metaphor detection usually relies on various features, incorporating e.g. selectional preference violations or con-creteness ratings to detect metaphors in text.", "labels": [], "entities": [{"text": "Automatic metaphor detection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6701371471087137}]}, {"text": "These features rely on background corpora, hand-coded rules or additional, manually created resources, all specific to the language the system is being used on.", "labels": [], "entities": []}, {"text": "We present a novel approach to metaphor detection using a neural network in combination with word embeddings, a method that has already proven to yield promising results for other natural language processing tasks.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9503217339515686}]}, {"text": "We show that foregoing manual feature engineering by solely relying on word embeddings trained on large corpora produces comparable results to other systems, while removing the need for additional resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "According to, metaphors are cognitive mappings of concepts from a source to a target domain.", "labels": [], "entities": []}, {"text": "While in some works identifying those mappings (conceptual metaphors) themselves is the subject of analysis, we concern ourselves with detecting their manifestations in text (linguistic metaphors).", "labels": [], "entities": []}, {"text": "Various features have been designed to model either representation of metaphor, prominently e.g. violations of (generalized) selectional preferences in grammatical relations, concreteness ratings to model the difference between source and target concepts (), supersenses and hypernym relations (, or topic models ().", "labels": [], "entities": []}, {"text": "Some of these features can be obtained in an unsupervised way, but many require additional resources such as concreteness databases or word taxonomies.", "labels": [], "entities": []}, {"text": "While this is a good approach for resource-rich languages, this poses problems for languages where such resources are not readily available.", "labels": [], "entities": []}, {"text": "Approaches to alleviate this issue often make use of bilingual dictionaries or machine translation (, in itself introducing the need fora new resource resp.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7322646975517273}]}, {"text": "introducing a possible new source for misclassification.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel approach for metaphor detection using neural networks on the token-level in running text, relying solely on word embeddings used in context.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.9473913311958313}]}, {"text": "In recent years, neural networks have been used to solve natural language processing tasks with great effect, but so far have not been applied to metaphor detection.", "labels": [], "entities": [{"text": "solve natural language processing tasks", "start_pos": 51, "end_pos": 90, "type": "TASK", "confidence": 0.6462885499000549}, {"text": "metaphor detection", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.9616020619869232}]}, {"text": "While our approach still has to be tested on data in other languages, it already shows promising results on English data, all the more considering it is not using an elaborate feature set, deriving the representation only from distributed and local context.", "labels": [], "entities": []}, {"text": "We start in Section 2 by discussing previous work on metaphor detection which compares to our work in at least one aspect: granularity of classification, language/resource independence, or the usage of word embeddings.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.9234136939048767}]}, {"text": "Section 3 details the architecture of our neural network.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the used resources and training data and present the results of our method.", "labels": [], "entities": []}, {"text": "Concluding this paper, in Section 5 we also give an outlook for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we use the pre-trained 300-dimensional word embeddings created with word2vec 1 using the Google News dataset (Mikolov: Results on the test sets for the tuned neural network.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 110, "end_pos": 129, "type": "DATASET", "confidence": 0.9147245089213053}]}, {"text": "Showing precision (P), recall (R) and F1-measure (F1) regarding metaphorically used tokens, for different feature combinations and VUAMC subcorpora, as well as for the whole corpus (complete).", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 8, "end_pos": 21, "type": "METRIC", "confidence": 0.9507457762956619}, {"text": "recall (R)", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9456964731216431}, {"text": "F1-measure (F1)", "start_pos": 38, "end_pos": 53, "type": "METRIC", "confidence": 0.9340008050203323}]}, {"text": "B denotes a pseduo-baseline classifying all tokens as metaphorical.", "labels": [], "entities": []}, {"text": "et al., 2013).", "labels": [], "entities": []}, {"text": "Training and testing data is taken from the VU Amsterdam Metaphor Corpus (VUAMC), a subset of the BNC Baby in which each token is annotated as being used literally or metaphorically.", "labels": [], "entities": [{"text": "VU Amsterdam Metaphor Corpus (VUAMC), a subset of the BNC Baby", "start_pos": 44, "end_pos": 106, "type": "DATASET", "confidence": 0.8201197300638471}]}, {"text": "This is done using various fine-grained tags; however, we only use the most clear cut tag mrw (metaphor-related word), labeling everything else as literal.", "labels": [], "entities": []}, {"text": "Furthermore, because generally only the detection of metaphoricity of content tokens is of interest, we only incorporate labels for tokens having one of the following POS tags (as supplied with the VUAMC): noun, verb, adjective, adverb.", "labels": [], "entities": [{"text": "VUAMC", "start_pos": 198, "end_pos": 203, "type": "DATASET", "confidence": 0.9596496820449829}]}, {"text": "Also auxiliary verbs, having lemmas have, be, or do, were filtered out.", "labels": [], "entities": []}, {"text": "An overview over the remaining tokens in the subcorpora can be seen in.", "labels": [], "entities": []}, {"text": "The system is trained on each contained genre (news, conversation, fiction, academic) separately; for each subcorpus we use a random subset of 76% of the data as a training set, 12% as development set and 12% as test set.", "labels": [], "entities": []}, {"text": "We also extend our system to incorporate 10-dimensional POS embeddings, which were initialized randomly and updated over the course of the network's training phase.", "labels": [], "entities": []}, {"text": "For comparison, we finally include concreteness values taken from, and train an additional network with it.", "labels": [], "entities": []}, {"text": "The reported inter-annotator agreement for the VUAMC is 0.84 in terms of Fleiss' Kappa-in comparison, treating the gold annotations in the test set (complete) and our system as annotators, we achieve 0.56, indicating room for improvement.", "labels": [], "entities": [{"text": "VUAMC", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.8920779824256897}, {"text": "Fleiss' Kappa-in", "start_pos": 73, "end_pos": 89, "type": "METRIC", "confidence": 0.8934749364852905}]}, {"text": "More detailed results for our final network as evaluated on the test sets can be seen in.", "labels": [], "entities": []}, {"text": "We observe that extending the word embeddings with the 10-dimensional POS embeddings only has a small influence on the results.", "labels": [], "entities": []}, {"text": "Adding the concreteness values does not significantly change the results compared to the token-only based approach, which could be due to several factors.", "labels": [], "entities": []}, {"text": "Firstly, the used concreteness values cover only about 80% of the data, with the remaining 20% being assigned a default neutral value.", "labels": [], "entities": []}, {"text": "The one-dimensionality of the concreteness feature is also likely to be part of the problem, demanding fora better representation.", "labels": [], "entities": []}, {"text": "Last, there is a chance of the word embeddings implicitly capturing the concreteness which needs to be investigated further.", "labels": [], "entities": []}, {"text": "We added a pseudo-baseline (B) where each token is labeled as metaphorical; this is handily beaten by our system.", "labels": [], "entities": []}, {"text": "However, more informative is a comparison with the work done by other researchers.", "labels": [], "entities": []}, {"text": "Beigman use the same classification granularity for their experiments, but employ cross-validation on 77% of the VUAMC for their evaluation.", "labels": [], "entities": [{"text": "VUAMC", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.9790787696838379}]}, {"text": "Still, comparing the results gives an indication of our system's performance.", "labels": [], "entities": []}, {"text": "Their best performing feature set shows similar performance on the academic dataset, achieving an F1 score of 0.564 compared to 0.558 for our tokenonly based approach; the fiction subcorpus yields 0.493, respectively 0.507.", "labels": [], "entities": [{"text": "academic dataset", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.7507895231246948}, {"text": "F1 score", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9883568286895752}]}, {"text": "Larger gaps can be observed on the news and especially the conversation sets, where they report results of 0.590 and 0.396 respectively, compared to our scores of 0.635 and 0.559.", "labels": [], "entities": []}, {"text": "The strong performance on the news subcorpus can partly be attested to our choice of word embeddings, which were constructed using news texts and thus best capture usage of words in this genre.", "labels": [], "entities": []}, {"text": "We also trained and tested our network on the complete corpus (complete, again using a 76%/12%/12% split), with the results indicating that it generalizes rather well.", "labels": [], "entities": []}, {"text": "Looking at the data, we can observe some limitations to our approach.", "labels": [], "entities": []}, {"text": "E.g., in the sentence \"To throw up an impenetrable Berlin Wall between you and them could be tactless.\", \"Berlin\" and \"Wall\" are wrongly being tagged as literal, because the used context window is too small to detect their metaphoric usage.", "labels": [], "entities": []}, {"text": "Similar problems occur in other cases where insufficient information is available, because of too short sentences, and also at the beginning or end of a sentence, where the vectors are padded with generic embeddings and thus contain less information.", "labels": [], "entities": []}, {"text": "Such cases could be treated by using larger parts of text instead of sentences, or by adding topical information gained in unsupervised fashion, as it has been done in related work, e.g. via topic models.", "labels": [], "entities": []}, {"text": "Unique problems arise with classification of tokens in the (often colloquial) conversation texts.", "labels": [], "entities": [{"text": "classification of tokens", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8656444946924845}]}, {"text": "The sentences in these transcripts are sometimes missing words, have non-grammatical structure, or are wrongly split.", "labels": [], "entities": []}, {"text": "For example, consider the sentence, \"Yeah, I want a whole with that whole.\"", "labels": [], "entities": []}, {"text": "The only content words are \"want\", \"whole\", and \"whole\", of which the latter two are being wrongly tagged as metaphoric by our system.", "labels": [], "entities": []}, {"text": "However, even additional context likely would not improve the classification in this case-because of missing words and incomplete sentences, even humans have a hard time grasping the meaning of this sentence in (textual) context, let alone assessing metaphoricity.", "labels": [], "entities": []}, {"text": "When examining errors by POS tag, we note that verbs get misclassified twice as often as nouns, which seems intuitive given that verbs generally are more polysemous than nouns.", "labels": [], "entities": []}, {"text": "We can observe increased error rates for tokens that are tagged as being ambiguous between nouns and other POS; considering only these, the percentage of misclassified tokens is double that of the whole noun set.", "labels": [], "entities": [{"text": "error rates", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.9726579785346985}]}, {"text": "Proper nouns are being tagged as literal by the system in all cases, differing from the gold annotations for just 5 out of 813 instances.", "labels": [], "entities": []}, {"text": "However, a metaphoric meaning for 4 of those annotations could be disputed-at least, these annotations are inconsistent with other annotations in the corpus.", "labels": [], "entities": []}, {"text": "adjectives and adverbs: adverbially used prepositions (e.g. \"out\" in \"carry out\"), and borderline cases between adverb and preposition (e.g. \"on\" in \"what's going on\").", "labels": [], "entities": []}, {"text": "Arguably these could be considered non-content words and filtered out as well, which would further increase F1 values of our system.", "labels": [], "entities": [{"text": "F1", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.999169111251831}]}], "tableCaptions": [{"text": " Table 1: Subcorpora of the VU Amsterdam Metaphor Corpus,", "labels": [], "entities": [{"text": "VU Amsterdam Metaphor Corpus", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.908813402056694}]}, {"text": " Table 2: Results on the test sets for the tuned neural network. Showing precision (P), recall (R) and F1-measure (F1) regarding", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9519465267658234}, {"text": "recall (R)", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9544108062982559}, {"text": "F1-measure (F1)", "start_pos": 103, "end_pos": 118, "type": "METRIC", "confidence": 0.9348075091838837}]}]}