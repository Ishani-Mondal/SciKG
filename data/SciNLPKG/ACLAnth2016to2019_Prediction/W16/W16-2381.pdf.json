{"title": [], "abstractContent": [{"text": "We describe University of Sheffield's submission to the word-level Quality Estimation shared task.", "labels": [], "entities": [{"text": "word-level Quality Estimation shared task", "start_pos": 56, "end_pos": 97, "type": "TASK", "confidence": 0.6231256008148194}]}, {"text": "Our system is based on imitation learning, an approach to struc-tured prediction which relies on a classifier trained on data generated appropriately to ameliorate error propagation.", "labels": [], "entities": [{"text": "struc-tured prediction", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7750180959701538}]}, {"text": "Compared to other structure prediction approaches such as conditional random fields, it allows the use of arbitrary information from previous tag predictions and the use of non-decomposable loss functions over the structure.", "labels": [], "entities": [{"text": "structure prediction", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7535447478294373}]}, {"text": "We explore these two aspects in our submission while using the baseline features provided by the shared task organ-isers.", "labels": [], "entities": []}, {"text": "Our system outperformed the conditional random field baseline while using the same feature set.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality estimation (QE) models aim at predicting the quality of machine translated (MT) text.", "labels": [], "entities": [{"text": "Quality estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7569354951381684}]}, {"text": "This prediction can beat several levels, including word-, sentenceand document-level.", "labels": [], "entities": []}, {"text": "In this paper we focus on our submission to the word-level QE WMT 2016 shared task, where the goal is to assign quality labels to each word of the output of an MT system.", "labels": [], "entities": [{"text": "QE WMT 2016 shared task", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.5343961536884307}, {"text": "MT system", "start_pos": 160, "end_pos": 169, "type": "TASK", "confidence": 0.8944969475269318}]}, {"text": "Word-level QE is traditionally treated as a structured prediction problem, similar to part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "Word-level QE", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.42228779196739197}, {"text": "part-of-speech (POS) tagging", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.6759562492370605}]}, {"text": "The baseline model used in the shared task employs a Conditional Random Field (CRF) () with a set of baseline features.", "labels": [], "entities": []}, {"text": "Our system uses a linear classification model trained with imitation learning).", "labels": [], "entities": []}, {"text": "Compared to the baseline approach that uses a CRF, imitation learning has two benefits: \u2022 We can directly use the proposed evaluation metric as the loss to be minimised during training; \u2022 It allows using richer information from previous label predictions in the sentence.", "labels": [], "entities": [{"text": "imitation learning", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9267689287662506}]}, {"text": "Our primary goal with our submissions was to examine if the above benefits would result in better accuracy than that for the CRF.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.998908519744873}]}, {"text": "For this reason, we did not perform any feature engineering: we made use instead of the same features as the baseline model.", "labels": [], "entities": []}, {"text": "Both our submissions outperformed the baseline, showing that there is still room for improvements in terms of modelling, beyond feature engineering.", "labels": [], "entities": []}], "datasetContent": [{"text": "The shared task dataset consists of 15k sentences translated from English to German using an MT system and post-edited by professional translators.", "labels": [], "entities": []}, {"text": "The post-edited version of each sentence is used to obtain quality tags for each word in the MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9172025322914124}]}, {"text": "In this shared task version, two tags are employed: an 'OK' tag means the word is correct and a 'BAD' tag corresponds to a word that needs a post-editing action (either deletion, substitution or the insertion of anew word).", "labels": [], "entities": [{"text": "BAD", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9937923550605774}]}, {"text": "The official split corresponds to 12k, 1k and 2k for training, development and test sets.", "labels": [], "entities": []}, {"text": "Model Following (), we use AROW () for costsensitive classification learning.", "labels": [], "entities": [{"text": "AROW", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.8678074479103088}, {"text": "classification learning", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7964474856853485}]}, {"text": "The loss function is based on the official shared task evaluation metric: = 1 \u2212 [F (OK) \u00d7 F (BAD)], where F is the tag F-measure at the sentence level.", "labels": [], "entities": [{"text": "F (OK) \u00d7 F (BAD)]", "start_pos": 81, "end_pos": 98, "type": "METRIC", "confidence": 0.8433287541071574}]}, {"text": "We experimented with two values for the learning rate \u03b2 and we submitted the best model found for each value.", "labels": [], "entities": [{"text": "learning rate \u03b2", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.7799349824587504}]}, {"text": "The first value is 0.3, which is the same used by.", "labels": [], "entities": []}, {"text": "The second one is 1.0, which essentially means we use the expert policy only in the first iteration, switching to using the learned policy afterwards.", "labels": [], "entities": []}, {"text": "For each setting we run up to 10 iterations of imitation learning on the training set and evaluate the score on the dev set after each iteration.", "labels": [], "entities": []}, {"text": "We select our model in each learning rate setting by choosing the one which performs the best on the dev set.", "labels": [], "entities": []}, {"text": "For \u03b2 = 1.0 this was achieved after 10 iterations, but for \u03b2 = 0.3 the best model was the one obtained after the 6th iteration.", "labels": [], "entities": []}, {"text": "Observed features The features based on the observed instance are the same 22 used in the baseline provided by the task organisers.", "labels": [], "entities": [{"text": "Observed", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9426582455635071}]}, {"text": "Given a word w i in the MT output, these features are defined below: \u2022 Word and context features: -w i (the word itself) The language model backoff behavior features were calculated following the approach in).", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.8819192051887512}]}, {"text": "Structural features As explained in Section 2, a key advantage of imitation learning is the ability to use arbitrary information from previous predictions.", "labels": [], "entities": []}, {"text": "Our submission explores this by defining a set of features based on this information.", "labels": [], "entities": []}, {"text": "Taking ti as the tag to be predicted for the current word, these features are defined in the following way: \u2022 Previous tags: shows the official shared task results for the baseline and our systems, in terms of F1-MULT, the official evaluation metric, and also F1 for each of the classes.", "labels": [], "entities": [{"text": "F1-MULT", "start_pos": 210, "end_pos": 217, "type": "METRIC", "confidence": 0.9982378482818604}, {"text": "F1", "start_pos": 260, "end_pos": 262, "type": "METRIC", "confidence": 0.9995501637458801}]}, {"text": "We report two versions for our submissions: the official one, which had an implementation bug 1 and anew version after the bug fix.", "labels": [], "entities": []}, {"text": "Both official submissions outperformed the baseline, which is an encouraging result considering that we used the same set of features as the baseline.", "labels": [], "entities": []}, {"text": "The submission which employed \u03b2 = 1 performed the best between the two.", "labels": [], "entities": [{"text": "\u03b2", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9484054446220398}]}, {"text": "This is inline with the observations of in similar sequential tagging tasks.", "labels": [], "entities": [{"text": "sequential tagging tasks", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.7057349383831024}]}, {"text": "This setting allows the classifier to move away from using the expert policy as soon as the first classifier is trained.", "labels": [], "entities": []}, {"text": "Analysis To obtain further insights about the benefits of imitation learning for this task we performed additional experiments with different settings.", "labels": [], "entities": [{"text": "imitation learning", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8821796476840973}]}, {"text": "In we compare our systems with a system trained using a single round of training (called exact imitation), which corresponds to using the same classifier trained only on the gold standard tags.", "labels": [], "entities": []}, {"text": "We can see that imitation learning improves over this setting substantially.", "labels": [], "entities": [{"text": "imitation learning", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.826997309923172}]}, {"text": "also shows results obtained using the original DAGGER algorithm, which uses a single 0/1-loss per tag.", "labels": [], "entities": []}, {"text": "While DAGGER improves results over the exact imitation setting, it is outperformed by V-DAGGER.", "labels": [], "entities": [{"text": "DAGGER", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.8322337865829468}]}, {"text": "This is due to the ability of V-DAGGER to incorporate the task loss into its training procedure 2 . In we compare how the F1-MULT scores evolve through the imitation learning iterations for both DAGGER and V-DAGGER.", "labels": [], "entities": [{"text": "F1-MULT", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.998305082321167}]}, {"text": "Even though the performance of V-DAGGER fluctuates The structural feature ti\u22121 was not computed properly.", "labels": [], "entities": []}, {"text": "Formally, our loss is not exactly the same as the official shared task evaluation metric since the former is measured at the sentence level and the latter at the corpus level.", "labels": [], "entities": []}, {"text": "Nevertheless, the loss in V-DAGGER is much closer to the official metric than the 0/1-loss used by DAGGER.: Comparison between our systems (V-DAGGER), exact imitation and DAGGER on the test data.", "labels": [], "entities": [{"text": "exact imitation", "start_pos": 151, "end_pos": 166, "type": "METRIC", "confidence": 0.7733042240142822}, {"text": "DAGGER", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9916936755180359}]}, {"text": "more than that of DAGGER, it is consistently better for both development and test sets.", "labels": [], "entities": [{"text": "DAGGER", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.45673397183418274}]}, {"text": "Finally, we also compare our systems with simpler versions using a smaller set of structural features.", "labels": [], "entities": []}, {"text": "The findings, presented in, show an interesting trend.", "labels": [], "entities": []}, {"text": "The systems do not seem to benefit from the additional structural information available in imitation learning and even a system with no information at all (\"None\" in) outperforms the baseline.", "labels": [], "entities": []}, {"text": "We speculate that this is because the task only deals with a linear chain of binary labels, which makes the structure much less informative compared to the observed features.: Comparison between V-DAGGER systems using different structural feature sets.", "labels": [], "entities": []}, {"text": "All models use the full set of observed features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Official shared task results.", "labels": [], "entities": []}, {"text": " Table 2: Comparison between our systems (V- DAGGER), exact imitation and DAGGER on the  test data.", "labels": [], "entities": [{"text": "exact imitation", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.81801638007164}, {"text": "DAGGER", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9961166381835938}]}, {"text": " Table 3: Comparison between V-DAGGER sys- tems using different structural feature sets. All  models use the full set of observed features.", "labels": [], "entities": []}]}