{"title": [], "abstractContent": [{"text": "This paper presents the contribution of the Unbabel team to the WMT 2016 Shared Task on Word-Level Translation Quality Estimation.", "labels": [], "entities": [{"text": "WMT 2016 Shared Task on Word-Level Translation Quality Estimation", "start_pos": 64, "end_pos": 129, "type": "TASK", "confidence": 0.5911254518561893}]}, {"text": "We describe our two submitted systems: (i) UNBABEL-LINEAR, a feature-rich sequential linear model with syntactic features, and (ii) UNBABEL-ENSEMBLE, a stacked combination of the linear system with three different deep neural networks, mixing feed-forward, convolutional, and recurrent layers.", "labels": [], "entities": [{"text": "UNBABEL-LINEAR", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.8571873903274536}]}, {"text": "Our systems achieved F OK 1 \u00d7 F BAD 1 scores of 46.29% and 49.52%, respectively , which were the two highest scores in the challenge.", "labels": [], "entities": [{"text": "F OK 1 \u00d7 F BAD 1 scores", "start_pos": 21, "end_pos": 44, "type": "METRIC", "confidence": 0.8079589754343033}]}], "introductionContent": [{"text": "Quality estimation is the task of evaluating a translation system's quality without access to reference translations (.", "labels": [], "entities": [{"text": "Quality estimation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6563927531242371}]}, {"text": "This paper describes the contribution of the Unbabel team to the Shared Task on Word-Level Quality Estimation (QE Task 2) at the 2016 Conference on Statistical Machine Translation (WMT 2016).", "labels": [], "entities": [{"text": "Shared Task on Word-Level Quality Estimation (QE Task 2) at the 2016 Conference on Statistical Machine Translation (WMT 2016)", "start_pos": 65, "end_pos": 190, "type": "TASK", "confidence": 0.7634655684232712}]}, {"text": "The task aims to predict the word-level quality of English-to-German machine translated text, by assigning a label of OK or BAD to each word in the translation.", "labels": [], "entities": [{"text": "OK", "start_pos": 118, "end_pos": 120, "type": "METRIC", "confidence": 0.9875310063362122}, {"text": "BAD", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.8660399913787842}]}, {"text": "Our system's architecture is inspired by the recent QUETCH+ system (, which achieved top performance in the WMT 2015 Word Level QE task (.", "labels": [], "entities": [{"text": "WMT 2015 Word Level QE task", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.6275964876015981}]}, {"text": "QUETCH+ predicts the labels of individual words by combining a linear feature-based classifier with a feedforward neural network (called QUETCH, for QUality Estimation from scraTCH).", "labels": [], "entities": []}, {"text": "The linear classifier is based upon and uses the baseline features provided in the shared task.", "labels": [], "entities": []}, {"text": "The QUETCH neural network is a multilayer perceptron, which takes as input the embeddings of the target words and the aligned source words, along with their context, and outputs a binary label for the target word.", "labels": [], "entities": []}, {"text": "The combination is done by stacking the scores of the neural network and the linear classifier as additional features in another linear classifier.", "labels": [], "entities": []}, {"text": "Our main contributions are the following: \u2022 We replaced the word-level linear classifier in QUETCH+ by a sentence-level first-order sequential model.", "labels": [], "entities": []}, {"text": "Our model incorporates rich features for label unigrams and bigrams, detailed in \u00a72.1-2.2.", "labels": [], "entities": []}, {"text": "\u2022 We included syntactic features that look at second-order dependencies between target words.", "labels": [], "entities": []}, {"text": "This is explained in \u00a72.3.", "labels": [], "entities": []}, {"text": "\u2022 We implemented three different neural systems, one extension of the original QUETCH model and two recurrent models with different depth.", "labels": [], "entities": []}, {"text": "These are detailed in \u00a73.1-3.3.", "labels": [], "entities": []}, {"text": "\u2022 We ensembled multiple versions of each neural system for different data shuffles and initializations as additional features for the linear system, via a stacking architecture.", "labels": [], "entities": []}, {"text": "This is detailed in \u00a74.", "labels": [], "entities": []}, {"text": "The following external resources were used: part-of-speech tags and extra syntactic dependency information were obtained with TurboTagger and TurboParser), 1 trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 173, "end_pos": 186, "type": "DATASET", "confidence": 0.9953982830047607}, {"text": "German TIGER corpus", "start_pos": 227, "end_pos": 246, "type": "DATASET", "confidence": 0.6732456286748251}, {"text": "SPMRL shared task", "start_pos": 259, "end_pos": 276, "type": "TASK", "confidence": 0.7916681369145712}]}, {"text": "For the neural models, we used pre-trained word embeddings from Polyglot (Al- and embeddings obtained from a trained neural MT system ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance on the dev-set of several  configurations of the UNBABEL-LINEAR system.  The model with simple bigrams has a single BIAS  bigram feature, conjoined with the label pairs.", "labels": [], "entities": [{"text": "BIAS", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9069130420684814}]}, {"text": " Table 2: Effect of intra-model ensembling of the  feed-forward network reproducing QUETCH.", "labels": [], "entities": []}, {"text": " Table 4: Effect of intra-model ensembling of  the multi-feature convolutional recurrent network  model.", "labels": [], "entities": []}, {"text": " Table 5: Performance of a stacked network en- sembling each of the three deep models and the  linear model, and of a full ensemble (UNBABEL-", "labels": [], "entities": []}, {"text": " Table 6: Performance of the submitted systems  on the test set.", "labels": [], "entities": []}]}