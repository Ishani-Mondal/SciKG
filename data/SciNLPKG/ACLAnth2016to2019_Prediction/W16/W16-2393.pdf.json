{"title": [], "abstractContent": [{"text": "This paper describes the submission of the UGENT-LT3 SCATE system to the WMT16 Shared Task on Quality Estimation (QE), viz.", "labels": [], "entities": [{"text": "UGENT-LT3 SCATE", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.543236643075943}, {"text": "WMT16 Shared Task on Quality Estimation (QE)", "start_pos": 73, "end_pos": 117, "type": "TASK", "confidence": 0.6860697004530165}]}, {"text": "English-German word and sentence-level QE.", "labels": [], "entities": []}, {"text": "Based on the observation that the data set is homogeneous (all sentences belong to the IT domain), we performed bilingual terminology extraction and added features derived from the resulting term list to the well-performing features of the word-level QE task of last year.", "labels": [], "entities": [{"text": "bilingual terminology extraction", "start_pos": 112, "end_pos": 144, "type": "TASK", "confidence": 0.6571030219395956}]}, {"text": "For sentence-level QE, we analyzed the importance of the features and based on those insights extended the feature set of last year.", "labels": [], "entities": []}, {"text": "We also experimented with different learning methods and ensembles.", "labels": [], "entities": []}, {"text": "We present our observations from the different experiments we conducted and our submissions for both tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Translation (MT) Quality Estimation (QE) is the task of providing a quality indicator for unseen automatically translated sentences without relying on reference translations ().", "labels": [], "entities": [{"text": "Machine Translation (MT) Quality Estimation (QE)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8229852557182312}]}, {"text": "The WMT16 QE shared task proposes three evaluation tasks: (1) scoring and ranking sentences according to predicted post-editing effort given a source sentence and its translation; predicting the individual (2a) words and (2b) phrases (segmented by the Statistical Machine Translation (SMT) decoder) that require post-editing; and (3) predicting the quality at document level.", "labels": [], "entities": [{"text": "WMT16 QE shared task", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6741136610507965}, {"text": "Statistical Machine Translation (SMT) decoder", "start_pos": 252, "end_pos": 297, "type": "TASK", "confidence": 0.796561815908977}]}, {"text": "In this paper, we describe the UGENT-LT3 SCATE submissions to task 1 (sentence-level QE) and task 2a (word-level QE).", "labels": [], "entities": [{"text": "UGENT-LT3 SCATE submissions", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.5760746399561564}]}, {"text": "By conceiving the QE as a supervised Machine Learning (ML) problem for both tasks, we extended the features that we extracted for our last year's submission (, which try to capture the accuracy and fluency errors in MT output.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.998627781867981}, {"text": "MT output", "start_pos": 216, "end_pos": 225, "type": "TASK", "confidence": 0.8611445724964142}]}, {"text": "While accuracy is concerned with how much of the meaning expressed in the source is also expressed in the target text, fluency is concerned with to what extent the translation is wellformed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9991394281387329}]}, {"text": "This distinction between accuracy and fluency was suggested to breakdown human translation quality judgments into separate and smaller units and is well known in quality assessment schemes for MT).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9983854293823242}, {"text": "MT", "start_pos": 193, "end_pos": 195, "type": "TASK", "confidence": 0.9879487752914429}]}, {"text": "Similarly, we use the same distinction to breakdown the QE task into separate units.", "labels": [], "entities": []}, {"text": "In addition to the features that try to capture accuracy and fluency errors, given the specialized domain of this year's data set (IT), for word-level QE, we extracted features that try to capture terminological problems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9964330196380615}, {"text": "year's data set (IT)", "start_pos": 114, "end_pos": 134, "type": "DATASET", "confidence": 0.8354858756065369}]}, {"text": "For both tasks, we experimented with different learning methods.", "labels": [], "entities": []}, {"text": "For word-level QE we also built ensemble systems that are based on majority voting and bagging (random forests), in which multiple decision trees are constructed using bootstrapped training sets and the predictions of these trees are averaged.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 and Section 3 give an overview of the shared task on word-level QE and sentence-level QE respectively and describe the extracted features, the additional language resources that were used for feature extraction, the learning methods and the experiments that were conducted.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 202, "end_pos": 220, "type": "TASK", "confidence": 0.7459557354450226}]}, {"text": "Section 4 concludes by discussing the results and observations that were made.", "labels": [], "entities": []}, {"text": "rors at word level by marking words either as OK or BAD.", "labels": [], "entities": [{"text": "BAD", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9270084500312805}]}, {"text": "In WMT16, submissions are evaluated in terms of classification performance via the multiplication of F1-scores for the OK and BAD classes against the original labels due to the fact that the F1-score for the BAD class, which has been used as a primary metric in previous years, is biased towards 'pessimistic' labeling.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.7258083820343018}, {"text": "F1-scores", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.99574214220047}, {"text": "F1-score", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9903111457824707}]}, {"text": "In contrast, the multiplication of F1-OK and F1-BAD has two components and is more balanced.", "labels": [], "entities": [{"text": "F1-OK", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.8996171355247498}]}, {"text": "The organizers provided a data set of English source sentences with the corresponding German MT output, generated by a statistical MT system and the post-edited MT output.", "labels": [], "entities": [{"text": "German MT output", "start_pos": 86, "end_pos": 102, "type": "DATASET", "confidence": 0.6754483977953593}, {"text": "MT", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.917298436164856}]}, {"text": "This data set consists of a training set of 12,000 sentences, a development set of 1,000 sentences and a test set of 2,000 sentences.", "labels": [], "entities": []}, {"text": "As in previous years, the MT output in the training and development data are automatically annotated for errors with binary wordlevel labels by using the alignments provided by the TER tool ().", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9771776795387268}, {"text": "TER", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.7628471255302429}]}, {"text": "The distribution of the binary labels and the average sentence length for the training and development sets (in number of tokens) are given in: Number of words, distribution of the binary labels and the average sentence length, on the training and development set.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the first round of our experiments, we used two feature sets, namely the baseline features (b) and the additional features (a) that are described in.", "labels": [], "entities": []}, {"text": "We applied hyper-parameter optimization for the ML algorithms (when applicable) using 10-fold cross validation on the training set and tested the regression performance on the development set.", "labels": [], "entities": [{"text": "ML", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9145343899726868}]}, {"text": "The performance of the different ML algorithms and the different feature sets, with respect to Pearson's correlation (r), Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are provided in.: The performance of different ML algorithms and feature sets on the development set.", "labels": [], "entities": [{"text": "Pearson's correlation (r)", "start_pos": 95, "end_pos": 120, "type": "METRIC", "confidence": 0.9629465242226919}, {"text": "Mean Absolute Error (MAE)", "start_pos": 122, "end_pos": 147, "type": "METRIC", "confidence": 0.968381812175115}, {"text": "Root Mean Squared Error (RMSE)", "start_pos": 152, "end_pos": 182, "type": "METRIC", "confidence": 0.7422721045357841}]}, {"text": "The plus sign '+' indicates the combined feature sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of words, distribution of the bi- nary labels and the average sentence length, on the  training and development set.", "labels": [], "entities": []}, {"text": " Table 3: Additional language resources that were  used to extract features and the number of seg- ments in each data set.", "labels": [], "entities": []}, {"text": " Table 4: The performance of different ML algo- rithms and feature sets on the development set.  The plus sign '+' indicates the combined feature  sets.", "labels": [], "entities": []}, {"text": " Table 5: The disagreement ratios between the pre- dicted labels by different algorithms (feature set  'b+s+t').", "labels": [], "entities": []}, {"text": " Table 6. In this table, we provide  the MLT scores for these two ensemble systems.  For the second system, which combines an even  number of algorithms, we consider the both possi- ble output types (OK or BAD) in case of ties.", "labels": [], "entities": [{"text": "MLT", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.8205357193946838}, {"text": "BAD", "start_pos": 206, "end_pos": 209, "type": "METRIC", "confidence": 0.9032036066055298}]}, {"text": " Table 6: The MLT scores for the two ensemble  systems. The plus sign '+' indicates the combined  algorithms.", "labels": [], "entities": [{"text": "MLT", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.4880297780036926}]}, {"text": " Table 7: The performance of different ML algo- rithms and feature sets on the development set.  The plus sign '+' indicates the combined feature  sets.", "labels": [], "entities": []}, {"text": " Table 8: The top five features for the RF system,  with respect to gini importance scores, which uses  the b+a feature set. Each feature is marked in  brackets with the feature set that it comes from.", "labels": [], "entities": [{"text": "RF", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9572335481643677}, {"text": "gini importance scores", "start_pos": 68, "end_pos": 90, "type": "METRIC", "confidence": 0.7825059692064921}]}, {"text": " Table 9: The performance of different ML algo- rithms and feature sets on the development set.  The plus sign '+' indicates the combined feature  sets.", "labels": [], "entities": []}, {"text": " Table 10: The performance of different ML al- gorithms and feature sets on the development set.  While the plus sign '+' indicates inclusion, the mi- nus sign '-' indicates the exclusion of a particular  feature(s).", "labels": [], "entities": [{"text": "ML al- gorithms", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.6781696826219559}]}, {"text": " Table 11: The mean, standard deviation and max- imum values for each data set consisting of pre- dicted and gold standard HTER scores.", "labels": [], "entities": [{"text": "mean", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9591386318206787}, {"text": "standard deviation and max- imum", "start_pos": 21, "end_pos": 53, "type": "METRIC", "confidence": 0.782407725850741}]}]}