{"title": [{"text": "Towards Generating Colour Terms for Referents in Photographs: Prefer the Expected or the Unexpected?", "labels": [], "entities": [{"text": "Generating Colour Terms", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.6526157955328623}]}], "abstractContent": [{"text": "Colour terms have been a prime phenomenon for studying language grounding, though previous work focussed mostly on descriptions of simple objects or colour swatches.", "labels": [], "entities": [{"text": "language grounding", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.697924792766571}]}, {"text": "This paper investigates whether colour terms can be learned from more realistic and potentially noisy visual inputs, using a corpus of referring expressions to objects represented as regions in real-world images.", "labels": [], "entities": []}, {"text": "We obtain promising results from combining a classifier that grounds colour terms in visual input with a recalibra-tion model that adjusts probability distributions over colour terms according to contex-tual and object-specific preferences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Pioneering work on natural language generation from perceptual inputs has developed approaches that learn to describe visual scenes from multimodal corpus data and model the connection between words and non-symbolic perceptual features.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.6929402152697245}]}, {"text": "In this paradigm, colour terms have received special attention.", "labels": [], "entities": []}, {"text": "Intuitively, a model of perceptually grounded meaning should associate words for colour with particular points or regions in a colour space, e.g.).", "labels": [], "entities": []}, {"text": "On the other hand, their visual association seems to vary with the linguistic context such as 'red' in the context of 'hair', 'car' or 'wine'.", "labels": [], "entities": []}, {"text": "Recently, large-scale data sets of real-world images and image descriptions, e.g. (), or referring expressions () have become available and can now serve as a realistic test bed for models of language grounding.", "labels": [], "entities": []}, {"text": "In this paper, we use the ReferIt corpus () to assess the performance of classifiers that predict colour terms from low-level visual representations of their corresponding image regions.", "labels": [], "entities": [{"text": "ReferIt corpus", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.7421969175338745}]}, {"text": "A number of studies on colour naming have looked at experimental settings where speakers referred to simple objects or colour swatches instantiating a single value in a colour space.", "labels": [], "entities": [{"text": "colour naming", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.8415646255016327}]}, {"text": "Even in these controlled settings, speakers use colour terms in flexible, context-dependent ways ().", "labels": [], "entities": []}, {"text": "Therefore, probabilistic models and classifiers, allowing for variable thresholds and boundaries between regions in a colour space, have been proposed to capture their grounded meaning.", "labels": [], "entities": []}, {"text": "Can we learn to predict colour terms for more complex and potentially noisy visual inputs?", "labels": [], "entities": []}, {"text": "In contrast to simple colour swatches, real-world objects often have internal structure, their visual colour values are hardly ever uniform and the colour terms can refer to a specific segment of the referent (see image a) and b) in).", "labels": [], "entities": []}, {"text": "Moreover, the low-level visual representation of objects in real-world images can vary tremendously with illumination conditions, whereas human colour perception seems to be robust to illumination, which is known as the \"colour constancy\" problem.", "labels": [], "entities": []}, {"text": "Research on colour perception suggests that speakers use \"top-down\" world knowledge about the prototypical colours of an object to recalibrate their per-246 (a)\"small red car on right\" (b\"yellow building\" (c)\"green\" (d)\"first set of green on right\" (e)\"red plants in the middle\" (f)\"red rock bluff center\" ception of an object to its expected colours).", "labels": [], "entities": []}, {"text": "For instance, the use 'green' for the two, rather different hues in (c-d) might be attributed to the fact that both objects are plants and expected to be green.", "labels": [], "entities": []}, {"text": "However, recalibration to expected colours is not the only possible effect of context.", "labels": [], "entities": []}, {"text": "Despite or because of special illumination conditions, the mountain in (f) and the plants in (e) are described as 'red', a rather atypical, unexpected colour that is, therefore, contextually salient and informative.", "labels": [], "entities": []}, {"text": "This relates to research on referential over-specification showing that speakers are more likely to (redundantly) name a colour if it is atypical).", "labels": [], "entities": []}, {"text": "In our corpus study, we find that these various contextual effects pose a considerable challenge for accurate colour term classification.", "labels": [], "entities": [{"text": "accurate colour term classification", "start_pos": 101, "end_pos": 136, "type": "TASK", "confidence": 0.6160719469189644}]}, {"text": "We explore two ways to make perceptually grounded classifiers sensitive to context: grounded classifiers that are restricted to particular object types and \"recalibration\" classifiers that learn to adjust predictions by a general visual classifier to the preferences of an object and its context.", "labels": [], "entities": []}, {"text": "Whereas objectspecific colour classifiers perform poorly, we find that the latter recalibration approach yields promising results.", "labels": [], "entities": [{"text": "objectspecific colour classifiers", "start_pos": 8, "end_pos": 41, "type": "TASK", "confidence": 0.6468220353126526}]}, {"text": "This seems to be inline with a model by that assumes contextindependent colour prototypes which can be projected into the space of known colours for an object.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Classifiers We use the same training data as in our previous experiment (Section 2.3).", "labels": [], "entities": []}, {"text": "But now, we separate the training instances according to their labels (Section 3.1) and train several visual colour classifiers, i.e. one multi-class multi-layer perceptron per object label.", "labels": [], "entities": []}, {"text": "In order to assess the impact of the underlying object classification, we used la-bels corresponding to (i) to the annotated, specific object types, (ii) the more general object classes.", "labels": [], "entities": []}, {"text": "In each case, we only trained visual classifiers for labels with more than 50 instances in the training data.", "labels": [], "entities": []}, {"text": "This leaves us with 52 visual classifiers for object types, and 33 visual classifiers for object classes.", "labels": [], "entities": []}, {"text": "Evaluation During testing, we assume that the object labels are known and we retrieve the corresponding visual classifiers.", "labels": [], "entities": []}, {"text": "For objects with unknown labels (not contained in the training set) or an infrequent label (with less than 50 instances in the training set) we use the general visual classifier from Section 2.3 (the perceptron trained on RGB histograms).", "labels": [], "entities": []}, {"text": "In, we report the colour prediction accuracy on the overall test set and on the subset of testing instances where the object-specific classifiers predicted a different colour term than the general visual classifier.", "labels": [], "entities": [{"text": "colour prediction", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.6148219704627991}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9177742600440979}]}, {"text": "This way, we assess how often the object-specific classifiers actually 'recalibrate' the decision of the general classifier and whether this calibration leads to an improvement.", "labels": [], "entities": []}, {"text": "shows that the classifiers trained for object types (visual object ) revise the decisions of the general classifier (visual general ) relatively often (for 619 out of 1328 testing instances), but rarely make a prediction that is different from the general classifier and correct (19% of the cases).", "labels": [], "entities": []}, {"text": "Thus, overall, they severely decrease the performance of the colour term prediction.", "labels": [], "entities": [{"text": "colour term prediction", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.5802029073238373}]}, {"text": "Similarly, the visual classifiers for object classes lead to a considerable decrease in performance.", "labels": [], "entities": []}, {"text": "Interestingly, the predictions 250: colour term prediction for general (visual general ) and object-specific (visual object ) visual classifiers, accuracies reported on the recalibrated subset where predictions differ between the general and the object-specific classifiers, and for the whole testset of this model seem to often differ from the general visual classifier when the latter is relatively confident: the general visual accuracy on this subset is much higher (72%) than on the overall test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 431, "end_pos": 439, "type": "METRIC", "confidence": 0.952006995677948}]}, {"text": "This suggests that the object-specific visual classifiers do not learn prototypical meanings of colour terms and are much more sensitive to noise whereas the general colour classifier has an advantage rather than a disadvantage from seeing a lot of different instances of a particular colour.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracies for general visual colour classifiers", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9962921142578125}, {"text": "general visual colour classifiers", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.6837609112262726}]}, {"text": " Table 4: colour term prediction for general (visual general ) and object-specific (visual object ) visual classifiers, accuracies reported", "labels": [], "entities": [{"text": "colour term prediction", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6279975473880768}]}]}