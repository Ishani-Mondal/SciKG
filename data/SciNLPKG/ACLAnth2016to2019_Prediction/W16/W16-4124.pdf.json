{"title": [{"text": "Upper Bound of Entropy Rate Revisited -A New Extrapolation of Compressed Large-Scale Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "The article presents results of entropy rate estimation for human languages across six languages by using large, state-of-the-art corpora of up to 7.8 gigabytes.", "labels": [], "entities": [{"text": "entropy rate estimation", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.7746440768241882}]}, {"text": "To obtain the estimates for data length tending to infinity, we use an extrapolation function given by an ansatz.", "labels": [], "entities": []}, {"text": "Whereas some ansatzes of this kind were proposed in previous research papers, here we introduce a stretched exponential extrapolation function that has a smaller error of fit.", "labels": [], "entities": [{"text": "error of fit", "start_pos": 162, "end_pos": 174, "type": "METRIC", "confidence": 0.8150404691696167}]}, {"text": "In this way, we uncover a possibility that the entropy rates of human languages are positive but 20% smaller than previously reported.", "labels": [], "entities": []}], "introductionContent": [{"text": "Estimation of the entropy rate of natural language is a challenge originally setup by Shannon.", "labels": [], "entities": []}, {"text": "The entropy rate quantifies the complexity of language, precisely the rate how fast the amount of information grows in our communication with respect to the text length.", "labels": [], "entities": []}, {"text": "Today, the entropy rate provides an important target for data compression algorithms, where the speed of convergence of the compression rate to the entropy rate is an informative benchmark.", "labels": [], "entities": [{"text": "data compression algorithms", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.8221457004547119}]}, {"text": "Measuring the entropy rate is also the first step in answering what kind of a stochastic process can model generation of texts in natural language, an important question for many practical tasks of natural language engineering.", "labels": [], "entities": []}, {"text": "An important theoretical question concerning the entropy rate, which has also been noted in the domains of computational linguistics) and speech processing (, is whether the entropy rate of human language is a strictly positive constant.", "labels": [], "entities": [{"text": "speech processing", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.7621199488639832}]}, {"text": "The overwhelming evidence collected so far suggests that it is so-in particular, the amount of information communicated per unit time in English text is generally agreed to be about 1 bpc (bit per character).", "labels": [], "entities": []}, {"text": "Although this is what we might intuitively expect, Hilberg formulated a hypothesis that the entropy rate of natural language is zero.", "labels": [], "entities": []}, {"text": "Zero entropy rate does not imply that the amount of information in texts is not growing, but that it grows with a speed slower than linear.", "labels": [], "entities": []}, {"text": "From this perspective we want to provide as exact estimates of the entropy rate for natural language as possible.", "labels": [], "entities": []}, {"text": "Precise estimation of the entropy rate is a challenging task mainly because, mathematically speaking, the sought parameter is a limit for text length tending to infinity.", "labels": [], "entities": []}, {"text": "To alleviate this problem, previous great minds proposed estimation methods based on human cognitive testing.", "labels": [], "entities": []}, {"text": "Since human testing is costly, however, such attempts remain limited in terms of the scale and number of tested languages.", "labels": [], "entities": []}, {"text": "In contrast, although any conceivable data size can only be finite, today's language data have become so large in scale that we may reconsider estimation of the entropy rate using big data computation.", "labels": [], "entities": []}, {"text": "This point was already raised by, which led to important previous works such as) in the domain of computational linguistics.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 98, "end_pos": 123, "type": "TASK", "confidence": 0.7290699481964111}]}, {"text": "Both of these articles and many other that followed, however, mostly considered the English language only.", "labels": [], "entities": []}, {"text": "In contrast, in this article, we present the results of entropy rate estimation using state-of-the-art large data sets in six different languages, including up to 7.8 gigabytes of data in English.", "labels": [], "entities": [{"text": "entropy rate estimation", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.7054151097933451}]}, {"text": "We try to estimate the entropy rate by compressing these data sets using the PPM algorithm and extrapolating the data points with a carefully selected ansatz function.", "labels": [], "entities": []}, {"text": "Whereas a couple of ansatz functions were previously proposed in, here we introduce another function, which is a stretched exponential function and enjoys the same number of parameters as previous proposals.", "labels": [], "entities": []}, {"text": "The new functions yields a smaller error of fit.", "labels": [], "entities": [{"text": "error of fit", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.8126925230026245}]}, {"text": "As a result, we arrive at the entropy rate estimates which are positive but 20% smaller than previously reported.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data used in this work, its size, its encoding rate, entropy rate and the error", "labels": [], "entities": [{"text": "entropy rate", "start_pos": 63, "end_pos": 75, "type": "METRIC", "confidence": 0.9032681882381439}]}]}