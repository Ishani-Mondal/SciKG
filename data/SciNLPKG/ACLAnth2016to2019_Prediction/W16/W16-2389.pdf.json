{"title": [], "abstractContent": [{"text": "This paper describes our submission to the shared task on word/phrase level Quality Estimation (QE) in the First Conference on Statistical Machine Translation (WMT16).", "labels": [], "entities": [{"text": "word/phrase level Quality Estimation (QE) in the First Conference on Statistical Machine Translation (WMT16)", "start_pos": 58, "end_pos": 166, "type": "TASK", "confidence": 0.7030779153108597}]}, {"text": "The objective of the shared task was to predict if the given word/phrase is a correct/incorrect (OK/BAD) translation in the given sentence.", "labels": [], "entities": [{"text": "BAD", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.8663318157196045}]}, {"text": "In this paper, we propose a novel approach for word level Quality Estimation using Recurrent Neural Network Language Model (RNN-LM) architecture.", "labels": [], "entities": [{"text": "word level Quality Estimation", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.6393179371953011}]}, {"text": "RNN-LMs have been found very effective in different Natural Language Processing (NLP) applications.", "labels": [], "entities": []}, {"text": "RNN-LM is mainly used for vector space language model-ing for different NLP problems.", "labels": [], "entities": [{"text": "RNN-LM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7790848016738892}]}, {"text": "For this task, we modify the architecture of RNN-LM.", "labels": [], "entities": [{"text": "RNN-LM", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.8687782287597656}]}, {"text": "The modified system predicts a label (OK/BAD) in the slot rather than predicting the word.", "labels": [], "entities": [{"text": "predicts a label (OK/BAD)", "start_pos": 20, "end_pos": 45, "type": "METRIC", "confidence": 0.5999133437871933}]}, {"text": "The input to the system is a word sequence, similar to the standard RNN-LM.", "labels": [], "entities": [{"text": "RNN-LM", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.8955645561218262}]}, {"text": "The approach is language independent and requires only the translated text for QE.", "labels": [], "entities": []}, {"text": "To estimate the phrase level quality, we use the output of the word level QE system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality estimation is the process to predict the quality of translation without any reference translation (.", "labels": [], "entities": [{"text": "Quality estimation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6906833648681641}]}, {"text": "Whereas, Machine Translation (MT) system evaluation does require references (human translation).", "labels": [], "entities": [{"text": "Machine Translation (MT) system evaluation", "start_pos": 9, "end_pos": 51, "type": "TASK", "confidence": 0.8672609414373126}]}, {"text": "QE could be done at word, phrase, sentence or document level.", "labels": [], "entities": [{"text": "QE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8615553975105286}]}, {"text": "This paper describes the submission to the shared task on word and phrase level QE (Task 2) for English-German (en-de) MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 119, "end_pos": 121, "type": "TASK", "confidence": 0.6229379177093506}]}, {"text": "The shared task has the trace of last five years' research in the field of QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.8396839499473572}]}, {"text": "In recent years, RNN-LM has demonstrated exceptional performance in a variety of NLP applications ().", "labels": [], "entities": []}, {"text": "The RNN-LM represents each word as high-dimensional real-valued vectors, like the other continuous space language models such as feed forward neural network language models ( and Hierarchical Log-Bi-linear language models.", "labels": [], "entities": []}, {"text": "In this paper, we have used a modified version of RNN-LM, which accepts the word sequence (context window) as input and predicts label at the output for the middle word.", "labels": [], "entities": []}, {"text": "Whereas, for standard RNN-LM model, \"Effekte standardmig\" would be the input to the network with \"sind\" as the output.", "labels": [], "entities": [{"text": "Effekte", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.8803472518920898}]}, {"text": "We add padding at the start and end of the sentence according to the context window.", "labels": [], "entities": []}, {"text": "The detailed description of the model and its implementation is given in section 3.", "labels": [], "entities": []}, {"text": "We have used the data provided by the or-ganizers for the shared task on quality estimation (2016) which includes: (i) source sentence (ii) translated output (word/phrase level) (iii) word/phrase level tagging (OK/BAD) (iv) post edited translation (v) 22 baseline features (vi) word alignment.", "labels": [], "entities": [{"text": "word/phrase level tagging", "start_pos": 184, "end_pos": 209, "type": "TASK", "confidence": 0.5921493470668793}, {"text": "BAD", "start_pos": 214, "end_pos": 217, "type": "METRIC", "confidence": 0.6858116984367371}, {"text": "word alignment", "start_pos": 278, "end_pos": 292, "type": "TASK", "confidence": 0.720735639333725}]}, {"text": "The goal of the task is to predict whether the given word/phrase is a correct/incorrect (OK/BAD) translation in the given sentence.", "labels": [], "entities": [{"text": "BAD", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.8653944134712219}]}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the related work.", "labels": [], "entities": []}, {"text": "Section 3 presents RNN models we use, and its implementation.", "labels": [], "entities": []}, {"text": "In section 4, we discuss the data distribution, our approaches, and results.", "labels": [], "entities": []}, {"text": "Discussion of our methodology and different models is covered in section 5 followed by concluding remarks in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the experiments carried out for the shared task and present the experimental results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: F1 scores of different experiments for  Word level QE. (PT: Pretrain; BL: Bilingual; SL:  Sublabels)", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9988064765930176}, {"text": "Word level QE", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.47578585147857666}, {"text": "PT", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9855527281761169}]}, {"text": " Table 3: F1 scores of different experiments for  Phrase level QE.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9992961883544922}, {"text": "Phrase level QE", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7088064750035604}]}, {"text": " Table 4: Results, word level submission.", "labels": [], "entities": [{"text": "word level submission", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.7213025887807211}]}, {"text": " Table 5: Results, phrase level submission.", "labels": [], "entities": [{"text": "phrase level submission", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.7036889394124349}]}]}