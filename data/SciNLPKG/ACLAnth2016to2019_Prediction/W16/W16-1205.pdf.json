{"title": [{"text": "Cross-lingual alignment transfer: a chicken-and-egg story?", "labels": [], "entities": [{"text": "Cross-lingual alignment transfer", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7602070967356364}]}], "abstractContent": [{"text": "In this paper, we challenge a basic assumption of many cross-lingual transfer techniques: the availability of word aligned parallel corpora, and consider ways to accommodate situations in which such resources do not exist.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7160961180925369}]}, {"text": "We show experimentally that, here again, weakly supervised cross-lingual learning techniques can prove useful, once adapted to transfer knowledge across pairs of languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised machine learning techniques lie at the core of many robust Natural Language Processing (NLP) systems and components.", "labels": [], "entities": []}, {"text": "The dissemination of such methodologies is however hindered by the lack of appropriate supervision data, which are costly to produce and only available fora restricted number of genres, tasks, domains and languages.", "labels": [], "entities": []}, {"text": "Weakly supervised learning techniques have emerged as an effective way to remedy, at least partially, to this unsatisfactory situation.", "labels": [], "entities": []}, {"text": "Among them, cross-lingual learning methods enable to transfer useful supervision information from well-resourced to under-resourced languages, speeding up the development of NLP tools for new domains and tasks.", "labels": [], "entities": []}, {"text": "Many techniques for transferring knowledge across languages have been proposed in the literature (see \u00a7 2 fora brief overview).", "labels": [], "entities": []}, {"text": "A widely-used methodology consists in generating automatic annotations for the resource-poor language by projecting linguistic information through word alignment links (see eg. () for PoS tagging, () for dependency parsing, () for Named Entity Recognition, () for Semantic Role Labeling, etc.).", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 184, "end_pos": 195, "type": "TASK", "confidence": 0.7978726327419281}, {"text": "dependency parsing", "start_pos": 204, "end_pos": 222, "type": "TASK", "confidence": 0.8122806251049042}, {"text": "Named Entity Recognition", "start_pos": 231, "end_pos": 255, "type": "TASK", "confidence": 0.6073455909887949}, {"text": "Semantic Role Labeling", "start_pos": 264, "end_pos": 286, "type": "TASK", "confidence": 0.7507293224334717}]}, {"text": "Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side.", "labels": [], "entities": []}, {"text": "However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages.", "labels": [], "entities": []}, {"text": "In this study, we explore ways to overcome this paradox and consider techniques for transferring alignment models or annotations across language pairs, a task that has hardly been addressed in literature (see however (;).", "labels": [], "entities": []}, {"text": "Based on a high-level typology of cross-lingual transfer methodologies ( \u00a7 2), our contribution is to formalize realistic scenarios (defined in \u00a7 3) as well as some basic methodologies for projecting knowledge about bilingual alignments crosslinguistically ( \u00a7 4).", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.6997849494218826}]}, {"text": "Experiments in \u00a7 5 show that, at least for some of these scenarios, simple-minded methods can be surprisingly effective and open a discussion on further prospects and perspectives for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we experiment with the methods introduced above and compare them with standard unsupervised models of varying sizes.", "labels": [], "entities": []}, {"text": "Experimental setup We evaluate the proposed alignment transfer methods on the English-Swedish test set provided by, which consists of 192 word aligned sentence pairs extracted from the English-Swedish part of Europarl (Koehn, ).", "labels": [], "entities": [{"text": "alignment transfer", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9165077805519104}, {"text": "English-Swedish test set", "start_pos": 78, "end_pos": 102, "type": "DATASET", "confidence": 0.780326634645462}, {"text": "Europarl", "start_pos": 209, "end_pos": 217, "type": "DATASET", "confidence": 0.5656405091285706}]}, {"text": "We score the methods according to the intrinsic Alignment Error Rate (AER) metric proposed by.", "labels": [], "entities": [{"text": "intrinsic Alignment Error Rate (AER) metric", "start_pos": 38, "end_pos": 81, "type": "METRIC", "confidence": 0.8843174874782562}]}, {"text": "As documented in a large body of literature (, AER poorly correlates with translation quality of the systems trained on the evaluated alignments, especially for large corpora, and extrinsic metrics like the BLEU score should be preferred, were MT training the final goal of alignment.", "labels": [], "entities": [{"text": "AER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9988483190536499}, {"text": "BLEU score", "start_pos": 207, "end_pos": 217, "type": "METRIC", "confidence": 0.9713215827941895}, {"text": "MT", "start_pos": 244, "end_pos": 246, "type": "TASK", "confidence": 0.9716049432754517}]}, {"text": "The concatenation methods proposed here are intended for very small data, with large unbalance between the target and the bridge sets, a data size for which the SMT application is not relevant.", "labels": [], "entities": [{"text": "SMT", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.983026921749115}]}, {"text": "Consequently, we use the PoS accuracy of a cross-lingual tagger () weakly supervised by the word alignments as the extrinsic evaluation metric of our methods.", "labels": [], "entities": [{"text": "PoS accuracy", "start_pos": 25, "end_pos": 37, "type": "METRIC", "confidence": 0.7264648675918579}]}, {"text": "In such a system, each extra sentence pair brings valuable knowledge, while incorrect links strongly noise the system, making the accuracy a direct indicator of alignment quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9989745616912842}]}, {"text": "Besides, this step completes a realistic low-resource scenario where word alignments are needed for an intended cross-lingual use.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7022216469049454}]}, {"text": "We use the English-Swedish bitext both as a test set for intrinsic evaluation and as projection data to train cross-lingual taggers.", "labels": [], "entities": [{"text": "cross-lingual taggers", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.7265877723693848}]}, {"text": "PoS accuracies are computed on the coarse PoS tags of the Swedish test treebank of the Universal Dependencies 1.) and the source English tagger is trained on the training portion of the same corpus.", "labels": [], "entities": [{"text": "Swedish test treebank of the Universal Dependencies 1.", "start_pos": 58, "end_pos": 112, "type": "DATASET", "confidence": 0.8478633537888527}]}, {"text": "In every method where additional parallel data is required, we use Europarl without the Q4-2000 section, which is reserved for tests.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.9747589826583862}]}, {"text": "We evaluate AER and PoS accuracy for the three concatenation methods presented in \u00a7 4, with transfer through Danish: CAT-DA, TR-DA, DA-TR, and the three parameter transfer methods: DA, GLOSSES-DA, PARAM-DA.", "labels": [], "entities": [{"text": "AER", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9983673691749573}, {"text": "PoS", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9004764556884766}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.8541810512542725}, {"text": "GLOSSES-DA", "start_pos": 185, "end_pos": 195, "type": "METRIC", "confidence": 0.9506098628044128}, {"text": "PARAM-DA", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9610076546669006}]}, {"text": "To evaluate the benefits of using a related bridge language, we also run the concatenation experiments with transfer through Greek, that is only distantly related to Swedish and also uses a different alphabet (methods CAT-EL, TR-EL, EL-TR).", "labels": [], "entities": []}, {"text": "Finally, the alignment performance is confronted with that of concatenation with EnglishSwedish data of various sizes, from no added pair (BASELINE) to full concatenation of the 1.8M sentence pairs in Europarl (CAT-SV).", "labels": [], "entities": [{"text": "EnglishSwedish data", "start_pos": 81, "end_pos": 100, "type": "DATASET", "confidence": 0.9037780463695526}, {"text": "BASELINE", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9821502566337585}, {"text": "Europarl (CAT-SV)", "start_pos": 201, "end_pos": 218, "type": "DATASET", "confidence": 0.8723767250776291}]}, {"text": "Results reports the AERs of the various methods when using IBM 1, HMM or IBM 4 models.", "labels": [], "entities": [{"text": "AERs", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9993570446968079}]}, {"text": "The methods involving test set translation (DA-TR and GLOSSES-DA) consistently yield the best cross-lingual accuracies for each model, with a relative error reduction of 45%, 52% and 59% respectively for DA-TR and scores comparable to the full English-Swedish ones.", "labels": [], "entities": [{"text": "test set translation", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.77511994043986}, {"text": "GLOSSES-DA", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.8643773198127747}, {"text": "error reduction", "start_pos": 151, "end_pos": 166, "type": "METRIC", "confidence": 0.8726429045200348}]}, {"text": "reports those AERs along the learning curves of English-Swedish models for increasing data sizes.", "labels": [], "entities": [{"text": "AERs", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9867144227027893}]}, {"text": "It shows that the DA-TR method yields alignment quality comparable to unsupervised learning on 0.1M to 0.5M sentence pairs.", "labels": [], "entities": []}, {"text": "The PoS accuracy measures are reported in Table 3.", "labels": [], "entities": [{"text": "PoS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8522076606750488}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.8709658980369568}]}, {"text": "They show a clear correlation of the most effective (in AER) transfer methods with high PoS accuracies.", "labels": [], "entities": [{"text": "AER) transfer", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.6813452243804932}, {"text": "PoS accuracies", "start_pos": 88, "end_pos": 102, "type": "METRIC", "confidence": 0.8258906602859497}]}, {"text": "However, this measure does not allow to clearly rank the top few models (CAT-SV, TR-DA, DA-TR and GLOSSES-DA).", "labels": [], "entities": [{"text": "GLOSSES-DA", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.939235508441925}]}, {"text": "Notably, here CAT-DA and DA-TR respectively outperform BASELINE and CAT-SV.", "labels": [], "entities": [{"text": "DA-TR", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.7349280118942261}, {"text": "BASELINE", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9679315686225891}]}, {"text": "It maybe that word alignments obtained by transfer are less accurate but focus more on general cross-lingual structures which, in turn, enables a better annotation projection, or that these score differences are simply not significant.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.7261007130146027}]}, {"text": "Coming up with a reliable interpretation of this issue however will require further experiments that are beyond the scope of this paper.", "labels": [], "entities": []}, {"text": "Discussion Unsurprisingly, direct data transfer methods CAT-DA and CAT-EL perform at best in par with the baseline, and often worse.", "labels": [], "entities": []}, {"text": "Indeed, because of mostly disjoint vocabularies, the translation model is globally not improved by the external knowledge, while training the Swedish parameters on few pairs compared to the whole data produces weak parameters that are easily subject to any noise coming from the few common word forms (see IBM 1 results).", "labels": [], "entities": [{"text": "translation", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.9806971549987793}]}, {"text": "This approach could however perform better in a DIALECT scenario, assuming that the\u02dcTthe\u02dc the\u02dcT and T languages have large enough common vocabularies to structure the alignment sets, and that shared word forms are not accidental but actually correspond to common words that can be reliably exploited to transfer knowledge.", "labels": [], "entities": []}, {"text": "The absolute AER gap reduces when adding shared distortion and fertility models, but this does not allow us to conclude on a positive or negative effect of unlexicalized parameter sharing.", "labels": [], "entities": [{"text": "AER gap", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9692372679710388}]}, {"text": "This suggests however the need for experimenting with more selective parameter sharing.", "labels": [], "entities": []}, {"text": "Compared to the Danish ones, we notice that the Greek systems yield smaller but still significant improvements over the baseline, even if (a) Greek and Swedish are only distantly related and have therefore less common structures (b) the impact of untranslated OOV words is higher in Greek because of the change of alphabet: indeed, it is quite unlikely to find identical word forms in Swedish and Greek.", "labels": [], "entities": []}, {"text": "The Greek systems have also been trained on slightly smaller bitexts (1.2M pairs, compared to 1.9M for English-Danish), but this ratio corresponds to a loss of at most 1 AER point in standard learning, which remains negligible in comparison to the cost of choosing Greek over Danish.", "labels": [], "entities": [{"text": "AER", "start_pos": 170, "end_pos": 173, "type": "METRIC", "confidence": 0.999200165271759}]}, {"text": "All in all, using Greek as abridge language is comparable to training with 10,000 English-Swedish pairs, instead of 400,000 when using Danish.", "labels": [], "entities": []}, {"text": "This suggests that our methods can be useful even for unrelated languages in the BRIDGE scenario, even though the return ratio is much lesser than when languages are related: for Greek, 1.2M parallel sentences used crosslinguistically yield the same accuracy as 10,000 parallel sentences of the targeted pair (a ratio of approximately 1%); for Danish the ratio is closer to 20% (1.9M sentences providing the same information as 400,000 en:sv sentence pairs).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 250, "end_pos": 258, "type": "METRIC", "confidence": 0.9971410036087036}]}, {"text": "Finally, the comparison of the TR-DA and DA-TR columns shows that English-Swedish alignment biased by the English-Danish alignment is more accurate when performed in the Danish domain than in the Swedish one.", "labels": [], "entities": []}, {"text": "Intuitively, the noisy application of a valid model performs better than a valid application of a noisy model.: Extrinsic cross-lingual PoS accuracies achieved by the proposed cross-lingual methods with Danish and Greek as bridge languages, compared to the baseline (unsupervised alignment on test data only) and the cat-sv higher bound (addition of large English-Swedish data).", "labels": [], "entities": []}, {"text": "GLOSSES-DA also support that interpretation, and the fact that among the evaluated strategies, the most refined data transfer models outperform the parameter ones shows that even from a very small piece of target data, it is still possible to extract valuable knowledge to guide model adaptation to anew language.", "labels": [], "entities": [{"text": "GLOSSES-DA", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8930979371070862}]}], "tableCaptions": [{"text": " Table 2: AER achieved by the proposed cross-lingual methods with Danish and Greek as bridge languages,  compared to the baseline (unsupervised alignment on test data only) and the cat-sv higher bound (addition  of large English-Swedish data).", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9983922839164734}]}, {"text": " Table 3: Extrinsic cross-lingual PoS accuracies achieved by the proposed cross-lingual methods with Danish  and Greek as bridge languages, compared to the baseline (unsupervised alignment on test data only) and the  cat-sv higher bound (addition of large English-Swedish data).", "labels": [], "entities": []}]}