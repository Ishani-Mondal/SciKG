{"title": [], "abstractContent": [{"text": "This paper provides an overview of the submissions the University of Sheffield for the English-Romanian Translation Task of the ACL 2016 First Conference on Machine Translation (WMT16).", "labels": [], "entities": [{"text": "English-Romanian Translation Task of the ACL 2016 First Conference on Machine Translation (WMT16)", "start_pos": 87, "end_pos": 184, "type": "TASK", "confidence": 0.8328551689783732}]}, {"text": "The submitted translations were produced with a phrase-based system trained using the Moses toolkit, in two variants: (i) n-best rescoring using additional features from Quality Estimation (primary submission), and (ii) a novel weighted ranking optimi-sation approach (secondary submission).", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents the submissions the University of Sheffield for the shared translation task, which is part of the ACL 2016 First Conference on Machine Translation (WMT16).", "labels": [], "entities": [{"text": "shared translation task", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.6708690722783407}, {"text": "ACL 2016 First Conference on Machine Translation (WMT16)", "start_pos": 118, "end_pos": 174, "type": "TASK", "confidence": 0.670154196023941}]}, {"text": "We participated in the English-Romanian language pair.", "labels": [], "entities": []}, {"text": "Our primary submission investigates the use of additional features from Quality Estimation (QE) to better discriminate translation hypothesis within an n-best list produced by a phrase-based MT system built with the Moses toolkit (.", "labels": [], "entities": []}, {"text": "The idea is to expand the n-best list feature set with additional features coming not from the MT system, but from external resources.", "labels": [], "entities": []}, {"text": "Our expectation is that external, potentially richer features could help guide the decoder to produce better quality translations.", "labels": [], "entities": []}, {"text": "In addition to our primary system, we investigate the use of a different optimisation algorithm to tune the parameters of our phrase-based SMT system: the Weighted Ranking Optimisation (WRO) algorithm.", "labels": [], "entities": [{"text": "SMT", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.8293117880821228}]}, {"text": "Derived from the Pairwise Ranking Optimisation (PRO) algorithm), WRO addresses various limitations of PRO, as we discuss in Section 4.", "labels": [], "entities": [{"text": "Pairwise Ranking Optimisation (PRO)", "start_pos": 17, "end_pos": 52, "type": "TASK", "confidence": 0.6306791653235754}]}, {"text": "In the following section we describe the settings of our phrase-based MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.83962082862854}]}, {"text": "The two versions of our phrase-based system are presented in Section 3 and 4, respectively.", "labels": [], "entities": []}, {"text": "We report our results on the newstest2016 test set in Section 5.", "labels": [], "entities": [{"text": "newstest2016 test set in Section 5", "start_pos": 29, "end_pos": 63, "type": "DATASET", "confidence": 0.9114000797271729}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of the available data for  the English-Romanian Machine Translation Task  (constrained submission). For our language mod- elling we only used 93% and 13% of the News  Commentary and the Common Crawl corpus, re- spectively, after data selection.", "labels": [], "entities": [{"text": "English-Romanian Machine Translation Task", "start_pos": 52, "end_pos": 93, "type": "TASK", "confidence": 0.6647593080997467}, {"text": "Common Crawl corpus", "start_pos": 207, "end_pos": 226, "type": "DATASET", "confidence": 0.8343243598937988}]}, {"text": " Table 2: BLEU, BLEU-Cased and Translation Er- ror Rate (TER) scores on newstest2016 of our  phrase-based SMT submission with and without  the use of n-best rescoring. The third line shows  the upper bound of our system with the n-best en- tries scored and sorted against the reference trans- lations using Meteor. The improvement in BLEU  for our n-best rescoring over the baseline MERT  is statistically significant with p \u2264 0.05.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991661310195923}, {"text": "BLEU-Cased and Translation Er- ror Rate (TER)", "start_pos": 16, "end_pos": 61, "type": "METRIC", "confidence": 0.8673029512166976}, {"text": "newstest2016", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.9717745780944824}, {"text": "SMT submission", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.872559517621994}, {"text": "Meteor", "start_pos": 307, "end_pos": 313, "type": "DATASET", "confidence": 0.963171124458313}, {"text": "BLEU", "start_pos": 334, "end_pos": 338, "type": "METRIC", "confidence": 0.9992702603340149}, {"text": "MERT", "start_pos": 383, "end_pos": 387, "type": "METRIC", "confidence": 0.9651144742965698}]}, {"text": " Table 3: BLEU, BLEU-Cased and Translation Er- ror Rate (TER) scores of our phrase-based SMT  submission on newstest2016 and tuned either with  WRO or PRO. In the first row, we only used  newsdev 2 as dev set, while in the second row  we concatenated all the three dev sets together.  The * indicates that the observed improvement of  WRO over PRO are statistically significant with  p \u2264 0.05.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988014698028564}, {"text": "BLEU-Cased and Translation Er- ror Rate (TER)", "start_pos": 16, "end_pos": 61, "type": "METRIC", "confidence": 0.8750934779644013}, {"text": "SMT  submission", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.8723538219928741}]}]}