{"title": [{"text": "Regularizing Relation Representations by First-order Implications", "labels": [], "entities": [{"text": "Regularizing Relation Representations by First-order Implications", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.8211585680643717}]}], "abstractContent": [{"text": "Methods for automated knowledge base construction often rely on trained fixed-length vector representations of relations and entities to predict facts.", "labels": [], "entities": [{"text": "automated knowledge base construction", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.5862855836749077}]}, {"text": "Recent work showed that such representations can be regularized to inject first-order logic formulae.", "labels": [], "entities": []}, {"text": "This enables to incorporate domain-knowledge for improved prediction of facts, especially for uncommon relations.", "labels": [], "entities": []}, {"text": "However, current approaches rely on propositionalization of formulae and thus do not scale to large sets of formulae or knowledge bases with many facts.", "labels": [], "entities": []}, {"text": "Here we propose a method that imposes first-order constraints directly on relation representations, avoiding costly grounding of formulae.", "labels": [], "entities": []}, {"text": "We show that our approach works well for implications between pairs of relations on artificial datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many methods for automated knowledge base (KB) construction rely on learned relation and entity vector representations (.", "labels": [], "entities": [{"text": "automated knowledge base (KB) construction", "start_pos": 17, "end_pos": 59, "type": "TASK", "confidence": 0.6710029925618853}]}, {"text": "Such representations are hard to learn for relations with only few supporting facts in KBs.", "labels": [], "entities": []}, {"text": "Moreover, inference on KBs such as Freebase ( could still benefit from common-sense knowledge contained in ontologies like WordNet or PPDB (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.9628350734710693}]}, {"text": "It is thus desirable to be able to use various kinds of domain or ontological knowledge, for instance in the form of first-order logic formulae, to help knowledge base inference.", "labels": [], "entities": []}, {"text": "Furthermore, such formulae make use of learned representations as well as help to learn better representations.", "labels": [], "entities": []}, {"text": "One way to incorporate logical formulae is to regularize relation and entity-pair representations).", "labels": [], "entities": []}, {"text": "However, in their method first-order formulae need to be grounded for all entity pairs in the KB.", "labels": [], "entities": []}, {"text": "As a result of this propositionalization, the method does not scale to large KBs or many formulae.", "labels": [], "entities": []}, {"text": "Another recent method is based on imposing rules as constraints in an integer linear program (.", "labels": [], "entities": []}, {"text": "This approach suffers from a similar scalability problem, since every rule is imposed for all occurrences of facts in the training data.", "labels": [], "entities": []}, {"text": "To alleviate this computational bottleneck, we propose a method to incorporate first-order implications directly (and only) into relation representations.", "labels": [], "entities": []}, {"text": "The idea is to map relation and entity-pair representations into a well-chosen subspace in which formulae can be expressed as direct regularizers of relation representations without imposing them on entity representations too.", "labels": [], "entities": []}, {"text": "As such, the proposed method is suited for problems with large numbers of rules and facts.", "labels": [], "entities": []}, {"text": "Our approach is based on the concept of orderembeddings, introduced by.", "labels": [], "entities": []}, {"text": "Order-embeddings capture partial orderings, such as textual entailment, directly in vector representations.", "labels": [], "entities": []}, {"text": "This idea can be extended towards relation representations in KBs.", "labels": [], "entities": []}, {"text": "In particular, we show how to construct order-embeddings for capturing implications between relations, such that these implications hold for any possible entity-pair.", "labels": [], "entities": []}, {"text": "The model presented here is also related to.", "labels": [], "entities": []}, {"text": "They demonstrate that textual entailment can be captured by mapping real-valued vectors into (approximate) Boolean valued vectors.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7448571920394897}]}, {"text": "This is achieved by requiring that Boolean vector representations of more specific words or sentences are included in the representation of more general ones.", "labels": [], "entities": []}, {"text": "Furthermore, these representations maybe useful for modeling other types of logical relationships, such as negation or conjunction.", "labels": [], "entities": []}, {"text": "It is our goal to extend the approach towards arbitrary firstorder formulae between relations.", "labels": [], "entities": []}, {"text": "Therefore, as a first step we investigate whether restricting the relation embedding space to approximate Boolean vectors still allows us to reconstruct training facts and imposed implications.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first revisit matrix factorization for KB construction ( \u00a72), before introducing a factorization model that regularizes approximately Boolean relation representations to incorporate first-order implications ( \u00a73).", "labels": [], "entities": [{"text": "KB construction", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7809147238731384}]}, {"text": "Finally, we show empirical results on synthetic knowledge bases.", "labels": [], "entities": []}, {"text": "We explore how enforcing restrictions on representations influences the ability to model the observed data, analyze the learned relation representations qualitatively, and investigate the impact of injecting implications ( \u00a74).", "labels": [], "entities": []}], "datasetContent": [{"text": "To gain insights into the proposed models, we investigate their behavior on small-scale artificial KB inference datasets that we can adapt to different possible scenarios.", "labels": [], "entities": []}, {"text": "Concretely, we sample facts fora predefined number of entities and relations.", "labels": [], "entities": []}, {"text": "Then, we generate implications for sampled pairs of relations and add a fraction of implied facts to the training data and the rest to a test set.", "labels": [], "entities": []}, {"text": "This gives us control over how much an implication is visible for training representations of facts in the KB.", "labels": [], "entities": []}, {"text": "Fact Reconstruction in Non-Negative Space We first investigate whether restricting embedding spaces still allows to reconstruct observed facts.", "labels": [], "entities": [{"text": "Fact Reconstruction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7979815602302551}]}, {"text": "To this end, we consider a dataset with 20 relations and 50 entities, leading to observations for 249 entity pairs.", "labels": [], "entities": []}, {"text": "We calculate the F 1 score for reconstructing all training facts, assuming that all unobserved facts are negative.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.990505039691925}]}, {"text": "shows the result for different combinations of restricting the relation and entity pair embedding spaces.", "labels": [], "entities": []}, {"text": "Every model maps a relation rand entity-pair e into vector space, denoted by v(r), v(e) where \u03c1 and e represent the learned real-valued (i.e., non-restricted) representations before mapping into a non-negative subspace.", "labels": [], "entities": []}, {"text": "The results are shown as a function of the embedding size k.", "labels": [], "entities": []}, {"text": "We found that from the two models that satisfy both the relation and the entity pair restriction, the one with v(e) = exp(e) seems to work best and will be used in the remainder of the experiments.", "labels": [], "entities": []}, {"text": "As expected, imposing restrictions leads to a reduced ability to fit the data exactly and hence requires higher-dimensional vector representations of relations and entity-pairs.", "labels": [], "entities": []}], "tableCaptions": []}