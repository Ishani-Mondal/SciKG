{"title": [{"text": "Learning Word Importance with the Neural Bag-of-Words Model", "labels": [], "entities": [{"text": "Learning Word Importance", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6270399789015452}]}], "abstractContent": [{"text": "The Neural Bag-of-Words (NBOW) model performs classification with an average of the input word vectors and achieves an impressive performance.", "labels": [], "entities": []}, {"text": "While the NBOW model learns word vectors targeted for the classification task it does not explicitly model which words are important forgiven task.", "labels": [], "entities": []}, {"text": "In this paper we propose an improved NBOW model with this ability to learn task specific word importance weights.", "labels": [], "entities": []}, {"text": "The word importance weights are learned by introducing anew weighted sum composition of the word vectors.", "labels": [], "entities": []}, {"text": "With experiments on standard topic and sentiment classification tasks, we show that (a) our proposed model learns meaningful word importance fora given task (b) our model gives best accuracies among the BOW approaches.", "labels": [], "entities": [{"text": "sentiment classification tasks", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.8701136310895284}]}, {"text": "We also show that the learned word importance weights are comparable to tf-idf based word weights when used as features in a BOW SVM classifier.", "labels": [], "entities": [{"text": "BOW SVM classifier", "start_pos": 125, "end_pos": 143, "type": "DATASET", "confidence": 0.7166755795478821}]}], "introductionContent": [{"text": "A Bag-of-Words BOW represents text (a sentence, paragraph or a document) as a vector of word features.", "labels": [], "entities": []}, {"text": "Traditional BOW methods have used word occurrence frequency and variants of TermFrequency-InverseDocumentFrequency (tf-idf) as the word feature (.", "labels": [], "entities": []}, {"text": "Development in neural network and deep learning based language processing has led to the development of more powerful continuous vector representation of words ().", "labels": [], "entities": []}, {"text": "It was shown that these predictive word vector representations capture syntactic and/or semantic characteristics of words and their surrounding context (, and that they outperform the count based word (vector) representations ().", "labels": [], "entities": []}, {"text": "Many approaches in text classification are now driven by models built on word vectors.", "labels": [], "entities": [{"text": "text classification", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7732456624507904}]}, {"text": "Earlier works proposed models which learned word vectors targeted for the classification task).", "labels": [], "entities": []}, {"text": "In more recent works, text classification is performed with compositional representations learned with neural networks or by training the network specifically for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8824483752250671}, {"text": "text classification", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.7711396515369415}]}, {"text": "One such network is the Neural Bag-of-Words (NBOW) model).", "labels": [], "entities": []}, {"text": "The NBOW model takes an average of the word vectors in the input text and performs classification with a logistic regression layer.", "labels": [], "entities": [{"text": "classification", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.960843563079834}]}, {"text": "Essentially the NBOW model is a fully connected feed forward network with BOW input.", "labels": [], "entities": [{"text": "BOW", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9721195101737976}]}, {"text": "The averaging operation can be attributed to the absence of non-linearity at the hidden layer and the BOW inputs where words are set to 1 (or number of occurrences of that word) and 0.", "labels": [], "entities": []}, {"text": "While the NBOW model learns word vectors targeted for the classification task, it does not explicitly model which words are important forgiven task.", "labels": [], "entities": []}, {"text": "In this paper, we propose an improved NBOW model which learns these task specific word importance weights.", "labels": [], "entities": []}, {"text": "We replace the average with a weighted sum, where the weights applied to each word (vector) are learned during the training of the model.", "labels": [], "entities": []}, {"text": "With experiments on sentiment and topic classification tasks, we will show that our proposed model learns meaningful word importance weights and it can perform better than the NBOW model.", "labels": [], "entities": [{"text": "sentiment and topic classification tasks", "start_pos": 20, "end_pos": 60, "type": "TASK", "confidence": 0.7311402261257172}]}, {"text": "The rest of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "First in Section 2 we discuss about the related works.", "labels": [], "entities": []}, {"text": "In Section 3 we briefly introduce the NBOW model and present our proposed model, termed as the Neural Bag-of-WeightedWords (NBOW2) model, in Section 4.", "labels": [], "entities": []}, {"text": "In Section we give details about the experiment setup and the tasks used in our experiments.", "labels": [], "entities": []}, {"text": "Section 6 presents a discussion on the word importance weights learned and the classification performance achieved by the proposed NBOW2 model, followed by the conclusion in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "To analyse the working and performance of our proposed NBOW2 model, we consider two common tasks: (a) binary sentiment classification on IMDB (Maas et al., 2011) and Rotten Tomatoes movie review dataset (Pang and) (b) topic classification of 20 Newsgroup dataset.", "labels": [], "entities": [{"text": "binary sentiment classification", "start_pos": 102, "end_pos": 133, "type": "TASK", "confidence": 0.6558506389458975}, {"text": "Rotten Tomatoes movie review dataset", "start_pos": 166, "end_pos": 202, "type": "DATASET", "confidence": 0.8515401482582092}, {"text": "Pang", "start_pos": 204, "end_pos": 208, "type": "DATASET", "confidence": 0.7113437056541443}, {"text": "topic classification", "start_pos": 218, "end_pos": 238, "type": "TASK", "confidence": 0.7456608414649963}, {"text": "20 Newsgroup dataset", "start_pos": 242, "end_pos": 262, "type": "DATASET", "confidence": 0.6595405240853628}]}, {"text": "We make available the source code used in our experiments 1 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification accuracy obtained for the  IMDB and Rotten Tomatoes (RT) movie reviews  sentiment classification task by training an SVM  classifier on different word weights as features.  (For IMDB 0.1% corresponds to 25 test docu- ments. For RT 1% is about 10 test sentences.)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9585953950881958}, {"text": "RT) movie reviews  sentiment classification", "start_pos": 78, "end_pos": 121, "type": "TASK", "confidence": 0.8443922599156698}]}, {"text": " Table 2:  IMDB and Rotten Tomatoes (RT)  movie reviews binary classification accuracy. First  group lists BOW methods; including different ini- tialisations of NBOW and NBOW2 (this work).  The next group shows best reported results with  bi-gram BOW and CNN methods, followed by  LSTM RNN. Best method in each group is shown  in bold. (For IMDB 0.1% corresponds to 25 test  documents. For RT 1% is about 10 test sentences.)", "labels": [], "entities": [{"text": "Rotten Tomatoes (RT)  movie", "start_pos": 20, "end_pos": 47, "type": "DATASET", "confidence": 0.8686493237813314}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9323330521583557}, {"text": "BOW", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9293873906135559}, {"text": "LSTM RNN", "start_pos": 281, "end_pos": 289, "type": "DATASET", "confidence": 0.6566987335681915}]}, {"text": " Table 3: 20 Newsgroup topic classification ac- curacy. First group lists BOW methods; includ- ing different initialisations of NBOW (Iyyer et al.,  2015) and NBOW2 (this work). The second group  shows best reported results with LSTM RNN. Best  method in each group is shown in bold. (0.2% cor- responds to about 15 test set documents.)", "labels": [], "entities": [{"text": "BOW", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.898106575012207}, {"text": "NBOW", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.9294142723083496}, {"text": "NBOW2", "start_pos": 159, "end_pos": 164, "type": "DATASET", "confidence": 0.9080973267555237}, {"text": "LSTM RNN", "start_pos": 229, "end_pos": 237, "type": "DATASET", "confidence": 0.7536798417568207}]}]}