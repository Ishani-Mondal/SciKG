{"title": [{"text": "Exploring the steps of Verb Phrase Ellipsis", "labels": [], "entities": []}], "abstractContent": [{"text": "Verb Phrase Ellipsis is a well-studied topic in theoretical linguistics but has received little attention as a computational problem.", "labels": [], "entities": [{"text": "Verb Phrase Ellipsis", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7403906087080637}]}, {"text": "Here we propose a decomposition of the overall resolution problem into three tasks-target detection, antecedent head resolution, and antecedent boundary detection-and implement a number of computational approaches for each one.", "labels": [], "entities": [{"text": "antecedent head resolution", "start_pos": 101, "end_pos": 127, "type": "TASK", "confidence": 0.6027173300584158}, {"text": "antecedent boundary detection-and", "start_pos": 133, "end_pos": 166, "type": "TASK", "confidence": 0.6556493639945984}]}, {"text": "We also explore the relationships among these tasks by attempting joint learning over different combinations.", "labels": [], "entities": []}, {"text": "Our new decomposition of the problem yields significantly improved performance on publicly available datasets, including a newly contributed one.", "labels": [], "entities": []}], "introductionContent": [{"text": "Verb Phrase Ellipsis (VPE) is the anaphoric process where a verbal constituent is partially or totally unexpressed, but can be resolved through an antecedent in the context, as in the following examples: (1) His wife also [antecedent works for the paper], as did his father.", "labels": [], "entities": [{"text": "Verb Phrase Ellipsis (VPE)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8359562903642654}]}, {"text": "(2) In particular, Mr. Coxon says, businesses are [antecedent paying out a smaller percentage of their profits and cash flow in the form of dividends] than they have historically.", "labels": [], "entities": [{"text": "Coxon", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.9227458834648132}]}, {"text": "In example 1, alight verb did is used to represent the verb phrase works for the paper; example 2 shows a much longer antecedent phrase, which in addition differs intense from the elided one.", "labels": [], "entities": []}, {"text": "Following, we refer to the full verb expression as the \"antecedent\", and to the anaphor as the \"target\".", "labels": [], "entities": []}, {"text": "VPE resolution is necessary for deeper Natural Language Understanding, and can be beneficial for instance in dialogue systems or Information Extraction applications.", "labels": [], "entities": [{"text": "VPE resolution", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9559007287025452}, {"text": "Natural Language Understanding", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6623243689537048}, {"text": "Information Extraction", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.7804560363292694}]}, {"text": "Computationally, VPE resolution can be modeled as a pipeline process: first detect the VPE targets, then identify their antecedents.", "labels": [], "entities": [{"text": "VPE resolution", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9916739165782928}]}, {"text": "Prior work on this topic) has used this pipeline approach but without analysis of the interaction of the different steps.", "labels": [], "entities": []}, {"text": "In this paper, we analyze the steps needed to resolve VPE.", "labels": [], "entities": [{"text": "VPE", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.8025349974632263}]}, {"text": "We preserve the target identification task, but propose a decomposition of the antecedent selection step in two subtasks.", "labels": [], "entities": [{"text": "target identification task", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.7913396954536438}]}, {"text": "We use learning-based models to address each task separately, and also explore the combination of contiguous steps.", "labels": [], "entities": []}, {"text": "Although the features used in our system are relatively simple, our models yield state-of-the-art results on the overall task.", "labels": [], "entities": []}, {"text": "We also observe a small performance improvement from our decomposition modeling of the tasks.", "labels": [], "entities": []}, {"text": "There are only a few small datasets that include manual VPE annotations.", "labels": [], "entities": [{"text": "VPE annotations", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.8558108806610107}]}, {"text": "While Bos and Spenader (2011) provide publicly available VPE annotations for Wall Street Journal (WSJ) news documents, the annotations created by include a more diverse set of genres (e.g., articles and plays) from the British National Corpus (BNC).", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) news documents", "start_pos": 77, "end_pos": 117, "type": "DATASET", "confidence": 0.8787341937422752}, {"text": "British National Corpus (BNC)", "start_pos": 219, "end_pos": 248, "type": "DATASET", "confidence": 0.970138152440389}]}, {"text": "We semi-automatically transform these latter annotations into the same format used by the former.", "labels": [], "entities": []}, {"text": "The unified format allows better benchmarking and will facilitate more meaningful comparisons in the future.", "labels": [], "entities": []}, {"text": "We evaluate our methods on both datasets, making our results directly comparable to those published by 32 Nielsen (2005).", "labels": [], "entities": [{"text": "32 Nielsen (2005)", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.9510374665260315}]}], "datasetContent": [{"text": "We conduct our experiments on two datasets (see for corpus counts).", "labels": [], "entities": []}, {"text": "The first one is the corpus of, which provides VPE annotation on the WSJ section of the Penn Treebank.", "labels": [], "entities": [{"text": "WSJ section of the Penn Treebank", "start_pos": 69, "end_pos": 101, "type": "DATASET", "confidence": 0.9349238872528076}]}, {"text": "propose a train-test split that we follow . To facilitate more meaningful comparison, we converted the sections of the British National Corpus annotated by into the format used by, and manually fixed conversion errors introduced during the process 6 (Our version of the dataset is publicly available for research 7 .) We use a train-test split similar to Nielsen 5 Section 20 to 24 are used as test data.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 119, "end_pos": 142, "type": "DATASET", "confidence": 0.9431696534156799}]}, {"text": "We also found 3 annotation instances that could be deemed errors, but decided to preserve the annotations as they were.", "labels": [], "entities": []}, {"text": "Whether the lemmas of the ith word before the antecedent and i \u2212 1th word before the target match respectively (for i \u2208 {1, 2, 3}, with the 0th word of the target being the target itself) H&B Semantic Whether the subjects of the antecedent and the target are coreferent H  We evaluate and compare our models following the metrics used by.", "labels": [], "entities": []}, {"text": "VPE target detection is a per-word binary classification problem, which can be evaluated using the conventional precision (Prec), recall (Rec) and F1 scores.", "labels": [], "entities": [{"text": "VPE target detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.849997858206431}, {"text": "precision (Prec)", "start_pos": 112, "end_pos": 128, "type": "METRIC", "confidence": 0.8814658373594284}, {"text": "recall (Rec)", "start_pos": 130, "end_pos": 142, "type": "METRIC", "confidence": 0.9518470019102097}, {"text": "F1", "start_pos": 147, "end_pos": 149, "type": "METRIC", "confidence": 0.9906094670295715}]}, {"text": "Bos and Spenader (2011) propose a token-based evaluation metric for antecedent selection.", "labels": [], "entities": []}, {"text": "The antecedent scores are computed over the correctly identified tokens per antecedent: precision is the number of correctly identified tokens divided by the number of predicted tokens, and recall is the number of correctly identified tokens divided by the number of gold standard tokens.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9993288516998291}, {"text": "recall", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.9992920160293579}]}, {"text": "Averaged scores refer to a \"macro\"-average overall antecedents.", "labels": [], "entities": []}, {"text": "Finally, in order to asses the performance of antecedent head resolution, we compute precision, recall and F1 where credit is given if the proposed head is included inside the golden antecedent boundaries.", "labels": [], "entities": [{"text": "antecedent head resolution", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6721833745638529}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9995554089546204}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9996743202209473}, {"text": "F1", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9997363686561584}]}, {"text": "The trends we observed with gold targets are preserved: approaches using the Rank H maintain an advantage over Log H , but the improvement of Log B over Rank B for boundary determination is diminished with non-gold heads.", "labels": [], "entities": [{"text": "boundary determination", "start_pos": 164, "end_pos": 186, "type": "TASK", "confidence": 0.6502118855714798}]}, {"text": "Also, the 3-step approaches seem to perform slightly better than the 2-step ones.", "labels": [], "entities": []}, {"text": "Together with the fact that the smaller problems are easier to train, this appears to validate our decomposition choice.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results for Target Detection", "labels": [], "entities": [{"text": "Target Detection", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.8311377763748169}]}, {"text": " Table 4: Results for Antecedent Head Resolution", "labels": [], "entities": [{"text": "Antecedent Head Resolution", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.8018663823604584}]}, {"text": " Table 5: Soft results for Antecedent Boundary Determination", "labels": [], "entities": [{"text": "Antecedent Boundary Determination", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.7106444040934244}]}, {"text": " Table 6: Soft results for Antecedent Boundary Determination with non-gold heads", "labels": [], "entities": [{"text": "Antecedent Boundary Determination", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.776850958665212}]}, {"text": " Table 7: Soft end-to-end results", "labels": [], "entities": []}]}