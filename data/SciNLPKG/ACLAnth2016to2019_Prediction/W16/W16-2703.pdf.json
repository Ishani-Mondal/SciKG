{"title": [{"text": "Evaluating and Combining Named Entity Recognition Systems", "labels": [], "entities": [{"text": "Combining Named Entity Recognition", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.5724999830126762}]}], "abstractContent": [{"text": "Name entity recognition (NER) is an important subtask in natural language processing.", "labels": [], "entities": [{"text": "Name entity recognition (NER)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8726001878579458}]}, {"text": "Various NER systems have been developed in the last decade.", "labels": [], "entities": [{"text": "NER", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.8008849024772644}]}, {"text": "They may target for different domains, employ different methodologies, work on different languages, detect different types of entities, and support different inputs and output formats.", "labels": [], "entities": []}, {"text": "These conditions make it difficult fora user to select the right NER tools fora specific task.", "labels": [], "entities": []}, {"text": "Motivated by the need of NER tools in our research work, we select several publicly available and well-established NER tools to validate their outputs against both Wikipedia gold standard corpus and a small set of manually annotated documents.", "labels": [], "entities": [{"text": "Wikipedia gold standard corpus", "start_pos": 164, "end_pos": 194, "type": "DATASET", "confidence": 0.9074337035417557}]}, {"text": "All the evaluations show consistent results on the selected tools.", "labels": [], "entities": []}, {"text": "Finally, we constructed a hybrid NER tool by combining the best performing tools for the domains of our interest.", "labels": [], "entities": [{"text": "NER", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.971860945224762}]}], "introductionContent": [{"text": "Name entity recognition is an important subtask in natural language processing (NLP).", "labels": [], "entities": [{"text": "Name entity recognition", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8759583632151285}, {"text": "natural language processing (NLP)", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.7749191025892893}]}, {"text": "The results of recognition and classification of proper nouns in a text document are widely used in information retrieval, information extraction, machine translation, question answering and automatic summarization (.", "labels": [], "entities": [{"text": "recognition and classification of proper nouns in a text document", "start_pos": 15, "end_pos": 80, "type": "TASK", "confidence": 0.8356632471084595}, {"text": "information retrieval", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.7950014173984528}, {"text": "information extraction", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.846203476190567}, {"text": "machine translation", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.7768644392490387}, {"text": "question answering", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.9106436967849731}, {"text": "automatic summarization", "start_pos": 191, "end_pos": 214, "type": "TASK", "confidence": 0.5821043848991394}]}, {"text": "Depending on the requirements of specific tasks, the types to be recognized can be person, location, organization and date, which are mostly used in newswire (, or other commonly used measures (percent, weight, money), email address, etc.", "labels": [], "entities": []}, {"text": "It can also be domain specific entity types such as medical drug names, disease symptoms and treatment, etc.", "labels": [], "entities": []}, {"text": "(Asma Ben Abacha and Pierre).", "labels": [], "entities": [{"text": "Pierre", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8810954689979553}]}, {"text": "Name entity recognition is a challenging task which needs massive prior knowledge sources for better performance).", "labels": [], "entities": [{"text": "Name entity recognition", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8671552538871765}]}, {"text": "Many researches works have been conducted in different domains with various approaches.", "labels": [], "entities": []}, {"text": "Early studies focus on heuristic and handcrafted rules.", "labels": [], "entities": []}, {"text": "By defining the formation patterns and context over lexical-syntactic features and term constituents, entities are recognized by matching the patterns against the input documents (Rau,).", "labels": [], "entities": []}, {"text": "Rule-based system may achieve high degree of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9991475343704224}]}, {"text": "However, the development process is time-consuming and porting these developed rules from one domain to another is a major challenge.", "labels": [], "entities": []}, {"text": "Recent research in NER tends to use machine learning approaches.", "labels": [], "entities": [{"text": "NER", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9765676856040955}]}, {"text": "The learning methods include various supervised, semi-supervised and unsupervised learning.", "labels": [], "entities": []}, {"text": "The supervised learning tends to be the dominant technique for named entity recognition and classification.", "labels": [], "entities": [{"text": "named entity recognition and classification", "start_pos": 63, "end_pos": 106, "type": "TASK", "confidence": 0.6619024693965911}]}, {"text": "However, supervised machine learning methods require large amount of annotated documents for model training and its performance typically depends on the availability of sufficient high quality training data in the domain of interest.", "labels": [], "entities": []}, {"text": "There are some systems which use hybrid methods to combine different rule-based and/or machine learning systems for improved performance over individual approaches.", "labels": [], "entities": []}, {"text": "Hybrid systems make the best use of the good features of different systems or methods to achieve the best overall performance.", "labels": [], "entities": []}, {"text": "In this paper, we first select several publicly available and well-established NER tools in section 2.", "labels": [], "entities": []}, {"text": "Then all the tools are validated in section 3 with CONLL 2003 metrics and a customized partial matching measurement.", "labels": [], "entities": [{"text": "CONLL 2003 metrics", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.8996152679125468}]}, {"text": "Then we constructed a hybrid NER system based on the best performed NER tools in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The overall system structure of the integrated evaluation system is shown in.", "labels": [], "entities": []}, {"text": "In the process of evaluation, an annotated (gold-standard) input document must be provided.", "labels": [], "entities": []}, {"text": "Currently, the supported format is IOB (short for Inside, Outside, Beginning) (.", "labels": [], "entities": [{"text": "IOB", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.8894258141517639}]}, {"text": "In this scheme, every line in the file represents one token with two fields: the word itself and its named entity type.", "labels": [], "entities": []}, {"text": "Empty lines denote sentence boundaries.", "labels": [], "entities": []}, {"text": "Following is an example of the representation: The prefix \"I-\" in the tag means that the tag is inside a chunk.", "labels": [], "entities": []}, {"text": "While the prefix \"B-\" indicates that the tag is the beginning of a chunk and is only used when a tag is followed by a tag of the same type without \"O\" tag between them.", "labels": [], "entities": [{"text": "B-\"", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9231265485286713}]}, {"text": "The \"O\" tag just means it is out of the chunk.", "labels": [], "entities": [{"text": "O", "start_pos": 5, "end_pos": 6, "type": "METRIC", "confidence": 0.955055296421051}]}, {"text": "This IOB chunk representation is much easier for manual annotation than inside XML annotation scheme.", "labels": [], "entities": []}, {"text": "An Analyzer module is used to extract the source document as well as all chunks and their types from the annotated file.", "labels": [], "entities": []}, {"text": "Every chunk is represented in the format of a three-element tuple: (chunk, type, start_position), where the start_position is the sequence position (character index) of the chunk in the source document.", "labels": [], "entities": []}, {"text": "This tuple representation contains all the necessary information for the validation of a chunk, including its boundary.", "labels": [], "entities": [{"text": "validation", "start_pos": 73, "end_pos": 83, "type": "TASK", "confidence": 0.9628693461418152}]}, {"text": "The Dispatcher module will pass the source document to all NER tools.", "labels": [], "entities": []}, {"text": "All the tools will first tokenize the sentences, analyze these sentences and then create their respective list of tuples dynamically.", "labels": [], "entities": []}, {"text": "Every output list from the NER tools will be compared against the standard list generated from the annotated file.", "labels": [], "entities": []}, {"text": "The comparison results will be used to calculate true positives (TP), false positives (FP) and false negatives (FN).", "labels": [], "entities": [{"text": "true positives (TP), false positives (FP)", "start_pos": 49, "end_pos": 90, "type": "METRIC", "confidence": 0.7404976730996912}, {"text": "false negatives (FN)", "start_pos": 95, "end_pos": 115, "type": "METRIC", "confidence": 0.8362093567848206}]}, {"text": "Then precision, recall and Fmeasure can be further calculated for evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9997294545173645}, {"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9997218251228333}, {"text": "Fmeasure", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9996517896652222}]}, {"text": "All the calculation results can be directly exported to excel file for easy comparison.", "labels": [], "entities": []}, {"text": "With the methodology defined in section 2, it is ready to evaluate all the selected tools with any data file annotated in the IOB format.", "labels": [], "entities": []}, {"text": "Since all the selected NER tools are able to classify the three entity types: PERSON, LOCATION and ORGANIZATION, the evaluation corpus must contain at least the above three entity types.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9358187317848206}, {"text": "LOCATION", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9353509545326233}, {"text": "ORGANIZATION", "start_pos": 99, "end_pos": 111, "type": "METRIC", "confidence": 0.9846096038818359}]}, {"text": "The format is better to be in the supported IOB chunk representation.", "labels": [], "entities": []}, {"text": "We found that WikiGold 2 meets the above requirements.", "labels": [], "entities": []}, {"text": "WikiGold () is an annotated corpus over a small sample of Wikipedia articles in CoNLL format (IOB  There are different evaluation metrics for the evaluation of NER systems (.", "labels": [], "entities": [{"text": "WikiGold", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8997091054916382}]}, {"text": "The evaluation is basically to check the tool's ability on finding the boundaries of names and their correct types.", "labels": [], "entities": []}, {"text": "Most evaluation systems require exact match on both boundary and entity type.", "labels": [], "entities": []}, {"text": "The share task for CONLL 2003) is one of the examples for the exact matching.", "labels": [], "entities": [{"text": "CONLL 2003", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.9220342338085175}]}, {"text": "However, in some cases, the exact boundary detection is not so important as long as the major part of the name has been identified.", "labels": [], "entities": [{"text": "exact boundary detection", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.6616124908129374}]}, {"text": "For instance, \"The United Nations\" and \"United Nations\", \"in November 2015\" and \"November 2015\", they are almost the same except the minor differences in the definite article and preposition.", "labels": [], "entities": [{"text": "United Nations", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.7652129232883453}, {"text": "United Nations\"", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9252217213312784}]}, {"text": "The metrics used for evaluation in the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) adopted more loose matching conditions which allow for partial credit when partial span or wrong type detection happened.", "labels": [], "entities": [{"text": "Message Understanding Conference (MUC)", "start_pos": 39, "end_pos": 77, "type": "TASK", "confidence": 0.8376355767250061}]}, {"text": "The credit was given to any correct entity type detected regardless of its boundary as long as there is an overlap, as well as the correct boundary identified regardless of the type.", "labels": [], "entities": []}, {"text": "Here we score NER systems based on the following two metrics: a) Exact matching for both boundary and type (similar to CONLL) which measures a system's capability for accurate named entity detection.", "labels": [], "entities": [{"text": "NER", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9512097239494324}, {"text": "accurate named entity detection", "start_pos": 167, "end_pos": 198, "type": "TASK", "confidence": 0.5843663215637207}]}, {"text": "b) Partial matching for boundary is also counted, only when the detected type is correct.", "labels": [], "entities": []}, {"text": "This measurement will mitigate the failures of exact matching when the boundary differences are caused by some unimportant words in the names such as the articles and prepositions.", "labels": [], "entities": [{"text": "exact matching", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.5835500955581665}]}, {"text": "Based on the above two scoring protocols, the measuring system counts TP, FP and FN for every NER toolkit.", "labels": [], "entities": [{"text": "TP", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9898457527160645}, {"text": "FP", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9814349412918091}, {"text": "FN", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9720902442932129}]}, {"text": "Then typical precision: p = TP / (TP + FP) and recall: R = TP / (TP + FN) are further calculated to check the NER system's type I (false alarm) and type II (miss) errors respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9992648959159851}, {"text": "TP", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9636420607566833}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9996693134307861}, {"text": "TP / (TP + FN)", "start_pos": 59, "end_pos": 73, "type": "METRIC", "confidence": 0.6969275219099862}, {"text": "type II (miss) errors", "start_pos": 148, "end_pos": 169, "type": "METRIC", "confidence": 0.5993416508038839}]}, {"text": "shows the results of the four selected NER systems on the WikiGold data set.", "labels": [], "entities": [{"text": "WikiGold data set", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.9857746561368307}]}, {"text": "In the table, Precision (P), Recall (R) and F1 measure (F) are calculated against every entity type and a final overall score is also given for all the measurements.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 14, "end_pos": 27, "type": "METRIC", "confidence": 0.952357605099678}, {"text": "Recall (R)", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9594169557094574}, {"text": "F1 measure (F)", "start_pos": 44, "end_pos": 58, "type": "METRIC", "confidence": 0.9853654861450195}]}, {"text": "Similarly, the Precision (PP), Recall (PR) and F1 measure (PF) for partial boundary matching as described in section 3.2 are also calculated.", "labels": [], "entities": [{"text": "Precision (PP)", "start_pos": 15, "end_pos": 29, "type": "METRIC", "confidence": 0.9271136969327927}, {"text": "Recall (PR)", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9658809453248978}, {"text": "F1 measure (PF)", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.9850450754165649}]}, {"text": "From the results depicted in we can derive the following conclusions: a) Loose boundary matching shows better results than the exact matching for every entity type across all the NER tools.", "labels": [], "entities": [{"text": "Loose boundary matching", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.60313152273496}]}, {"text": "That means there exist quite a number of cases where NER systems detected the right entity types but the boundaries are not exactly matched.", "labels": [], "entities": []}, {"text": "b) ORGANIZATION appears to be the entity type which is more difficult for detecting for all the NER tools.", "labels": [], "entities": [{"text": "ORGANIZATION", "start_pos": 3, "end_pos": 15, "type": "METRIC", "confidence": 0.9901751279830933}]}, {"text": "This is proved by its lower scores compared with the PERSON and LOCATION types.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.8661772608757019}, {"text": "LOCATION", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.8188799619674683}]}, {"text": "c) Stanford NER and spaCy generally show better performance in this data set for both exact matching and partial matching.", "labels": [], "entities": [{"text": "Stanford NER", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.8843404054641724}, {"text": "spaCy", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.8459538817405701}]}, {"text": "In order to evaluate the performance of the hybrid system, we manually annotated twenty two web pages.", "labels": [], "entities": []}, {"text": "All the web pages are from Singapore National Library Board eResources . Half of the web pages are about Singapore history, another half are from Infopedia pages.", "labels": [], "entities": [{"text": "Singapore National Library Board eResources", "start_pos": 27, "end_pos": 70, "type": "DATASET", "confidence": 0.9456048727035522}]}, {"text": "We first use Stanford tool to tokenize all the documents and save them into different files.", "labels": [], "entities": [{"text": "Stanford tool", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.9265588223934174}]}, {"text": "Every token is in anew line with a space line to separate the sentence.", "labels": [], "entities": []}, {"text": "Then every token is manually annotated in IOB format.", "labels": [], "entities": []}, {"text": "shows the statistics of the two manually annotated datasets.", "labels": [], "entities": []}, {"text": "NER for History data for these three entity types, but slight worse for Infopedia data.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6703467965126038}, {"text": "Infopedia data", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.8792625069618225}]}, {"text": "d) In general, the hybrid system has better overall performance over both Stanford NER and spaCy.", "labels": [], "entities": [{"text": "Stanford NER", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.9137751162052155}, {"text": "spaCy", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.8842799663543701}]}, {"text": "This is especially true for History testing data.", "labels": [], "entities": [{"text": "History testing data", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.7280624111493429}]}, {"text": "However, most of the advantages are contributed by its better DATE entity recognition.", "labels": [], "entities": [{"text": "DATE entity recognition", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.8323158621788025}]}, {"text": "e) Overall, all the NER tools, including the hybrid system, showed better performance on History data than Infopedia data.", "labels": [], "entities": [{"text": "History data", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.9088035225868225}]}, {"text": "This is mostly caused by some noise present in the Infopedia documents, for instance, html codes: &rsquo;, un-delimited words \"COMPASS.FamilyWife\" in the document due to the data extraction from the html pages.", "labels": [], "entities": [{"text": "Infopedia documents", "start_pos": 51, "end_pos": 70, "type": "DATASET", "confidence": 0.8999865651130676}]}], "tableCaptions": [{"text": " Table 3. Entity statistics on History and  Infopedia testing datasets", "labels": [], "entities": [{"text": "History and  Infopedia testing datasets", "start_pos": 31, "end_pos": 70, "type": "DATASET", "confidence": 0.7841529011726379}]}]}