{"title": [{"text": "A Simple but Effective Approach to Improve Arabizi-to-English Statistical Machine Translation", "labels": [], "entities": [{"text": "Improve Arabizi-to-English Statistical Machine Translation", "start_pos": 35, "end_pos": 93, "type": "TASK", "confidence": 0.6179325222969055}]}], "abstractContent": [{"text": "A major challenge for statistical machine translation (SMT) of Arabic-to-English user-generated text is the prevalence of text written in Arabizi, or Romanized Arabic.", "labels": [], "entities": [{"text": "statistical machine translation (SMT) of Arabic-to-English user-generated text", "start_pos": 22, "end_pos": 100, "type": "TASK", "confidence": 0.8432257920503616}]}, {"text": "When facing such texts, a translation system trained on conventional Arabic-English data will suffer from extremely low model coverage.", "labels": [], "entities": []}, {"text": "In addition, Arabizi is not regulated by any official standardization and therefore highly ambiguous, which prevents rule-based approaches from achieving good translation results.", "labels": [], "entities": []}, {"text": "In this paper, we improve Arabizi-to-English machine translation by presenting a simple but effective Arabizi-to-Arabic transliteration pipeline that does not require knowledge by experts or native Arabic speakers.", "labels": [], "entities": [{"text": "Arabizi-to-English machine translation", "start_pos": 26, "end_pos": 64, "type": "TASK", "confidence": 0.6095139185587565}]}, {"text": "We incorporate this pipeline into a phrase-based SMT system, and show that translation quality after automatically transliterating Arabizi to Arabic yields results that are comparable to those achieved after human transliteration.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.871016263961792}]}], "introductionContent": [{"text": "Almost all current state-of-the-art statistical machine translation (SMT) systems for Arabic-to-English translation are trained on data comprising Modern Standard Arabic (MSA).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.8180393079916636}, {"text": "Arabic-to-English translation", "start_pos": 86, "end_pos": 115, "type": "TASK", "confidence": 0.7331962585449219}, {"text": "Modern Standard Arabic (MSA)", "start_pos": 147, "end_pos": 175, "type": "DATASET", "confidence": 0.8350625336170197}]}, {"text": "MSA is widely used by professional publishers, such as news agencies, government agencies, and non-governmental organisations (NGOs).", "labels": [], "entities": []}, {"text": "On the other hand, in user-generated content, such as weblogs, Internet forums, and short text messages, MSA is substantially less prevalent.", "labels": [], "entities": []}, {"text": "Here one can often encounter dialectal variations resulting in a slightly different vocabulary and morphological constructions.", "labels": [], "entities": []}, {"text": "In addition to dialectal variations, user-generated content often also contains Arabic that is not written in the Arabic script, but in Romanized form, typically referred to as Arabizi.", "labels": [], "entities": []}, {"text": "This is not to be confused with standardized research transliteration schemes such as the Buckwalter encoding for Arabic, or an official phonetic system such as Pinyin for Chinese.", "labels": [], "entities": [{"text": "Buckwalter encoding", "start_pos": 90, "end_pos": 109, "type": "DATASET", "confidence": 0.9300054907798767}]}, {"text": "Instead, Arabizi encountered in user-generated text emerged from practical limitations such as the lack of an Arabic keyboard.", "labels": [], "entities": []}, {"text": "While Arabizi is not regulated by any standardization and many-to-many Arabizi-Arabic character mappings are ubiquitous, certain conventions have emerged.", "labels": [], "entities": []}, {"text": "These conventions mostly rely on reflecting phonetic approximations by using a combination of numbers and Latin letters.", "labels": [], "entities": []}, {"text": "Since Arabizi is mostly guided by pronunciation, it is also very sensitive to dialectal variations, which are more noticeable in spoken than in written Arabic.", "labels": [], "entities": []}, {"text": "Note that some Arabizi representations are also based on orthographic similarities, such as '3' for the Arabic letter . shows an example sentence in Arabizi, along with its MSA transliteration, Buckwalter transliteration and English translation, and illustrates the difference between Arabizi and formal transliteration.", "labels": [], "entities": [{"text": "MSA", "start_pos": 173, "end_pos": 176, "type": "DATASET", "confidence": 0.8620054721832275}, {"text": "Buckwalter", "start_pos": 194, "end_pos": 204, "type": "DATASET", "confidence": 0.9278338551521301}]}, {"text": "Since Arabizi is highly ambiguous and difficult to transliterate with rule-based approaches, there is an extreme scarcity of gold standard transliterated Arabizi, making it a challenging task to develop datadriven statistical approaches involving Arabizi, such as Arabizi-to-English machine translation.", "labels": [], "entities": [{"text": "Arabizi-to-English machine translation", "start_pos": 264, "end_pos": 302, "type": "TASK", "confidence": 0.6036567588647207}]}, {"text": "Moreover, the standard data sets used by the MT research community do not contain Arabizi, meaning that any attempt to translate Arabic in Arabizi writing will suffer from extremely high (close to 100%) outof-vocabulary (OOV) rates.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9643025994300842}, {"text": "outof-vocabulary (OOV) rates", "start_pos": 203, "end_pos": 231, "type": "METRIC", "confidence": 0.9427283525466919}]}, {"text": "In this paper, we use a handful of small resources to build a simple but effective Arabizi-to-Arabic Arabizi (lowercased) la2 laa m7adsh by7eb el ka7k ela pappi :( w howa mesh henaaa transliteration pipeline, which we incorporate into a state-of-the-art phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 267, "end_pos": 270, "type": "TASK", "confidence": 0.8944198489189148}]}, {"text": "Concretely, our contributions are as follows: (i) We present and release an Arabizi-to-Arabic transliteration pipeline that combines character-level mapping with contextual disambiguation of Arabic candidate words.", "labels": [], "entities": []}, {"text": "We improve transliteration candidate selection by incorporating common Arabizi-Arabic word pairs.", "labels": [], "entities": [{"text": "transliteration candidate selection", "start_pos": 11, "end_pos": 46, "type": "TASK", "confidence": 0.8261527021725973}]}, {"text": "We evaluate our end-to-end Arabizi-English translation system using two test sets, and show that translation quality after automatically transliterating Arabizi to Arabic yields results that are comparable to those achieved after human transliteration.", "labels": [], "entities": []}, {"text": "(ii) We collect and release a web-crawled Arabizi-English parallel corpus of approximately 10,000 sentence pairs.", "labels": [], "entities": []}, {"text": "Despite being too small to train a fully data-driven translation system, this corpus is useful for words that cannot be transliterated successfully by our transliteration pipeline.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform our translation experiments using an in-house state-of-the-art phrase-based SMT system similar to).", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9620410203933716}, {"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.8535624742507935}]}, {"text": "The system is trained on the collection of Arabic-English parallel corpora discussed in Section 2, comprising 1.75M lines (52.9M Arabic tokens) of parallel text.", "labels": [], "entities": []}, {"text": "In addition, we use a 5-gram English language model that linearly interpolates different English Gigaword subcorpora with the English side of our bitext.", "labels": [], "entities": []}, {"text": "When no valid Arabic transliteration is found for an Arabizi word, our software component leaves it unchanged.", "labels": [], "entities": []}, {"text": "To increase the chances of handling such cases, we exploit our in-house Arabizi-English corpus of web-crawled user comments (see Section 2), on which we train a separate Arabizi-English system.", "labels": [], "entities": []}, {"text": "Instead of using this system for the actual translation task, which would suffer from very low coverage, we merge the Arabizi-English phrase translation and phrase reordering models to the main Arabic-English models using a fillup technique).", "labels": [], "entities": [{"text": "coverage", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9519438147544861}, {"text": "Arabizi-English phrase translation and phrase reordering", "start_pos": 118, "end_pos": 174, "type": "TASK", "confidence": 0.5698574135700861}]}, {"text": "In this way, a non-transliterated word that is not matched by the main Arabic-English models has still a chance of being translated directly by the Arabizi-English models.", "labels": [], "entities": []}, {"text": "We tokenize all Arabic data-training data as well as transliterated Arabizi-using the MADA toolkit).", "labels": [], "entities": [{"text": "MADA toolkit", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.8615337014198303}]}, {"text": "shows SMT quality measured with case-insensitive BLEU () fora number of transliteration scenarios.", "labels": [], "entities": [{"text": "SMT", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9893943667411804}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.983366847038269}]}, {"text": "First, we see that Arabizi-to-English translation without any preprocessing (top row) results in very poor translation quality.", "labels": [], "entities": [{"text": "Arabizi-to-English translation", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.6530575007200241}]}, {"text": "There is, however, a large difference in BLEU between the two test sets, with test set 2 achieving a surprisingly high score given that almost the entire source text is: BLEU scores of Arabizi-to-English translation experiments using different preprocessing schemes: no transliteration, two variants of our automatic transliteration pipeline, and human transliteration, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9993744492530823}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9989080429077148}]}, {"text": "The latter can be considered as a gold standard, so we also present the results relative to what is achieved by human transliteration.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. We use this resource in Section 3.1 to generate transliteration candidates.", "labels": [], "entities": []}, {"text": " Table 2: Mapping of Arabic letters to Arabizi character sequences. Source: Wikipedia. (http://en.  wikipedia.org/wiki/Arabic_chat_alphabet).", "labels": [], "entities": []}, {"text": " Table 3: Word-level transliteration error rates for two variants of our transliteration pipeline measured on  two held-out test sets.", "labels": [], "entities": []}, {"text": " Table 4: BLEU scores of Arabizi-to-English translation experiments using different preprocessing  schemes: no transliteration, two variants of our automatic transliteration pipeline, and human transliter- ation, respectively. The latter can be considered as a gold standard, so we also present the results relative  to what is achieved by human transliteration.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984405636787415}, {"text": "Arabizi-to-English translation", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.600641593337059}]}]}