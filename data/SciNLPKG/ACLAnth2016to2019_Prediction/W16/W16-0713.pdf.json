{"title": [{"text": "When Annotation Schemes Change Rules Help: A Configurable Approach to Coreference Resolution beyond OntoNotes", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.9773929119110107}]}], "abstractContent": [{"text": "This paper approaches the challenge of adapting coreference resolution to different coref-erence phenomena and mention-border definitions when there is no access to large training data in the desired target scheme.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.9588071405887604}]}, {"text": "We take a configurable, rule-based approach centered on dependency syntax input, which we test by examining coreference types not covered in benchmark corpora such as OntoNotes.", "labels": [], "entities": []}, {"text": "These include cataphora, compound modifier coref-erence, generic anaphors, predicate marka-bles, i-within-i, and metonymy.", "labels": [], "entities": []}, {"text": "We test our system, called xrenner, using different configurations on two very different datasets: Wall Street Journal material from OntoNotes and four types Wiki data from the GUM corpus.", "labels": [], "entities": [{"text": "Wall Street Journal material from OntoNotes", "start_pos": 99, "end_pos": 142, "type": "DATASET", "confidence": 0.9052112003167471}, {"text": "GUM corpus", "start_pos": 177, "end_pos": 187, "type": "DATASET", "confidence": 0.9023911952972412}]}, {"text": "Our system compares favorably with two leading rule based and stochastic approaches in handling the different annotation formats.", "labels": [], "entities": []}], "introductionContent": [{"text": "Previous work) has suggested that a trainable coreference resolution approach can outperform rulebased approaches (e.g.) because of its ability to model similar constraints in a lexicalized way that more closely matches training data.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.8406376242637634}]}, {"text": "However, in many cases the amount of training data required for such approaches is large: if the phenomenon that we wish to include is not annotated in the data, we can only use a trainable system after considerable annotation work to adjust the training set to include it.", "labels": [], "entities": []}, {"text": "Permutations of what to include or exclude and how to model each phenomenon, can compound such problems further.", "labels": [], "entities": []}, {"text": "Rule-based approaches), by contrast, can more easily add new behaviors, but have been described as \"difficult to interpret or modify\".", "labels": [], "entities": []}, {"text": "Although they can achieve results competitive with trainable systems, the hard-wired aspects of rule-based systems are problematic if we wish to adapt to different annotation schemes, languages, and target domains.", "labels": [], "entities": []}, {"text": "The current paper approaches the challenge of different target schemes with a system called xrenner: an externally configurable reference and non-named entity recognizer.", "labels": [], "entities": []}, {"text": "By using a large number of highly configurable mechanisms and rules in easily modifiable text files, with almost no hard-wired languageor domain-specific knowledge, we are able to adapt our system to include or exclude a variety of less standard coreference phenomena, including cataphora, generic indefinite anaphors, compound modifier nominals, predicate markables, clause-nested markables (iwithin-i) and metonymy.", "labels": [], "entities": []}, {"text": "We test our system on two datasets with very different schemes: Wall Street Journal data from OntoNotes (), which does not include the above cases, and a small test corpus, GUM (Zeldes 2016), which captures these phenomena and more.", "labels": [], "entities": [{"text": "Wall Street Journal data from OntoNotes", "start_pos": 64, "end_pos": 103, "type": "DATASET", "confidence": 0.8924370606740316}]}], "datasetContent": [{"text": "We compare our configurable rule based approach Since it is not reasonable to expect systems designed around schemes such as OntoNotes to perform well on GUM data, our main goal is to look at the impact of the scheme on performance for our system and less configurable ones.", "labels": [], "entities": [{"text": "GUM data", "start_pos": 154, "end_pos": 162, "type": "DATASET", "confidence": 0.7978963255882263}]}, {"text": "This is especially interesting considering the fact that there is insufficient training data to address the GUM scheme with a machine learning approach.", "labels": [], "entities": []}, {"text": "We are also interested in how much of a difference the scheme will make, on the assumptions that high precision in particular should still carryover to settings where more annotation density is expected.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9983938336372375}]}, {"text": "None of the systems attempt to resolve bridging, so we will leave the bridging data out of the evaluation: only cases of the GUM coreference labels corresponding to anaphora, lexical coreference and apposition are included.", "labels": [], "entities": []}, {"text": "Although our coreference resolution is rulebased, we nevertheless divide both datasets into training and test data, which means that gazetteer data, including dependency to entity type mappings, as well as 'is-a' data, maybe harvested for our system from the training portions, but not from the test portions.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.8403584063053131}]}, {"text": "Since we do not have gold dependency data to compare to the gold constituent parses in OntoNotes 8 , we evaluate all systems on automatically parsed data using the CoreNLP pipeline for dcoref (including the Stanford Parser) and the Berkeley system's builtin pipeline for the joint Entity Resolution System.", "labels": [], "entities": [{"text": "joint Entity Resolution", "start_pos": 275, "end_pos": 298, "type": "TASK", "confidence": 0.5369775593280792}]}, {"text": "Dependency parses for our system are generated using the Stanford Parser.", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.9128456115722656}]}, {"text": "gives precision and recall for mention detection, while shows coreference resolution performance according to several measures calculated using the official CoNLL scorer (version 8.01, see).", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9995458722114563}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9993910789489746}, {"text": "mention detection", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8449500203132629}, {"text": "coreference resolution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7390855848789215}, {"text": "CoNLL scorer", "start_pos": 157, "end_pos": 169, "type": "DATASET", "confidence": 0.7727302312850952}]}, {"text": "Since dcoref and the Berkeley system only output coreferent mentions (in keeping with the absence of singletons in OntoNotes), mention detection performance is tightly linked to coreference resolution.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7339684367179871}, {"text": "coreference resolution", "start_pos": 178, "end_pos": 200, "type": "TASK", "confidence": 0.883447676897049}]}, {"text": "On both datasets, xrenner has the highest recall, but on GUM it has the lowest precision and on WSJ the second lowest.", "labels": [], "entities": [{"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9996826648712158}, {"text": "GUM", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.7771939635276794}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9986205101013184}, {"text": "WSJ", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.8303187489509583}]}, {"text": "This is likely related to the fact that under the GUM scheme, virtually all nominals (notably common noun compound modifiers) are candidates for coreference, and many are mentioned multiple times: for each rementioned compound, the modifier is likely to be caught as a nested coreferent markable, even if it is non-referential, unless the entire compound is flagged as 'atomic' by lexical resources.", "labels": [], "entities": []}, {"text": "Based on 71 cases in the gold data, our precision against compound modifiers judged as referential and coreferring by GUM annotators, is 61%, and recall is at 66%, which we consider to be a good result.", "labels": [], "entities": [{"text": "gold data", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.8521732985973358}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9993557333946228}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9997624754905701}]}, {"text": "Only very few compound modifiers are found other than by lexical identity, though there are some 'isa' cases, such as the false negative in (12).", "labels": [], "entities": []}, {"text": "Indeed, the most frequent reason fora false positive is identical modifiers not judged by annotators to be referential, as in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Coreference in GUM and WSJ.", "labels": [], "entities": [{"text": "GUM", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.6885218620300293}, {"text": "WSJ", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7269015312194824}]}, {"text": " Table 5: Mention detection in GUM and WSJ.", "labels": [], "entities": [{"text": "Mention detection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8485363125801086}, {"text": "GUM", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.49718183279037476}, {"text": "WSJ", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.577916145324707}]}, {"text": " Table 6: Coreference precision and recall on GUM and WSJ plain text data for three systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9598986506462097}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9992192983627319}, {"text": "WSJ plain text data", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.8274316936731339}]}]}