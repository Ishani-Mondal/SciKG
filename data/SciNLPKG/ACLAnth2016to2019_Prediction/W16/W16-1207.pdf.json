{"title": [{"text": "Learning Translations for Tagged Words: Extending the Translation Lexicon of an ITG for Low Resource Languages", "labels": [], "entities": [{"text": "Learning Translations for Tagged Words", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8351312518119812}, {"text": "Extending the Translation Lexicon", "start_pos": 40, "end_pos": 73, "type": "TASK", "confidence": 0.8029015809297562}]}], "abstractContent": [{"text": "We tackle the challenge of learning part-of-speech classified translations as part of an inversion transduction grammar, by learning translations for English words with known part-of-speech tags, both from existing translation lexica and from parallel corpora.", "labels": [], "entities": []}, {"text": "When translating from a low resource language into English, we can expect to have rich resources for English, such as treebanks, and small amounts of bilingual resources, such as translation lexica and parallel corpora.", "labels": [], "entities": []}, {"text": "We solve the problem of integrating these heterogeneous resources into a single model using stochastic Inversion Transduction Grammars, which we augment with wildcards to handle unknown translations.", "labels": [], "entities": []}], "introductionContent": [{"text": "We introduce an augmentation to Inversion Transduction Grammars, or ITGs (, that allow us to under specify the translation of lexical rules, and defer to observed usage to decide, within the syntactic context of the lexical rule, how the wildcard should be instantiated.", "labels": [], "entities": []}, {"text": "Having specific wildcard rules instead of just instantiating all possible translations for the sentence pair at hand (a) allows us to use it as a back-off: we can limit the use of these spurious rules to the circumstances when we have no other choice, and (b) allows us to explicitly reserve some probability mass for the unknown translations of a lexical unit, which also gives us a hint about how certain we are about the known translations.", "labels": [], "entities": []}, {"text": "This allows us to say things like \"we know that twelve is a cardinal number but it's not in our translation lexicon, let's see what it is translated to in the parallel corpus\".", "labels": [], "entities": []}, {"text": "Whith small amounts of data, it is imperative to get the structural generalizations right in order to get adequate statistics; there simply aren't enough examples of longer chunks to get reliable counts.", "labels": [], "entities": []}, {"text": "This approach allows us to make good use of the limited parallel resources that are available in away that traditional statistical machine translation systems are unable to; and to make good use of the human translation examples in away that traditional rule-based machine translation systems are unable to.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 119, "end_pos": 150, "type": "TASK", "confidence": 0.6651584307352701}]}, {"text": "Even statistical tree-based models typically require word alignments as input, which again requires large amounts of parallel data to learn well.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7239233702421188}]}, {"text": "In contrast, Inversion Transduction Grammars learn the translation model and the word alignments simultaneously, effectively integrating overall possible alignments during training.", "labels": [], "entities": [{"text": "Inversion Transduction Grammars", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.8248904546101888}]}, {"text": "This is possible because of the strong modeling bias, that limits the search space of possible compositions to make the seemingly intractable problem of bilingual composition tractable, without resorting to heuristics.", "labels": [], "entities": []}, {"text": "The modeling bias has been empirically shown to still allows the model to express most structural differences that have been observed between natural languages.", "labels": [], "entities": []}, {"text": "Taken together, this lets us mostly rely on what we already know, and incorporate new knowledge as we encounter the need to.", "labels": [], "entities": []}, {"text": "This is ideal in the low resource language setting, where we want to stick to presumably hard earned prior knowledge when possible, but still have the option of adding to this knowledge base when needed.", "labels": [], "entities": []}, {"text": "The low-resource language translation setting presumes that we have small amounts of resources inthe input language, and large amounts of resources in the output language (English).", "labels": [], "entities": [{"text": "low-resource language translation", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7251395583152771}]}, {"text": "The job of a translation system is to produce fluent output that adequately represents the meaning of the input, so a translation system should be biased towards the output language.", "labels": [], "entities": []}, {"text": "We do this by basing our ITG model on an English treebank, which allows us to (a) extract a binarized context-free grammar, CFG, and (b) estimate initial probabilities for the structural rules.", "labels": [], "entities": [{"text": "CFG", "start_pos": 124, "end_pos": 127, "type": "DATASET", "confidence": 0.7102082371711731}]}, {"text": "The stochastic CFG can then be mirrored to form a grammatical channel model (.", "labels": [], "entities": []}, {"text": "Conventional statistical machine translation, or SMT, systems such as phrase-based SMT rely on large amounts of parallel data to collect statistics over how large chunks translate between two languages.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6656911273797353}, {"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9621273279190063}, {"text": "phrase-based SMT", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.48771166801452637}]}, {"text": "These models are highly specific, and may have two different rules for example, fora long and complicated noun phrase with the determiner and for the very same noun phrase without it.", "labels": [], "entities": []}, {"text": "Needless to say this kind of modeling is too wasteful to be of much use when there is very small amounts of parallel data available.", "labels": [], "entities": []}, {"text": "Tree-based models, models that allow for chunks containing general categories as opposed to fully lexicalized chunks, are better able to generalize, but make poor use of the data by disregarding translations that fall outside of the single monolingually motivated parse tree that has been committed to.", "labels": [], "entities": []}, {"text": "A forest-based system would alleviate this problem, but two problems still remain: The conventional systems all take a preexisting word alignment as input, disregarding anything that does not conform to it.", "labels": [], "entities": []}, {"text": "And none of them make use of forests in the output language.", "labels": [], "entities": []}, {"text": "In contrast to conventional systems, our proposed model jointly models the parse forest of the output language, the lexical alignment to the input language, and thus also the projection of the output language parse forest onto the input language sentence, as well as the structural differences needed to make this projection happen.", "labels": [], "entities": []}, {"text": "All of this is possible because of ITGs: Their inductive bias allows us to disregard huge swathes of the search space when explaining the structural differences.", "labels": [], "entities": []}, {"text": "Their similarity with CFGs allows us to use an English context-free grammar as a starting point for induction.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.9216477274894714}]}, {"text": "And their computational complexity allows us to efficiently collect rule expectations.", "labels": [], "entities": []}, {"text": "The presented model is capable of exploring the entire space of structural differences between, in our case, English and Chinese, that conform to (a) the English CFG, and (b) the ITG-constraints.", "labels": [], "entities": [{"text": "English CFG", "start_pos": 154, "end_pos": 165, "type": "DATASET", "confidence": 0.5892917811870575}]}, {"text": "It is also capable of expanding the translation dictionary beyond what we initialize it with to cover the small parallel corpus that we assume is available.", "labels": [], "entities": []}, {"text": "Although Chinese is by no means a low resource language, it is vastly different from English, meaning that there are plenty of structural differences to learn.", "labels": [], "entities": []}, {"text": "We also allow very limited Chinese resources in building our model, simulating a low resource language setting.", "labels": [], "entities": []}, {"text": "Inversion transductions are formally a family of transductions, where a transduction is the bilingual version of a formal language, such that it relates two formal languages to each other.", "labels": [], "entities": [{"text": "Inversion transductions", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7970455884933472}]}, {"text": "Inversion transductions are generated by Inversion Transduction Grammars, or ITGs (Wu, 1997), which share several traits with CFGs in that the transduction rules have single non-terminals on the left-hand side, and in that there is always a 2-normal form equivalence for every ITG.", "labels": [], "entities": []}, {"text": "The latter is quite rare for transduction grammars, and limits the structural differences that can be generated between the languages.", "labels": [], "entities": []}, {"text": "These limits in structural differences have been empirically shown to include most of the differences found between natural languages (S\u00f8gaard and Wu, 2009), and make efficient processing possible.", "labels": [], "entities": []}, {"text": "Formally, an ITG is a tuple \u27e8N , W 0 , W 1 , R, S\u27e9, where N is a finite nonempty set of nonterminals, W 0 is a finite set of terminals in the output language L 0 , W 1 is a finite set of terminals in the input language L 1 , R is a finite nonempty set of inversion transduction rules and S \u2208 N is a designated start symbol.", "labels": [], "entities": []}, {"text": "An inversion transduction rule is restricted to take one of the following forms: where S \u2208 N is the start symbol, A \u2208 N is a nonterminal, and \u03c6 + is a nonempty sequence of nonterminals and biterminals.", "labels": [], "entities": []}, {"text": "A biterminal is a pair: , whereat least one of the strings have to be nonempty.", "labels": [], "entities": []}, {"text": "The square and angled brackets signal straight and inverted order respectively.", "labels": [], "entities": []}, {"text": "The brackets are frequently left out when there is only one element on the right-hand side.", "labels": [], "entities": []}, {"text": "The ITG 2-normal form is analogous to the Chomsky normal form for CFGs, where the rules are fur-56 ther restricted to only the following forms: where S \u2208 N is the start symbol, A, B, C \u2208 N are nonterminals, e \u2208 W 0 is an L 0 token, f \u2208 W 1 is an L 1 token, and \u03f5 is the empty token.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.8948011994361877}]}, {"text": "A bracketing ITG, or BITG, has only one nonterminal symbol (other than the dedicated start symbol), which means that the nonterminals carry no information at all other than the fact that their yields are discrete unit.", "labels": [], "entities": [{"text": "BITG", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9620760083198547}]}], "datasetContent": [{"text": "To test the induction algorithm, we empirically compare the results of our proposed system to a bracketing ITG induced from the same parallel corpus, but without any prior knowledge of English.", "labels": [], "entities": []}, {"text": "To extract a CFG over English, we use the Penn treebank, with relative frequencies of the productions as the rule probabilities.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9876619577407837}]}, {"text": "As translation dictionary, we use the Chinese-English Translation Lexicon().", "labels": [], "entities": [{"text": "Chinese-English Translation Lexicon", "start_pos": 38, "end_pos": 73, "type": "DATASET", "confidence": 0.59334663550059}]}, {"text": "When transforming the CFG into an ITG (Section 3.1) we divide the probability mass of the CFG-rules uniformly among the ITG rules they spawn.", "labels": [], "entities": []}, {"text": "As the small parallel data set we use the IWSLT07 Chinese-English data set, which contains 46,867 sentence pairs.", "labels": [], "entities": [{"text": "IWSLT07 Chinese-English data set", "start_pos": 42, "end_pos": 74, "type": "DATASET", "confidence": 0.939786434173584}]}, {"text": "Chinese sentence are typically written without spaces, so we use a tool) to segment it into more \"word like\" units.", "labels": [], "entities": []}, {"text": "We allow the wildcard to match zero or one such Chinese tokens.", "labels": [], "entities": []}, {"text": "To make use of the parallel data, we perform 10 iterations of reestimation (Section 3.2), with abeam width b = 100, and the following priors: preexisting lexical rules: 10 \u22122 , lexical rules with wildcards: 10 \u22125 , other rules: 10 \u221210 . We qualitatively evaluate the resulting ITG by looking at how it explains some of the parallel sentences, as opposed to how the baseline bracketing ITG explains them.", "labels": [], "entities": [{"text": "reestimation", "start_pos": 62, "end_pos": 74, "type": "METRIC", "confidence": 0.9552690386772156}]}], "tableCaptions": []}