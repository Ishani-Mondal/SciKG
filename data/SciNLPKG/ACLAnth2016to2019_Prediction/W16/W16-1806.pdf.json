{"title": [{"text": "Accounting ngrams and multi-word terms can improve topic models", "labels": [], "entities": []}], "abstractContent": [{"text": "The paper presents an empirical study of integrating ngrams and multi-word terms into topic models, while maintaining similarities between them and words based on their component structure.", "labels": [], "entities": []}, {"text": "First, we adapt the PLSA-SIM algorithm to the more widespread LDA model and ngrams.", "labels": [], "entities": []}, {"text": "Then we propose a novel algorithm LDA-ITER that allows the incorporation of the most suitable ngrams into topic models.", "labels": [], "entities": []}, {"text": "The experiments of integrating ngrams and multi-word terms conducted on five text collections in different languages and domains demonstrate a significant improvement in all the metrics under consideration.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic models, such as PLSA ( and LDA (), have shown great success in discovering latent topics in text collections.", "labels": [], "entities": []}, {"text": "They have considerable applications in the information retrieval, text clustering and categorization (, word sense disambiguation, etc.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.8273999989032745}, {"text": "text clustering", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.8134545385837555}, {"text": "word sense disambiguation", "start_pos": 104, "end_pos": 129, "type": "TASK", "confidence": 0.6846203009287516}]}, {"text": "However, these unsupervised models may not produce topics that conform to the user's existing knowledge.", "labels": [], "entities": []}, {"text": "One key reason is that the objective functions of topic models do not correlate well with human judgements (.", "labels": [], "entities": []}, {"text": "Therefore, it is often necessary to incorporate semantic knowledge into topic models to improve the model's performance.", "labels": [], "entities": []}, {"text": "Recent work has shown that interactive human feedback () and information about words can improve the inferred topic quality.", "labels": [], "entities": []}, {"text": "Another key limitation of the original algorithms is that they rely on a \"bag-of-words\" assumption, which means that words are assumed to be uncorrelated and generated independently.", "labels": [], "entities": []}, {"text": "While this assumption facilitates computational efficiency, it loses the rich correlations between words.", "labels": [], "entities": []}, {"text": "There are several studies, in which the integration of collocations, ngrams and multi-word terms is investigated.", "labels": [], "entities": []}, {"text": "However, they are often limited to bigrams and often result in a worsening of the model quality due to increasing the size of a vocabulary or to a complication of the model, which requires time-intensive computation (.", "labels": [], "entities": []}, {"text": "The paper presents two novel methods that take into account ngrams and maintain relationships between them and the words in topic models (e.g, weapon -nuclear weapon -weapon of mass destruction; discrimination -discrimination on basis of nationality -racial discrimination).", "labels": [], "entities": []}, {"text": "The proposed algorithms do not rely on any additional resources, human help or topic-independent rules.", "labels": [], "entities": []}, {"text": "Moreover, they lead to a huge improvement of the quality of topic models.", "labels": [], "entities": []}, {"text": "All experiments were carried out using the LDA algorithm and its modifications on five corpora in different domains and languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we used English and Russian text collections in different domains.", "labels": [], "entities": []}, {"text": "To compare the proposed algorithms with the original one, we extracted all the ngrams in each text corpus.", "labels": [], "entities": []}, {"text": "For ranking ngrams we used Term Frequency (TF) and one of the eight context measures: C-Value (Frantzi and Ananiadou, 1999), two versions of NC-Value (), Token-FLR, Token-LR, Type-FLR, Type-LR (, and Modified Gravity Count ().", "labels": [], "entities": []}, {"text": "We should note that context measures are the most well-known method for extracting ngrams and multi-word terms.", "labels": [], "entities": []}, {"text": "According to the results of (Lau et al., 2013) we decided to integrate the top-1000 ngrams and multi-word terms into all the topic models under consideration.", "labels": [], "entities": []}, {"text": "We should note that in all experiments we fixed the number of topics |T | = 100 and the hyperparameters \u03b1 t = 50 |T | and \u03b2 w = 0.01.", "labels": [], "entities": []}, {"text": "We conducted experiments with all nine aforementioned measures on all the text collections to compare the quality of the LDA, the LDA with top-1000 ngrams or multi-word terms added as \"black boxes\" (similar to (), and the LDA-SIM with the same top-1000 elements.", "labels": [], "entities": []}, {"text": "In we present the results of integrating the top-1000 ngrams and multi-word terms ranked by NC-Value (Frantzi and Ananiadou, 1999) for all five text collections.", "labels": [], "entities": []}, {"text": "Other measures under consideration demonstrate similar results.", "labels": [], "entities": []}, {"text": "As we can see, there is a huge improvement in topic coherence using the proposed algorithm in all five text collections.", "labels": [], "entities": []}, {"text": "This means that the inferred topics become more interpretable.", "labels": [], "entities": []}, {"text": "As for: Results of integrating top-1000 ngrams and terms ranked by NC-Value into topic models perplexity, there is also a significant improvement compared to LDA with ngrams as \"black boxes\".", "labels": [], "entities": [{"text": "NC-Value", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.9251485466957092}]}, {"text": "Moreover, sometimes the perplexity is even better than in the original LDA, although the proposed algorithm works on the larger vocabularies, which usually leads to the increase of perplexity.", "labels": [], "entities": []}, {"text": "We should note that the results of the ACL and NIPS corpora area little different.", "labels": [], "entities": [{"text": "NIPS corpora", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.782223105430603}]}, {"text": "This is because the ACL corpus contains a lot of word segments hyphenated at ends of lines, while the NIPS corpus is relatively small.", "labels": [], "entities": [{"text": "ACL corpus", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.9357154369354248}, {"text": "NIPS corpus", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.9520181715488434}]}, {"text": "At the last stage of the experiments, we compare the iterative and original algorithms.", "labels": [], "entities": []}, {"text": "In we present the results of the first iteration of the LDA-ITER algorithm (with the numbers of the added ngrams and terms) alongside the LDA.", "labels": [], "entities": []}, {"text": "As we can see, there is also an improvement in the topics, despite the fact that the LDA-ITER algorithm selects much more ngrams than in the experiments with the LDA-SIM.", "labels": [], "entities": []}, {"text": "As for the multiword terms, selecting just a few hundreds of them results in the similar or even better topic quality: Results of integrating ngrams and multiword terms into the LDA-ITER algorithm than selecting regular ngrams.", "labels": [], "entities": []}, {"text": "Thus, it seems very important that in the case of the LDA-ITER algorithm there is no need to select the desired number of integrating ngrams (cf. the LDA-SIM algorithm).", "labels": [], "entities": []}, {"text": "We should also note that on the next iterations the results start to hover around the same values of the measures.", "labels": [], "entities": []}, {"text": "In we present working time of the LDA-SIM and the first iteration of the LDA-ITER alongside the original LDA.", "labels": [], "entities": [{"text": "LDA-SIM", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.8522539138793945}]}, {"text": "All the algorithms conducted on a notebook with 2.1 GHz Intel Core i7-4600U and 8 GB RAM, running Lubuntu 16.04.", "labels": [], "entities": []}, {"text": "the top-10 elements from the two random topics inferred by the LDA-SIM with 1000 most frequent ngrams and the first iteration of the LDA-ITER on the ACL corpus.", "labels": [], "entities": [{"text": "ACL corpus", "start_pos": 149, "end_pos": 159, "type": "DATASET", "confidence": 0.9203633069992065}]}], "tableCaptions": [{"text": " Table 2: Results of integrating top-1000 ngrams  and terms ranked by NC-Value into topic models", "labels": [], "entities": [{"text": "NC-Value", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.94939786195755}]}, {"text": " Table 3: Results of integrating ngrams and multi- word terms into the LDA-ITER algorithm", "labels": [], "entities": []}]}