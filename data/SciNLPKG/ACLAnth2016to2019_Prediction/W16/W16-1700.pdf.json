{"title": [], "abstractContent": [], "introductionContent": [], "datasetContent": [{"text": "This special theme considers current practice in evaluation of linguistic annotations and its successes and failures by asking questions such as: How are we as a community measuring inter-annotator agreement to date, and are there sounder ways to measure it?", "labels": [], "entities": [{"text": "evaluation of linguistic annotations", "start_pos": 49, "end_pos": 85, "type": "TASK", "confidence": 0.8049074411392212}]}, {"text": "How can we estimate the annotation quality of existing resources, and what can be done to document annotated data to help others assess its reliability?", "labels": [], "entities": []}, {"text": "1. How agreement is measured in various (new or existing) annotation projects, and what the different scores tell us in each case.", "labels": [], "entities": []}, {"text": "2. Good acceptance thresholds for different annotation tasks and metrics, and/or how to determine them.", "labels": [], "entities": []}, {"text": "3. Previously proposed but not widely used measures for agreement or annotation quality.", "labels": [], "entities": [{"text": "agreement or annotation quality", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.6302504390478134}]}, {"text": "4. Proposals for quantitative or qualitative methods to measure agreement or annotation quality.", "labels": [], "entities": []}, {"text": "5. Proposals for documentation of published resources to support their evaluation, means and methods to achieve community evaluation of linguistically-annotated resources, etc.", "labels": [], "entities": []}], "tableCaptions": []}