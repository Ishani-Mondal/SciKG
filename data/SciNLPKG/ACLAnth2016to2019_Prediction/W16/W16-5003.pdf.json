{"title": [{"text": "Detecting Level of Belief in Chinese and Spanish", "labels": [], "entities": [{"text": "Detecting Level of Belief", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8677266389131546}]}], "abstractContent": [{"text": "There has been extensive work on detecting the level of committed belief (also known as \"fac-tuality\") that an author is expressing towards the propositions in his or her utterances.", "labels": [], "entities": []}, {"text": "Previous work on English has revealed that this can be done as a word tagging task.", "labels": [], "entities": [{"text": "word tagging task", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7836294670899709}]}, {"text": "In this paper, we investigate the same task for Chinese and Spanish, two very different languages from English and from each other.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "For both languages, we trained and fine-tuned the parameters of a Linear SVM classifiers from Scikitlearn library.", "labels": [], "entities": []}, {"text": "This classifier implements a one-vs-all strategy which has a similar performance as an SVM classifier with one-vs-one strategy, but its runtime is considerably faster.", "labels": [], "entities": []}, {"text": "We report results only on the tags for the heads of propositions (i.e., not on the O tag).", "labels": [], "entities": []}, {"text": "We use F-measure to report results, and used a weighted average F-measure to summarize the results.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9691590070724487}, {"text": "F-measure", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.8853343725204468}]}], "tableCaptions": [{"text": " Table 2: Chinese and Spanish words per label and data set", "labels": [], "entities": []}, {"text": " Table 3. As can be seen, using words far outperforms characters, even if we  use a much larger window for characters than for words. We therefore use words for the remainder of  our experiments.", "labels": [], "entities": []}, {"text": " Table 3: Chinese: Comparison of labeling characters with labeling words (after word segmentation); first  six result columns are based on characters with different context windows, next three columns are based  on words. Boldface indicates the best F1-measure performance per label across the three experiments.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7292388528585434}, {"text": "F1-measure", "start_pos": 250, "end_pos": 260, "type": "METRIC", "confidence": 0.994629979133606}]}, {"text": " Table 4: Chinese: Using POS tags, experimenting with different positions for the target word w 0 in the  window. Boldface indicates the best F1-measure performance per label across the three experiments.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 142, "end_pos": 152, "type": "METRIC", "confidence": 0.9944264888763428}]}, {"text": " Table 5: Chinese: Using word embeddings, with context window [-2,+2] (word in position 3 of 5-word  context window, first three result columns) and context window [-3,+1] (word in position 4 of 5-word  context window, last three result columns). Boldface indicates the best F1-measure performance per  label across the three experiments.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 275, "end_pos": 285, "type": "METRIC", "confidence": 0.9863699674606323}]}, {"text": " Table 6: Spanish: Word Features. Boldface indicates the best F1-measure performance per label across  the three experiments.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.997951090335846}]}, {"text": " Table 8. We see only relatively small changes resulting from the use of lemmas. For  reasons that are not clear to us, the [-3,+1] context window now performs best on average as well as for  the specific tags CB, NCB, and ROB. For the NA tag, even more left context is useful, with the [-4,0]  context window performing best. When comparing the best results per label to the best results per label  without lemmas", "labels": [], "entities": []}, {"text": " Table 8: Spanish: Adding Lemmas. Boldface indicates the best F1-measure performance per label across  the three experiments.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9971500039100647}]}, {"text": " Table 9: Spanish: Using word embeddings. Boldface indicates the best F1-measure performance per  label across the three experiments.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9971108436584473}]}, {"text": " Table 10: The results of the best configurations on the test sets", "labels": [], "entities": []}]}