{"title": [{"text": "Frequency Relevant introduction 25.55% abstract 6.98% title 5.58% conclusion. 4.46% the approach. 4.05% potential for improvement. 3.35% evaluation. 3.21%. The frequency refers to how many times the section contained a relevant sentence. This was calculated from all the documents in the dataset.", "labels": [], "entities": [{"text": "title", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9688557386398315}, {"text": "conclusion", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.834010124206543}]}], "abstractContent": [{"text": "This paper describes the University of Houston team's efforts toward the problem of identifying reference spans in a reference document given sentences from other documents that cite the reference document.", "labels": [], "entities": []}, {"text": "We investigated the following approaches: cosine similarity with multiple incremental modifications and SVMs with a tree kernel.", "labels": [], "entities": []}, {"text": "Although the best performing approach in our experiments is quite simple, it is not the best under every metric used for comparison.", "labels": [], "entities": []}, {"text": "We also present a brief analysis of the dataset which includes information on its sparsity and frequency of section titles.", "labels": [], "entities": [{"text": "sparsity", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9503241181373596}]}], "introductionContent": [{"text": "The CL-SciSumm 2016 shared task poses the problem of automatic summarization in the Computational Linguistics (CL) domain.", "labels": [], "entities": [{"text": "summarization", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.8274315595626831}]}, {"text": "Single text summarization is hardly new, however, in addition to the reference text to be summarized we are also given citances i.e. sentences that cite our reference text.", "labels": [], "entities": [{"text": "Single text summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6002658208211263}]}, {"text": "The shared task is broken into multiple tasks with the unifying theme of leveraging citances.", "labels": [], "entities": []}, {"text": "Task 1a is, given a citance, to identify the span of reference text that best reflects what has been cited.", "labels": [], "entities": []}, {"text": "Task 1b asks us to classify the cited aspect according to a predefined set of facets: hypothesis, aim, method, results, and implication.", "labels": [], "entities": []}, {"text": "Finally, Task 2 is generating a structured summary.", "labels": [], "entities": []}, {"text": "We experimented with the following approaches: SVMs with a tree kernel and cosine similarity based on TF/IDF weights for sentences.", "labels": [], "entities": [{"text": "TF/IDF weights", "start_pos": 102, "end_pos": 116, "type": "METRIC", "confidence": 0.7951516658067703}]}, {"text": "The best results when measuring by sentence inclusion are obtained by cosine similarity.", "labels": [], "entities": [{"text": "sentence inclusion", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.6978117376565933}]}, {"text": "However, ROUGE-L scores are better for the tree kernel approach.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9853701591491699}]}, {"text": "We also study two characteristics of the dataset: sparsity and section importance.", "labels": [], "entities": [{"text": "sparsity", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9455916285514832}]}, {"text": "We define section importance as the normalized frequency of the section, i.e., the number of correct reference sentences that belong to this section across all the citances.", "labels": [], "entities": []}, {"text": "We found that the introduction is the most cited section in this year's dataset.", "labels": [], "entities": []}, {"text": "We also find that citances are less sparse than the average sentence within the corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset consists of 30 total documents separated into three sets of 10 documents each: training, development, and test sets.", "labels": [], "entities": []}, {"text": "For the following analysis no preprocessing has been done (for instance, stemming).", "labels": [], "entities": [{"text": "stemming", "start_pos": 73, "end_pos": 81, "type": "TASK", "confidence": 0.9687350392341614}]}, {"text": "There are 23784 unique words among the reference documents in the dataset.", "labels": [], "entities": []}, {"text": "The citances contain 6415 unique words.", "labels": [], "entities": []}, {"text": "The most frequent word among reference documents appears in 4125 sentences.", "labels": [], "entities": []}, {"text": "The most frequent word among citances appears in 598 sentences.", "labels": [], "entities": []}, {"text": "There are 6706 reference sentences and 913 citance sentences (a few annotations have more than one).", "labels": [], "entities": []}, {"text": "The average reference sentence has approximately 22 words in this dataset whereas citances have an average of approximately 30 words.", "labels": [], "entities": []}, {"text": "In we can see how sparse the dataset is; the quicker the decay, the greater the sparsity.", "labels": [], "entities": []}, {"text": "Noise in the dataset is one of the factors for the sparsity.", "labels": [], "entities": [{"text": "sparsity", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9254961013793945}]}, {"text": "We can see that citances, seen as a corpus, are in general less sparse than the reference texts.", "labels": [], "entities": []}, {"text": "This can bean indication that citances have some common structure or semantics.", "labels": [], "entities": []}, {"text": "The evaluation of our systems is done by comparison of several metrics which are detailed below.", "labels": [], "entities": []}, {"text": "For the Tree kernel method, these values are averaged over 5 runs.", "labels": [], "entities": []}, {"text": "The ROUGE metrics are useful for evaluating summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9326292276382446}, {"text": "summaries", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.9728843569755554}]}, {"text": "In particular we look at the ROUGE-L metric, which has the fewest parameters.", "labels": [], "entities": [{"text": "ROUGE-L metric", "start_pos": 29, "end_pos": 43, "type": "METRIC", "confidence": 0.9666054844856262}]}, {"text": "The ROUGE-L metric is based on the Longest Common Subsequence (LCS).", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9608961343765259}, {"text": "Longest Common Subsequence (LCS)", "start_pos": 35, "end_pos": 67, "type": "METRIC", "confidence": 0.7880018651485443}]}, {"text": "Consequently, it is more lenient since sentences that share words will be considered as somewhat correct.", "labels": [], "entities": []}, {"text": "Our systems output the top 3 sentences, thus we compute recall, precision, and F1 score for these sentences.", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9995691180229187}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9993984699249268}, {"text": "F1 score", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9844639301300049}]}, {"text": "If a relevant sentence appears in the top-3, then it factors into recall, precision, and F1 score.", "labels": [], "entities": [{"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9996127486228943}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9996598958969116}, {"text": "F1 score", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9828383028507233}]}, {"text": "Note that due to sentence limiting (SL) we impose a limit on the F1 score attainable.", "labels": [], "entities": [{"text": "sentence limiting (SL)", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7375661730766296}, {"text": "F1 score attainable", "start_pos": 65, "end_pos": 84, "type": "METRIC", "confidence": 0.9506887594858805}]}, {"text": "Since we always return the top 3 sentences, for the 279 citances of the test set we return 837 total sentences.", "labels": [], "entities": []}, {"text": "We can calculate the maximum attainable F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9850776195526123}]}, {"text": "For the Tree kernel approach it is 35.04%.", "labels": [], "entities": []}, {"text": "For the TF/IDF aproach it varies between 44.98% and 58.02%.", "labels": [], "entities": [{"text": "IDF", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.27361002564430237}, {"text": "aproach", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.6620913743972778}]}, {"text": "We compute the average rank by obtaining the rank of all the relevant reference sentences of each citance.", "labels": [], "entities": []}, {"text": "These are normalized according to the total number of sentences being considered.", "labels": [], "entities": []}, {"text": "The normalized rank is a value within the interval.", "labels": [], "entities": []}, {"text": "We average these ranks among the citances fora document.", "labels": [], "entities": []}, {"text": "Finally we find the mean of these averages for all documents.", "labels": [], "entities": [{"text": "mean", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9721550345420837}]}, {"text": "For example, fora single reference text with two citances, each referring to a single sentence, we would average the normalized rank of these two sentences according to their appropriate citance.", "labels": [], "entities": []}, {"text": "We then find the mean among the documents processed.", "labels": [], "entities": []}, {"text": "Method ROUGE-L TKern(1:1)+SL 58.78% TKern(1:4)+SL 57.90% TKern(1:8)+SL 57.76% TKern(1:1)+SL+CE 58.84% TKern(1:4)+SL+CE 58.12% TKern(1:8)+SL+CE 57.87%    BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries", "labels": [], "entities": [{"text": "BIRNDL", "start_pos": 153, "end_pos": 159, "type": "DATASET", "confidence": 0.6529801487922668}, {"text": "Information Retrieval", "start_pos": 205, "end_pos": 226, "type": "TASK", "confidence": 0.6880032420158386}]}], "tableCaptions": [{"text": " Table 3. F1-score of ROUGE-L metric for TF/IDF approach.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991889595985413}, {"text": "ROUGE-L metric", "start_pos": 22, "end_pos": 36, "type": "METRIC", "confidence": 0.9475325047969818}]}, {"text": " Table 4. Recall, precision, and F1 score at Top-3. Average rank of relevant sentences.  In parentheses we have the number of relevant sentences with non-zero similarity.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9982965588569641}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.999289870262146}, {"text": "F1 score", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9818106889724731}, {"text": "Average rank", "start_pos": 52, "end_pos": 64, "type": "METRIC", "confidence": 0.9624817967414856}]}, {"text": " Table 5. F1-score of ROUGE-L metric for Task 2.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995137453079224}, {"text": "ROUGE-L", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9809849262237549}]}]}