{"title": [{"text": "Sentence Embedding Evaluation Using Pyramid Annotation", "labels": [], "entities": [{"text": "Sentence Embedding Evaluation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9206112424532572}]}], "abstractContent": [{"text": "Word embedding vectors are used as input fora variety of tasks.", "labels": [], "entities": []}, {"text": "Choosing the right model and features for producing such vectors is not a trivial task and different embedding methods can greatly affect results.", "labels": [], "entities": []}, {"text": "In this paper we re-purpose the \"Pyramid Method\" annotations used for evaluating automatic summarization to create a benchmark for comparing embedding models when identifying paraphrases of text snippets containing a single clause.", "labels": [], "entities": []}, {"text": "We present a method of converting pyramid annotation files into two distinct sentence embedding tests.", "labels": [], "entities": []}, {"text": "We show that our method can produce a good amount of testing data, analyze the quality of the testing data, perform test on several leading embedding methods, and finally explain the downstream usages of our task and its significance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word vector embeddings have become a standard building block for NLP applications.", "labels": [], "entities": []}, {"text": "By representing words using continuous multi-dimensional vectors, applications take advantage of the natural associations among words to improve task performance.", "labels": [], "entities": []}, {"text": "For example, POS tagging, NER, parsing, Semantic Role Labeling or sentiment analysis -have all been shown to benefit from word embeddings, either as additional features in existing supervised machine learning architectures, or as exclusive word representation features.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.8358124792575836}, {"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9676330089569092}, {"text": "Semantic Role Labeling", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7295061548550924}, {"text": "sentiment analysis", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9254275560379028}]}, {"text": "In deep learning applications, word embeddings are typically used as pre-trained initial layers in deep architectures, and have been shown to improve performance on a wide range of tasks as well (see for example,).", "labels": [], "entities": []}, {"text": "One of the key benefits of word embeddings is that they can bring to tasks with small annotated datasets and small observed vocabulary, the capacity to generalize to large vocabularies and to smoothly handle unseen words, trained on massive scale datasets in an unsupervised manner.", "labels": [], "entities": []}, {"text": "Training word embedding models is still an art with various embedding algorithms possible and many parameters that can greatly affect the results of each algorithm.", "labels": [], "entities": []}, {"text": "It remains difficult to predict which word embeddings are most appropriate to a given task, whether fine tuning of the embeddings is required, and which parameters perform best fora given application.", "labels": [], "entities": []}, {"text": "We introduce a novel dataset for comparing embedding algorithms and their settings on the specific task of comparing short clauses.", "labels": [], "entities": []}, {"text": "The current state-of-the-art paraphrase dataset] is quite small with 4,076 sentence pairs (2,753 positive).", "labels": [], "entities": []}, {"text": "The Stanford Natural Language Inference (SNLI) corpus contains 570k sentences pairs labeled with one of the tags: entailment, contradiction, and neutral.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 4, "end_pos": 53, "type": "DATASET", "confidence": 0.5662968158721924}]}, {"text": "SNLI improves on previous paraphrase datasets by eliminating indeterminacy of event and entity coreference which make human entailment judgment difficult.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.6835436820983887}, {"text": "entity coreference", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7090082615613937}, {"text": "human entailment judgment", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.6717507243156433}]}, {"text": "Such indeterminacies are avoided by eliciting descriptions of the same images by different annotators.", "labels": [], "entities": []}, {"text": "We repurpose manually created data sets from automatic summarization to create anew paraphrase dataset with 197,619 pairs (8,390 positive and challenging distractors in the negative pairs).", "labels": [], "entities": []}, {"text": "Like SNLI, our dataset avoids semantic indeterminacy because the texts are generated from the same news reports -we thus obtain definite entailment judgments but in the richer domain of news report as opposed to image descriptions.", "labels": [], "entities": []}, {"text": "The propositions in our dataset are on average 12.1 words long (as opposed to about 8 words for the SNLI hypotheses).", "labels": [], "entities": [{"text": "SNLI hypotheses", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.729712575674057}]}, {"text": "In addition to paraphrase, our dataset captures a notion of centrality -the clause elements captured are Summary Content Units (SCU) which are typically shorter than full sentences and intended to capture proposition-level facts.", "labels": [], "entities": []}, {"text": "As such, the new dataset is relevant for exercising the large family of \"Sequence to Sequence\" (seq2seq) tasks involving the generation of short text clauses.", "labels": [], "entities": [{"text": "generation of short text clauses", "start_pos": 125, "end_pos": 157, "type": "TASK", "confidence": 0.7315108060836792}]}, {"text": "The paper is structured as follows: \u00a72 describes the pyramid method; \u00a73 describes the process for generating a paraphrase dataset from a pyramid dataset; in \u00a74, we evaluate a number of algorithms on the new benchmark and in \u00a75, we explain the importance of the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to verify that this task indeed is sensitive to differences in word embeddings, we evaluated 8 different word embeddings on the task as a baseline: Random, None (One-Hot embedding),) trained on Google News and two models trained on Wikipedia with different window sizes (Levy and Goldberg 2014), word2vec trained with Wikipedia dependencies (,) and Open IE based embeddings.", "labels": [], "entities": []}, {"text": "For all of the embeddings, we measured sentence similarity as the cosine similarity 1 of the normalized sum of all the words in the sentences.", "labels": [], "entities": [{"text": "cosine similarity 1", "start_pos": 66, "end_pos": 85, "type": "METRIC", "confidence": 0.7498524288336436}]}, {"text": "For the binary decision test, we evaluated the embedding by finding a threshold for answering where a pair is a paraphrase that maximizes the F-measure (trained over 10% the dataset and tested on the rest) of the embedding decision.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9955032467842102}]}, {"text": "For the rank test, we computed the percentage of questions where the correct answer achieved the highest similarity score and the MRR measure  significantly better than the word2vec model trained on Wikipedia (62.8%).", "labels": [], "entities": [{"text": "similarity score", "start_pos": 105, "end_pos": 121, "type": "METRIC", "confidence": 0.976527601480484}, {"text": "MRR", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9966850876808167}]}, {"text": "MRR for ranking was dominated by word2vec with 0.41.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6739938259124756}]}, {"text": "Given an annotated dataset (e.g., aligned translations), unannotated sentences could be annotated the same as their paraphrases", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Different embedding performance on binary and", "labels": [], "entities": []}]}