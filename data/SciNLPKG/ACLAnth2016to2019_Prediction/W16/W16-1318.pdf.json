{"title": [{"text": "A Comparison of Weak Supervision methods for Knowledge Base Construction", "labels": [], "entities": [{"text": "Weak Supervision", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.8282723426818848}]}], "abstractContent": [{"text": "We present a comparison of weak and distant supervision methods for producing proxy examples for supervised relation extraction.", "labels": [], "entities": [{"text": "supervised relation extraction", "start_pos": 97, "end_pos": 127, "type": "TASK", "confidence": 0.6309789617856344}]}, {"text": "We find that knowledge-based weak supervision tends to outperform popular distance supervision techniques, providing a higher yield of positive examples and more accurate models.", "labels": [], "entities": []}], "introductionContent": [{"text": "In performing relation extraction in knowledge base population (KBP), the need for human-annotated examples (i.e., gold-standard) examples, is prohibitively expensive.", "labels": [], "entities": [{"text": "relation extraction in knowledge base population (KBP)", "start_pos": 14, "end_pos": 68, "type": "TASK", "confidence": 0.8179503348138597}]}, {"text": "One solution is to generate a set of so-called silver-standard examples from weak or distant supervision methods.", "labels": [], "entities": []}, {"text": "While several papers have demonstrated the benefits of using these approaches (), we are not familiar with any work that compares methods for generating weak labels for KBP.", "labels": [], "entities": []}, {"text": "In this work, we seek to address the question of which weak supervision techniques provide the best basis for learning accurate models and scale appropriately with the KBP task.", "labels": [], "entities": []}, {"text": "We address two approaches: \u2022 Distant supervision (DS) -this popular technique entails referencing external knowledge bases, such as Freebase, as a source of seed facts.", "labels": [], "entities": [{"text": "Distant supervision (DS)", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.6823388397693634}]}, {"text": "These facts are then linked to a corpus to identify positive training examples.", "labels": [], "entities": []}, {"text": "We consider two variations fora corpus -extracting positive sentences from the actual training/testing corpus (CDS) (i.e., newswire documents) versus using sentences from external data sources (EDS) (e.g., Wikipedia articles).", "labels": [], "entities": []}, {"text": "\u2022 Knowledge-based weak supervision (KWS) - showed that we can encode the \"world knowledge\" of domain experts, who have some inherent rules for identifying positive training examples during manual annotation (e.g., \"home teams are more likely to win a game\" fora sports corpus).", "labels": [], "entities": []}, {"text": "Using these rules, we can automatically generate new positive examples that simulate the human expert's annotations in a training corpus.", "labels": [], "entities": []}, {"text": "In this paper, we present our approaches for generating examples in further detail.", "labels": [], "entities": []}, {"text": "We evaluate all three approaches on the TAC KBP corpus.", "labels": [], "entities": [{"text": "TAC KBP corpus", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.8774039347966512}]}, {"text": "We will also describe our pipeline, which utilizes relational dependency networks (RDNs).", "labels": [], "entities": []}, {"text": "We note that the central focus of this paper is not to showcase RDNs for this task -that has been done in previous work -but rather to investigate weak supervision techniques.", "labels": [], "entities": []}, {"text": "Our results show that knowledge-based weak supervision is the preferred choice for producing training examples when good rules are available, approaching the accuracy of gold-standard data sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9986282587051392}]}, {"text": "This method produces examples at a higher rate than DS with fewer mislabels and is flexible to adapt to a diverse set of relations.", "labels": [], "entities": []}, {"text": "Distant supervision techniques scale quicker and excel when domain knowledge is difficult to encode.", "labels": [], "entities": []}, {"text": "However, they tend to yield fewer results and are not applicable when a relevant database does not already exist.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider five TAC KBP relations from two categories, person and organization, chosen based on prior work for TAC KBP 2015.", "labels": [], "entities": [{"text": "TAC KBP relations", "start_pos": 17, "end_pos": 34, "type": "DATASET", "confidence": 0.7612793842951456}, {"text": "TAC KBP 2015", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.897300918896993}]}, {"text": "The relations are listed in the middle of along with the counts of number of positives retrieved by each method (left).", "labels": [], "entities": []}, {"text": "The TAC KBP 2014 corpus is used for training (or Wikipedia articles as is the case for EDS) while TAC KBP 2015 is used for testing.", "labels": [], "entities": [{"text": "TAC KBP 2014 corpus", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9113862216472626}, {"text": "TAC KBP 2015", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.8849167227745056}]}, {"text": "Another relation, age, is omitted from the results since a corresponding database is not available for distant supervision.", "labels": [], "entities": [{"text": "age", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9953864216804504}]}, {"text": "In our experiments, Freebase yields entity pairs for three out of the five relations (siblings, spouse, and f oundedBy) while Wikipedia Infoboxes provides entity pairs for the parents and countryOf Headquarters (countryHQ for short) relations.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.9586508274078369}]}, {"text": "NELL is utilized to supplement mentions for siblings.", "labels": [], "entities": []}, {"text": "For KWS, a range of 4 to 8 rules are derived for each relation; only 5% of the training corpus was queried to generate KWS examples due -this proved sufficient for most relations although the number could easily be expanded as part of future work.", "labels": [], "entities": []}, {"text": "Additionally, only the first 500 examples are actually utilized from.", "labels": [], "entities": []}, {"text": "Performing larger runs is part of work in progress.", "labels": [], "entities": []}, {"text": "The results are obtained from averaging 5 different runs for each condition/relation.", "labels": [], "entities": []}, {"text": "Across the runs, the test set is constant but the training set is subsampled with 75% membership to create more robust estimates of performance.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "We consider two standard metrics -area under the ROC curve and F1 score . also includes results after supplementing each weak/distant supervision with a small set (20) of gold-standard examples.", "labels": [], "entities": [{"text": "area", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9735230803489685}, {"text": "ROC", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.8578476309776306}, {"text": "F1 score", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9906722903251648}]}], "tableCaptions": []}