{"title": [{"text": "Modelling the Combination of Generic and Target Domain Embeddings in a Convolutional Neural Network for Sentence Classification", "labels": [], "entities": [{"text": "Sentence Classification", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.9114130139350891}]}], "abstractContent": [{"text": "Word embeddings have been successfully exploited in systems for NLP tasks, such as parsing and text classification.", "labels": [], "entities": [{"text": "parsing", "start_pos": 83, "end_pos": 90, "type": "TASK", "confidence": 0.9593676328659058}, {"text": "text classification", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.720146581530571}]}, {"text": "It is intuitive that word embeddings created from a larger corpus would provide a better coverage of vocabulary.", "labels": [], "entities": []}, {"text": "Meanwhile, word em-beddings trained on a corpus related to the given task or target domain would more effectively represent the semantics of terms.", "labels": [], "entities": []}, {"text": "However, in some emerging domains (e.g. bio-surveillance using social media data), it maybe difficult to find a domain corpus that is large enough for creating effective word embeddings.", "labels": [], "entities": []}, {"text": "To deal with this problem , we propose novel approaches that use both word embeddings created from generic and target domain corpora.", "labels": [], "entities": []}, {"text": "Our experimental results on sentence classification tasks show that our approaches significantly improve the performance of an existing convolutional neural network that achieved state-of-the-art performances on several text classification tasks.", "labels": [], "entities": [{"text": "sentence classification tasks", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.8172646959622701}, {"text": "text classification tasks", "start_pos": 220, "end_pos": 245, "type": "TASK", "confidence": 0.8029758632183075}]}], "introductionContent": [{"text": "Word embeddings (i.e. distributed vector representation) represent words using dense, lowdimensional and real-valued vectors, where each dimension represents a latent feature of the word ().", "labels": [], "entities": []}, {"text": "It has been empirically shown that word embeddings could capture semantic and syntactic similarities between words ().", "labels": [], "entities": []}, {"text": "Importantly, word embeddings have been effectively used for several NLP tasks.", "labels": [], "entities": []}, {"text": "For example, used word embeddings as input features for several NLP systems, including a traditional chunking system based on conditional random fields (CRFs) ().", "labels": [], "entities": []}, {"text": "used word embeddings as inputs of a multilayer neural network for part-of-speech tagging, chunking, named entity recognition and semantic role labelling.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.7098431289196014}, {"text": "named entity recognition", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.6258901357650757}, {"text": "semantic role labelling", "start_pos": 129, "end_pos": 152, "type": "TASK", "confidence": 0.6047964692115784}]}, {"text": "leveraged semantics from word embeddings when identifying medical concepts mentioned in social media messages.", "labels": [], "entities": []}, {"text": "showed that using pre-built word embeddings, induced from 100 billion words of Google News using word2vec, as inputs of a simple convolutional neural network (CNN) could achieve state-of-the-art performances on several sentence classification tasks, such as classification of positive and negative reviews of movies (Pang and) and consumer products, e.g. cameras ().", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 219, "end_pos": 242, "type": "TASK", "confidence": 0.7510707974433899}, {"text": "classification of positive and negative reviews of movies", "start_pos": 258, "end_pos": 315, "type": "TASK", "confidence": 0.8504242524504662}, {"text": "Pang", "start_pos": 317, "end_pos": 321, "type": "DATASET", "confidence": 0.9023981094360352}]}, {"text": "The quality of word embeddings (e.g. the ability to capture semantics of words) highly depends on the corpus from which they are induced).", "labels": [], "entities": []}, {"text": "For instance, when induced from a generic corpus, such as Google News, the vector representation of 'tissue' would be similar to the vectors of 'paper' and 'toilet'.", "labels": [], "entities": []}, {"text": "However, when induced from medical corpora, such as PubMed 1 or BioMed Central 2 , the vector of 'tissue' would be more similar to those of 'cell' and 'organ'.", "labels": [], "entities": [{"text": "PubMed 1", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9373583197593689}]}, {"text": "Hence, word embeddings induced from the corpus related to the task or target domain are likely to be more useful.", "labels": [], "entities": []}, {"text": "Meanwhile, it is intuitive that the more training documents used, the more likely that more vocabulary is covered.", "labels": [], "entities": []}, {"text": "Recent studies (e.g. () have attempted to improve the quality of word embeddings by enhancing the learning algorithm or injecting an existing knowledge-base, e.g. WordNet or UMLS semantic network . incorporated aggregated global word co-occurrence statistics from the corpus when inducing word embeddings. and exploited semantic knowledge to improve the semantic representation of word embeddings.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 163, "end_pos": 170, "type": "DATASET", "confidence": 0.9443516731262207}]}, {"text": "Nevertheless, in some emerging domains, e.g. detecting adverse drug reactions (ADR) reported in social media, existing knowledge resources or corpora may not be large enough for creating effective embeddings.", "labels": [], "entities": [{"text": "detecting adverse drug reactions (ADR) reported in social media", "start_pos": 45, "end_pos": 108, "type": "TASK", "confidence": 0.818072194402868}]}, {"text": "In this work, we investigate novel approaches to incorporate both generic and target domain embeddings in CNN for sentence classification.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.750947505235672}]}, {"text": "We hypothesise that using both generic and target domain embeddings further improves the performance of CNN, since it can benefit from both the good coverage of vocabulary from the generic embedding, and the effective semantic representation of the target domain embedding.", "labels": [], "entities": []}, {"text": "This would enable CNN to perform effectively without requiring new target domain embeddings induced from a large amount of domain documents specifically related to individual tasks.", "labels": [], "entities": [{"text": "CNN", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9638692140579224}]}, {"text": "We thoroughly evaluate our proposed approaches using an ADR tweet classification task ().", "labels": [], "entities": [{"text": "ADR tweet classification", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.7104297081629435}]}, {"text": "In addition, to show that our approaches are effective for different target domains, we also evaluate them using a movie review classification task (Pang and).", "labels": [], "entities": [{"text": "movie review classification", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.677734762430191}]}, {"text": "Our experimental results show that our approaches significantly improve the performance in term of accuracy over an existing strong baseline that uses only either the generic or the target domain embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9985828399658203}]}], "datasetContent": [{"text": "We compare the performance of our approaches, i.e. vector concatenation (Section 3.1) and fully connected layer combination (Section 3.2), with that of the effective CNN model of Kim  variant of the CNN model, which does not allow the input embeddings to be updated during training, as we aim to investigate the performance when using original embeddings . In addition to the pre-trained embeddings described in Section 4.2, we use 300-dimension randomly generated word embeddings, as an alternative baseline.", "labels": [], "entities": []}, {"text": "reports the accuracy performance of our approaches and the simple CNN baselines on the ADR tweet and movie review classification tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9993564486503601}, {"text": "ADR tweet and movie review classification", "start_pos": 87, "end_pos": 128, "type": "TASK", "confidence": 0.5705004235108694}]}, {"text": "We first compare the effectiveness of the simple CNN baselines when applied with different word embeddings.", "labels": [], "entities": []}, {"text": "For both tasks, the simple CNN with the target domain word embeddings (accuracy 88.75% and 80.88%) outperforms the simple CNN with either the generic (accuracy 88.47% and 80.56%) or the random (accuracy 87.97% and 72.41%) word embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9970781803131104}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9703046679496765}, {"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.9788091778755188}]}, {"text": "The performance differences between using the target domain and the random word embeddings are statistically significant (p < 0.05) for both tasks.", "labels": [], "entities": []}, {"text": "These results show the importance of target domain embedding for the simple CNN on the classification tasks.", "labels": [], "entities": []}, {"text": "Next, we discuss the performance of our two proposed approaches.", "labels": [], "entities": []}, {"text": "As shown in Table 1, Fully Connected Layer Combination (Generic+Domain) performs better than all of the other approached reported in this paper for both the ADR tweet (accuracy 89.74%) and movie review (accuracy 81.59%) classification tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.85716712474823}, {"text": "accuracy", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.9410644173622131}]}, {"text": "Importantly, it significantly (p < 0.05) outperforms the simple CNN baselines that use either the random, generic or target domain word embeddings for both tasks.", "labels": [], "entities": []}, {"text": "Meanwhile, Vector Concatenation (Generic+Domain) also outperforms all of the simple CNN baselines.", "labels": [], "entities": []}, {"text": "These support our hy- The performances of both Kim's and our approaches will further improve, if we allow the embeddings to be updated.", "labels": [], "entities": []}, {"text": "pothesis that exploiting both the generic and target domain word embeddings further improves the performance of CNN for sentence classification.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 120, "end_pos": 143, "type": "TASK", "confidence": 0.7311562448740005}]}, {"text": "To further support that our approaches are effective because of exploiting both generic and target domain embeddings rather than because of allowing the model to learn more parameters, we compare our approaches with another set of baselines that use either the generic, target domain, or random embedding twice in both of our proposed approaches.", "labels": [], "entities": []}, {"text": "We observe that Fully Connected Layer Combination (Generic+Domain) outperforms all of its corresponding baselines, e.g. Domain+Domain, for both tasks.", "labels": [], "entities": []}, {"text": "The same trends of performance are also observed for the vector concatenation approach, excepting that Vector Concatenation (Domain+Domain) marginally outperforms Vector Concatenation (Generic+Domain) on the ADR tweet classification task.", "labels": [], "entities": [{"text": "ADR tweet classification task", "start_pos": 208, "end_pos": 237, "type": "TASK", "confidence": 0.7490138709545135}]}], "tableCaptions": [{"text": " Table 1: The accuracy performance of the proposed approaches and the simple CNN baselines (Kim,  2014). Significant differences (p < 0.05, paired t-test) compared to the simple CNN baselines with the  Random, Generic and Domain word embeddings, are denoted  *  , \u2022 and \u2022 , respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9992107152938843}]}]}