{"title": [{"text": "Human versus Machine Attention in Document Classification: A Dataset with Crowdsourced Annotations", "labels": [], "entities": [{"text": "Human versus Machine Attention in Document Classification", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.6721913048199245}]}], "abstractContent": [{"text": "We present a dataset in which the contribution of each sentence of a review to the review-level rating is quantified by human judges.", "labels": [], "entities": []}, {"text": "We define an annotation task and crowdsource it for 100 audiobook reviews with 1,662 sentences and 3 aspects: story, performance, and overall quality.", "labels": [], "entities": []}, {"text": "The dataset is suitable for intrinsic evaluation of explicit document models with attention mechanisms, for multi-aspect sentiment analysis and summarization.", "labels": [], "entities": [{"text": "multi-aspect sentiment analysis", "start_pos": 108, "end_pos": 139, "type": "TASK", "confidence": 0.7292384107907613}, {"text": "summarization", "start_pos": 144, "end_pos": 157, "type": "TASK", "confidence": 0.9828868508338928}]}, {"text": "We evaluated one such document attention model which uses weighted multiple-instance learning to jointly model aspect ratings and sentence-level rating contributions, and found that there is positive correlation between human and machine attention especially for sentences with high human agreement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Classifying the sentiment of documents has moved past global categories to target finer-grained ones, such as specific aspects of an item -a task known as multi-aspect sentiment analysis.", "labels": [], "entities": [{"text": "Classifying the sentiment of documents", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.9044464707374573}, {"text": "multi-aspect sentiment analysis", "start_pos": 155, "end_pos": 186, "type": "TASK", "confidence": 0.7039487560590109}]}, {"text": "An important challenge for this task is that target categories have \"weak\" relations to the input documents, as it is unknown which parts of the documents convey information about each category refer to.", "labels": [], "entities": []}, {"text": "Using supervised learning to solve this task requires labeled data.", "labels": [], "entities": []}, {"text": "Several previous studies have adopted a strongly-supervised approach using sentence-level labels (), obtained with a significant human annotation effort.", "labels": [], "entities": []}, {"text": "However, document-level labels are often available in social media, but learning from them requires a weakly-supervised approach.", "labels": [], "entities": []}, {"text": "Recently, attention mechanisms for document modeling, either using hierarchical neural networks ( or weighted multiple-instance learning), have proved superior in classification performance and are also able to quantify the contribution of each sentence to the documentlevel category.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.8114892542362213}]}, {"text": "While explicit document models can be indirectly evaluated on aspect rating prediction or document segmentation, a more direct way to estimate their qualities is to compare the sentence-level weights or attention scores that they assign with those assigned by human judges.", "labels": [], "entities": [{"text": "aspect rating prediction", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.7448925773302714}, {"text": "document segmentation", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.7081027626991272}]}, {"text": "In this paper, we present a dataset 1 containing human estimates of the contribution of each sentence of an audiobook review to the review-level aspect rating, along three aspects: story, performance, and overall quality.", "labels": [], "entities": []}, {"text": "Following a pilot experiment (Sec. 2), the annotation task was fully specified and crowdsourced.", "labels": [], "entities": []}, {"text": "Statistics about the resulting dataset are given in Sec.", "labels": [], "entities": []}, {"text": "3. We show how the dataset can be used to evaluate a document attention model based on multipleinstance learning (outlined in Sec.", "labels": [], "entities": []}, {"text": "4), by comparing In this task we ask you to rate the explanatory power of sentences in a user review of an audiobook with respect to the user's opinion about the following aspects of the audiobook (recorded reading of a paper book): Overall: General rating based on all aspects, including also author attributes (writing style, imagination, etc.)", "labels": [], "entities": []}, {"text": "Performance: Rating based on narrator attributes (acting, voice, role, etc.)", "labels": [], "entities": []}, {"text": "Story: Rating based on the story attributes (plot, characters, setting, etc.)", "labels": [], "entities": []}, {"text": "We provide: the sentence under examination highlighted in the entire user review; the user's rating on a five-star scale towards an aspect of the audiobook (namely, 1: very negative, 2: negative, 3: neutral, 4: positive, 5: very positive).", "labels": [], "entities": []}, {"text": "The question and possible answers are displayed for each required rating.", "labels": [], "entities": []}, {"text": "The question is: \"How much does the highlighted sentence explain the given aspect rating?\" or in other words \"How much does the highlighted sentence carry the user's opinion about each aspect?\"", "labels": [], "entities": []}, {"text": "The answer is one of the following choices of how much each sentence explains the displayed aspect rating: 'not at all', 'a little', 'moderately', 'rather well', and 'very well'.", "labels": [], "entities": []}, {"text": "the sentence attention scores with those obtained by humans (Sec. 5).", "labels": [], "entities": []}, {"text": "We find a positive correlation between human and machine attention for high confidence annotations and show that the system is more reliable than some of the qualified annotators.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}