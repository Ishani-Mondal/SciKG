{"title": [{"text": "How to Train Good Word Embeddings for Biomedical NLP", "labels": [], "entities": [{"text": "Biomedical NLP", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.7011509537696838}]}], "abstractContent": [{"text": "The quality of word embeddings depends on the input corpora, model architec-tures, and hyper-parameter settings.", "labels": [], "entities": []}, {"text": "Using the state-of-the-art neural embedding tool word2vec and both intrinsic and ex-trinsic evaluations, we present a comprehensive study of how the quality of em-beddings changes according to these features.", "labels": [], "entities": []}, {"text": "Apart from identifying the most influential hyper-parameters, we also observe one that creates contradictory results between intrinsic and extrinsic evaluations.", "labels": [], "entities": []}, {"text": "Furthermore, we find that bigger corpora do not necessarily produce better biomedical domain word embeddings.", "labels": [], "entities": []}, {"text": "We make our evaluation tools and resources as well as the created state-of-the-art word embeddings available under open licenses from https://github.com/ cambridgeltl/BioNLP-2016.", "labels": [], "entities": []}], "introductionContent": [{"text": "As one of the main inputs of many NLP methods, word representations have long been a major focus of research.", "labels": [], "entities": [{"text": "word representations", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7228105664253235}]}, {"text": "Recently, the embedding of words into a low-dimensional space using neural networks was suggested ().", "labels": [], "entities": []}, {"text": "These approaches represent each word as a dense vector of real numbers, where words that are semantically related to one another map to similar vectors.", "labels": [], "entities": []}, {"text": "Among neural embedding approaches, the skip-gram model of has achieved cutting-edge results in many NLP tasks, including sentence completion, analogy and sentiment analysis).", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7610026299953461}, {"text": "sentiment analysis", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.8127991259098053}]}, {"text": "Although word embeddings have been studied extensively in recent work (e.g.), most such studies only involve general domain texts and evaluation datasets, and their results do not necessarily apply to biomedical NLP tasks.", "labels": [], "entities": []}, {"text": "In the biomedical domain, studied the effect of corpus size and domain on various word clustering and embedding methods, and compared two state-of-the-art word embedding tools: word2vec and Global Vectors (GloVe) on a word-similarity task.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7065766751766205}]}, {"text": "They showed that skip-gram significantly out-performs other models and that its performance can be further improved by using higher dimensional vectors.", "labels": [], "entities": []}, {"text": "The word2vec tool was also used to create biomedical domain word representations by and.", "labels": [], "entities": []}, {"text": "Given that word2vec has been shown to achieve state-of-the-art performance that can be further improved with parameter tuning, we focus on its performance on biomedical data with different inputs and hyper-parameters.", "labels": [], "entities": []}, {"text": "We use all available biomedical scientific literature for learning word embeddings using models implemented in word2vec.", "labels": [], "entities": []}, {"text": "For intrinsic evaluation, we use the standard UMNSRS-Rel and UMNSRS-Sim datasets (, which enable us to measure similarity and relatedness separately.", "labels": [], "entities": [{"text": "UMNSRS-Rel", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.9429318308830261}, {"text": "UMNSRS-Sim datasets", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.9634377062320709}]}, {"text": "For extrinsic evaluation, we apply a neural network-based named entity recognition (NER) model to two standard benchmark NER tasks, JNLPBA) and the BioCreative II Gene Mention task (.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.7661566932996114}, {"text": "JNLPBA", "start_pos": 132, "end_pos": 138, "type": "DATASET", "confidence": 0.8280943036079407}, {"text": "BioCreative II Gene Mention task", "start_pos": 148, "end_pos": 180, "type": "TASK", "confidence": 0.6915112972259522}]}, {"text": "Apart from showing that the optimization of hyper-parameters boosts the performance of vectors, we also find that one such parameter leads to contradictory results between intrinsic and extrinsic evaluations.", "labels": [], "entities": []}, {"text": "We further observe that a larger corpus does not necessarily guarantee better re-: Corpus statistics sults in our tasks.", "labels": [], "entities": [{"text": "re", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9721273183822632}]}, {"text": "We hope that our results can serve as a reference for researchers who use neural word embeddings in biomedical NLP.", "labels": [], "entities": []}], "datasetContent": [{"text": "A standardized intrinsic measure for word representations in the biomedical domain is the UMN-SRS word similarity dataset (.", "labels": [], "entities": [{"text": "UMN-SRS word similarity dataset", "start_pos": 90, "end_pos": 121, "type": "DATASET", "confidence": 0.6978988498449326}]}, {"text": "We use its UMNSRS-Sim (Sim) and UMNSRS-Rel (Rel) subsets as our references.", "labels": [], "entities": [{"text": "UMNSRS-Rel", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.8306789994239807}]}, {"text": "They have 566 and 587 word pairs for measuring similarity and relatedness (respectively) whose degree of association was rated by participants from the University of Minnesota Medical School.", "labels": [], "entities": []}, {"text": "In UMNSRS, the human evaluation on every word pair is converted to a score to determine its degree of similarity, a higher score implying a more similar pair.", "labels": [], "entities": []}, {"text": "The range of the score is on an arbitrary scale.", "labels": [], "entities": []}, {"text": "While UMNSRS provides scores to determine the degree of similarity for each word pair, we will measure this by calculating the cosine similarity score for each word pair using the learned word vectors.", "labels": [], "entities": [{"text": "UMNSRS", "start_pos": 6, "end_pos": 12, "type": "DATASET", "confidence": 0.8373970985412598}, {"text": "cosine similarity score", "start_pos": 127, "end_pos": 150, "type": "METRIC", "confidence": 0.7062856952349345}]}, {"text": "Afterwards, we compare the two scores using Spearman's correlation coefficient (\u03c1), which is a standard metric to compare ranking between variables regardless of scale in word similarity task.", "labels": [], "entities": [{"text": "Spearman's correlation coefficient (\u03c1)", "start_pos": 44, "end_pos": 82, "type": "METRIC", "confidence": 0.8395717101437705}]}, {"text": "We systematically ignore words that appear only in the reference but not in our models.", "labels": [], "entities": []}, {"text": "Given that the ultimate evaluation for word vectors is their performance in downstream applications, we also assess the quality of the vectors by performing NER using two well-established biomedical reference standards: the BioCreative II Gene Mention task corpus (BC2) ( and the JNLPBA corpus (PBA) ().", "labels": [], "entities": [{"text": "NER", "start_pos": 157, "end_pos": 160, "type": "TASK", "confidence": 0.9661989808082581}, {"text": "BioCreative II Gene Mention task", "start_pos": 224, "end_pos": 256, "type": "TASK", "confidence": 0.6038198590278625}, {"text": "JNLPBA corpus (PBA)", "start_pos": 280, "end_pos": 299, "type": "DATASET", "confidence": 0.9304674625396728}]}, {"text": "Both of these corpora consist of approximately 20,000 sentences from PubMed abstracts manually annotated for mentions of biomedical entity names.", "labels": [], "entities": []}, {"text": "Following the window approach architecture with word-level likelihood proposed by, we apply a tagger built on a simple feed-forward neural network, with a window of five words, one hidden layer of 300 neurons and a hard sigmoid activation, leading to a Softmax output layer.", "labels": [], "entities": []}, {"text": "Our word vectors are used as the embedding layer of the network, with the only other input being a low-dimensional binary vector of word surface features.", "labels": [], "entities": []}, {"text": "To emphasize the effect of the input word vectors on performance, we avoid fine-tuning the word vectors during training as well as introducing any external resources such as entity name dictionaries.", "labels": [], "entities": []}, {"text": "While this causes the performance of the method to fall notably below the state of the art, we believe this minimal approach to bean effective way to focus on the quality of the word vectors as they are created by the tool (word2vec).", "labels": [], "entities": []}, {"text": "For parameter selection, we estimate the extrinsic performance of word vectors on the development sets of the two corpora using mention-level F-score.", "labels": [], "entities": [{"text": "parameter selection", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7475126087665558}, {"text": "F-score", "start_pos": 142, "end_pos": 149, "type": "METRIC", "confidence": 0.7707970142364502}]}, {"text": "For the final experiment with selected parameters we apply the test sets and evaluation scripts of the two tasks in accordance with their original evaluation protocols.", "labels": [], "entities": []}, {"text": "From, we see that most vectors benefit from lower-casing and shuffling the corpus sentences.", "labels": [], "entities": []}, {"text": "Since in word2vec, the learning rate is decayed as training progresses, text appearing early has a larger effect on the model.", "labels": [], "entities": []}, {"text": "Shuffling makes the effect of all text (roughly) equivalent.", "labels": [], "entities": []}, {"text": "On the other hand, lower-casing ensures that same word but different cases, such as protein, Protein and PROTEIN are normalised (indexed as one term) for training.", "labels": [], "entities": [{"text": "PROTEIN", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9753572940826416}]}, {"text": "Although the shuffled-lower vectors perform better, in the following, we report further results based on the unshuffled-text vector to preserve the comparability of results.", "labels": [], "entities": []}, {"text": "Based on the parameter selection experiments covering three corpora (PMC, PubMed and both), various preprocessing options (normal-text, sentenceshuffled text, lower-cased text), two model architectures (skip-gram vs. CBOW) and six hyper-.", "labels": [], "entities": [{"text": "PMC", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.922764241695404}, {"text": "PubMed", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.8612034320831299}]}, {"text": "Since the size of the context window (win) showed contradictory results between the intrinsic and extrinsic tasks, we created vectors for two different values of this parameter.", "labels": [], "entities": []}, {"text": "Note that for this comparative evaluation we use the test sets and test evaluation scripts of the two extrinsic tasks.", "labels": [], "entities": []}, {"text": "summarizes the results of the comparative evaluation.", "labels": [], "entities": []}, {"text": "For our intrinsic tasks, our vectors with win = 30 show the best performance, clearly outperforming the baselines as well as our otherwise identically created vectors with win = 2.", "labels": [], "entities": []}, {"text": "This further supports the suggestion that a higher context window facilitates the learning of domain similarity for the intrinsic task.", "labels": [], "entities": []}, {"text": "For extrinsic tasks, while the difference to the baselines is smaller, our vectors with win = 2 show the best results for JNLPBA and the second best in BC2GM, while the vectors with win = 30 are clearly less competitive.", "labels": [], "entities": [{"text": "JNLPBA", "start_pos": 122, "end_pos": 128, "type": "DATASET", "confidence": 0.8611543774604797}, {"text": "BC2GM", "start_pos": 152, "end_pos": 157, "type": "DATASET", "confidence": 0.5919473767280579}]}, {"text": "The comparative evaluation on test set data thus confirms the indications from parameter selection that the context window size has opposite effects on the intrinsic and extrinsic metrics and indicates that our experiments have succeeded in creating a pair of word embeddings that show state-of-the-art performance when applied to tasks appropriate for each.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Hyper-parameters and tested values.  Default values shown in bold.", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9469658732414246}, {"text": "Default", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9734520316123962}]}, {"text": " Table 4: Intrinsic evaluation results for vectors  with different pre-processing: Original Text,  Sentence-shuffled (S), lowercased (L), and both  (SL)", "labels": [], "entities": []}, {"text": " Table 5: Extrinsic evaluation results for vectors  with different pre-processing: Original text,  Sentence-shuffled (S), lowercased (L), and both  (SL)", "labels": [], "entities": []}, {"text": " Table 6: Intrinsic evaluation results for number of  negative samples (default = 5)", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9773424863815308}]}, {"text": " Table 8: Intrinsic evaluation results for  sub-sampling (default = 1e-3)", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9223868250846863}]}, {"text": " Table 9: Extrinsic evaluation results for  sub-sampling (default = 1e-3)", "labels": [], "entities": []}, {"text": " Table 10: Intrinsic evaluation results for  min-count (default = 5)", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9519120454788208}]}, {"text": " Table 11: Extrinsic evaluation results for  min-count (default = 5)", "labels": [], "entities": []}, {"text": " Table 12: Intrinsic evaluation results for learning  rate (default = 0.025)", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9514990448951721}]}, {"text": " Table 13: Extrinsic evaluation results for learning  rate (default = 0.025)", "labels": [], "entities": []}, {"text": " Table 14: Intrinsic evaluation results for vector  dimension (default = 100)", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.925001859664917}]}, {"text": " Table 15: Extrinsic evaluation results for vector  dimension (default = 100)", "labels": [], "entities": []}, {"text": " Table 16: Intrinsic evaluation results for context  window size (default = 5)", "labels": [], "entities": []}, {"text": " Table 17: Extrinsic evaluation results for context  window size (default = 5)", "labels": [], "entities": []}, {"text": " Table 18: Settings selected for comparative  evaluation", "labels": [], "entities": []}]}