{"title": [{"text": "A Factorization Machine Framework for Testing Bigram Embeddings in Knowledgebase Completion", "labels": [], "entities": []}], "abstractContent": [{"text": "Embedding-based Knowledge Base Completion models have so far mostly combined distributed representations of individual entities or relations to compute truth scores of missing links.", "labels": [], "entities": [{"text": "Embedding-based Knowledge Base Completion", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5402002930641174}]}, {"text": "Facts can however also be represented using pairwise embeddings, i.e. em-beddings for pairs of entities and relations.", "labels": [], "entities": []}, {"text": "In this paper we explore such bigram embed-dings with a flexible Factorization Machine model and several ablations from it.", "labels": [], "entities": []}, {"text": "We investigate the relevance of various bigram types on the fb15k237 dataset and find relative improvements compared to a compositional model.", "labels": [], "entities": [{"text": "fb15k237 dataset", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.9881940186023712}]}], "introductionContent": [{"text": "Present day Knowledge Bases (KBs) such as YAGO (, or the Google Knowledge Vault () provide immense collections of structured knowledge.", "labels": [], "entities": [{"text": "YAGO", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.8004012703895569}]}, {"text": "Relationships in these KBs often exhibit regularities and models that capture these can be used to predict missing KB entries.", "labels": [], "entities": []}, {"text": "A common approach to KB completion is via tensor factorization, where a collection of fact triplets is represented as a sparse mode-3 tensor which is decomposed into several low-rank sub-components.", "labels": [], "entities": [{"text": "KB completion", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9398349523544312}]}, {"text": "Textual relations, i.e. relations between entity pairs extracted from text, can aid the imputation of missing KB facts by modelling them together with the KB relations (.", "labels": [], "entities": []}, {"text": "The general merit of factorization methods for KB completion has been demonstrated by a variety of models, such as RESCAL (),) and DistMult ().", "labels": [], "entities": [{"text": "KB completion", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9152214229106903}, {"text": "RESCAL", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9249691367149353}]}, {"text": "These models learn distributed representations for entities and relations (be it as vector or as matrix) and infer the truth value of a fact by combining embeddings for these constituents in an appropriate composition function.", "labels": [], "entities": []}, {"text": "Most of these factorization models however operate on the level of embeddings for single entities and relations.", "labels": [], "entities": []}, {"text": "The implicit assumption here is that facts are compositional, i.e. that the subject, relation and object of a fact are its atomic constituents.", "labels": [], "entities": []}, {"text": "Semantic aspects relevant for imputing its truth can directly be recovered from its constituents when composing their respective embeddings in a score.", "labels": [], "entities": []}, {"text": "For further notation let E and R be sets of entities and relations, respectively.", "labels": [], "entities": []}, {"text": "We denote a fact f stating a relation r \u2208 R between subject s \u2208 E and object o \u2208 E as f = (s, r, o).", "labels": [], "entities": []}, {"text": "Our goal is to learn embeddings for larger sub-constituents off than just s, r, and o: we want to learn embeddings also for the entity pair bigram (s, o) as well as the relation-entity bigrams (s, r) and (r, o).", "labels": [], "entities": []}, {"text": "As an example, consider Freebase facts with relation eating/practicer of diet/diet and object Veganism.", "labels": [], "entities": []}, {"text": "Overall only two objects are observed for this relation and it thus makes sense to learn a joint embedding for bigrams (r, o) together, instead of distinct embeddings for each atom alone and then having to learn their compatibility.", "labels": [], "entities": []}, {"text": "While have trained embeddings only for entity pairs, we will in this paper explore the role of general bigram embeddings for KB completion, i.e. also the embeddings for other possible pairs of entities and relations.", "labels": [], "entities": [{"text": "KB completion", "start_pos": 125, "end_pos": 138, "type": "TASK", "confidence": 0.7867951393127441}]}, {"text": "This is achieved using a Factorization Machine (FM) framework) that is modular in its feature components, allowing us to selectively add or discard certain bigram embeddings and compare their relative importance.", "labels": [], "entities": []}, {"text": "All models are empirically compared and evaluated on the fb15k237 dataset from . In summary, our main contributions are: i) Adressing the question of generic bigram embeddings in a KB completion model for the first time; ii) The adaption of Factorization Machines for this matter; iii) Experimental findings for comparing different bigram embedding models on fb15k237.", "labels": [], "entities": [{"text": "fb15k237 dataset", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.9869081079959869}, {"text": "fb15k237", "start_pos": 359, "end_pos": 367, "type": "DATASET", "confidence": 0.9619234204292297}]}], "datasetContent": [{"text": "The bigram embedding models are tested on fb15k237 ( ), a dataset comprising both Freebase facts and lexicalized dependency path relationships between entities.", "labels": [], "entities": []}, {"text": "Training Details and Evaluation We optimized the loss using AdaM () with minibatches of size 1024, using initial learning rate 1.0 and initialize model parameters from N (0, 1).", "labels": [], "entities": []}, {"text": "Furthermore, a hyperparameter \u03c4 < 1 like in ( ) is introduced to discount the importance of textual mentions in the loss.", "labels": [], "entities": []}, {"text": "When sampling a negative fact we alter the object of a given training fact (s, r, o) at random too \u2208 E, and repeat this \u03b7 times, sampling negative facts every epoch anew.", "labels": [], "entities": []}, {"text": "There is a small implied risk of sampling positive facts as negative, but this is rare and the discounted loss weight of negative samples mitigates the issue further.", "labels": [], "entities": []}, {"text": "Hyperparameters (L 2 -regularisation, \u03b7, \u03c4 , latent dimension k) are selected in a grid search for minimising Mean Reciprocal Rank (MRR) on a fixed random subsample of size 1000 of the validation set.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 110, "end_pos": 136, "type": "METRIC", "confidence": 0.8963920474052429}]}, {"text": "All reported results are for the test set.", "labels": [], "entities": []}, {"text": "We use the competitive unit model in % and best result in bold.", "labels": [], "entities": []}, {"text": "The optimal value for \u03c4 is indicated as well.", "labels": [], "entities": []}, {"text": "DistMult as baseline and employ the same ranking evaluation scheme as in (  and , computing filtered MRR and HITS scores whilst ranking true test facts among candidate facts with altered object.", "labels": [], "entities": [{"text": "HITS", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.7539021372795105}]}, {"text": "Particular bigrams that have not been observed during training have no learned embedding; a 0-embedding is used for these.", "labels": [], "entities": []}, {"text": "This nullifies their impact on the score and models the back-off to using nonzero embeddings.", "labels": [], "entities": []}, {"text": "Results gives an overview of the general results for the different models.", "labels": [], "entities": []}, {"text": "Clearly, some of the bigram models can obtain an improvement over the unit DistMult model.", "labels": [], "entities": []}, {"text": "Ina more fine-grained analysis of model performances, characterized by whether entity pairs of test facts had textual mentions available in training (with TM) or not (without TM), the results exhibit a similar pattern like in ( ): most models perform worse on test facts with TM, only model F , which can learn very little without relations has a reversed behavior.", "labels": [], "entities": []}, {"text": "A side observation is that several models achieved highest overall MRR with \u03c4 = 0, i.e. when not using TM.", "labels": [], "entities": [{"text": "MRR", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.7984208464622498}]}, {"text": "The sum of the three more light-weight bigram models performs better than the full FM, even though the same types of embeddings are used.", "labels": [], "entities": []}, {"text": "A possible explanation is that applying the same embedding in several interactions with other embeddings (as in the full FM) instead of only one interaction (like in (*)+(**)+(***)) makes it harder to learn since its multiple functionalities are competing.", "labels": [], "entities": []}, {"text": "Another interesting finding is that some bigram types achieve much better results than others, in particular model (**).", "labels": [], "entities": []}, {"text": "A possible explanation becomes apparent with closer inspection of the test set: a given test fact f usually contains at least one bigram b \u2208 B f which has never been observed yet.", "labels": [], "entities": []}, {"text": "In these cases the bigram embedding is 0 by design and only the offset values are used.", "labels": [], "entities": [{"text": "offset", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9535147547721863}]}, {"text": "The proportions of test facts for which this happens are 73%, 10% and 24% respectively for the bigrams (s, o), (r, o), and (s, r).", "labels": [], "entities": []}, {"text": "Thus models (**) and (***) already have a definite advantage over model (*) that originates purely from the nature of the data.", "labels": [], "entities": []}, {"text": "A trivial but somehow important lesson we can learn from this is that if we know about the relative prevalence of different bigrams (or more generally: sub-tuples) in our dataset, we can incorporate and exploit this in the sub-tuples we choose.", "labels": [], "entities": []}, {"text": "Finally, for the initial example with relation eating/practicer of diet/diet and object Veganism, we indeed find that in all instances model (**) with its (r, o) embedding gives the correct fact in the top 2 predictions, while the purely compositional DistMult model ranks it far outside the top 10.", "labels": [], "entities": []}, {"text": "More generally, cases in which only a single object co-appeared with a test fact relation during training had 95, 3% HITS@1 with model (**) while only 52, 6% for DistMult.", "labels": [], "entities": [{"text": "HITS@1", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9530932704607645}]}, {"text": "This supports the intuition that bigram embeddings of (r, o) are in fact better suited for cases in which very few objects are possible fora relation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set metrics for different models and varying unit and bigram embeddings on fb15k237, all performance numbers", "labels": [], "entities": []}]}