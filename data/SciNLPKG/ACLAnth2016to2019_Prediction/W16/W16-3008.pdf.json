{"title": [{"text": "Identification of Mentions and Relations between Bacteria and Biotope from PubMed Abstracts", "labels": [], "entities": [{"text": "Identification of Mentions and Relations between Bacteria and Biotope", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.894238273302714}, {"text": "PubMed Abstracts", "start_pos": 75, "end_pos": 91, "type": "DATASET", "confidence": 0.9018460214138031}]}], "abstractContent": [{"text": "This paper presents our participation in the Bacteria/Biotope track from the 2016 BioNLP Shared-Task.", "labels": [], "entities": []}, {"text": "Our methods rely on a combination of distinct machine-learning and rule-based systems.", "labels": [], "entities": []}, {"text": "We used CRF and post-processing rules to identify mentions of bacteria and biotopes, a rule-based approach to normalize the concepts in the ontology and the taxonomy, and SVM to identify relations between bacteria and biotopes.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we present the methods we used while participating in the Bacteria/Biotope track from the 2016 BioNLP Shared-Task.", "labels": [], "entities": [{"text": "Bacteria/Biotope track from the 2016 BioNLP Shared-Task", "start_pos": 73, "end_pos": 128, "type": "DATASET", "confidence": 0.7028043733702766}]}, {"text": "We partially reused the method we designed while participating in the previous edition of the challenge, and we updated afterwards while designing new experiments ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We designed several experiments, depending on the size of the training corpus and whether we used or not post-processing rules and embedded entities processing.", "labels": [], "entities": []}, {"text": "Results are presented in section 5.1.1.", "labels": [], "entities": []}, {"text": "The configuration we used on the test dataset is the following one: we trained the final CRF model on all available annotated files (193 files), we applied post-processing rules to correct the CRF outputs, and we processed the embedded entities through a last script.", "labels": [], "entities": []}, {"text": "In this section, we present the results we achieved on the development dataset.", "labels": [], "entities": []}, {"text": "Since we produced outputs compatible with the BRAT annotation tool, results were computed using the BRATeval evaluation tool developed by and updated by.", "labels": [], "entities": []}, {"text": "This evaluation tool allows us to evaluate all kinds of entities (single, embedded and discontinuous entities) as well as relations between entities.", "labels": [], "entities": []}, {"text": "presents the results we achieved on the development dataset in the named entity recognition sub-task.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6551546653111776}]}, {"text": "We give both the F-measure we achieved on each category (bacteria, habitat, geographical) and the detailed overall results (exact match).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9983986020088196}, {"text": "exact match", "start_pos": 124, "end_pos": 135, "type": "METRIC", "confidence": 0.9568372070789337}]}, {"text": "We designed five experiments:: Results on the development dataset, F-measure for each category (Bact=Bacteria, Hab=Habitat, Geo=Geographical) and overall results (P=Precision, R=Recall, F=F-measure) depending on the experiment presents the results we achieved on the development dataset for the categorization task.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9873691201210022}, {"text": "Recall", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.5266833901405334}, {"text": "F-measure", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.5148825645446777}]}, {"text": "Our evaluation only computes an exact match between the IDs from the taxonomy and the ontology provided in the hypothesis and the reference.", "labels": [], "entities": []}, {"text": "This evaluation does not compute any similarity distance within the hypothesis and reference categories.", "labels": [], "entities": [{"text": "similarity distance", "start_pos": 37, "end_pos": 56, "type": "METRIC", "confidence": 0.9367244839668274}]}, {"text": "We give the overall and detailed results for both the OntoBiotope ontology and the NCBI taxonomy.", "labels": [], "entities": [{"text": "OntoBiotope ontology", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.5619863569736481}, {"text": "NCBI taxonomy", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.9372412860393524}]}, {"text": "Results are provided for two tasks:   Since the online evaluation service provides a distinct evaluation (giving final scores and using different metrics), in order to compare the results we achieved on both the development and the test datasets, we present in table 5 the results we achieved on all tasks on the development datasets using our last configuration, as computed by the evaluation service.", "labels": [], "entities": []}, {"text": "presents the results we achieved on the test dataset.", "labels": [], "entities": []}, {"text": "Our results are similar to results obtained on the development datasets.", "labels": [], "entities": []}, {"text": "This observation highlights the robustness of our methods.", "labels": [], "entities": []}, {"text": "We ranked second (out of 2) on all categorization tasks.", "labels": [], "entities": []}, {"text": "We ranked third (out of 11) on the event task, and first (out of 3) on the event+ner task.", "labels": [], "entities": []}, {"text": "At last, we were the only participant on all knowledge-based tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of annotations per category in  each dataset (test #1=dataset with reference an- notations of entities, #2=dataset without annota- tions). Grey background refers to the number of  predictions to be made during the challenge", "labels": [], "entities": []}, {"text": " Table 2: Results on the development dataset,  F-measure for each category (Bact=Bacteria,  Hab=Habitat, Geo=Geographical) and overall re- sults (P=Precision, R=Recall, F=F-measure) de- pending on the experiment", "labels": [], "entities": [{"text": "F-measure", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9906678795814514}, {"text": "re- sults (P=Precision, R=Recall", "start_pos": 135, "end_pos": 167, "type": "METRIC", "confidence": 0.7158462323925712}, {"text": "F-measure", "start_pos": 171, "end_pos": 180, "type": "METRIC", "confidence": 0.6513959169387817}]}, {"text": " Table 3: Results (exact match) on the development  dataset on the categorization tasks (P=Precision,  R=Recall, F=F-measure)", "labels": [], "entities": [{"text": "Precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.6398224830627441}, {"text": "Recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.6446661353111267}, {"text": "F-measure", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.8198943734169006}]}, {"text": " Table 4: Results on the development dataset  on the relation identification tasks (P=Precision,  R=Recall, F=F-measure)", "labels": [], "entities": [{"text": "relation identification tasks", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.8573705951372782}, {"text": "Precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.6715313196182251}, {"text": "Recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.5416412353515625}, {"text": "F-measure", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.8255612850189209}]}, {"text": " Table 5: Official results computed on the  development datasets (SER=Slot Error Rate,  Mism=Mismatch, Ins=Insertion, Del=Deletion,  P=Precision, R=Recall, F=F-measure)", "labels": [], "entities": [{"text": "SER", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9990054965019226}, {"text": "Slot Error Rate", "start_pos": 70, "end_pos": 85, "type": "METRIC", "confidence": 0.8931050499280294}, {"text": "Mism=Mismatch", "start_pos": 88, "end_pos": 101, "type": "METRIC", "confidence": 0.8243248263994852}, {"text": "Ins=Insertion", "start_pos": 103, "end_pos": 116, "type": "METRIC", "confidence": 0.8401536146799723}, {"text": "Recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.5519855618476868}, {"text": "F-measure", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.8575940728187561}]}, {"text": " Table 6: Official results computed on the test  dataset (SER=Slot Error Rate, Mism=Mismatch,  Ins=Insertion,  Del=Deletion,  P=Precision,  R=Recall, F=F-measure)", "labels": [], "entities": [{"text": "SER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9991294741630554}, {"text": "Slot Error Rate", "start_pos": 62, "end_pos": 77, "type": "METRIC", "confidence": 0.9132516185442606}, {"text": "Mism=Mismatch", "start_pos": 79, "end_pos": 92, "type": "METRIC", "confidence": 0.8509421348571777}, {"text": "Ins=Insertion", "start_pos": 95, "end_pos": 108, "type": "METRIC", "confidence": 0.8503137429555258}, {"text": "Recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.5228945016860962}, {"text": "F-measure", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.8770642280578613}]}]}