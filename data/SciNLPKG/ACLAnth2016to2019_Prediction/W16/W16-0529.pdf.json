{"title": [{"text": "Combining Off-the-shelf Grammar and Spelling Tools for the Automatic Evaluation of Scientific Writing (AESW) Shared Task 2016", "labels": [], "entities": [{"text": "Automatic Evaluation of Scientific Writing (AESW) Shared Task", "start_pos": 59, "end_pos": 120, "type": "TASK", "confidence": 0.6940858155488968}]}], "abstractContent": [{"text": "We applied two standard, open source tools for detecting spelling and grammar errors to the AESW 2016 shared task: After the Deadline and LanguageTool.", "labels": [], "entities": [{"text": "detecting spelling and grammar errors", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.7536911725997925}, {"text": "AESW 2016 shared task", "start_pos": 92, "end_pos": 113, "type": "DATASET", "confidence": 0.9000182598829269}]}, {"text": "The tools' output was combined with a Maximum Entropy machine learning model to classify each input sentence as requiring or not requiring any edits.", "labels": [], "entities": []}, {"text": "This approach yielded the second-highest precision of 64.41% in the binary estimation task at AESW 2016, but also the lowest recall of 36.85%, resulting in an F-Measure of 46.34%.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9987818598747253}, {"text": "AESW 2016", "start_pos": 94, "end_pos": 103, "type": "DATASET", "confidence": 0.9148520231246948}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9993890523910522}, {"text": "F-Measure", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9987452030181885}]}], "introductionContent": [{"text": "The Automated Evaluation of Scientific Writing Shared Task) targeted the analysis of individual sentences, in order to assess whether or not a sentence as a whole requires editing or not).", "labels": [], "entities": []}, {"text": "The long-term vision behind this task is to \"promote the development of automated writing evaluation tools that can assist authors in writing scientific papers\".", "labels": [], "entities": []}, {"text": "Our work in this area is similarly motivated by the idea of providing an interactive \"virtual research assistant\" that supports researchers in their daily tasks.", "labels": [], "entities": []}, {"text": "Writing support includes providing interactive feedback on the quality of textual artifacts to academic authors.", "labels": [], "entities": []}, {"text": "Such a support generally requires achieving high precision over recall, as tools with too many false positives tend to be ignored by authors.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9988829493522644}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9971600770950317}]}, {"text": "Additionally, providing salient feedback on detected mistakes -ideally together with suggestions for improvements -to the authors is an important feature.", "labels": [], "entities": []}, {"text": "Stan-1 AESW 2016, http://textmining.lt/aesw/index.html dard, open source grammar and spelling tools have addressed these questions for quite sometime, but are generally not focused on academic texts.", "labels": [], "entities": [{"text": "Stan-1 AESW 2016", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.7887571454048157}]}, {"text": "Hence, we were interested in how well existing, general-purpose tools perform when applied to academic writing.", "labels": [], "entities": []}, {"text": "In our experiments, we applied two well-known tools, After the Deadline and LanguageTool (described in Section 2.3), to the AESW data sets.", "labels": [], "entities": [{"text": "AESW data sets", "start_pos": 124, "end_pos": 138, "type": "DATASET", "confidence": 0.9611161351203918}]}, {"text": "Our hypothesis was that these two tools are (i) complementary to some degree, so that their combination can increase precision and/or recall; and (ii) a machine learning approach can combine the tools' output with additional syntactical context information, thereby attuning them to academic writing.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9986981153488159}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9976211190223694}]}], "datasetContent": [{"text": "To facilitate the combination of the individual results, we integrated both tools into a pipeline through the General Architecture for Text Engineering (GATE)).", "labels": [], "entities": []}, {"text": "Each error reported by one of the tools is added to the input text in the form of an annotation, which holds a start-and end-offset, as well as a number of features, such as the type of the error, the internal rule that generated the error, and possibly suggestions for improvements, as shown in.", "labels": [], "entities": []}, {"text": "Additionally, we added a number of standard GATE plugins from the ANNIE pipeline) to perform tokenization, part-ofspeech tagging, and lemmatization on the input texts.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.7237620949745178}]}, {"text": "Finally, annotations spanning placeholder texts in the sentences, such as MATH , were filtered out, as these were particular to the AESW data.", "labels": [], "entities": [{"text": "MATH", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.6652188301086426}, {"text": "AESW data", "start_pos": 132, "end_pos": 141, "type": "DATASET", "confidence": 0.9538209736347198}]}], "tableCaptions": [{"text": " Table 3: Baseline experiments: Evaluation of the individual tools", "labels": [], "entities": []}, {"text": " Table 2: Three-fold cross-validation of the MaxEnt classifier on the training data with different feature sets", "labels": [], "entities": []}]}