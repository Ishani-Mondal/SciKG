{"title": [{"text": "Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning", "labels": [], "entities": [{"text": "Dialog State Tracking and Management", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.8966911196708679}]}], "abstractContent": [{"text": "This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN).", "labels": [], "entities": []}, {"text": "The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.7232399433851242}]}, {"text": "Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed.", "labels": [], "entities": []}, {"text": "We evaluated the proposed model on a 20 Question Game conversational game simulator.", "labels": [], "entities": []}, {"text": "Results show that the proposed method out-performs the modular-based baseline and learns a distributed representation of the latent dialog state.", "labels": [], "entities": []}], "introductionContent": [{"text": "Task-oriented dialog systems have been an important branch of spoken dialog system (SDS) research.", "labels": [], "entities": [{"text": "spoken dialog system (SDS)", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.6777183910210928}]}, {"text": "The SDS agent has to achieve some predefined targets (e.g. booking a flight) through natural language interaction with the users.", "labels": [], "entities": []}, {"text": "The typical structure of a task-oriented dialog system is outlined in).", "labels": [], "entities": []}, {"text": "This pipeline consists of several independently-developed modules: natural language understanding (the NLU) maps the user utterances to some semantic representation.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.6980274120966593}]}, {"text": "This information is further processed by the dialog state tracker (DST), which accumulates the input of the turn along with the dialog history.", "labels": [], "entities": []}, {"text": "The DST outputs the current dialog state and the dialog policy selects the next system action based on the dialog state.", "labels": [], "entities": []}, {"text": "Then natural language generation (NLG) maps the selected action to its surface form which is sent to the TTS (Text-to-Speech).", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 5, "end_pos": 32, "type": "TASK", "confidence": 0.7035898963610331}]}, {"text": "This process repeats until the agent's goal is satisfied.", "labels": [], "entities": []}, {"text": "The first issue is the credit assignment problem.", "labels": [], "entities": [{"text": "credit assignment problem", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.8970309098561605}]}, {"text": "Developers usually only get feedback from the end users, who inform them about system performance quality.", "labels": [], "entities": []}, {"text": "Determining the source of the error requires tedious error analysis in each module because errors from upstream modules can propagate to the rest of the pipeline.", "labels": [], "entities": []}, {"text": "The second limitation is process interdependence, which makes online adaptation challenging.", "labels": [], "entities": [{"text": "online adaptation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7250384092330933}]}, {"text": "For example, when one module (e.g. NLU) is retrained with new data, all the others (e.g DM) that depend on it become sub-optimal due to the fact that they were trained on the output distributions of the older version of the module.", "labels": [], "entities": []}, {"text": "Although the ideal solution is to retrain the entire pipeline to ensure global optimality, this requires significant human effort.", "labels": [], "entities": []}, {"text": "Due to these limitations, the goal of this study is to develop an end-to-end framework for taskoriented SDS that replaces 3 important modules: the NLU, the DST and the dialog policy with a single module that can be jointly optimized.", "labels": [], "entities": []}, {"text": "Developing such a model for task-oriented dialog sys-1 tems faces several challenges.", "labels": [], "entities": []}, {"text": "The foremost challenge is that a task-oriented system must learn a strategic dialog policy that can achieve the goal of a given task which is beyond the ability of standard supervised learning ().", "labels": [], "entities": []}, {"text": "The second challenge is that often a task-oriented agent needs to interface with structured external databases, which have symbolic query formats (e.g. SQL query).", "labels": [], "entities": []}, {"text": "In order to find answers to the users' requests from the databases, the agent must formulate a valid database query.", "labels": [], "entities": []}, {"text": "This is difficult for conventional neural network models which do not provide intermediate symbolic representations.", "labels": [], "entities": []}, {"text": "This paper describes a deep reinforcement learning based end-to-end framework for both dialog state tracking and dialog policy that addresses the above-mentioned issues.", "labels": [], "entities": [{"text": "dialog state tracking", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.861283520857493}]}, {"text": "We evaluated the proposed approach on a conversational game simulator that requires both language understanding and strategic planning.", "labels": [], "entities": []}, {"text": "Our studies yield promising results 1) in jointly learning policies for state tracking and dialog strategies that are superior to a modular-based baseline, 2) in efficiently incorporating various types of labelled data and 3) in learning dialog state representations.", "labels": [], "entities": [{"text": "state tracking", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.7434439659118652}]}, {"text": "Section 2 of the paper discusses related work; Section 3 reviews the basics of deep reinforcement learning; Section 4 describes the proposed framework; Section 5 gives experimental results and model analysis; and Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Performance of the three systems", "labels": [], "entities": []}, {"text": " Table 4 reports the precision and recall of  slot filling in these trajectories. The results indi-", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9996141195297241}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9994065761566162}, {"text": "slot filling", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.8397350311279297}]}]}