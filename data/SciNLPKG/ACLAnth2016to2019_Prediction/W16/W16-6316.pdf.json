{"title": [{"text": "POS Tagging Experts via Topic Modeling", "labels": [], "entities": [{"text": "POS Tagging Experts", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8277714848518372}, {"text": "Topic Modeling", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.8117311298847198}]}], "abstractContent": [{"text": "Part of speech taggers generally perform well on homogeneous data sets, but their performance often varies considerably across different genres.", "labels": [], "entities": [{"text": "Part of speech taggers", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6348042637109756}]}, {"text": "In this paper we investigate the adaptation of POS taggers to individual genres by creating POS tagging experts.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.774187296628952}, {"text": "POS tagging", "start_pos": 92, "end_pos": 103, "type": "TASK", "confidence": 0.6902061402797699}]}, {"text": "We use topic model-ing to determine genres automatically and then build a tagging expert for each genre.", "labels": [], "entities": []}, {"text": "We use Latent Dirichlet Allocation to cluster sentences into related topics, based on which we create the training experts for the POS tagger.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 131, "end_pos": 141, "type": "TASK", "confidence": 0.57835853099823}]}, {"text": "Likewise, we cluster the test sentences into the same topics and annotate each sentence with the corresponding POS tagging expert.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.5561222285032272}]}, {"text": "We show that using topic model experts enhances the accuracy of POS tagging by around half a percent point on average over the random baseline, and the 2-topic hard clustering model and the 10-topic soft clustering model improve over the full training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.999613344669342}, {"text": "POS tagging", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9110143482685089}]}], "introductionContent": [{"text": "Part-of-speech (POS) tagging is the task of assigning word classes to lexical items and is often considered a solved problem.", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6171987533569336}]}, {"text": "However, even though we can reach high accuracies on the Penn Treebank, POS taggers are sensitive to differences in genre (cf. e.g. ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.9958138167858124}, {"text": "POS taggers", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.8433437943458557}]}, {"text": "In the current research, we investigate a novel way of adapting POS taggers to different genres, but also to specific lexical and syntactic characteristics of texts.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.7239021956920624}]}, {"text": "We propose to use topic modeling, an unsupervised soft clustering method that clusters documents, or sentences in our case, into a distribution of individual topics.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.8134143948554993}]}, {"text": "We interpret the topics as specialized training sets, which are used to train a POS tagging expert for each topic.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.7544833719730377}]}, {"text": "Test sentences are also clustered into the same topics, and each test sentence is annotated by the corresponding POS tagging expert.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 113, "end_pos": 124, "type": "TASK", "confidence": 0.5847018957138062}]}, {"text": "We investigate different methods of converting topics into expert training sets.", "labels": [], "entities": []}, {"text": "Thus, our method is related to domain adaptation approaches () in that it focuses on adapting to specific characteristics of texts, but it is more generally applicable because it determines the domains and the experts automatically.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7455728650093079}]}, {"text": "It is also related to approaches of mitigating domain effects (e.g.,), but in contrast to those methods, we obtain individual experts that can be used and investigated separately.", "labels": [], "entities": []}, {"text": "Our results show that the topic modeling experts are sensitive to different genres (financial news vs. medical text) as well as to smaller differences between the Wall Street sentences.", "labels": [], "entities": [{"text": "Wall Street sentences", "start_pos": 163, "end_pos": 184, "type": "DATASET", "confidence": 0.9361405769983927}]}, {"text": "On average, the improvement over randomly selected subsets is around 0.5-1 percent point.", "labels": [], "entities": []}, {"text": "Our results also show that one major difference between the POS tagging experts based on topics models concerns the treatment of unknown words.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.7160240858793259}]}, {"text": "In the financial expert, such words have a much higher tendency to be assigned to the noun class.", "labels": [], "entities": []}, {"text": "And even though names are one of the most difficult classes, the error rate for them is reduced in the POS experts based on topic models.", "labels": [], "entities": [{"text": "error rate", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9812554121017456}]}, {"text": "The remainder of the paper is structured as follows: Section 2 discusses our research questions in more detail.", "labels": [], "entities": []}, {"text": "Section 3 discusses related work, and in section 4, we provide details about the data sets, the topic modeler, and the POS tagger.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 119, "end_pos": 129, "type": "TASK", "confidence": 0.5162590444087982}]}, {"text": "In section 5, we show the results, and in section 6, we draw our conclusions and discuss future extensions of our work.", "labels": [], "entities": []}, {"text": "120 Our investigation into creating POS tagging experts is based on the assumption that the data that we need to analyze is not homogeneous but rather a collection of different text types or even syntactic constructions.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.8396018147468567}]}, {"text": "In one setting, we may have a mixed set of newspaper articles, research papers, financial reports, and weblogs to analyze.", "labels": [], "entities": []}, {"text": "Ina different setting, we may have texts that use specialized vocabulary, such as in the biomedical domain or in law texts, or we could have headlines with an elliptical sentence structure.", "labels": [], "entities": []}, {"text": "For this reason, we assume that the POS tagger can reach a higher accuracy if we can split the data sets into more homogeneous subsets and then train individual expert POS taggers, specialized for individual subsets.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 36, "end_pos": 46, "type": "TASK", "confidence": 0.7891329526901245}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9978532195091248}, {"text": "POS taggers", "start_pos": 168, "end_pos": 179, "type": "TASK", "confidence": 0.6662670224905014}]}, {"text": "We determine these homogeneous subsets by using topic modeling.", "labels": [], "entities": []}, {"text": "Since topic modeling is unsupervised, the sentences will be divided into sets based on similarity.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.8444161713123322}]}, {"text": "This similarity maybe based on similarity of content, but it can also be based on similarity on the structural level.", "labels": [], "entities": []}, {"text": "For example, if we use the Penn Treebank, we could assume that one topic consists of sentences reporting changes of the stock market while another topic consists of sentences about the earthquake in San Francisco.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.9916559159755707}]}, {"text": "Yet, another topic may consist of mostly questions.", "labels": [], "entities": []}, {"text": "Our current research concentrates on answering the four questions described below.", "labels": [], "entities": []}, {"text": "In this investigation, we use a setting in which we perform topic modeling jointly on the training and test data.", "labels": [], "entities": []}, {"text": "This is a simplification of the problem since this means that we would have to create new experts every time anew sentence needs to be tagged.", "labels": [], "entities": []}, {"text": "We assume that we can rerun the topic modeling including test sentences and then match the new topics to the ones we obtained on the training set alone.", "labels": [], "entities": []}, {"text": "Another approach would be to use the similarity metrics by.", "labels": [], "entities": []}, {"text": "We will test these hypotheses in the future.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this set of experiments, we use the manually created corpus that contains WSJ and GENIA sentences in equal parts.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.8546817302703857}]}, {"text": "A logical first setting is to have the topic modeler distinguish between two different topics, to see if these two topics correspond to the two gold topics, WSJ and GENIA.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 157, "end_pos": 160, "type": "DATASET", "confidence": 0.9149906635284424}, {"text": "GENIA", "start_pos": 165, "end_pos": 170, "type": "DATASET", "confidence": 0.8821828365325928}]}, {"text": "We repeat the experiment using 5 and 10 topics to see if a finer granularity improves results.", "labels": [], "entities": []}, {"text": "We then use the trained POS tagging experts to annotate the test sentences based on their assigned topic.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.7277825772762299}]}, {"text": "Investigating the topic modeler splits.", "labels": [], "entities": []}, {"text": "The distributions of sentences in the training set and test set resulting from topic modeling are shown in  the 5-topic setting, and topic 10 for the 10 topics, which are most likely the topics for difficult to classify sentences.", "labels": [], "entities": []}, {"text": "Thus, in all cases, we have good splits, which should allow the POS tagging experts to learn specifics of the two corpora.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.6785628795623779}]}, {"text": "shows example words from the 2-topic experiment, which show a clear separation of topics into biomedical and financial terms.", "labels": [], "entities": []}, {"text": "The results of the POS tagging experiments for the 2-topic, 5-topic, and the 10-topic settings are shown in.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.7827399373054504}]}, {"text": "The results show that the experts created by the topic models outperform the randomly split models in all cases: For the 2-topic setting, we seethe smallest increase from 96.48% to 96.84%, while the 10-topic setting reaches the largest increase, from 95.49% to 96.34%.", "labels": [], "entities": []}, {"text": "However, note that the results in the 5-and 10-topic settings are slightly lower than the ones in the 2-topic setting.", "labels": [], "entities": []}, {"text": "This is due to the reduced training set size.", "labels": [], "entities": []}, {"text": "When we compare the topic modeling experts to the full training set, the 2-topic model reaches an improvement over the full training set.", "labels": [], "entities": []}, {"text": "The accuracy of the 5-topic setting almost reaches that of the full training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997130036354065}]}, {"text": "Thus, even with a fifth of the training set compared to the full set, the per-124  formance of topic models is almost at par with the results on the full training set.", "labels": [], "entities": []}, {"text": "The results lead us to the conclusion that a higher number of topics results in better experts, as shown by the gains over the random baseline.", "labels": [], "entities": []}, {"text": "However, the gain of a high number of experts is offset by the reduction of the training set.", "labels": [], "entities": []}, {"text": "Next, we investigate the results of the 10-topic setting more closely: shows the results of this setting per topic.", "labels": [], "entities": []}, {"text": "These results show that the individual topics vary considerably in size, from around 25 000 words (topic 10) to 150 000 words (topic 6).", "labels": [], "entities": []}, {"text": "However, contrary to expectation, there is no direct correlation between training set size and accuracy: Topic 10 has the lowest number of sentences, but its expert performs better than the topic 5 expert, which had access to more than 3 times the amount of training sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9993512034416199}]}, {"text": "There is also no clear correlation between accuracy and WSJ or GENIA topics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9994638562202454}, {"text": "WSJ", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.5893265008926392}, {"text": "GENIA", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.5619862675666809}]}, {"text": "While the WSJ topics 4, 5, and 7 are at the lower end of the accuracy range, topic 9 has a higher accuracy than GENIA topics 1 and 10 and a similar performance to topic 8.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7627463936805725}, {"text": "accuracy range", "start_pos": 61, "end_pos": 75, "type": "METRIC", "confidence": 0.9716810882091522}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9983718991279602}, {"text": "GENIA", "start_pos": 112, "end_pos": 117, "type": "DATASET", "confidence": 0.8598586320877075}]}, {"text": "Here, we investigate whether we can also successfully use topic modeling to create POS tagging experts in cases where there is only one genre.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.8153999447822571}]}, {"text": "That is, is topic modeling only sensitive towards genre differences or can it also detect smaller types of variation, and can those variations be translated into specialized POS tagging experts?", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.8002519607543945}, {"text": "POS tagging", "start_pos": 174, "end_pos": 185, "type": "TASK", "confidence": 0.770406037569046}]}, {"text": "We use the WSJ corpus for this set of experiments, and we compare an experiment with 10 topics to the two baselines.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 11, "end_pos": 21, "type": "DATASET", "confidence": 0.987028956413269}]}, {"text": "The results of these experiments are shown in.", "labels": [], "entities": []}, {"text": "We see a positive effect of using experts based on the hard clustering topic models over the random split: Accuracy increases from 95.16% to 95.53%.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9996793270111084}]}, {"text": "Similar to the GENIA+WSJ 10-topic experiment, we also do not reach the baseline using all training data.", "labels": [], "entities": [{"text": "GENIA+WSJ 10-topic experiment", "start_pos": 15, "end_pos": 44, "type": "DATASET", "confidence": 0.851672101020813}]}, {"text": "Per topic, there are similar trends to the ones for the WSJ+GENIA set-125: Comparing the standard data split to a random data split for WSJ data (10 topics).", "labels": [], "entities": [{"text": "WSJ+GENIA set-125", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.8479175567626953}, {"text": "WSJ data", "start_pos": 136, "end_pos": 144, "type": "DATASET", "confidence": 0.9109609425067902}]}, {"text": "ting, a large variation of topic sizes and no direct correlation of training set size and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9985916018486023}]}, {"text": "However, the soft clustering results show that there is a 0.8 percent improvement over the topic models and a small improvement over the full training set.", "labels": [], "entities": []}, {"text": "This reinforces the hypothesis that soft clustering can indeed handle the data sparseness issue even when the genres are not as clearly distinguishable as WSJ vs. GENIA.", "labels": [], "entities": [{"text": "GENIA", "start_pos": 163, "end_pos": 168, "type": "DATASET", "confidence": 0.9035732746124268}]}, {"text": "The differences between topic models and a random split are less pronounced than in the case of the combined WSJ+GENIA corpus.", "labels": [], "entities": [{"text": "WSJ+GENIA corpus", "start_pos": 109, "end_pos": 125, "type": "DATASET", "confidence": 0.8018758893013}]}, {"text": "One explanation for this maybe that the topics are less different from each other than in the WSJ+GENIA setting so that the POS tagging expert are not very different from each other.", "labels": [], "entities": [{"text": "WSJ+GENIA setting", "start_pos": 94, "end_pos": 111, "type": "DATASET", "confidence": 0.8033133894205093}, {"text": "POS tagging", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.7230931520462036}]}, {"text": "Another possible explanation is that this is a consequence of the way we split the WSJ corpus into training and test sets: As described in section 4, we use the standard split with section 02-21 as training set and section 22 as our current test set.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 83, "end_pos": 93, "type": "DATASET", "confidence": 0.9348679184913635}]}, {"text": "This means that the test set may contain different topics from the training set, which may generate problems in topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 112, "end_pos": 126, "type": "TASK", "confidence": 0.716946542263031}]}, {"text": "To test this hypothesis, we repeat the experiment with 10 topics, but this time, we perform a fivefold cross-validation for POS tagging on sections 02-22.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.8278339803218842}]}, {"text": "I.e., we vary the test set across all sections, to create a more homogeneous data split.", "labels": [], "entities": []}, {"text": "The results of this experiment, in comparison to previous results, are shown in.", "labels": [], "entities": []}, {"text": "We reach slightly higher results in the 5-fold cross-validation; accuracy increases from 95.53% to 95.70%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9997331500053406}]}, {"text": "This means that the training and test set in the standard setting are not optimally homogeneous.", "labels": [], "entities": []}, {"text": "However, since the difference inaccuracy is rather small, the differences between training and test set do not seem to impact the performance of our system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the data sets.", "labels": [], "entities": []}, {"text": " Table 2: Distribution of sentences from the WSJ+GENIA data set given 2, 5, and 10 topics (showing the  percentage of GENIA sentences per topic).", "labels": [], "entities": [{"text": "WSJ+GENIA data set", "start_pos": 45, "end_pos": 63, "type": "DATASET", "confidence": 0.8976891756057739}]}, {"text": " Table 3: Examples of words in topics for the 2- topic experiments on the WSJ+Genia corpus.", "labels": [], "entities": [{"text": "WSJ+Genia corpus", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.9406731575727463}]}, {"text": " Table 4: Comparing the topic model experts to the  baselines on the WSJ+GENIA data set.", "labels": [], "entities": [{"text": "WSJ+GENIA data set", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.8833168506622314}]}, {"text": " Table 5: Results for the individual topic model ex- perts for the WSJ+GENIA data.", "labels": [], "entities": [{"text": "WSJ+GENIA data", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.8456269055604935}]}, {"text": " Table 6: Results for soft clustering on 2, 5, and 10  topics experiments", "labels": [], "entities": [{"text": "soft clustering", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.5979401469230652}]}, {"text": " Table 7: Comparing topic model experts to the  baselines on WSJ data (10 topics).", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.935463160276413}]}, {"text": " Table 9: Unknown word rates and accuracies for known and unknown words in the WSJ+GENIA exper- iment using 2 topics.", "labels": [], "entities": [{"text": "Unknown word rates and accuracies", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.6678602635860443}, {"text": "WSJ+GENIA exper- iment", "start_pos": 79, "end_pos": 101, "type": "DATASET", "confidence": 0.8430118064085642}]}]}