{"title": [{"text": "Examining the Relationship between Preordering and Word Order Freedom in Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7324341088533401}]}], "abstractContent": [{"text": "We study the relationship between word order freedom and preordering in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 72, "end_pos": 103, "type": "TASK", "confidence": 0.649648537238439}]}, {"text": "To assess word order freedom, we first introduce a novel entropy measure which quantifies how difficult it is to predict word order given a source sentence and its syntactic analysis.", "labels": [], "entities": []}, {"text": "We then address preordering for two target languages at the far ends of the word order freedom spectrum, German and Japanese, and argue that for languages with more word order freedom, attempting to predict a unique word order given source clues only is less justified.", "labels": [], "entities": []}, {"text": "Subsequently, we examine lattices of n-best word order predictions as a unified representation for languages from across this broad spectrum and present an effective solution to a resulting technical issue, namely how to select a suitable source word order from the lattice during training.", "labels": [], "entities": []}, {"text": "Our experiments show that lattices are crucial for good empirical performance for languages with freer word order (English-German) and can provide additional improvements for fixed word order languages (English-Japanese).", "labels": [], "entities": []}], "introductionContent": [{"text": "Word order differences between a source and a target language area major challenge for machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.778602808713913}]}, {"text": "For phrase-based models, the number of possible phrase permutations is so large that reordering must be constrained locally to make the search space for the best hypothesis feasible.", "labels": [], "entities": []}, {"text": "However, constraining the space locally runs the risk that the optimal hypothesis is rendered out of reach.", "labels": [], "entities": []}, {"text": "Preordering of the source sentence has been embraced as away to ensure the reachability of certain target word order constellations for improved prediction of the target word order.", "labels": [], "entities": []}, {"text": "Preordering aims at predicting a permutation of the source sentence which has minimal word order differences with the target sentence; the permuted source sentence is passed onto a backend translation system trained to translate target-order source sentences into target sentences.", "labels": [], "entities": []}, {"text": "In essence, the preordering approach makes the assumption that it is feasible to predict target word order given only clues from the source sentence.", "labels": [], "entities": []}, {"text": "In the vast majority of work on preordering, a single preordered source sentence is passed onto the backend system, thereby making the stronger assumption that it is feasible to predict a unique preferred target word order.", "labels": [], "entities": []}, {"text": "But how reasonable are these assumptions and for which target languages?", "labels": [], "entities": []}, {"text": "Intuitively, the assumption of a unique preordering seems reasonable for translating into fixed word order languages such as Japanese, but for translation into languages with less strict word order such as German, this is unlikely to work.", "labels": [], "entities": []}, {"text": "In such languages there are multiple comparably plausible target word orders per source sentence because the underlying predicate-argument structure can be expressed with mechanisms other than word order alone (e.g. morphological inflections or intonation).", "labels": [], "entities": []}, {"text": "For these languages, it seems rather unlikely to be able to choose a unique word order given only source sentence clues.", "labels": [], "entities": []}, {"text": "In this paper, we want to shed light on the relationship between the target language's word order freedom and the feasibility of preordering.", "labels": [], "entities": []}, {"text": "We start out by contributing an information-theoretic measure to quantify the difficulty in predicting a preferred word order given the source sentence and its syntax.", "labels": [], "entities": []}, {"text": "Our measure provides empirical support for the intuition that it is often not possible to predict a unique word order for free word order languages, whereas it is more feasible for fixed word order languages such as Japanese.", "labels": [], "entities": []}, {"text": "Subsequently, we study the option of passing the n-best word order predictions, instead of 1-best, to the backend system as a lattice of possible word orders of the source sentence.", "labels": [], "entities": []}, {"text": "For the training of the backend system, the use of such permutation lattices raises a question: What should constitute the training corpus fora lattice-preordered translation system?", "labels": [], "entities": []}, {"text": "In previous work using single word order predictions, the training data consists of pairs of source and target sentences where the source sentence is either in target order (i.e. order based on word alignments) or preordered (i.e. predicted order).", "labels": [], "entities": []}, {"text": "In this work we contribute a novel approach for selecting training instances from the lattice of word order permutations: We select the permutation providing the best match with the target-order source sentence (we call this process \"lattice silver training\").", "labels": [], "entities": []}, {"text": "Our experiments show that for EnglishJapanese and English-German lattice preordering has a positive impact on the translation quality.", "labels": [], "entities": []}, {"text": "Whereas lattices enable further improvement for preordering English into the strict word order language Japanese, lattices in conjunction with our proposed lattice silver training scheme turnout to be crucial to reach satisfactory empirical performance for English-German.", "labels": [], "entities": []}, {"text": "This result highlights that when predicting word order of free word order languages given source clues only, it is important to ensure that the word order predictions and the backend system are suitably fitted together.", "labels": [], "entities": [{"text": "predicting word order of free word order languages given source clues", "start_pos": 33, "end_pos": 102, "type": "TASK", "confidence": 0.7982230457392606}]}], "datasetContent": [{"text": "In our translation experiments, we use the following experimental setup, datasets and parameters.", "labels": [], "entities": []}, {"text": "Translation system Translation experiments are performed with a phrase-based machine translation system, aversion of Moses () with extended lattice support.", "labels": [], "entities": [{"text": "Translation system Translation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8510425090789795}, {"text": "phrase-based machine translation", "start_pos": 64, "end_pos": 96, "type": "TASK", "confidence": 0.643680989742279}]}, {"text": "We use the basic Moses features and perform 15 iterations of batch MIRA).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.7772426605224609}]}, {"text": "English-Japanese Our experiments are performed on the NTCIR-8 Patent Translation (PATMT) Task.", "labels": [], "entities": [{"text": "NTCIR-8 Patent Translation (PATMT) Task", "start_pos": 54, "end_pos": 93, "type": "TASK", "confidence": 0.7706791843686785}]}, {"text": "Tuning is performed on the NTCIR-7 dev sets, and translation is evaluated on the test set from NTCIR-9.", "labels": [], "entities": [{"text": "NTCIR-7 dev sets", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.9516425927480062}, {"text": "translation", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9663604497909546}, {"text": "NTCIR-9", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9799264073371887}]}, {"text": "All data is tokenized (using the Moses tokenizer for English and KyTea 5 for Japanese (Neubig et al., 2011)) and filtered for sentences between 4 and 50 words.", "labels": [], "entities": []}, {"text": "As a baseline we use a translation system with distortion limit 6 and a lexicalized reordering model (.", "labels": [], "entities": []}, {"text": "We use a 5-gram language model estimated using lmplz ( on the target side of the parallel corpus.", "labels": [], "entities": []}, {"text": "English-German For translation into German, we built a machine translation system based on the WMT 2016 news translation data.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9697808027267456}, {"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.6979915797710419}, {"text": "WMT 2016 news translation data", "start_pos": 95, "end_pos": 125, "type": "DATASET", "confidence": 0.9305862069129944}]}, {"text": "The system is trained on all available parallel data, consisting of 4.5m sentence pairs from Europarl ( Preordering models For German, we use the neural lattice preordering model introduced in Section 4.1.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.9765675663948059}]}, {"text": "The model is trained on the full parallel training data (4.5m sentences) based on the automatic word alignments used by the translation system.", "labels": [], "entities": []}, {"text": "Source dependency trees are produced by TurboParser, 12 which was trained on the English version of) with content-head dependencies.", "labels": [], "entities": []}, {"text": "For translation into Japanese, we train a Reordering Grammar model for 10 iterations of EM on a training set consisting of 786k sentence pairs with automatic alignments.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9738403558731079}]}, {"text": "We report lowercased BLEU () and Kendall \u03c4 calculated from the forcealigned hypothesis and reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9961950778961182}, {"text": "Kendall \u03c4", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9411403834819794}]}, {"text": "Statistical significance tests are performed for the translation scores using the bootstrap resampling method with p-value < 0.05).", "labels": [], "entities": [{"text": "translation", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.9291505217552185}]}, {"text": "The standard preordering systems (\"first-best\" in and 4) use an additional lexicalized reordering model (MSD), while the lattice systems use only lattice distortion.", "labels": [], "entities": [{"text": "lexicalized reordering model (MSD)", "start_pos": 75, "end_pos": 109, "type": "METRIC", "confidence": 0.6669098089138666}]}, {"text": "For training preordered translation models, we recreate word alignments from the original MGIZA alignments and the permutation for EnDe and re-align preordered and target sentences for En-Ja using MGIZA.", "labels": [], "entities": []}, {"text": "13 English-German Translation results for translation into German are shown in.", "labels": [], "entities": [{"text": "translation into German", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.8833479682604471}]}, {"text": "For this language pair, we found standard preordering to work poorly.", "labels": [], "entities": []}, {"text": "This is despite the fact that the oracle order (i.e. the source words in the test set are preordered according to the word alignments) shows significant potential.", "labels": [], "entities": []}, {"text": "A lattice packed with 1000 permutations on the other hand, http://cs.cmu.edu/ \u02dc ark/TurboParser/ 13 Re-aligning the sentences with MGIZA generally improves results, which implies that we are likely underestimating the results for En-De.  performs better even when translating monotonically with a distortion limit of 0.", "labels": [], "entities": []}, {"text": "Lattice silver training To examine the utility of the lattice silver training scheme, we train systems which differ only in the way the training data is extracted.", "labels": [], "entities": []}, {"text": "shows that for EnglishGerman, lattice silver training is successful in bridging the gap between the preordering model and the alignment-based target word order, both for monotonic translation and when allowing the decoder to additionally reorder translations.: Lattice silver training (BLEU, En-De).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 286, "end_pos": 290, "type": "METRIC", "confidence": 0.9966953992843628}]}, {"text": "English-Japanese Results for translation into Japanese are shown in.", "labels": [], "entities": [{"text": "translation into Japanese", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.8841522137324015}]}, {"text": "Discussion Although preordering with a single permutation already works well for the strict word order language Japanese, packing the word order ambiguity into a lattice allows the machine translation system to achieve even better translation monotonically than allowing a distortion of 6 and an additional lexicalized reordering model on top  of a single permutation.", "labels": [], "entities": []}, {"text": "We noticed that lexicalized reordering helped the first-best systems and hence report this stronger baseline.", "labels": [], "entities": []}, {"text": "In principle, lexicalized reordering can also be used with 0-distortion lattice translation, and we plan to investigate this option in the future.", "labels": [], "entities": []}, {"text": "Linguistic intuition and the empirical results presented in Section 3 suggest that compared to Japanese, German shows more word order freedom.", "labels": [], "entities": []}, {"text": "Consequently, we assumed that a first-best preordering model would not perform well on the language pair EnglishGerman, and indeed the results in confirm this assumption.", "labels": [], "entities": [{"text": "EnglishGerman", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.8379827737808228}]}, {"text": "For both language pairs, translating a lattice of predicted permutations outperforms the baselines, thus reducing the gap between translation with predicted word order and oracle word order.", "labels": [], "entities": []}, {"text": "However, permutation lattices turnout to be the key to enabling any improvement at all for the language pair English-German in the context of preordering.", "labels": [], "entities": []}, {"text": "This language pair can benefit from the improved interaction between word order and translation decisions.", "labels": [], "entities": []}, {"text": "These findings go in tandem with our analysis in Section 3 (see, particularly, the prediction of our informationtheoretic word order freedom metric that it should be more difficult to determine German word order from English clues.", "labels": [], "entities": []}, {"text": "Our main focus in this paper was on the language pairs English-German and English-Japanese.", "labels": [], "entities": []}, {"text": "Hence, while our results provide an empirical data point for the utility of permutation lattices for free word order languages, we plan to provide further empirical support by performing experiments with a broader range of language pairs in future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Permutations and lattice size (En-De).", "labels": [], "entities": []}, {"text": " Table 2: Translation results English-German.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9567035436630249}]}, {"text": " Table 3: Lattice silver training (BLEU, En-De).", "labels": [], "entities": [{"text": "Lattice silver training", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8609340985616049}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9773271679878235}, {"text": "En-De", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.951972246170044}]}, {"text": " Table 4: Translation results English-Japanese.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9671052694320679}]}]}