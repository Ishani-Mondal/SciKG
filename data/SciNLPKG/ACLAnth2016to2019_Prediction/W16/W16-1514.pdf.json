{"title": [{"text": "Identifying Referenced Text in Scientific Publications by Summarisation and Classification Techniques", "labels": [], "entities": [{"text": "Identifying Referenced Text in Scientific Publications by Summarisation and Classification", "start_pos": 0, "end_pos": 90, "type": "TASK", "confidence": 0.7681740939617157}]}], "abstractContent": [{"text": "This report describes our contribution to the 2nd Computational Linguistics Scientific Document Summarization Shared Task (CL-SciSumm 2016), which asked to identify the relevant text span in a reference paper that corresponds to a citation in another document that cites this paper.", "labels": [], "entities": [{"text": "Computational Linguistics Scientific Document Summarization Shared Task (CL-SciSumm 2016)", "start_pos": 50, "end_pos": 139, "type": "TASK", "confidence": 0.7867170030420477}]}, {"text": "We developed three different approaches based on summari-sation and classification techniques.", "labels": [], "entities": []}, {"text": "First, we applied a modified version of an unsupervised summarisation technique, TextSentenceRank, to the reference document, which incorporates the similarity of sentences to the citation on a textual level.", "labels": [], "entities": []}, {"text": "Second, we employed classification to select from candidates previously extracted through the original TextSen-tenceRank algorithm.", "labels": [], "entities": []}, {"text": "Third, we used unsupervised summarisation of the relevant sub-part of the document that was previously selected in a supervised manner.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extractive summarisation of a textual document is the process of finding a representative subset of the document text that captures as much information about the original document as possible.", "labels": [], "entities": [{"text": "Extractive summarisation of a textual document", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8960589667161306}]}, {"text": "A promising idea in the realm of scientific publications is to consider the set of sentences that cite a paper as a summary created by the research community.", "labels": [], "entities": []}, {"text": "Here we describe our contribution to the 2nd Computational Linguistics Scientific Document Summarization Shared Task (CL-SciSumm 2016) 1, which aims at exploring and encouraging novel techniques for scientific paper summarisation along this direction.", "labels": [], "entities": [{"text": "2nd Computational Linguistics Scientific Document Summarization Shared Task (CL-SciSumm 2016) 1", "start_pos": 41, "end_pos": 136, "type": "TASK", "confidence": 0.7627854347229004}, {"text": "scientific paper summarisation", "start_pos": 199, "end_pos": 229, "type": "TASK", "confidence": 0.6559408704439799}]}, {"text": "This task takes place at the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2016) [2] at the Joint Conference on Digital Libraries (JCDL '16) and is a follow-up on the CL Pilot Task that has been conducted as apart of the BiomedSumm Track at the Text Analysis Conference 2014 [5].", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.7178542613983154}, {"text": "Joint Conference on Digital Libraries (JCDL '16)", "start_pos": 170, "end_pos": 218, "type": "TASK", "confidence": 0.5515878349542618}, {"text": "BiomedSumm Track at the Text Analysis Conference 2014", "start_pos": 299, "end_pos": 352, "type": "TASK", "confidence": 0.5351085402071476}]}, {"text": "The dataset provided for this year's task consists of a set of reference papers (RP), each of which is accompanied by a set of citing papers (CP).", "labels": [], "entities": []}, {"text": "The goal is to identify the text span in the RP which corresponds to the citations in CP (Task 1A) as well as the discourse facet of the RP this text span belongs to.", "labels": [], "entities": []}, {"text": "Human annotators have created the ground truth in the form of pairs of citation text and cited text on the granularity level of sentences.", "labels": [], "entities": []}, {"text": "For Task 1A, we implemented a number of approaches that employ both unsupervised and supervised techniques that differ in the way how information from the citing sentence in the CP is incorporated into the process.", "labels": [], "entities": []}, {"text": "In total, we submitted three runs, corresponding to our three approaches for Task 1A: (i) modified-tsr, (ii) tsr-sent-class, and (iii) sect-class-tsr.", "labels": [], "entities": []}, {"text": "First, in a completely unsupervised setting, we applied a modified variant of our TextSentenceRank algorithm to the RP (modified-tsr ).", "labels": [], "entities": []}, {"text": "TextSentenceRank is a graph based ranking algorithm, a refinement of the well-known TextRank algorithm, which is applied to text in order to extract key words and/or key sentences.", "labels": [], "entities": []}, {"text": "For the task at hand, we investigated a specific weighting that takes into account the information provided by the CP.", "labels": [], "entities": []}, {"text": "As a second approach for Task 1A, we employed a supervised classification setting following an unsupervised preprocessing (tsr-sent-class).", "labels": [], "entities": []}, {"text": "Through the original version of TextSentenceRank we pre-selected candidate sentences from the RP, independently from the CP, that are potential text spans for being cited.", "labels": [], "entities": []}, {"text": "For a given citation in the CP, we then selected the corresponding candidate through supervised classification.", "labels": [], "entities": []}, {"text": "In our third option, we took a dual approach (sect-class-tsr ).", "labels": [], "entities": []}, {"text": "We first used supervised learning to identify the relevant sub-part (section) of the RP that corresponds to the citing sentence in the CP.", "labels": [], "entities": []}, {"text": "Once the relevant section has been found, we used the original version of TextSentenceRank on this sub-part, independent from the CP, to identify referenced text spans.", "labels": [], "entities": []}, {"text": "For Task 1B, we used a similar classifier as for identification of the section.", "labels": [], "entities": [{"text": "identification of the section", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.8324763625860214}]}, {"text": "We used features derived from the citing sentence as well es from the extracted text span in the reference document to determine the discourse facet.", "labels": [], "entities": []}, {"text": "This is applied in all three approaches to Task 1A.", "labels": [], "entities": [{"text": "Task 1A", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.6391458809375763}]}, {"text": "In principle, TextSentenceRank is able to extract multiple candidate text spans scattered across the document, but since the task description required the extraction of consecutive referenced text, we decided to output a single sentence as the extracted reference span in all of our approaches.", "labels": [], "entities": []}, {"text": "This report is structured as follows.", "labels": [], "entities": []}, {"text": "In sections 2 and 3 we explain our approaches for both Task 1A and Task 1B in detail.", "labels": [], "entities": []}, {"text": "In section 4 we individually evaluate the classifiers that we used in our approaches and present our results for the overall task.", "labels": [], "entities": []}, {"text": "In the end, in section 5 we conclude, discuss our findings, and give an outlook to potential future work.", "labels": [], "entities": []}, {"text": "In this subsection we detail our three approaches for Task 1A, the identification of referenced text spans.", "labels": [], "entities": [{"text": "identification of referenced text spans", "start_pos": 67, "end_pos": 106, "type": "TASK", "confidence": 0.8704458236694336}]}, {"text": "We also introduce TextSentenceRank as abase method which is used in all three runs.", "labels": [], "entities": [{"text": "TextSentenceRank", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.8695113062858582}]}], "datasetContent": [{"text": "In our evaluation, we first determine the isolated performance of individual components that we used in our approaches.", "labels": [], "entities": []}, {"text": "Then we present our results for the overall task.", "labels": [], "entities": []}, {"text": "We performed these evaluations on both the provided development set and the training set, as the results on the test corpus have not yet been made available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Performance of the sentence classifier on the development set and on the  training set, evaluated by 10-fold cross validation. Precision, recall, and F1 values are  given with respect to the positive class. Accuracy is the amount of correctly classi- fied instances. The numbers in brackets denote the total number of instances for the  respective scenario.", "labels": [], "entities": [{"text": "Precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9984433054924011}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9954313039779663}, {"text": "F1", "start_pos": 160, "end_pos": 162, "type": "METRIC", "confidence": 0.9997032284736633}, {"text": "Accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.997802197933197}]}, {"text": " Table 2. Performance of the section classifier on the development set and on the  training set, evaluated by 10-fold cross validation. Precision, recall, and F1 values are  given with respect to the positive class. Accuracy is the amount of correctly classi- fied instances. The numbers in brackets denote the total number of instances for the  respective scenario.", "labels": [], "entities": [{"text": "Precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.998757004737854}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9953863024711609}, {"text": "F1", "start_pos": 159, "end_pos": 161, "type": "METRIC", "confidence": 0.9997616410255432}, {"text": "Accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.9981504082679749}]}, {"text": " Table 4. Confusion matrix of the discourse facet classifier on the training set, as well  as precision and recall for each label, evaluated by 10-fold cross validation. Column  headers denote classifier output, row headers denote true labels created by the human  annotators.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9996426105499268}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9990070462226868}]}, {"text": " Table 5. Overall task performance on the development set. For each of the three runs  and for each topic in the development set we show the number of citances for which the  extracted reference span lies within 10 sentences of the true reference span. Numbers  in brackets, if available, count citances where the extracted reference span overlaps the  true reference span created by human annotators.", "labels": [], "entities": []}]}